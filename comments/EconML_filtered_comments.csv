Version,Commit Message
v0.15.1,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.15.1,configuration is all pulled from setup.cfg
v0.15.1,-*- coding: utf-8 -*-
v0.15.1,
v0.15.1,Configuration file for the Sphinx documentation builder.
v0.15.1,
v0.15.1,This file does only contain a selection of the most common options. For a
v0.15.1,full list see the documentation:
v0.15.1,http://www.sphinx-doc.org/en/main/config
v0.15.1,-- Path setup --------------------------------------------------------------
v0.15.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.15.1,add these directories to sys.path here. If the directory is relative to the
v0.15.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.15.1,
v0.15.1,-- Project information -----------------------------------------------------
v0.15.1,-- General configuration ---------------------------------------------------
v0.15.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.15.1,
v0.15.1,needs_sphinx = '1.0'
v0.15.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.15.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.15.1,ones.
v0.15.1,TODO: enable type aliases
v0.15.1,napoleon_preprocess_types = True  # needed for type aliases to work
v0.15.1,napoleon_type_aliases = {
v0.15.1,"""array_like"": "":term:`array_like`"","
v0.15.1,"""ndarray"": ""~numpy.ndarray"","
v0.15.1,"""RandomState"": "":class:`~numpy.random.RandomState`"","
v0.15.1,"""DataFrame"": "":class:`~pandas.DataFrame`"","
v0.15.1,"""Series"": "":class:`~pandas.Series`"","
v0.15.1,}
v0.15.1,"Add any paths that contain templates here, relative to this directory."
v0.15.1,The suffix(es) of source filenames.
v0.15.1,You can specify multiple suffix as a list of strings:
v0.15.1,
v0.15.1,"source_suffix = ['.rst', '.md']"
v0.15.1,The root toctree document.
v0.15.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.15.1,for a list of supported languages.
v0.15.1,
v0.15.1,This is also used if you do content translation via gettext catalogs.
v0.15.1,"Usually you set ""language"" from the command line for these cases."
v0.15.1,"List of patterns, relative to source directory, that match files and"
v0.15.1,directories to ignore when looking for source files.
v0.15.1,This pattern also affects html_static_path and html_extra_path.
v0.15.1,The name of the Pygments (syntax highlighting) style to use.
v0.15.1,-- Options for HTML output -------------------------------------------------
v0.15.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.15.1,a list of builtin themes.
v0.15.1,
v0.15.1,Theme options are theme-specific and customize the look and feel of a theme
v0.15.1,"further.  For a list of options available for each theme, see the"
v0.15.1,documentation.
v0.15.1,
v0.15.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.15.1,"relative to this directory. They are copied after the builtin static files,"
v0.15.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.15.1,html_static_path = ['_static']
v0.15.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.15.1,to template names.
v0.15.1,
v0.15.1,The default sidebars (for documents that don't match any pattern) are
v0.15.1,defined by theme itself.  Builtin themes are using these templates by
v0.15.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.15.1,'searchbox.html']``.
v0.15.1,
v0.15.1,html_sidebars = {}
v0.15.1,-- Options for HTMLHelp output ---------------------------------------------
v0.15.1,Output file base name for HTML help builder.
v0.15.1,-- Options for LaTeX output ------------------------------------------------
v0.15.1,The paper size ('letterpaper' or 'a4paper').
v0.15.1,
v0.15.1,"'papersize': 'letterpaper',"
v0.15.1,"The font size ('10pt', '11pt' or '12pt')."
v0.15.1,
v0.15.1,"'pointsize': '10pt',"
v0.15.1,Additional stuff for the LaTeX preamble.
v0.15.1,
v0.15.1,"'preamble': '',"
v0.15.1,Latex figure (float) alignment
v0.15.1,
v0.15.1,"'figure_align': 'htbp',"
v0.15.1,Grouping the document tree into LaTeX files. List of tuples
v0.15.1,"(source start file, target name, title,"
v0.15.1,"author, documentclass [howto, manual, or own class])."
v0.15.1,-- Options for manual page output ------------------------------------------
v0.15.1,One entry per manual page. List of tuples
v0.15.1,"(source start file, name, description, authors, manual section)."
v0.15.1,-- Options for Texinfo output ----------------------------------------------
v0.15.1,Grouping the document tree into Texinfo files. List of tuples
v0.15.1,"(source start file, target name, title, author,"
v0.15.1,"dir menu entry, description, category)"
v0.15.1,-- Options for Epub output -------------------------------------------------
v0.15.1,Bibliographic Dublin Core info.
v0.15.1,The unique identifier of the text. This can be a ISBN number
v0.15.1,or the project homepage.
v0.15.1,
v0.15.1,epub_identifier = ''
v0.15.1,A unique identification for the text.
v0.15.1,
v0.15.1,epub_uid = ''
v0.15.1,A list of files that should not be packed into the epub file.
v0.15.1,-- Extension configuration -------------------------------------------------
v0.15.1,-- Options for intersphinx extension ---------------------------------------
v0.15.1,Example configuration for intersphinx: refer to the Python standard library.
v0.15.1,-- Options for todo extension ----------------------------------------------
v0.15.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.15.1,-- Options for doctest extension -------------------------------------------
v0.15.1,we can document otherwise excluded entities here by returning False
v0.15.1,or skip otherwise included entities by returning True
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Calculate residuals
v0.15.1,Estimate E[T_res | Z_res]
v0.15.1,TODO. Deal with multi-class instrument
v0.15.1,Calculate nuisances
v0.15.1,Estimate E[T_res | Z_res]
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.15.1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.15.1,TODO. Deal with multi-class instrument
v0.15.1,Estimate final model of theta(X) by minimizing the square loss:
v0.15.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.15.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.15.1,at the expense of some small bias. For points with very small covariance we revert
v0.15.1,to the model-based preliminary estimate and do not add the correction term.
v0.15.1,Estimate preliminary theta in cross fitting manner
v0.15.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.15.1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.15.1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.15.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.15.1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.15.1,TODO. The solution below is not really a valid cross-fitting
v0.15.1,as the test data are used to create the proj_t on the train
v0.15.1,which in the second train-test loop is used to create the nuisance
v0.15.1,cov on the test data. Hence the T variable of some sample
v0.15.1,"is implicitly correlated with its cov nuisance, through this flow"
v0.15.1,"of information. However, this seems a rather weak correlation."
v0.15.1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.15.1,model.
v0.15.1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.15.1,Estimate preliminary theta in cross fitting manner
v0.15.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.15.1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.15.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.15.1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.15.1,#############################################################################
v0.15.1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.15.1,A/B test
v0.15.1,#############################################################################
v0.15.1,Estimate preliminary theta in cross fitting manner
v0.15.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.15.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.15.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.15.1,We can use statsmodel for all hypothesis testing capabilities
v0.15.1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.15.1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.15.1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.15.1,model_T_XZ = lambda: model_clf()
v0.15.1,#'days_visited': lambda:
v0.15.1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.15.1,Turn strings into categories for numeric mapping
v0.15.1,### Defining some generic regressors and classifiers
v0.15.1,This a generic non-parametric regressor
v0.15.1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.15.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.15.1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.15.1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.15.1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.15.1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.15.1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.15.1,model = lambda: LinearRegression(n_jobs=-1)
v0.15.1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.15.1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.15.1,underlying model whenever predict is called.
v0.15.1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.15.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.15.1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.15.1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.15.1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.15.1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.15.1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.15.1,We need to specify models to be used for each of these residualizations
v0.15.1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.15.1,"E[T | X, Z]"
v0.15.1,E[TZ | X]
v0.15.1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.15.1,n_splits determines the number of splits to be used for cross-fitting.
v0.15.1,# Algorithm 2 - Current Method
v0.15.1,In[121]:
v0.15.1,# Algorithm 3 - DRIV ATE
v0.15.1,dmliv_model_effect = lambda: model()
v0.15.1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.15.1,"dmliv_model_effect(),"
v0.15.1,n_splits=1)
v0.15.1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.15.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.15.1,"Once multiple treatments are supported, we'll need to fix this"
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.15.1,We can use statsmodel for all hypothesis testing capabilities
v0.15.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.15.1,We can use statsmodel for all hypothesis testing capabilities
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,TODO. Deal with multi-class instrument/treatment
v0.15.1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.15.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.15.1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.15.1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.15.1,##################
v0.15.1,Global settings #
v0.15.1,##################
v0.15.1,Global plotting controls
v0.15.1,"Control for support size, can control for more"
v0.15.1,#################
v0.15.1,File utilities #
v0.15.1,#################
v0.15.1,#################
v0.15.1,Plotting utils #
v0.15.1,#################
v0.15.1,bias
v0.15.1,var
v0.15.1,rmse
v0.15.1,r2
v0.15.1,Infer feature dimension
v0.15.1,Metrics by support plots
v0.15.1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.15.1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.15.1,Steven Wu <zhiww@microsoft.com>
v0.15.1,Initialize causal tree parameters
v0.15.1,Create splits of causal tree
v0.15.1,Estimate treatment effects at the leafs
v0.15.1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.15.1,the corresponding split and associating the effect computed on that leaf
v0.15.1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.15.1,Safety check
v0.15.1,Weighted linear regression
v0.15.1,Calculates weights
v0.15.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.15.1,over all indices
v0.15.1,Similar for `a` weights
v0.15.1,Doesn't have sample weights
v0.15.1,Is a linear model
v0.15.1,Weighted linear regression
v0.15.1,Calculates weights
v0.15.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.15.1,over all indices
v0.15.1,Similar for `a` weights
v0.15.1,normalize weights
v0.15.1,"Split the data in half, train and test"
v0.15.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.15.1,"a function of W, using only the train fold"
v0.15.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.15.1,"Split the data in half, train and test"
v0.15.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.15.1,"a function of W, using only the train fold"
v0.15.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.15.1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.15.1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.15.1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.15.1,"Split the data in half, train and test"
v0.15.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.15.1,"a function of x, using only the train fold"
v0.15.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.15.1,Compute coefficient by OLS on residuals
v0.15.1,"Split the data in half, train and test"
v0.15.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.15.1,"a function of x, using only the train fold"
v0.15.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.15.1,Estimate multipliers for second order orthogonal method
v0.15.1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.15.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.15.1,Create local sample set
v0.15.1,compute the base estimate for the current node using double ml or second order double ml
v0.15.1,compute the influence functions here that are used for the criterion
v0.15.1,generate random proposals of dimensions to split
v0.15.1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.15.1,compute criterion for each proposal
v0.15.1,if splitting creates valid leafs in terms of mean leaf size
v0.15.1,Calculate criterion for split
v0.15.1,Else set criterion to infinity so that this split is not chosen
v0.15.1,If no good split was found
v0.15.1,Find split that minimizes criterion
v0.15.1,Set the split attributes at the node
v0.15.1,Create child nodes with corresponding subsamples
v0.15.1,Recursively split children
v0.15.1,Return parent node
v0.15.1,estimate the local parameter at the leaf using the estimate data
v0.15.1,###################
v0.15.1,Argument parsing #
v0.15.1,###################
v0.15.1,#########################################
v0.15.1,Parameters constant across experiments #
v0.15.1,#########################################
v0.15.1,Outcome support
v0.15.1,Treatment support
v0.15.1,Evaluation grid
v0.15.1,Treatment effects array
v0.15.1,Other variables
v0.15.1,##########################
v0.15.1,Data Generating Process #
v0.15.1,##########################
v0.15.1,Log iteration
v0.15.1,"Generate controls, features, treatment and outcome"
v0.15.1,T and Y residuals to be used in later scripts
v0.15.1,Save generated dataset
v0.15.1,#################
v0.15.1,ORF parameters #
v0.15.1,#################
v0.15.1,######################################
v0.15.1,Train and evaluate treatment effect #
v0.15.1,######################################
v0.15.1,########
v0.15.1,Plots #
v0.15.1,########
v0.15.1,###############
v0.15.1,Save results #
v0.15.1,###############
v0.15.1,##############
v0.15.1,Run Rscript #
v0.15.1,##############
v0.15.1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.15.1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.15.1,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.15.1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.15.1,def mlasso_model(): return MultiTaskLassoCV(
v0.15.1,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.15.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.1,heterogeneity
v0.15.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.1,heterogeneity
v0.15.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.1,heterogeneity
v0.15.1,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.15.1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.15.1,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.15.1,subset of features that are exogenous and create heterogeneity
v0.15.1,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.15.1,subset of features wrt we estimate heterogeneity
v0.15.1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.15.1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,introspect the constructor arguments to find the model parameters
v0.15.1,to represent
v0.15.1,"if the argument is deprecated, ignore it"
v0.15.1,Extract and sort argument names excluding 'self'
v0.15.1,column names
v0.15.1,transfer input to numpy arrays
v0.15.1,transfer input to 2d arrays
v0.15.1,create dataframe
v0.15.1,currently dowhy only support single outcome and single treatment
v0.15.1,call dowhy
v0.15.1,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.15.1,cate estimator but not the effect.
v0.15.1,don't proxy special methods
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Check if model is sparse enough for this model
v0.15.1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.15.1,TODO: any way to avoid creating a copy if the array was already dense?
v0.15.1,"the call is necessary if the input was something like a list, though"
v0.15.1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.15.1,so convert to pydata sparse first
v0.15.1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.15.1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.15.1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.15.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.15.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.15.1,For when checking input values is disabled
v0.15.1,Type to column extraction function
v0.15.1,if not all column names are strings
v0.15.1,coerce feature names to be strings
v0.15.1,Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
v0.15.1,"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
v0.15.1,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.15.1,Handles cases where the passed feature names create issues
v0.15.1,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.15.1,Get feature names using featurizer
v0.15.1,All attempts at retrieving transformed feature names have failed
v0.15.1,Delegate handling to downstream logic
v0.15.1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.15.1,same number of input definitions as arrays
v0.15.1,input definitions have same number of dimensions as each array
v0.15.1,all result indices are unique
v0.15.1,all result indices must match at least one input index
v0.15.1,"map indices to all array, axis pairs for that index"
v0.15.1,each index has the same cardinality wherever it appears
v0.15.1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.15.1,Algo: while list contains more than one entry
v0.15.1,take two entries
v0.15.1,sort both lists by intersection of their indices
v0.15.1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.15.1,"take the union of indices and the product of values), stepping through each list linearly"
v0.15.1,TODO: might be faster to break into connected components first
v0.15.1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.15.1,"so compute their content separately, then take cartesian product"
v0.15.1,this would save a few pointless sorts by empty tuples
v0.15.1,TODO: Consider investigating other performance ideas for these cases
v0.15.1,where the dense method beat the sparse method (usually sparse is faster)
v0.15.1,"e,facd,c->cfed"
v0.15.1,sparse: 0.0335489
v0.15.1,dense:  0.011465999999999997
v0.15.1,"gbd,da,egb->da"
v0.15.1,sparse: 0.0791625
v0.15.1,dense:  0.007319099999999995
v0.15.1,"dcc,d,faedb,c->abe"
v0.15.1,sparse: 1.2868097
v0.15.1,dense:  0.44605229999999985
v0.15.1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.15.1,TODO: would using einsum's paths to optimize the order of merging help?
v0.15.1,Normalize weights
v0.15.1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.15.1,"if we're decorating a class, just update the __init__ method,"
v0.15.1,so that the result is still a class instead of a wrapper method
v0.15.1,"want to enforce that each bad_arg was either in kwargs,"
v0.15.1,or else it was in neither and is just taking its default value
v0.15.1,Any access should throw
v0.15.1,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.15.1,return plain dictionary so that erroneous accesses don't half work (see e.g. #708)
v0.15.1,for every dimension of the treatment add some epsilon and observe change in featurized treatment
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.1,input feature name is already updated by cate_feature_names.
v0.15.1,define the index of d_x to filter for each given T
v0.15.1,filter X after broadcast with T for each given T
v0.15.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,
v0.15.1,This code contains some snippets of code from:
v0.15.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
v0.15.1,published under the following license and copyright:
v0.15.1,BSD 3-Clause License
v0.15.1,
v0.15.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.1,All rights reserved.
v0.15.1,make any access to matplotlib or plt throw an exception
v0.15.1,make any access to graphviz or plt throw an exception
v0.15.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.15.1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.15.1,Initialize saturation & value; calculate chroma & value shift
v0.15.1,Calculate some intermediate values
v0.15.1,Initialize RGB with same hue & chroma as our color
v0.15.1,Shift the initial RGB values to match value and store
v0.15.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.15.1,clean way of achieving this
v0.15.1,make sure we don't accidentally escape anything in the substitution
v0.15.1,Fetch appropriate color for node
v0.15.1,"red for negative, green for positive"
v0.15.1,in multi-target use mean of targets
v0.15.1,Write node mean CATE
v0.15.1,Write node std of CATE
v0.15.1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.15.1,Fetch appropriate color for node
v0.15.1,Write node mean CATE
v0.15.1,Write node mean CATE
v0.15.1,Write recommended treatment and value - cost
v0.15.1,Licensed under the MIT License.
v0.15.1,"since inference objects can be stateful, we must copy it before fitting;"
v0.15.1,otherwise this sequence wouldn't work:
v0.15.1,"est1.fit(..., inference=inf)"
v0.15.1,"est2.fit(..., inference=inf)"
v0.15.1,est1.effect_interval(...)
v0.15.1,because inf now stores state from fitting est2
v0.15.1,This flag is true when names are set in a child class instead
v0.15.1,"If names are set in a child class, add an attribute reflecting that"
v0.15.1,This works only if X is passed as a kwarg
v0.15.1,We plan to enforce X as kwarg only in future releases
v0.15.1,This checks if names have been set in a child class
v0.15.1,"If names were set in a child class, don't do it again"
v0.15.1,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.15.1,call the wrapped fit method
v0.15.1,NOTE: we call inference fit *after* calling the main fit method
v0.15.1,apply defaults before calling inference method
v0.15.1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.15.1,but tensordot can't be applied to this problem because we don't sum over m
v0.15.1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.15.1,of rows of T was not taken into account
v0.15.1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.15.1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.15.1,"Treatment names is None, default to BaseCateEstimator"
v0.15.1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.15.1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.15.1,Get input names
v0.15.1,Summary
v0.15.1,add statsmodels to parent's options
v0.15.1,add debiasedlasso to parent's options
v0.15.1,add blb to parent's options
v0.15.1,TODO Share some logic with non-discrete version
v0.15.1,Get input names
v0.15.1,Note: we do not transform feature names since that is done within summary_frame
v0.15.1,Summary
v0.15.1,add statsmodels to parent's options
v0.15.1,add statsmodels to parent's options
v0.15.1,add blb to parent's options
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,fully materialize folds so that they can be reused across models
v0.15.1,and precompute fitted indices so that we fail fast if there's an issue with them
v0.15.1,NOTE: if any model is missing scores we will just return None even if another model
v0.15.1,has scores. this is because we don't know how many scores are missing
v0.15.1,"for the models that are missing them, so we don't know how to pad the array"
v0.15.1,for convenience we allos a single model to be passed in lieu of a singleton list
v0.15.1,"in that case, we will also unwrap the model output"
v0.15.1,"when there is more than one model, nuisances from previous models"
v0.15.1,come first as positional arguments
v0.15.1,"scores entries should be lists of scores, so make each entry a singleton list"
v0.15.1,Adding the kwargs to ray object store to be used by remote functions
v0.15.1,for each fold to avoid IO overhead
v0.15.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.15.1,generate an instance of the final model
v0.15.1,generate an instance of the nuisance model
v0.15.1,Define Ray remote function (Ray remote wrapper of the _fit_nuisances function)
v0.15.1,Create Ray remote jobs for parallel processing
v0.15.1,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.15.1,alteration even when we only want to fit_final.
v0.15.1,use a binary array to get stratified split in case of discrete treatment
v0.15.1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.15.1,upgrade to a GroupKFold or StratiGroupKFold if groups is not None
v0.15.1,"we won't have generated a KFold or StratifiedKFold ourselves when groups are passed,"
v0.15.1,"but the user might have supplied one, which won't work"
v0.15.1,self._models_nuisance will be a list of lists or a list of list of lists
v0.15.1,so we use self._ortho_learner_model_nuisance to determine the nesting level
v0.15.1,for each mc iteration
v0.15.1,for each model under cross fit setting
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"TODO: This could be extended to also work with our sparse and 2SLS estimators,"
v0.15.1,if we add an aggregate method to them
v0.15.1,Remember to update the docs if this changes
v0.15.1,mix in the appropriate inference class
v0.15.1,assign all of the attributes from the dummy estimator that would normally be assigned during fitting
v0.15.1,TODO: This seems hacky; is there a better abstraction to maintain these?
v0.15.1,"This should also include bias_part_of_coef, model_final_, and fitted_models_final above"
v0.15.1,Assign treatment expansion attributes
v0.15.1,Methods needed to implement the LinearCateEstimator interface
v0.15.1,Methods needed to implement the LinearFinalModelCateEstimatorMixin
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,
v0.15.1,This code contains snippets of code from
v0.15.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.15.1,published under the following license and copyright:
v0.15.1,BSD 3-Clause License
v0.15.1,
v0.15.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.1,All rights reserved.
v0.15.1,=============================================================================
v0.15.1,Policy Forest
v0.15.1,=============================================================================
v0.15.1,Remap output
v0.15.1,reshape is necessary to preserve the data contiguity against vs
v0.15.1,"[:, np.newaxis] that does not."
v0.15.1,Get subsample sample size
v0.15.1,Check parameters
v0.15.1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.15.1,if this is the first `fit` call of the warm start mode.
v0.15.1,"Free allocated memory, if any"
v0.15.1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.15.1,We draw from the random state to get the random state we
v0.15.1,would have got if we hadn't used a warm_start.
v0.15.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.15.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.15.1,but would still advance randomness enough so that tree subsamples will be different.
v0.15.1,Parallel loop: we prefer the threading backend as the Cython code
v0.15.1,for fitting the trees is internally releasing the Python GIL
v0.15.1,making threading more efficient than multiprocessing in
v0.15.1,"that case. However, for joblib 0.12+ we respect any"
v0.15.1,"parallel_backend contexts set at a higher level,"
v0.15.1,since correctness does not rely on using threads.
v0.15.1,Collect newly grown trees
v0.15.1,Check data
v0.15.1,Assign chunk of trees to jobs
v0.15.1,avoid storing the output of every estimator by summing them here
v0.15.1,Parallel loop
v0.15.1,Check data
v0.15.1,Assign chunk of trees to jobs
v0.15.1,avoid storing the output of every estimator by summing them here
v0.15.1,Parallel loop
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,
v0.15.1,This code contains snippets of code from:
v0.15.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.15.1,published under the following license and copyright:
v0.15.1,BSD 3-Clause License
v0.15.1,
v0.15.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.1,All rights reserved.
v0.15.1,=============================================================================
v0.15.1,Types and constants
v0.15.1,=============================================================================
v0.15.1,=============================================================================
v0.15.1,Base Policy tree
v0.15.1,=============================================================================
v0.15.1,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.15.1,HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor
v0.15.1,This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"Unique treatments (ordered, includes control)"
v0.15.1,Number of treatments (excluding control)
v0.15.1,Indicator for whether
v0.15.1,Get DR outcomes in training sample
v0.15.1,Get DR outcomes in validation sample
v0.15.1,Get DR outcomes in validation sample
v0.15.1,Calculate ATE in the validation sample
v0.15.1,Fit propensity in treatment
v0.15.1,Predict propensity scores
v0.15.1,Possible treatments (need to allow more than 2)
v0.15.1,Predict outcomes
v0.15.1,T-learner logic
v0.15.1,"if CATE is given explicitly or has not been fitted at all previously, fit it now"
v0.15.1,Assign units in validation set to groups
v0.15.1,Proportion of validations set in group
v0.15.1,Group average treatment effect (GATE) -- average of DR outcomes in group
v0.15.1,Average of CATE predictions in group
v0.15.1,Calculate group calibration score
v0.15.1,Calculate overall calibration score
v0.15.1,Calculate R-square calibration score
v0.15.1,"if CATE is given explicitly or has not been fitted at all previously, fit it now"
v0.15.1,treat each treatment as a separate regression
v0.15.1,"here, prop_preds should be a matrix"
v0.15.1,with rows corresponding to units and columns corresponding to treatment statuses
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.15.1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.15.1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.15.1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.15.1,checks that X is 2D array.
v0.15.1,"since we only allow single dimensional y, we could flatten the prediction"
v0.15.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.1,Handles the corner case when X=None but featurizer might be not None
v0.15.1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.15.1,"Replacing this method which is invalid for this class, so that we make the"
v0.15.1,dosctring empty and not appear in the docs.
v0.15.1,TODO: support freq_weight and sample_var in debiased lasso
v0.15.1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.15.1,Replacing to remove docstring
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"if both X and W are None, just return a column of ones"
v0.15.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.15.1,We need to go back to the label representation of the one-hot so as to call
v0.15.1,the classifier.
v0.15.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.15.1,We need to go back to the label representation of the one-hot so as to call
v0.15.1,the classifier.
v0.15.1,data is already validated at initial fit time
v0.15.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.15.1,This works both with our without the weighting trick as the treatments T are unit vector
v0.15.1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.15.1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.15.1,both Parametric and Non Parametric DML.
v0.15.1,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.15.1,attribute so that the trained featurizer will be passed through
v0.15.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.15.1,for internal use by the library
v0.15.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.1,We need to use the rlearner's copy to retain the information from fitting
v0.15.1,Handles the corner case when X=None but featurizer might be not None
v0.15.1,override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
v0.15.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.15.1,TODO: support freq_weight and sample_var in debiased lasso
v0.15.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.15.1,since we clone it and fit separate copies
v0.15.1,add blb to parent's options
v0.15.1,override only so that we can update the docstring to indicate
v0.15.1,support for `GenericSingleTreatmentModelFinalInference`
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,note that groups are not passed to score because they are only used for fitting
v0.15.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.1,NOTE: important to get parent's wrapped copy so that
v0.15.1,"after training wrapped featurizer is also trained, etc."
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.15.1,Fit a doubly robust average effect
v0.15.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.15.1,(which needs to have been expanded if there's a discrete treatment)
v0.15.1,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.15.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.15.1,since we clone it and fit separate copies
v0.15.1,"If custom param grid, check that only estimator parameters are being altered"
v0.15.1,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.15.1,override only so that we can update the docstring to indicate support for `blb`
v0.15.1,Get input names
v0.15.1,Summary
v0.15.1,Determine output settings
v0.15.1,"Important: This must be the first invocation of the random state at fit time, so that"
v0.15.1,train/test splits are re-generatable from an external object simply by knowing the
v0.15.1,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.15.1,linear predictions. Currently is also useful for testing.
v0.15.1,reshape is necessary to preserve the data contiguity against vs
v0.15.1,"[:, np.newaxis] that does not."
v0.15.1,Check parameters
v0.15.1,Set min_weight_leaf from min_weight_fraction_leaf
v0.15.1,Build tree
v0.15.1,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.15.1,hold. Used by criterion for memory space savings.
v0.15.1,Initialize the criterion object and the criterion_val object if honest.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,
v0.15.1,This code is a fork from:
v0.15.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
v0.15.1,published under the following license and copyright:
v0.15.1,BSD 3-Clause License
v0.15.1,
v0.15.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.1,All rights reserved.
v0.15.1,Set parameters
v0.15.1,Don't instantiate estimators now! Parameters of base_estimator might
v0.15.1,"still change. Eg., when grid-searching with the nested object syntax."
v0.15.1,self.estimators_ needs to be filled by the derived classes in fit.
v0.15.1,Compute the number of jobs
v0.15.1,Partition estimators between jobs
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,covariance matrix
v0.15.1,get eigen value and eigen vectors
v0.15.1,simulate eigen vectors
v0.15.1,keep the top 4 eigen value and corresponding eigen vector
v0.15.1,replace the negative eigen values
v0.15.1,generate a new covariance matrix
v0.15.1,get linear approximation of eigen values
v0.15.1,coefs
v0.15.1,get the indices of each group of features
v0.15.1,print(ind_same_proxy)
v0.15.1,demo
v0.15.1,same proxy
v0.15.1,residuals
v0.15.1,gmm
v0.15.1,log normal on outliers
v0.15.1,positive outliers
v0.15.1,negative outliers
v0.15.1,demean the new residual again
v0.15.1,generate data
v0.15.1,sample residuals
v0.15.1,get prediction for current investment
v0.15.1,get prediction for current proxy
v0.15.1,get first period prediction
v0.15.1,iterate the step ahead contruction
v0.15.1,prepare new x
v0.15.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.1,heterogeneity
v0.15.1,get new covariance matrix
v0.15.1,get coefs
v0.15.1,get residuals
v0.15.1,proxy 1 is the outcome
v0.15.1,make fixed residuals
v0.15.1,Remove children with nonwhite mothers from the treatment group
v0.15.1,Remove children with nonwhite mothers from the treatment group
v0.15.1,Select columns
v0.15.1,Scale the numeric variables
v0.15.1,"Change the binary variable 'first' takes values in {1,2}"
v0.15.1,Append a column of ones as intercept
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.15.1,(which needs to have been expanded if there's a discrete treatment)
v0.15.1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.15.1,d_t=None here since we measure the effect across all Ts
v0.15.1,"y is a vector, rather than a 2D array"
v0.15.1,once the estimator has been fit
v0.15.1,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.15.1,an intercept even when bias is part of coef.
v0.15.1,We can write effect inference as a function of prediction and prediction standard error of
v0.15.1,the final method for linear models
v0.15.1,squeeze the first axis
v0.15.1,d_t=None here since we measure the effect across all Ts
v0.15.1,set the mean_pred_stderr
v0.15.1,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.15.1,squeeze the first axis
v0.15.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.15.1,(which needs to have been expanded if there's a discrete treatment)
v0.15.1,"send treatment to the end, pull bounds to the front"
v0.15.1,d_t=None here since we measure the effect across all Ts
v0.15.1,set the mean_pred_stderr
v0.15.1,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.15.1,d_t=None here since we measure the effect across all Ts
v0.15.1,d_t=None here since we measure the effect across all Ts
v0.15.1,need to set the fit args before the estimator is fit
v0.15.1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.15.1,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.15.1,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.15.1,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.15.1,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.15.1,scale preds
v0.15.1,scale std errs
v0.15.1,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.15.1,offset preds
v0.15.1,"offset the distribution, too"
v0.15.1,scale preds
v0.15.1,"scale the distribution, too"
v0.15.1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.15.1,1. Uncertainty of Mean Point Estimate
v0.15.1,2. Distribution of Point Estimate
v0.15.1,3. Total Variance of Point Estimate
v0.15.1,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.15.1,so bail out early; note that it might be possible to correct the algorithm for
v0.15.1,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.15.1,be clean
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,TODO: Add a __dir__ implementation?
v0.15.1,don't proxy special methods
v0.15.1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.15.1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.15.1,then we should be computing a confidence interval for the wrapped calls
v0.15.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.15.1,second level bootstrap which would be prohibitive computationally?
v0.15.1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.15.1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.15.1,can't import from econml.inference at top level without creating cyclical dependencies
v0.15.1,Note that inference results are always methods even if the inference is for a property
v0.15.1,(e.g. coef__inference() is a method but coef_ is a property)
v0.15.1,Therefore we must insert a lambda if getting inference for a non-callable
v0.15.1,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.15.1,"try to get interval/std first if appropriate,"
v0.15.1,since we don't prefer a wrapped method with this name
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,
v0.15.1,This code contains snippets of code from:
v0.15.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.15.1,published under the following license and copyright:
v0.15.1,BSD 3-Clause License
v0.15.1,
v0.15.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.1,All rights reserved.
v0.15.1,=============================================================================
v0.15.1,Types and constants
v0.15.1,=============================================================================
v0.15.1,=============================================================================
v0.15.1,Base GRF tree
v0.15.1,=============================================================================
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,=============================================================================
v0.15.1,A MultOutputWrapper for GRF classes
v0.15.1,=============================================================================
v0.15.1,=============================================================================
v0.15.1,Instantiations of Generalized Random Forest
v0.15.1,=============================================================================
v0.15.1,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.15.1,in front of the constant treatment is the intercept in the moment equation.
v0.15.1,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.15.1,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,
v0.15.1,This code contains snippets of code from
v0.15.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.15.1,published under the following license and copyright:
v0.15.1,BSD 3-Clause License
v0.15.1,
v0.15.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.1,All rights reserved.
v0.15.1,=============================================================================
v0.15.1,Base Generalized Random Forest
v0.15.1,=============================================================================
v0.15.1,TODO: support freq_weight and sample_var
v0.15.1,Remap output
v0.15.1,reshape is necessary to preserve the data contiguity against vs
v0.15.1,"[:, np.newaxis] that does not."
v0.15.1,reshape is necessary to preserve the data contiguity against vs
v0.15.1,"[:, np.newaxis] that does not."
v0.15.1,Get subsample sample size
v0.15.1,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.15.1,We calculate the min eigenvalue proxy that each criterion is considering
v0.15.1,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.15.1,Check parameters
v0.15.1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.15.1,if this is the first `fit` call of the warm start mode.
v0.15.1,"Free allocated memory, if any"
v0.15.1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.15.1,We draw from the random state to get the random state we
v0.15.1,would have got if we hadn't used a warm_start.
v0.15.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.15.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.15.1,but would still advance randomness enough so that tree subsamples will be different.
v0.15.1,Generating indices a priori before parallelism ended up being orders of magnitude
v0.15.1,faster than how sklearn does it. The reason is that random samplers do not release the
v0.15.1,gil it seems.
v0.15.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.15.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.15.1,but would still advance randomness enough so that tree subsamples will be different.
v0.15.1,Parallel loop: we prefer the threading backend as the Cython code
v0.15.1,for fitting the trees is internally releasing the Python GIL
v0.15.1,making threading more efficient than multiprocessing in
v0.15.1,"that case. However, for joblib 0.12+ we respect any"
v0.15.1,"parallel_backend contexts set at a higher level,"
v0.15.1,since correctness does not rely on using threads.
v0.15.1,Collect newly grown trees
v0.15.1,Check data
v0.15.1,Assign chunk of trees to jobs
v0.15.1,avoid storing the output of every estimator by summing them here
v0.15.1,Parallel loop
v0.15.1,Check data
v0.15.1,Assign chunk of trees to jobs
v0.15.1,Parallel loop
v0.15.1,Check data
v0.15.1,Assign chunk of trees to jobs
v0.15.1,Parallel loop
v0.15.1,####################
v0.15.1,Variance correction
v0.15.1,####################
v0.15.1,Subtract the average within bag variance. This ends up being equal to the
v0.15.1,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.15.1,The negative part is just sq_between.
v0.15.1,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.15.1,"The off diagonals we have no objective prior, so no correction is applied."
v0.15.1,Finally correcting the pred_cov or pred_var
v0.15.1,avoid storing the output of every estimator by summing them here
v0.15.1,Parallel loop
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,we have to filter the folds because they contain the indices in the original data not
v0.15.1,the indices in the period-filtered data
v0.15.1,translate the indices in a fold to the indices in the period-filtered data
v0.15.1,"if groups was [3,3,4,4,5,5,6,6,1,1,2,2,0,0] (the group ids can be in any order, but the"
v0.15.1,"time periods for each group should be contguous), and we had [10,11,0,1] as the indices in a fold"
v0.15.1,(so the fold is taking the entries corresponding to groups 2 and 3)
v0.15.1,"then group_period_filter(0) is [0,2,4,6,8,10,12] and gpf(1) is [1,3,5,7,9,11,13]"
v0.15.1,"so for period 1, the fold should be [10,0] => [5,0] (the indices that return 10 and 0 in the t=0 data)"
v0.15.1,"and for period 2, the fold should be [11,1] => [5,0] again (the indices that return 11,1 in the t=1 data)"
v0.15.1,filter to the indices for the time period
v0.15.1,"now find their index in the period-filtered data, which is always sorted"
v0.15.1,sanity check that the folds are the same no matter the time period
v0.15.1,TODO: update docs
v0.15.1,"NOTE: sample weight, sample var are not passed in"
v0.15.1,Compose final model
v0.15.1,Calculate auxiliary quantities
v0.15.1,X ⨂ T_res
v0.15.1,"sum(model_final.predict(X, T_res))"
v0.15.1,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.15.1,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
v0.15.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.1,"we need to set the number of periods before calling super()._prefit, since that will generate the"
v0.15.1,"final and nuisance models, which need to have self._n_periods set"
v0.15.1,Set _d_t to effective number of treatments
v0.15.1,Required for bootstrap inference
v0.15.1,for each mc iteration
v0.15.1,for each model under cross fit setting
v0.15.1,Handles the corner case when X=None but featurizer might be not None
v0.15.1,Expand treatments for each time period
v0.15.1,NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
v0.15.1,attribute so that the trained featurizer will be passed through
v0.15.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.15.1,for internal use by the library
v0.15.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.1,We need to use the _ortho_learner's copy to retain the information from fitting
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,testing importances
v0.15.1,testing heterogeneity importances
v0.15.1,Testing that all parameters do what they are supposed to
v0.15.1,"testing predict, apply and decision path"
v0.15.1,test that the subsampling scheme past to the trees is correct
v0.15.1,The sample size is chosen in particular to test rounding based error when subsampling
v0.15.1,test that the estimator calcualtes var correctly
v0.15.1,test api
v0.15.1,test accuracy
v0.15.1,test the projection functionality of forests
v0.15.1,test that the estimator calcualtes var correctly
v0.15.1,test api
v0.15.1,test that the estimator calcualtes var correctly
v0.15.1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.15.1,test that we raise errors in mishandled situations.
v0.15.1,test that the subsampling scheme past to the trees is correct
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
v0.15.1,omit the lalonde notebook
v0.15.1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.15.1,creating notebooks that are annoying for our users to actually run themselves
v0.15.1,create directory if necessary
v0.15.1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.15.1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.15.1,"prior to calling interpret, can't plot, render, etc."
v0.15.1,can interpret without uncertainty
v0.15.1,can't interpret with uncertainty if inference wasn't used during fit
v0.15.1,can interpret with uncertainty if we refit
v0.15.1,can interpret without uncertainty
v0.15.1,can't treat before interpreting
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,for is_discrete in [False]:
v0.15.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.1,ensure we can serialize the unfit estimator
v0.15.1,ensure we can pickle the fit estimator
v0.15.1,make sure we can call the marginal_effect and effect methods
v0.15.1,test const marginal inference
v0.15.1,test effect inference
v0.15.1,test marginal effect inference
v0.15.1,test coef__inference and intercept__inference
v0.15.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.1,"make sure we can call effect with implied scalar treatments,"
v0.15.1,"no matter the dimensions of T, and also that we warn when there"
v0.15.1,are multiple treatments
v0.15.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.1,No heterogeneity
v0.15.1,Define indices to test
v0.15.1,Heterogeneous effects
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,simple DGP only for illustration
v0.15.1,Define the treatment model neural network architecture
v0.15.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.15.1,"so the input shape is (d_z + d_x,)"
v0.15.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.15.1,add extra layers on top for the mixture density network
v0.15.1,Define the response model neural network architecture
v0.15.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.15.1,"so the input shape is (d_t + d_x,)"
v0.15.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.15.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.15.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.15.1,so that the same weights will be reused in each instantiation
v0.15.1,number of samples to use in second estimate of the response
v0.15.1,(to make loss estimate unbiased)
v0.15.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.15.1,do something with predictions...
v0.15.1,also test vector t and y
v0.15.1,simple DGP only for illustration
v0.15.1,Define the treatment model neural network architecture
v0.15.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.15.1,"so the input shape is (d_z + d_x,)"
v0.15.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.15.1,add extra layers on top for the mixture density network
v0.15.1,Define the response model neural network architecture
v0.15.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.15.1,"so the input shape is (d_t + d_x,)"
v0.15.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.15.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.15.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.15.1,so that the same weights will be reused in each instantiation
v0.15.1,number of samples to use in second estimate of the response
v0.15.1,(to make loss estimate unbiased)
v0.15.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.15.1,do something with predictions...
v0.15.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.15.1,test = True ensures we draw test set images
v0.15.1,test = True ensures we draw test set images
v0.15.1,re-draw to get new independent treatment and implied response
v0.15.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.15.1,above is necesary so that reduced form doesn't win
v0.15.1,covariates: time and emotion
v0.15.1,random instrument
v0.15.1,z -> price
v0.15.1,true observable demand function
v0.15.1,errors
v0.15.1,response
v0.15.1,test = True ensures we draw test set images
v0.15.1,test = True ensures we draw test set images
v0.15.1,re-draw to get new independent treatment and implied response
v0.15.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.15.1,above is necesary so that reduced form doesn't win
v0.15.1,covariates: time and emotion
v0.15.1,random instrument
v0.15.1,z -> price
v0.15.1,true observable demand function
v0.15.1,errors
v0.15.1,response
v0.15.1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.15.1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.15.1,For some reason this doesn't work at all when run against the CNTK backend...
v0.15.1,"model.compile('nadam', loss=lambda _,l:l)"
v0.15.1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.15.1,generate a valiation set
v0.15.1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.15.1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,DGP constants
v0.15.1,Generate data
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.15.1,pass sample weight to final step of pipeline
v0.15.1,create data with missing values
v0.15.1,model that can handle missing values
v0.15.1,"test X, W only"
v0.15.1,test W only
v0.15.1,dowhy does not support missing values in X
v0.15.1,assert that fitting with missing values fails when allow_missing is False
v0.15.1,and that setting allow_missing after init still works
v0.15.1,assert that we fail with a value error when we pass missing X to a model that doesn't support it
v0.15.1,assert that fitting with missing values fails when allow_missing is False
v0.15.1,and that setting allow_missing after init still works
v0.15.1,metalearners don't support W
v0.15.1,metalearners do support missing values in X
v0.15.1,dowhy never supports missing values in X
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,identity featurization effect functions
v0.15.1,polynomial featurization effect functions
v0.15.1,1d polynomial featurization functions
v0.15.1,2d-to-1d featurization functions
v0.15.1,2d-to-1d vector featurization functions
v0.15.1,use LassoCV rather than also selecting over RandomForests to save time
v0.15.1,test that treatment names are assigned for the featurized treatment
v0.15.1,expected shapes
v0.15.1,check effects
v0.15.1,ate
v0.15.1,loose inference checks
v0.15.1,temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons
v0.15.1,effect inference
v0.15.1,marginal effect inference
v0.15.1,const marginal effect inference
v0.15.1,fit a dummy estimator first so the featurizer can be fit to the treatment
v0.15.1,edge case with transformer that only takes a vector treatment
v0.15.1,so far will always return None for cate_treatment_names
v0.15.1,assert proper handling of improper feature names passed to certain transformers
v0.15.1,"depending on sklearn version, bad feature names either throws error or only uses first relevant name"
v0.15.1,ensure alpha is passed
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,testing importances
v0.15.1,testing heterogeneity importances
v0.15.1,Testing that all parameters do what they are supposed to
v0.15.1,"testing predict, apply and decision path"
v0.15.1,initialize parameters
v0.15.1,initialize config wtih base config and overwite some values
v0.15.1,predict tree using config parameters and assert
v0.15.1,shape of trained tree is the same as y_test
v0.15.1,initialize config wtih base honest config and overwite some values
v0.15.1,predict tree using config parameters and assert
v0.15.1,shape of trained tree is the same as y_test
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.15.1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.15.1,so we need to transpose the result
v0.15.1,1-d output
v0.15.1,2-d output
v0.15.1,Single dimensional output y
v0.15.1,compare with weight
v0.15.1,compare with weight
v0.15.1,compare with weight
v0.15.1,compare with weight
v0.15.1,Multi-dimensional output y
v0.15.1,1-d y
v0.15.1,compare when both sample_var and sample_weight exist
v0.15.1,multi-d y
v0.15.1,compare when both sample_var and sample_weight exist
v0.15.1,compare when both sample_var and sample_weight exist
v0.15.1,compare when both sample_var and sample_weight exist
v0.15.1,compare when both sample_var and sample_weight exist
v0.15.1,compare when both sample_var and sample_weight exist
v0.15.1,compare when both sample_var and sample_weight exist
v0.15.1,dgp
v0.15.1,StatsModels2SLS
v0.15.1,IV2SLS
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,test that we can fit with the same arguments as the base estimator
v0.15.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.15.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.1,with the same shape for the lower and upper bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,test that we can do the same thing once we provide percentile bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.15.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.1,with the same shape for the lower and upper bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,test that we can do the same thing once we provide percentile bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,test that we can fit with the same arguments as the base estimator
v0.15.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.15.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.1,with the same shape for the lower and upper bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,test that we can do the same thing once we provide percentile bounds
v0.15.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.15.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.1,with the same shape for the lower and upper bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,test that we can do the same thing once we provide percentile bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,test that we can fit with the same arguments as the base estimator
v0.15.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.15.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.1,with the same shape for the lower and upper bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,test that we can do the same thing once we provide percentile bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.15.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.1,with the same shape for the lower and upper bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,test that we can do the same thing once we provide percentile bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.1,with the same shape for the lower and upper bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,test that the estimated effect is usually within the bounds
v0.15.1,test that we can do the same thing once we provide alpha explicitly
v0.15.1,test that the lower and upper bounds differ
v0.15.1,test that the estimated effect is usually within the bounds
v0.15.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.1,with the same shape for the lower and upper bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,TODO: test that the estimated effect is usually within the bounds
v0.15.1,and that the true effect is also usually within the bounds
v0.15.1,test that we can do the same thing once we provide percentile bounds
v0.15.1,test that the lower and upper bounds differ
v0.15.1,TODO: test that the estimated effect is usually within the bounds
v0.15.1,and that the true effect is also usually within the bounds
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,fixed functions as first stage models
v0.15.1,they can be anything as long as fitting doesn't modify the predictions
v0.15.1,"that way, it doesn't matter if they are trained on different subsets of the data"
v0.15.1,all estimators must have opted in to federation
v0.15.1,all estimators must have the same covariance type
v0.15.1,test coefficients
v0.15.1,test effects
v0.15.1,fixed functions as first stage models
v0.15.1,they can be anything as long as fitting doesn't modify the predictions
v0.15.1,"that way, it doesn't matter if they are trained on different subsets of the data"
v0.15.1,test coefficients
v0.15.1,test effects
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,test that the subsampling scheme past to the trees is correct
v0.15.1,test that the estimator calcualtes var correctly
v0.15.1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.15.1,test that we raise errors in mishandled situations.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,DGP constants
v0.15.1,Generate data
v0.15.1,Test inference results when `cate_feature_names` doesn not exist
v0.15.1,Test inference results when `cate_feature_names` doesn not exist
v0.15.1,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.15.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.15.1,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.15.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.15.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.15.1,pvalue for second column should be greater than zero since some points are on either side
v0.15.1,of the tested value
v0.15.1,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.15.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.15.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.15.1,ensure alpha is passed
v0.15.1,only is not None when T1 is a constant or a list of constant
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Generate synthetic data
v0.15.1,Run _crossfit with Ray enabled
v0.15.1,Run _crossfit without Ray
v0.15.1,Compare the results
v0.15.1,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.15.1,Test non keyword based calls to fit
v0.15.1,test non-array inputs
v0.15.1,Test custom splitter
v0.15.1,Test incomplete set of test folds
v0.15.1,"y scores should be positive, since W predicts Y somewhat"
v0.15.1,"t scores might not be, since W and T are uncorrelated"
v0.15.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,make sure cross product varies more slowly with first array
v0.15.1,and that vectors are okay as inputs
v0.15.1,number of inputs in specification must match number of inputs
v0.15.1,must have an output
v0.15.1,output indices must be unique
v0.15.1,output indices must be present in an input
v0.15.1,number of indices must match number of dimensions for each input
v0.15.1,repeated indices must always have consistent sizes
v0.15.1,transpose
v0.15.1,tensordot
v0.15.1,trace
v0.15.1,TODO: set up proper flag for this
v0.15.1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.15.1,"of all of the distinct indices that appear in any input,"
v0.15.1,pick a random subset of them (of size at most 5) to appear in the output
v0.15.1,creating an instance should warn
v0.15.1,using the instance should not warn
v0.15.1,using the deprecated method should warn
v0.15.1,don't warn if b and c are passed by keyword
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,make any access to matplotlib or plt throw an exception
v0.15.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.1,heterogeneity
v0.15.1,Invert indices to match latest API
v0.15.1,Invert indices to match latest API
v0.15.1,The feature for heterogeneity stays constant
v0.15.1,Auxiliary function for adding xticks and vertical lines when plotting results
v0.15.1,for dynamic dml vs ground truth parameters.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,tests that we can recover the right degree of polynomial features
v0.15.1,implicitly also tests ability to handle pipelines
v0.15.1,since 'poly' uses pipelines containing PolynomialFeatures
v0.15.1,"generate larger coefficients in a set of high degree features,"
v0.15.1,weighted towards higher degree features
v0.15.1,"just test a polynomial T model, since for Y the correct degree also depends on"
v0.15.1,the interation of T and X
v0.15.1,test corner case with just one model in a list
v0.15.1,test corner case with empty list
v0.15.1,test selecting between two fixed models
v0.15.1,"DGP is a linear model, so linear regression should fit better"
v0.15.1,"DGP is now non-linear, so random forest should fit better"
v0.15.1,these models only work on multi-output data
v0.15.1,SeparatedModel doesn't support scoring; that should be fine when not compared to other models
v0.15.1,"on the other hand, when we need to compare the score to other models, it should raise an error"
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Preprocess data
v0.15.1,Convert 'week' to a date
v0.15.1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.15.1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.15.1,Take log of price
v0.15.1,Make brand numeric
v0.15.1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.15.1,which have all zero coeffs
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,test at least one estimator from each category
v0.15.1,test causal graph
v0.15.1,need to set matplotlib backend before viewing model
v0.15.1,test refutation estimate
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.15.1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.15.1,TODO: test something rather than just print...
v0.15.1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.15.1,pick some arbitrary X
v0.15.1,pick some arbitrary T
v0.15.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.15.1,Generate random Xs
v0.15.1,Random covariance matrix of Xs
v0.15.1,Effect of Xs on outcome
v0.15.1,Effect of treatment on outcomes
v0.15.1,Effect of treatment on outcome conditional on X1
v0.15.1,Generate treatments based on X and random noise
v0.15.1,"Generate Y (based on X, D, and random noise)"
v0.15.1,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.1,test the DR outcome difference
v0.15.1,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.1,test the DR outcome difference
v0.15.1,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.1,test the DR outcome difference
v0.15.1,use evaluate_blp to fit on validation only
v0.15.1,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.1,test the DR outcome difference
v0.15.1,fit nothing
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,accuracy test
v0.15.1,"accuracy test, DML"
v0.15.1,uncomment when issue #837 is resolved
v0.15.1,"NonParamDMLIV(discrete_outcome=discrete_outcome, discrete_treatment=discrete_treatment,"
v0.15.1,"discrete_instrument=discrete_instrument, model_final=LinearRegression())"
v0.15.1,make sure the auto outcome model is a classifier
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.15.1,The average variance should be lower when using monte carlo iterations
v0.15.1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.15.1,The average variance should be lower when using monte carlo iterations
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"since we're running so many combinations, just use LassoCV/LogisticRegressionCV"
v0.15.1,for the models instead of also selecting over random forest models
v0.15.1,ensure we can serialize unfit estimator
v0.15.1,ensure we can serialize fit estimator
v0.15.1,expected effect size
v0.15.1,test effect
v0.15.1,test inference
v0.15.1,only OrthoIV support inference other than bootstrap
v0.15.1,test summary
v0.15.1,test can run score
v0.15.1,test cate_feature_names
v0.15.1,test can run shap values
v0.15.1,dgp
v0.15.1,no heterogeneity
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"if we aren't fitting on the whole dataset, ensure that the limits are respected"
v0.15.1,ensure that the grouping has worked correctly and we get exactly the number of copies
v0.15.1,of the items in whichever groups we see
v0.15.1,DML nested CV works via a 'cv' attribute
v0.15.1,"want to validate the nested grouping, not the outer grouping in the nesting tests"
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,parameter combinations to test
v0.15.1,"we're running a lot of tests, so use fixed models instead of model selection"
v0.15.1,"IntentToTreatDRIV only supports binary treatments and instruments, and doesn't support fit_cov_directly"
v0.15.1,TODO: serializing/deserializing for every combination -- is this necessary?
v0.15.1,ensure we can serialize unfit estimator
v0.15.1,ensure we can serialize fit estimator
v0.15.1,expected effect size
v0.15.1,assert calculated constant marginal effect shape is expected
v0.15.1,const_marginal effect is defined in LinearCateEstimator class
v0.15.1,assert calculated marginal effect shape is expected
v0.15.1,test inference
v0.15.1,test can run score
v0.15.1,test cate_feature_names
v0.15.1,test can run shap values
v0.15.1,"dgp (binary T, binary Z)"
v0.15.1,no heterogeneity
v0.15.1,with heterogeneity
v0.15.1,fitting the covariance directly should be at least as good as computing the covariance from separate models
v0.15.1,set the models so that model selection over random forests doesn't take too much time in the repeated trials
v0.15.1,directly fitting the covariance should be better than indirectly fitting it
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Set random seed
v0.15.1,Generate data
v0.15.1,DGP constants
v0.15.1,Test data
v0.15.1,Constant treatment effect
v0.15.1,Constant treatment with multi output Y
v0.15.1,Heterogeneous treatment
v0.15.1,Heterogeneous treatment with multi output Y
v0.15.1,TLearner test
v0.15.1,Instantiate TLearner
v0.15.1,Test inputs
v0.15.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.1,Instantiate SLearner
v0.15.1,Test inputs
v0.15.1,Test constant treatment effect
v0.15.1,Test constant treatment effect with multi output Y
v0.15.1,Test heterogeneous treatment effect
v0.15.1,Need interactions between T and features
v0.15.1,Test heterogeneous treatment effect with multi output Y
v0.15.1,Instantiate XLearner
v0.15.1,Test inputs
v0.15.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.1,Instantiate DomainAdaptationLearner
v0.15.1,Test inputs
v0.15.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.1,Get the true treatment effect
v0.15.1,Get the true treatment effect
v0.15.1,Fit learner and get the effect and marginal effect
v0.15.1,Compute treatment effect residuals (absolute)
v0.15.1,Check that at least 90% of predictions are within tolerance interval
v0.15.1,Check whether the output shape is right
v0.15.1,Check that one can pass in regular lists
v0.15.1,Check that it fails correctly if lists of different shape are passed in
v0.15.1,"Check that it works when T, Y have shape (n, 1)"
v0.15.1,Generate covariates
v0.15.1,Generate treatment
v0.15.1,Calculate outcome
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,DGP constants
v0.15.1,Generate data
v0.15.1,Test data
v0.15.1,Remove warnings that might be raised by the models passed into the ORF
v0.15.1,Generate data with continuous treatments
v0.15.1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.15.1,does not work well with parallelism.
v0.15.1,Test inputs for continuous treatments
v0.15.1,--> Check that one can pass in regular lists
v0.15.1,--> Check that it fails correctly if lists of different shape are passed in
v0.15.1,Check that outputs have the correct shape
v0.15.1,Test continuous treatments with controls
v0.15.1,Test continuous treatments without controls
v0.15.1,Generate data with binary treatments
v0.15.1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.15.1,does not work well with parallelism.
v0.15.1,Test inputs for binary treatments
v0.15.1,--> Check that one can pass in regular lists
v0.15.1,--> Check that it fails correctly if lists of different shape are passed in
v0.15.1,"--> Check that it works when T, Y have shape (n, 1)"
v0.15.1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.15.1,--> Check that it fails correctly when the treatments are not numeric
v0.15.1,Check that outputs have the correct shape
v0.15.1,Test binary treatments with controls
v0.15.1,Test binary treatments without controls
v0.15.1,Only applicable to continuous treatments
v0.15.1,Generate data for 2 treatments
v0.15.1,Test multiple treatments with controls
v0.15.1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.15.1,The rest for controls. Just as an example.
v0.15.1,Generating A/B test data
v0.15.1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.15.1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.15.1,Create a wrapper around Lasso that doesn't support weights
v0.15.1,since Lasso does natively support them starting in sklearn 0.23
v0.15.1,Generate data with continuous treatments
v0.15.1,Instantiate model with most of the default parameters
v0.15.1,Compute the treatment effect on test points
v0.15.1,Compute treatment effect residuals
v0.15.1,Multiple treatments
v0.15.1,Allow at most 10% test points to be outside of the tolerance interval
v0.15.1,Compute treatment effect residuals
v0.15.1,Multiple treatments
v0.15.1,Allow at most 20% test points to be outside of the confidence interval
v0.15.1,Check that the intervals are not too wide
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.15.1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.15.1,ensure that we've got at least 6 of every element
v0.15.1,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.15.1,NOTE: this number may need to change if the default number of folds in
v0.15.1,WeightedStratifiedKFold changes
v0.15.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.1,ensure we can serialize the unfit estimator
v0.15.1,ensure we can pickle the fit estimator
v0.15.1,make sure we can call the marginal_effect and effect methods
v0.15.1,test const marginal inference
v0.15.1,test effect inference
v0.15.1,test marginal effect inference
v0.15.1,test coef__inference and intercept__inference
v0.15.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.1,"make sure we can call effect with implied scalar treatments,"
v0.15.1,"no matter the dimensions of T, and also that we warn when there"
v0.15.1,are multiple treatments
v0.15.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.1,ensure that we've got at least two of every element
v0.15.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.1,make sure we can call the marginal_effect and effect methods
v0.15.1,test const marginal inference
v0.15.1,test effect inference
v0.15.1,test marginal effect inference
v0.15.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.15.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.15.1,We concatenate the two copies data
v0.15.1,make sure we can get out post-fit stuff
v0.15.1,create a simple artificial setup where effect of moving from treatment
v0.15.1,"1 -> 2 is 2,"
v0.15.1,"1 -> 3 is 1, and"
v0.15.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.15.1,"Using an uneven number of examples from different classes,"
v0.15.1,"and having the treatments in non-lexicographic order,"
v0.15.1,Should rule out some basic issues.
v0.15.1,test that we can fit with a KFold instance
v0.15.1,test that we can fit with a train/test iterable
v0.15.1,predetermined splits ensure that all features are seen in each split
v0.15.1,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.15.1,(incorrectly) use a final model with an intercept
v0.15.1,"Because final model is fixed, actual values of T and Y don't matter"
v0.15.1,Ensure reproducibility
v0.15.1,Sparse DGP
v0.15.1,Treatment effect coef
v0.15.1,Other coefs
v0.15.1,Features and controls
v0.15.1,Test sparse estimator
v0.15.1,"--> test coef_, intercept_"
v0.15.1,"with this DGP, since T depends linearly on X, Y depends on X quadratically"
v0.15.1,so we should use a quadratic featurizer
v0.15.1,--> test treatment effects
v0.15.1,Restrict x_test to vectors of norm < 1
v0.15.1,--> check inference
v0.15.1,Check that a majority of true effects lie in the 5-95% CI
v0.15.1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.15.1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.15.1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.15.1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.15.1,sparse test case: heterogeneous effect by product
v0.15.1,need at least as many rows in e_y as there are distinct columns
v0.15.1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.15.1,"we have quadratic terms in y, so we need to pipeline with a quadratic featurizer"
v0.15.1,Compare results with and without Ray
v0.15.1,create a simple artificial setup where effect of moving from treatment
v0.15.1,"a -> b is 2,"
v0.15.1,"a -> c is 1, and"
v0.15.1,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.15.1,"Using an uneven number of examples from different classes,"
v0.15.1,"and having the treatments in non-lexicographic order,"
v0.15.1,should rule out some basic issues.
v0.15.1,Note that explicitly specifying the dtype as object is necessary until
v0.15.1,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.15.1,estimated effects should be identical when treatment is explicitly given
v0.15.1,but const_marginal_effect should be reordered based on the explicit cagetories
v0.15.1,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.15.1,test outer grouping
v0.15.1,"with 2 folds, we should get exactly 3 groups per split, each with 10 copies of the y or t value"
v0.15.1,test nested grouping
v0.15.1,"with 2-fold outer and 2-fold inner grouping, and six total groups,"
v0.15.1,should get 1 or 2 groups per split
v0.15.1,"Try default, integer, and new user-passed treatment name"
v0.15.1,FunctionTransformers are agnostic to passed treatment names
v0.15.1,Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Set random seed
v0.15.1,Generate data
v0.15.1,DGP constants
v0.15.1,Test data
v0.15.1,Constant treatment effect and propensity
v0.15.1,Heterogeneous treatment and propensity
v0.15.1,ensure that we've got at least two of every element
v0.15.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.1,ensure that we can serialize unfit estimator
v0.15.1,ensure that we can serialize fit estimator
v0.15.1,make sure we can call the marginal_effect and effect methods
v0.15.1,test const marginal inference
v0.15.1,test effect inference
v0.15.1,test marginal effect inference
v0.15.1,test coef_ and intercept_ inference
v0.15.1,verify we can generate the summary
v0.15.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.15.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.15.1,create a simple artificial setup where effect of moving from treatment
v0.15.1,"1 -> 2 is 2,"
v0.15.1,"1 -> 3 is 1, and"
v0.15.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.15.1,"Using an uneven number of examples from different classes,"
v0.15.1,"and having the treatments in non-lexicographic order,"
v0.15.1,Should rule out some basic issues.
v0.15.1,test that we can fit with a KFold instance
v0.15.1,test that we can fit with a train/test iterable
v0.15.1,"for at least some of the examples, the CI should have nonzero width"
v0.15.1,"for at least some of the examples, the CI should have nonzero width"
v0.15.1,"for at least some of the examples, the CI should have nonzero width"
v0.15.1,test coef__inference function works
v0.15.1,test intercept__inference function works
v0.15.1,test summary function works
v0.15.1,Test inputs
v0.15.1,self._test_inputs(DR_learner)
v0.15.1,Test constant treatment effect
v0.15.1,Test heterogeneous treatment effect
v0.15.1,Test heterogenous treatment effect for W =/= None
v0.15.1,Sparse DGP
v0.15.1,Treatment effect coef
v0.15.1,Other coefs
v0.15.1,Features and controls
v0.15.1,Test sparse estimator
v0.15.1,"--> test coef_, intercept_"
v0.15.1,--> test treatment effects
v0.15.1,Restrict x_test to vectors of norm < 1
v0.15.1,--> check inference
v0.15.1,Check that a majority of true effects lie in the 5-95% CI
v0.15.1,test outer grouping
v0.15.1,NOTE: StratifiedGroupKFold has a bug when shuffle is True where it doesn't always stratify properly
v0.15.1,so we explicitly pass a StratifiedGroupKFold with shuffle=False (the default) rather than letting
v0.15.1,cross-fit generate one
v0.15.1,"with 2-fold grouping, we should get exactly 3 groups per split"
v0.15.1,test nested grouping
v0.15.1,"with 2-fold outer and 2-fold inner grouping, we should get 1-2 groups per split"
v0.15.1,helper class
v0.15.1,Fit learner and get the effect
v0.15.1,Get the true treatment effect
v0.15.1,Compute treatment effect residuals (absolute)
v0.15.1,Check that at least 90% of predictions are within tolerance interval
v0.15.1,Only for heterogeneous TE
v0.15.1,Fit learner on X and W and get the effect
v0.15.1,Get the true treatment effect
v0.15.1,Compute treatment effect residuals (absolute)
v0.15.1,Check that at least 90% of predictions are within tolerance interval
v0.15.1,Check that one can pass in regular lists
v0.15.1,Check that it fails correctly if lists of different shape are passed in
v0.15.1,Check that it fails when T contains values other than 0 and 1
v0.15.1,"Check that it works when T, Y have shape (n, 1)"
v0.15.1,Generate covariates
v0.15.1,Generate treatment
v0.15.1,Calculate outcome
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,DGP constants
v0.15.1,DGP coefficients
v0.15.1,Generated outcomes
v0.15.1,################
v0.15.1,WeightedLasso #
v0.15.1,################
v0.15.1,Define weights
v0.15.1,Define extended datasets
v0.15.1,Range of alphas
v0.15.1,Compare with Lasso
v0.15.1,--> No intercept
v0.15.1,--> With intercept
v0.15.1,When DGP has no intercept
v0.15.1,When DGP has intercept
v0.15.1,--> Coerce coefficients to be positive
v0.15.1,--> Toggle max_iter & tol
v0.15.1,Define weights
v0.15.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.15.1,Mixed DGP scenario.
v0.15.1,Define extended datasets
v0.15.1,Define weights
v0.15.1,Define multioutput
v0.15.1,##################
v0.15.1,WeightedLassoCV #
v0.15.1,##################
v0.15.1,Define alphas to test
v0.15.1,Compare with LassoCV
v0.15.1,--> No intercept
v0.15.1,--> With intercept
v0.15.1,--> Force parameters to be positive
v0.15.1,Choose a smaller n to speed-up process
v0.15.1,Compare fold weights
v0.15.1,Define weights
v0.15.1,Define extended datasets
v0.15.1,Define splitters
v0.15.1,WeightedKFold splitter
v0.15.1,Map weighted splitter to an extended splitter
v0.15.1,Define alphas to test
v0.15.1,Compare with LassoCV
v0.15.1,--> No intercept
v0.15.1,--> With intercept
v0.15.1,--> Force parameters to be positive
v0.15.1,###########################
v0.15.1,MultiTaskWeightedLassoCV #
v0.15.1,###########################
v0.15.1,Define alphas to test
v0.15.1,Define splitter
v0.15.1,Compare with MultiTaskLassoCV
v0.15.1,--> No intercept
v0.15.1,--> With intercept
v0.15.1,Define weights
v0.15.1,Define extended datasets
v0.15.1,Define splitters
v0.15.1,WeightedKFold splitter
v0.15.1,Map weighted splitter to an extended splitter
v0.15.1,Define alphas to test
v0.15.1,Compare with LassoCV
v0.15.1,--> No intercept
v0.15.1,--> With intercept
v0.15.1,#########################
v0.15.1,WeightedLassoCVWrapper #
v0.15.1,#########################
v0.15.1,perform 1D fit
v0.15.1,perform 2D fit
v0.15.1,################
v0.15.1,DebiasedLasso #
v0.15.1,################
v0.15.1,Test DebiasedLasso without weights
v0.15.1,--> Check debiased coeffcients without intercept
v0.15.1,--> Check debiased coeffcients with intercept
v0.15.1,--> Check 5-95 CI coverage for unit vectors
v0.15.1,Test DebiasedLasso with weights for one DGP
v0.15.1,Define weights
v0.15.1,Define extended datasets
v0.15.1,--> Check debiased coefficients
v0.15.1,Define weights
v0.15.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.15.1,--> Check debiased coeffcients
v0.15.1,Test that attributes propagate correctly
v0.15.1,Test MultiOutputDebiasedLasso without weights
v0.15.1,--> Check debiased coeffcients without intercept
v0.15.1,--> Check debiased coeffcients with intercept
v0.15.1,--> Check CI coverage
v0.15.1,Test MultiOutputDebiasedLasso with weights
v0.15.1,Define weights
v0.15.1,Define extended datasets
v0.15.1,--> Check debiased coefficients
v0.15.1,Unit vectors
v0.15.1,Unit vectors
v0.15.1,Check coeffcients and intercept are the same within tolerance
v0.15.1,Check results are similar with tolerance 1e-6
v0.15.1,Check if multitask
v0.15.1,Check that same alpha is chosen
v0.15.1,Check that the coefficients are similar
v0.15.1,selective ridge has a simple implementation that we can test against
v0.15.1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.15.1,"it should be the case that when we set fit_intercept to true,"
v0.15.1,it doesn't matter whether the penalized model also fits an intercept or not
v0.15.1,create an extra copy of rows with weight 2
v0.15.1,"instead of a slice, explicitly return an array of indices"
v0.15.1,_penalized_inds is only set during fitting
v0.15.1,cv exists on penalized model
v0.15.1,now we can access _penalized_inds
v0.15.1,check that we can read the cv attribute back out from the underlying model
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"global and cohort data should have exactly the same structure, but different values"
v0.15.1,local index should have as many times entries as global as there were rows passed in
v0.15.1,continuous treatments have typical treatment values equal to
v0.15.1,the mean of the absolute value of non-zero entries
v0.15.1,discrete treatments have typical treatment value 1
v0.15.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.1,policy value should exceed always treating with any treatment
v0.15.1,"global shape is (d_y, sum(d_t))"
v0.15.1,global and cohort row-wise dicts have d_y * d_t entries
v0.15.1,local dictionary is flattened to n_rows * d_y * d_t
v0.15.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.1,features; for categoricals they should appear #cats-1 times each
v0.15.1,"global and cohort data should have exactly the same structure, but different values"
v0.15.1,local index should have as many times entries as global as there were rows passed in
v0.15.1,features; for categoricals they should appear #cats-1 times each
v0.15.1,"global shape is (d_y, sum(d_t))"
v0.15.1,global and cohort row-wise dicts have d_y * d_t entries
v0.15.1,local dictionary is flattened to n_rows * d_y * d_t
v0.15.1,continuous treatments have typical treatment values equal to
v0.15.1,the mean of the absolute value of non-zero entries
v0.15.1,discrete treatments have typical treatment value 1
v0.15.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.1,policy value should exceed always treating with any treatment
v0.15.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.1,"global and cohort data should have exactly the same structure, but different values"
v0.15.1,local index should have as many times entries as global as there were rows passed in
v0.15.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.1,policy value should exceed always treating with any treatment
v0.15.1,"global shape is (d_y, sum(d_t))"
v0.15.1,global and cohort row-wise dicts have d_y * d_t entries
v0.15.1,local dictionary is flattened to n_rows * d_y * d_t
v0.15.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.1,features; for categoricals they should appear #cats-1 times each
v0.15.1,make sure we don't run into problems dropping every index
v0.15.1,"global and cohort data should have exactly the same structure, but different values"
v0.15.1,local index should have as many times entries as global as there were rows passed in
v0.15.1,"global shape is (d_y, sum(d_t))"
v0.15.1,global and cohort row-wise dicts have d_y * d_t entries
v0.15.1,local dictionary is flattened to n_rows * d_y * d_t
v0.15.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.1,policy value should exceed always treating with any treatment
v0.15.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.1,"global and cohort data should have exactly the same structure, but different values"
v0.15.1,local index should have as many times entries as global as there were rows passed in
v0.15.1,features; for categoricals they should appear #cats-1 times each
v0.15.1,"global shape is (d_y, sum(d_t))"
v0.15.1,global and cohort row-wise dicts have d_y * d_t entries
v0.15.1,local dictionary is flattened to n_rows * d_y * d_t
v0.15.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.1,policy value should exceed always treating with any treatment
v0.15.1,dgp
v0.15.1,model
v0.15.1,model
v0.15.1,"columns 'd', 'e', 'h' have too many values"
v0.15.1,"columns 'd', 'e' have too many values"
v0.15.1,lowering bound shouldn't affect already fit columns when warm starting
v0.15.1,"column d is now okay, too"
v0.15.1,verify that we can use a scalar treatment cost
v0.15.1,verify that we can specify per-treatment costs for each sample
v0.15.1,verify that using the same state returns the same results each time
v0.15.1,set the categories for column 'd' explicitly so that b is default
v0.15.1,"first column: 10 ones, this is fine"
v0.15.1,"second column: 6 categories, plenty of random instances of each"
v0.15.1,this is fine only if we increase the cateogry limit
v0.15.1,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.15.1,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.15.1,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.15.1,forest heterogeneity won't work
v0.15.1,"sixth column: just 1 one, not enough even without check"
v0.15.1,increase bound on cat expansion
v0.15.1,skip checks (reducing folds accordingly)
v0.15.1,"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
v0.15.1,"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
v0.15.1,Pass an example where W is irrelevant and X is confounder
v0.15.1,"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
v0.15.1,zeroed out and the test will fail
v0.15.1,"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
v0.15.1,rescaling X shouldn't affect the first stage models because they normalize the inputs
v0.15.1,"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
v0.15.1,cross terms
v0.15.1,scale by 1000 to match the input to this model:
v0.15.1,"the scale of X does matter for the final model, which keeps results in user-denominated units"
v0.15.1,rescaling X still shouldn't affect the first stage models
v0.15.1,TODO: we don't recover the correct values with enough accuracy to enable this assertion
v0.15.1,is there a different way to verify that we are learning the correct coefficients?
v0.15.1,"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,DGP constants
v0.15.1,Define data features
v0.15.1,Added `_df`to names to be different from the default cate_estimator names
v0.15.1,Generate data
v0.15.1,################################
v0.15.1,Single treatment and outcome #
v0.15.1,################################
v0.15.1,Test LinearDML
v0.15.1,|--> Test featurizers
v0.15.1,"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
v0.15.1,|--> Test re-fit
v0.15.1,Test SparseLinearDML
v0.15.1,Test ForestDML
v0.15.1,###################################
v0.15.1,Mutiple treatments and outcomes #
v0.15.1,###################################
v0.15.1,Test LinearDML
v0.15.1,Test SparseLinearDML
v0.15.1,"Single outcome only, ORF does not support multiple outcomes"
v0.15.1,Test DMLOrthoForest
v0.15.1,Test DROrthoForest
v0.15.1,Test XLearner
v0.15.1,Skipping population summary names test because bootstrap inference is too slow
v0.15.1,Test SLearner
v0.15.1,Test TLearner
v0.15.1,Test LinearDRLearner
v0.15.1,Test SparseLinearDRLearner
v0.15.1,Test ForestDRLearner
v0.15.1,Test LinearIntentToTreatDRIV
v0.15.1,Test DeepIV
v0.15.1,Test categorical treatments
v0.15.1,Check refit
v0.15.1,Check refit after setting categories
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Linear models are required for parametric dml
v0.15.1,sample weighting models are required for nonparametric dml
v0.15.1,Test values
v0.15.1,TLearner test
v0.15.1,Instantiate TLearner
v0.15.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.1,Test constant treatment effect with multi output Y
v0.15.1,Test heterogeneous treatment effect
v0.15.1,Need interactions between T and features
v0.15.1,Test heterogeneous treatment effect with multi output Y
v0.15.1,Instantiate DomainAdaptationLearner
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,test base values equals to mean of constant marginal effect
v0.15.1,test shape of shap values output is as expected
v0.15.1,test shape of attribute of explanation object is as expected
v0.15.1,test length of feature names equals to shap values shape
v0.15.1,test base values equals to mean of constant marginal effect
v0.15.1,test shape of shap values output is as expected
v0.15.1,test shape of attribute of explanation object is as expected
v0.15.1,test length of feature names equals to shap values shape
v0.15.1,import here since otherwise test collection would fail if matplotlib is not installed
v0.15.1,Treatment effect function
v0.15.1,Outcome support
v0.15.1,Treatment support
v0.15.1,"Generate controls, covariates, treatments and outcomes"
v0.15.1,Heterogeneous treatment effects
v0.15.1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.15.1,through shap package.
v0.15.1,test shap could generate the plot from the shap_values
v0.15.1,"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Check inputs
v0.15.1,Check inputs
v0.15.1,Check inputs
v0.15.1,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.15.1,"Thus, we append the controls column before the one-hot-encoded T"
v0.15.1,"We might want to revisit, though, since it's linearly determined by the others"
v0.15.1,Check inputs
v0.15.1,Check inputs
v0.15.1,Estimate response function
v0.15.1,Check inputs
v0.15.1,Train model on controls. Assign higher weight to units resembling
v0.15.1,treated units.
v0.15.1,Train model on the treated. Assign higher weight to units resembling
v0.15.1,control units.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,TODO: make sure to use random seeds wherever necessary
v0.15.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.15.1,"unfortunately with the Theano and Tensorflow backends,"
v0.15.1,the straightforward use of K.stop_gradient can cause an error
v0.15.1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.15.1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.15.1,so that those layers remain connected but with 0 gradient
v0.15.1,|| t - mu_i || ^2
v0.15.1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.15.1,Use logsumexp for numeric stability:
v0.15.1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.15.1,TODO: does the numeric stability actually make any difference?
v0.15.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.15.1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.15.1,generate cumulative sum via matrix multiplication
v0.15.1,"Generate standard uniform values in shape (batch_size,1)"
v0.15.1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.15.1,we use uniform_like instead with an input of an appropriate shape)
v0.15.1,convert to floats and multiply to perform equivalent of logical AND
v0.15.1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.15.1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.15.1,we use normal_like instead with an input of an appropriate shape)
v0.15.1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.15.1,prevent gradient from passing through sampling
v0.15.1,three options: biased or upper-bound loss require a single number of samples;
v0.15.1,unbiased can take different numbers for the network and its gradient
v0.15.1,"sample: (() -> Layer, int) -> Layer"
v0.15.1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.15.1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.15.1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.15.1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.15.1,the dimensionality of the output of the network
v0.15.1,TODO: is there a more robust way to do this?
v0.15.1,TODO: do we need to give the user more control over other arguments to fit?
v0.15.1,"subtle point: we need to build a new model each time,"
v0.15.1,because each model encapsulates its randomness
v0.15.1,TODO: do we need to give the user more control over other arguments to fit?
v0.15.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.15.1,not a general tensor (because of how backprop works in every framework)
v0.15.1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.15.1,but this seems annoying...)
v0.15.1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.15.1,TODO: any way to get this to work on batches of arbitrary size?
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.15.1,TODO: prel_model_effect could allow sample_var and freq_weight?
v0.15.1,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.15.1,"we need to undo the one-hot encoding for calling effect,"
v0.15.1,since it expects raw values
v0.15.1,"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
v0.15.1,reshape the predictions
v0.15.1,concat W and Z
v0.15.1,check nuisances outcome shape
v0.15.1,Y_res could be a vector or 1-dimensional 2d-array
v0.15.1,"We're projecting, so we're treating E[T|X,Z] as the instrument (ignoring W for simplicity)"
v0.15.1,"Then beta(X) = E[T̃ (E[T|X,Z]-E[E[T|X,Z]|X)|X] and we can apply the tower rule several times to get"
v0.15.1,"= E[(E[T|X,Z]-E[T|X])^2|X]"
v0.15.1,"and also     = E[(E[T|X,Z]-T)^2|X]"
v0.15.1,so we can compute it either from (T_proj-T_pred)^2 or from (T_proj-T)^2
v0.15.1,The first of these is just Z_res^2
v0.15.1,"fit on T*T_proj, covariance will be computed by E[T_res * T_proj] = E[T*T_proj] - E[T]^2"
v0.15.1,"return shape (n,)"
v0.15.1,we will fit on the covariance (T_res*Z_res) directly
v0.15.1,"fit on TZ, covariance will be computed by E[T_res * Z_res] = TZ_pred - T_pred * Z_pred"
v0.15.1,"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
v0.15.1,"shape (n,)"
v0.15.1,"shape (n,)"
v0.15.1,"shape(n,)"
v0.15.1,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.15.1,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.15.1,"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
v0.15.1,reshape the predictions
v0.15.1,"in the projection case, this is a variance and should always be non-negative"
v0.15.1,check nuisances outcome shape
v0.15.1,"all could be reshaped to vector since Y, T, Z are all single dimensional."
v0.15.1,Estimate final model of theta(X) by minimizing the square loss:
v0.15.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.15.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.15.1,at the expense of some small bias. For points with very small covariance we revert
v0.15.1,to the model-based preliminary estimate and do not add the correction term.
v0.15.1,A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
v0.15.1,Used by both DRIV and IntentToTreatDRIV.
v0.15.1,Maggie: I think that would be the case?
v0.15.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.15.1,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.15.1,attribute so that the trained featurizer will be passed through
v0.15.1,Handles the corner case when X=None but featurizer might be not None
v0.15.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.1,"this is a regression model since the instrument E[T|X,W,Z] is always continuous"
v0.15.1,"we're using E[T|X,W,Z] as the instrument"
v0.15.1,Define the data generation functions
v0.15.1,Define the data generation functions
v0.15.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.1,Define the data generation functions
v0.15.1,TODO: support freq_weight and sample_var in debiased lasso
v0.15.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.1,Define the data generation functions
v0.15.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.1,concat W and Z
v0.15.1,"we need to undo the one-hot encoding for calling effect,"
v0.15.1,since it expects raw values
v0.15.1,concat W and Z
v0.15.1,"we need to undo the one-hot encoding for calling effect,"
v0.15.1,since it expects raw values
v0.15.1,reshape the predictions
v0.15.1,"T_res, Z_res, beta expect shape to be (n,1)"
v0.15.1,Define the data generation functions
v0.15.1,maybe shouldn't expose fit_cate_intercept in this class?
v0.15.1,Define the data generation functions
v0.15.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.15.1,TODO: do correct adjustment for sample_var
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,concat W and Z
v0.15.1,concat W and Z
v0.15.1,concat W and Z
v0.15.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.15.1,Define the data generation functions
v0.15.1,"train E[T|X,W,Z]"
v0.15.1,"train E[Z|X,W]"
v0.15.1,note: discrete_instrument rather than discrete_treatment in call to _make_first_stage_selector
v0.15.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.15.1,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.15.1,attribute so that the trained featurizer will be passed through
v0.15.1,Handles the corner case when X=None but featurizer might be not None
v0.15.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.1,concat W and Z
v0.15.1,note that groups are not passed to score because they are only used for fitting
v0.15.1,concat W and Z
v0.15.1,note that sample_weight and groups are not passed to predict because they are only used for fitting
v0.15.1,concat W and Z
v0.15.1,A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
v0.15.1,Used by both Parametric and Non Parametric DMLIV.
v0.15.1,override only so that we can enforce Z to be required
v0.15.1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.15.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.15.1,for internal use by the library
v0.15.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.1,Handles the corner case when X=None but featurizer might be not None
v0.15.1,Define the data generation functions
v0.15.1,Get input names
v0.15.1,Summary
v0.15.1,coefficient
v0.15.1,intercept
v0.15.1,Define the data generation functions
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,"this will have dimension (d,) + shape(X)"
v0.15.1,send the first dimension to the end
v0.15.1,columns are featurized independently; partial derivatives are only non-zero
v0.15.1,when taken with respect to the same column each time
v0.15.1,don't fit intercept; manually add column of ones to the data instead;
v0.15.1,this allows us to ignore the intercept when computing marginal effects
v0.15.1,make T 2D if if was a vector
v0.15.1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.15.1,two stage approximation
v0.15.1,"first, get basis expansions of T, X, and Z"
v0.15.1,TODO: is it right that the effective number of intruments is the
v0.15.1,"product of ft_X and ft_Z, not just ft_Z?"
v0.15.1,"regress T expansion on X,Z expansions concatenated with W"
v0.15.1,"predict ft_T from interacted ft_X, ft_Z"
v0.15.1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.15.1,dT may be only 2-dimensional)
v0.15.1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.15.1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,TODO: this utility is documented but internal; reimplement?
v0.15.1,TODO: this utility is even less public...
v0.15.1,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.15.1,use same Cs as would be used by default by LogisticRegressionCV
v0.15.1,NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
v0.15.1,which could affect how many times each distinct Y value needs to be present in the data
v0.15.1,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.15.1,but also supports get_feature_names with expected signature
v0.15.1,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.15.1,NOTE: we rely on the passthrough columns coming first in the concatenated X;W
v0.15.1,"when we pipeline scaling with our first stage models later, so the order here is important"
v0.15.1,TODO: remove once older sklearn support is no longer needed
v0.15.1,Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
v0.15.1,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.15.1,Convert SingleTreeInterpreter to a python dictionary
v0.15.1,named tuple type for storing results inside CausalAnalysis class;
v0.15.1,must be lifted to module level to enable pickling
v0.15.1,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
v0.15.1,"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
v0.15.1,"when running the first stage models, but don't want to scale the X columns when running the final model,"
v0.15.1,since then our coefficients will have odd units and our trees will also have decisions using those units.
v0.15.1,
v0.15.1,"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
v0.15.1,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.15.1,Controls are all other columns of X
v0.15.1,"can't use X[:, feat_ind] when X is a DataFrame"
v0.15.1,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.15.1,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.15.1,so that the user can opt-in to allowing unseen treatment values
v0.15.1,(and return NaN or something in that case)
v0.15.1,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
v0.15.1,and so we can just peel the first columns off of that combined array for rescaling in the pipeline
v0.15.1,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
v0.15.1,"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
v0.15.1,becomes a valid approach to handling this
v0.15.1,array checking routines don't accept 0-width arrays
v0.15.1,perform model selection
v0.15.1,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.15.1,convert to NormalInferenceResults for consistency
v0.15.1,Set the dictionary values shared between local and global summaries
v0.15.1,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.15.1,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.15.1,required to fit a discrete DML model
v0.15.1,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
v0.15.1,sub-cases of models or also integrate with azure autoML. (post-MVP)
v0.15.1,"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
v0.15.1,"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
v0.15.1,TODO: Enable multi-class classification (post-MVP)
v0.15.1,Validate inputs
v0.15.1,TODO: check compatibility of X and Y lengths
v0.15.1,"no previous fit, cancel warm start"
v0.15.1,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.15.1,"if heterogeneity_inds is 1D, repeat it"
v0.15.1,heterogeneity inds should be a 2D list of length same as train_inds
v0.15.1,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.15.1,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.15.1,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.15.1,train the Y model
v0.15.1,"perform model selection for the Y model using all X, not on a per-column basis"
v0.15.1,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.15.1,work with the regression wrapper
v0.15.1,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.15.1,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.15.1,since otherwise we'll have too many columns to be able to train a classifier
v0.15.1,start with empty results and default shared insights
v0.15.1,convert categorical indicators to numeric indices
v0.15.1,check for indices over the categorical expansion bound
v0.15.1,assume we'll be able to train former failures this time; we'll add them back if not
v0.15.1,"can't remove in place while iterating over new_inds, so store in separate list"
v0.15.1,"train the model, but warn"
v0.15.1,no model can be trained in this case since we need more folds
v0.15.1,"don't train a model, but suggest workaround since there are enough instances of least"
v0.15.1,populated class
v0.15.1,also remove from train_inds so we don't try to access the result later
v0.15.1,extract subset of names matching new columns
v0.15.1,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.15.1,don't want to cache this failed result
v0.15.1,properties to return from effect InferenceResults
v0.15.1,properties to return from PopulationSummaryResults
v0.15.1,Converts strings to property lookups or method calls as a convenience so that the
v0.15.1,_point_props and _summary_props above can be applied to an inference object
v0.15.1,Create a summary combining all results into a single output; this is used
v0.15.1,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.15.1,"or a dictionary, respectively, based on the summary function passed into this method"
v0.15.1,"ensure array has shape (m,y,t)"
v0.15.1,population summary is missing sample dimension; add it for consistency
v0.15.1,outcome dimension is missing; add it for consistency
v0.15.1,add singleton treatment dimension if missing
v0.15.1,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.15.1,"each attr has dimension (m,y) or (m,y,t)"
v0.15.1,concatenate along treatment dimension
v0.15.1,"for dictionary representation, want to remove unneeded sample dimension"
v0.15.1,in cohort and global results
v0.15.1,TODO: enrich outcome logic for multi-class classification when that is supported
v0.15.1,There is no actual sample level in this data
v0.15.1,can't drop only level
v0.15.1,should be serialization-ready and contain no numpy arrays
v0.15.1,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.15.1,TODO: Note that there's no column metadata for the sample number - should there be?
v0.15.1,"need to replicate the column info for each sample, then remove from the shared data"
v0.15.1,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.15.1,which may need to be revisited once we support multiclass
v0.15.1,get the length of the list corresponding to the first dictionary key
v0.15.1,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.15.1,a global inference indicates the effect of that one feature on the outcome
v0.15.1,need to reshape the output to match the input
v0.15.1,we want to offset the inference object by the baseline estimate of y
v0.15.1,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.15.1,get the length of the list corresponding to the first dictionary key
v0.15.1,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.15.1,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.15.1,because then scaling the difference between treatment value and treatment costs is the
v0.15.1,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.15.1,
v0.15.1,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.15.1,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.15.1,(rather than just not treating at all)
v0.15.1,
v0.15.1,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.15.1,2 * policy_value - always_treat
v0.15.1,includes exactly their contribution because policy_value and always_treat both include it
v0.15.1,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.15.1,2 * policy_value - always-treat
v0.15.1,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.15.1,is zero and the contribution to always_treat is negative
v0.15.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.15.1,"however, the tree can't store the feature and treatment names we compute here..."
v0.15.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.15.1,"however, the tree can't store the feature and treatment names we compute here..."
v0.15.1,get dataframe with all but selected column
v0.15.1,apply 10% of a typical treatment for this feature
v0.15.1,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.15.1,set the effect bounds; for positive treatments these agree with
v0.15.1,"the estimates; for negative treatments, we need to invert the interval"
v0.15.1,the effect is now always positive since we decrease treatment when negative
v0.15.1,"for discrete treatment, stack a zero result in front for control"
v0.15.1,we need to call effect_inference to get the correct CI between the two treatment options
v0.15.1,we now need to construct the delta in the cost between the two treatments and translate the effect
v0.15.1,remove third dimenions potentially added
v0.15.1,"find cost of current treatment: equality creates a 2d array with True on each row,"
v0.15.1,only if its the location of the current treatment. Then we take the corresponding cost.
v0.15.1,construct index of current treatment
v0.15.1,add second dimension if needed for broadcasting during translation of effect
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,TODO: conisder working around relying on sklearn implementation details
v0.15.1,"Found a good split, return."
v0.15.1,Record all splits in case the stratification by weight yeilds a worse partition
v0.15.1,Reseed random generator and try again
v0.15.1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.15.1,"Found a good split, return."
v0.15.1,Did not find a good split
v0.15.1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.15.1,Return most weight-balanced partition
v0.15.1,Weight stratification algorithm
v0.15.1,Sort weights for weight strata search
v0.15.1,There are some leftover indices that have yet to be assigned
v0.15.1,Append stratum splits to overall splits
v0.15.1,only expose predict_proba if best_model has predict_proba
v0.15.1,used because logic elsewhere uses hasattr predict proba to check if model is a classifier
v0.15.1,logic copied from check_cv
v0.15.1,otherwise we will assume the user already set the cv attribute to something
v0.15.1,compatible with splitting with a `groups` argument
v0.15.1,"drop groups from arg list, which were already used at the outer level and may not be supported by the model"
v0.15.1,the score needs to be compared to another model's
v0.15.1,"so we don't need to fit the model itself on all of the data, just get the out-of-sample score"
v0.15.1,use _fit_with_groups instead of just fit to handle nested grouping
v0.15.1,we need to train the model on the data
v0.15.1,copy common parameters
v0.15.1,copy common fitted variables
v0.15.1,make sure all classes agree on best c/l1 combo
v0.15.1,"We need an R^2 score to compare to other models; ElasticNetCV doesn't provide it,"
v0.15.1,but we can calculate it ourselves from the MSE plus the variance of the target y
v0.15.1,"l1 ratio doesn't apply to Lasso, only ElasticNet"
v0.15.1,max R^2 corresponds to min MSE
v0.15.1,"constructor takes cv as a positional or kwarg, just pull it out of a new instance"
v0.15.1,"If classification methods produce multiple columns of output,"
v0.15.1,we need to manually encode classes to ensure consistent column ordering.
v0.15.1,We clone the estimator to make sure that all the folds are
v0.15.1,"independent, and that it is pickle-able."
v0.15.1,verbose was removed from sklearn's non-public _fit_and_predict method in 1.4
v0.15.1,`predictions` is a list of method outputs from each fold.
v0.15.1,"If each of those is also a list, then treat this as a"
v0.15.1,multioutput-multiclass task. We need to separately concatenate
v0.15.1,the method outputs for each label into an `n_labels` long list.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Our classes that derive from sklearn ones sometimes include
v0.15.1,inherited docstrings that have embedded doctests; we need the following imports
v0.15.1,so that they don't break.
v0.15.1,TODO: consider working around relying on sklearn implementation details
v0.15.1,local import to avoid circular imports
v0.15.1,"Convert X, y into numpy arrays"
v0.15.1,Define fit parameters
v0.15.1,Some algorithms don't have a check_input option
v0.15.1,Check weights array
v0.15.1,Check that weights are size-compatible
v0.15.1,Normalize inputs
v0.15.1,Weight inputs
v0.15.1,Fit base class without intercept
v0.15.1,Fit Lasso
v0.15.1,Reset intercept
v0.15.1,The intercept is not calculated properly due the sqrt(weights) factor
v0.15.1,so it must be recomputed
v0.15.1,Fit lasso without weights
v0.15.1,Make weighted splitter
v0.15.1,Fit weighted model
v0.15.1,Make weighted splitter
v0.15.1,Fit weighted model
v0.15.1,Call weighted lasso on reduced design matrix
v0.15.1,Weighted tau
v0.15.1,Select optimal penalty
v0.15.1,Warn about consistency
v0.15.1,"Convert X, y into numpy arrays"
v0.15.1,Fit weighted lasso with user input
v0.15.1,"Center X, y"
v0.15.1,Calculate quantities that will be used later on. Account for centered data
v0.15.1,Calculate coefficient and error variance
v0.15.1,Add coefficient correction
v0.15.1,Set coefficients and intercept standard errors
v0.15.1,Set intercept
v0.15.1,Return alpha to 'auto' state
v0.15.1,"Note that in the case of no intercept, X_offset is 0"
v0.15.1,Calculate the variance of the predictions
v0.15.1,Calculate prediction confidence intervals
v0.15.1,Assumes flattened y
v0.15.1,Compute weighted residuals
v0.15.1,To be done once per target. Assumes y can be flattened.
v0.15.1,Assumes that X has already been offset
v0.15.1,Special case: n_features=1
v0.15.1,Compute Lasso coefficients for the columns of the design matrix
v0.15.1,Compute C_hat
v0.15.1,Compute theta_hat
v0.15.1,Allow for single output as well
v0.15.1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.15.1,Set coef_ attribute
v0.15.1,Set intercept_ attribute
v0.15.1,Set selected_alpha_ attribute
v0.15.1,Set coef_stderr_
v0.15.1,intercept_stderr_
v0.15.1,set model to the single-target estimator by default so there's always a model to get and set attributes on
v0.15.1,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.15.1,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.15.1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.15.1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.15.1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.15.1,now regress X1 on y - X2 * beta2 to learn beta1
v0.15.1,set coef_ and intercept_ attributes
v0.15.1,Note that the penalized model should *not* have an intercept
v0.15.1,don't proxy special methods
v0.15.1,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.15.1,regressor incorrectly
v0.15.1,"Note: for known attributes that have been set this method will not be called,"
v0.15.1,so we should just throw here because this is an attribute belonging to this class
v0.15.1,but which hasn't yet been set on this instance
v0.15.1,set default values for None
v0.15.1,check freq_weight should be integer and should be accompanied by sample_var
v0.15.1,check array shape
v0.15.1,weight X and y and sample_var
v0.15.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.15.1,"For aggregation calculations, always treat wy as an array so that einsum expressions don't need to change"
v0.15.1,We'll collapse results back down afterwards if necessary
v0.15.1,"for federation, we need to store these 5 arrays when using heteroskedasticity-robust inference"
v0.15.1,y dimension is always first in the output when present so that broadcasting works correctly
v0.15.1,set default values for None
v0.15.1,check array shape
v0.15.1,check dimension of instruments is more than dimension of treatments
v0.15.1,weight X and y
v0.15.1,learn point estimate
v0.15.1,solve first stage linear regression E[T|Z]
v0.15.1,"""that"" means T̂"
v0.15.1,solve second stage linear regression E[Y|that]
v0.15.1,(T̂.T*T̂)^{-1}
v0.15.1,learn cov(theta)
v0.15.1,(T̂.T*T̂)^{-1}
v0.15.1,sigma^2
v0.15.1,reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,AzureML
v0.15.1,helper imports
v0.15.1,write the details of the workspace to a configuration file to the notebook library
v0.15.1,if y is a multioutput model
v0.15.1,Make sure second dimension has 1 or more item
v0.15.1,switch _inner Model to a MultiOutputRegressor
v0.15.1,flatten array as automl only takes vectors for y
v0.15.1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.15.1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.15.1,as an sklearn estimator
v0.15.1,fit implementation for a single output model.
v0.15.1,Create experiment for specified workspace
v0.15.1,Configure automl_config with training set information.
v0.15.1,"Wait for remote run to complete, the set the model"
v0.15.1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.15.1,create model and pass model into final.
v0.15.1,"If item is an automl config, get its corresponding"
v0.15.1,AutomatedML Model and add it to new_Args
v0.15.1,"If item is an automl config, get its corresponding"
v0.15.1,AutomatedML Model and set it for this key in
v0.15.1,kwargs
v0.15.1,takes in either automated_ml config and instantiates
v0.15.1,an AutomatedMLModel
v0.15.1,The prefix can only be 18 characters long
v0.15.1,"because prefixes come from kwarg_names, we must ensure they are"
v0.15.1,short enough.
v0.15.1,Get workspace from config file.
v0.15.1,Take the intersect of the white for sample
v0.15.1,weights and linear models
v0.15.1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,average the outcome dimension if it exists and ensure 2d y_pred
v0.15.1,get index of best treatment
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,TODO: consider working around relying on sklearn implementation details
v0.15.1,Create splits of causal tree
v0.15.1,Make sure the correct exception is being rethrown
v0.15.1,Must make sure indices are merged correctly
v0.15.1,Convert rows to columns
v0.15.1,Require group assignment t to be one-hot-encoded
v0.15.1,Get predictions for the 2 splits
v0.15.1,Must make sure indices are merged correctly
v0.15.1,Crossfitting
v0.15.1,Compute weighted nuisance estimates
v0.15.1,-------------------------------------------------------------------------------
v0.15.1,Calculate the covariance matrix corresponding to the BLB inference
v0.15.1,
v0.15.1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.15.1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.15.1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.15.1,in that slice from the overall parameter estimate.
v0.15.1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.15.1,-------------------------------------------------------------------------------
v0.15.1,Calclulate covariance matrix through BLB
v0.15.1,Estimators
v0.15.1,OrthoForest parameters
v0.15.1,Sub-forests
v0.15.1,Auxiliary attributes
v0.15.1,Fit check
v0.15.1,TODO: Check performance
v0.15.1,Must normalize weights
v0.15.1,Override the CATE inference options
v0.15.1,Add blb inference to parent's options
v0.15.1,Generate subsample indices
v0.15.1,Build trees in parallel
v0.15.1,Bootstraping has repetitions in tree sample
v0.15.1,Similar for `a` weights
v0.15.1,Bootstraping has repetitions in tree sample
v0.15.1,Define subsample size
v0.15.1,Safety check
v0.15.1,Draw points to create little bags
v0.15.1,Copy and/or define models
v0.15.1,TODO: ideally the below private attribute logic should be in .fit but is needed in init
v0.15.1,for nuisance estimator generation for parent class
v0.15.1,should refactor later
v0.15.1,Define nuisance estimators
v0.15.1,Define parameter estimators
v0.15.1,Define
v0.15.1,Need to redefine fit here for auto inference to work due to a quirk in how
v0.15.1,wrap_fit is defined
v0.15.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.15.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.15.1,Override to flatten output if T is flat
v0.15.1,Check that all discrete treatments are represented
v0.15.1,Nuissance estimates evaluated with cross-fitting
v0.15.1,Define 2-fold iterator
v0.15.1,Check if there is only one example of some class
v0.15.1,Define 2-fold iterator
v0.15.1,need safe=False when cloning for WeightedModelWrapper
v0.15.1,Compute residuals
v0.15.1,Compute coefficient by OLS on residuals
v0.15.1,"Parameter returned by LinearRegression is (d_T, )"
v0.15.1,Compute residuals
v0.15.1,Compute coefficient by OLS on residuals
v0.15.1,ell_2 regularization
v0.15.1,Ridge regression estimate
v0.15.1,"Parameter returned is of shape (d_T, )"
v0.15.1,Return moments and gradients
v0.15.1,Compute residuals
v0.15.1,Compute moments
v0.15.1,"Moments shape is (n, d_T)"
v0.15.1,Compute moment gradients
v0.15.1,returns shape-conforming residuals
v0.15.1,Copy and/or define models
v0.15.1,Define parameter estimators
v0.15.1,Define moment and mean gradient estimator
v0.15.1,"Check that T is shape (n, )"
v0.15.1,Check T is numeric
v0.15.1,Train label encoder
v0.15.1,Call `fit` from parent class
v0.15.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.15.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.15.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.1,Override to flatten output if T is flat
v0.15.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.1,Expand one-hot encoding to include the zero treatment
v0.15.1,"Test that T contains all treatments. If not, return None"
v0.15.1,Nuissance estimates evaluated with cross-fitting
v0.15.1,Define 2-fold iterator
v0.15.1,Check if there is only one example of some class
v0.15.1,No need to crossfit for internal nodes
v0.15.1,Compute partial moments
v0.15.1,"If any of the values in the parameter estimate is nan, return None"
v0.15.1,Compute partial moments
v0.15.1,Compute coefficient by OLS on residuals
v0.15.1,ell_2 regularization
v0.15.1,Ridge regression estimate
v0.15.1,"Parameter returned is of shape (d_T, )"
v0.15.1,Return moments and gradients
v0.15.1,Compute partial moments
v0.15.1,Compute moments
v0.15.1,"Moments shape is (n, d_T-1)"
v0.15.1,Compute moment gradients
v0.15.1,Need to calculate this in an elegant way for when propensity is 0
v0.15.1,This will flatten T
v0.15.1,Check that T is numeric
v0.15.1,Test whether the input estimator is supported
v0.15.1,Calculate confidence intervals for the parameter (marginal effect)
v0.15.1,Calculate confidence intervals for the effect
v0.15.1,Calculate the effects
v0.15.1,Calculate the standard deviations for the effects
v0.15.1,d_t=None here since we measure the effect across all Ts
v0.15.1,conditionally expand jacobian dimensions to align with einsum str
v0.15.1,Calculate the effects
v0.15.1,Calculate the standard deviations for the effects
v0.15.1,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.1,Licensed under the MIT License.
v0.15.1,Causal tree parameters
v0.15.1,Tree structure
v0.15.1,No need for a random split since the data is already
v0.15.1,a random subsample from the original input
v0.15.1,node list stores the nodes that are yet to be splitted
v0.15.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.15.1,Create local sample set
v0.15.1,Compute nuisance estimates for the current node
v0.15.1,Nuisance estimate cannot be calculated
v0.15.1,Estimate parameter for current node
v0.15.1,Node estimate cannot be calculated
v0.15.1,Calculate moments and gradient of moments for current data
v0.15.1,Calculate inverse gradient
v0.15.1,The gradient matrix is not invertible.
v0.15.1,No good split can be found
v0.15.1,Calculate point-wise pseudo-outcomes rho
v0.15.1,a split is determined by a feature and a sample pair
v0.15.1,the number of possible splits is at most (number of features) * (number of node samples)
v0.15.1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.15.1,parse row and column of random pair
v0.15.1,the sample of the pair is the integer division of the random number with n_feats
v0.15.1,calculate the binary indicator of whether sample i is on the left or the right
v0.15.1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.15.1,calculate the number of samples on the left child for each proposed split
v0.15.1,calculate the analogous binary indicator for the samples in the estimation set
v0.15.1,calculate the number of estimation samples on the left child of each proposed split
v0.15.1,find the upper and lower bound on the size of the left split for the split
v0.15.1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.15.1,on each side.
v0.15.1,similarly for the estimation sample set
v0.15.1,if there is no valid split then don't create any children
v0.15.1,filter only the valid splits
v0.15.1,calculate the average influence vector of the samples in the left child
v0.15.1,calculate the average influence vector of the samples in the right child
v0.15.1,take the square of each of the entries of the influence vectors and normalize
v0.15.1,by size of each child
v0.15.1,calculate the vector score of each candidate split as the average of left and right
v0.15.1,influence vectors
v0.15.1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.15.1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.15.1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.15.1,calculate the scalar score of each split by aggregating across the vector of scores
v0.15.1,Find split that minimizes criterion
v0.15.1,Create child nodes with corresponding subsamples
v0.15.1,add the created children to the list of not yet split nodes
v0.15.0,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.15.0,configuration is all pulled from setup.cfg
v0.15.0,-*- coding: utf-8 -*-
v0.15.0,
v0.15.0,Configuration file for the Sphinx documentation builder.
v0.15.0,
v0.15.0,This file does only contain a selection of the most common options. For a
v0.15.0,full list see the documentation:
v0.15.0,http://www.sphinx-doc.org/en/main/config
v0.15.0,-- Path setup --------------------------------------------------------------
v0.15.0,"If extensions (or modules to document with autodoc) are in another directory,"
v0.15.0,add these directories to sys.path here. If the directory is relative to the
v0.15.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.15.0,
v0.15.0,-- Project information -----------------------------------------------------
v0.15.0,-- General configuration ---------------------------------------------------
v0.15.0,"If your documentation needs a minimal Sphinx version, state it here."
v0.15.0,
v0.15.0,needs_sphinx = '1.0'
v0.15.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.15.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.15.0,ones.
v0.15.0,TODO: enable type aliases
v0.15.0,napoleon_preprocess_types = True  # needed for type aliases to work
v0.15.0,napoleon_type_aliases = {
v0.15.0,"""array_like"": "":term:`array_like`"","
v0.15.0,"""ndarray"": ""~numpy.ndarray"","
v0.15.0,"""RandomState"": "":class:`~numpy.random.RandomState`"","
v0.15.0,"""DataFrame"": "":class:`~pandas.DataFrame`"","
v0.15.0,"""Series"": "":class:`~pandas.Series`"","
v0.15.0,}
v0.15.0,"Add any paths that contain templates here, relative to this directory."
v0.15.0,The suffix(es) of source filenames.
v0.15.0,You can specify multiple suffix as a list of strings:
v0.15.0,
v0.15.0,"source_suffix = ['.rst', '.md']"
v0.15.0,The root toctree document.
v0.15.0,The language for content autogenerated by Sphinx. Refer to documentation
v0.15.0,for a list of supported languages.
v0.15.0,
v0.15.0,This is also used if you do content translation via gettext catalogs.
v0.15.0,"Usually you set ""language"" from the command line for these cases."
v0.15.0,"List of patterns, relative to source directory, that match files and"
v0.15.0,directories to ignore when looking for source files.
v0.15.0,This pattern also affects html_static_path and html_extra_path.
v0.15.0,The name of the Pygments (syntax highlighting) style to use.
v0.15.0,-- Options for HTML output -------------------------------------------------
v0.15.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.15.0,a list of builtin themes.
v0.15.0,
v0.15.0,Theme options are theme-specific and customize the look and feel of a theme
v0.15.0,"further.  For a list of options available for each theme, see the"
v0.15.0,documentation.
v0.15.0,
v0.15.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.15.0,"relative to this directory. They are copied after the builtin static files,"
v0.15.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.15.0,html_static_path = ['_static']
v0.15.0,"Custom sidebar templates, must be a dictionary that maps document names"
v0.15.0,to template names.
v0.15.0,
v0.15.0,The default sidebars (for documents that don't match any pattern) are
v0.15.0,defined by theme itself.  Builtin themes are using these templates by
v0.15.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.15.0,'searchbox.html']``.
v0.15.0,
v0.15.0,html_sidebars = {}
v0.15.0,-- Options for HTMLHelp output ---------------------------------------------
v0.15.0,Output file base name for HTML help builder.
v0.15.0,-- Options for LaTeX output ------------------------------------------------
v0.15.0,The paper size ('letterpaper' or 'a4paper').
v0.15.0,
v0.15.0,"'papersize': 'letterpaper',"
v0.15.0,"The font size ('10pt', '11pt' or '12pt')."
v0.15.0,
v0.15.0,"'pointsize': '10pt',"
v0.15.0,Additional stuff for the LaTeX preamble.
v0.15.0,
v0.15.0,"'preamble': '',"
v0.15.0,Latex figure (float) alignment
v0.15.0,
v0.15.0,"'figure_align': 'htbp',"
v0.15.0,Grouping the document tree into LaTeX files. List of tuples
v0.15.0,"(source start file, target name, title,"
v0.15.0,"author, documentclass [howto, manual, or own class])."
v0.15.0,-- Options for manual page output ------------------------------------------
v0.15.0,One entry per manual page. List of tuples
v0.15.0,"(source start file, name, description, authors, manual section)."
v0.15.0,-- Options for Texinfo output ----------------------------------------------
v0.15.0,Grouping the document tree into Texinfo files. List of tuples
v0.15.0,"(source start file, target name, title, author,"
v0.15.0,"dir menu entry, description, category)"
v0.15.0,-- Options for Epub output -------------------------------------------------
v0.15.0,Bibliographic Dublin Core info.
v0.15.0,The unique identifier of the text. This can be a ISBN number
v0.15.0,or the project homepage.
v0.15.0,
v0.15.0,epub_identifier = ''
v0.15.0,A unique identification for the text.
v0.15.0,
v0.15.0,epub_uid = ''
v0.15.0,A list of files that should not be packed into the epub file.
v0.15.0,-- Extension configuration -------------------------------------------------
v0.15.0,-- Options for intersphinx extension ---------------------------------------
v0.15.0,Example configuration for intersphinx: refer to the Python standard library.
v0.15.0,-- Options for todo extension ----------------------------------------------
v0.15.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.15.0,-- Options for doctest extension -------------------------------------------
v0.15.0,we can document otherwise excluded entities here by returning False
v0.15.0,or skip otherwise included entities by returning True
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Calculate residuals
v0.15.0,Estimate E[T_res | Z_res]
v0.15.0,TODO. Deal with multi-class instrument
v0.15.0,Calculate nuisances
v0.15.0,Estimate E[T_res | Z_res]
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"We do a three way split, as typically a preliminary theta estimator would require"
v0.15.0,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.15.0,TODO. Deal with multi-class instrument
v0.15.0,Estimate final model of theta(X) by minimizing the square loss:
v0.15.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.15.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.15.0,at the expense of some small bias. For points with very small covariance we revert
v0.15.0,to the model-based preliminary estimate and do not add the correction term.
v0.15.0,Estimate preliminary theta in cross fitting manner
v0.15.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.15.0,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.15.0,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.15.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.15.0,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.15.0,TODO. The solution below is not really a valid cross-fitting
v0.15.0,as the test data are used to create the proj_t on the train
v0.15.0,which in the second train-test loop is used to create the nuisance
v0.15.0,cov on the test data. Hence the T variable of some sample
v0.15.0,"is implicitly correlated with its cov nuisance, through this flow"
v0.15.0,"of information. However, this seems a rather weak correlation."
v0.15.0,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.15.0,model.
v0.15.0,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.15.0,Estimate preliminary theta in cross fitting manner
v0.15.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.15.0,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.15.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.15.0,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.15.0,#############################################################################
v0.15.0,Classes for the DRIV implementation for the special case of intent-to-treat
v0.15.0,A/B test
v0.15.0,#############################################################################
v0.15.0,Estimate preliminary theta in cross fitting manner
v0.15.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.15.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.15.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.15.0,We can use statsmodel for all hypothesis testing capabilities
v0.15.0,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.15.0,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.15.0,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.15.0,model_T_XZ = lambda: model_clf()
v0.15.0,#'days_visited': lambda:
v0.15.0,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.15.0,Turn strings into categories for numeric mapping
v0.15.0,### Defining some generic regressors and classifiers
v0.15.0,This a generic non-parametric regressor
v0.15.0,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.15.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.15.0,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.15.0,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.15.0,model = lambda: RandomForestRegressor(n_estimators=100)
v0.15.0,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.15.0,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.15.0,model = lambda: LinearRegression(n_jobs=-1)
v0.15.0,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.15.0,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.15.0,underlying model whenever predict is called.
v0.15.0,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.15.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.15.0,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.15.0,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.15.0,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.15.0,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.15.0,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.15.0,We need to specify models to be used for each of these residualizations
v0.15.0,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.15.0,"E[T | X, Z]"
v0.15.0,E[TZ | X]
v0.15.0,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.15.0,n_splits determines the number of splits to be used for cross-fitting.
v0.15.0,# Algorithm 2 - Current Method
v0.15.0,In[121]:
v0.15.0,# Algorithm 3 - DRIV ATE
v0.15.0,dmliv_model_effect = lambda: model()
v0.15.0,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.15.0,"dmliv_model_effect(),"
v0.15.0,n_splits=1)
v0.15.0,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.15.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.15.0,"Once multiple treatments are supported, we'll need to fix this"
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.15.0,We can use statsmodel for all hypothesis testing capabilities
v0.15.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.15.0,We can use statsmodel for all hypothesis testing capabilities
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,TODO. Deal with multi-class instrument/treatment
v0.15.0,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.15.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.15.0,Estimate p(X) = E[T | X] in cross-fitting manner
v0.15.0,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.15.0,##################
v0.15.0,Global settings #
v0.15.0,##################
v0.15.0,Global plotting controls
v0.15.0,"Control for support size, can control for more"
v0.15.0,#################
v0.15.0,File utilities #
v0.15.0,#################
v0.15.0,#################
v0.15.0,Plotting utils #
v0.15.0,#################
v0.15.0,bias
v0.15.0,var
v0.15.0,rmse
v0.15.0,r2
v0.15.0,Infer feature dimension
v0.15.0,Metrics by support plots
v0.15.0,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.15.0,Vasilis Syrgkanis <vasy@microsoft.com>
v0.15.0,Steven Wu <zhiww@microsoft.com>
v0.15.0,Initialize causal tree parameters
v0.15.0,Create splits of causal tree
v0.15.0,Estimate treatment effects at the leafs
v0.15.0,Compute heterogeneous treatement effect for x's in x_list by finding
v0.15.0,the corresponding split and associating the effect computed on that leaf
v0.15.0,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.15.0,Safety check
v0.15.0,Weighted linear regression
v0.15.0,Calculates weights
v0.15.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.15.0,over all indices
v0.15.0,Similar for `a` weights
v0.15.0,Doesn't have sample weights
v0.15.0,Is a linear model
v0.15.0,Weighted linear regression
v0.15.0,Calculates weights
v0.15.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.15.0,over all indices
v0.15.0,Similar for `a` weights
v0.15.0,normalize weights
v0.15.0,"Split the data in half, train and test"
v0.15.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.15.0,"a function of W, using only the train fold"
v0.15.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.15.0,"Split the data in half, train and test"
v0.15.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.15.0,"a function of W, using only the train fold"
v0.15.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.15.0,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.15.0,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.15.0,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.15.0,"Split the data in half, train and test"
v0.15.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.15.0,"a function of x, using only the train fold"
v0.15.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.15.0,Compute coefficient by OLS on residuals
v0.15.0,"Split the data in half, train and test"
v0.15.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.15.0,"a function of x, using only the train fold"
v0.15.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.15.0,Estimate multipliers for second order orthogonal method
v0.15.0,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.15.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.15.0,Create local sample set
v0.15.0,compute the base estimate for the current node using double ml or second order double ml
v0.15.0,compute the influence functions here that are used for the criterion
v0.15.0,generate random proposals of dimensions to split
v0.15.0,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.15.0,compute criterion for each proposal
v0.15.0,if splitting creates valid leafs in terms of mean leaf size
v0.15.0,Calculate criterion for split
v0.15.0,Else set criterion to infinity so that this split is not chosen
v0.15.0,If no good split was found
v0.15.0,Find split that minimizes criterion
v0.15.0,Set the split attributes at the node
v0.15.0,Create child nodes with corresponding subsamples
v0.15.0,Recursively split children
v0.15.0,Return parent node
v0.15.0,estimate the local parameter at the leaf using the estimate data
v0.15.0,###################
v0.15.0,Argument parsing #
v0.15.0,###################
v0.15.0,#########################################
v0.15.0,Parameters constant across experiments #
v0.15.0,#########################################
v0.15.0,Outcome support
v0.15.0,Treatment support
v0.15.0,Evaluation grid
v0.15.0,Treatment effects array
v0.15.0,Other variables
v0.15.0,##########################
v0.15.0,Data Generating Process #
v0.15.0,##########################
v0.15.0,Log iteration
v0.15.0,"Generate controls, features, treatment and outcome"
v0.15.0,T and Y residuals to be used in later scripts
v0.15.0,Save generated dataset
v0.15.0,#################
v0.15.0,ORF parameters #
v0.15.0,#################
v0.15.0,######################################
v0.15.0,Train and evaluate treatment effect #
v0.15.0,######################################
v0.15.0,########
v0.15.0,Plots #
v0.15.0,########
v0.15.0,###############
v0.15.0,Save results #
v0.15.0,###############
v0.15.0,##############
v0.15.0,Run Rscript #
v0.15.0,##############
v0.15.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.15.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.15.0,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.15.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.15.0,def mlasso_model(): return MultiTaskLassoCV(
v0.15.0,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.15.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.0,heterogeneity
v0.15.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.0,heterogeneity
v0.15.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.0,heterogeneity
v0.15.0,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.15.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.15.0,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.15.0,subset of features that are exogenous and create heterogeneity
v0.15.0,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.15.0,subset of features wrt we estimate heterogeneity
v0.15.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.15.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,introspect the constructor arguments to find the model parameters
v0.15.0,to represent
v0.15.0,"if the argument is deprecated, ignore it"
v0.15.0,Extract and sort argument names excluding 'self'
v0.15.0,column names
v0.15.0,transfer input to numpy arrays
v0.15.0,transfer input to 2d arrays
v0.15.0,create dataframe
v0.15.0,currently dowhy only support single outcome and single treatment
v0.15.0,call dowhy
v0.15.0,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.15.0,cate estimator but not the effect.
v0.15.0,don't proxy special methods
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Check if model is sparse enough for this model
v0.15.0,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.15.0,TODO: any way to avoid creating a copy if the array was already dense?
v0.15.0,"the call is necessary if the input was something like a list, though"
v0.15.0,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.15.0,so convert to pydata sparse first
v0.15.0,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.15.0,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.15.0,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.15.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.15.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.15.0,For when checking input values is disabled
v0.15.0,Type to column extraction function
v0.15.0,if not all column names are strings
v0.15.0,coerce feature names to be strings
v0.15.0,Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
v0.15.0,"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
v0.15.0,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.15.0,Handles cases where the passed feature names create issues
v0.15.0,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.15.0,Get feature names using featurizer
v0.15.0,All attempts at retrieving transformed feature names have failed
v0.15.0,Delegate handling to downstream logic
v0.15.0,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.15.0,same number of input definitions as arrays
v0.15.0,input definitions have same number of dimensions as each array
v0.15.0,all result indices are unique
v0.15.0,all result indices must match at least one input index
v0.15.0,"map indices to all array, axis pairs for that index"
v0.15.0,each index has the same cardinality wherever it appears
v0.15.0,"State: list of (set of letters, list of (corresponding indices, value))"
v0.15.0,Algo: while list contains more than one entry
v0.15.0,take two entries
v0.15.0,sort both lists by intersection of their indices
v0.15.0,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.15.0,"take the union of indices and the product of values), stepping through each list linearly"
v0.15.0,TODO: might be faster to break into connected components first
v0.15.0,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.15.0,"so compute their content separately, then take cartesian product"
v0.15.0,this would save a few pointless sorts by empty tuples
v0.15.0,TODO: Consider investigating other performance ideas for these cases
v0.15.0,where the dense method beat the sparse method (usually sparse is faster)
v0.15.0,"e,facd,c->cfed"
v0.15.0,sparse: 0.0335489
v0.15.0,dense:  0.011465999999999997
v0.15.0,"gbd,da,egb->da"
v0.15.0,sparse: 0.0791625
v0.15.0,dense:  0.007319099999999995
v0.15.0,"dcc,d,faedb,c->abe"
v0.15.0,sparse: 1.2868097
v0.15.0,dense:  0.44605229999999985
v0.15.0,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.15.0,TODO: would using einsum's paths to optimize the order of merging help?
v0.15.0,Normalize weights
v0.15.0,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.15.0,"if we're decorating a class, just update the __init__ method,"
v0.15.0,so that the result is still a class instead of a wrapper method
v0.15.0,"want to enforce that each bad_arg was either in kwargs,"
v0.15.0,or else it was in neither and is just taking its default value
v0.15.0,Any access should throw
v0.15.0,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.15.0,return plain dictionary so that erroneous accesses don't half work (see e.g. #708)
v0.15.0,for every dimension of the treatment add some epsilon and observe change in featurized treatment
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.0,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.0,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.0,input feature name is already updated by cate_feature_names.
v0.15.0,define the index of d_x to filter for each given T
v0.15.0,filter X after broadcast with T for each given T
v0.15.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.0,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.0,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,
v0.15.0,This code contains some snippets of code from:
v0.15.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
v0.15.0,published under the following license and copyright:
v0.15.0,BSD 3-Clause License
v0.15.0,
v0.15.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0,All rights reserved.
v0.15.0,make any access to matplotlib or plt throw an exception
v0.15.0,make any access to graphviz or plt throw an exception
v0.15.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.15.0,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.15.0,Initialize saturation & value; calculate chroma & value shift
v0.15.0,Calculate some intermediate values
v0.15.0,Initialize RGB with same hue & chroma as our color
v0.15.0,Shift the initial RGB values to match value and store
v0.15.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.15.0,clean way of achieving this
v0.15.0,make sure we don't accidentally escape anything in the substitution
v0.15.0,Fetch appropriate color for node
v0.15.0,"red for negative, green for positive"
v0.15.0,in multi-target use mean of targets
v0.15.0,Write node mean CATE
v0.15.0,Write node std of CATE
v0.15.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.15.0,Fetch appropriate color for node
v0.15.0,Write node mean CATE
v0.15.0,Write node mean CATE
v0.15.0,Write recommended treatment and value - cost
v0.15.0,Licensed under the MIT License.
v0.15.0,"since inference objects can be stateful, we must copy it before fitting;"
v0.15.0,otherwise this sequence wouldn't work:
v0.15.0,"est1.fit(..., inference=inf)"
v0.15.0,"est2.fit(..., inference=inf)"
v0.15.0,est1.effect_interval(...)
v0.15.0,because inf now stores state from fitting est2
v0.15.0,This flag is true when names are set in a child class instead
v0.15.0,"If names are set in a child class, add an attribute reflecting that"
v0.15.0,This works only if X is passed as a kwarg
v0.15.0,We plan to enforce X as kwarg only in future releases
v0.15.0,This checks if names have been set in a child class
v0.15.0,"If names were set in a child class, don't do it again"
v0.15.0,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.15.0,call the wrapped fit method
v0.15.0,NOTE: we call inference fit *after* calling the main fit method
v0.15.0,apply defaults before calling inference method
v0.15.0,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.15.0,but tensordot can't be applied to this problem because we don't sum over m
v0.15.0,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.15.0,of rows of T was not taken into account
v0.15.0,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.15.0,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.15.0,"Treatment names is None, default to BaseCateEstimator"
v0.15.0,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.15.0,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.15.0,Get input names
v0.15.0,Summary
v0.15.0,add statsmodels to parent's options
v0.15.0,add debiasedlasso to parent's options
v0.15.0,add blb to parent's options
v0.15.0,TODO Share some logic with non-discrete version
v0.15.0,Get input names
v0.15.0,Note: we do not transform feature names since that is done within summary_frame
v0.15.0,Summary
v0.15.0,add statsmodels to parent's options
v0.15.0,add statsmodels to parent's options
v0.15.0,add blb to parent's options
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,fully materialize folds so that they can be reused across models
v0.15.0,and precompute fitted indices so that we fail fast if there's an issue with them
v0.15.0,NOTE: if any model is missing scores we will just return None even if another model
v0.15.0,has scores. this is because we don't know how many scores are missing
v0.15.0,"for the models that are missing them, so we don't know how to pad the array"
v0.15.0,for convenience we allos a single model to be passed in lieu of a singleton list
v0.15.0,"in that case, we will also unwrap the model output"
v0.15.0,"when there is more than one model, nuisances from previous models"
v0.15.0,come first as positional arguments
v0.15.0,"scores entries should be lists of scores, so make each entry a singleton list"
v0.15.0,Adding the kwargs to ray object store to be used by remote functions
v0.15.0,for each fold to avoid IO overhead
v0.15.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.15.0,generate an instance of the final model
v0.15.0,generate an instance of the nuisance model
v0.15.0,Define Ray remote function (Ray remote wrapper of the _fit_nuisances function)
v0.15.0,Create Ray remote jobs for parallel processing
v0.15.0,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.15.0,alteration even when we only want to fit_final.
v0.15.0,use a binary array to get stratified split in case of discrete treatment
v0.15.0,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.15.0,upgrade to a GroupKFold or StratiGroupKFold if groups is not None
v0.15.0,"we won't have generated a KFold or StratifiedKFold ourselves when groups are passed,"
v0.15.0,"but the user might have supplied one, which won't work"
v0.15.0,self._models_nuisance will be a list of lists or a list of list of lists
v0.15.0,so we use self._ortho_learner_model_nuisance to determine the nesting level
v0.15.0,for each mc iteration
v0.15.0,for each model under cross fit setting
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"TODO: This could be extended to also work with our sparse and 2SLS estimators,"
v0.15.0,if we add an aggregate method to them
v0.15.0,Remember to update the docs if this changes
v0.15.0,mix in the appropriate inference class
v0.15.0,assign all of the attributes from the dummy estimator that would normally be assigned during fitting
v0.15.0,TODO: This seems hacky; is there a better abstraction to maintain these?
v0.15.0,"This should also include bias_part_of_coef, model_final_, and fitted_models_final above"
v0.15.0,Assign treatment expansion attributes
v0.15.0,Methods needed to implement the LinearCateEstimator interface
v0.15.0,Methods needed to implement the LinearFinalModelCateEstimatorMixin
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,
v0.15.0,This code contains snippets of code from
v0.15.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.15.0,published under the following license and copyright:
v0.15.0,BSD 3-Clause License
v0.15.0,
v0.15.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0,All rights reserved.
v0.15.0,=============================================================================
v0.15.0,Policy Forest
v0.15.0,=============================================================================
v0.15.0,Remap output
v0.15.0,reshape is necessary to preserve the data contiguity against vs
v0.15.0,"[:, np.newaxis] that does not."
v0.15.0,Get subsample sample size
v0.15.0,Check parameters
v0.15.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.15.0,if this is the first `fit` call of the warm start mode.
v0.15.0,"Free allocated memory, if any"
v0.15.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.15.0,We draw from the random state to get the random state we
v0.15.0,would have got if we hadn't used a warm_start.
v0.15.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.15.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.15.0,but would still advance randomness enough so that tree subsamples will be different.
v0.15.0,Parallel loop: we prefer the threading backend as the Cython code
v0.15.0,for fitting the trees is internally releasing the Python GIL
v0.15.0,making threading more efficient than multiprocessing in
v0.15.0,"that case. However, for joblib 0.12+ we respect any"
v0.15.0,"parallel_backend contexts set at a higher level,"
v0.15.0,since correctness does not rely on using threads.
v0.15.0,Collect newly grown trees
v0.15.0,Check data
v0.15.0,Assign chunk of trees to jobs
v0.15.0,avoid storing the output of every estimator by summing them here
v0.15.0,Parallel loop
v0.15.0,Check data
v0.15.0,Assign chunk of trees to jobs
v0.15.0,avoid storing the output of every estimator by summing them here
v0.15.0,Parallel loop
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,
v0.15.0,This code contains snippets of code from:
v0.15.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.15.0,published under the following license and copyright:
v0.15.0,BSD 3-Clause License
v0.15.0,
v0.15.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0,All rights reserved.
v0.15.0,=============================================================================
v0.15.0,Types and constants
v0.15.0,=============================================================================
v0.15.0,=============================================================================
v0.15.0,Base Policy tree
v0.15.0,=============================================================================
v0.15.0,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.15.0,HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor
v0.15.0,This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"Unique treatments (ordered, includes control)"
v0.15.0,Number of treatments (excluding control)
v0.15.0,Indicator for whether
v0.15.0,Get DR outcomes in training sample
v0.15.0,Get DR outcomes in validation sample
v0.15.0,Get DR outcomes in validation sample
v0.15.0,Calculate ATE in the validation sample
v0.15.0,Fit propensity in treatment
v0.15.0,Predict propensity scores
v0.15.0,Possible treatments (need to allow more than 2)
v0.15.0,Predict outcomes
v0.15.0,T-learner logic
v0.15.0,"if CATE is given explicitly or has not been fitted at all previously, fit it now"
v0.15.0,Assign units in validation set to groups
v0.15.0,Proportion of validations set in group
v0.15.0,Group average treatment effect (GATE) -- average of DR outcomes in group
v0.15.0,Average of CATE predictions in group
v0.15.0,Calculate group calibration score
v0.15.0,Calculate overall calibration score
v0.15.0,Calculate R-square calibration score
v0.15.0,"if CATE is given explicitly or has not been fitted at all previously, fit it now"
v0.15.0,treat each treatment as a separate regression
v0.15.0,"here, prop_preds should be a matrix"
v0.15.0,with rows corresponding to units and columns corresponding to treatment statuses
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.15.0,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.15.0,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.15.0,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.15.0,checks that X is 2D array.
v0.15.0,"since we only allow single dimensional y, we could flatten the prediction"
v0.15.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.0,Handles the corner case when X=None but featurizer might be not None
v0.15.0,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.15.0,"Replacing this method which is invalid for this class, so that we make the"
v0.15.0,dosctring empty and not appear in the docs.
v0.15.0,TODO: support freq_weight and sample_var in debiased lasso
v0.15.0,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.15.0,Replacing to remove docstring
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"if both X and W are None, just return a column of ones"
v0.15.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.15.0,We need to go back to the label representation of the one-hot so as to call
v0.15.0,the classifier.
v0.15.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.15.0,We need to go back to the label representation of the one-hot so as to call
v0.15.0,the classifier.
v0.15.0,data is already validated at initial fit time
v0.15.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.15.0,This works both with our without the weighting trick as the treatments T are unit vector
v0.15.0,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.15.0,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.15.0,both Parametric and Non Parametric DML.
v0.15.0,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.15.0,attribute so that the trained featurizer will be passed through
v0.15.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.15.0,for internal use by the library
v0.15.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.0,We need to use the rlearner's copy to retain the information from fitting
v0.15.0,Handles the corner case when X=None but featurizer might be not None
v0.15.0,override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
v0.15.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.15.0,TODO: support freq_weight and sample_var in debiased lasso
v0.15.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.15.0,since we clone it and fit separate copies
v0.15.0,add blb to parent's options
v0.15.0,override only so that we can update the docstring to indicate
v0.15.0,support for `GenericSingleTreatmentModelFinalInference`
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,note that groups are not passed to score because they are only used for fitting
v0.15.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.0,NOTE: important to get parent's wrapped copy so that
v0.15.0,"after training wrapped featurizer is also trained, etc."
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.15.0,Fit a doubly robust average effect
v0.15.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.15.0,(which needs to have been expanded if there's a discrete treatment)
v0.15.0,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.15.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.15.0,since we clone it and fit separate copies
v0.15.0,"If custom param grid, check that only estimator parameters are being altered"
v0.15.0,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.15.0,override only so that we can update the docstring to indicate support for `blb`
v0.15.0,Get input names
v0.15.0,Summary
v0.15.0,Determine output settings
v0.15.0,"Important: This must be the first invocation of the random state at fit time, so that"
v0.15.0,train/test splits are re-generatable from an external object simply by knowing the
v0.15.0,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.15.0,linear predictions. Currently is also useful for testing.
v0.15.0,reshape is necessary to preserve the data contiguity against vs
v0.15.0,"[:, np.newaxis] that does not."
v0.15.0,Check parameters
v0.15.0,Set min_weight_leaf from min_weight_fraction_leaf
v0.15.0,Build tree
v0.15.0,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.15.0,hold. Used by criterion for memory space savings.
v0.15.0,Initialize the criterion object and the criterion_val object if honest.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,
v0.15.0,This code is a fork from:
v0.15.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
v0.15.0,published under the following license and copyright:
v0.15.0,BSD 3-Clause License
v0.15.0,
v0.15.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0,All rights reserved.
v0.15.0,Set parameters
v0.15.0,Don't instantiate estimators now! Parameters of base_estimator might
v0.15.0,"still change. Eg., when grid-searching with the nested object syntax."
v0.15.0,self.estimators_ needs to be filled by the derived classes in fit.
v0.15.0,Compute the number of jobs
v0.15.0,Partition estimators between jobs
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,covariance matrix
v0.15.0,get eigen value and eigen vectors
v0.15.0,simulate eigen vectors
v0.15.0,keep the top 4 eigen value and corresponding eigen vector
v0.15.0,replace the negative eigen values
v0.15.0,generate a new covariance matrix
v0.15.0,get linear approximation of eigen values
v0.15.0,coefs
v0.15.0,get the indices of each group of features
v0.15.0,print(ind_same_proxy)
v0.15.0,demo
v0.15.0,same proxy
v0.15.0,residuals
v0.15.0,gmm
v0.15.0,log normal on outliers
v0.15.0,positive outliers
v0.15.0,negative outliers
v0.15.0,demean the new residual again
v0.15.0,generate data
v0.15.0,sample residuals
v0.15.0,get prediction for current investment
v0.15.0,get prediction for current proxy
v0.15.0,get first period prediction
v0.15.0,iterate the step ahead contruction
v0.15.0,prepare new x
v0.15.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.0,heterogeneity
v0.15.0,get new covariance matrix
v0.15.0,get coefs
v0.15.0,get residuals
v0.15.0,proxy 1 is the outcome
v0.15.0,make fixed residuals
v0.15.0,Remove children with nonwhite mothers from the treatment group
v0.15.0,Remove children with nonwhite mothers from the treatment group
v0.15.0,Select columns
v0.15.0,Scale the numeric variables
v0.15.0,"Change the binary variable 'first' takes values in {1,2}"
v0.15.0,Append a column of ones as intercept
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.15.0,(which needs to have been expanded if there's a discrete treatment)
v0.15.0,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.15.0,d_t=None here since we measure the effect across all Ts
v0.15.0,"y is a vector, rather than a 2D array"
v0.15.0,once the estimator has been fit
v0.15.0,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.15.0,an intercept even when bias is part of coef.
v0.15.0,We can write effect inference as a function of prediction and prediction standard error of
v0.15.0,the final method for linear models
v0.15.0,squeeze the first axis
v0.15.0,d_t=None here since we measure the effect across all Ts
v0.15.0,set the mean_pred_stderr
v0.15.0,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.15.0,squeeze the first axis
v0.15.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.15.0,(which needs to have been expanded if there's a discrete treatment)
v0.15.0,"send treatment to the end, pull bounds to the front"
v0.15.0,d_t=None here since we measure the effect across all Ts
v0.15.0,set the mean_pred_stderr
v0.15.0,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.15.0,d_t=None here since we measure the effect across all Ts
v0.15.0,d_t=None here since we measure the effect across all Ts
v0.15.0,need to set the fit args before the estimator is fit
v0.15.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.15.0,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.15.0,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.15.0,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.15.0,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.15.0,scale preds
v0.15.0,scale std errs
v0.15.0,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.15.0,offset preds
v0.15.0,"offset the distribution, too"
v0.15.0,scale preds
v0.15.0,"scale the distribution, too"
v0.15.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.15.0,1. Uncertainty of Mean Point Estimate
v0.15.0,2. Distribution of Point Estimate
v0.15.0,3. Total Variance of Point Estimate
v0.15.0,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.15.0,so bail out early; note that it might be possible to correct the algorithm for
v0.15.0,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.15.0,be clean
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,TODO: Add a __dir__ implementation?
v0.15.0,don't proxy special methods
v0.15.0,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.15.0,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.15.0,then we should be computing a confidence interval for the wrapped calls
v0.15.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.15.0,second level bootstrap which would be prohibitive computationally?
v0.15.0,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.15.0,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.15.0,can't import from econml.inference at top level without creating cyclical dependencies
v0.15.0,Note that inference results are always methods even if the inference is for a property
v0.15.0,(e.g. coef__inference() is a method but coef_ is a property)
v0.15.0,Therefore we must insert a lambda if getting inference for a non-callable
v0.15.0,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.15.0,"try to get interval/std first if appropriate,"
v0.15.0,since we don't prefer a wrapped method with this name
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,
v0.15.0,This code contains snippets of code from:
v0.15.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.15.0,published under the following license and copyright:
v0.15.0,BSD 3-Clause License
v0.15.0,
v0.15.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0,All rights reserved.
v0.15.0,=============================================================================
v0.15.0,Types and constants
v0.15.0,=============================================================================
v0.15.0,=============================================================================
v0.15.0,Base GRF tree
v0.15.0,=============================================================================
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,=============================================================================
v0.15.0,A MultOutputWrapper for GRF classes
v0.15.0,=============================================================================
v0.15.0,=============================================================================
v0.15.0,Instantiations of Generalized Random Forest
v0.15.0,=============================================================================
v0.15.0,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.15.0,in front of the constant treatment is the intercept in the moment equation.
v0.15.0,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.15.0,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,
v0.15.0,This code contains snippets of code from
v0.15.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.15.0,published under the following license and copyright:
v0.15.0,BSD 3-Clause License
v0.15.0,
v0.15.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0,All rights reserved.
v0.15.0,=============================================================================
v0.15.0,Base Generalized Random Forest
v0.15.0,=============================================================================
v0.15.0,TODO: support freq_weight and sample_var
v0.15.0,Remap output
v0.15.0,reshape is necessary to preserve the data contiguity against vs
v0.15.0,"[:, np.newaxis] that does not."
v0.15.0,reshape is necessary to preserve the data contiguity against vs
v0.15.0,"[:, np.newaxis] that does not."
v0.15.0,Get subsample sample size
v0.15.0,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.15.0,We calculate the min eigenvalue proxy that each criterion is considering
v0.15.0,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.15.0,Check parameters
v0.15.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.15.0,if this is the first `fit` call of the warm start mode.
v0.15.0,"Free allocated memory, if any"
v0.15.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.15.0,We draw from the random state to get the random state we
v0.15.0,would have got if we hadn't used a warm_start.
v0.15.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.15.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.15.0,but would still advance randomness enough so that tree subsamples will be different.
v0.15.0,Generating indices a priori before parallelism ended up being orders of magnitude
v0.15.0,faster than how sklearn does it. The reason is that random samplers do not release the
v0.15.0,gil it seems.
v0.15.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.15.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.15.0,but would still advance randomness enough so that tree subsamples will be different.
v0.15.0,Parallel loop: we prefer the threading backend as the Cython code
v0.15.0,for fitting the trees is internally releasing the Python GIL
v0.15.0,making threading more efficient than multiprocessing in
v0.15.0,"that case. However, for joblib 0.12+ we respect any"
v0.15.0,"parallel_backend contexts set at a higher level,"
v0.15.0,since correctness does not rely on using threads.
v0.15.0,Collect newly grown trees
v0.15.0,Check data
v0.15.0,Assign chunk of trees to jobs
v0.15.0,avoid storing the output of every estimator by summing them here
v0.15.0,Parallel loop
v0.15.0,Check data
v0.15.0,Assign chunk of trees to jobs
v0.15.0,Parallel loop
v0.15.0,Check data
v0.15.0,Assign chunk of trees to jobs
v0.15.0,Parallel loop
v0.15.0,####################
v0.15.0,Variance correction
v0.15.0,####################
v0.15.0,Subtract the average within bag variance. This ends up being equal to the
v0.15.0,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.15.0,The negative part is just sq_between.
v0.15.0,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.15.0,"The off diagonals we have no objective prior, so no correction is applied."
v0.15.0,Finally correcting the pred_cov or pred_var
v0.15.0,avoid storing the output of every estimator by summing them here
v0.15.0,Parallel loop
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,we have to filter the folds because they contain the indices in the original data not
v0.15.0,the indices in the period-filtered data
v0.15.0,translate the indices in a fold to the indices in the period-filtered data
v0.15.0,"if groups was [3,3,4,4,5,5,6,6,1,1,2,2,0,0] (the group ids can be in any order, but the"
v0.15.0,"time periods for each group should be contguous), and we had [10,11,0,1] as the indices in a fold"
v0.15.0,(so the fold is taking the entries corresponding to groups 2 and 3)
v0.15.0,"then group_period_filter(0) is [0,2,4,6,8,10,12] and gpf(1) is [1,3,5,7,9,11,13]"
v0.15.0,"so for period 1, the fold should be [10,0] => [5,0] (the indices that return 10 and 0 in the t=0 data)"
v0.15.0,"and for period 2, the fold should be [11,1] => [5,0] again (the indices that return 11,1 in the t=1 data)"
v0.15.0,filter to the indices for the time period
v0.15.0,"now find their index in the period-filtered data, which is always sorted"
v0.15.0,sanity check that the folds are the same no matter the time period
v0.15.0,TODO: update docs
v0.15.0,"NOTE: sample weight, sample var are not passed in"
v0.15.0,Compose final model
v0.15.0,Calculate auxiliary quantities
v0.15.0,X ⨂ T_res
v0.15.0,"sum(model_final.predict(X, T_res))"
v0.15.0,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.15.0,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
v0.15.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0,"we need to set the number of periods before calling super()._prefit, since that will generate the"
v0.15.0,"final and nuisance models, which need to have self._n_periods set"
v0.15.0,Set _d_t to effective number of treatments
v0.15.0,Required for bootstrap inference
v0.15.0,for each mc iteration
v0.15.0,for each model under cross fit setting
v0.15.0,Handles the corner case when X=None but featurizer might be not None
v0.15.0,Expand treatments for each time period
v0.15.0,NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
v0.15.0,attribute so that the trained featurizer will be passed through
v0.15.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.15.0,for internal use by the library
v0.15.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.0,We need to use the _ortho_learner's copy to retain the information from fitting
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,testing importances
v0.15.0,testing heterogeneity importances
v0.15.0,Testing that all parameters do what they are supposed to
v0.15.0,"testing predict, apply and decision path"
v0.15.0,test that the subsampling scheme past to the trees is correct
v0.15.0,The sample size is chosen in particular to test rounding based error when subsampling
v0.15.0,test that the estimator calcualtes var correctly
v0.15.0,test api
v0.15.0,test accuracy
v0.15.0,test the projection functionality of forests
v0.15.0,test that the estimator calcualtes var correctly
v0.15.0,test api
v0.15.0,test that the estimator calcualtes var correctly
v0.15.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.15.0,test that we raise errors in mishandled situations.
v0.15.0,test that the subsampling scheme past to the trees is correct
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
v0.15.0,omit the lalonde notebook
v0.15.0,make sure that coverage outputs reflect notebook contents
v0.15.0,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.15.0,creating notebooks that are annoying for our users to actually run themselves
v0.15.0,"remove added coverage cell, then decrement execution_count for other cells to account for it"
v0.15.0,create directory if necessary
v0.15.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.15.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.15.0,"prior to calling interpret, can't plot, render, etc."
v0.15.0,can interpret without uncertainty
v0.15.0,can't interpret with uncertainty if inference wasn't used during fit
v0.15.0,can interpret with uncertainty if we refit
v0.15.0,can interpret without uncertainty
v0.15.0,can't treat before interpreting
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,for is_discrete in [False]:
v0.15.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.0,ensure we can serialize the unfit estimator
v0.15.0,ensure we can pickle the fit estimator
v0.15.0,make sure we can call the marginal_effect and effect methods
v0.15.0,test const marginal inference
v0.15.0,test effect inference
v0.15.0,test marginal effect inference
v0.15.0,test coef__inference and intercept__inference
v0.15.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0,"make sure we can call effect with implied scalar treatments,"
v0.15.0,"no matter the dimensions of T, and also that we warn when there"
v0.15.0,are multiple treatments
v0.15.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0,No heterogeneity
v0.15.0,Define indices to test
v0.15.0,Heterogeneous effects
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,simple DGP only for illustration
v0.15.0,Define the treatment model neural network architecture
v0.15.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.15.0,"so the input shape is (d_z + d_x,)"
v0.15.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.15.0,add extra layers on top for the mixture density network
v0.15.0,Define the response model neural network architecture
v0.15.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.15.0,"so the input shape is (d_t + d_x,)"
v0.15.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.15.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.15.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.15.0,so that the same weights will be reused in each instantiation
v0.15.0,number of samples to use in second estimate of the response
v0.15.0,(to make loss estimate unbiased)
v0.15.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.15.0,do something with predictions...
v0.15.0,also test vector t and y
v0.15.0,simple DGP only for illustration
v0.15.0,Define the treatment model neural network architecture
v0.15.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.15.0,"so the input shape is (d_z + d_x,)"
v0.15.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.15.0,add extra layers on top for the mixture density network
v0.15.0,Define the response model neural network architecture
v0.15.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.15.0,"so the input shape is (d_t + d_x,)"
v0.15.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.15.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.15.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.15.0,so that the same weights will be reused in each instantiation
v0.15.0,number of samples to use in second estimate of the response
v0.15.0,(to make loss estimate unbiased)
v0.15.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.15.0,do something with predictions...
v0.15.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.15.0,test = True ensures we draw test set images
v0.15.0,test = True ensures we draw test set images
v0.15.0,re-draw to get new independent treatment and implied response
v0.15.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.15.0,above is necesary so that reduced form doesn't win
v0.15.0,covariates: time and emotion
v0.15.0,random instrument
v0.15.0,z -> price
v0.15.0,true observable demand function
v0.15.0,errors
v0.15.0,response
v0.15.0,test = True ensures we draw test set images
v0.15.0,test = True ensures we draw test set images
v0.15.0,re-draw to get new independent treatment and implied response
v0.15.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.15.0,above is necesary so that reduced form doesn't win
v0.15.0,covariates: time and emotion
v0.15.0,random instrument
v0.15.0,z -> price
v0.15.0,true observable demand function
v0.15.0,errors
v0.15.0,response
v0.15.0,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.15.0,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.15.0,For some reason this doesn't work at all when run against the CNTK backend...
v0.15.0,"model.compile('nadam', loss=lambda _,l:l)"
v0.15.0,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.15.0,generate a valiation set
v0.15.0,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.15.0,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,DGP constants
v0.15.0,Generate data
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.15.0,pass sample weight to final step of pipeline
v0.15.0,create data with missing values
v0.15.0,model that can handle missing values
v0.15.0,"test X, W only"
v0.15.0,test W only
v0.15.0,dowhy does not support missing values in X
v0.15.0,assert that fitting with missing values fails when allow_missing is False
v0.15.0,and that setting allow_missing after init still works
v0.15.0,assert that we fail with a value error when we pass missing X to a model that doesn't support it
v0.15.0,assert that fitting with missing values fails when allow_missing is False
v0.15.0,and that setting allow_missing after init still works
v0.15.0,metalearners don't support W
v0.15.0,metalearners do support missing values in X
v0.15.0,dowhy never supports missing values in X
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,identity featurization effect functions
v0.15.0,polynomial featurization effect functions
v0.15.0,1d polynomial featurization functions
v0.15.0,2d-to-1d featurization functions
v0.15.0,2d-to-1d vector featurization functions
v0.15.0,use LassoCV rather than also selecting over RandomForests to save time
v0.15.0,test that treatment names are assigned for the featurized treatment
v0.15.0,expected shapes
v0.15.0,check effects
v0.15.0,ate
v0.15.0,loose inference checks
v0.15.0,temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons
v0.15.0,effect inference
v0.15.0,marginal effect inference
v0.15.0,const marginal effect inference
v0.15.0,fit a dummy estimator first so the featurizer can be fit to the treatment
v0.15.0,edge case with transformer that only takes a vector treatment
v0.15.0,so far will always return None for cate_treatment_names
v0.15.0,assert proper handling of improper feature names passed to certain transformers
v0.15.0,"depending on sklearn version, bad feature names either throws error or only uses first relevant name"
v0.15.0,ensure alpha is passed
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,testing importances
v0.15.0,testing heterogeneity importances
v0.15.0,Testing that all parameters do what they are supposed to
v0.15.0,"testing predict, apply and decision path"
v0.15.0,initialize parameters
v0.15.0,initialize config wtih base config and overwite some values
v0.15.0,predict tree using config parameters and assert
v0.15.0,shape of trained tree is the same as y_test
v0.15.0,initialize config wtih base honest config and overwite some values
v0.15.0,predict tree using config parameters and assert
v0.15.0,shape of trained tree is the same as y_test
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.15.0,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.15.0,so we need to transpose the result
v0.15.0,1-d output
v0.15.0,2-d output
v0.15.0,Single dimensional output y
v0.15.0,compare with weight
v0.15.0,compare with weight
v0.15.0,compare with weight
v0.15.0,compare with weight
v0.15.0,Multi-dimensional output y
v0.15.0,1-d y
v0.15.0,compare when both sample_var and sample_weight exist
v0.15.0,multi-d y
v0.15.0,compare when both sample_var and sample_weight exist
v0.15.0,compare when both sample_var and sample_weight exist
v0.15.0,compare when both sample_var and sample_weight exist
v0.15.0,compare when both sample_var and sample_weight exist
v0.15.0,compare when both sample_var and sample_weight exist
v0.15.0,compare when both sample_var and sample_weight exist
v0.15.0,dgp
v0.15.0,StatsModels2SLS
v0.15.0,IV2SLS
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,test that we can fit with the same arguments as the base estimator
v0.15.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.15.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0,with the same shape for the lower and upper bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,test that we can do the same thing once we provide percentile bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.15.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0,with the same shape for the lower and upper bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,test that we can do the same thing once we provide percentile bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,test that we can fit with the same arguments as the base estimator
v0.15.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.15.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0,with the same shape for the lower and upper bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,test that we can do the same thing once we provide percentile bounds
v0.15.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.15.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0,with the same shape for the lower and upper bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,test that we can do the same thing once we provide percentile bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,test that we can fit with the same arguments as the base estimator
v0.15.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.15.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0,with the same shape for the lower and upper bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,test that we can do the same thing once we provide percentile bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.15.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0,with the same shape for the lower and upper bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,test that we can do the same thing once we provide percentile bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0,with the same shape for the lower and upper bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,test that the estimated effect is usually within the bounds
v0.15.0,test that we can do the same thing once we provide alpha explicitly
v0.15.0,test that the lower and upper bounds differ
v0.15.0,test that the estimated effect is usually within the bounds
v0.15.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0,with the same shape for the lower and upper bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,TODO: test that the estimated effect is usually within the bounds
v0.15.0,and that the true effect is also usually within the bounds
v0.15.0,test that we can do the same thing once we provide percentile bounds
v0.15.0,test that the lower and upper bounds differ
v0.15.0,TODO: test that the estimated effect is usually within the bounds
v0.15.0,and that the true effect is also usually within the bounds
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,fixed functions as first stage models
v0.15.0,they can be anything as long as fitting doesn't modify the predictions
v0.15.0,"that way, it doesn't matter if they are trained on different subsets of the data"
v0.15.0,all estimators must have opted in to federation
v0.15.0,all estimators must have the same covariance type
v0.15.0,test coefficients
v0.15.0,test effects
v0.15.0,fixed functions as first stage models
v0.15.0,they can be anything as long as fitting doesn't modify the predictions
v0.15.0,"that way, it doesn't matter if they are trained on different subsets of the data"
v0.15.0,test coefficients
v0.15.0,test effects
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,test that the subsampling scheme past to the trees is correct
v0.15.0,test that the estimator calcualtes var correctly
v0.15.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.15.0,test that we raise errors in mishandled situations.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,DGP constants
v0.15.0,Generate data
v0.15.0,Test inference results when `cate_feature_names` doesn not exist
v0.15.0,Test inference results when `cate_feature_names` doesn not exist
v0.15.0,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.15.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.15.0,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.15.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.15.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.15.0,pvalue for second column should be greater than zero since some points are on either side
v0.15.0,of the tested value
v0.15.0,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.15.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.15.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.15.0,ensure alpha is passed
v0.15.0,only is not None when T1 is a constant or a list of constant
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Generate synthetic data
v0.15.0,Run _crossfit with Ray enabled
v0.15.0,Run _crossfit without Ray
v0.15.0,Compare the results
v0.15.0,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.15.0,Test non keyword based calls to fit
v0.15.0,test non-array inputs
v0.15.0,Test custom splitter
v0.15.0,Test incomplete set of test folds
v0.15.0,"y scores should be positive, since W predicts Y somewhat"
v0.15.0,"t scores might not be, since W and T are uncorrelated"
v0.15.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,make sure cross product varies more slowly with first array
v0.15.0,and that vectors are okay as inputs
v0.15.0,number of inputs in specification must match number of inputs
v0.15.0,must have an output
v0.15.0,output indices must be unique
v0.15.0,output indices must be present in an input
v0.15.0,number of indices must match number of dimensions for each input
v0.15.0,repeated indices must always have consistent sizes
v0.15.0,transpose
v0.15.0,tensordot
v0.15.0,trace
v0.15.0,TODO: set up proper flag for this
v0.15.0,pick indices at random with replacement from the first 7 letters of the alphabet
v0.15.0,"of all of the distinct indices that appear in any input,"
v0.15.0,pick a random subset of them (of size at most 5) to appear in the output
v0.15.0,creating an instance should warn
v0.15.0,using the instance should not warn
v0.15.0,using the deprecated method should warn
v0.15.0,don't warn if b and c are passed by keyword
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,make any access to matplotlib or plt throw an exception
v0.15.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.0,heterogeneity
v0.15.0,Invert indices to match latest API
v0.15.0,Invert indices to match latest API
v0.15.0,The feature for heterogeneity stays constant
v0.15.0,Auxiliary function for adding xticks and vertical lines when plotting results
v0.15.0,for dynamic dml vs ground truth parameters.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,tests that we can recover the right degree of polynomial features
v0.15.0,implicitly also tests ability to handle pipelines
v0.15.0,since 'poly' uses pipelines containing PolynomialFeatures
v0.15.0,"generate larger coefficients in a set of high degree features,"
v0.15.0,weighted towards higher degree features
v0.15.0,"just test a polynomial T model, since for Y the correct degree also depends on"
v0.15.0,the interation of T and X
v0.15.0,test corner case with just one model in a list
v0.15.0,test corner case with empty list
v0.15.0,test selecting between two fixed models
v0.15.0,"DGP is a linear model, so linear regression should fit better"
v0.15.0,"DGP is now non-linear, so random forest should fit better"
v0.15.0,these models only work on multi-output data
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Preprocess data
v0.15.0,Convert 'week' to a date
v0.15.0,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.15.0,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.15.0,Take log of price
v0.15.0,Make brand numeric
v0.15.0,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.15.0,which have all zero coeffs
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,test at least one estimator from each category
v0.15.0,test causal graph
v0.15.0,need to set matplotlib backend before viewing model
v0.15.0,test refutation estimate
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.15.0,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.15.0,TODO: test something rather than just print...
v0.15.0,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.15.0,pick some arbitrary X
v0.15.0,pick some arbitrary T
v0.15.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.15.0,Generate random Xs
v0.15.0,Random covariance matrix of Xs
v0.15.0,Effect of Xs on outcome
v0.15.0,Effect of treatment on outcomes
v0.15.0,Effect of treatment on outcome conditional on X1
v0.15.0,Generate treatments based on X and random noise
v0.15.0,"Generate Y (based on X, D, and random noise)"
v0.15.0,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.0,test the DR outcome difference
v0.15.0,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.0,test the DR outcome difference
v0.15.0,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.0,test the DR outcome difference
v0.15.0,use evaluate_blp to fit on validation only
v0.15.0,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.0,test the DR outcome difference
v0.15.0,fit nothing
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,accuracy test
v0.15.0,"accuracy test, DML"
v0.15.0,uncomment when issue #837 is resolved
v0.15.0,"NonParamDMLIV(discrete_outcome=discrete_outcome, discrete_treatment=discrete_treatment,"
v0.15.0,"discrete_instrument=discrete_instrument, model_final=LinearRegression())"
v0.15.0,make sure the auto outcome model is a classifier
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.15.0,The average variance should be lower when using monte carlo iterations
v0.15.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.15.0,The average variance should be lower when using monte carlo iterations
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"since we're running so many combinations, just use LassoCV/LogisticRegressionCV"
v0.15.0,for the models instead of also selecting over random forest models
v0.15.0,ensure we can serialize unfit estimator
v0.15.0,ensure we can serialize fit estimator
v0.15.0,expected effect size
v0.15.0,test effect
v0.15.0,test inference
v0.15.0,only OrthoIV support inference other than bootstrap
v0.15.0,test summary
v0.15.0,test can run score
v0.15.0,test cate_feature_names
v0.15.0,test can run shap values
v0.15.0,dgp
v0.15.0,no heterogeneity
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"if we aren't fitting on the whole dataset, ensure that the limits are respected"
v0.15.0,ensure that the grouping has worked correctly and we get exactly the number of copies
v0.15.0,of the items in whichever groups we see
v0.15.0,DML nested CV works via a 'cv' attribute
v0.15.0,"want to validate the nested grouping, not the outer grouping in the nesting tests"
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,parameter combinations to test
v0.15.0,"we're running a lot of tests, so use fixed models instead of model selection"
v0.15.0,"IntentToTreatDRIV only supports binary treatments and instruments, and doesn't support fit_cov_directly"
v0.15.0,TODO: serializing/deserializing for every combination -- is this necessary?
v0.15.0,ensure we can serialize unfit estimator
v0.15.0,ensure we can serialize fit estimator
v0.15.0,expected effect size
v0.15.0,assert calculated constant marginal effect shape is expected
v0.15.0,const_marginal effect is defined in LinearCateEstimator class
v0.15.0,assert calculated marginal effect shape is expected
v0.15.0,test inference
v0.15.0,test can run score
v0.15.0,test cate_feature_names
v0.15.0,test can run shap values
v0.15.0,"dgp (binary T, binary Z)"
v0.15.0,no heterogeneity
v0.15.0,with heterogeneity
v0.15.0,fitting the covariance directly should be at least as good as computing the covariance from separate models
v0.15.0,set the models so that model selection over random forests doesn't take too much time in the repeated trials
v0.15.0,directly fitting the covariance should be better than indirectly fitting it
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Set random seed
v0.15.0,Generate data
v0.15.0,DGP constants
v0.15.0,Test data
v0.15.0,Constant treatment effect
v0.15.0,Constant treatment with multi output Y
v0.15.0,Heterogeneous treatment
v0.15.0,Heterogeneous treatment with multi output Y
v0.15.0,TLearner test
v0.15.0,Instantiate TLearner
v0.15.0,Test inputs
v0.15.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.0,Instantiate SLearner
v0.15.0,Test inputs
v0.15.0,Test constant treatment effect
v0.15.0,Test constant treatment effect with multi output Y
v0.15.0,Test heterogeneous treatment effect
v0.15.0,Need interactions between T and features
v0.15.0,Test heterogeneous treatment effect with multi output Y
v0.15.0,Instantiate XLearner
v0.15.0,Test inputs
v0.15.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.0,Instantiate DomainAdaptationLearner
v0.15.0,Test inputs
v0.15.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.0,Get the true treatment effect
v0.15.0,Get the true treatment effect
v0.15.0,Fit learner and get the effect and marginal effect
v0.15.0,Compute treatment effect residuals (absolute)
v0.15.0,Check that at least 90% of predictions are within tolerance interval
v0.15.0,Check whether the output shape is right
v0.15.0,Check that one can pass in regular lists
v0.15.0,Check that it fails correctly if lists of different shape are passed in
v0.15.0,"Check that it works when T, Y have shape (n, 1)"
v0.15.0,Generate covariates
v0.15.0,Generate treatment
v0.15.0,Calculate outcome
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,DGP constants
v0.15.0,Generate data
v0.15.0,Test data
v0.15.0,Remove warnings that might be raised by the models passed into the ORF
v0.15.0,Generate data with continuous treatments
v0.15.0,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.15.0,does not work well with parallelism.
v0.15.0,Test inputs for continuous treatments
v0.15.0,--> Check that one can pass in regular lists
v0.15.0,--> Check that it fails correctly if lists of different shape are passed in
v0.15.0,Check that outputs have the correct shape
v0.15.0,Test continuous treatments with controls
v0.15.0,Test continuous treatments without controls
v0.15.0,Generate data with binary treatments
v0.15.0,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.15.0,does not work well with parallelism.
v0.15.0,Test inputs for binary treatments
v0.15.0,--> Check that one can pass in regular lists
v0.15.0,--> Check that it fails correctly if lists of different shape are passed in
v0.15.0,"--> Check that it works when T, Y have shape (n, 1)"
v0.15.0,"--> Check that it fails correctly when T has shape (n, 2)"
v0.15.0,--> Check that it fails correctly when the treatments are not numeric
v0.15.0,Check that outputs have the correct shape
v0.15.0,Test binary treatments with controls
v0.15.0,Test binary treatments without controls
v0.15.0,Only applicable to continuous treatments
v0.15.0,Generate data for 2 treatments
v0.15.0,Test multiple treatments with controls
v0.15.0,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.15.0,The rest for controls. Just as an example.
v0.15.0,Generating A/B test data
v0.15.0,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.15.0,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.15.0,Create a wrapper around Lasso that doesn't support weights
v0.15.0,since Lasso does natively support them starting in sklearn 0.23
v0.15.0,Generate data with continuous treatments
v0.15.0,Instantiate model with most of the default parameters
v0.15.0,Compute the treatment effect on test points
v0.15.0,Compute treatment effect residuals
v0.15.0,Multiple treatments
v0.15.0,Allow at most 10% test points to be outside of the tolerance interval
v0.15.0,Compute treatment effect residuals
v0.15.0,Multiple treatments
v0.15.0,Allow at most 20% test points to be outside of the confidence interval
v0.15.0,Check that the intervals are not too wide
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.15.0,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.15.0,ensure that we've got at least 6 of every element
v0.15.0,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.15.0,NOTE: this number may need to change if the default number of folds in
v0.15.0,WeightedStratifiedKFold changes
v0.15.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.0,ensure we can serialize the unfit estimator
v0.15.0,ensure we can pickle the fit estimator
v0.15.0,make sure we can call the marginal_effect and effect methods
v0.15.0,test const marginal inference
v0.15.0,test effect inference
v0.15.0,test marginal effect inference
v0.15.0,test coef__inference and intercept__inference
v0.15.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0,"make sure we can call effect with implied scalar treatments,"
v0.15.0,"no matter the dimensions of T, and also that we warn when there"
v0.15.0,are multiple treatments
v0.15.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0,ensure that we've got at least two of every element
v0.15.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.0,make sure we can call the marginal_effect and effect methods
v0.15.0,test const marginal inference
v0.15.0,test effect inference
v0.15.0,test marginal effect inference
v0.15.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.15.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.15.0,We concatenate the two copies data
v0.15.0,make sure we can get out post-fit stuff
v0.15.0,create a simple artificial setup where effect of moving from treatment
v0.15.0,"1 -> 2 is 2,"
v0.15.0,"1 -> 3 is 1, and"
v0.15.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.15.0,"Using an uneven number of examples from different classes,"
v0.15.0,"and having the treatments in non-lexicographic order,"
v0.15.0,Should rule out some basic issues.
v0.15.0,test that we can fit with a KFold instance
v0.15.0,test that we can fit with a train/test iterable
v0.15.0,predetermined splits ensure that all features are seen in each split
v0.15.0,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.15.0,(incorrectly) use a final model with an intercept
v0.15.0,"Because final model is fixed, actual values of T and Y don't matter"
v0.15.0,Ensure reproducibility
v0.15.0,Sparse DGP
v0.15.0,Treatment effect coef
v0.15.0,Other coefs
v0.15.0,Features and controls
v0.15.0,Test sparse estimator
v0.15.0,"--> test coef_, intercept_"
v0.15.0,"with this DGP, since T depends linearly on X, Y depends on X quadratically"
v0.15.0,so we should use a quadratic featurizer
v0.15.0,--> test treatment effects
v0.15.0,Restrict x_test to vectors of norm < 1
v0.15.0,--> check inference
v0.15.0,Check that a majority of true effects lie in the 5-95% CI
v0.15.0,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.15.0,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.15.0,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.15.0,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.15.0,sparse test case: heterogeneous effect by product
v0.15.0,need at least as many rows in e_y as there are distinct columns
v0.15.0,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.15.0,"we have quadratic terms in y, so we need to pipeline with a quadratic featurizer"
v0.15.0,Compare results with and without Ray
v0.15.0,create a simple artificial setup where effect of moving from treatment
v0.15.0,"a -> b is 2,"
v0.15.0,"a -> c is 1, and"
v0.15.0,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.15.0,"Using an uneven number of examples from different classes,"
v0.15.0,"and having the treatments in non-lexicographic order,"
v0.15.0,should rule out some basic issues.
v0.15.0,Note that explicitly specifying the dtype as object is necessary until
v0.15.0,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.15.0,estimated effects should be identical when treatment is explicitly given
v0.15.0,but const_marginal_effect should be reordered based on the explicit cagetories
v0.15.0,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.15.0,test outer grouping
v0.15.0,"with 2 folds, we should get exactly 3 groups per split, each with 10 copies of the y or t value"
v0.15.0,test nested grouping
v0.15.0,"with 2-fold outer and 2-fold inner grouping, and six total groups,"
v0.15.0,should get 1 or 2 groups per split
v0.15.0,"Try default, integer, and new user-passed treatment name"
v0.15.0,FunctionTransformers are agnostic to passed treatment names
v0.15.0,Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Set random seed
v0.15.0,Generate data
v0.15.0,DGP constants
v0.15.0,Test data
v0.15.0,Constant treatment effect and propensity
v0.15.0,Heterogeneous treatment and propensity
v0.15.0,ensure that we've got at least two of every element
v0.15.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.0,ensure that we can serialize unfit estimator
v0.15.0,ensure that we can serialize fit estimator
v0.15.0,make sure we can call the marginal_effect and effect methods
v0.15.0,test const marginal inference
v0.15.0,test effect inference
v0.15.0,test marginal effect inference
v0.15.0,test coef_ and intercept_ inference
v0.15.0,verify we can generate the summary
v0.15.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.15.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.15.0,create a simple artificial setup where effect of moving from treatment
v0.15.0,"1 -> 2 is 2,"
v0.15.0,"1 -> 3 is 1, and"
v0.15.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.15.0,"Using an uneven number of examples from different classes,"
v0.15.0,"and having the treatments in non-lexicographic order,"
v0.15.0,Should rule out some basic issues.
v0.15.0,test that we can fit with a KFold instance
v0.15.0,test that we can fit with a train/test iterable
v0.15.0,"for at least some of the examples, the CI should have nonzero width"
v0.15.0,"for at least some of the examples, the CI should have nonzero width"
v0.15.0,"for at least some of the examples, the CI should have nonzero width"
v0.15.0,test coef__inference function works
v0.15.0,test intercept__inference function works
v0.15.0,test summary function works
v0.15.0,Test inputs
v0.15.0,self._test_inputs(DR_learner)
v0.15.0,Test constant treatment effect
v0.15.0,Test heterogeneous treatment effect
v0.15.0,Test heterogenous treatment effect for W =/= None
v0.15.0,Sparse DGP
v0.15.0,Treatment effect coef
v0.15.0,Other coefs
v0.15.0,Features and controls
v0.15.0,Test sparse estimator
v0.15.0,"--> test coef_, intercept_"
v0.15.0,--> test treatment effects
v0.15.0,Restrict x_test to vectors of norm < 1
v0.15.0,--> check inference
v0.15.0,Check that a majority of true effects lie in the 5-95% CI
v0.15.0,test outer grouping
v0.15.0,NOTE: StratifiedGroupKFold has a bug when shuffle is True where it doesn't always stratify properly
v0.15.0,so we explicitly pass a StratifiedGroupKFold with shuffle=False (the default) rather than letting
v0.15.0,cross-fit generate one
v0.15.0,"with 2-fold grouping, we should get exactly 3 groups per split"
v0.15.0,test nested grouping
v0.15.0,"with 2-fold outer and 2-fold inner grouping, we should get 1-2 groups per split"
v0.15.0,helper class
v0.15.0,Fit learner and get the effect
v0.15.0,Get the true treatment effect
v0.15.0,Compute treatment effect residuals (absolute)
v0.15.0,Check that at least 90% of predictions are within tolerance interval
v0.15.0,Only for heterogeneous TE
v0.15.0,Fit learner on X and W and get the effect
v0.15.0,Get the true treatment effect
v0.15.0,Compute treatment effect residuals (absolute)
v0.15.0,Check that at least 90% of predictions are within tolerance interval
v0.15.0,Check that one can pass in regular lists
v0.15.0,Check that it fails correctly if lists of different shape are passed in
v0.15.0,Check that it fails when T contains values other than 0 and 1
v0.15.0,"Check that it works when T, Y have shape (n, 1)"
v0.15.0,Generate covariates
v0.15.0,Generate treatment
v0.15.0,Calculate outcome
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,DGP constants
v0.15.0,DGP coefficients
v0.15.0,Generated outcomes
v0.15.0,################
v0.15.0,WeightedLasso #
v0.15.0,################
v0.15.0,Define weights
v0.15.0,Define extended datasets
v0.15.0,Range of alphas
v0.15.0,Compare with Lasso
v0.15.0,--> No intercept
v0.15.0,--> With intercept
v0.15.0,When DGP has no intercept
v0.15.0,When DGP has intercept
v0.15.0,--> Coerce coefficients to be positive
v0.15.0,--> Toggle max_iter & tol
v0.15.0,Define weights
v0.15.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.15.0,Mixed DGP scenario.
v0.15.0,Define extended datasets
v0.15.0,Define weights
v0.15.0,Define multioutput
v0.15.0,##################
v0.15.0,WeightedLassoCV #
v0.15.0,##################
v0.15.0,Define alphas to test
v0.15.0,Compare with LassoCV
v0.15.0,--> No intercept
v0.15.0,--> With intercept
v0.15.0,--> Force parameters to be positive
v0.15.0,Choose a smaller n to speed-up process
v0.15.0,Compare fold weights
v0.15.0,Define weights
v0.15.0,Define extended datasets
v0.15.0,Define splitters
v0.15.0,WeightedKFold splitter
v0.15.0,Map weighted splitter to an extended splitter
v0.15.0,Define alphas to test
v0.15.0,Compare with LassoCV
v0.15.0,--> No intercept
v0.15.0,--> With intercept
v0.15.0,--> Force parameters to be positive
v0.15.0,###########################
v0.15.0,MultiTaskWeightedLassoCV #
v0.15.0,###########################
v0.15.0,Define alphas to test
v0.15.0,Define splitter
v0.15.0,Compare with MultiTaskLassoCV
v0.15.0,--> No intercept
v0.15.0,--> With intercept
v0.15.0,Define weights
v0.15.0,Define extended datasets
v0.15.0,Define splitters
v0.15.0,WeightedKFold splitter
v0.15.0,Map weighted splitter to an extended splitter
v0.15.0,Define alphas to test
v0.15.0,Compare with LassoCV
v0.15.0,--> No intercept
v0.15.0,--> With intercept
v0.15.0,#########################
v0.15.0,WeightedLassoCVWrapper #
v0.15.0,#########################
v0.15.0,perform 1D fit
v0.15.0,perform 2D fit
v0.15.0,################
v0.15.0,DebiasedLasso #
v0.15.0,################
v0.15.0,Test DebiasedLasso without weights
v0.15.0,--> Check debiased coeffcients without intercept
v0.15.0,--> Check debiased coeffcients with intercept
v0.15.0,--> Check 5-95 CI coverage for unit vectors
v0.15.0,Test DebiasedLasso with weights for one DGP
v0.15.0,Define weights
v0.15.0,Define extended datasets
v0.15.0,--> Check debiased coefficients
v0.15.0,Define weights
v0.15.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.15.0,--> Check debiased coeffcients
v0.15.0,Test that attributes propagate correctly
v0.15.0,Test MultiOutputDebiasedLasso without weights
v0.15.0,--> Check debiased coeffcients without intercept
v0.15.0,--> Check debiased coeffcients with intercept
v0.15.0,--> Check CI coverage
v0.15.0,Test MultiOutputDebiasedLasso with weights
v0.15.0,Define weights
v0.15.0,Define extended datasets
v0.15.0,--> Check debiased coefficients
v0.15.0,Unit vectors
v0.15.0,Unit vectors
v0.15.0,Check coeffcients and intercept are the same within tolerance
v0.15.0,Check results are similar with tolerance 1e-6
v0.15.0,Check if multitask
v0.15.0,Check that same alpha is chosen
v0.15.0,Check that the coefficients are similar
v0.15.0,selective ridge has a simple implementation that we can test against
v0.15.0,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.15.0,"it should be the case that when we set fit_intercept to true,"
v0.15.0,it doesn't matter whether the penalized model also fits an intercept or not
v0.15.0,create an extra copy of rows with weight 2
v0.15.0,"instead of a slice, explicitly return an array of indices"
v0.15.0,_penalized_inds is only set during fitting
v0.15.0,cv exists on penalized model
v0.15.0,now we can access _penalized_inds
v0.15.0,check that we can read the cv attribute back out from the underlying model
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"global and cohort data should have exactly the same structure, but different values"
v0.15.0,local index should have as many times entries as global as there were rows passed in
v0.15.0,continuous treatments have typical treatment values equal to
v0.15.0,the mean of the absolute value of non-zero entries
v0.15.0,discrete treatments have typical treatment value 1
v0.15.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.0,policy value should exceed always treating with any treatment
v0.15.0,"global shape is (d_y, sum(d_t))"
v0.15.0,global and cohort row-wise dicts have d_y * d_t entries
v0.15.0,local dictionary is flattened to n_rows * d_y * d_t
v0.15.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0,features; for categoricals they should appear #cats-1 times each
v0.15.0,"global and cohort data should have exactly the same structure, but different values"
v0.15.0,local index should have as many times entries as global as there were rows passed in
v0.15.0,features; for categoricals they should appear #cats-1 times each
v0.15.0,"global shape is (d_y, sum(d_t))"
v0.15.0,global and cohort row-wise dicts have d_y * d_t entries
v0.15.0,local dictionary is flattened to n_rows * d_y * d_t
v0.15.0,continuous treatments have typical treatment values equal to
v0.15.0,the mean of the absolute value of non-zero entries
v0.15.0,discrete treatments have typical treatment value 1
v0.15.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.0,policy value should exceed always treating with any treatment
v0.15.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0,"global and cohort data should have exactly the same structure, but different values"
v0.15.0,local index should have as many times entries as global as there were rows passed in
v0.15.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.0,policy value should exceed always treating with any treatment
v0.15.0,"global shape is (d_y, sum(d_t))"
v0.15.0,global and cohort row-wise dicts have d_y * d_t entries
v0.15.0,local dictionary is flattened to n_rows * d_y * d_t
v0.15.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0,features; for categoricals they should appear #cats-1 times each
v0.15.0,make sure we don't run into problems dropping every index
v0.15.0,"global and cohort data should have exactly the same structure, but different values"
v0.15.0,local index should have as many times entries as global as there were rows passed in
v0.15.0,"global shape is (d_y, sum(d_t))"
v0.15.0,global and cohort row-wise dicts have d_y * d_t entries
v0.15.0,local dictionary is flattened to n_rows * d_y * d_t
v0.15.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.0,policy value should exceed always treating with any treatment
v0.15.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0,"global and cohort data should have exactly the same structure, but different values"
v0.15.0,local index should have as many times entries as global as there were rows passed in
v0.15.0,features; for categoricals they should appear #cats-1 times each
v0.15.0,"global shape is (d_y, sum(d_t))"
v0.15.0,global and cohort row-wise dicts have d_y * d_t entries
v0.15.0,local dictionary is flattened to n_rows * d_y * d_t
v0.15.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.0,policy value should exceed always treating with any treatment
v0.15.0,dgp
v0.15.0,model
v0.15.0,model
v0.15.0,"columns 'd', 'e', 'h' have too many values"
v0.15.0,"columns 'd', 'e' have too many values"
v0.15.0,lowering bound shouldn't affect already fit columns when warm starting
v0.15.0,"column d is now okay, too"
v0.15.0,verify that we can use a scalar treatment cost
v0.15.0,verify that we can specify per-treatment costs for each sample
v0.15.0,verify that using the same state returns the same results each time
v0.15.0,set the categories for column 'd' explicitly so that b is default
v0.15.0,"first column: 10 ones, this is fine"
v0.15.0,"second column: 6 categories, plenty of random instances of each"
v0.15.0,this is fine only if we increase the cateogry limit
v0.15.0,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.15.0,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.15.0,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.15.0,forest heterogeneity won't work
v0.15.0,"sixth column: just 1 one, not enough even without check"
v0.15.0,increase bound on cat expansion
v0.15.0,skip checks (reducing folds accordingly)
v0.15.0,"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
v0.15.0,"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
v0.15.0,Pass an example where W is irrelevant and X is confounder
v0.15.0,"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
v0.15.0,zeroed out and the test will fail
v0.15.0,"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
v0.15.0,rescaling X shouldn't affect the first stage models because they normalize the inputs
v0.15.0,"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
v0.15.0,cross terms
v0.15.0,scale by 1000 to match the input to this model:
v0.15.0,"the scale of X does matter for the final model, which keeps results in user-denominated units"
v0.15.0,rescaling X still shouldn't affect the first stage models
v0.15.0,TODO: we don't recover the correct values with enough accuracy to enable this assertion
v0.15.0,is there a different way to verify that we are learning the correct coefficients?
v0.15.0,"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,DGP constants
v0.15.0,Define data features
v0.15.0,Added `_df`to names to be different from the default cate_estimator names
v0.15.0,Generate data
v0.15.0,################################
v0.15.0,Single treatment and outcome #
v0.15.0,################################
v0.15.0,Test LinearDML
v0.15.0,|--> Test featurizers
v0.15.0,"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
v0.15.0,|--> Test re-fit
v0.15.0,Test SparseLinearDML
v0.15.0,Test ForestDML
v0.15.0,###################################
v0.15.0,Mutiple treatments and outcomes #
v0.15.0,###################################
v0.15.0,Test LinearDML
v0.15.0,Test SparseLinearDML
v0.15.0,"Single outcome only, ORF does not support multiple outcomes"
v0.15.0,Test DMLOrthoForest
v0.15.0,Test DROrthoForest
v0.15.0,Test XLearner
v0.15.0,Skipping population summary names test because bootstrap inference is too slow
v0.15.0,Test SLearner
v0.15.0,Test TLearner
v0.15.0,Test LinearDRLearner
v0.15.0,Test SparseLinearDRLearner
v0.15.0,Test ForestDRLearner
v0.15.0,Test LinearIntentToTreatDRIV
v0.15.0,Test DeepIV
v0.15.0,Test categorical treatments
v0.15.0,Check refit
v0.15.0,Check refit after setting categories
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Linear models are required for parametric dml
v0.15.0,sample weighting models are required for nonparametric dml
v0.15.0,Test values
v0.15.0,TLearner test
v0.15.0,Instantiate TLearner
v0.15.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.0,Test constant treatment effect with multi output Y
v0.15.0,Test heterogeneous treatment effect
v0.15.0,Need interactions between T and features
v0.15.0,Test heterogeneous treatment effect with multi output Y
v0.15.0,Instantiate DomainAdaptationLearner
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,test base values equals to mean of constant marginal effect
v0.15.0,test shape of shap values output is as expected
v0.15.0,test shape of attribute of explanation object is as expected
v0.15.0,test length of feature names equals to shap values shape
v0.15.0,test base values equals to mean of constant marginal effect
v0.15.0,test shape of shap values output is as expected
v0.15.0,test shape of attribute of explanation object is as expected
v0.15.0,test length of feature names equals to shap values shape
v0.15.0,import here since otherwise test collection would fail if matplotlib is not installed
v0.15.0,Treatment effect function
v0.15.0,Outcome support
v0.15.0,Treatment support
v0.15.0,"Generate controls, covariates, treatments and outcomes"
v0.15.0,Heterogeneous treatment effects
v0.15.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.15.0,through shap package.
v0.15.0,test shap could generate the plot from the shap_values
v0.15.0,"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Check inputs
v0.15.0,Check inputs
v0.15.0,Check inputs
v0.15.0,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.15.0,"Thus, we append the controls column before the one-hot-encoded T"
v0.15.0,"We might want to revisit, though, since it's linearly determined by the others"
v0.15.0,Check inputs
v0.15.0,Check inputs
v0.15.0,Estimate response function
v0.15.0,Check inputs
v0.15.0,Train model on controls. Assign higher weight to units resembling
v0.15.0,treated units.
v0.15.0,Train model on the treated. Assign higher weight to units resembling
v0.15.0,control units.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,TODO: make sure to use random seeds wherever necessary
v0.15.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.15.0,"unfortunately with the Theano and Tensorflow backends,"
v0.15.0,the straightforward use of K.stop_gradient can cause an error
v0.15.0,because the parameters of the intermediate layers are now disconnected from the loss;
v0.15.0,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.15.0,so that those layers remain connected but with 0 gradient
v0.15.0,|| t - mu_i || ^2
v0.15.0,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.15.0,Use logsumexp for numeric stability:
v0.15.0,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.15.0,TODO: does the numeric stability actually make any difference?
v0.15.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.15.0,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.15.0,generate cumulative sum via matrix multiplication
v0.15.0,"Generate standard uniform values in shape (batch_size,1)"
v0.15.0,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.15.0,we use uniform_like instead with an input of an appropriate shape)
v0.15.0,convert to floats and multiply to perform equivalent of logical AND
v0.15.0,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.15.0,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.15.0,we use normal_like instead with an input of an appropriate shape)
v0.15.0,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.15.0,prevent gradient from passing through sampling
v0.15.0,three options: biased or upper-bound loss require a single number of samples;
v0.15.0,unbiased can take different numbers for the network and its gradient
v0.15.0,"sample: (() -> Layer, int) -> Layer"
v0.15.0,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.15.0,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.15.0,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.15.0,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.15.0,the dimensionality of the output of the network
v0.15.0,TODO: is there a more robust way to do this?
v0.15.0,TODO: do we need to give the user more control over other arguments to fit?
v0.15.0,"subtle point: we need to build a new model each time,"
v0.15.0,because each model encapsulates its randomness
v0.15.0,TODO: do we need to give the user more control over other arguments to fit?
v0.15.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.15.0,not a general tensor (because of how backprop works in every framework)
v0.15.0,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.15.0,but this seems annoying...)
v0.15.0,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.15.0,TODO: any way to get this to work on batches of arbitrary size?
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.15.0,TODO: prel_model_effect could allow sample_var and freq_weight?
v0.15.0,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.15.0,"we need to undo the one-hot encoding for calling effect,"
v0.15.0,since it expects raw values
v0.15.0,"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
v0.15.0,reshape the predictions
v0.15.0,concat W and Z
v0.15.0,check nuisances outcome shape
v0.15.0,Y_res could be a vector or 1-dimensional 2d-array
v0.15.0,"We're projecting, so we're treating E[T|X,Z] as the instrument (ignoring W for simplicity)"
v0.15.0,"Then beta(X) = E[T̃ (E[T|X,Z]-E[E[T|X,Z]|X)|X] and we can apply the tower rule several times to get"
v0.15.0,"= E[(E[T|X,Z]-E[T|X])^2|X]"
v0.15.0,"and also     = E[(E[T|X,Z]-T)^2|X]"
v0.15.0,so we can compute it either from (T_proj-T_pred)^2 or from (T_proj-T)^2
v0.15.0,The first of these is just Z_res^2
v0.15.0,"fit on T*T_proj, covariance will be computed by E[T_res * T_proj] = E[T*T_proj] - E[T]^2"
v0.15.0,"return shape (n,)"
v0.15.0,we will fit on the covariance (T_res*Z_res) directly
v0.15.0,"fit on TZ, covariance will be computed by E[T_res * Z_res] = TZ_pred - T_pred * Z_pred"
v0.15.0,"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
v0.15.0,"shape (n,)"
v0.15.0,"shape (n,)"
v0.15.0,"shape(n,)"
v0.15.0,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.15.0,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.15.0,"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
v0.15.0,reshape the predictions
v0.15.0,"in the projection case, this is a variance and should always be non-negative"
v0.15.0,check nuisances outcome shape
v0.15.0,"all could be reshaped to vector since Y, T, Z are all single dimensional."
v0.15.0,Estimate final model of theta(X) by minimizing the square loss:
v0.15.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.15.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.15.0,at the expense of some small bias. For points with very small covariance we revert
v0.15.0,to the model-based preliminary estimate and do not add the correction term.
v0.15.0,A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
v0.15.0,Used by both DRIV and IntentToTreatDRIV.
v0.15.0,Maggie: I think that would be the case?
v0.15.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.15.0,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.15.0,attribute so that the trained featurizer will be passed through
v0.15.0,Handles the corner case when X=None but featurizer might be not None
v0.15.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.0,"this is a regression model since the instrument E[T|X,W,Z] is always continuous"
v0.15.0,"we're using E[T|X,W,Z] as the instrument"
v0.15.0,Define the data generation functions
v0.15.0,Define the data generation functions
v0.15.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.0,Define the data generation functions
v0.15.0,TODO: support freq_weight and sample_var in debiased lasso
v0.15.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.0,Define the data generation functions
v0.15.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.0,concat W and Z
v0.15.0,"we need to undo the one-hot encoding for calling effect,"
v0.15.0,since it expects raw values
v0.15.0,concat W and Z
v0.15.0,"we need to undo the one-hot encoding for calling effect,"
v0.15.0,since it expects raw values
v0.15.0,reshape the predictions
v0.15.0,"T_res, Z_res, beta expect shape to be (n,1)"
v0.15.0,Define the data generation functions
v0.15.0,maybe shouldn't expose fit_cate_intercept in this class?
v0.15.0,Define the data generation functions
v0.15.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.15.0,TODO: do correct adjustment for sample_var
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,concat W and Z
v0.15.0,concat W and Z
v0.15.0,concat W and Z
v0.15.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.15.0,Define the data generation functions
v0.15.0,"train E[T|X,W,Z]"
v0.15.0,"train E[Z|X,W]"
v0.15.0,note: discrete_instrument rather than discrete_treatment in call to _make_first_stage_selector
v0.15.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.15.0,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.15.0,attribute so that the trained featurizer will be passed through
v0.15.0,Handles the corner case when X=None but featurizer might be not None
v0.15.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.0,concat W and Z
v0.15.0,note that groups are not passed to score because they are only used for fitting
v0.15.0,concat W and Z
v0.15.0,note that sample_weight and groups are not passed to predict because they are only used for fitting
v0.15.0,concat W and Z
v0.15.0,A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
v0.15.0,Used by both Parametric and Non Parametric DMLIV.
v0.15.0,override only so that we can enforce Z to be required
v0.15.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.15.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.15.0,for internal use by the library
v0.15.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.0,Handles the corner case when X=None but featurizer might be not None
v0.15.0,Define the data generation functions
v0.15.0,Get input names
v0.15.0,Summary
v0.15.0,coefficient
v0.15.0,intercept
v0.15.0,Define the data generation functions
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,"this will have dimension (d,) + shape(X)"
v0.15.0,send the first dimension to the end
v0.15.0,columns are featurized independently; partial derivatives are only non-zero
v0.15.0,when taken with respect to the same column each time
v0.15.0,don't fit intercept; manually add column of ones to the data instead;
v0.15.0,this allows us to ignore the intercept when computing marginal effects
v0.15.0,make T 2D if if was a vector
v0.15.0,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.15.0,two stage approximation
v0.15.0,"first, get basis expansions of T, X, and Z"
v0.15.0,TODO: is it right that the effective number of intruments is the
v0.15.0,"product of ft_X and ft_Z, not just ft_Z?"
v0.15.0,"regress T expansion on X,Z expansions concatenated with W"
v0.15.0,"predict ft_T from interacted ft_X, ft_Z"
v0.15.0,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.15.0,dT may be only 2-dimensional)
v0.15.0,promote dT to 3D if necessary (e.g. if T was a vector)
v0.15.0,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,TODO: this utility is documented but internal; reimplement?
v0.15.0,TODO: this utility is even less public...
v0.15.0,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.15.0,use same Cs as would be used by default by LogisticRegressionCV
v0.15.0,NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
v0.15.0,which could affect how many times each distinct Y value needs to be present in the data
v0.15.0,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.15.0,but also supports get_feature_names with expected signature
v0.15.0,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.15.0,NOTE: we rely on the passthrough columns coming first in the concatenated X;W
v0.15.0,"when we pipeline scaling with our first stage models later, so the order here is important"
v0.15.0,TODO: remove once older sklearn support is no longer needed
v0.15.0,Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
v0.15.0,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.15.0,Convert SingleTreeInterpreter to a python dictionary
v0.15.0,named tuple type for storing results inside CausalAnalysis class;
v0.15.0,must be lifted to module level to enable pickling
v0.15.0,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
v0.15.0,"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
v0.15.0,"when running the first stage models, but don't want to scale the X columns when running the final model,"
v0.15.0,since then our coefficients will have odd units and our trees will also have decisions using those units.
v0.15.0,
v0.15.0,"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
v0.15.0,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.15.0,Controls are all other columns of X
v0.15.0,"can't use X[:, feat_ind] when X is a DataFrame"
v0.15.0,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.15.0,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.15.0,so that the user can opt-in to allowing unseen treatment values
v0.15.0,(and return NaN or something in that case)
v0.15.0,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
v0.15.0,and so we can just peel the first columns off of that combined array for rescaling in the pipeline
v0.15.0,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
v0.15.0,"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
v0.15.0,becomes a valid approach to handling this
v0.15.0,array checking routines don't accept 0-width arrays
v0.15.0,perform model selection
v0.15.0,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.15.0,convert to NormalInferenceResults for consistency
v0.15.0,Set the dictionary values shared between local and global summaries
v0.15.0,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.15.0,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.15.0,required to fit a discrete DML model
v0.15.0,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
v0.15.0,sub-cases of models or also integrate with azure autoML. (post-MVP)
v0.15.0,"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
v0.15.0,"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
v0.15.0,TODO: Enable multi-class classification (post-MVP)
v0.15.0,Validate inputs
v0.15.0,TODO: check compatibility of X and Y lengths
v0.15.0,"no previous fit, cancel warm start"
v0.15.0,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.15.0,"if heterogeneity_inds is 1D, repeat it"
v0.15.0,heterogeneity inds should be a 2D list of length same as train_inds
v0.15.0,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.15.0,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.15.0,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.15.0,train the Y model
v0.15.0,"perform model selection for the Y model using all X, not on a per-column basis"
v0.15.0,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.15.0,work with the regression wrapper
v0.15.0,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.15.0,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.15.0,since otherwise we'll have too many columns to be able to train a classifier
v0.15.0,start with empty results and default shared insights
v0.15.0,convert categorical indicators to numeric indices
v0.15.0,check for indices over the categorical expansion bound
v0.15.0,assume we'll be able to train former failures this time; we'll add them back if not
v0.15.0,"can't remove in place while iterating over new_inds, so store in separate list"
v0.15.0,"train the model, but warn"
v0.15.0,no model can be trained in this case since we need more folds
v0.15.0,"don't train a model, but suggest workaround since there are enough instances of least"
v0.15.0,populated class
v0.15.0,also remove from train_inds so we don't try to access the result later
v0.15.0,extract subset of names matching new columns
v0.15.0,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.15.0,don't want to cache this failed result
v0.15.0,properties to return from effect InferenceResults
v0.15.0,properties to return from PopulationSummaryResults
v0.15.0,Converts strings to property lookups or method calls as a convenience so that the
v0.15.0,_point_props and _summary_props above can be applied to an inference object
v0.15.0,Create a summary combining all results into a single output; this is used
v0.15.0,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.15.0,"or a dictionary, respectively, based on the summary function passed into this method"
v0.15.0,"ensure array has shape (m,y,t)"
v0.15.0,population summary is missing sample dimension; add it for consistency
v0.15.0,outcome dimension is missing; add it for consistency
v0.15.0,add singleton treatment dimension if missing
v0.15.0,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.15.0,"each attr has dimension (m,y) or (m,y,t)"
v0.15.0,concatenate along treatment dimension
v0.15.0,"for dictionary representation, want to remove unneeded sample dimension"
v0.15.0,in cohort and global results
v0.15.0,TODO: enrich outcome logic for multi-class classification when that is supported
v0.15.0,There is no actual sample level in this data
v0.15.0,can't drop only level
v0.15.0,should be serialization-ready and contain no numpy arrays
v0.15.0,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.15.0,TODO: Note that there's no column metadata for the sample number - should there be?
v0.15.0,"need to replicate the column info for each sample, then remove from the shared data"
v0.15.0,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.15.0,which may need to be revisited once we support multiclass
v0.15.0,get the length of the list corresponding to the first dictionary key
v0.15.0,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.15.0,a global inference indicates the effect of that one feature on the outcome
v0.15.0,need to reshape the output to match the input
v0.15.0,we want to offset the inference object by the baseline estimate of y
v0.15.0,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.15.0,get the length of the list corresponding to the first dictionary key
v0.15.0,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.15.0,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.15.0,because then scaling the difference between treatment value and treatment costs is the
v0.15.0,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.15.0,
v0.15.0,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.15.0,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.15.0,(rather than just not treating at all)
v0.15.0,
v0.15.0,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.15.0,2 * policy_value - always_treat
v0.15.0,includes exactly their contribution because policy_value and always_treat both include it
v0.15.0,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.15.0,2 * policy_value - always-treat
v0.15.0,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.15.0,is zero and the contribution to always_treat is negative
v0.15.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.15.0,"however, the tree can't store the feature and treatment names we compute here..."
v0.15.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.15.0,"however, the tree can't store the feature and treatment names we compute here..."
v0.15.0,get dataframe with all but selected column
v0.15.0,apply 10% of a typical treatment for this feature
v0.15.0,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.15.0,set the effect bounds; for positive treatments these agree with
v0.15.0,"the estimates; for negative treatments, we need to invert the interval"
v0.15.0,the effect is now always positive since we decrease treatment when negative
v0.15.0,"for discrete treatment, stack a zero result in front for control"
v0.15.0,we need to call effect_inference to get the correct CI between the two treatment options
v0.15.0,we now need to construct the delta in the cost between the two treatments and translate the effect
v0.15.0,remove third dimenions potentially added
v0.15.0,"find cost of current treatment: equality creates a 2d array with True on each row,"
v0.15.0,only if its the location of the current treatment. Then we take the corresponding cost.
v0.15.0,construct index of current treatment
v0.15.0,add second dimension if needed for broadcasting during translation of effect
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,TODO: conisder working around relying on sklearn implementation details
v0.15.0,"Found a good split, return."
v0.15.0,Record all splits in case the stratification by weight yeilds a worse partition
v0.15.0,Reseed random generator and try again
v0.15.0,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.15.0,"Found a good split, return."
v0.15.0,Did not find a good split
v0.15.0,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.15.0,Return most weight-balanced partition
v0.15.0,Weight stratification algorithm
v0.15.0,Sort weights for weight strata search
v0.15.0,There are some leftover indices that have yet to be assigned
v0.15.0,Append stratum splits to overall splits
v0.15.0,only expose predict_proba if best_model has predict_proba
v0.15.0,used because logic elsewhere uses hasattr predict proba to check if model is a classifier
v0.15.0,logic copied from check_cv
v0.15.0,otherwise we will assume the user already set the cv attribute to something
v0.15.0,compatible with splitting with a `groups` argument
v0.15.0,"drop groups from arg list, which were already used at the outer level and may not be supported by the model"
v0.15.0,"since needs_fit is False, is_selecting will only be true if"
v0.15.0,the score needs to be compared to another model's
v0.15.0,"so we don't need to fit the model itself, just get the out-of-sample score"
v0.15.0,use _fit_with_groups instead of just fit to handle nested grouping
v0.15.0,we need to train the model on the data
v0.15.0,copy common parameters
v0.15.0,copy common fitted variables
v0.15.0,make sure all classes agree on best c/l1 combo
v0.15.0,"We need an R^2 score to compare to other models; ElasticNetCV doesn't provide it,"
v0.15.0,but we can calculate it ourselves from the MSE plus the variance of the target y
v0.15.0,"l1 ratio doesn't apply to Lasso, only ElasticNet"
v0.15.0,max R^2 corresponds to min MSE
v0.15.0,"constructor takes cv as a positional or kwarg, just pull it out of a new instance"
v0.15.0,but it would be complicated to check that
v0.15.0,"technically, if there is just one model and it doesn't need to be fit we don't need to fit it,"
v0.15.0,but that complicates the training logic so we don't bother with that case
v0.15.0,"If classification methods produce multiple columns of output,"
v0.15.0,we need to manually encode classes to ensure consistent column ordering.
v0.15.0,We clone the estimator to make sure that all the folds are
v0.15.0,"independent, and that it is pickle-able."
v0.15.0,verbose was removed from sklearn's non-public _fit_and_predict method in 1.4
v0.15.0,`predictions` is a list of method outputs from each fold.
v0.15.0,"If each of those is also a list, then treat this as a"
v0.15.0,multioutput-multiclass task. We need to separately concatenate
v0.15.0,the method outputs for each label into an `n_labels` long list.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Our classes that derive from sklearn ones sometimes include
v0.15.0,inherited docstrings that have embedded doctests; we need the following imports
v0.15.0,so that they don't break.
v0.15.0,TODO: consider working around relying on sklearn implementation details
v0.15.0,local import to avoid circular imports
v0.15.0,"Convert X, y into numpy arrays"
v0.15.0,Define fit parameters
v0.15.0,Some algorithms don't have a check_input option
v0.15.0,Check weights array
v0.15.0,Check that weights are size-compatible
v0.15.0,Normalize inputs
v0.15.0,Weight inputs
v0.15.0,Fit base class without intercept
v0.15.0,Fit Lasso
v0.15.0,Reset intercept
v0.15.0,The intercept is not calculated properly due the sqrt(weights) factor
v0.15.0,so it must be recomputed
v0.15.0,Fit lasso without weights
v0.15.0,Make weighted splitter
v0.15.0,Fit weighted model
v0.15.0,Make weighted splitter
v0.15.0,Fit weighted model
v0.15.0,Call weighted lasso on reduced design matrix
v0.15.0,Weighted tau
v0.15.0,Select optimal penalty
v0.15.0,Warn about consistency
v0.15.0,"Convert X, y into numpy arrays"
v0.15.0,Fit weighted lasso with user input
v0.15.0,"Center X, y"
v0.15.0,Calculate quantities that will be used later on. Account for centered data
v0.15.0,Calculate coefficient and error variance
v0.15.0,Add coefficient correction
v0.15.0,Set coefficients and intercept standard errors
v0.15.0,Set intercept
v0.15.0,Return alpha to 'auto' state
v0.15.0,"Note that in the case of no intercept, X_offset is 0"
v0.15.0,Calculate the variance of the predictions
v0.15.0,Calculate prediction confidence intervals
v0.15.0,Assumes flattened y
v0.15.0,Compute weighted residuals
v0.15.0,To be done once per target. Assumes y can be flattened.
v0.15.0,Assumes that X has already been offset
v0.15.0,Special case: n_features=1
v0.15.0,Compute Lasso coefficients for the columns of the design matrix
v0.15.0,Compute C_hat
v0.15.0,Compute theta_hat
v0.15.0,Allow for single output as well
v0.15.0,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.15.0,Set coef_ attribute
v0.15.0,Set intercept_ attribute
v0.15.0,Set selected_alpha_ attribute
v0.15.0,Set coef_stderr_
v0.15.0,intercept_stderr_
v0.15.0,set model to the single-target estimator by default so there's always a model to get and set attributes on
v0.15.0,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.15.0,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.15.0,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.15.0,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.15.0,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.15.0,now regress X1 on y - X2 * beta2 to learn beta1
v0.15.0,set coef_ and intercept_ attributes
v0.15.0,Note that the penalized model should *not* have an intercept
v0.15.0,don't proxy special methods
v0.15.0,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.15.0,regressor incorrectly
v0.15.0,"Note: for known attributes that have been set this method will not be called,"
v0.15.0,so we should just throw here because this is an attribute belonging to this class
v0.15.0,but which hasn't yet been set on this instance
v0.15.0,set default values for None
v0.15.0,check freq_weight should be integer and should be accompanied by sample_var
v0.15.0,check array shape
v0.15.0,weight X and y and sample_var
v0.15.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.15.0,"For aggregation calculations, always treat wy as an array so that einsum expressions don't need to change"
v0.15.0,We'll collapse results back down afterwards if necessary
v0.15.0,"for federation, we need to store these 5 arrays when using heteroskedasticity-robust inference"
v0.15.0,y dimension is always first in the output when present so that broadcasting works correctly
v0.15.0,set default values for None
v0.15.0,check array shape
v0.15.0,check dimension of instruments is more than dimension of treatments
v0.15.0,weight X and y
v0.15.0,learn point estimate
v0.15.0,solve first stage linear regression E[T|Z]
v0.15.0,"""that"" means T̂"
v0.15.0,solve second stage linear regression E[Y|that]
v0.15.0,(T̂.T*T̂)^{-1}
v0.15.0,learn cov(theta)
v0.15.0,(T̂.T*T̂)^{-1}
v0.15.0,sigma^2
v0.15.0,reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,AzureML
v0.15.0,helper imports
v0.15.0,write the details of the workspace to a configuration file to the notebook library
v0.15.0,if y is a multioutput model
v0.15.0,Make sure second dimension has 1 or more item
v0.15.0,switch _inner Model to a MultiOutputRegressor
v0.15.0,flatten array as automl only takes vectors for y
v0.15.0,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.15.0,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.15.0,as an sklearn estimator
v0.15.0,fit implementation for a single output model.
v0.15.0,Create experiment for specified workspace
v0.15.0,Configure automl_config with training set information.
v0.15.0,"Wait for remote run to complete, the set the model"
v0.15.0,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.15.0,create model and pass model into final.
v0.15.0,"If item is an automl config, get its corresponding"
v0.15.0,AutomatedML Model and add it to new_Args
v0.15.0,"If item is an automl config, get its corresponding"
v0.15.0,AutomatedML Model and set it for this key in
v0.15.0,kwargs
v0.15.0,takes in either automated_ml config and instantiates
v0.15.0,an AutomatedMLModel
v0.15.0,The prefix can only be 18 characters long
v0.15.0,"because prefixes come from kwarg_names, we must ensure they are"
v0.15.0,short enough.
v0.15.0,Get workspace from config file.
v0.15.0,Take the intersect of the white for sample
v0.15.0,weights and linear models
v0.15.0,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,average the outcome dimension if it exists and ensure 2d y_pred
v0.15.0,get index of best treatment
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,TODO: consider working around relying on sklearn implementation details
v0.15.0,Create splits of causal tree
v0.15.0,Make sure the correct exception is being rethrown
v0.15.0,Must make sure indices are merged correctly
v0.15.0,Convert rows to columns
v0.15.0,Require group assignment t to be one-hot-encoded
v0.15.0,Get predictions for the 2 splits
v0.15.0,Must make sure indices are merged correctly
v0.15.0,Crossfitting
v0.15.0,Compute weighted nuisance estimates
v0.15.0,-------------------------------------------------------------------------------
v0.15.0,Calculate the covariance matrix corresponding to the BLB inference
v0.15.0,
v0.15.0,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.15.0,2. Calculate the weighted moments for each tree slice to create a matrix
v0.15.0,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.15.0,in that slice from the overall parameter estimate.
v0.15.0,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.15.0,-------------------------------------------------------------------------------
v0.15.0,Calclulate covariance matrix through BLB
v0.15.0,Estimators
v0.15.0,OrthoForest parameters
v0.15.0,Sub-forests
v0.15.0,Auxiliary attributes
v0.15.0,Fit check
v0.15.0,TODO: Check performance
v0.15.0,Must normalize weights
v0.15.0,Override the CATE inference options
v0.15.0,Add blb inference to parent's options
v0.15.0,Generate subsample indices
v0.15.0,Build trees in parallel
v0.15.0,Bootstraping has repetitions in tree sample
v0.15.0,Similar for `a` weights
v0.15.0,Bootstraping has repetitions in tree sample
v0.15.0,Define subsample size
v0.15.0,Safety check
v0.15.0,Draw points to create little bags
v0.15.0,Copy and/or define models
v0.15.0,TODO: ideally the below private attribute logic should be in .fit but is needed in init
v0.15.0,for nuisance estimator generation for parent class
v0.15.0,should refactor later
v0.15.0,Define nuisance estimators
v0.15.0,Define parameter estimators
v0.15.0,Define
v0.15.0,Need to redefine fit here for auto inference to work due to a quirk in how
v0.15.0,wrap_fit is defined
v0.15.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.15.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.15.0,Override to flatten output if T is flat
v0.15.0,Check that all discrete treatments are represented
v0.15.0,Nuissance estimates evaluated with cross-fitting
v0.15.0,Define 2-fold iterator
v0.15.0,Check if there is only one example of some class
v0.15.0,Define 2-fold iterator
v0.15.0,need safe=False when cloning for WeightedModelWrapper
v0.15.0,Compute residuals
v0.15.0,Compute coefficient by OLS on residuals
v0.15.0,"Parameter returned by LinearRegression is (d_T, )"
v0.15.0,Compute residuals
v0.15.0,Compute coefficient by OLS on residuals
v0.15.0,ell_2 regularization
v0.15.0,Ridge regression estimate
v0.15.0,"Parameter returned is of shape (d_T, )"
v0.15.0,Return moments and gradients
v0.15.0,Compute residuals
v0.15.0,Compute moments
v0.15.0,"Moments shape is (n, d_T)"
v0.15.0,Compute moment gradients
v0.15.0,returns shape-conforming residuals
v0.15.0,Copy and/or define models
v0.15.0,Define parameter estimators
v0.15.0,Define moment and mean gradient estimator
v0.15.0,"Check that T is shape (n, )"
v0.15.0,Check T is numeric
v0.15.0,Train label encoder
v0.15.0,Call `fit` from parent class
v0.15.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.15.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.15.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0,Override to flatten output if T is flat
v0.15.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0,Expand one-hot encoding to include the zero treatment
v0.15.0,"Test that T contains all treatments. If not, return None"
v0.15.0,Nuissance estimates evaluated with cross-fitting
v0.15.0,Define 2-fold iterator
v0.15.0,Check if there is only one example of some class
v0.15.0,No need to crossfit for internal nodes
v0.15.0,Compute partial moments
v0.15.0,"If any of the values in the parameter estimate is nan, return None"
v0.15.0,Compute partial moments
v0.15.0,Compute coefficient by OLS on residuals
v0.15.0,ell_2 regularization
v0.15.0,Ridge regression estimate
v0.15.0,"Parameter returned is of shape (d_T, )"
v0.15.0,Return moments and gradients
v0.15.0,Compute partial moments
v0.15.0,Compute moments
v0.15.0,"Moments shape is (n, d_T-1)"
v0.15.0,Compute moment gradients
v0.15.0,Need to calculate this in an elegant way for when propensity is 0
v0.15.0,This will flatten T
v0.15.0,Check that T is numeric
v0.15.0,Test whether the input estimator is supported
v0.15.0,Calculate confidence intervals for the parameter (marginal effect)
v0.15.0,Calculate confidence intervals for the effect
v0.15.0,Calculate the effects
v0.15.0,Calculate the standard deviations for the effects
v0.15.0,d_t=None here since we measure the effect across all Ts
v0.15.0,conditionally expand jacobian dimensions to align with einsum str
v0.15.0,Calculate the effects
v0.15.0,Calculate the standard deviations for the effects
v0.15.0,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0,Licensed under the MIT License.
v0.15.0,Causal tree parameters
v0.15.0,Tree structure
v0.15.0,No need for a random split since the data is already
v0.15.0,a random subsample from the original input
v0.15.0,node list stores the nodes that are yet to be splitted
v0.15.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.15.0,Create local sample set
v0.15.0,Compute nuisance estimates for the current node
v0.15.0,Nuisance estimate cannot be calculated
v0.15.0,Estimate parameter for current node
v0.15.0,Node estimate cannot be calculated
v0.15.0,Calculate moments and gradient of moments for current data
v0.15.0,Calculate inverse gradient
v0.15.0,The gradient matrix is not invertible.
v0.15.0,No good split can be found
v0.15.0,Calculate point-wise pseudo-outcomes rho
v0.15.0,a split is determined by a feature and a sample pair
v0.15.0,the number of possible splits is at most (number of features) * (number of node samples)
v0.15.0,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.15.0,parse row and column of random pair
v0.15.0,the sample of the pair is the integer division of the random number with n_feats
v0.15.0,calculate the binary indicator of whether sample i is on the left or the right
v0.15.0,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.15.0,calculate the number of samples on the left child for each proposed split
v0.15.0,calculate the analogous binary indicator for the samples in the estimation set
v0.15.0,calculate the number of estimation samples on the left child of each proposed split
v0.15.0,find the upper and lower bound on the size of the left split for the split
v0.15.0,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.15.0,on each side.
v0.15.0,similarly for the estimation sample set
v0.15.0,if there is no valid split then don't create any children
v0.15.0,filter only the valid splits
v0.15.0,calculate the average influence vector of the samples in the left child
v0.15.0,calculate the average influence vector of the samples in the right child
v0.15.0,take the square of each of the entries of the influence vectors and normalize
v0.15.0,by size of each child
v0.15.0,calculate the vector score of each candidate split as the average of left and right
v0.15.0,influence vectors
v0.15.0,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.15.0,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.15.0,where there might be large discontinuities in some parameter as the conditioning set varies
v0.15.0,calculate the scalar score of each split by aggregating across the vector of scores
v0.15.0,Find split that minimizes criterion
v0.15.0,Create child nodes with corresponding subsamples
v0.15.0,add the created children to the list of not yet split nodes
v0.15.0b1,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.15.0b1,configuration is all pulled from setup.cfg
v0.15.0b1,-*- coding: utf-8 -*-
v0.15.0b1,
v0.15.0b1,Configuration file for the Sphinx documentation builder.
v0.15.0b1,
v0.15.0b1,This file does only contain a selection of the most common options. For a
v0.15.0b1,full list see the documentation:
v0.15.0b1,http://www.sphinx-doc.org/en/main/config
v0.15.0b1,-- Path setup --------------------------------------------------------------
v0.15.0b1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.15.0b1,add these directories to sys.path here. If the directory is relative to the
v0.15.0b1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.15.0b1,
v0.15.0b1,-- Project information -----------------------------------------------------
v0.15.0b1,-- General configuration ---------------------------------------------------
v0.15.0b1,"If your documentation needs a minimal Sphinx version, state it here."
v0.15.0b1,
v0.15.0b1,needs_sphinx = '1.0'
v0.15.0b1,"Add any Sphinx extension module names here, as strings. They can be"
v0.15.0b1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.15.0b1,ones.
v0.15.0b1,TODO: enable type aliases
v0.15.0b1,napoleon_preprocess_types = True  # needed for type aliases to work
v0.15.0b1,napoleon_type_aliases = {
v0.15.0b1,"""array_like"": "":term:`array_like`"","
v0.15.0b1,"""ndarray"": ""~numpy.ndarray"","
v0.15.0b1,"""RandomState"": "":class:`~numpy.random.RandomState`"","
v0.15.0b1,"""DataFrame"": "":class:`~pandas.DataFrame`"","
v0.15.0b1,"""Series"": "":class:`~pandas.Series`"","
v0.15.0b1,}
v0.15.0b1,"Add any paths that contain templates here, relative to this directory."
v0.15.0b1,The suffix(es) of source filenames.
v0.15.0b1,You can specify multiple suffix as a list of strings:
v0.15.0b1,
v0.15.0b1,"source_suffix = ['.rst', '.md']"
v0.15.0b1,The root toctree document.
v0.15.0b1,The language for content autogenerated by Sphinx. Refer to documentation
v0.15.0b1,for a list of supported languages.
v0.15.0b1,
v0.15.0b1,This is also used if you do content translation via gettext catalogs.
v0.15.0b1,"Usually you set ""language"" from the command line for these cases."
v0.15.0b1,"List of patterns, relative to source directory, that match files and"
v0.15.0b1,directories to ignore when looking for source files.
v0.15.0b1,This pattern also affects html_static_path and html_extra_path.
v0.15.0b1,The name of the Pygments (syntax highlighting) style to use.
v0.15.0b1,-- Options for HTML output -------------------------------------------------
v0.15.0b1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.15.0b1,a list of builtin themes.
v0.15.0b1,
v0.15.0b1,Theme options are theme-specific and customize the look and feel of a theme
v0.15.0b1,"further.  For a list of options available for each theme, see the"
v0.15.0b1,documentation.
v0.15.0b1,
v0.15.0b1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.15.0b1,"relative to this directory. They are copied after the builtin static files,"
v0.15.0b1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.15.0b1,html_static_path = ['_static']
v0.15.0b1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.15.0b1,to template names.
v0.15.0b1,
v0.15.0b1,The default sidebars (for documents that don't match any pattern) are
v0.15.0b1,defined by theme itself.  Builtin themes are using these templates by
v0.15.0b1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.15.0b1,'searchbox.html']``.
v0.15.0b1,
v0.15.0b1,html_sidebars = {}
v0.15.0b1,-- Options for HTMLHelp output ---------------------------------------------
v0.15.0b1,Output file base name for HTML help builder.
v0.15.0b1,-- Options for LaTeX output ------------------------------------------------
v0.15.0b1,The paper size ('letterpaper' or 'a4paper').
v0.15.0b1,
v0.15.0b1,"'papersize': 'letterpaper',"
v0.15.0b1,"The font size ('10pt', '11pt' or '12pt')."
v0.15.0b1,
v0.15.0b1,"'pointsize': '10pt',"
v0.15.0b1,Additional stuff for the LaTeX preamble.
v0.15.0b1,
v0.15.0b1,"'preamble': '',"
v0.15.0b1,Latex figure (float) alignment
v0.15.0b1,
v0.15.0b1,"'figure_align': 'htbp',"
v0.15.0b1,Grouping the document tree into LaTeX files. List of tuples
v0.15.0b1,"(source start file, target name, title,"
v0.15.0b1,"author, documentclass [howto, manual, or own class])."
v0.15.0b1,-- Options for manual page output ------------------------------------------
v0.15.0b1,One entry per manual page. List of tuples
v0.15.0b1,"(source start file, name, description, authors, manual section)."
v0.15.0b1,-- Options for Texinfo output ----------------------------------------------
v0.15.0b1,Grouping the document tree into Texinfo files. List of tuples
v0.15.0b1,"(source start file, target name, title, author,"
v0.15.0b1,"dir menu entry, description, category)"
v0.15.0b1,-- Options for Epub output -------------------------------------------------
v0.15.0b1,Bibliographic Dublin Core info.
v0.15.0b1,The unique identifier of the text. This can be a ISBN number
v0.15.0b1,or the project homepage.
v0.15.0b1,
v0.15.0b1,epub_identifier = ''
v0.15.0b1,A unique identification for the text.
v0.15.0b1,
v0.15.0b1,epub_uid = ''
v0.15.0b1,A list of files that should not be packed into the epub file.
v0.15.0b1,-- Extension configuration -------------------------------------------------
v0.15.0b1,-- Options for intersphinx extension ---------------------------------------
v0.15.0b1,Example configuration for intersphinx: refer to the Python standard library.
v0.15.0b1,-- Options for todo extension ----------------------------------------------
v0.15.0b1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.15.0b1,-- Options for doctest extension -------------------------------------------
v0.15.0b1,we can document otherwise excluded entities here by returning False
v0.15.0b1,or skip otherwise included entities by returning True
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Calculate residuals
v0.15.0b1,Estimate E[T_res | Z_res]
v0.15.0b1,TODO. Deal with multi-class instrument
v0.15.0b1,Calculate nuisances
v0.15.0b1,Estimate E[T_res | Z_res]
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.15.0b1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.15.0b1,TODO. Deal with multi-class instrument
v0.15.0b1,Estimate final model of theta(X) by minimizing the square loss:
v0.15.0b1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.15.0b1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.15.0b1,at the expense of some small bias. For points with very small covariance we revert
v0.15.0b1,to the model-based preliminary estimate and do not add the correction term.
v0.15.0b1,Estimate preliminary theta in cross fitting manner
v0.15.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.15.0b1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.15.0b1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.15.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.15.0b1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.15.0b1,TODO. The solution below is not really a valid cross-fitting
v0.15.0b1,as the test data are used to create the proj_t on the train
v0.15.0b1,which in the second train-test loop is used to create the nuisance
v0.15.0b1,cov on the test data. Hence the T variable of some sample
v0.15.0b1,"is implicitly correlated with its cov nuisance, through this flow"
v0.15.0b1,"of information. However, this seems a rather weak correlation."
v0.15.0b1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.15.0b1,model.
v0.15.0b1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.15.0b1,Estimate preliminary theta in cross fitting manner
v0.15.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.15.0b1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.15.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.15.0b1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.15.0b1,#############################################################################
v0.15.0b1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.15.0b1,A/B test
v0.15.0b1,#############################################################################
v0.15.0b1,Estimate preliminary theta in cross fitting manner
v0.15.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.15.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.15.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.15.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.15.0b1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.15.0b1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.15.0b1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.15.0b1,model_T_XZ = lambda: model_clf()
v0.15.0b1,#'days_visited': lambda:
v0.15.0b1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.15.0b1,Turn strings into categories for numeric mapping
v0.15.0b1,### Defining some generic regressors and classifiers
v0.15.0b1,This a generic non-parametric regressor
v0.15.0b1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.15.0b1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.15.0b1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.15.0b1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.15.0b1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.15.0b1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.15.0b1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.15.0b1,model = lambda: LinearRegression(n_jobs=-1)
v0.15.0b1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.15.0b1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.15.0b1,underlying model whenever predict is called.
v0.15.0b1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.15.0b1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.15.0b1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.15.0b1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.15.0b1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.15.0b1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.15.0b1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.15.0b1,We need to specify models to be used for each of these residualizations
v0.15.0b1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.15.0b1,"E[T | X, Z]"
v0.15.0b1,E[TZ | X]
v0.15.0b1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.15.0b1,n_splits determines the number of splits to be used for cross-fitting.
v0.15.0b1,# Algorithm 2 - Current Method
v0.15.0b1,In[121]:
v0.15.0b1,# Algorithm 3 - DRIV ATE
v0.15.0b1,dmliv_model_effect = lambda: model()
v0.15.0b1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.15.0b1,"dmliv_model_effect(),"
v0.15.0b1,n_splits=1)
v0.15.0b1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.15.0b1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.15.0b1,"Once multiple treatments are supported, we'll need to fix this"
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.15.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.15.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.15.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,TODO. Deal with multi-class instrument/treatment
v0.15.0b1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.15.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.15.0b1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.15.0b1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.15.0b1,##################
v0.15.0b1,Global settings #
v0.15.0b1,##################
v0.15.0b1,Global plotting controls
v0.15.0b1,"Control for support size, can control for more"
v0.15.0b1,#################
v0.15.0b1,File utilities #
v0.15.0b1,#################
v0.15.0b1,#################
v0.15.0b1,Plotting utils #
v0.15.0b1,#################
v0.15.0b1,bias
v0.15.0b1,var
v0.15.0b1,rmse
v0.15.0b1,r2
v0.15.0b1,Infer feature dimension
v0.15.0b1,Metrics by support plots
v0.15.0b1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.15.0b1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.15.0b1,Steven Wu <zhiww@microsoft.com>
v0.15.0b1,Initialize causal tree parameters
v0.15.0b1,Create splits of causal tree
v0.15.0b1,Estimate treatment effects at the leafs
v0.15.0b1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.15.0b1,the corresponding split and associating the effect computed on that leaf
v0.15.0b1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.15.0b1,Safety check
v0.15.0b1,Weighted linear regression
v0.15.0b1,Calculates weights
v0.15.0b1,Bootstraping has repetitions in tree sample so we need to iterate
v0.15.0b1,over all indices
v0.15.0b1,Similar for `a` weights
v0.15.0b1,Doesn't have sample weights
v0.15.0b1,Is a linear model
v0.15.0b1,Weighted linear regression
v0.15.0b1,Calculates weights
v0.15.0b1,Bootstraping has repetitions in tree sample so we need to iterate
v0.15.0b1,over all indices
v0.15.0b1,Similar for `a` weights
v0.15.0b1,normalize weights
v0.15.0b1,"Split the data in half, train and test"
v0.15.0b1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.15.0b1,"a function of W, using only the train fold"
v0.15.0b1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.15.0b1,"Split the data in half, train and test"
v0.15.0b1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.15.0b1,"a function of W, using only the train fold"
v0.15.0b1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.15.0b1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.15.0b1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.15.0b1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.15.0b1,"Split the data in half, train and test"
v0.15.0b1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.15.0b1,"a function of x, using only the train fold"
v0.15.0b1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.15.0b1,Compute coefficient by OLS on residuals
v0.15.0b1,"Split the data in half, train and test"
v0.15.0b1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.15.0b1,"a function of x, using only the train fold"
v0.15.0b1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.15.0b1,Estimate multipliers for second order orthogonal method
v0.15.0b1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.15.0b1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.15.0b1,Create local sample set
v0.15.0b1,compute the base estimate for the current node using double ml or second order double ml
v0.15.0b1,compute the influence functions here that are used for the criterion
v0.15.0b1,generate random proposals of dimensions to split
v0.15.0b1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.15.0b1,compute criterion for each proposal
v0.15.0b1,if splitting creates valid leafs in terms of mean leaf size
v0.15.0b1,Calculate criterion for split
v0.15.0b1,Else set criterion to infinity so that this split is not chosen
v0.15.0b1,If no good split was found
v0.15.0b1,Find split that minimizes criterion
v0.15.0b1,Set the split attributes at the node
v0.15.0b1,Create child nodes with corresponding subsamples
v0.15.0b1,Recursively split children
v0.15.0b1,Return parent node
v0.15.0b1,estimate the local parameter at the leaf using the estimate data
v0.15.0b1,###################
v0.15.0b1,Argument parsing #
v0.15.0b1,###################
v0.15.0b1,#########################################
v0.15.0b1,Parameters constant across experiments #
v0.15.0b1,#########################################
v0.15.0b1,Outcome support
v0.15.0b1,Treatment support
v0.15.0b1,Evaluation grid
v0.15.0b1,Treatment effects array
v0.15.0b1,Other variables
v0.15.0b1,##########################
v0.15.0b1,Data Generating Process #
v0.15.0b1,##########################
v0.15.0b1,Log iteration
v0.15.0b1,"Generate controls, features, treatment and outcome"
v0.15.0b1,T and Y residuals to be used in later scripts
v0.15.0b1,Save generated dataset
v0.15.0b1,#################
v0.15.0b1,ORF parameters #
v0.15.0b1,#################
v0.15.0b1,######################################
v0.15.0b1,Train and evaluate treatment effect #
v0.15.0b1,######################################
v0.15.0b1,########
v0.15.0b1,Plots #
v0.15.0b1,########
v0.15.0b1,###############
v0.15.0b1,Save results #
v0.15.0b1,###############
v0.15.0b1,##############
v0.15.0b1,Run Rscript #
v0.15.0b1,##############
v0.15.0b1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.15.0b1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.15.0b1,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.15.0b1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.15.0b1,def mlasso_model(): return MultiTaskLassoCV(
v0.15.0b1,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.15.0b1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.0b1,heterogeneity
v0.15.0b1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.0b1,heterogeneity
v0.15.0b1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.0b1,heterogeneity
v0.15.0b1,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.15.0b1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.15.0b1,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.15.0b1,subset of features that are exogenous and create heterogeneity
v0.15.0b1,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.15.0b1,subset of features wrt we estimate heterogeneity
v0.15.0b1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.15.0b1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,introspect the constructor arguments to find the model parameters
v0.15.0b1,to represent
v0.15.0b1,"if the argument is deprecated, ignore it"
v0.15.0b1,Extract and sort argument names excluding 'self'
v0.15.0b1,column names
v0.15.0b1,transfer input to numpy arrays
v0.15.0b1,transfer input to 2d arrays
v0.15.0b1,create dataframe
v0.15.0b1,currently dowhy only support single outcome and single treatment
v0.15.0b1,call dowhy
v0.15.0b1,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.15.0b1,cate estimator but not the effect.
v0.15.0b1,don't proxy special methods
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Check if model is sparse enough for this model
v0.15.0b1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.15.0b1,TODO: any way to avoid creating a copy if the array was already dense?
v0.15.0b1,"the call is necessary if the input was something like a list, though"
v0.15.0b1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.15.0b1,so convert to pydata sparse first
v0.15.0b1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.15.0b1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.15.0b1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.15.0b1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.15.0b1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.15.0b1,For when checking input values is disabled
v0.15.0b1,Type to column extraction function
v0.15.0b1,if not all column names are strings
v0.15.0b1,coerce feature names to be strings
v0.15.0b1,Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
v0.15.0b1,"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
v0.15.0b1,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.15.0b1,Handles cases where the passed feature names create issues
v0.15.0b1,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.15.0b1,Get feature names using featurizer
v0.15.0b1,All attempts at retrieving transformed feature names have failed
v0.15.0b1,Delegate handling to downstream logic
v0.15.0b1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.15.0b1,same number of input definitions as arrays
v0.15.0b1,input definitions have same number of dimensions as each array
v0.15.0b1,all result indices are unique
v0.15.0b1,all result indices must match at least one input index
v0.15.0b1,"map indices to all array, axis pairs for that index"
v0.15.0b1,each index has the same cardinality wherever it appears
v0.15.0b1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.15.0b1,Algo: while list contains more than one entry
v0.15.0b1,take two entries
v0.15.0b1,sort both lists by intersection of their indices
v0.15.0b1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.15.0b1,"take the union of indices and the product of values), stepping through each list linearly"
v0.15.0b1,TODO: might be faster to break into connected components first
v0.15.0b1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.15.0b1,"so compute their content separately, then take cartesian product"
v0.15.0b1,this would save a few pointless sorts by empty tuples
v0.15.0b1,TODO: Consider investigating other performance ideas for these cases
v0.15.0b1,where the dense method beat the sparse method (usually sparse is faster)
v0.15.0b1,"e,facd,c->cfed"
v0.15.0b1,sparse: 0.0335489
v0.15.0b1,dense:  0.011465999999999997
v0.15.0b1,"gbd,da,egb->da"
v0.15.0b1,sparse: 0.0791625
v0.15.0b1,dense:  0.007319099999999995
v0.15.0b1,"dcc,d,faedb,c->abe"
v0.15.0b1,sparse: 1.2868097
v0.15.0b1,dense:  0.44605229999999985
v0.15.0b1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.15.0b1,TODO: would using einsum's paths to optimize the order of merging help?
v0.15.0b1,Normalize weights
v0.15.0b1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.15.0b1,"if we're decorating a class, just update the __init__ method,"
v0.15.0b1,so that the result is still a class instead of a wrapper method
v0.15.0b1,"want to enforce that each bad_arg was either in kwargs,"
v0.15.0b1,or else it was in neither and is just taking its default value
v0.15.0b1,Any access should throw
v0.15.0b1,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.15.0b1,return plain dictionary so that erroneous accesses don't half work (see e.g. #708)
v0.15.0b1,for every dimension of the treatment add some epsilon and observe change in featurized treatment
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.0b1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.0b1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.0b1,input feature name is already updated by cate_feature_names.
v0.15.0b1,define the index of d_x to filter for each given T
v0.15.0b1,filter X after broadcast with T for each given T
v0.15.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.0b1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.15.0b1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,
v0.15.0b1,This code contains some snippets of code from:
v0.15.0b1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
v0.15.0b1,published under the following license and copyright:
v0.15.0b1,BSD 3-Clause License
v0.15.0b1,
v0.15.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0b1,All rights reserved.
v0.15.0b1,make any access to matplotlib or plt throw an exception
v0.15.0b1,make any access to graphviz or plt throw an exception
v0.15.0b1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.15.0b1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.15.0b1,Initialize saturation & value; calculate chroma & value shift
v0.15.0b1,Calculate some intermediate values
v0.15.0b1,Initialize RGB with same hue & chroma as our color
v0.15.0b1,Shift the initial RGB values to match value and store
v0.15.0b1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.15.0b1,clean way of achieving this
v0.15.0b1,make sure we don't accidentally escape anything in the substitution
v0.15.0b1,Fetch appropriate color for node
v0.15.0b1,"red for negative, green for positive"
v0.15.0b1,in multi-target use mean of targets
v0.15.0b1,Write node mean CATE
v0.15.0b1,Write node std of CATE
v0.15.0b1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.15.0b1,Fetch appropriate color for node
v0.15.0b1,Write node mean CATE
v0.15.0b1,Write node mean CATE
v0.15.0b1,Write recommended treatment and value - cost
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"since inference objects can be stateful, we must copy it before fitting;"
v0.15.0b1,otherwise this sequence wouldn't work:
v0.15.0b1,"est1.fit(..., inference=inf)"
v0.15.0b1,"est2.fit(..., inference=inf)"
v0.15.0b1,est1.effect_interval(...)
v0.15.0b1,because inf now stores state from fitting est2
v0.15.0b1,This flag is true when names are set in a child class instead
v0.15.0b1,"If names are set in a child class, add an attribute reflecting that"
v0.15.0b1,This works only if X is passed as a kwarg
v0.15.0b1,We plan to enforce X as kwarg only in future releases
v0.15.0b1,This checks if names have been set in a child class
v0.15.0b1,"If names were set in a child class, don't do it again"
v0.15.0b1,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.15.0b1,call the wrapped fit method
v0.15.0b1,NOTE: we call inference fit *after* calling the main fit method
v0.15.0b1,apply defaults before calling inference method
v0.15.0b1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.15.0b1,but tensordot can't be applied to this problem because we don't sum over m
v0.15.0b1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.15.0b1,of rows of T was not taken into account
v0.15.0b1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.15.0b1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.15.0b1,"Treatment names is None, default to BaseCateEstimator"
v0.15.0b1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.15.0b1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.15.0b1,Get input names
v0.15.0b1,Summary
v0.15.0b1,add statsmodels to parent's options
v0.15.0b1,add debiasedlasso to parent's options
v0.15.0b1,add blb to parent's options
v0.15.0b1,TODO Share some logic with non-discrete version
v0.15.0b1,Get input names
v0.15.0b1,Note: we do not transform feature names since that is done within summary_frame
v0.15.0b1,Summary
v0.15.0b1,add statsmodels to parent's options
v0.15.0b1,add statsmodels to parent's options
v0.15.0b1,add blb to parent's options
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,remove None arguments
v0.15.0b1,"scores entries should be lists of scores, so make each entry a singleton list"
v0.15.0b1,Adding the kwargs to ray object store to be used by remote functions for each fold to avoid IO overhead
v0.15.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.15.0b1,generate an instance of the final model
v0.15.0b1,generate an instance of the nuisance model
v0.15.0b1,Define Ray remote function (Ray remote wrapper of the _fit_nuisances function)
v0.15.0b1,Create Ray remote jobs for parallel processing
v0.15.0b1,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.15.0b1,alteration even when we only want to fit_final.
v0.15.0b1,use a binary array to get stratified split in case of discrete treatment
v0.15.0b1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.15.0b1,upgrade to a GroupKFold or StratiGroupKFold if groups is not None
v0.15.0b1,"we won't have generated a KFold or StratifiedKFold ourselves when groups are passed,"
v0.15.0b1,"but the user might have supplied one, which won't work"
v0.15.0b1,for each mc iteration
v0.15.0b1,for each model under cross fit setting
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"TODO: This could be extended to also work with our sparse and 2SLS estimators,"
v0.15.0b1,if we add an aggregate method to them
v0.15.0b1,Remember to update the docs if this changes
v0.15.0b1,mix in the appropriate inference class
v0.15.0b1,assign all of the attributes from the dummy estimator that would normally be assigned during fitting
v0.15.0b1,TODO: This seems hacky; is there a better abstraction to maintain these?
v0.15.0b1,"This should also include bias_part_of_coef, model_final_, and fitted_models_final above"
v0.15.0b1,Assign treatment expansion attributes
v0.15.0b1,Methods needed to implement the LinearCateEstimator interface
v0.15.0b1,Methods needed to implement the LinearFinalModelCateEstimatorMixin
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,
v0.15.0b1,This code contains snippets of code from
v0.15.0b1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.15.0b1,published under the following license and copyright:
v0.15.0b1,BSD 3-Clause License
v0.15.0b1,
v0.15.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0b1,All rights reserved.
v0.15.0b1,=============================================================================
v0.15.0b1,Policy Forest
v0.15.0b1,=============================================================================
v0.15.0b1,Remap output
v0.15.0b1,reshape is necessary to preserve the data contiguity against vs
v0.15.0b1,"[:, np.newaxis] that does not."
v0.15.0b1,Get subsample sample size
v0.15.0b1,Check parameters
v0.15.0b1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.15.0b1,if this is the first `fit` call of the warm start mode.
v0.15.0b1,"Free allocated memory, if any"
v0.15.0b1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.15.0b1,We draw from the random state to get the random state we
v0.15.0b1,would have got if we hadn't used a warm_start.
v0.15.0b1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.15.0b1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.15.0b1,but would still advance randomness enough so that tree subsamples will be different.
v0.15.0b1,Parallel loop: we prefer the threading backend as the Cython code
v0.15.0b1,for fitting the trees is internally releasing the Python GIL
v0.15.0b1,making threading more efficient than multiprocessing in
v0.15.0b1,"that case. However, for joblib 0.12+ we respect any"
v0.15.0b1,"parallel_backend contexts set at a higher level,"
v0.15.0b1,since correctness does not rely on using threads.
v0.15.0b1,Collect newly grown trees
v0.15.0b1,Check data
v0.15.0b1,Assign chunk of trees to jobs
v0.15.0b1,avoid storing the output of every estimator by summing them here
v0.15.0b1,Parallel loop
v0.15.0b1,Check data
v0.15.0b1,Assign chunk of trees to jobs
v0.15.0b1,avoid storing the output of every estimator by summing them here
v0.15.0b1,Parallel loop
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,
v0.15.0b1,This code contains snippets of code from:
v0.15.0b1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.15.0b1,published under the following license and copyright:
v0.15.0b1,BSD 3-Clause License
v0.15.0b1,
v0.15.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0b1,All rights reserved.
v0.15.0b1,=============================================================================
v0.15.0b1,Types and constants
v0.15.0b1,=============================================================================
v0.15.0b1,=============================================================================
v0.15.0b1,Base Policy tree
v0.15.0b1,=============================================================================
v0.15.0b1,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.15.0b1,HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor
v0.15.0b1,This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"Unique treatments (ordered, includes control)"
v0.15.0b1,Number of treatments (excluding control)
v0.15.0b1,Indicator for whether
v0.15.0b1,Get DR outcomes in training sample
v0.15.0b1,Get DR outcomes in validation sample
v0.15.0b1,Get DR outcomes in validation sample
v0.15.0b1,Calculate ATE in the validation sample
v0.15.0b1,Fit propensity in treatment
v0.15.0b1,Predict propensity scores
v0.15.0b1,Possible treatments (need to allow more than 2)
v0.15.0b1,Predict outcomes
v0.15.0b1,T-learner logic
v0.15.0b1,"if CATE is given explicitly or has not been fitted at all previously, fit it now"
v0.15.0b1,Assign units in validation set to groups
v0.15.0b1,Proportion of validations set in group
v0.15.0b1,Group average treatment effect (GATE) -- average of DR outcomes in group
v0.15.0b1,Average of CATE predictions in group
v0.15.0b1,Calculate group calibration score
v0.15.0b1,Calculate overall calibration score
v0.15.0b1,Calculate R-square calibration score
v0.15.0b1,"if CATE is given explicitly or has not been fitted at all previously, fit it now"
v0.15.0b1,treat each treatment as a separate regression
v0.15.0b1,"here, prop_preds should be a matrix"
v0.15.0b1,with rows corresponding to units and columns corresponding to treatment statuses
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.15.0b1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.15.0b1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.15.0b1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.15.0b1,checks that X is 2D array.
v0.15.0b1,"since we only allow single dimensional y, we could flatten the prediction"
v0.15.0b1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0b1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0b1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.0b1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.0b1,Handles the corner case when X=None but featurizer might be not None
v0.15.0b1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.15.0b1,"Replacing this method which is invalid for this class, so that we make the"
v0.15.0b1,dosctring empty and not appear in the docs.
v0.15.0b1,TODO: support freq_weight and sample_var in debiased lasso
v0.15.0b1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.15.0b1,Replacing to remove docstring
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"if both X and W are None, just return a column of ones"
v0.15.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.15.0b1,We need to go back to the label representation of the one-hot so as to call
v0.15.0b1,the classifier.
v0.15.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.15.0b1,We need to go back to the label representation of the one-hot so as to call
v0.15.0b1,the classifier.
v0.15.0b1,data is already validated at initial fit time
v0.15.0b1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.15.0b1,This works both with our without the weighting trick as the treatments T are unit vector
v0.15.0b1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.15.0b1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.15.0b1,both Parametric and Non Parametric DML.
v0.15.0b1,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.15.0b1,attribute so that the trained featurizer will be passed through
v0.15.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.15.0b1,for internal use by the library
v0.15.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.0b1,We need to use the rlearner's copy to retain the information from fitting
v0.15.0b1,Handles the corner case when X=None but featurizer might be not None
v0.15.0b1,override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
v0.15.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.15.0b1,TODO: support freq_weight and sample_var in debiased lasso
v0.15.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.15.0b1,since we clone it and fit separate copies
v0.15.0b1,add blb to parent's options
v0.15.0b1,override only so that we can update the docstring to indicate
v0.15.0b1,support for `GenericSingleTreatmentModelFinalInference`
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,note that groups are not passed to score because they are only used for fitting
v0.15.0b1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.0b1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.15.0b1,NOTE: important to get parent's wrapped copy so that
v0.15.0b1,"after training wrapped featurizer is also trained, etc."
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.15.0b1,Fit a doubly robust average effect
v0.15.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.15.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.15.0b1,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.15.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.15.0b1,since we clone it and fit separate copies
v0.15.0b1,"If custom param grid, check that only estimator parameters are being altered"
v0.15.0b1,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.15.0b1,override only so that we can update the docstring to indicate support for `blb`
v0.15.0b1,Get input names
v0.15.0b1,Summary
v0.15.0b1,Determine output settings
v0.15.0b1,"Important: This must be the first invocation of the random state at fit time, so that"
v0.15.0b1,train/test splits are re-generatable from an external object simply by knowing the
v0.15.0b1,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.15.0b1,linear predictions. Currently is also useful for testing.
v0.15.0b1,reshape is necessary to preserve the data contiguity against vs
v0.15.0b1,"[:, np.newaxis] that does not."
v0.15.0b1,Check parameters
v0.15.0b1,Set min_weight_leaf from min_weight_fraction_leaf
v0.15.0b1,Build tree
v0.15.0b1,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.15.0b1,hold. Used by criterion for memory space savings.
v0.15.0b1,Initialize the criterion object and the criterion_val object if honest.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,
v0.15.0b1,This code is a fork from:
v0.15.0b1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
v0.15.0b1,published under the following license and copyright:
v0.15.0b1,BSD 3-Clause License
v0.15.0b1,
v0.15.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0b1,All rights reserved.
v0.15.0b1,Set parameters
v0.15.0b1,Don't instantiate estimators now! Parameters of base_estimator might
v0.15.0b1,"still change. Eg., when grid-searching with the nested object syntax."
v0.15.0b1,self.estimators_ needs to be filled by the derived classes in fit.
v0.15.0b1,Compute the number of jobs
v0.15.0b1,Partition estimators between jobs
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,covariance matrix
v0.15.0b1,get eigen value and eigen vectors
v0.15.0b1,simulate eigen vectors
v0.15.0b1,keep the top 4 eigen value and corresponding eigen vector
v0.15.0b1,replace the negative eigen values
v0.15.0b1,generate a new covariance matrix
v0.15.0b1,get linear approximation of eigen values
v0.15.0b1,coefs
v0.15.0b1,get the indices of each group of features
v0.15.0b1,print(ind_same_proxy)
v0.15.0b1,demo
v0.15.0b1,same proxy
v0.15.0b1,residuals
v0.15.0b1,gmm
v0.15.0b1,log normal on outliers
v0.15.0b1,positive outliers
v0.15.0b1,negative outliers
v0.15.0b1,demean the new residual again
v0.15.0b1,generate data
v0.15.0b1,sample residuals
v0.15.0b1,get prediction for current investment
v0.15.0b1,get prediction for current proxy
v0.15.0b1,get first period prediction
v0.15.0b1,iterate the step ahead contruction
v0.15.0b1,prepare new x
v0.15.0b1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.0b1,heterogeneity
v0.15.0b1,get new covariance matrix
v0.15.0b1,get coefs
v0.15.0b1,get residuals
v0.15.0b1,proxy 1 is the outcome
v0.15.0b1,make fixed residuals
v0.15.0b1,Remove children with nonwhite mothers from the treatment group
v0.15.0b1,Remove children with nonwhite mothers from the treatment group
v0.15.0b1,Select columns
v0.15.0b1,Scale the numeric variables
v0.15.0b1,"Change the binary variable 'first' takes values in {1,2}"
v0.15.0b1,Append a column of ones as intercept
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.15.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.15.0b1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.15.0b1,d_t=None here since we measure the effect across all Ts
v0.15.0b1,"y is a vector, rather than a 2D array"
v0.15.0b1,once the estimator has been fit
v0.15.0b1,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.15.0b1,an intercept even when bias is part of coef.
v0.15.0b1,We can write effect inference as a function of prediction and prediction standard error of
v0.15.0b1,the final method for linear models
v0.15.0b1,squeeze the first axis
v0.15.0b1,d_t=None here since we measure the effect across all Ts
v0.15.0b1,set the mean_pred_stderr
v0.15.0b1,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.15.0b1,squeeze the first axis
v0.15.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.15.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.15.0b1,"send treatment to the end, pull bounds to the front"
v0.15.0b1,d_t=None here since we measure the effect across all Ts
v0.15.0b1,set the mean_pred_stderr
v0.15.0b1,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.15.0b1,d_t=None here since we measure the effect across all Ts
v0.15.0b1,d_t=None here since we measure the effect across all Ts
v0.15.0b1,need to set the fit args before the estimator is fit
v0.15.0b1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.15.0b1,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.15.0b1,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.15.0b1,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.15.0b1,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.15.0b1,scale preds
v0.15.0b1,scale std errs
v0.15.0b1,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.15.0b1,offset preds
v0.15.0b1,"offset the distribution, too"
v0.15.0b1,scale preds
v0.15.0b1,"scale the distribution, too"
v0.15.0b1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.15.0b1,1. Uncertainty of Mean Point Estimate
v0.15.0b1,2. Distribution of Point Estimate
v0.15.0b1,3. Total Variance of Point Estimate
v0.15.0b1,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.15.0b1,so bail out early; note that it might be possible to correct the algorithm for
v0.15.0b1,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.15.0b1,be clean
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,TODO: Add a __dir__ implementation?
v0.15.0b1,don't proxy special methods
v0.15.0b1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.15.0b1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.15.0b1,then we should be computing a confidence interval for the wrapped calls
v0.15.0b1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.15.0b1,second level bootstrap which would be prohibitive computationally?
v0.15.0b1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.15.0b1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.15.0b1,can't import from econml.inference at top level without creating cyclical dependencies
v0.15.0b1,Note that inference results are always methods even if the inference is for a property
v0.15.0b1,(e.g. coef__inference() is a method but coef_ is a property)
v0.15.0b1,Therefore we must insert a lambda if getting inference for a non-callable
v0.15.0b1,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.15.0b1,"try to get interval/std first if appropriate,"
v0.15.0b1,since we don't prefer a wrapped method with this name
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,
v0.15.0b1,This code contains snippets of code from:
v0.15.0b1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.15.0b1,published under the following license and copyright:
v0.15.0b1,BSD 3-Clause License
v0.15.0b1,
v0.15.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0b1,All rights reserved.
v0.15.0b1,=============================================================================
v0.15.0b1,Types and constants
v0.15.0b1,=============================================================================
v0.15.0b1,=============================================================================
v0.15.0b1,Base GRF tree
v0.15.0b1,=============================================================================
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,=============================================================================
v0.15.0b1,A MultOutputWrapper for GRF classes
v0.15.0b1,=============================================================================
v0.15.0b1,=============================================================================
v0.15.0b1,Instantiations of Generalized Random Forest
v0.15.0b1,=============================================================================
v0.15.0b1,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.15.0b1,in front of the constant treatment is the intercept in the moment equation.
v0.15.0b1,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.15.0b1,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,
v0.15.0b1,This code contains snippets of code from
v0.15.0b1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.15.0b1,published under the following license and copyright:
v0.15.0b1,BSD 3-Clause License
v0.15.0b1,
v0.15.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.15.0b1,All rights reserved.
v0.15.0b1,=============================================================================
v0.15.0b1,Base Generalized Random Forest
v0.15.0b1,=============================================================================
v0.15.0b1,TODO: support freq_weight and sample_var
v0.15.0b1,Remap output
v0.15.0b1,reshape is necessary to preserve the data contiguity against vs
v0.15.0b1,"[:, np.newaxis] that does not."
v0.15.0b1,reshape is necessary to preserve the data contiguity against vs
v0.15.0b1,"[:, np.newaxis] that does not."
v0.15.0b1,Get subsample sample size
v0.15.0b1,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.15.0b1,We calculate the min eigenvalue proxy that each criterion is considering
v0.15.0b1,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.15.0b1,Check parameters
v0.15.0b1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.15.0b1,if this is the first `fit` call of the warm start mode.
v0.15.0b1,"Free allocated memory, if any"
v0.15.0b1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.15.0b1,We draw from the random state to get the random state we
v0.15.0b1,would have got if we hadn't used a warm_start.
v0.15.0b1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.15.0b1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.15.0b1,but would still advance randomness enough so that tree subsamples will be different.
v0.15.0b1,Generating indices a priori before parallelism ended up being orders of magnitude
v0.15.0b1,faster than how sklearn does it. The reason is that random samplers do not release the
v0.15.0b1,gil it seems.
v0.15.0b1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.15.0b1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.15.0b1,but would still advance randomness enough so that tree subsamples will be different.
v0.15.0b1,Parallel loop: we prefer the threading backend as the Cython code
v0.15.0b1,for fitting the trees is internally releasing the Python GIL
v0.15.0b1,making threading more efficient than multiprocessing in
v0.15.0b1,"that case. However, for joblib 0.12+ we respect any"
v0.15.0b1,"parallel_backend contexts set at a higher level,"
v0.15.0b1,since correctness does not rely on using threads.
v0.15.0b1,Collect newly grown trees
v0.15.0b1,Check data
v0.15.0b1,Assign chunk of trees to jobs
v0.15.0b1,avoid storing the output of every estimator by summing them here
v0.15.0b1,Parallel loop
v0.15.0b1,Check data
v0.15.0b1,Assign chunk of trees to jobs
v0.15.0b1,Parallel loop
v0.15.0b1,Check data
v0.15.0b1,Assign chunk of trees to jobs
v0.15.0b1,Parallel loop
v0.15.0b1,####################
v0.15.0b1,Variance correction
v0.15.0b1,####################
v0.15.0b1,Subtract the average within bag variance. This ends up being equal to the
v0.15.0b1,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.15.0b1,The negative part is just sq_between.
v0.15.0b1,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.15.0b1,"The off diagonals we have no objective prior, so no correction is applied."
v0.15.0b1,Finally correcting the pred_cov or pred_var
v0.15.0b1,avoid storing the output of every estimator by summing them here
v0.15.0b1,Parallel loop
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,TODO: update docs
v0.15.0b1,"NOTE: sample weight, sample var are not passed in"
v0.15.0b1,Compose final model
v0.15.0b1,Calculate auxiliary quantities
v0.15.0b1,X ⨂ T_res
v0.15.0b1,"sum(model_final.predict(X, T_res))"
v0.15.0b1,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.15.0b1,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
v0.15.0b1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0b1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0b1,"we need to set the number of periods before calling super()._prefit, since that will generate the"
v0.15.0b1,"final and nuisance models, which need to have self._n_periods set"
v0.15.0b1,Set _d_t to effective number of treatments
v0.15.0b1,Required for bootstrap inference
v0.15.0b1,for each mc iteration
v0.15.0b1,for each model under cross fit setting
v0.15.0b1,Handles the corner case when X=None but featurizer might be not None
v0.15.0b1,Expand treatments for each time period
v0.15.0b1,NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
v0.15.0b1,attribute so that the trained featurizer will be passed through
v0.15.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.15.0b1,for internal use by the library
v0.15.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.0b1,We need to use the _ortho_learner's copy to retain the information from fitting
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,testing importances
v0.15.0b1,testing heterogeneity importances
v0.15.0b1,Testing that all parameters do what they are supposed to
v0.15.0b1,"testing predict, apply and decision path"
v0.15.0b1,test that the subsampling scheme past to the trees is correct
v0.15.0b1,The sample size is chosen in particular to test rounding based error when subsampling
v0.15.0b1,test that the estimator calcualtes var correctly
v0.15.0b1,test api
v0.15.0b1,test accuracy
v0.15.0b1,test the projection functionality of forests
v0.15.0b1,test that the estimator calcualtes var correctly
v0.15.0b1,test api
v0.15.0b1,test that the estimator calcualtes var correctly
v0.15.0b1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.15.0b1,test that we raise errors in mishandled situations.
v0.15.0b1,test that the subsampling scheme past to the trees is correct
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
v0.15.0b1,omit the lalonde notebook
v0.15.0b1,make sure that coverage outputs reflect notebook contents
v0.15.0b1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.15.0b1,creating notebooks that are annoying for our users to actually run themselves
v0.15.0b1,"remove added coverage cell, then decrement execution_count for other cells to account for it"
v0.15.0b1,create directory if necessary
v0.15.0b1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.15.0b1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.15.0b1,"prior to calling interpret, can't plot, render, etc."
v0.15.0b1,can interpret without uncertainty
v0.15.0b1,can't interpret with uncertainty if inference wasn't used during fit
v0.15.0b1,can interpret with uncertainty if we refit
v0.15.0b1,can interpret without uncertainty
v0.15.0b1,can't treat before interpreting
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,for is_discrete in [False]:
v0.15.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.0b1,ensure we can serialize the unfit estimator
v0.15.0b1,ensure we can pickle the fit estimator
v0.15.0b1,make sure we can call the marginal_effect and effect methods
v0.15.0b1,test const marginal inference
v0.15.0b1,test effect inference
v0.15.0b1,test marginal effect inference
v0.15.0b1,test coef__inference and intercept__inference
v0.15.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0b1,"make sure we can call effect with implied scalar treatments,"
v0.15.0b1,"no matter the dimensions of T, and also that we warn when there"
v0.15.0b1,are multiple treatments
v0.15.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0b1,No heterogeneity
v0.15.0b1,Define indices to test
v0.15.0b1,Heterogeneous effects
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,simple DGP only for illustration
v0.15.0b1,Define the treatment model neural network architecture
v0.15.0b1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.15.0b1,"so the input shape is (d_z + d_x,)"
v0.15.0b1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.15.0b1,add extra layers on top for the mixture density network
v0.15.0b1,Define the response model neural network architecture
v0.15.0b1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.15.0b1,"so the input shape is (d_t + d_x,)"
v0.15.0b1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.15.0b1,"NOTE: For the response model, it is important to define the model *outside*"
v0.15.0b1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.15.0b1,so that the same weights will be reused in each instantiation
v0.15.0b1,number of samples to use in second estimate of the response
v0.15.0b1,(to make loss estimate unbiased)
v0.15.0b1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.15.0b1,do something with predictions...
v0.15.0b1,also test vector t and y
v0.15.0b1,simple DGP only for illustration
v0.15.0b1,Define the treatment model neural network architecture
v0.15.0b1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.15.0b1,"so the input shape is (d_z + d_x,)"
v0.15.0b1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.15.0b1,add extra layers on top for the mixture density network
v0.15.0b1,Define the response model neural network architecture
v0.15.0b1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.15.0b1,"so the input shape is (d_t + d_x,)"
v0.15.0b1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.15.0b1,"NOTE: For the response model, it is important to define the model *outside*"
v0.15.0b1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.15.0b1,so that the same weights will be reused in each instantiation
v0.15.0b1,number of samples to use in second estimate of the response
v0.15.0b1,(to make loss estimate unbiased)
v0.15.0b1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.15.0b1,do something with predictions...
v0.15.0b1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.15.0b1,test = True ensures we draw test set images
v0.15.0b1,test = True ensures we draw test set images
v0.15.0b1,re-draw to get new independent treatment and implied response
v0.15.0b1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.15.0b1,above is necesary so that reduced form doesn't win
v0.15.0b1,covariates: time and emotion
v0.15.0b1,random instrument
v0.15.0b1,z -> price
v0.15.0b1,true observable demand function
v0.15.0b1,errors
v0.15.0b1,response
v0.15.0b1,test = True ensures we draw test set images
v0.15.0b1,test = True ensures we draw test set images
v0.15.0b1,re-draw to get new independent treatment and implied response
v0.15.0b1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.15.0b1,above is necesary so that reduced form doesn't win
v0.15.0b1,covariates: time and emotion
v0.15.0b1,random instrument
v0.15.0b1,z -> price
v0.15.0b1,true observable demand function
v0.15.0b1,errors
v0.15.0b1,response
v0.15.0b1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.15.0b1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.15.0b1,For some reason this doesn't work at all when run against the CNTK backend...
v0.15.0b1,"model.compile('nadam', loss=lambda _,l:l)"
v0.15.0b1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.15.0b1,generate a valiation set
v0.15.0b1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.15.0b1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,DGP constants
v0.15.0b1,Generate data
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.15.0b1,pass sample weight to final step of pipeline
v0.15.0b1,create data with missing values
v0.15.0b1,model that can handle missing values
v0.15.0b1,"test X, W only"
v0.15.0b1,test W only
v0.15.0b1,dowhy does not support missing values in X
v0.15.0b1,assert that fitting with missing values fails when allow_missing is False
v0.15.0b1,and that setting allow_missing after init still works
v0.15.0b1,assert that we fail with a value error when we pass missing X to a model that doesn't support it
v0.15.0b1,assert that fitting with missing values fails when allow_missing is False
v0.15.0b1,and that setting allow_missing after init still works
v0.15.0b1,metalearners don't support W
v0.15.0b1,metalearners do support missing values in X
v0.15.0b1,dowhy never supports missing values in X
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,identity featurization effect functions
v0.15.0b1,polynomial featurization effect functions
v0.15.0b1,1d polynomial featurization functions
v0.15.0b1,2d-to-1d featurization functions
v0.15.0b1,2d-to-1d vector featurization functions
v0.15.0b1,use LassoCV rather than also selecting over RandomForests to save time
v0.15.0b1,test that treatment names are assigned for the featurized treatment
v0.15.0b1,expected shapes
v0.15.0b1,check effects
v0.15.0b1,ate
v0.15.0b1,loose inference checks
v0.15.0b1,temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons
v0.15.0b1,effect inference
v0.15.0b1,marginal effect inference
v0.15.0b1,const marginal effect inference
v0.15.0b1,fit a dummy estimator first so the featurizer can be fit to the treatment
v0.15.0b1,edge case with transformer that only takes a vector treatment
v0.15.0b1,so far will always return None for cate_treatment_names
v0.15.0b1,assert proper handling of improper feature names passed to certain transformers
v0.15.0b1,"depending on sklearn version, bad feature names either throws error or only uses first relevant name"
v0.15.0b1,ensure alpha is passed
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,testing importances
v0.15.0b1,testing heterogeneity importances
v0.15.0b1,Testing that all parameters do what they are supposed to
v0.15.0b1,"testing predict, apply and decision path"
v0.15.0b1,initialize parameters
v0.15.0b1,initialize config wtih base config and overwite some values
v0.15.0b1,predict tree using config parameters and assert
v0.15.0b1,shape of trained tree is the same as y_test
v0.15.0b1,initialize config wtih base honest config and overwite some values
v0.15.0b1,predict tree using config parameters and assert
v0.15.0b1,shape of trained tree is the same as y_test
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.15.0b1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.15.0b1,so we need to transpose the result
v0.15.0b1,1-d output
v0.15.0b1,2-d output
v0.15.0b1,Single dimensional output y
v0.15.0b1,compare with weight
v0.15.0b1,compare with weight
v0.15.0b1,compare with weight
v0.15.0b1,compare with weight
v0.15.0b1,Multi-dimensional output y
v0.15.0b1,1-d y
v0.15.0b1,compare when both sample_var and sample_weight exist
v0.15.0b1,multi-d y
v0.15.0b1,compare when both sample_var and sample_weight exist
v0.15.0b1,compare when both sample_var and sample_weight exist
v0.15.0b1,compare when both sample_var and sample_weight exist
v0.15.0b1,compare when both sample_var and sample_weight exist
v0.15.0b1,compare when both sample_var and sample_weight exist
v0.15.0b1,compare when both sample_var and sample_weight exist
v0.15.0b1,dgp
v0.15.0b1,StatsModels2SLS
v0.15.0b1,IV2SLS
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,test that we can fit with the same arguments as the base estimator
v0.15.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.15.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0b1,with the same shape for the lower and upper bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,test that we can do the same thing once we provide percentile bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.15.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0b1,with the same shape for the lower and upper bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,test that we can do the same thing once we provide percentile bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,test that we can fit with the same arguments as the base estimator
v0.15.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.15.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0b1,with the same shape for the lower and upper bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,test that we can do the same thing once we provide percentile bounds
v0.15.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.15.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0b1,with the same shape for the lower and upper bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,test that we can do the same thing once we provide percentile bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,test that we can fit with the same arguments as the base estimator
v0.15.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.15.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0b1,with the same shape for the lower and upper bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,test that we can do the same thing once we provide percentile bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.15.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0b1,with the same shape for the lower and upper bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,test that we can do the same thing once we provide percentile bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0b1,with the same shape for the lower and upper bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,test that the estimated effect is usually within the bounds
v0.15.0b1,test that we can do the same thing once we provide alpha explicitly
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,test that the estimated effect is usually within the bounds
v0.15.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.15.0b1,with the same shape for the lower and upper bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,TODO: test that the estimated effect is usually within the bounds
v0.15.0b1,and that the true effect is also usually within the bounds
v0.15.0b1,test that we can do the same thing once we provide percentile bounds
v0.15.0b1,test that the lower and upper bounds differ
v0.15.0b1,TODO: test that the estimated effect is usually within the bounds
v0.15.0b1,and that the true effect is also usually within the bounds
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,fixed functions as first stage models
v0.15.0b1,they can be anything as long as fitting doesn't modify the predictions
v0.15.0b1,"that way, it doesn't matter if they are trained on different subsets of the data"
v0.15.0b1,test coefficients
v0.15.0b1,test effects
v0.15.0b1,fixed functions as first stage models
v0.15.0b1,they can be anything as long as fitting doesn't modify the predictions
v0.15.0b1,"that way, it doesn't matter if they are trained on different subsets of the data"
v0.15.0b1,test coefficients
v0.15.0b1,test effects
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,test that the subsampling scheme past to the trees is correct
v0.15.0b1,test that the estimator calcualtes var correctly
v0.15.0b1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.15.0b1,test that we raise errors in mishandled situations.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,DGP constants
v0.15.0b1,Generate data
v0.15.0b1,Test inference results when `cate_feature_names` doesn not exist
v0.15.0b1,Test inference results when `cate_feature_names` doesn not exist
v0.15.0b1,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.15.0b1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.15.0b1,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.15.0b1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.15.0b1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.15.0b1,pvalue for second column should be greater than zero since some points are on either side
v0.15.0b1,of the tested value
v0.15.0b1,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.15.0b1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.15.0b1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.15.0b1,ensure alpha is passed
v0.15.0b1,only is not None when T1 is a constant or a list of constant
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Generate synthetic data
v0.15.0b1,Run _crossfit with Ray enabled
v0.15.0b1,Run _crossfit without Ray
v0.15.0b1,Compare the results
v0.15.0b1,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.15.0b1,Test non keyword based calls to fit
v0.15.0b1,test non-array inputs
v0.15.0b1,Test custom splitter
v0.15.0b1,Test incomplete set of test folds
v0.15.0b1,"y scores should be positive, since W predicts Y somewhat"
v0.15.0b1,"t scores might not be, since W and T are uncorrelated"
v0.15.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,make sure cross product varies more slowly with first array
v0.15.0b1,and that vectors are okay as inputs
v0.15.0b1,number of inputs in specification must match number of inputs
v0.15.0b1,must have an output
v0.15.0b1,output indices must be unique
v0.15.0b1,output indices must be present in an input
v0.15.0b1,number of indices must match number of dimensions for each input
v0.15.0b1,repeated indices must always have consistent sizes
v0.15.0b1,transpose
v0.15.0b1,tensordot
v0.15.0b1,trace
v0.15.0b1,TODO: set up proper flag for this
v0.15.0b1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.15.0b1,"of all of the distinct indices that appear in any input,"
v0.15.0b1,pick a random subset of them (of size at most 5) to appear in the output
v0.15.0b1,creating an instance should warn
v0.15.0b1,using the instance should not warn
v0.15.0b1,using the deprecated method should warn
v0.15.0b1,don't warn if b and c are passed by keyword
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,make any access to matplotlib or plt throw an exception
v0.15.0b1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.15.0b1,heterogeneity
v0.15.0b1,Invert indices to match latest API
v0.15.0b1,Invert indices to match latest API
v0.15.0b1,The feature for heterogeneity stays constant
v0.15.0b1,Auxiliary function for adding xticks and vertical lines when plotting results
v0.15.0b1,for dynamic dml vs ground truth parameters.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Preprocess data
v0.15.0b1,Convert 'week' to a date
v0.15.0b1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.15.0b1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.15.0b1,Take log of price
v0.15.0b1,Make brand numeric
v0.15.0b1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.15.0b1,which have all zero coeffs
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,test at least one estimator from each category
v0.15.0b1,test causal graph
v0.15.0b1,test refutation estimate
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.15.0b1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.15.0b1,TODO: test something rather than just print...
v0.15.0b1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.15.0b1,pick some arbitrary X
v0.15.0b1,pick some arbitrary T
v0.15.0b1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.15.0b1,Generate random Xs
v0.15.0b1,Random covariance matrix of Xs
v0.15.0b1,Effect of Xs on outcome
v0.15.0b1,Effect of treatment on outcomes
v0.15.0b1,Effect of treatment on outcome conditional on X1
v0.15.0b1,Generate treatments based on X and random noise
v0.15.0b1,"Generate Y (based on X, D, and random noise)"
v0.15.0b1,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.0b1,test the DR outcome difference
v0.15.0b1,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.0b1,test the DR outcome difference
v0.15.0b1,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.0b1,test the DR outcome difference
v0.15.0b1,use evaluate_blp to fit on validation only
v0.15.0b1,"Simple classifier and regressor for propensity, outcome, and cate"
v0.15.0b1,test the DR outcome difference
v0.15.0b1,fit nothing
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.15.0b1,The average variance should be lower when using monte carlo iterations
v0.15.0b1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.15.0b1,The average variance should be lower when using monte carlo iterations
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"since we're running so many combinations, just use LassoCV/LogisticRegressionCV"
v0.15.0b1,for the models instead of also selecting over random forest models
v0.15.0b1,ensure we can serialize unfit estimator
v0.15.0b1,ensure we can serialize fit estimator
v0.15.0b1,expected effect size
v0.15.0b1,test effect
v0.15.0b1,test inference
v0.15.0b1,only OrthoIV support inference other than bootstrap
v0.15.0b1,test summary
v0.15.0b1,test can run score
v0.15.0b1,test cate_feature_names
v0.15.0b1,test can run shap values
v0.15.0b1,dgp
v0.15.0b1,no heterogeneity
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"if we aren't fitting on the whole dataset, ensure that the limits are respected"
v0.15.0b1,ensure that the grouping has worked correctly and we get exactly the number of copies
v0.15.0b1,of the items in whichever groups we see
v0.15.0b1,DML nested CV works via a 'cv' attribute
v0.15.0b1,"want to validate the nested grouping, not the outer grouping in the nesting tests"
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,parameter combinations to test
v0.15.0b1,"we're running a lot of tests, so use fixed models instead of model selection"
v0.15.0b1,"IntentToTreatDRIV only supports binary treatments and instruments, and doesn't support fit_cov_directly"
v0.15.0b1,TODO: serializing/deserializing for every combination -- is this necessary?
v0.15.0b1,ensure we can serialize unfit estimator
v0.15.0b1,ensure we can serialize fit estimator
v0.15.0b1,expected effect size
v0.15.0b1,assert calculated constant marginal effect shape is expected
v0.15.0b1,const_marginal effect is defined in LinearCateEstimator class
v0.15.0b1,assert calculated marginal effect shape is expected
v0.15.0b1,test inference
v0.15.0b1,test can run score
v0.15.0b1,test cate_feature_names
v0.15.0b1,test can run shap values
v0.15.0b1,"dgp (binary T, binary Z)"
v0.15.0b1,no heterogeneity
v0.15.0b1,with heterogeneity
v0.15.0b1,fitting the covariance directly should be at least as good as computing the covariance from separate models
v0.15.0b1,set the models so that model selection over random forests doesn't take too much time in the repeated trials
v0.15.0b1,directly fitting the covariance should be better than indirectly fitting it
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Set random seed
v0.15.0b1,Generate data
v0.15.0b1,DGP constants
v0.15.0b1,Test data
v0.15.0b1,Constant treatment effect
v0.15.0b1,Constant treatment with multi output Y
v0.15.0b1,Heterogeneous treatment
v0.15.0b1,Heterogeneous treatment with multi output Y
v0.15.0b1,TLearner test
v0.15.0b1,Instantiate TLearner
v0.15.0b1,Test inputs
v0.15.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.0b1,Instantiate SLearner
v0.15.0b1,Test inputs
v0.15.0b1,Test constant treatment effect
v0.15.0b1,Test constant treatment effect with multi output Y
v0.15.0b1,Test heterogeneous treatment effect
v0.15.0b1,Need interactions between T and features
v0.15.0b1,Test heterogeneous treatment effect with multi output Y
v0.15.0b1,Instantiate XLearner
v0.15.0b1,Test inputs
v0.15.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.0b1,Instantiate DomainAdaptationLearner
v0.15.0b1,Test inputs
v0.15.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.0b1,Get the true treatment effect
v0.15.0b1,Get the true treatment effect
v0.15.0b1,Fit learner and get the effect and marginal effect
v0.15.0b1,Compute treatment effect residuals (absolute)
v0.15.0b1,Check that at least 90% of predictions are within tolerance interval
v0.15.0b1,Check whether the output shape is right
v0.15.0b1,Check that one can pass in regular lists
v0.15.0b1,Check that it fails correctly if lists of different shape are passed in
v0.15.0b1,"Check that it works when T, Y have shape (n, 1)"
v0.15.0b1,Generate covariates
v0.15.0b1,Generate treatment
v0.15.0b1,Calculate outcome
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,DGP constants
v0.15.0b1,Generate data
v0.15.0b1,Test data
v0.15.0b1,Remove warnings that might be raised by the models passed into the ORF
v0.15.0b1,Generate data with continuous treatments
v0.15.0b1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.15.0b1,does not work well with parallelism.
v0.15.0b1,Test inputs for continuous treatments
v0.15.0b1,--> Check that one can pass in regular lists
v0.15.0b1,--> Check that it fails correctly if lists of different shape are passed in
v0.15.0b1,Check that outputs have the correct shape
v0.15.0b1,Test continuous treatments with controls
v0.15.0b1,Test continuous treatments without controls
v0.15.0b1,Generate data with binary treatments
v0.15.0b1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.15.0b1,does not work well with parallelism.
v0.15.0b1,Test inputs for binary treatments
v0.15.0b1,--> Check that one can pass in regular lists
v0.15.0b1,--> Check that it fails correctly if lists of different shape are passed in
v0.15.0b1,"--> Check that it works when T, Y have shape (n, 1)"
v0.15.0b1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.15.0b1,--> Check that it fails correctly when the treatments are not numeric
v0.15.0b1,Check that outputs have the correct shape
v0.15.0b1,Test binary treatments with controls
v0.15.0b1,Test binary treatments without controls
v0.15.0b1,Only applicable to continuous treatments
v0.15.0b1,Generate data for 2 treatments
v0.15.0b1,Test multiple treatments with controls
v0.15.0b1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.15.0b1,The rest for controls. Just as an example.
v0.15.0b1,Generating A/B test data
v0.15.0b1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.15.0b1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.15.0b1,Create a wrapper around Lasso that doesn't support weights
v0.15.0b1,since Lasso does natively support them starting in sklearn 0.23
v0.15.0b1,Generate data with continuous treatments
v0.15.0b1,Instantiate model with most of the default parameters
v0.15.0b1,Compute the treatment effect on test points
v0.15.0b1,Compute treatment effect residuals
v0.15.0b1,Multiple treatments
v0.15.0b1,Allow at most 10% test points to be outside of the tolerance interval
v0.15.0b1,Compute treatment effect residuals
v0.15.0b1,Multiple treatments
v0.15.0b1,Allow at most 20% test points to be outside of the confidence interval
v0.15.0b1,Check that the intervals are not too wide
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.15.0b1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.15.0b1,ensure that we've got at least 6 of every element
v0.15.0b1,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.15.0b1,NOTE: this number may need to change if the default number of folds in
v0.15.0b1,WeightedStratifiedKFold changes
v0.15.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.0b1,ensure we can serialize the unfit estimator
v0.15.0b1,ensure we can pickle the fit estimator
v0.15.0b1,make sure we can call the marginal_effect and effect methods
v0.15.0b1,test const marginal inference
v0.15.0b1,test effect inference
v0.15.0b1,test marginal effect inference
v0.15.0b1,test coef__inference and intercept__inference
v0.15.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0b1,"make sure we can call effect with implied scalar treatments,"
v0.15.0b1,"no matter the dimensions of T, and also that we warn when there"
v0.15.0b1,are multiple treatments
v0.15.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0b1,ensure that we've got at least two of every element
v0.15.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.0b1,make sure we can call the marginal_effect and effect methods
v0.15.0b1,test const marginal inference
v0.15.0b1,test effect inference
v0.15.0b1,test marginal effect inference
v0.15.0b1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.15.0b1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.15.0b1,We concatenate the two copies data
v0.15.0b1,make sure we can get out post-fit stuff
v0.15.0b1,create a simple artificial setup where effect of moving from treatment
v0.15.0b1,"1 -> 2 is 2,"
v0.15.0b1,"1 -> 3 is 1, and"
v0.15.0b1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.15.0b1,"Using an uneven number of examples from different classes,"
v0.15.0b1,"and having the treatments in non-lexicographic order,"
v0.15.0b1,Should rule out some basic issues.
v0.15.0b1,test that we can fit with a KFold instance
v0.15.0b1,test that we can fit with a train/test iterable
v0.15.0b1,predetermined splits ensure that all features are seen in each split
v0.15.0b1,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.15.0b1,(incorrectly) use a final model with an intercept
v0.15.0b1,"Because final model is fixed, actual values of T and Y don't matter"
v0.15.0b1,Ensure reproducibility
v0.15.0b1,Sparse DGP
v0.15.0b1,Treatment effect coef
v0.15.0b1,Other coefs
v0.15.0b1,Features and controls
v0.15.0b1,Test sparse estimator
v0.15.0b1,"--> test coef_, intercept_"
v0.15.0b1,"with this DGP, since T depends linearly on X, Y depends on X quadratically"
v0.15.0b1,so we should use a quadratic featurizer
v0.15.0b1,--> test treatment effects
v0.15.0b1,Restrict x_test to vectors of norm < 1
v0.15.0b1,--> check inference
v0.15.0b1,Check that a majority of true effects lie in the 5-95% CI
v0.15.0b1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.15.0b1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.15.0b1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.15.0b1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.15.0b1,sparse test case: heterogeneous effect by product
v0.15.0b1,need at least as many rows in e_y as there are distinct columns
v0.15.0b1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.15.0b1,"we have quadratic terms in y, so we need to pipeline with a quadratic featurizer"
v0.15.0b1,Compare results with and without Ray
v0.15.0b1,create a simple artificial setup where effect of moving from treatment
v0.15.0b1,"a -> b is 2,"
v0.15.0b1,"a -> c is 1, and"
v0.15.0b1,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.15.0b1,"Using an uneven number of examples from different classes,"
v0.15.0b1,"and having the treatments in non-lexicographic order,"
v0.15.0b1,should rule out some basic issues.
v0.15.0b1,Note that explicitly specifying the dtype as object is necessary until
v0.15.0b1,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.15.0b1,estimated effects should be identical when treatment is explicitly given
v0.15.0b1,but const_marginal_effect should be reordered based on the explicit cagetories
v0.15.0b1,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.15.0b1,test outer grouping
v0.15.0b1,"with 2 folds, we should get exactly 3 groups per split, each with 10 copies of the y or t value"
v0.15.0b1,test nested grouping
v0.15.0b1,"with 2-fold outer and 2-fold inner grouping, and six total groups,"
v0.15.0b1,should get 1 or 2 groups per split
v0.15.0b1,"Try default, integer, and new user-passed treatment name"
v0.15.0b1,FunctionTransformers are agnostic to passed treatment names
v0.15.0b1,Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Set random seed
v0.15.0b1,Generate data
v0.15.0b1,DGP constants
v0.15.0b1,Test data
v0.15.0b1,Constant treatment effect and propensity
v0.15.0b1,Heterogeneous treatment and propensity
v0.15.0b1,ensure that we've got at least two of every element
v0.15.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.15.0b1,ensure that we can serialize unfit estimator
v0.15.0b1,ensure that we can serialize fit estimator
v0.15.0b1,make sure we can call the marginal_effect and effect methods
v0.15.0b1,test const marginal inference
v0.15.0b1,test effect inference
v0.15.0b1,test marginal effect inference
v0.15.0b1,test coef_ and intercept_ inference
v0.15.0b1,verify we can generate the summary
v0.15.0b1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.15.0b1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.15.0b1,create a simple artificial setup where effect of moving from treatment
v0.15.0b1,"1 -> 2 is 2,"
v0.15.0b1,"1 -> 3 is 1, and"
v0.15.0b1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.15.0b1,"Using an uneven number of examples from different classes,"
v0.15.0b1,"and having the treatments in non-lexicographic order,"
v0.15.0b1,Should rule out some basic issues.
v0.15.0b1,test that we can fit with a KFold instance
v0.15.0b1,test that we can fit with a train/test iterable
v0.15.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.15.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.15.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.15.0b1,test coef__inference function works
v0.15.0b1,test intercept__inference function works
v0.15.0b1,test summary function works
v0.15.0b1,Test inputs
v0.15.0b1,self._test_inputs(DR_learner)
v0.15.0b1,Test constant treatment effect
v0.15.0b1,Test heterogeneous treatment effect
v0.15.0b1,Test heterogenous treatment effect for W =/= None
v0.15.0b1,Sparse DGP
v0.15.0b1,Treatment effect coef
v0.15.0b1,Other coefs
v0.15.0b1,Features and controls
v0.15.0b1,Test sparse estimator
v0.15.0b1,"--> test coef_, intercept_"
v0.15.0b1,--> test treatment effects
v0.15.0b1,Restrict x_test to vectors of norm < 1
v0.15.0b1,--> check inference
v0.15.0b1,Check that a majority of true effects lie in the 5-95% CI
v0.15.0b1,test outer grouping
v0.15.0b1,NOTE: StratifiedGroupKFold has a bug when shuffle is True where it doesn't always stratify properly
v0.15.0b1,so we explicitly pass a StratifiedGroupKFold with shuffle=False (the default) rather than letting
v0.15.0b1,cross-fit generate one
v0.15.0b1,"with 2-fold grouping, we should get exactly 3 groups per split"
v0.15.0b1,test nested grouping
v0.15.0b1,"with 2-fold outer and 2-fold inner grouping, we should get 1-2 groups per split"
v0.15.0b1,helper class
v0.15.0b1,Fit learner and get the effect
v0.15.0b1,Get the true treatment effect
v0.15.0b1,Compute treatment effect residuals (absolute)
v0.15.0b1,Check that at least 90% of predictions are within tolerance interval
v0.15.0b1,Only for heterogeneous TE
v0.15.0b1,Fit learner on X and W and get the effect
v0.15.0b1,Get the true treatment effect
v0.15.0b1,Compute treatment effect residuals (absolute)
v0.15.0b1,Check that at least 90% of predictions are within tolerance interval
v0.15.0b1,Check that one can pass in regular lists
v0.15.0b1,Check that it fails correctly if lists of different shape are passed in
v0.15.0b1,Check that it fails when T contains values other than 0 and 1
v0.15.0b1,"Check that it works when T, Y have shape (n, 1)"
v0.15.0b1,Generate covariates
v0.15.0b1,Generate treatment
v0.15.0b1,Calculate outcome
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,DGP constants
v0.15.0b1,DGP coefficients
v0.15.0b1,Generated outcomes
v0.15.0b1,################
v0.15.0b1,WeightedLasso #
v0.15.0b1,################
v0.15.0b1,Define weights
v0.15.0b1,Define extended datasets
v0.15.0b1,Range of alphas
v0.15.0b1,Compare with Lasso
v0.15.0b1,--> No intercept
v0.15.0b1,--> With intercept
v0.15.0b1,When DGP has no intercept
v0.15.0b1,When DGP has intercept
v0.15.0b1,--> Coerce coefficients to be positive
v0.15.0b1,--> Toggle max_iter & tol
v0.15.0b1,Define weights
v0.15.0b1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.15.0b1,Mixed DGP scenario.
v0.15.0b1,Define extended datasets
v0.15.0b1,Define weights
v0.15.0b1,Define multioutput
v0.15.0b1,##################
v0.15.0b1,WeightedLassoCV #
v0.15.0b1,##################
v0.15.0b1,Define alphas to test
v0.15.0b1,Compare with LassoCV
v0.15.0b1,--> No intercept
v0.15.0b1,--> With intercept
v0.15.0b1,--> Force parameters to be positive
v0.15.0b1,Choose a smaller n to speed-up process
v0.15.0b1,Compare fold weights
v0.15.0b1,Define weights
v0.15.0b1,Define extended datasets
v0.15.0b1,Define splitters
v0.15.0b1,WeightedKFold splitter
v0.15.0b1,Map weighted splitter to an extended splitter
v0.15.0b1,Define alphas to test
v0.15.0b1,Compare with LassoCV
v0.15.0b1,--> No intercept
v0.15.0b1,--> With intercept
v0.15.0b1,--> Force parameters to be positive
v0.15.0b1,###########################
v0.15.0b1,MultiTaskWeightedLassoCV #
v0.15.0b1,###########################
v0.15.0b1,Define alphas to test
v0.15.0b1,Define splitter
v0.15.0b1,Compare with MultiTaskLassoCV
v0.15.0b1,--> No intercept
v0.15.0b1,--> With intercept
v0.15.0b1,Define weights
v0.15.0b1,Define extended datasets
v0.15.0b1,Define splitters
v0.15.0b1,WeightedKFold splitter
v0.15.0b1,Map weighted splitter to an extended splitter
v0.15.0b1,Define alphas to test
v0.15.0b1,Compare with LassoCV
v0.15.0b1,--> No intercept
v0.15.0b1,--> With intercept
v0.15.0b1,#########################
v0.15.0b1,WeightedLassoCVWrapper #
v0.15.0b1,#########################
v0.15.0b1,perform 1D fit
v0.15.0b1,perform 2D fit
v0.15.0b1,################
v0.15.0b1,DebiasedLasso #
v0.15.0b1,################
v0.15.0b1,Test DebiasedLasso without weights
v0.15.0b1,--> Check debiased coeffcients without intercept
v0.15.0b1,--> Check debiased coeffcients with intercept
v0.15.0b1,--> Check 5-95 CI coverage for unit vectors
v0.15.0b1,Test DebiasedLasso with weights for one DGP
v0.15.0b1,Define weights
v0.15.0b1,Define extended datasets
v0.15.0b1,--> Check debiased coefficients
v0.15.0b1,Define weights
v0.15.0b1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.15.0b1,--> Check debiased coeffcients
v0.15.0b1,Test that attributes propagate correctly
v0.15.0b1,Test MultiOutputDebiasedLasso without weights
v0.15.0b1,--> Check debiased coeffcients without intercept
v0.15.0b1,--> Check debiased coeffcients with intercept
v0.15.0b1,--> Check CI coverage
v0.15.0b1,Test MultiOutputDebiasedLasso with weights
v0.15.0b1,Define weights
v0.15.0b1,Define extended datasets
v0.15.0b1,--> Check debiased coefficients
v0.15.0b1,Unit vectors
v0.15.0b1,Unit vectors
v0.15.0b1,Check coeffcients and intercept are the same within tolerance
v0.15.0b1,Check results are similar with tolerance 1e-6
v0.15.0b1,Check if multitask
v0.15.0b1,Check that same alpha is chosen
v0.15.0b1,Check that the coefficients are similar
v0.15.0b1,selective ridge has a simple implementation that we can test against
v0.15.0b1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.15.0b1,"it should be the case that when we set fit_intercept to true,"
v0.15.0b1,it doesn't matter whether the penalized model also fits an intercept or not
v0.15.0b1,create an extra copy of rows with weight 2
v0.15.0b1,"instead of a slice, explicitly return an array of indices"
v0.15.0b1,_penalized_inds is only set during fitting
v0.15.0b1,cv exists on penalized model
v0.15.0b1,now we can access _penalized_inds
v0.15.0b1,check that we can read the cv attribute back out from the underlying model
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"global and cohort data should have exactly the same structure, but different values"
v0.15.0b1,local index should have as many times entries as global as there were rows passed in
v0.15.0b1,continuous treatments have typical treatment values equal to
v0.15.0b1,the mean of the absolute value of non-zero entries
v0.15.0b1,discrete treatments have typical treatment value 1
v0.15.0b1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.0b1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.0b1,policy value should exceed always treating with any treatment
v0.15.0b1,"global shape is (d_y, sum(d_t))"
v0.15.0b1,global and cohort row-wise dicts have d_y * d_t entries
v0.15.0b1,local dictionary is flattened to n_rows * d_y * d_t
v0.15.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0b1,features; for categoricals they should appear #cats-1 times each
v0.15.0b1,"global and cohort data should have exactly the same structure, but different values"
v0.15.0b1,local index should have as many times entries as global as there were rows passed in
v0.15.0b1,features; for categoricals they should appear #cats-1 times each
v0.15.0b1,"global shape is (d_y, sum(d_t))"
v0.15.0b1,global and cohort row-wise dicts have d_y * d_t entries
v0.15.0b1,local dictionary is flattened to n_rows * d_y * d_t
v0.15.0b1,continuous treatments have typical treatment values equal to
v0.15.0b1,the mean of the absolute value of non-zero entries
v0.15.0b1,discrete treatments have typical treatment value 1
v0.15.0b1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.0b1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.0b1,policy value should exceed always treating with any treatment
v0.15.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0b1,"global and cohort data should have exactly the same structure, but different values"
v0.15.0b1,local index should have as many times entries as global as there were rows passed in
v0.15.0b1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.0b1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.0b1,policy value should exceed always treating with any treatment
v0.15.0b1,"global shape is (d_y, sum(d_t))"
v0.15.0b1,global and cohort row-wise dicts have d_y * d_t entries
v0.15.0b1,local dictionary is flattened to n_rows * d_y * d_t
v0.15.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0b1,features; for categoricals they should appear #cats-1 times each
v0.15.0b1,make sure we don't run into problems dropping every index
v0.15.0b1,"global and cohort data should have exactly the same structure, but different values"
v0.15.0b1,local index should have as many times entries as global as there were rows passed in
v0.15.0b1,"global shape is (d_y, sum(d_t))"
v0.15.0b1,global and cohort row-wise dicts have d_y * d_t entries
v0.15.0b1,local dictionary is flattened to n_rows * d_y * d_t
v0.15.0b1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.0b1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.0b1,policy value should exceed always treating with any treatment
v0.15.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.15.0b1,"global and cohort data should have exactly the same structure, but different values"
v0.15.0b1,local index should have as many times entries as global as there were rows passed in
v0.15.0b1,features; for categoricals they should appear #cats-1 times each
v0.15.0b1,"global shape is (d_y, sum(d_t))"
v0.15.0b1,global and cohort row-wise dicts have d_y * d_t entries
v0.15.0b1,local dictionary is flattened to n_rows * d_y * d_t
v0.15.0b1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.15.0b1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.15.0b1,policy value should exceed always treating with any treatment
v0.15.0b1,dgp
v0.15.0b1,model
v0.15.0b1,model
v0.15.0b1,"columns 'd', 'e', 'h' have too many values"
v0.15.0b1,"columns 'd', 'e' have too many values"
v0.15.0b1,lowering bound shouldn't affect already fit columns when warm starting
v0.15.0b1,"column d is now okay, too"
v0.15.0b1,verify that we can use a scalar treatment cost
v0.15.0b1,verify that we can specify per-treatment costs for each sample
v0.15.0b1,verify that using the same state returns the same results each time
v0.15.0b1,set the categories for column 'd' explicitly so that b is default
v0.15.0b1,"first column: 10 ones, this is fine"
v0.15.0b1,"second column: 6 categories, plenty of random instances of each"
v0.15.0b1,this is fine only if we increase the cateogry limit
v0.15.0b1,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.15.0b1,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.15.0b1,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.15.0b1,forest heterogeneity won't work
v0.15.0b1,"sixth column: just 1 one, not enough even without check"
v0.15.0b1,increase bound on cat expansion
v0.15.0b1,skip checks (reducing folds accordingly)
v0.15.0b1,"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
v0.15.0b1,"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
v0.15.0b1,Pass an example where W is irrelevant and X is confounder
v0.15.0b1,"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
v0.15.0b1,zeroed out and the test will fail
v0.15.0b1,"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
v0.15.0b1,rescaling X shouldn't affect the first stage models because they normalize the inputs
v0.15.0b1,"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
v0.15.0b1,cross terms
v0.15.0b1,scale by 1000 to match the input to this model:
v0.15.0b1,"the scale of X does matter for the final model, which keeps results in user-denominated units"
v0.15.0b1,rescaling X still shouldn't affect the first stage models
v0.15.0b1,TODO: we don't recover the correct values with enough accuracy to enable this assertion
v0.15.0b1,is there a different way to verify that we are learning the correct coefficients?
v0.15.0b1,"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,DGP constants
v0.15.0b1,Define data features
v0.15.0b1,Added `_df`to names to be different from the default cate_estimator names
v0.15.0b1,Generate data
v0.15.0b1,################################
v0.15.0b1,Single treatment and outcome #
v0.15.0b1,################################
v0.15.0b1,Test LinearDML
v0.15.0b1,|--> Test featurizers
v0.15.0b1,"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
v0.15.0b1,|--> Test re-fit
v0.15.0b1,Test SparseLinearDML
v0.15.0b1,Test ForestDML
v0.15.0b1,###################################
v0.15.0b1,Mutiple treatments and outcomes #
v0.15.0b1,###################################
v0.15.0b1,Test LinearDML
v0.15.0b1,Test SparseLinearDML
v0.15.0b1,"Single outcome only, ORF does not support multiple outcomes"
v0.15.0b1,Test DMLOrthoForest
v0.15.0b1,Test DROrthoForest
v0.15.0b1,Test XLearner
v0.15.0b1,Skipping population summary names test because bootstrap inference is too slow
v0.15.0b1,Test SLearner
v0.15.0b1,Test TLearner
v0.15.0b1,Test LinearDRLearner
v0.15.0b1,Test SparseLinearDRLearner
v0.15.0b1,Test ForestDRLearner
v0.15.0b1,Test LinearIntentToTreatDRIV
v0.15.0b1,Test DeepIV
v0.15.0b1,Test categorical treatments
v0.15.0b1,Check refit
v0.15.0b1,Check refit after setting categories
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Linear models are required for parametric dml
v0.15.0b1,sample weighting models are required for nonparametric dml
v0.15.0b1,Test values
v0.15.0b1,TLearner test
v0.15.0b1,Instantiate TLearner
v0.15.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.15.0b1,Test constant treatment effect with multi output Y
v0.15.0b1,Test heterogeneous treatment effect
v0.15.0b1,Need interactions between T and features
v0.15.0b1,Test heterogeneous treatment effect with multi output Y
v0.15.0b1,Instantiate DomainAdaptationLearner
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,test base values equals to mean of constant marginal effect
v0.15.0b1,test shape of shap values output is as expected
v0.15.0b1,test shape of attribute of explanation object is as expected
v0.15.0b1,test length of feature names equals to shap values shape
v0.15.0b1,test base values equals to mean of constant marginal effect
v0.15.0b1,test shape of shap values output is as expected
v0.15.0b1,test shape of attribute of explanation object is as expected
v0.15.0b1,test length of feature names equals to shap values shape
v0.15.0b1,import here since otherwise test collection would fail if matplotlib is not installed
v0.15.0b1,Treatment effect function
v0.15.0b1,Outcome support
v0.15.0b1,Treatment support
v0.15.0b1,"Generate controls, covariates, treatments and outcomes"
v0.15.0b1,Heterogeneous treatment effects
v0.15.0b1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.15.0b1,through shap package.
v0.15.0b1,test shap could generate the plot from the shap_values
v0.15.0b1,"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Check inputs
v0.15.0b1,Check inputs
v0.15.0b1,Check inputs
v0.15.0b1,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.15.0b1,"Thus, we append the controls column before the one-hot-encoded T"
v0.15.0b1,"We might want to revisit, though, since it's linearly determined by the others"
v0.15.0b1,Check inputs
v0.15.0b1,Check inputs
v0.15.0b1,Estimate response function
v0.15.0b1,Check inputs
v0.15.0b1,Train model on controls. Assign higher weight to units resembling
v0.15.0b1,treated units.
v0.15.0b1,Train model on the treated. Assign higher weight to units resembling
v0.15.0b1,control units.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,TODO: make sure to use random seeds wherever necessary
v0.15.0b1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.15.0b1,"unfortunately with the Theano and Tensorflow backends,"
v0.15.0b1,the straightforward use of K.stop_gradient can cause an error
v0.15.0b1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.15.0b1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.15.0b1,so that those layers remain connected but with 0 gradient
v0.15.0b1,|| t - mu_i || ^2
v0.15.0b1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.15.0b1,Use logsumexp for numeric stability:
v0.15.0b1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.15.0b1,TODO: does the numeric stability actually make any difference?
v0.15.0b1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.15.0b1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.15.0b1,generate cumulative sum via matrix multiplication
v0.15.0b1,"Generate standard uniform values in shape (batch_size,1)"
v0.15.0b1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.15.0b1,we use uniform_like instead with an input of an appropriate shape)
v0.15.0b1,convert to floats and multiply to perform equivalent of logical AND
v0.15.0b1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.15.0b1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.15.0b1,we use normal_like instead with an input of an appropriate shape)
v0.15.0b1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.15.0b1,prevent gradient from passing through sampling
v0.15.0b1,three options: biased or upper-bound loss require a single number of samples;
v0.15.0b1,unbiased can take different numbers for the network and its gradient
v0.15.0b1,"sample: (() -> Layer, int) -> Layer"
v0.15.0b1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.15.0b1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.15.0b1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.15.0b1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.15.0b1,the dimensionality of the output of the network
v0.15.0b1,TODO: is there a more robust way to do this?
v0.15.0b1,TODO: do we need to give the user more control over other arguments to fit?
v0.15.0b1,"subtle point: we need to build a new model each time,"
v0.15.0b1,because each model encapsulates its randomness
v0.15.0b1,TODO: do we need to give the user more control over other arguments to fit?
v0.15.0b1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.15.0b1,not a general tensor (because of how backprop works in every framework)
v0.15.0b1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.15.0b1,but this seems annoying...)
v0.15.0b1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.15.0b1,TODO: any way to get this to work on batches of arbitrary size?
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.15.0b1,"need to fit, too, since we call predict later inside this train method"
v0.15.0b1,"need to fit, too, since we call predict later inside this train method"
v0.15.0b1,"need to fit, too, since we call predict later inside this train method"
v0.15.0b1,"We're projecting, so we're treating E[T|X,Z] as the instrument (ignoring W for simplicity)"
v0.15.0b1,"Then beta(X) = E[T̃ (E[T|X,Z]-E[E[T|X,Z]|X)|X] and we can apply the tower rule several times to get"
v0.15.0b1,"= E[(E[T|X,Z]-E[T|X])^2|X]"
v0.15.0b1,"and also     = E[(E[T|X,Z]-T)^2|X]"
v0.15.0b1,so we can compute it either from (T_proj-T_pred)^2 or from (T_proj-T)^2
v0.15.0b1,"return shape (n,)"
v0.15.0b1,"need to avoid erroneous broadcasting when one of Z_res or T_res is (n,1) and the other is (n,)"
v0.15.0b1,"TODO: if the T and Z models overfit, then this will be biased towards 0;"
v0.15.0b1,consider using nested cross-fitting
v0.15.0b1,a similar comment applies to the projection case
v0.15.0b1,"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
v0.15.0b1,"shape (n,)"
v0.15.0b1,"shape (n,)"
v0.15.0b1,"shape(n,)"
v0.15.0b1,TODO: prel_model_effect could allow sample_var and freq_weight?
v0.15.0b1,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.15.0b1,"we need to undo the one-hot encoding for calling effect,"
v0.15.0b1,since it expects raw values
v0.15.0b1,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.15.0b1,target will be discrete and will be inversed from FirstStageWrapper
v0.15.0b1,"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
v0.15.0b1,reshape the predictions
v0.15.0b1,concat W and Z
v0.15.0b1,"in the projection case, this is a variance and should always be non-negative"
v0.15.0b1,check nuisances outcome shape
v0.15.0b1,Y_res could be a vector or 1-dimensional 2d-array
v0.15.0b1,"all could be reshaped to vector since Y, T, Z are all single dimensional."
v0.15.0b1,Estimate final model of theta(X) by minimizing the square loss:
v0.15.0b1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.15.0b1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.15.0b1,at the expense of some small bias. For points with very small covariance we revert
v0.15.0b1,to the model-based preliminary estimate and do not add the correction term.
v0.15.0b1,A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
v0.15.0b1,Used by both DRIV and IntentToTreatDRIV.
v0.15.0b1,Maggie: I think that would be the case?
v0.15.0b1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.0b1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.15.0b1,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.15.0b1,attribute so that the trained featurizer will be passed through
v0.15.0b1,Handles the corner case when X=None but featurizer might be not None
v0.15.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.0b1,"this is a regression model since the instrument E[T|X,W,Z] is always continuous"
v0.15.0b1,"we're using E[T|X,W,Z] as the instrument"
v0.15.0b1,Define the data generation functions
v0.15.0b1,Define the data generation functions
v0.15.0b1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.0b1,Define the data generation functions
v0.15.0b1,TODO: support freq_weight and sample_var in debiased lasso
v0.15.0b1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.0b1,Define the data generation functions
v0.15.0b1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.0b1,concat W and Z
v0.15.0b1,"we need to undo the one-hot encoding for calling effect,"
v0.15.0b1,since it expects raw values
v0.15.0b1,concat W and Z
v0.15.0b1,"we need to undo the one-hot encoding for calling effect,"
v0.15.0b1,since it expects raw values
v0.15.0b1,reshape the predictions
v0.15.0b1,"T_res, Z_res, beta expect shape to be (n,1)"
v0.15.0b1,Define the data generation functions
v0.15.0b1,maybe shouldn't expose fit_cate_intercept in this class?
v0.15.0b1,Define the data generation functions
v0.15.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.15.0b1,TODO: do correct adjustment for sample_var
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,concat W and Z
v0.15.0b1,concat W and Z
v0.15.0b1,concat W and Z
v0.15.0b1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.15.0b1,Define the data generation functions
v0.15.0b1,"train E[T|X,W,Z]"
v0.15.0b1,"train E[Z|X,W]"
v0.15.0b1,note: discrete_instrument rather than discrete_treatment in call to _make_first_stage_selector
v0.15.0b1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.15.0b1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.15.0b1,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.15.0b1,attribute so that the trained featurizer will be passed through
v0.15.0b1,Handles the corner case when X=None but featurizer might be not None
v0.15.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.0b1,concat W and Z
v0.15.0b1,note that groups are not passed to score because they are only used for fitting
v0.15.0b1,concat W and Z
v0.15.0b1,note that sample_weight and groups are not passed to predict because they are only used for fitting
v0.15.0b1,concat W and Z
v0.15.0b1,A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
v0.15.0b1,Used by both Parametric and Non Parametric DMLIV.
v0.15.0b1,override only so that we can enforce Z to be required
v0.15.0b1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.15.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.15.0b1,for internal use by the library
v0.15.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.15.0b1,Handles the corner case when X=None but featurizer might be not None
v0.15.0b1,Define the data generation functions
v0.15.0b1,Get input names
v0.15.0b1,Summary
v0.15.0b1,coefficient
v0.15.0b1,intercept
v0.15.0b1,Define the data generation functions
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,"this will have dimension (d,) + shape(X)"
v0.15.0b1,send the first dimension to the end
v0.15.0b1,columns are featurized independently; partial derivatives are only non-zero
v0.15.0b1,when taken with respect to the same column each time
v0.15.0b1,don't fit intercept; manually add column of ones to the data instead;
v0.15.0b1,this allows us to ignore the intercept when computing marginal effects
v0.15.0b1,make T 2D if if was a vector
v0.15.0b1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.15.0b1,two stage approximation
v0.15.0b1,"first, get basis expansions of T, X, and Z"
v0.15.0b1,TODO: is it right that the effective number of intruments is the
v0.15.0b1,"product of ft_X and ft_Z, not just ft_Z?"
v0.15.0b1,"regress T expansion on X,Z expansions concatenated with W"
v0.15.0b1,"predict ft_T from interacted ft_X, ft_Z"
v0.15.0b1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.15.0b1,dT may be only 2-dimensional)
v0.15.0b1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.15.0b1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,TODO: this utility is documented but internal; reimplement?
v0.15.0b1,TODO: this utility is even less public...
v0.15.0b1,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.15.0b1,use same Cs as would be used by default by LogisticRegressionCV
v0.15.0b1,NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
v0.15.0b1,which could affect how many times each distinct Y value needs to be present in the data
v0.15.0b1,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.15.0b1,but also supports get_feature_names with expected signature
v0.15.0b1,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.15.0b1,NOTE: we rely on the passthrough columns coming first in the concatenated X;W
v0.15.0b1,"when we pipeline scaling with our first stage models later, so the order here is important"
v0.15.0b1,TODO: remove once older sklearn support is no longer needed
v0.15.0b1,Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
v0.15.0b1,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.15.0b1,Convert SingleTreeInterpreter to a python dictionary
v0.15.0b1,named tuple type for storing results inside CausalAnalysis class;
v0.15.0b1,must be lifted to module level to enable pickling
v0.15.0b1,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
v0.15.0b1,"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
v0.15.0b1,"when running the first stage models, but don't want to scale the X columns when running the final model,"
v0.15.0b1,since then our coefficients will have odd units and our trees will also have decisions using those units.
v0.15.0b1,
v0.15.0b1,"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
v0.15.0b1,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.15.0b1,Controls are all other columns of X
v0.15.0b1,"can't use X[:, feat_ind] when X is a DataFrame"
v0.15.0b1,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.15.0b1,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.15.0b1,so that the user can opt-in to allowing unseen treatment values
v0.15.0b1,(and return NaN or something in that case)
v0.15.0b1,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
v0.15.0b1,and so we can just peel the first columns off of that combined array for rescaling in the pipeline
v0.15.0b1,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
v0.15.0b1,"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
v0.15.0b1,becomes a valid approach to handling this
v0.15.0b1,array checking routines don't accept 0-width arrays
v0.15.0b1,perform model selection
v0.15.0b1,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.15.0b1,convert to NormalInferenceResults for consistency
v0.15.0b1,Set the dictionary values shared between local and global summaries
v0.15.0b1,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.15.0b1,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.15.0b1,required to fit a discrete DML model
v0.15.0b1,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
v0.15.0b1,sub-cases of models or also integrate with azure autoML. (post-MVP)
v0.15.0b1,"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
v0.15.0b1,"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
v0.15.0b1,TODO: Enable multi-class classification (post-MVP)
v0.15.0b1,Validate inputs
v0.15.0b1,TODO: check compatibility of X and Y lengths
v0.15.0b1,"no previous fit, cancel warm start"
v0.15.0b1,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.15.0b1,"if heterogeneity_inds is 1D, repeat it"
v0.15.0b1,heterogeneity inds should be a 2D list of length same as train_inds
v0.15.0b1,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.15.0b1,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.15.0b1,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.15.0b1,train the Y model
v0.15.0b1,"perform model selection for the Y model using all X, not on a per-column basis"
v0.15.0b1,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.15.0b1,work with the regression wrapper
v0.15.0b1,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.15.0b1,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.15.0b1,since otherwise we'll have too many columns to be able to train a classifier
v0.15.0b1,start with empty results and default shared insights
v0.15.0b1,convert categorical indicators to numeric indices
v0.15.0b1,check for indices over the categorical expansion bound
v0.15.0b1,assume we'll be able to train former failures this time; we'll add them back if not
v0.15.0b1,"can't remove in place while iterating over new_inds, so store in separate list"
v0.15.0b1,"train the model, but warn"
v0.15.0b1,no model can be trained in this case since we need more folds
v0.15.0b1,"don't train a model, but suggest workaround since there are enough instances of least"
v0.15.0b1,populated class
v0.15.0b1,also remove from train_inds so we don't try to access the result later
v0.15.0b1,extract subset of names matching new columns
v0.15.0b1,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.15.0b1,don't want to cache this failed result
v0.15.0b1,properties to return from effect InferenceResults
v0.15.0b1,properties to return from PopulationSummaryResults
v0.15.0b1,Converts strings to property lookups or method calls as a convenience so that the
v0.15.0b1,_point_props and _summary_props above can be applied to an inference object
v0.15.0b1,Create a summary combining all results into a single output; this is used
v0.15.0b1,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.15.0b1,"or a dictionary, respectively, based on the summary function passed into this method"
v0.15.0b1,"ensure array has shape (m,y,t)"
v0.15.0b1,population summary is missing sample dimension; add it for consistency
v0.15.0b1,outcome dimension is missing; add it for consistency
v0.15.0b1,add singleton treatment dimension if missing
v0.15.0b1,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.15.0b1,"each attr has dimension (m,y) or (m,y,t)"
v0.15.0b1,concatenate along treatment dimension
v0.15.0b1,"for dictionary representation, want to remove unneeded sample dimension"
v0.15.0b1,in cohort and global results
v0.15.0b1,TODO: enrich outcome logic for multi-class classification when that is supported
v0.15.0b1,There is no actual sample level in this data
v0.15.0b1,can't drop only level
v0.15.0b1,should be serialization-ready and contain no numpy arrays
v0.15.0b1,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.15.0b1,TODO: Note that there's no column metadata for the sample number - should there be?
v0.15.0b1,"need to replicate the column info for each sample, then remove from the shared data"
v0.15.0b1,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.15.0b1,which may need to be revisited once we support multiclass
v0.15.0b1,get the length of the list corresponding to the first dictionary key
v0.15.0b1,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.15.0b1,a global inference indicates the effect of that one feature on the outcome
v0.15.0b1,need to reshape the output to match the input
v0.15.0b1,we want to offset the inference object by the baseline estimate of y
v0.15.0b1,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.15.0b1,get the length of the list corresponding to the first dictionary key
v0.15.0b1,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.15.0b1,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.15.0b1,because then scaling the difference between treatment value and treatment costs is the
v0.15.0b1,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.15.0b1,
v0.15.0b1,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.15.0b1,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.15.0b1,(rather than just not treating at all)
v0.15.0b1,
v0.15.0b1,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.15.0b1,2 * policy_value - always_treat
v0.15.0b1,includes exactly their contribution because policy_value and always_treat both include it
v0.15.0b1,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.15.0b1,2 * policy_value - always-treat
v0.15.0b1,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.15.0b1,is zero and the contribution to always_treat is negative
v0.15.0b1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.15.0b1,"however, the tree can't store the feature and treatment names we compute here..."
v0.15.0b1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.15.0b1,"however, the tree can't store the feature and treatment names we compute here..."
v0.15.0b1,get dataframe with all but selected column
v0.15.0b1,apply 10% of a typical treatment for this feature
v0.15.0b1,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.15.0b1,set the effect bounds; for positive treatments these agree with
v0.15.0b1,"the estimates; for negative treatments, we need to invert the interval"
v0.15.0b1,the effect is now always positive since we decrease treatment when negative
v0.15.0b1,"for discrete treatment, stack a zero result in front for control"
v0.15.0b1,we need to call effect_inference to get the correct CI between the two treatment options
v0.15.0b1,we now need to construct the delta in the cost between the two treatments and translate the effect
v0.15.0b1,remove third dimenions potentially added
v0.15.0b1,"find cost of current treatment: equality creates a 2d array with True on each row,"
v0.15.0b1,only if its the location of the current treatment. Then we take the corresponding cost.
v0.15.0b1,construct index of current treatment
v0.15.0b1,add second dimension if needed for broadcasting during translation of effect
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,TODO: conisder working around relying on sklearn implementation details
v0.15.0b1,"Found a good split, return."
v0.15.0b1,Record all splits in case the stratification by weight yeilds a worse partition
v0.15.0b1,Reseed random generator and try again
v0.15.0b1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.15.0b1,"Found a good split, return."
v0.15.0b1,Did not find a good split
v0.15.0b1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.15.0b1,Return most weight-balanced partition
v0.15.0b1,Weight stratification algorithm
v0.15.0b1,Sort weights for weight strata search
v0.15.0b1,There are some leftover indices that have yet to be assigned
v0.15.0b1,Append stratum splits to overall splits
v0.15.0b1,logic copied from check_cv
v0.15.0b1,otherwise we will assume the user already set the cv attribute to something
v0.15.0b1,compatible with splitting with a `groups` argument
v0.15.0b1,"drop groups from arg list, which were already used at the outer level and may not be supported by the model"
v0.15.0b1,"whether selecting or not, need to train the model on the data"
v0.15.0b1,"TODO: we need to alter this to use out-of-sample score here, which"
v0.15.0b1,"will require cross-validation, but should respect grouping, stratifying, etc."
v0.15.0b1,copy common parameters
v0.15.0b1,copy common fitted variables
v0.15.0b1,copy attributes unique to this class
v0.15.0b1,make sure all classes agree on best c/l1 combo
v0.15.0b1,"l1 ratio doesn't apply to Lasso, only ElasticNet"
v0.15.0b1,don't need to use _fit_with_groups here since none of these models support it
v0.15.0b1,"If classification methods produce multiple columns of output,"
v0.15.0b1,we need to manually encode classes to ensure consistent column ordering.
v0.15.0b1,We clone the estimator to make sure that all the folds are
v0.15.0b1,"independent, and that it is pickle-able."
v0.15.0b1,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.15.0b1,`predictions` is a list of method outputs from each fold.
v0.15.0b1,"If each of those is also a list, then treat this as a"
v0.15.0b1,multioutput-multiclass task. We need to separately concatenate
v0.15.0b1,the method outputs for each label into an `n_labels` long list.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Our classes that derive from sklearn ones sometimes include
v0.15.0b1,inherited docstrings that have embedded doctests; we need the following imports
v0.15.0b1,so that they don't break.
v0.15.0b1,TODO: consider working around relying on sklearn implementation details
v0.15.0b1,local import to avoid circular imports
v0.15.0b1,"Convert X, y into numpy arrays"
v0.15.0b1,Define fit parameters
v0.15.0b1,Some algorithms don't have a check_input option
v0.15.0b1,Check weights array
v0.15.0b1,Check that weights are size-compatible
v0.15.0b1,Normalize inputs
v0.15.0b1,Weight inputs
v0.15.0b1,Fit base class without intercept
v0.15.0b1,Fit Lasso
v0.15.0b1,Reset intercept
v0.15.0b1,The intercept is not calculated properly due the sqrt(weights) factor
v0.15.0b1,so it must be recomputed
v0.15.0b1,Fit lasso without weights
v0.15.0b1,Make weighted splitter
v0.15.0b1,Fit weighted model
v0.15.0b1,Make weighted splitter
v0.15.0b1,Fit weighted model
v0.15.0b1,Call weighted lasso on reduced design matrix
v0.15.0b1,Weighted tau
v0.15.0b1,Select optimal penalty
v0.15.0b1,Warn about consistency
v0.15.0b1,"Convert X, y into numpy arrays"
v0.15.0b1,Fit weighted lasso with user input
v0.15.0b1,"Center X, y"
v0.15.0b1,Calculate quantities that will be used later on. Account for centered data
v0.15.0b1,Calculate coefficient and error variance
v0.15.0b1,Add coefficient correction
v0.15.0b1,Set coefficients and intercept standard errors
v0.15.0b1,Set intercept
v0.15.0b1,Return alpha to 'auto' state
v0.15.0b1,"Note that in the case of no intercept, X_offset is 0"
v0.15.0b1,Calculate the variance of the predictions
v0.15.0b1,Calculate prediction confidence intervals
v0.15.0b1,Assumes flattened y
v0.15.0b1,Compute weighted residuals
v0.15.0b1,To be done once per target. Assumes y can be flattened.
v0.15.0b1,Assumes that X has already been offset
v0.15.0b1,Special case: n_features=1
v0.15.0b1,Compute Lasso coefficients for the columns of the design matrix
v0.15.0b1,Compute C_hat
v0.15.0b1,Compute theta_hat
v0.15.0b1,Allow for single output as well
v0.15.0b1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.15.0b1,Set coef_ attribute
v0.15.0b1,Set intercept_ attribute
v0.15.0b1,Set selected_alpha_ attribute
v0.15.0b1,Set coef_stderr_
v0.15.0b1,intercept_stderr_
v0.15.0b1,set model to the single-target estimator by default so there's always a model to get and set attributes on
v0.15.0b1,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.15.0b1,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.15.0b1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.15.0b1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.15.0b1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.15.0b1,now regress X1 on y - X2 * beta2 to learn beta1
v0.15.0b1,set coef_ and intercept_ attributes
v0.15.0b1,Note that the penalized model should *not* have an intercept
v0.15.0b1,don't proxy special methods
v0.15.0b1,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.15.0b1,regressor incorrectly
v0.15.0b1,"Note: for known attributes that have been set this method will not be called,"
v0.15.0b1,so we should just throw here because this is an attribute belonging to this class
v0.15.0b1,but which hasn't yet been set on this instance
v0.15.0b1,set default values for None
v0.15.0b1,check freq_weight should be integer and should be accompanied by sample_var
v0.15.0b1,check array shape
v0.15.0b1,weight X and y and sample_var
v0.15.0b1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.15.0b1,"For aggregation calculations, always treat wy as an array so that einsum expressions don't need to change"
v0.15.0b1,We'll collapse results back down afterwards if necessary
v0.15.0b1,"for federation, we need to store these 5 arrays when using heteroskedasticity-robust inference"
v0.15.0b1,y dimension is always first in the output when present so that broadcasting works correctly
v0.15.0b1,set default values for None
v0.15.0b1,check array shape
v0.15.0b1,check dimension of instruments is more than dimension of treatments
v0.15.0b1,weight X and y
v0.15.0b1,learn point estimate
v0.15.0b1,solve first stage linear regression E[T|Z]
v0.15.0b1,"""that"" means T̂"
v0.15.0b1,solve second stage linear regression E[Y|that]
v0.15.0b1,(T̂.T*T̂)^{-1}
v0.15.0b1,learn cov(theta)
v0.15.0b1,(T̂.T*T̂)^{-1}
v0.15.0b1,sigma^2
v0.15.0b1,reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,AzureML
v0.15.0b1,helper imports
v0.15.0b1,write the details of the workspace to a configuration file to the notebook library
v0.15.0b1,if y is a multioutput model
v0.15.0b1,Make sure second dimension has 1 or more item
v0.15.0b1,switch _inner Model to a MultiOutputRegressor
v0.15.0b1,flatten array as automl only takes vectors for y
v0.15.0b1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.15.0b1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.15.0b1,as an sklearn estimator
v0.15.0b1,fit implementation for a single output model.
v0.15.0b1,Create experiment for specified workspace
v0.15.0b1,Configure automl_config with training set information.
v0.15.0b1,"Wait for remote run to complete, the set the model"
v0.15.0b1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.15.0b1,create model and pass model into final.
v0.15.0b1,"If item is an automl config, get its corresponding"
v0.15.0b1,AutomatedML Model and add it to new_Args
v0.15.0b1,"If item is an automl config, get its corresponding"
v0.15.0b1,AutomatedML Model and set it for this key in
v0.15.0b1,kwargs
v0.15.0b1,takes in either automated_ml config and instantiates
v0.15.0b1,an AutomatedMLModel
v0.15.0b1,The prefix can only be 18 characters long
v0.15.0b1,"because prefixes come from kwarg_names, we must ensure they are"
v0.15.0b1,short enough.
v0.15.0b1,Get workspace from config file.
v0.15.0b1,Take the intersect of the white for sample
v0.15.0b1,weights and linear models
v0.15.0b1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,average the outcome dimension if it exists and ensure 2d y_pred
v0.15.0b1,get index of best treatment
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,TODO: consider working around relying on sklearn implementation details
v0.15.0b1,Create splits of causal tree
v0.15.0b1,Make sure the correct exception is being rethrown
v0.15.0b1,Must make sure indices are merged correctly
v0.15.0b1,Convert rows to columns
v0.15.0b1,Require group assignment t to be one-hot-encoded
v0.15.0b1,Get predictions for the 2 splits
v0.15.0b1,Must make sure indices are merged correctly
v0.15.0b1,Crossfitting
v0.15.0b1,Compute weighted nuisance estimates
v0.15.0b1,-------------------------------------------------------------------------------
v0.15.0b1,Calculate the covariance matrix corresponding to the BLB inference
v0.15.0b1,
v0.15.0b1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.15.0b1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.15.0b1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.15.0b1,in that slice from the overall parameter estimate.
v0.15.0b1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.15.0b1,-------------------------------------------------------------------------------
v0.15.0b1,Calclulate covariance matrix through BLB
v0.15.0b1,Estimators
v0.15.0b1,OrthoForest parameters
v0.15.0b1,Sub-forests
v0.15.0b1,Auxiliary attributes
v0.15.0b1,Fit check
v0.15.0b1,TODO: Check performance
v0.15.0b1,Must normalize weights
v0.15.0b1,Override the CATE inference options
v0.15.0b1,Add blb inference to parent's options
v0.15.0b1,Generate subsample indices
v0.15.0b1,Build trees in parallel
v0.15.0b1,Bootstraping has repetitions in tree sample
v0.15.0b1,Similar for `a` weights
v0.15.0b1,Bootstraping has repetitions in tree sample
v0.15.0b1,Define subsample size
v0.15.0b1,Safety check
v0.15.0b1,Draw points to create little bags
v0.15.0b1,Copy and/or define models
v0.15.0b1,TODO: ideally the below private attribute logic should be in .fit but is needed in init
v0.15.0b1,for nuisance estimator generation for parent class
v0.15.0b1,should refactor later
v0.15.0b1,Define nuisance estimators
v0.15.0b1,Define parameter estimators
v0.15.0b1,Define
v0.15.0b1,Need to redefine fit here for auto inference to work due to a quirk in how
v0.15.0b1,wrap_fit is defined
v0.15.0b1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.15.0b1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.15.0b1,Override to flatten output if T is flat
v0.15.0b1,Check that all discrete treatments are represented
v0.15.0b1,Nuissance estimates evaluated with cross-fitting
v0.15.0b1,Define 2-fold iterator
v0.15.0b1,Check if there is only one example of some class
v0.15.0b1,Define 2-fold iterator
v0.15.0b1,need safe=False when cloning for WeightedModelWrapper
v0.15.0b1,Compute residuals
v0.15.0b1,Compute coefficient by OLS on residuals
v0.15.0b1,"Parameter returned by LinearRegression is (d_T, )"
v0.15.0b1,Compute residuals
v0.15.0b1,Compute coefficient by OLS on residuals
v0.15.0b1,ell_2 regularization
v0.15.0b1,Ridge regression estimate
v0.15.0b1,"Parameter returned is of shape (d_T, )"
v0.15.0b1,Return moments and gradients
v0.15.0b1,Compute residuals
v0.15.0b1,Compute moments
v0.15.0b1,"Moments shape is (n, d_T)"
v0.15.0b1,Compute moment gradients
v0.15.0b1,returns shape-conforming residuals
v0.15.0b1,Copy and/or define models
v0.15.0b1,Define parameter estimators
v0.15.0b1,Define moment and mean gradient estimator
v0.15.0b1,"Check that T is shape (n, )"
v0.15.0b1,Check T is numeric
v0.15.0b1,Train label encoder
v0.15.0b1,Call `fit` from parent class
v0.15.0b1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.15.0b1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.15.0b1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0b1,Override to flatten output if T is flat
v0.15.0b1,override only so that we can exclude treatment featurization verbiage in docstring
v0.15.0b1,Expand one-hot encoding to include the zero treatment
v0.15.0b1,"Test that T contains all treatments. If not, return None"
v0.15.0b1,Nuissance estimates evaluated with cross-fitting
v0.15.0b1,Define 2-fold iterator
v0.15.0b1,Check if there is only one example of some class
v0.15.0b1,No need to crossfit for internal nodes
v0.15.0b1,Compute partial moments
v0.15.0b1,"If any of the values in the parameter estimate is nan, return None"
v0.15.0b1,Compute partial moments
v0.15.0b1,Compute coefficient by OLS on residuals
v0.15.0b1,ell_2 regularization
v0.15.0b1,Ridge regression estimate
v0.15.0b1,"Parameter returned is of shape (d_T, )"
v0.15.0b1,Return moments and gradients
v0.15.0b1,Compute partial moments
v0.15.0b1,Compute moments
v0.15.0b1,"Moments shape is (n, d_T-1)"
v0.15.0b1,Compute moment gradients
v0.15.0b1,Need to calculate this in an elegant way for when propensity is 0
v0.15.0b1,This will flatten T
v0.15.0b1,Check that T is numeric
v0.15.0b1,Test whether the input estimator is supported
v0.15.0b1,Calculate confidence intervals for the parameter (marginal effect)
v0.15.0b1,Calculate confidence intervals for the effect
v0.15.0b1,Calculate the effects
v0.15.0b1,Calculate the standard deviations for the effects
v0.15.0b1,d_t=None here since we measure the effect across all Ts
v0.15.0b1,conditionally expand jacobian dimensions to align with einsum str
v0.15.0b1,Calculate the effects
v0.15.0b1,Calculate the standard deviations for the effects
v0.15.0b1,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Causal tree parameters
v0.15.0b1,Tree structure
v0.15.0b1,No need for a random split since the data is already
v0.15.0b1,a random subsample from the original input
v0.15.0b1,node list stores the nodes that are yet to be splitted
v0.15.0b1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.15.0b1,Create local sample set
v0.15.0b1,Compute nuisance estimates for the current node
v0.15.0b1,Nuisance estimate cannot be calculated
v0.15.0b1,Estimate parameter for current node
v0.15.0b1,Node estimate cannot be calculated
v0.15.0b1,Calculate moments and gradient of moments for current data
v0.15.0b1,Calculate inverse gradient
v0.15.0b1,The gradient matrix is not invertible.
v0.15.0b1,No good split can be found
v0.15.0b1,Calculate point-wise pseudo-outcomes rho
v0.15.0b1,a split is determined by a feature and a sample pair
v0.15.0b1,the number of possible splits is at most (number of features) * (number of node samples)
v0.15.0b1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.15.0b1,parse row and column of random pair
v0.15.0b1,the sample of the pair is the integer division of the random number with n_feats
v0.15.0b1,calculate the binary indicator of whether sample i is on the left or the right
v0.15.0b1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.15.0b1,calculate the number of samples on the left child for each proposed split
v0.15.0b1,calculate the analogous binary indicator for the samples in the estimation set
v0.15.0b1,calculate the number of estimation samples on the left child of each proposed split
v0.15.0b1,find the upper and lower bound on the size of the left split for the split
v0.15.0b1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.15.0b1,on each side.
v0.15.0b1,similarly for the estimation sample set
v0.15.0b1,if there is no valid split then don't create any children
v0.15.0b1,filter only the valid splits
v0.15.0b1,calculate the average influence vector of the samples in the left child
v0.15.0b1,calculate the average influence vector of the samples in the right child
v0.15.0b1,take the square of each of the entries of the influence vectors and normalize
v0.15.0b1,by size of each child
v0.15.0b1,calculate the vector score of each candidate split as the average of left and right
v0.15.0b1,influence vectors
v0.15.0b1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.15.0b1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.15.0b1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.15.0b1,calculate the scalar score of each split by aggregating across the vector of scores
v0.15.0b1,Find split that minimizes criterion
v0.15.0b1,Create child nodes with corresponding subsamples
v0.15.0b1,add the created children to the list of not yet split nodes
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.15.0b1,Copyright (c) PyWhy contributors. All rights reserved.
v0.15.0b1,Licensed under the MIT License.
v0.14.1,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.14.1,configuration is all pulled from setup.cfg
v0.14.1,-*- coding: utf-8 -*-
v0.14.1,
v0.14.1,Configuration file for the Sphinx documentation builder.
v0.14.1,
v0.14.1,This file does only contain a selection of the most common options. For a
v0.14.1,full list see the documentation:
v0.14.1,http://www.sphinx-doc.org/en/main/config
v0.14.1,-- Path setup --------------------------------------------------------------
v0.14.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.14.1,add these directories to sys.path here. If the directory is relative to the
v0.14.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.14.1,
v0.14.1,-- Project information -----------------------------------------------------
v0.14.1,-- General configuration ---------------------------------------------------
v0.14.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.14.1,
v0.14.1,needs_sphinx = '1.0'
v0.14.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.14.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.14.1,ones.
v0.14.1,TODO: enable type aliases
v0.14.1,napoleon_preprocess_types = True  # needed for type aliases to work
v0.14.1,napoleon_type_aliases = {
v0.14.1,"""array_like"": "":term:`array_like`"","
v0.14.1,"""ndarray"": ""~numpy.ndarray"","
v0.14.1,"""RandomState"": "":class:`~numpy.random.RandomState`"","
v0.14.1,"""DataFrame"": "":class:`~pandas.DataFrame`"","
v0.14.1,"""Series"": "":class:`~pandas.Series`"","
v0.14.1,}
v0.14.1,"Add any paths that contain templates here, relative to this directory."
v0.14.1,The suffix(es) of source filenames.
v0.14.1,You can specify multiple suffix as a list of strings:
v0.14.1,
v0.14.1,"source_suffix = ['.rst', '.md']"
v0.14.1,The root toctree document.
v0.14.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.14.1,for a list of supported languages.
v0.14.1,
v0.14.1,This is also used if you do content translation via gettext catalogs.
v0.14.1,"Usually you set ""language"" from the command line for these cases."
v0.14.1,"List of patterns, relative to source directory, that match files and"
v0.14.1,directories to ignore when looking for source files.
v0.14.1,This pattern also affects html_static_path and html_extra_path.
v0.14.1,The name of the Pygments (syntax highlighting) style to use.
v0.14.1,-- Options for HTML output -------------------------------------------------
v0.14.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.14.1,a list of builtin themes.
v0.14.1,
v0.14.1,Theme options are theme-specific and customize the look and feel of a theme
v0.14.1,"further.  For a list of options available for each theme, see the"
v0.14.1,documentation.
v0.14.1,
v0.14.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.14.1,"relative to this directory. They are copied after the builtin static files,"
v0.14.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.14.1,html_static_path = ['_static']
v0.14.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.14.1,to template names.
v0.14.1,
v0.14.1,The default sidebars (for documents that don't match any pattern) are
v0.14.1,defined by theme itself.  Builtin themes are using these templates by
v0.14.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.14.1,'searchbox.html']``.
v0.14.1,
v0.14.1,html_sidebars = {}
v0.14.1,-- Options for HTMLHelp output ---------------------------------------------
v0.14.1,Output file base name for HTML help builder.
v0.14.1,-- Options for LaTeX output ------------------------------------------------
v0.14.1,The paper size ('letterpaper' or 'a4paper').
v0.14.1,
v0.14.1,"'papersize': 'letterpaper',"
v0.14.1,"The font size ('10pt', '11pt' or '12pt')."
v0.14.1,
v0.14.1,"'pointsize': '10pt',"
v0.14.1,Additional stuff for the LaTeX preamble.
v0.14.1,
v0.14.1,"'preamble': '',"
v0.14.1,Latex figure (float) alignment
v0.14.1,
v0.14.1,"'figure_align': 'htbp',"
v0.14.1,Grouping the document tree into LaTeX files. List of tuples
v0.14.1,"(source start file, target name, title,"
v0.14.1,"author, documentclass [howto, manual, or own class])."
v0.14.1,-- Options for manual page output ------------------------------------------
v0.14.1,One entry per manual page. List of tuples
v0.14.1,"(source start file, name, description, authors, manual section)."
v0.14.1,-- Options for Texinfo output ----------------------------------------------
v0.14.1,Grouping the document tree into Texinfo files. List of tuples
v0.14.1,"(source start file, target name, title, author,"
v0.14.1,"dir menu entry, description, category)"
v0.14.1,-- Options for Epub output -------------------------------------------------
v0.14.1,Bibliographic Dublin Core info.
v0.14.1,The unique identifier of the text. This can be a ISBN number
v0.14.1,or the project homepage.
v0.14.1,
v0.14.1,epub_identifier = ''
v0.14.1,A unique identification for the text.
v0.14.1,
v0.14.1,epub_uid = ''
v0.14.1,A list of files that should not be packed into the epub file.
v0.14.1,-- Extension configuration -------------------------------------------------
v0.14.1,-- Options for intersphinx extension ---------------------------------------
v0.14.1,Example configuration for intersphinx: refer to the Python standard library.
v0.14.1,-- Options for todo extension ----------------------------------------------
v0.14.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.14.1,-- Options for doctest extension -------------------------------------------
v0.14.1,we can document otherwise excluded entities here by returning False
v0.14.1,or skip otherwise included entities by returning True
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Calculate residuals
v0.14.1,Estimate E[T_res | Z_res]
v0.14.1,TODO. Deal with multi-class instrument
v0.14.1,Calculate nuisances
v0.14.1,Estimate E[T_res | Z_res]
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.14.1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.14.1,TODO. Deal with multi-class instrument
v0.14.1,Estimate final model of theta(X) by minimizing the square loss:
v0.14.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.14.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.14.1,at the expense of some small bias. For points with very small covariance we revert
v0.14.1,to the model-based preliminary estimate and do not add the correction term.
v0.14.1,Estimate preliminary theta in cross fitting manner
v0.14.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.14.1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.14.1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.14.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.14.1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.14.1,TODO. The solution below is not really a valid cross-fitting
v0.14.1,as the test data are used to create the proj_t on the train
v0.14.1,which in the second train-test loop is used to create the nuisance
v0.14.1,cov on the test data. Hence the T variable of some sample
v0.14.1,"is implicitly correlated with its cov nuisance, through this flow"
v0.14.1,"of information. However, this seems a rather weak correlation."
v0.14.1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.14.1,model.
v0.14.1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.14.1,Estimate preliminary theta in cross fitting manner
v0.14.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.14.1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.14.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.14.1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.14.1,#############################################################################
v0.14.1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.14.1,A/B test
v0.14.1,#############################################################################
v0.14.1,Estimate preliminary theta in cross fitting manner
v0.14.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.14.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.14.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.14.1,We can use statsmodel for all hypothesis testing capabilities
v0.14.1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.14.1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.14.1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.14.1,model_T_XZ = lambda: model_clf()
v0.14.1,#'days_visited': lambda:
v0.14.1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.14.1,Turn strings into categories for numeric mapping
v0.14.1,### Defining some generic regressors and classifiers
v0.14.1,This a generic non-parametric regressor
v0.14.1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.14.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.14.1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.14.1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.14.1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.14.1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.14.1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.14.1,model = lambda: LinearRegression(n_jobs=-1)
v0.14.1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.14.1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.14.1,underlying model whenever predict is called.
v0.14.1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.14.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.14.1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.14.1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.14.1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.14.1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.14.1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.14.1,We need to specify models to be used for each of these residualizations
v0.14.1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.14.1,"E[T | X, Z]"
v0.14.1,E[TZ | X]
v0.14.1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.14.1,n_splits determines the number of splits to be used for cross-fitting.
v0.14.1,# Algorithm 2 - Current Method
v0.14.1,In[121]:
v0.14.1,# Algorithm 3 - DRIV ATE
v0.14.1,dmliv_model_effect = lambda: model()
v0.14.1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.14.1,"dmliv_model_effect(),"
v0.14.1,n_splits=1)
v0.14.1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.14.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.14.1,"Once multiple treatments are supported, we'll need to fix this"
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.14.1,We can use statsmodel for all hypothesis testing capabilities
v0.14.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.14.1,We can use statsmodel for all hypothesis testing capabilities
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,TODO. Deal with multi-class instrument/treatment
v0.14.1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.14.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.14.1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.14.1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.14.1,##################
v0.14.1,Global settings #
v0.14.1,##################
v0.14.1,Global plotting controls
v0.14.1,"Control for support size, can control for more"
v0.14.1,#################
v0.14.1,File utilities #
v0.14.1,#################
v0.14.1,#################
v0.14.1,Plotting utils #
v0.14.1,#################
v0.14.1,bias
v0.14.1,var
v0.14.1,rmse
v0.14.1,r2
v0.14.1,Infer feature dimension
v0.14.1,Metrics by support plots
v0.14.1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.14.1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.14.1,Steven Wu <zhiww@microsoft.com>
v0.14.1,Initialize causal tree parameters
v0.14.1,Create splits of causal tree
v0.14.1,Estimate treatment effects at the leafs
v0.14.1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.14.1,the corresponding split and associating the effect computed on that leaf
v0.14.1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.14.1,Safety check
v0.14.1,Weighted linear regression
v0.14.1,Calculates weights
v0.14.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.14.1,over all indices
v0.14.1,Similar for `a` weights
v0.14.1,Doesn't have sample weights
v0.14.1,Is a linear model
v0.14.1,Weighted linear regression
v0.14.1,Calculates weights
v0.14.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.14.1,over all indices
v0.14.1,Similar for `a` weights
v0.14.1,normalize weights
v0.14.1,"Split the data in half, train and test"
v0.14.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.14.1,"a function of W, using only the train fold"
v0.14.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.14.1,"Split the data in half, train and test"
v0.14.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.14.1,"a function of W, using only the train fold"
v0.14.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.14.1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.14.1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.14.1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.14.1,"Split the data in half, train and test"
v0.14.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.14.1,"a function of x, using only the train fold"
v0.14.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.14.1,Compute coefficient by OLS on residuals
v0.14.1,"Split the data in half, train and test"
v0.14.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.14.1,"a function of x, using only the train fold"
v0.14.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.14.1,Estimate multipliers for second order orthogonal method
v0.14.1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.14.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.14.1,Create local sample set
v0.14.1,compute the base estimate for the current node using double ml or second order double ml
v0.14.1,compute the influence functions here that are used for the criterion
v0.14.1,generate random proposals of dimensions to split
v0.14.1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.14.1,compute criterion for each proposal
v0.14.1,if splitting creates valid leafs in terms of mean leaf size
v0.14.1,Calculate criterion for split
v0.14.1,Else set criterion to infinity so that this split is not chosen
v0.14.1,If no good split was found
v0.14.1,Find split that minimizes criterion
v0.14.1,Set the split attributes at the node
v0.14.1,Create child nodes with corresponding subsamples
v0.14.1,Recursively split children
v0.14.1,Return parent node
v0.14.1,estimate the local parameter at the leaf using the estimate data
v0.14.1,###################
v0.14.1,Argument parsing #
v0.14.1,###################
v0.14.1,#########################################
v0.14.1,Parameters constant across experiments #
v0.14.1,#########################################
v0.14.1,Outcome support
v0.14.1,Treatment support
v0.14.1,Evaluation grid
v0.14.1,Treatment effects array
v0.14.1,Other variables
v0.14.1,##########################
v0.14.1,Data Generating Process #
v0.14.1,##########################
v0.14.1,Log iteration
v0.14.1,"Generate controls, features, treatment and outcome"
v0.14.1,T and Y residuals to be used in later scripts
v0.14.1,Save generated dataset
v0.14.1,#################
v0.14.1,ORF parameters #
v0.14.1,#################
v0.14.1,######################################
v0.14.1,Train and evaluate treatment effect #
v0.14.1,######################################
v0.14.1,########
v0.14.1,Plots #
v0.14.1,########
v0.14.1,###############
v0.14.1,Save results #
v0.14.1,###############
v0.14.1,##############
v0.14.1,Run Rscript #
v0.14.1,##############
v0.14.1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.14.1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.14.1,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.14.1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.14.1,def mlasso_model(): return MultiTaskLassoCV(
v0.14.1,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.14.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.14.1,heterogeneity
v0.14.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.14.1,heterogeneity
v0.14.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.14.1,heterogeneity
v0.14.1,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.14.1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.14.1,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.14.1,subset of features that are exogenous and create heterogeneity
v0.14.1,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.14.1,subset of features wrt we estimate heterogeneity
v0.14.1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.14.1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,introspect the constructor arguments to find the model parameters
v0.14.1,to represent
v0.14.1,"if the argument is deprecated, ignore it"
v0.14.1,Extract and sort argument names excluding 'self'
v0.14.1,column names
v0.14.1,transfer input to numpy arrays
v0.14.1,transfer input to 2d arrays
v0.14.1,create dataframe
v0.14.1,currently dowhy only support single outcome and single treatment
v0.14.1,call dowhy
v0.14.1,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.14.1,cate estimator but not the effect.
v0.14.1,don't proxy special methods
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Check if model is sparse enough for this model
v0.14.1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.14.1,TODO: any way to avoid creating a copy if the array was already dense?
v0.14.1,"the call is necessary if the input was something like a list, though"
v0.14.1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.14.1,so convert to pydata sparse first
v0.14.1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.14.1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.14.1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.14.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.14.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.14.1,For when checking input values is disabled
v0.14.1,Type to column extraction function
v0.14.1,if not all column names are strings
v0.14.1,coerce feature names to be strings
v0.14.1,Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
v0.14.1,"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
v0.14.1,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.14.1,Handles cases where the passed feature names create issues
v0.14.1,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.14.1,Get feature names using featurizer
v0.14.1,All attempts at retrieving transformed feature names have failed
v0.14.1,Delegate handling to downstream logic
v0.14.1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.14.1,same number of input definitions as arrays
v0.14.1,input definitions have same number of dimensions as each array
v0.14.1,all result indices are unique
v0.14.1,all result indices must match at least one input index
v0.14.1,"map indices to all array, axis pairs for that index"
v0.14.1,each index has the same cardinality wherever it appears
v0.14.1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.14.1,Algo: while list contains more than one entry
v0.14.1,take two entries
v0.14.1,sort both lists by intersection of their indices
v0.14.1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.14.1,"take the union of indices and the product of values), stepping through each list linearly"
v0.14.1,TODO: might be faster to break into connected components first
v0.14.1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.14.1,"so compute their content separately, then take cartesian product"
v0.14.1,this would save a few pointless sorts by empty tuples
v0.14.1,TODO: Consider investigating other performance ideas for these cases
v0.14.1,where the dense method beat the sparse method (usually sparse is faster)
v0.14.1,"e,facd,c->cfed"
v0.14.1,sparse: 0.0335489
v0.14.1,dense:  0.011465999999999997
v0.14.1,"gbd,da,egb->da"
v0.14.1,sparse: 0.0791625
v0.14.1,dense:  0.007319099999999995
v0.14.1,"dcc,d,faedb,c->abe"
v0.14.1,sparse: 1.2868097
v0.14.1,dense:  0.44605229999999985
v0.14.1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.14.1,TODO: would using einsum's paths to optimize the order of merging help?
v0.14.1,assume that we should perform nested cross-validation if and only if
v0.14.1,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.14.1,logic copied from check_cv
v0.14.1,otherwise we will assume the user already set the cv attribute to something
v0.14.1,compatible with splitting with a 'groups' argument
v0.14.1,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.14.1,don't use the groups when calling split internally
v0.14.1,Normalize weights
v0.14.1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.14.1,"if we're decorating a class, just update the __init__ method,"
v0.14.1,so that the result is still a class instead of a wrapper method
v0.14.1,"want to enforce that each bad_arg was either in kwargs,"
v0.14.1,or else it was in neither and is just taking its default value
v0.14.1,Any access should throw
v0.14.1,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.14.1,return plain dictionary so that erroneous accesses don't half work (see e.g. #708)
v0.14.1,for every dimension of the treatment add some epsilon and observe change in featurized treatment
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.14.1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.14.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.14.1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.14.1,input feature name is already updated by cate_feature_names.
v0.14.1,define the index of d_x to filter for each given T
v0.14.1,filter X after broadcast with T for each given T
v0.14.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.14.1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.14.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.14.1,return plain dictionary so that erroneous accesses don't half work (see #708)
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,
v0.14.1,This code contains some snippets of code from:
v0.14.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
v0.14.1,published under the following license and copyright:
v0.14.1,BSD 3-Clause License
v0.14.1,
v0.14.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.1,All rights reserved.
v0.14.1,make any access to matplotlib or plt throw an exception
v0.14.1,make any access to graphviz or plt throw an exception
v0.14.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.14.1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.14.1,Initialize saturation & value; calculate chroma & value shift
v0.14.1,Calculate some intermediate values
v0.14.1,Initialize RGB with same hue & chroma as our color
v0.14.1,Shift the initial RGB values to match value and store
v0.14.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.14.1,clean way of achieving this
v0.14.1,make sure we don't accidentally escape anything in the substitution
v0.14.1,Fetch appropriate color for node
v0.14.1,"red for negative, green for positive"
v0.14.1,in multi-target use mean of targets
v0.14.1,Write node mean CATE
v0.14.1,Write node std of CATE
v0.14.1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.14.1,Fetch appropriate color for node
v0.14.1,Write node mean CATE
v0.14.1,Write node mean CATE
v0.14.1,Write recommended treatment and value - cost
v0.14.1,Licensed under the MIT License.
v0.14.1,"since inference objects can be stateful, we must copy it before fitting;"
v0.14.1,otherwise this sequence wouldn't work:
v0.14.1,"est1.fit(..., inference=inf)"
v0.14.1,"est2.fit(..., inference=inf)"
v0.14.1,est1.effect_interval(...)
v0.14.1,because inf now stores state from fitting est2
v0.14.1,This flag is true when names are set in a child class instead
v0.14.1,"If names are set in a child class, add an attribute reflecting that"
v0.14.1,This works only if X is passed as a kwarg
v0.14.1,We plan to enforce X as kwarg only in future releases
v0.14.1,This checks if names have been set in a child class
v0.14.1,"If names were set in a child class, don't do it again"
v0.14.1,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.14.1,call the wrapped fit method
v0.14.1,NOTE: we call inference fit *after* calling the main fit method
v0.14.1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.14.1,but tensordot can't be applied to this problem because we don't sum over m
v0.14.1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.14.1,of rows of T was not taken into account
v0.14.1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.14.1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.14.1,"Treatment names is None, default to BaseCateEstimator"
v0.14.1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.14.1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.14.1,Get input names
v0.14.1,Summary
v0.14.1,add statsmodels to parent's options
v0.14.1,add debiasedlasso to parent's options
v0.14.1,add blb to parent's options
v0.14.1,TODO Share some logic with non-discrete version
v0.14.1,Get input names
v0.14.1,Note: we do not transform feature names since that is done within summary_frame
v0.14.1,Summary
v0.14.1,add statsmodels to parent's options
v0.14.1,add statsmodels to parent's options
v0.14.1,add blb to parent's options
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,remove None arguments
v0.14.1,"scores entries should be lists of scores, so make each entry a singleton list"
v0.14.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.14.1,generate an instance of the final model
v0.14.1,generate an instance of the nuisance model
v0.14.1,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.14.1,alteration even when we only want to fit_final.
v0.14.1,use a binary array to get stratified split in case of discrete treatment
v0.14.1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.14.1,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.14.1,"however, sklearn doesn't support both stratifying and grouping (see"
v0.14.1,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.14.1,their own object that supports grouping if they want to use groups.
v0.14.1,for each mc iteration
v0.14.1,for each model under cross fit setting
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,
v0.14.1,This code contains snippets of code from
v0.14.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.14.1,published under the following license and copyright:
v0.14.1,BSD 3-Clause License
v0.14.1,
v0.14.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.1,All rights reserved.
v0.14.1,=============================================================================
v0.14.1,Policy Forest
v0.14.1,=============================================================================
v0.14.1,Remap output
v0.14.1,reshape is necessary to preserve the data contiguity against vs
v0.14.1,"[:, np.newaxis] that does not."
v0.14.1,Get subsample sample size
v0.14.1,Check parameters
v0.14.1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.14.1,if this is the first `fit` call of the warm start mode.
v0.14.1,"Free allocated memory, if any"
v0.14.1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.14.1,We draw from the random state to get the random state we
v0.14.1,would have got if we hadn't used a warm_start.
v0.14.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.14.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.14.1,but would still advance randomness enough so that tree subsamples will be different.
v0.14.1,Parallel loop: we prefer the threading backend as the Cython code
v0.14.1,for fitting the trees is internally releasing the Python GIL
v0.14.1,making threading more efficient than multiprocessing in
v0.14.1,"that case. However, for joblib 0.12+ we respect any"
v0.14.1,"parallel_backend contexts set at a higher level,"
v0.14.1,since correctness does not rely on using threads.
v0.14.1,Collect newly grown trees
v0.14.1,Check data
v0.14.1,Assign chunk of trees to jobs
v0.14.1,avoid storing the output of every estimator by summing them here
v0.14.1,Parallel loop
v0.14.1,Check data
v0.14.1,Assign chunk of trees to jobs
v0.14.1,avoid storing the output of every estimator by summing them here
v0.14.1,Parallel loop
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,
v0.14.1,This code contains snippets of code from:
v0.14.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.14.1,published under the following license and copyright:
v0.14.1,BSD 3-Clause License
v0.14.1,
v0.14.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.1,All rights reserved.
v0.14.1,=============================================================================
v0.14.1,Types and constants
v0.14.1,=============================================================================
v0.14.1,=============================================================================
v0.14.1,Base Policy tree
v0.14.1,=============================================================================
v0.14.1,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.14.1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.14.1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.14.1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.14.1,checks that X is 2D array.
v0.14.1,"since we only allow single dimensional y, we could flatten the prediction"
v0.14.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.14.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.14.1,Handles the corner case when X=None but featurizer might be not None
v0.14.1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.14.1,"Replacing this method which is invalid for this class, so that we make the"
v0.14.1,dosctring empty and not appear in the docs.
v0.14.1,TODO: support freq_weight and sample_var in debiased lasso
v0.14.1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.14.1,Replacing to remove docstring
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"if both X and W are None, just return a column of ones"
v0.14.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.14.1,We need to go back to the label representation of the one-hot so as to call
v0.14.1,the classifier.
v0.14.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.14.1,We need to go back to the label representation of the one-hot so as to call
v0.14.1,the classifier.
v0.14.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.14.1,This works both with our without the weighting trick as the treatments T are unit vector
v0.14.1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.14.1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.14.1,both Parametric and Non Parametric DML.
v0.14.1,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.14.1,attribute so that the trained featurizer will be passed through
v0.14.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.14.1,for internal use by the library
v0.14.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.14.1,We need to use the rlearner's copy to retain the information from fitting
v0.14.1,Handles the corner case when X=None but featurizer might be not None
v0.14.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.14.1,since we clone it and fit separate copies
v0.14.1,override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
v0.14.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.14.1,TODO: support freq_weight and sample_var in debiased lasso
v0.14.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.14.1,since we clone it and fit separate copies
v0.14.1,add blb to parent's options
v0.14.1,override only so that we can update the docstring to indicate
v0.14.1,support for `GenericSingleTreatmentModelFinalInference`
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,note that groups are not passed to score because they are only used for fitting
v0.14.1,note that groups are not passed to score because they are only used for fitting
v0.14.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.14.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.14.1,NOTE: important to get parent's wrapped copy so that
v0.14.1,"after training wrapped featurizer is also trained, etc."
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.14.1,Fit a doubly robust average effect
v0.14.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.14.1,(which needs to have been expanded if there's a discrete treatment)
v0.14.1,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.14.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.14.1,since we clone it and fit separate copies
v0.14.1,"If custom param grid, check that only estimator parameters are being altered"
v0.14.1,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.14.1,override only so that we can update the docstring to indicate support for `blb`
v0.14.1,Get input names
v0.14.1,Summary
v0.14.1,Determine output settings
v0.14.1,"Important: This must be the first invocation of the random state at fit time, so that"
v0.14.1,train/test splits are re-generatable from an external object simply by knowing the
v0.14.1,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.14.1,linear predictions. Currently is also useful for testing.
v0.14.1,reshape is necessary to preserve the data contiguity against vs
v0.14.1,"[:, np.newaxis] that does not."
v0.14.1,Check parameters
v0.14.1,Set min_weight_leaf from min_weight_fraction_leaf
v0.14.1,Build tree
v0.14.1,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.14.1,hold. Used by criterion for memory space savings.
v0.14.1,Initialize the criterion object and the criterion_val object if honest.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,
v0.14.1,This code is a fork from:
v0.14.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
v0.14.1,published under the following license and copyright:
v0.14.1,BSD 3-Clause License
v0.14.1,
v0.14.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.1,All rights reserved.
v0.14.1,Set parameters
v0.14.1,Don't instantiate estimators now! Parameters of base_estimator might
v0.14.1,"still change. Eg., when grid-searching with the nested object syntax."
v0.14.1,self.estimators_ needs to be filled by the derived classes in fit.
v0.14.1,Compute the number of jobs
v0.14.1,Partition estimators between jobs
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,covariance matrix
v0.14.1,get eigen value and eigen vectors
v0.14.1,simulate eigen vectors
v0.14.1,keep the top 4 eigen value and corresponding eigen vector
v0.14.1,replace the negative eigen values
v0.14.1,generate a new covariance matrix
v0.14.1,get linear approximation of eigen values
v0.14.1,coefs
v0.14.1,get the indices of each group of features
v0.14.1,print(ind_same_proxy)
v0.14.1,demo
v0.14.1,same proxy
v0.14.1,residuals
v0.14.1,gmm
v0.14.1,log normal on outliers
v0.14.1,positive outliers
v0.14.1,negative outliers
v0.14.1,demean the new residual again
v0.14.1,generate data
v0.14.1,sample residuals
v0.14.1,get prediction for current investment
v0.14.1,get prediction for current proxy
v0.14.1,get first period prediction
v0.14.1,iterate the step ahead contruction
v0.14.1,prepare new x
v0.14.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.14.1,heterogeneity
v0.14.1,get new covariance matrix
v0.14.1,get coefs
v0.14.1,get residuals
v0.14.1,proxy 1 is the outcome
v0.14.1,make fixed residuals
v0.14.1,Remove children with nonwhite mothers from the treatment group
v0.14.1,Remove children with nonwhite mothers from the treatment group
v0.14.1,Select columns
v0.14.1,Scale the numeric variables
v0.14.1,"Change the binary variable 'first' takes values in {1,2}"
v0.14.1,Append a column of ones as intercept
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.14.1,(which needs to have been expanded if there's a discrete treatment)
v0.14.1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.14.1,d_t=None here since we measure the effect across all Ts
v0.14.1,"y is a vector, rather than a 2D array"
v0.14.1,once the estimator has been fit
v0.14.1,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.14.1,an intercept even when bias is part of coef.
v0.14.1,We can write effect inference as a function of prediction and prediction standard error of
v0.14.1,the final method for linear models
v0.14.1,squeeze the first axis
v0.14.1,d_t=None here since we measure the effect across all Ts
v0.14.1,set the mean_pred_stderr
v0.14.1,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.14.1,squeeze the first axis
v0.14.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.14.1,(which needs to have been expanded if there's a discrete treatment)
v0.14.1,"send treatment to the end, pull bounds to the front"
v0.14.1,d_t=None here since we measure the effect across all Ts
v0.14.1,set the mean_pred_stderr
v0.14.1,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.14.1,d_t=None here since we measure the effect across all Ts
v0.14.1,d_t=None here since we measure the effect across all Ts
v0.14.1,need to set the fit args before the estimator is fit
v0.14.1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.14.1,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.14.1,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.14.1,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.14.1,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.14.1,scale preds
v0.14.1,scale std errs
v0.14.1,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.14.1,offset preds
v0.14.1,"offset the distribution, too"
v0.14.1,scale preds
v0.14.1,"scale the distribution, too"
v0.14.1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.14.1,1. Uncertainty of Mean Point Estimate
v0.14.1,2. Distribution of Point Estimate
v0.14.1,3. Total Variance of Point Estimate
v0.14.1,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.14.1,so bail out early; note that it might be possible to correct the algorithm for
v0.14.1,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.14.1,be clean
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,TODO: Add a __dir__ implementation?
v0.14.1,don't proxy special methods
v0.14.1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.14.1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.14.1,then we should be computing a confidence interval for the wrapped calls
v0.14.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.14.1,second level bootstrap which would be prohibitive computationally?
v0.14.1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.14.1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.14.1,can't import from econml.inference at top level without creating cyclical dependencies
v0.14.1,Note that inference results are always methods even if the inference is for a property
v0.14.1,(e.g. coef__inference() is a method but coef_ is a property)
v0.14.1,Therefore we must insert a lambda if getting inference for a non-callable
v0.14.1,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.14.1,"try to get interval/std first if appropriate,"
v0.14.1,since we don't prefer a wrapped method with this name
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,
v0.14.1,This code contains snippets of code from:
v0.14.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.14.1,published under the following license and copyright:
v0.14.1,BSD 3-Clause License
v0.14.1,
v0.14.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.1,All rights reserved.
v0.14.1,=============================================================================
v0.14.1,Types and constants
v0.14.1,=============================================================================
v0.14.1,=============================================================================
v0.14.1,Base GRF tree
v0.14.1,=============================================================================
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,=============================================================================
v0.14.1,A MultOutputWrapper for GRF classes
v0.14.1,=============================================================================
v0.14.1,=============================================================================
v0.14.1,Instantiations of Generalized Random Forest
v0.14.1,=============================================================================
v0.14.1,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.14.1,in front of the constant treatment is the intercept in the moment equation.
v0.14.1,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.14.1,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,
v0.14.1,This code contains snippets of code from
v0.14.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.14.1,published under the following license and copyright:
v0.14.1,BSD 3-Clause License
v0.14.1,
v0.14.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.1,All rights reserved.
v0.14.1,=============================================================================
v0.14.1,Base Generalized Random Forest
v0.14.1,=============================================================================
v0.14.1,TODO: support freq_weight and sample_var
v0.14.1,Remap output
v0.14.1,reshape is necessary to preserve the data contiguity against vs
v0.14.1,"[:, np.newaxis] that does not."
v0.14.1,reshape is necessary to preserve the data contiguity against vs
v0.14.1,"[:, np.newaxis] that does not."
v0.14.1,Get subsample sample size
v0.14.1,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.14.1,We calculate the min eigenvalue proxy that each criterion is considering
v0.14.1,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.14.1,Check parameters
v0.14.1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.14.1,if this is the first `fit` call of the warm start mode.
v0.14.1,"Free allocated memory, if any"
v0.14.1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.14.1,We draw from the random state to get the random state we
v0.14.1,would have got if we hadn't used a warm_start.
v0.14.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.14.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.14.1,but would still advance randomness enough so that tree subsamples will be different.
v0.14.1,Generating indices a priori before parallelism ended up being orders of magnitude
v0.14.1,faster than how sklearn does it. The reason is that random samplers do not release the
v0.14.1,gil it seems.
v0.14.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.14.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.14.1,but would still advance randomness enough so that tree subsamples will be different.
v0.14.1,Parallel loop: we prefer the threading backend as the Cython code
v0.14.1,for fitting the trees is internally releasing the Python GIL
v0.14.1,making threading more efficient than multiprocessing in
v0.14.1,"that case. However, for joblib 0.12+ we respect any"
v0.14.1,"parallel_backend contexts set at a higher level,"
v0.14.1,since correctness does not rely on using threads.
v0.14.1,Collect newly grown trees
v0.14.1,Check data
v0.14.1,Assign chunk of trees to jobs
v0.14.1,avoid storing the output of every estimator by summing them here
v0.14.1,Parallel loop
v0.14.1,Check data
v0.14.1,Assign chunk of trees to jobs
v0.14.1,Parallel loop
v0.14.1,Check data
v0.14.1,Assign chunk of trees to jobs
v0.14.1,Parallel loop
v0.14.1,####################
v0.14.1,Variance correction
v0.14.1,####################
v0.14.1,Subtract the average within bag variance. This ends up being equal to the
v0.14.1,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.14.1,The negative part is just sq_between.
v0.14.1,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.14.1,"The off diagonals we have no objective prior, so no correction is applied."
v0.14.1,Finally correcting the pred_cov or pred_var
v0.14.1,avoid storing the output of every estimator by summing them here
v0.14.1,Parallel loop
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,TODO: update docs
v0.14.1,"NOTE: sample weight, sample var are not passed in"
v0.14.1,Compose final model
v0.14.1,Calculate auxiliary quantities
v0.14.1,X ⨂ T_res
v0.14.1,"sum(model_final.predict(X, T_res))"
v0.14.1,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.14.1,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
v0.14.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.1,"we need to set the number of periods before calling super()._prefit, since that will generate the"
v0.14.1,"final and nuisance models, which need to have self._n_periods set"
v0.14.1,Set _d_t to effective number of treatments
v0.14.1,Required for bootstrap inference
v0.14.1,for each mc iteration
v0.14.1,for each model under cross fit setting
v0.14.1,Handles the corner case when X=None but featurizer might be not None
v0.14.1,Expand treatments for each time period
v0.14.1,NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
v0.14.1,attribute so that the trained featurizer will be passed through
v0.14.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.14.1,for internal use by the library
v0.14.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.14.1,We need to use the _ortho_learner's copy to retain the information from fitting
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,testing importances
v0.14.1,testing heterogeneity importances
v0.14.1,Testing that all parameters do what they are supposed to
v0.14.1,"testing predict, apply and decision path"
v0.14.1,test that the subsampling scheme past to the trees is correct
v0.14.1,The sample size is chosen in particular to test rounding based error when subsampling
v0.14.1,test that the estimator calcualtes var correctly
v0.14.1,test api
v0.14.1,test accuracy
v0.14.1,test the projection functionality of forests
v0.14.1,test that the estimator calcualtes var correctly
v0.14.1,test api
v0.14.1,test that the estimator calcualtes var correctly
v0.14.1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.14.1,test that we raise errors in mishandled situations.
v0.14.1,test that the subsampling scheme past to the trees is correct
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
v0.14.1,omit the lalonde notebook
v0.14.1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.14.1,creating notebooks that are annoying for our users to actually run themselves
v0.14.1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.14.1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.14.1,"prior to calling interpret, can't plot, render, etc."
v0.14.1,can interpret without uncertainty
v0.14.1,can't interpret with uncertainty if inference wasn't used during fit
v0.14.1,can interpret with uncertainty if we refit
v0.14.1,can interpret without uncertainty
v0.14.1,can't treat before interpreting
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,for is_discrete in [False]:
v0.14.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.14.1,ensure we can serialize the unfit estimator
v0.14.1,ensure we can pickle the fit estimator
v0.14.1,make sure we can call the marginal_effect and effect methods
v0.14.1,test const marginal inference
v0.14.1,test effect inference
v0.14.1,test marginal effect inference
v0.14.1,test coef__inference and intercept__inference
v0.14.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.1,"make sure we can call effect with implied scalar treatments,"
v0.14.1,"no matter the dimensions of T, and also that we warn when there"
v0.14.1,are multiple treatments
v0.14.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.1,No heterogeneity
v0.14.1,Define indices to test
v0.14.1,Heterogeneous effects
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,simple DGP only for illustration
v0.14.1,Define the treatment model neural network architecture
v0.14.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.14.1,"so the input shape is (d_z + d_x,)"
v0.14.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.14.1,add extra layers on top for the mixture density network
v0.14.1,Define the response model neural network architecture
v0.14.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.14.1,"so the input shape is (d_t + d_x,)"
v0.14.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.14.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.14.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.14.1,so that the same weights will be reused in each instantiation
v0.14.1,number of samples to use in second estimate of the response
v0.14.1,(to make loss estimate unbiased)
v0.14.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.14.1,do something with predictions...
v0.14.1,also test vector t and y
v0.14.1,simple DGP only for illustration
v0.14.1,Define the treatment model neural network architecture
v0.14.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.14.1,"so the input shape is (d_z + d_x,)"
v0.14.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.14.1,add extra layers on top for the mixture density network
v0.14.1,Define the response model neural network architecture
v0.14.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.14.1,"so the input shape is (d_t + d_x,)"
v0.14.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.14.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.14.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.14.1,so that the same weights will be reused in each instantiation
v0.14.1,number of samples to use in second estimate of the response
v0.14.1,(to make loss estimate unbiased)
v0.14.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.14.1,do something with predictions...
v0.14.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.14.1,test = True ensures we draw test set images
v0.14.1,test = True ensures we draw test set images
v0.14.1,re-draw to get new independent treatment and implied response
v0.14.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.14.1,above is necesary so that reduced form doesn't win
v0.14.1,covariates: time and emotion
v0.14.1,random instrument
v0.14.1,z -> price
v0.14.1,true observable demand function
v0.14.1,errors
v0.14.1,response
v0.14.1,test = True ensures we draw test set images
v0.14.1,test = True ensures we draw test set images
v0.14.1,re-draw to get new independent treatment and implied response
v0.14.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.14.1,above is necesary so that reduced form doesn't win
v0.14.1,covariates: time and emotion
v0.14.1,random instrument
v0.14.1,z -> price
v0.14.1,true observable demand function
v0.14.1,errors
v0.14.1,response
v0.14.1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.14.1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.14.1,For some reason this doesn't work at all when run against the CNTK backend...
v0.14.1,"model.compile('nadam', loss=lambda _,l:l)"
v0.14.1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.14.1,generate a valiation set
v0.14.1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.14.1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,DGP constants
v0.14.1,Generate data
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,identity featurization effect functions
v0.14.1,polynomial featurization effect functions
v0.14.1,1d polynomial featurization functions
v0.14.1,2d-to-1d featurization functions
v0.14.1,2d-to-1d vector featurization functions
v0.14.1,test that treatment names are assigned for the featurized treatment
v0.14.1,expected shapes
v0.14.1,check effects
v0.14.1,ate
v0.14.1,loose inference checks
v0.14.1,temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons
v0.14.1,effect inference
v0.14.1,marginal effect inference
v0.14.1,const marginal effect inference
v0.14.1,fit a dummy estimator first so the featurizer can be fit to the treatment
v0.14.1,edge case with transformer that only takes a vector treatment
v0.14.1,so far will always return None for cate_treatment_names
v0.14.1,assert proper handling of improper feature names passed to certain transformers
v0.14.1,"depending on sklearn version, bad feature names either throws error or only uses first relevant name"
v0.14.1,ensure alpha is passed
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,testing importances
v0.14.1,testing heterogeneity importances
v0.14.1,Testing that all parameters do what they are supposed to
v0.14.1,"testing predict, apply and decision path"
v0.14.1,initialize parameters
v0.14.1,initialize config wtih base config and overwite some values
v0.14.1,predict tree using config parameters and assert
v0.14.1,shape of trained tree is the same as y_test
v0.14.1,initialize config wtih base honest config and overwite some values
v0.14.1,predict tree using config parameters and assert
v0.14.1,shape of trained tree is the same as y_test
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.14.1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.14.1,so we need to transpose the result
v0.14.1,1-d output
v0.14.1,2-d output
v0.14.1,Single dimensional output y
v0.14.1,compare with weight
v0.14.1,compare with weight
v0.14.1,compare with weight
v0.14.1,compare with weight
v0.14.1,Multi-dimensional output y
v0.14.1,1-d y
v0.14.1,compare when both sample_var and sample_weight exist
v0.14.1,multi-d y
v0.14.1,compare when both sample_var and sample_weight exist
v0.14.1,compare when both sample_var and sample_weight exist
v0.14.1,compare when both sample_var and sample_weight exist
v0.14.1,compare when both sample_var and sample_weight exist
v0.14.1,compare when both sample_var and sample_weight exist
v0.14.1,compare when both sample_var and sample_weight exist
v0.14.1,dgp
v0.14.1,StatsModels2SLS
v0.14.1,IV2SLS
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,test that we can fit with the same arguments as the base estimator
v0.14.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.14.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.1,with the same shape for the lower and upper bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,test that we can do the same thing once we provide percentile bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.14.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.1,with the same shape for the lower and upper bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,test that we can do the same thing once we provide percentile bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,test that we can fit with the same arguments as the base estimator
v0.14.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.14.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.1,with the same shape for the lower and upper bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,test that we can do the same thing once we provide percentile bounds
v0.14.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.14.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.1,with the same shape for the lower and upper bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,test that we can do the same thing once we provide percentile bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,test that we can fit with the same arguments as the base estimator
v0.14.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.14.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.1,with the same shape for the lower and upper bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,test that we can do the same thing once we provide percentile bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.14.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.1,with the same shape for the lower and upper bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,test that we can do the same thing once we provide percentile bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.1,with the same shape for the lower and upper bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,test that the estimated effect is usually within the bounds
v0.14.1,test that we can do the same thing once we provide alpha explicitly
v0.14.1,test that the lower and upper bounds differ
v0.14.1,test that the estimated effect is usually within the bounds
v0.14.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.1,with the same shape for the lower and upper bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,TODO: test that the estimated effect is usually within the bounds
v0.14.1,and that the true effect is also usually within the bounds
v0.14.1,test that we can do the same thing once we provide percentile bounds
v0.14.1,test that the lower and upper bounds differ
v0.14.1,TODO: test that the estimated effect is usually within the bounds
v0.14.1,and that the true effect is also usually within the bounds
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,test that the subsampling scheme past to the trees is correct
v0.14.1,test that the estimator calcualtes var correctly
v0.14.1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.14.1,test that we raise errors in mishandled situations.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,DGP constants
v0.14.1,Generate data
v0.14.1,Test inference results when `cate_feature_names` doesn not exist
v0.14.1,Test inference results when `cate_feature_names` doesn not exist
v0.14.1,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.14.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.14.1,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.14.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.14.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.14.1,pvalue for second column should be greater than zero since some points are on either side
v0.14.1,of the tested value
v0.14.1,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.14.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.14.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.14.1,ensure alpha is passed
v0.14.1,only is not None when T1 is a constant or a list of constant
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.14.1,Test non keyword based calls to fit
v0.14.1,test non-array inputs
v0.14.1,Test custom splitter
v0.14.1,Test incomplete set of test folds
v0.14.1,"y scores should be positive, since W predicts Y somewhat"
v0.14.1,"t scores might not be, since W and T are uncorrelated"
v0.14.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,make sure cross product varies more slowly with first array
v0.14.1,and that vectors are okay as inputs
v0.14.1,number of inputs in specification must match number of inputs
v0.14.1,must have an output
v0.14.1,output indices must be unique
v0.14.1,output indices must be present in an input
v0.14.1,number of indices must match number of dimensions for each input
v0.14.1,repeated indices must always have consistent sizes
v0.14.1,transpose
v0.14.1,tensordot
v0.14.1,trace
v0.14.1,TODO: set up proper flag for this
v0.14.1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.14.1,"of all of the distinct indices that appear in any input,"
v0.14.1,pick a random subset of them (of size at most 5) to appear in the output
v0.14.1,creating an instance should warn
v0.14.1,using the instance should not warn
v0.14.1,using the deprecated method should warn
v0.14.1,don't warn if b and c are passed by keyword
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,make any access to matplotlib or plt throw an exception
v0.14.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.14.1,heterogeneity
v0.14.1,Invert indices to match latest API
v0.14.1,Invert indices to match latest API
v0.14.1,The feature for heterogeneity stays constant
v0.14.1,Auxiliary function for adding xticks and vertical lines when plotting results
v0.14.1,for dynamic dml vs ground truth parameters.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Preprocess data
v0.14.1,Convert 'week' to a date
v0.14.1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.14.1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.14.1,Take log of price
v0.14.1,Make brand numeric
v0.14.1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.14.1,which have all zero coeffs
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,test at least one estimator from each category
v0.14.1,test causal graph
v0.14.1,test refutation estimate
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.14.1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.14.1,TODO: test something rather than just print...
v0.14.1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.14.1,pick some arbitrary X
v0.14.1,pick some arbitrary T
v0.14.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.14.1,The average variance should be lower when using monte carlo iterations
v0.14.1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.14.1,The average variance should be lower when using monte carlo iterations
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,ensure we can serialize unfit estimator
v0.14.1,ensure we can serialize fit estimator
v0.14.1,expected effect size
v0.14.1,test effect
v0.14.1,test inference
v0.14.1,only OrthoIV support inference other than bootstrap
v0.14.1,test summary
v0.14.1,test can run score
v0.14.1,test cate_feature_names
v0.14.1,test can run shap values
v0.14.1,dgp
v0.14.1,no heterogeneity
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.14.1,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.14.1,The __warningregistry__'s need to be in a pristine state for tests
v0.14.1,to work properly.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,parameter combinations to test
v0.14.1,TODO: serializing/deserializing for every combination -- is this necessary?
v0.14.1,ensure we can serialize unfit estimator
v0.14.1,ensure we can serialize fit estimator
v0.14.1,expected effect size
v0.14.1,assert calculated constant marginal effect shape is expected
v0.14.1,const_marginal effect is defined in LinearCateEstimator class
v0.14.1,assert calculated marginal effect shape is expected
v0.14.1,test inference
v0.14.1,test can run score
v0.14.1,test cate_feature_names
v0.14.1,test can run shap values
v0.14.1,"dgp (binary T, binary Z)"
v0.14.1,no heterogeneity
v0.14.1,with heterogeneity
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Set random seed
v0.14.1,Generate data
v0.14.1,DGP constants
v0.14.1,Test data
v0.14.1,Constant treatment effect
v0.14.1,Constant treatment with multi output Y
v0.14.1,Heterogeneous treatment
v0.14.1,Heterogeneous treatment with multi output Y
v0.14.1,TLearner test
v0.14.1,Instantiate TLearner
v0.14.1,Test inputs
v0.14.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.14.1,Instantiate SLearner
v0.14.1,Test inputs
v0.14.1,Test constant treatment effect
v0.14.1,Test constant treatment effect with multi output Y
v0.14.1,Test heterogeneous treatment effect
v0.14.1,Need interactions between T and features
v0.14.1,Test heterogeneous treatment effect with multi output Y
v0.14.1,Instantiate XLearner
v0.14.1,Test inputs
v0.14.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.14.1,Instantiate DomainAdaptationLearner
v0.14.1,Test inputs
v0.14.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.14.1,Get the true treatment effect
v0.14.1,Get the true treatment effect
v0.14.1,Fit learner and get the effect and marginal effect
v0.14.1,Compute treatment effect residuals (absolute)
v0.14.1,Check that at least 90% of predictions are within tolerance interval
v0.14.1,Check whether the output shape is right
v0.14.1,Check that one can pass in regular lists
v0.14.1,Check that it fails correctly if lists of different shape are passed in
v0.14.1,"Check that it works when T, Y have shape (n, 1)"
v0.14.1,Generate covariates
v0.14.1,Generate treatment
v0.14.1,Calculate outcome
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,DGP constants
v0.14.1,Generate data
v0.14.1,Test data
v0.14.1,Remove warnings that might be raised by the models passed into the ORF
v0.14.1,Generate data with continuous treatments
v0.14.1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.14.1,does not work well with parallelism.
v0.14.1,Test inputs for continuous treatments
v0.14.1,--> Check that one can pass in regular lists
v0.14.1,--> Check that it fails correctly if lists of different shape are passed in
v0.14.1,Check that outputs have the correct shape
v0.14.1,Test continuous treatments with controls
v0.14.1,Test continuous treatments without controls
v0.14.1,Generate data with binary treatments
v0.14.1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.14.1,does not work well with parallelism.
v0.14.1,Test inputs for binary treatments
v0.14.1,--> Check that one can pass in regular lists
v0.14.1,--> Check that it fails correctly if lists of different shape are passed in
v0.14.1,"--> Check that it works when T, Y have shape (n, 1)"
v0.14.1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.14.1,--> Check that it fails correctly when the treatments are not numeric
v0.14.1,Check that outputs have the correct shape
v0.14.1,Test binary treatments with controls
v0.14.1,Test binary treatments without controls
v0.14.1,Only applicable to continuous treatments
v0.14.1,Generate data for 2 treatments
v0.14.1,Test multiple treatments with controls
v0.14.1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.14.1,The rest for controls. Just as an example.
v0.14.1,Generating A/B test data
v0.14.1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.14.1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.14.1,Create a wrapper around Lasso that doesn't support weights
v0.14.1,since Lasso does natively support them starting in sklearn 0.23
v0.14.1,Generate data with continuous treatments
v0.14.1,Instantiate model with most of the default parameters
v0.14.1,Compute the treatment effect on test points
v0.14.1,Compute treatment effect residuals
v0.14.1,Multiple treatments
v0.14.1,Allow at most 10% test points to be outside of the tolerance interval
v0.14.1,Compute treatment effect residuals
v0.14.1,Multiple treatments
v0.14.1,Allow at most 20% test points to be outside of the confidence interval
v0.14.1,Check that the intervals are not too wide
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.14.1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.14.1,ensure that we've got at least 6 of every element
v0.14.1,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.14.1,NOTE: this number may need to change if the default number of folds in
v0.14.1,WeightedStratifiedKFold changes
v0.14.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.14.1,ensure we can serialize the unfit estimator
v0.14.1,ensure we can pickle the fit estimator
v0.14.1,make sure we can call the marginal_effect and effect methods
v0.14.1,test const marginal inference
v0.14.1,test effect inference
v0.14.1,test marginal effect inference
v0.14.1,test coef__inference and intercept__inference
v0.14.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.1,"make sure we can call effect with implied scalar treatments,"
v0.14.1,"no matter the dimensions of T, and also that we warn when there"
v0.14.1,are multiple treatments
v0.14.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.1,ensure that we've got at least two of every element
v0.14.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.14.1,make sure we can call the marginal_effect and effect methods
v0.14.1,test const marginal inference
v0.14.1,test effect inference
v0.14.1,test marginal effect inference
v0.14.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.14.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.14.1,We concatenate the two copies data
v0.14.1,make sure we can get out post-fit stuff
v0.14.1,create a simple artificial setup where effect of moving from treatment
v0.14.1,"1 -> 2 is 2,"
v0.14.1,"1 -> 3 is 1, and"
v0.14.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.14.1,"Using an uneven number of examples from different classes,"
v0.14.1,"and having the treatments in non-lexicographic order,"
v0.14.1,Should rule out some basic issues.
v0.14.1,test that we can fit with a KFold instance
v0.14.1,test that we can fit with a train/test iterable
v0.14.1,predetermined splits ensure that all features are seen in each split
v0.14.1,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.14.1,(incorrectly) use a final model with an intercept
v0.14.1,"Because final model is fixed, actual values of T and Y don't matter"
v0.14.1,Ensure reproducibility
v0.14.1,Sparse DGP
v0.14.1,Treatment effect coef
v0.14.1,Other coefs
v0.14.1,Features and controls
v0.14.1,Test sparse estimator
v0.14.1,"--> test coef_, intercept_"
v0.14.1,--> test treatment effects
v0.14.1,Restrict x_test to vectors of norm < 1
v0.14.1,--> check inference
v0.14.1,Check that a majority of true effects lie in the 5-95% CI
v0.14.1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.14.1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.14.1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.14.1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.14.1,sparse test case: heterogeneous effect by product
v0.14.1,need at least as many rows in e_y as there are distinct columns
v0.14.1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.14.1,create a simple artificial setup where effect of moving from treatment
v0.14.1,"a -> b is 2,"
v0.14.1,"a -> c is 1, and"
v0.14.1,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.14.1,"Using an uneven number of examples from different classes,"
v0.14.1,"and having the treatments in non-lexicographic order,"
v0.14.1,should rule out some basic issues.
v0.14.1,Note that explicitly specifying the dtype as object is necessary until
v0.14.1,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.14.1,estimated effects should be identical when treatment is explicitly given
v0.14.1,but const_marginal_effect should be reordered based on the explicit cagetories
v0.14.1,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.14.1,test outer grouping
v0.14.1,test nested grouping
v0.14.1,DML nested CV works via a 'cv' attribute
v0.14.1,"with 2-fold outer and 2-fold inner grouping, and six total groups,"
v0.14.1,should get 1 or 2 groups per split
v0.14.1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.14.1,whichever groups we see
v0.14.1,test nested grouping
v0.14.1,"by default, we use 5 split cross-validation for our T and Y models"
v0.14.1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.14.1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.14.1,"Try default, integer, and new user-passed treatment name"
v0.14.1,FunctionTransformers are agnostic to passed treatment names
v0.14.1,Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Set random seed
v0.14.1,Generate data
v0.14.1,DGP constants
v0.14.1,Test data
v0.14.1,Constant treatment effect and propensity
v0.14.1,Heterogeneous treatment and propensity
v0.14.1,ensure that we've got at least two of every element
v0.14.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.14.1,ensure that we can serialize unfit estimator
v0.14.1,ensure that we can serialize fit estimator
v0.14.1,make sure we can call the marginal_effect and effect methods
v0.14.1,test const marginal inference
v0.14.1,test effect inference
v0.14.1,test marginal effect inference
v0.14.1,test coef_ and intercept_ inference
v0.14.1,verify we can generate the summary
v0.14.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.14.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.14.1,create a simple artificial setup where effect of moving from treatment
v0.14.1,"1 -> 2 is 2,"
v0.14.1,"1 -> 3 is 1, and"
v0.14.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.14.1,"Using an uneven number of examples from different classes,"
v0.14.1,"and having the treatments in non-lexicographic order,"
v0.14.1,Should rule out some basic issues.
v0.14.1,test that we can fit with a KFold instance
v0.14.1,test that we can fit with a train/test iterable
v0.14.1,"for at least some of the examples, the CI should have nonzero width"
v0.14.1,"for at least some of the examples, the CI should have nonzero width"
v0.14.1,"for at least some of the examples, the CI should have nonzero width"
v0.14.1,test coef__inference function works
v0.14.1,test intercept__inference function works
v0.14.1,test summary function works
v0.14.1,Test inputs
v0.14.1,self._test_inputs(DR_learner)
v0.14.1,Test constant treatment effect
v0.14.1,Test heterogeneous treatment effect
v0.14.1,Test heterogenous treatment effect for W =/= None
v0.14.1,Sparse DGP
v0.14.1,Treatment effect coef
v0.14.1,Other coefs
v0.14.1,Features and controls
v0.14.1,Test sparse estimator
v0.14.1,"--> test coef_, intercept_"
v0.14.1,--> test treatment effects
v0.14.1,Restrict x_test to vectors of norm < 1
v0.14.1,--> check inference
v0.14.1,Check that a majority of true effects lie in the 5-95% CI
v0.14.1,test outer grouping
v0.14.1,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.14.1,test nested grouping
v0.14.1,DML nested CV works via a 'cv' attribute
v0.14.1,"with 2-fold outer and 2-fold inner grouping, and six total groups,"
v0.14.1,should get 1 or 2 groups per split
v0.14.1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.14.1,whichever groups we see
v0.14.1,test nested grouping
v0.14.1,"by default, we use 5 split cross-validation for our T and Y models"
v0.14.1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.14.1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.14.1,helper class
v0.14.1,Fit learner and get the effect
v0.14.1,Get the true treatment effect
v0.14.1,Compute treatment effect residuals (absolute)
v0.14.1,Check that at least 90% of predictions are within tolerance interval
v0.14.1,Only for heterogeneous TE
v0.14.1,Fit learner on X and W and get the effect
v0.14.1,Get the true treatment effect
v0.14.1,Compute treatment effect residuals (absolute)
v0.14.1,Check that at least 90% of predictions are within tolerance interval
v0.14.1,Check that one can pass in regular lists
v0.14.1,Check that it fails correctly if lists of different shape are passed in
v0.14.1,Check that it fails when T contains values other than 0 and 1
v0.14.1,"Check that it works when T, Y have shape (n, 1)"
v0.14.1,Generate covariates
v0.14.1,Generate treatment
v0.14.1,Calculate outcome
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,DGP constants
v0.14.1,DGP coefficients
v0.14.1,Generated outcomes
v0.14.1,################
v0.14.1,WeightedLasso #
v0.14.1,################
v0.14.1,Define weights
v0.14.1,Define extended datasets
v0.14.1,Range of alphas
v0.14.1,Compare with Lasso
v0.14.1,--> No intercept
v0.14.1,--> With intercept
v0.14.1,When DGP has no intercept
v0.14.1,When DGP has intercept
v0.14.1,--> Coerce coefficients to be positive
v0.14.1,--> Toggle max_iter & tol
v0.14.1,Define weights
v0.14.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.14.1,Mixed DGP scenario.
v0.14.1,Define extended datasets
v0.14.1,Define weights
v0.14.1,Define multioutput
v0.14.1,##################
v0.14.1,WeightedLassoCV #
v0.14.1,##################
v0.14.1,Define alphas to test
v0.14.1,Compare with LassoCV
v0.14.1,--> No intercept
v0.14.1,--> With intercept
v0.14.1,--> Force parameters to be positive
v0.14.1,Choose a smaller n to speed-up process
v0.14.1,Compare fold weights
v0.14.1,Define weights
v0.14.1,Define extended datasets
v0.14.1,Define splitters
v0.14.1,WeightedKFold splitter
v0.14.1,Map weighted splitter to an extended splitter
v0.14.1,Define alphas to test
v0.14.1,Compare with LassoCV
v0.14.1,--> No intercept
v0.14.1,--> With intercept
v0.14.1,--> Force parameters to be positive
v0.14.1,###########################
v0.14.1,MultiTaskWeightedLassoCV #
v0.14.1,###########################
v0.14.1,Define alphas to test
v0.14.1,Define splitter
v0.14.1,Compare with MultiTaskLassoCV
v0.14.1,--> No intercept
v0.14.1,--> With intercept
v0.14.1,Define weights
v0.14.1,Define extended datasets
v0.14.1,Define splitters
v0.14.1,WeightedKFold splitter
v0.14.1,Map weighted splitter to an extended splitter
v0.14.1,Define alphas to test
v0.14.1,Compare with LassoCV
v0.14.1,--> No intercept
v0.14.1,--> With intercept
v0.14.1,#########################
v0.14.1,WeightedLassoCVWrapper #
v0.14.1,#########################
v0.14.1,perform 1D fit
v0.14.1,perform 2D fit
v0.14.1,################
v0.14.1,DebiasedLasso #
v0.14.1,################
v0.14.1,Test DebiasedLasso without weights
v0.14.1,--> Check debiased coeffcients without intercept
v0.14.1,--> Check debiased coeffcients with intercept
v0.14.1,--> Check 5-95 CI coverage for unit vectors
v0.14.1,Test DebiasedLasso with weights for one DGP
v0.14.1,Define weights
v0.14.1,Define extended datasets
v0.14.1,--> Check debiased coefficients
v0.14.1,Define weights
v0.14.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.14.1,--> Check debiased coeffcients
v0.14.1,Test that attributes propagate correctly
v0.14.1,Test MultiOutputDebiasedLasso without weights
v0.14.1,--> Check debiased coeffcients without intercept
v0.14.1,--> Check debiased coeffcients with intercept
v0.14.1,--> Check CI coverage
v0.14.1,Test MultiOutputDebiasedLasso with weights
v0.14.1,Define weights
v0.14.1,Define extended datasets
v0.14.1,--> Check debiased coefficients
v0.14.1,Unit vectors
v0.14.1,Unit vectors
v0.14.1,Check coeffcients and intercept are the same within tolerance
v0.14.1,Check results are similar with tolerance 1e-6
v0.14.1,Check if multitask
v0.14.1,Check that same alpha is chosen
v0.14.1,Check that the coefficients are similar
v0.14.1,selective ridge has a simple implementation that we can test against
v0.14.1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.14.1,"it should be the case that when we set fit_intercept to true,"
v0.14.1,it doesn't matter whether the penalized model also fits an intercept or not
v0.14.1,create an extra copy of rows with weight 2
v0.14.1,"instead of a slice, explicitly return an array of indices"
v0.14.1,_penalized_inds is only set during fitting
v0.14.1,cv exists on penalized model
v0.14.1,now we can access _penalized_inds
v0.14.1,check that we can read the cv attribute back out from the underlying model
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"global and cohort data should have exactly the same structure, but different values"
v0.14.1,local index should have as many times entries as global as there were rows passed in
v0.14.1,continuous treatments have typical treatment values equal to
v0.14.1,the mean of the absolute value of non-zero entries
v0.14.1,discrete treatments have typical treatment value 1
v0.14.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.14.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.14.1,policy value should exceed always treating with any treatment
v0.14.1,"global shape is (d_y, sum(d_t))"
v0.14.1,global and cohort row-wise dicts have d_y * d_t entries
v0.14.1,local dictionary is flattened to n_rows * d_y * d_t
v0.14.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.1,features; for categoricals they should appear #cats-1 times each
v0.14.1,"global and cohort data should have exactly the same structure, but different values"
v0.14.1,local index should have as many times entries as global as there were rows passed in
v0.14.1,features; for categoricals they should appear #cats-1 times each
v0.14.1,"global shape is (d_y, sum(d_t))"
v0.14.1,global and cohort row-wise dicts have d_y * d_t entries
v0.14.1,local dictionary is flattened to n_rows * d_y * d_t
v0.14.1,continuous treatments have typical treatment values equal to
v0.14.1,the mean of the absolute value of non-zero entries
v0.14.1,discrete treatments have typical treatment value 1
v0.14.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.14.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.14.1,policy value should exceed always treating with any treatment
v0.14.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.1,"global and cohort data should have exactly the same structure, but different values"
v0.14.1,local index should have as many times entries as global as there were rows passed in
v0.14.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.14.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.14.1,policy value should exceed always treating with any treatment
v0.14.1,"global shape is (d_y, sum(d_t))"
v0.14.1,global and cohort row-wise dicts have d_y * d_t entries
v0.14.1,local dictionary is flattened to n_rows * d_y * d_t
v0.14.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.1,features; for categoricals they should appear #cats-1 times each
v0.14.1,make sure we don't run into problems dropping every index
v0.14.1,"global and cohort data should have exactly the same structure, but different values"
v0.14.1,local index should have as many times entries as global as there were rows passed in
v0.14.1,"global shape is (d_y, sum(d_t))"
v0.14.1,global and cohort row-wise dicts have d_y * d_t entries
v0.14.1,local dictionary is flattened to n_rows * d_y * d_t
v0.14.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.14.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.14.1,policy value should exceed always treating with any treatment
v0.14.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.1,"global and cohort data should have exactly the same structure, but different values"
v0.14.1,local index should have as many times entries as global as there were rows passed in
v0.14.1,features; for categoricals they should appear #cats-1 times each
v0.14.1,"global shape is (d_y, sum(d_t))"
v0.14.1,global and cohort row-wise dicts have d_y * d_t entries
v0.14.1,local dictionary is flattened to n_rows * d_y * d_t
v0.14.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.14.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.14.1,policy value should exceed always treating with any treatment
v0.14.1,dgp
v0.14.1,model
v0.14.1,model
v0.14.1,"columns 'd', 'e', 'h' have too many values"
v0.14.1,"columns 'd', 'e' have too many values"
v0.14.1,lowering bound shouldn't affect already fit columns when warm starting
v0.14.1,"column d is now okay, too"
v0.14.1,verify that we can use a scalar treatment cost
v0.14.1,verify that we can specify per-treatment costs for each sample
v0.14.1,verify that using the same state returns the same results each time
v0.14.1,set the categories for column 'd' explicitly so that b is default
v0.14.1,"first column: 10 ones, this is fine"
v0.14.1,"second column: 6 categories, plenty of random instances of each"
v0.14.1,this is fine only if we increase the cateogry limit
v0.14.1,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.14.1,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.14.1,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.14.1,forest heterogeneity won't work
v0.14.1,"sixth column: just 1 one, not enough even without check"
v0.14.1,increase bound on cat expansion
v0.14.1,skip checks (reducing folds accordingly)
v0.14.1,"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
v0.14.1,"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
v0.14.1,Pass an example where W is irrelevant and X is confounder
v0.14.1,"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
v0.14.1,zeroed out and the test will fail
v0.14.1,"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
v0.14.1,rescaling X shouldn't affect the first stage models because they normalize the inputs
v0.14.1,"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
v0.14.1,cross terms
v0.14.1,scale by 1000 to match the input to this model:
v0.14.1,"the scale of X does matter for the final model, which keeps results in user-denominated units"
v0.14.1,rescaling X still shouldn't affect the first stage models
v0.14.1,TODO: we don't recover the correct values with enough accuracy to enable this assertion
v0.14.1,is there a different way to verify that we are learning the correct coefficients?
v0.14.1,"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,DGP constants
v0.14.1,Define data features
v0.14.1,Added `_df`to names to be different from the default cate_estimator names
v0.14.1,Generate data
v0.14.1,################################
v0.14.1,Single treatment and outcome #
v0.14.1,################################
v0.14.1,Test LinearDML
v0.14.1,|--> Test featurizers
v0.14.1,"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
v0.14.1,|--> Test re-fit
v0.14.1,Test SparseLinearDML
v0.14.1,Test ForestDML
v0.14.1,###################################
v0.14.1,Mutiple treatments and outcomes #
v0.14.1,###################################
v0.14.1,Test LinearDML
v0.14.1,Test SparseLinearDML
v0.14.1,"Single outcome only, ORF does not support multiple outcomes"
v0.14.1,Test DMLOrthoForest
v0.14.1,Test DROrthoForest
v0.14.1,Test XLearner
v0.14.1,Skipping population summary names test because bootstrap inference is too slow
v0.14.1,Test SLearner
v0.14.1,Test TLearner
v0.14.1,Test LinearDRLearner
v0.14.1,Test SparseLinearDRLearner
v0.14.1,Test ForestDRLearner
v0.14.1,Test LinearIntentToTreatDRIV
v0.14.1,Test DeepIV
v0.14.1,Test categorical treatments
v0.14.1,Check refit
v0.14.1,Check refit after setting categories
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Linear models are required for parametric dml
v0.14.1,sample weighting models are required for nonparametric dml
v0.14.1,Test values
v0.14.1,TLearner test
v0.14.1,Instantiate TLearner
v0.14.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.14.1,Test constant treatment effect with multi output Y
v0.14.1,Test heterogeneous treatment effect
v0.14.1,Need interactions between T and features
v0.14.1,Test heterogeneous treatment effect with multi output Y
v0.14.1,Instantiate DomainAdaptationLearner
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,test base values equals to mean of constant marginal effect
v0.14.1,test shape of shap values output is as expected
v0.14.1,test shape of attribute of explanation object is as expected
v0.14.1,test length of feature names equals to shap values shape
v0.14.1,test base values equals to mean of constant marginal effect
v0.14.1,test shape of shap values output is as expected
v0.14.1,test shape of attribute of explanation object is as expected
v0.14.1,test length of feature names equals to shap values shape
v0.14.1,Treatment effect function
v0.14.1,Outcome support
v0.14.1,Treatment support
v0.14.1,"Generate controls, covariates, treatments and outcomes"
v0.14.1,Heterogeneous treatment effects
v0.14.1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.14.1,through shap package.
v0.14.1,test shap could generate the plot from the shap_values
v0.14.1,"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Check inputs
v0.14.1,Check inputs
v0.14.1,Check inputs
v0.14.1,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.14.1,"Thus, we append the controls column before the one-hot-encoded T"
v0.14.1,"We might want to revisit, though, since it's linearly determined by the others"
v0.14.1,Check inputs
v0.14.1,Check inputs
v0.14.1,Estimate response function
v0.14.1,Check inputs
v0.14.1,Train model on controls. Assign higher weight to units resembling
v0.14.1,treated units.
v0.14.1,Train model on the treated. Assign higher weight to units resembling
v0.14.1,control units.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,TODO: make sure to use random seeds wherever necessary
v0.14.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.14.1,"unfortunately with the Theano and Tensorflow backends,"
v0.14.1,the straightforward use of K.stop_gradient can cause an error
v0.14.1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.14.1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.14.1,so that those layers remain connected but with 0 gradient
v0.14.1,|| t - mu_i || ^2
v0.14.1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.14.1,Use logsumexp for numeric stability:
v0.14.1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.14.1,TODO: does the numeric stability actually make any difference?
v0.14.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.14.1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.14.1,generate cumulative sum via matrix multiplication
v0.14.1,"Generate standard uniform values in shape (batch_size,1)"
v0.14.1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.14.1,we use uniform_like instead with an input of an appropriate shape)
v0.14.1,convert to floats and multiply to perform equivalent of logical AND
v0.14.1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.14.1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.14.1,we use normal_like instead with an input of an appropriate shape)
v0.14.1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.14.1,prevent gradient from passing through sampling
v0.14.1,three options: biased or upper-bound loss require a single number of samples;
v0.14.1,unbiased can take different numbers for the network and its gradient
v0.14.1,"sample: (() -> Layer, int) -> Layer"
v0.14.1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.14.1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.14.1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.14.1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.14.1,the dimensionality of the output of the network
v0.14.1,TODO: is there a more robust way to do this?
v0.14.1,TODO: do we need to give the user more control over other arguments to fit?
v0.14.1,"subtle point: we need to build a new model each time,"
v0.14.1,because each model encapsulates its randomness
v0.14.1,TODO: do we need to give the user more control over other arguments to fit?
v0.14.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.14.1,not a general tensor (because of how backprop works in every framework)
v0.14.1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.14.1,but this seems annoying...)
v0.14.1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.14.1,TODO: any way to get this to work on batches of arbitrary size?
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.14.1,"fit on projected Z: E[T * E[T|X,Z]|X]"
v0.14.1,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.14.1,"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
v0.14.1,"shape (n,)"
v0.14.1,"shape (n,)"
v0.14.1,"shape(n,)"
v0.14.1,TODO: prel_model_effect could allow sample_var and freq_weight?
v0.14.1,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.14.1,"we need to undo the one-hot encoding for calling effect,"
v0.14.1,since it expects raw values
v0.14.1,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.14.1,target will be discrete and will be inversed from FirstStageWrapper
v0.14.1,"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
v0.14.1,reshape the predictions
v0.14.1,concat W and Z
v0.14.1,check nuisances outcome shape
v0.14.1,Y_res could be a vector or 1-dimensional 2d-array
v0.14.1,"all could be reshaped to vector since Y, T, Z are all single dimensional."
v0.14.1,Estimate final model of theta(X) by minimizing the square loss:
v0.14.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.14.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.14.1,at the expense of some small bias. For points with very small covariance we revert
v0.14.1,to the model-based preliminary estimate and do not add the correction term.
v0.14.1,A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
v0.14.1,Used by both DRIV and IntentToTreatDRIV.
v0.14.1,Maggie: I think that would be the case?
v0.14.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.14.1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.14.1,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.14.1,attribute so that the trained featurizer will be passed through
v0.14.1,Handles the corner case when X=None but featurizer might be not None
v0.14.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.14.1,this is a regression model since proj_t is probability
v0.14.1,outcome is continuous since proj_t is probability
v0.14.1,Define the data generation functions
v0.14.1,Define the data generation functions
v0.14.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.14.1,Define the data generation functions
v0.14.1,TODO: support freq_weight and sample_var in debiased lasso
v0.14.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.14.1,Define the data generation functions
v0.14.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.14.1,concat W and Z
v0.14.1,"we need to undo the one-hot encoding for calling effect,"
v0.14.1,since it expects raw values
v0.14.1,concat W and Z
v0.14.1,"we need to undo the one-hot encoding for calling effect,"
v0.14.1,since it expects raw values
v0.14.1,reshape the predictions
v0.14.1,"T_res, Z_res, beta expect shape to be (n,1)"
v0.14.1,Define the data generation functions
v0.14.1,maybe shouldn't expose fit_cate_intercept in this class?
v0.14.1,Define the data generation functions
v0.14.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.14.1,TODO: do correct adjustment for sample_var
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,concat W and Z
v0.14.1,concat W and Z
v0.14.1,concat W and Z
v0.14.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.14.1,Define the data generation functions
v0.14.1,"train E[T|X,W,Z]"
v0.14.1,"train [Z|X,W]"
v0.14.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.14.1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.14.1,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.14.1,attribute so that the trained featurizer will be passed through
v0.14.1,Handles the corner case when X=None but featurizer might be not None
v0.14.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.14.1,concat W and Z
v0.14.1,note that groups are not passed to score because they are only used for fitting
v0.14.1,concat W and Z
v0.14.1,note that sample_weight and groups are not passed to predict because they are only used for fitting
v0.14.1,concat W and Z
v0.14.1,A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
v0.14.1,Used by both Parametric and Non Parametric DMLIV.
v0.14.1,override only so that we can enforce Z to be required
v0.14.1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.14.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.14.1,for internal use by the library
v0.14.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.14.1,Handles the corner case when X=None but featurizer might be not None
v0.14.1,Define the data generation functions
v0.14.1,Get input names
v0.14.1,Summary
v0.14.1,coefficient
v0.14.1,intercept
v0.14.1,Define the data generation functions
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,"this will have dimension (d,) + shape(X)"
v0.14.1,send the first dimension to the end
v0.14.1,columns are featurized independently; partial derivatives are only non-zero
v0.14.1,when taken with respect to the same column each time
v0.14.1,don't fit intercept; manually add column of ones to the data instead;
v0.14.1,this allows us to ignore the intercept when computing marginal effects
v0.14.1,make T 2D if if was a vector
v0.14.1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.14.1,two stage approximation
v0.14.1,"first, get basis expansions of T, X, and Z"
v0.14.1,TODO: is it right that the effective number of intruments is the
v0.14.1,"product of ft_X and ft_Z, not just ft_Z?"
v0.14.1,"regress T expansion on X,Z expansions concatenated with W"
v0.14.1,"predict ft_T from interacted ft_X, ft_Z"
v0.14.1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.14.1,dT may be only 2-dimensional)
v0.14.1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.14.1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,TODO: this utility is documented but internal; reimplement?
v0.14.1,TODO: this utility is even less public...
v0.14.1,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.14.1,use same Cs as would be used by default by LogisticRegressionCV
v0.14.1,NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
v0.14.1,which could affect how many times each distinct Y value needs to be present in the data
v0.14.1,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.14.1,but also supports get_feature_names with expected signature
v0.14.1,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.14.1,NOTE: we rely on the passthrough columns coming first in the concatenated X;W
v0.14.1,"when we pipeline scaling with our first stage models later, so the order here is important"
v0.14.1,TODO: remove once older sklearn support is no longer needed
v0.14.1,Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
v0.14.1,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.14.1,Convert SingleTreeInterpreter to a python dictionary
v0.14.1,named tuple type for storing results inside CausalAnalysis class;
v0.14.1,must be lifted to module level to enable pickling
v0.14.1,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
v0.14.1,"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
v0.14.1,"when running the first stage models, but don't want to scale the X columns when running the final model,"
v0.14.1,since then our coefficients will have odd units and our trees will also have decisions using those units.
v0.14.1,
v0.14.1,"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
v0.14.1,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.14.1,Controls are all other columns of X
v0.14.1,"can't use X[:, feat_ind] when X is a DataFrame"
v0.14.1,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.14.1,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.14.1,so that the user can opt-in to allowing unseen treatment values
v0.14.1,(and return NaN or something in that case)
v0.14.1,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
v0.14.1,and so we can just peel the first columns off of that combined array for rescaling in the pipeline
v0.14.1,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
v0.14.1,"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
v0.14.1,becomes a valid approach to handling this
v0.14.1,array checking routines don't accept 0-width arrays
v0.14.1,perform model selection
v0.14.1,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.14.1,convert to NormalInferenceResults for consistency
v0.14.1,Set the dictionary values shared between local and global summaries
v0.14.1,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.14.1,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.14.1,required to fit a discrete DML model
v0.14.1,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
v0.14.1,sub-cases of models or also integrate with azure autoML. (post-MVP)
v0.14.1,"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
v0.14.1,"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
v0.14.1,TODO: Enable multi-class classification (post-MVP)
v0.14.1,Validate inputs
v0.14.1,TODO: check compatibility of X and Y lengths
v0.14.1,"no previous fit, cancel warm start"
v0.14.1,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.14.1,"if heterogeneity_inds is 1D, repeat it"
v0.14.1,heterogeneity inds should be a 2D list of length same as train_inds
v0.14.1,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.14.1,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.14.1,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.14.1,train the Y model
v0.14.1,"perform model selection for the Y model using all X, not on a per-column basis"
v0.14.1,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.14.1,work with the regression wrapper
v0.14.1,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.14.1,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.14.1,since otherwise we'll have too many columns to be able to train a classifier
v0.14.1,start with empty results and default shared insights
v0.14.1,convert categorical indicators to numeric indices
v0.14.1,check for indices over the categorical expansion bound
v0.14.1,assume we'll be able to train former failures this time; we'll add them back if not
v0.14.1,"can't remove in place while iterating over new_inds, so store in separate list"
v0.14.1,"train the model, but warn"
v0.14.1,no model can be trained in this case since we need more folds
v0.14.1,"don't train a model, but suggest workaround since there are enough instances of least"
v0.14.1,populated class
v0.14.1,also remove from train_inds so we don't try to access the result later
v0.14.1,extract subset of names matching new columns
v0.14.1,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.14.1,don't want to cache this failed result
v0.14.1,properties to return from effect InferenceResults
v0.14.1,properties to return from PopulationSummaryResults
v0.14.1,Converts strings to property lookups or method calls as a convenience so that the
v0.14.1,_point_props and _summary_props above can be applied to an inference object
v0.14.1,Create a summary combining all results into a single output; this is used
v0.14.1,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.14.1,"or a dictionary, respectively, based on the summary function passed into this method"
v0.14.1,"ensure array has shape (m,y,t)"
v0.14.1,population summary is missing sample dimension; add it for consistency
v0.14.1,outcome dimension is missing; add it for consistency
v0.14.1,add singleton treatment dimension if missing
v0.14.1,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.14.1,"each attr has dimension (m,y) or (m,y,t)"
v0.14.1,concatenate along treatment dimension
v0.14.1,"for dictionary representation, want to remove unneeded sample dimension"
v0.14.1,in cohort and global results
v0.14.1,TODO: enrich outcome logic for multi-class classification when that is supported
v0.14.1,There is no actual sample level in this data
v0.14.1,can't drop only level
v0.14.1,should be serialization-ready and contain no numpy arrays
v0.14.1,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.14.1,TODO: Note that there's no column metadata for the sample number - should there be?
v0.14.1,"need to replicate the column info for each sample, then remove from the shared data"
v0.14.1,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.14.1,which may need to be revisited once we support multiclass
v0.14.1,get the length of the list corresponding to the first dictionary key
v0.14.1,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.14.1,a global inference indicates the effect of that one feature on the outcome
v0.14.1,need to reshape the output to match the input
v0.14.1,we want to offset the inference object by the baseline estimate of y
v0.14.1,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.14.1,get the length of the list corresponding to the first dictionary key
v0.14.1,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.14.1,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.14.1,because then scaling the difference between treatment value and treatment costs is the
v0.14.1,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.14.1,
v0.14.1,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.14.1,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.14.1,(rather than just not treating at all)
v0.14.1,
v0.14.1,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.14.1,2 * policy_value - always_treat
v0.14.1,includes exactly their contribution because policy_value and always_treat both include it
v0.14.1,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.14.1,2 * policy_value - always-treat
v0.14.1,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.14.1,is zero and the contribution to always_treat is negative
v0.14.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.14.1,"however, the tree can't store the feature and treatment names we compute here..."
v0.14.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.14.1,"however, the tree can't store the feature and treatment names we compute here..."
v0.14.1,get dataframe with all but selected column
v0.14.1,apply 10% of a typical treatment for this feature
v0.14.1,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.14.1,set the effect bounds; for positive treatments these agree with
v0.14.1,"the estimates; for negative treatments, we need to invert the interval"
v0.14.1,the effect is now always positive since we decrease treatment when negative
v0.14.1,"for discrete treatment, stack a zero result in front for control"
v0.14.1,we need to call effect_inference to get the correct CI between the two treatment options
v0.14.1,we now need to construct the delta in the cost between the two treatments and translate the effect
v0.14.1,remove third dimenions potentially added
v0.14.1,"find cost of current treatment: equality creates a 2d array with True on each row,"
v0.14.1,only if its the location of the current treatment. Then we take the corresponding cost.
v0.14.1,construct index of current treatment
v0.14.1,add second dimension if needed for broadcasting during translation of effect
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,TODO: conisder working around relying on sklearn implementation details
v0.14.1,"Found a good split, return."
v0.14.1,Record all splits in case the stratification by weight yeilds a worse partition
v0.14.1,Reseed random generator and try again
v0.14.1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.14.1,"Found a good split, return."
v0.14.1,Did not find a good split
v0.14.1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.14.1,Return most weight-balanced partition
v0.14.1,Weight stratification algorithm
v0.14.1,Sort weights for weight strata search
v0.14.1,There are some leftover indices that have yet to be assigned
v0.14.1,Append stratum splits to overall splits
v0.14.1,"If classification methods produce multiple columns of output,"
v0.14.1,we need to manually encode classes to ensure consistent column ordering.
v0.14.1,We clone the estimator to make sure that all the folds are
v0.14.1,"independent, and that it is pickle-able."
v0.14.1,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.14.1,`predictions` is a list of method outputs from each fold.
v0.14.1,"If each of those is also a list, then treat this as a"
v0.14.1,multioutput-multiclass task. We need to separately concatenate
v0.14.1,the method outputs for each label into an `n_labels` long list.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Our classes that derive from sklearn ones sometimes include
v0.14.1,inherited docstrings that have embedded doctests; we need the following imports
v0.14.1,so that they don't break.
v0.14.1,TODO: consider working around relying on sklearn implementation details
v0.14.1,"TODO: once we drop support for sklearn < 1.0, we can remove this"
v0.14.1,"if we're decorating a class, just update the __init__ method,"
v0.14.1,so that the result is still a class instead of a wrapper method
v0.14.1,normalize was deprecated or removed; don't need to do anything
v0.14.1,"Convert X, y into numpy arrays"
v0.14.1,Define fit parameters
v0.14.1,Some algorithms don't have a check_input option
v0.14.1,Check weights array
v0.14.1,Check that weights are size-compatible
v0.14.1,Normalize inputs
v0.14.1,Weight inputs
v0.14.1,Fit base class without intercept
v0.14.1,Fit Lasso
v0.14.1,Reset intercept
v0.14.1,The intercept is not calculated properly due the sqrt(weights) factor
v0.14.1,so it must be recomputed
v0.14.1,Fit lasso without weights
v0.14.1,Make weighted splitter
v0.14.1,Fit weighted model
v0.14.1,Make weighted splitter
v0.14.1,Fit weighted model
v0.14.1,Call weighted lasso on reduced design matrix
v0.14.1,Weighted tau
v0.14.1,Select optimal penalty
v0.14.1,Warn about consistency
v0.14.1,"Convert X, y into numpy arrays"
v0.14.1,Fit weighted lasso with user input
v0.14.1,"Center X, y"
v0.14.1,Calculate quantities that will be used later on. Account for centered data
v0.14.1,Calculate coefficient and error variance
v0.14.1,Add coefficient correction
v0.14.1,Set coefficients and intercept standard errors
v0.14.1,Set intercept
v0.14.1,Return alpha to 'auto' state
v0.14.1,"Note that in the case of no intercept, X_offset is 0"
v0.14.1,Calculate the variance of the predictions
v0.14.1,Calculate prediction confidence intervals
v0.14.1,Assumes flattened y
v0.14.1,Compute weighted residuals
v0.14.1,To be done once per target. Assumes y can be flattened.
v0.14.1,Assumes that X has already been offset
v0.14.1,Special case: n_features=1
v0.14.1,Compute Lasso coefficients for the columns of the design matrix
v0.14.1,Compute C_hat
v0.14.1,Compute theta_hat
v0.14.1,Allow for single output as well
v0.14.1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.14.1,Set coef_ attribute
v0.14.1,Set intercept_ attribute
v0.14.1,Set selected_alpha_ attribute
v0.14.1,Set coef_stderr_
v0.14.1,intercept_stderr_
v0.14.1,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.14.1,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.14.1,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.14.1,set intercept_ attribute
v0.14.1,set coef_ attribute
v0.14.1,set alpha_ attribute
v0.14.1,set alphas_ attribute
v0.14.1,set n_iter_ attribute
v0.14.1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.14.1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.14.1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.14.1,now regress X1 on y - X2 * beta2 to learn beta1
v0.14.1,set coef_ and intercept_ attributes
v0.14.1,Note that the penalized model should *not* have an intercept
v0.14.1,don't proxy special methods
v0.14.1,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.14.1,regressor incorrectly
v0.14.1,"Note: for known attributes that have been set this method will not be called,"
v0.14.1,so we should just throw here because this is an attribute belonging to this class
v0.14.1,but which hasn't yet been set on this instance
v0.14.1,set default values for None
v0.14.1,check freq_weight should be integer and should be accompanied by sample_var
v0.14.1,check array shape
v0.14.1,weight X and y and sample_var
v0.14.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.14.1,set default values for None
v0.14.1,check array shape
v0.14.1,check dimension of instruments is more than dimension of treatments
v0.14.1,weight X and y
v0.14.1,learn point estimate
v0.14.1,solve first stage linear regression E[T|Z]
v0.14.1,"""that"" means T̂"
v0.14.1,solve second stage linear regression E[Y|that]
v0.14.1,(T̂.T*T̂)^{-1}
v0.14.1,learn cov(theta)
v0.14.1,(T̂.T*T̂)^{-1}
v0.14.1,sigma^2
v0.14.1,reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,AzureML
v0.14.1,helper imports
v0.14.1,write the details of the workspace to a configuration file to the notebook library
v0.14.1,if y is a multioutput model
v0.14.1,Make sure second dimension has 1 or more item
v0.14.1,switch _inner Model to a MultiOutputRegressor
v0.14.1,flatten array as automl only takes vectors for y
v0.14.1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.14.1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.14.1,as an sklearn estimator
v0.14.1,fit implementation for a single output model.
v0.14.1,Create experiment for specified workspace
v0.14.1,Configure automl_config with training set information.
v0.14.1,"Wait for remote run to complete, the set the model"
v0.14.1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.14.1,create model and pass model into final.
v0.14.1,"If item is an automl config, get its corresponding"
v0.14.1,AutomatedML Model and add it to new_Args
v0.14.1,"If item is an automl config, get its corresponding"
v0.14.1,AutomatedML Model and set it for this key in
v0.14.1,kwargs
v0.14.1,takes in either automated_ml config and instantiates
v0.14.1,an AutomatedMLModel
v0.14.1,The prefix can only be 18 characters long
v0.14.1,"because prefixes come from kwarg_names, we must ensure they are"
v0.14.1,short enough.
v0.14.1,Get workspace from config file.
v0.14.1,Take the intersect of the white for sample
v0.14.1,weights and linear models
v0.14.1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,average the outcome dimension if it exists and ensure 2d y_pred
v0.14.1,get index of best treatment
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,TODO: consider working around relying on sklearn implementation details
v0.14.1,Create splits of causal tree
v0.14.1,Make sure the correct exception is being rethrown
v0.14.1,Must make sure indices are merged correctly
v0.14.1,Convert rows to columns
v0.14.1,Require group assignment t to be one-hot-encoded
v0.14.1,Get predictions for the 2 splits
v0.14.1,Must make sure indices are merged correctly
v0.14.1,Crossfitting
v0.14.1,Compute weighted nuisance estimates
v0.14.1,-------------------------------------------------------------------------------
v0.14.1,Calculate the covariance matrix corresponding to the BLB inference
v0.14.1,
v0.14.1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.14.1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.14.1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.14.1,in that slice from the overall parameter estimate.
v0.14.1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.14.1,-------------------------------------------------------------------------------
v0.14.1,Calclulate covariance matrix through BLB
v0.14.1,Estimators
v0.14.1,OrthoForest parameters
v0.14.1,Sub-forests
v0.14.1,Auxiliary attributes
v0.14.1,Fit check
v0.14.1,TODO: Check performance
v0.14.1,Must normalize weights
v0.14.1,Override the CATE inference options
v0.14.1,Add blb inference to parent's options
v0.14.1,Generate subsample indices
v0.14.1,Build trees in parallel
v0.14.1,Bootstraping has repetitions in tree sample
v0.14.1,Similar for `a` weights
v0.14.1,Bootstraping has repetitions in tree sample
v0.14.1,Define subsample size
v0.14.1,Safety check
v0.14.1,Draw points to create little bags
v0.14.1,Copy and/or define models
v0.14.1,Define nuisance estimators
v0.14.1,Define parameter estimators
v0.14.1,Define
v0.14.1,Need to redefine fit here for auto inference to work due to a quirk in how
v0.14.1,wrap_fit is defined
v0.14.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.14.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.14.1,Override to flatten output if T is flat
v0.14.1,Check that all discrete treatments are represented
v0.14.1,Nuissance estimates evaluated with cross-fitting
v0.14.1,Define 2-fold iterator
v0.14.1,Check if there is only one example of some class
v0.14.1,Define 2-fold iterator
v0.14.1,need safe=False when cloning for WeightedModelWrapper
v0.14.1,Compute residuals
v0.14.1,Compute coefficient by OLS on residuals
v0.14.1,"Parameter returned by LinearRegression is (d_T, )"
v0.14.1,Compute residuals
v0.14.1,Compute coefficient by OLS on residuals
v0.14.1,ell_2 regularization
v0.14.1,Ridge regression estimate
v0.14.1,"Parameter returned is of shape (d_T, )"
v0.14.1,Return moments and gradients
v0.14.1,Compute residuals
v0.14.1,Compute moments
v0.14.1,"Moments shape is (n, d_T)"
v0.14.1,Compute moment gradients
v0.14.1,returns shape-conforming residuals
v0.14.1,Copy and/or define models
v0.14.1,Define parameter estimators
v0.14.1,Define moment and mean gradient estimator
v0.14.1,"Check that T is shape (n, )"
v0.14.1,Check T is numeric
v0.14.1,Train label encoder
v0.14.1,Call `fit` from parent class
v0.14.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.14.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.14.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.1,Override to flatten output if T is flat
v0.14.1,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.1,Expand one-hot encoding to include the zero treatment
v0.14.1,"Test that T contains all treatments. If not, return None"
v0.14.1,Nuissance estimates evaluated with cross-fitting
v0.14.1,Define 2-fold iterator
v0.14.1,Check if there is only one example of some class
v0.14.1,No need to crossfit for internal nodes
v0.14.1,Compute partial moments
v0.14.1,"If any of the values in the parameter estimate is nan, return None"
v0.14.1,Compute partial moments
v0.14.1,Compute coefficient by OLS on residuals
v0.14.1,ell_2 regularization
v0.14.1,Ridge regression estimate
v0.14.1,"Parameter returned is of shape (d_T, )"
v0.14.1,Return moments and gradients
v0.14.1,Compute partial moments
v0.14.1,Compute moments
v0.14.1,"Moments shape is (n, d_T-1)"
v0.14.1,Compute moment gradients
v0.14.1,Need to calculate this in an elegant way for when propensity is 0
v0.14.1,This will flatten T
v0.14.1,Check that T is numeric
v0.14.1,Test whether the input estimator is supported
v0.14.1,Calculate confidence intervals for the parameter (marginal effect)
v0.14.1,Calculate confidence intervals for the effect
v0.14.1,Calculate the effects
v0.14.1,Calculate the standard deviations for the effects
v0.14.1,d_t=None here since we measure the effect across all Ts
v0.14.1,conditionally expand jacobian dimensions to align with einsum str
v0.14.1,Calculate the effects
v0.14.1,Calculate the standard deviations for the effects
v0.14.1,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Causal tree parameters
v0.14.1,Tree structure
v0.14.1,No need for a random split since the data is already
v0.14.1,a random subsample from the original input
v0.14.1,node list stores the nodes that are yet to be splitted
v0.14.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.14.1,Create local sample set
v0.14.1,Compute nuisance estimates for the current node
v0.14.1,Nuisance estimate cannot be calculated
v0.14.1,Estimate parameter for current node
v0.14.1,Node estimate cannot be calculated
v0.14.1,Calculate moments and gradient of moments for current data
v0.14.1,Calculate inverse gradient
v0.14.1,The gradient matrix is not invertible.
v0.14.1,No good split can be found
v0.14.1,Calculate point-wise pseudo-outcomes rho
v0.14.1,a split is determined by a feature and a sample pair
v0.14.1,the number of possible splits is at most (number of features) * (number of node samples)
v0.14.1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.14.1,parse row and column of random pair
v0.14.1,the sample of the pair is the integer division of the random number with n_feats
v0.14.1,calculate the binary indicator of whether sample i is on the left or the right
v0.14.1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.14.1,calculate the number of samples on the left child for each proposed split
v0.14.1,calculate the analogous binary indicator for the samples in the estimation set
v0.14.1,calculate the number of estimation samples on the left child of each proposed split
v0.14.1,find the upper and lower bound on the size of the left split for the split
v0.14.1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.14.1,on each side.
v0.14.1,similarly for the estimation sample set
v0.14.1,if there is no valid split then don't create any children
v0.14.1,filter only the valid splits
v0.14.1,calculate the average influence vector of the samples in the left child
v0.14.1,calculate the average influence vector of the samples in the right child
v0.14.1,take the square of each of the entries of the influence vectors and normalize
v0.14.1,by size of each child
v0.14.1,calculate the vector score of each candidate split as the average of left and right
v0.14.1,influence vectors
v0.14.1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.14.1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.14.1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.14.1,calculate the scalar score of each split by aggregating across the vector of scores
v0.14.1,Find split that minimizes criterion
v0.14.1,Create child nodes with corresponding subsamples
v0.14.1,add the created children to the list of not yet split nodes
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.1,Copyright (c) PyWhy contributors. All rights reserved.
v0.14.1,Licensed under the MIT License.
v0.14.0,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.14.0,configuration is all pulled from setup.cfg
v0.14.0,-*- coding: utf-8 -*-
v0.14.0,
v0.14.0,Configuration file for the Sphinx documentation builder.
v0.14.0,
v0.14.0,This file does only contain a selection of the most common options. For a
v0.14.0,full list see the documentation:
v0.14.0,http://www.sphinx-doc.org/en/main/config
v0.14.0,-- Path setup --------------------------------------------------------------
v0.14.0,"If extensions (or modules to document with autodoc) are in another directory,"
v0.14.0,add these directories to sys.path here. If the directory is relative to the
v0.14.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.14.0,
v0.14.0,-- Project information -----------------------------------------------------
v0.14.0,-- General configuration ---------------------------------------------------
v0.14.0,"If your documentation needs a minimal Sphinx version, state it here."
v0.14.0,
v0.14.0,needs_sphinx = '1.0'
v0.14.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.14.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.14.0,ones.
v0.14.0,TODO: enable type aliases
v0.14.0,napoleon_preprocess_types = True  # needed for type aliases to work
v0.14.0,napoleon_type_aliases = {
v0.14.0,"""array_like"": "":term:`array_like`"","
v0.14.0,"""ndarray"": ""~numpy.ndarray"","
v0.14.0,"""RandomState"": "":class:`~numpy.random.RandomState`"","
v0.14.0,"""DataFrame"": "":class:`~pandas.DataFrame`"","
v0.14.0,"""Series"": "":class:`~pandas.Series`"","
v0.14.0,}
v0.14.0,"Add any paths that contain templates here, relative to this directory."
v0.14.0,The suffix(es) of source filenames.
v0.14.0,You can specify multiple suffix as a list of strings:
v0.14.0,
v0.14.0,"source_suffix = ['.rst', '.md']"
v0.14.0,The root toctree document.
v0.14.0,The language for content autogenerated by Sphinx. Refer to documentation
v0.14.0,for a list of supported languages.
v0.14.0,
v0.14.0,This is also used if you do content translation via gettext catalogs.
v0.14.0,"Usually you set ""language"" from the command line for these cases."
v0.14.0,"List of patterns, relative to source directory, that match files and"
v0.14.0,directories to ignore when looking for source files.
v0.14.0,This pattern also affects html_static_path and html_extra_path.
v0.14.0,The name of the Pygments (syntax highlighting) style to use.
v0.14.0,-- Options for HTML output -------------------------------------------------
v0.14.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.14.0,a list of builtin themes.
v0.14.0,
v0.14.0,Theme options are theme-specific and customize the look and feel of a theme
v0.14.0,"further.  For a list of options available for each theme, see the"
v0.14.0,documentation.
v0.14.0,
v0.14.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.14.0,"relative to this directory. They are copied after the builtin static files,"
v0.14.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.14.0,html_static_path = ['_static']
v0.14.0,"Custom sidebar templates, must be a dictionary that maps document names"
v0.14.0,to template names.
v0.14.0,
v0.14.0,The default sidebars (for documents that don't match any pattern) are
v0.14.0,defined by theme itself.  Builtin themes are using these templates by
v0.14.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.14.0,'searchbox.html']``.
v0.14.0,
v0.14.0,html_sidebars = {}
v0.14.0,-- Options for HTMLHelp output ---------------------------------------------
v0.14.0,Output file base name for HTML help builder.
v0.14.0,-- Options for LaTeX output ------------------------------------------------
v0.14.0,The paper size ('letterpaper' or 'a4paper').
v0.14.0,
v0.14.0,"'papersize': 'letterpaper',"
v0.14.0,"The font size ('10pt', '11pt' or '12pt')."
v0.14.0,
v0.14.0,"'pointsize': '10pt',"
v0.14.0,Additional stuff for the LaTeX preamble.
v0.14.0,
v0.14.0,"'preamble': '',"
v0.14.0,Latex figure (float) alignment
v0.14.0,
v0.14.0,"'figure_align': 'htbp',"
v0.14.0,Grouping the document tree into LaTeX files. List of tuples
v0.14.0,"(source start file, target name, title,"
v0.14.0,"author, documentclass [howto, manual, or own class])."
v0.14.0,-- Options for manual page output ------------------------------------------
v0.14.0,One entry per manual page. List of tuples
v0.14.0,"(source start file, name, description, authors, manual section)."
v0.14.0,-- Options for Texinfo output ----------------------------------------------
v0.14.0,Grouping the document tree into Texinfo files. List of tuples
v0.14.0,"(source start file, target name, title, author,"
v0.14.0,"dir menu entry, description, category)"
v0.14.0,-- Options for Epub output -------------------------------------------------
v0.14.0,Bibliographic Dublin Core info.
v0.14.0,The unique identifier of the text. This can be a ISBN number
v0.14.0,or the project homepage.
v0.14.0,
v0.14.0,epub_identifier = ''
v0.14.0,A unique identification for the text.
v0.14.0,
v0.14.0,epub_uid = ''
v0.14.0,A list of files that should not be packed into the epub file.
v0.14.0,-- Extension configuration -------------------------------------------------
v0.14.0,-- Options for intersphinx extension ---------------------------------------
v0.14.0,Example configuration for intersphinx: refer to the Python standard library.
v0.14.0,-- Options for todo extension ----------------------------------------------
v0.14.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.14.0,-- Options for doctest extension -------------------------------------------
v0.14.0,we can document otherwise excluded entities here by returning False
v0.14.0,or skip otherwise included entities by returning True
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Calculate residuals
v0.14.0,Estimate E[T_res | Z_res]
v0.14.0,TODO. Deal with multi-class instrument
v0.14.0,Calculate nuisances
v0.14.0,Estimate E[T_res | Z_res]
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"We do a three way split, as typically a preliminary theta estimator would require"
v0.14.0,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.14.0,TODO. Deal with multi-class instrument
v0.14.0,Estimate final model of theta(X) by minimizing the square loss:
v0.14.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.14.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.14.0,at the expense of some small bias. For points with very small covariance we revert
v0.14.0,to the model-based preliminary estimate and do not add the correction term.
v0.14.0,Estimate preliminary theta in cross fitting manner
v0.14.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.14.0,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.14.0,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.14.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.14.0,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.14.0,TODO. The solution below is not really a valid cross-fitting
v0.14.0,as the test data are used to create the proj_t on the train
v0.14.0,which in the second train-test loop is used to create the nuisance
v0.14.0,cov on the test data. Hence the T variable of some sample
v0.14.0,"is implicitly correlated with its cov nuisance, through this flow"
v0.14.0,"of information. However, this seems a rather weak correlation."
v0.14.0,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.14.0,model.
v0.14.0,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.14.0,Estimate preliminary theta in cross fitting manner
v0.14.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.14.0,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.14.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.14.0,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.14.0,#############################################################################
v0.14.0,Classes for the DRIV implementation for the special case of intent-to-treat
v0.14.0,A/B test
v0.14.0,#############################################################################
v0.14.0,Estimate preliminary theta in cross fitting manner
v0.14.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.14.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.14.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.14.0,We can use statsmodel for all hypothesis testing capabilities
v0.14.0,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.14.0,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.14.0,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.14.0,model_T_XZ = lambda: model_clf()
v0.14.0,#'days_visited': lambda:
v0.14.0,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.14.0,Turn strings into categories for numeric mapping
v0.14.0,### Defining some generic regressors and classifiers
v0.14.0,This a generic non-parametric regressor
v0.14.0,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.14.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.14.0,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.14.0,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.14.0,model = lambda: RandomForestRegressor(n_estimators=100)
v0.14.0,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.14.0,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.14.0,model = lambda: LinearRegression(n_jobs=-1)
v0.14.0,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.14.0,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.14.0,underlying model whenever predict is called.
v0.14.0,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.14.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.14.0,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.14.0,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.14.0,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.14.0,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.14.0,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.14.0,We need to specify models to be used for each of these residualizations
v0.14.0,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.14.0,"E[T | X, Z]"
v0.14.0,E[TZ | X]
v0.14.0,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.14.0,n_splits determines the number of splits to be used for cross-fitting.
v0.14.0,# Algorithm 2 - Current Method
v0.14.0,In[121]:
v0.14.0,# Algorithm 3 - DRIV ATE
v0.14.0,dmliv_model_effect = lambda: model()
v0.14.0,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.14.0,"dmliv_model_effect(),"
v0.14.0,n_splits=1)
v0.14.0,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.14.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.14.0,"Once multiple treatments are supported, we'll need to fix this"
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.14.0,We can use statsmodel for all hypothesis testing capabilities
v0.14.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.14.0,We can use statsmodel for all hypothesis testing capabilities
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,TODO. Deal with multi-class instrument/treatment
v0.14.0,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.14.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.14.0,Estimate p(X) = E[T | X] in cross-fitting manner
v0.14.0,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.14.0,##################
v0.14.0,Global settings #
v0.14.0,##################
v0.14.0,Global plotting controls
v0.14.0,"Control for support size, can control for more"
v0.14.0,#################
v0.14.0,File utilities #
v0.14.0,#################
v0.14.0,#################
v0.14.0,Plotting utils #
v0.14.0,#################
v0.14.0,bias
v0.14.0,var
v0.14.0,rmse
v0.14.0,r2
v0.14.0,Infer feature dimension
v0.14.0,Metrics by support plots
v0.14.0,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.14.0,Vasilis Syrgkanis <vasy@microsoft.com>
v0.14.0,Steven Wu <zhiww@microsoft.com>
v0.14.0,Initialize causal tree parameters
v0.14.0,Create splits of causal tree
v0.14.0,Estimate treatment effects at the leafs
v0.14.0,Compute heterogeneous treatement effect for x's in x_list by finding
v0.14.0,the corresponding split and associating the effect computed on that leaf
v0.14.0,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.14.0,Safety check
v0.14.0,Weighted linear regression
v0.14.0,Calculates weights
v0.14.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.14.0,over all indices
v0.14.0,Similar for `a` weights
v0.14.0,Doesn't have sample weights
v0.14.0,Is a linear model
v0.14.0,Weighted linear regression
v0.14.0,Calculates weights
v0.14.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.14.0,over all indices
v0.14.0,Similar for `a` weights
v0.14.0,normalize weights
v0.14.0,"Split the data in half, train and test"
v0.14.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.14.0,"a function of W, using only the train fold"
v0.14.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.14.0,"Split the data in half, train and test"
v0.14.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.14.0,"a function of W, using only the train fold"
v0.14.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.14.0,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.14.0,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.14.0,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.14.0,"Split the data in half, train and test"
v0.14.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.14.0,"a function of x, using only the train fold"
v0.14.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.14.0,Compute coefficient by OLS on residuals
v0.14.0,"Split the data in half, train and test"
v0.14.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.14.0,"a function of x, using only the train fold"
v0.14.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.14.0,Estimate multipliers for second order orthogonal method
v0.14.0,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.14.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.14.0,Create local sample set
v0.14.0,compute the base estimate for the current node using double ml or second order double ml
v0.14.0,compute the influence functions here that are used for the criterion
v0.14.0,generate random proposals of dimensions to split
v0.14.0,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.14.0,compute criterion for each proposal
v0.14.0,if splitting creates valid leafs in terms of mean leaf size
v0.14.0,Calculate criterion for split
v0.14.0,Else set criterion to infinity so that this split is not chosen
v0.14.0,If no good split was found
v0.14.0,Find split that minimizes criterion
v0.14.0,Set the split attributes at the node
v0.14.0,Create child nodes with corresponding subsamples
v0.14.0,Recursively split children
v0.14.0,Return parent node
v0.14.0,estimate the local parameter at the leaf using the estimate data
v0.14.0,###################
v0.14.0,Argument parsing #
v0.14.0,###################
v0.14.0,#########################################
v0.14.0,Parameters constant across experiments #
v0.14.0,#########################################
v0.14.0,Outcome support
v0.14.0,Treatment support
v0.14.0,Evaluation grid
v0.14.0,Treatment effects array
v0.14.0,Other variables
v0.14.0,##########################
v0.14.0,Data Generating Process #
v0.14.0,##########################
v0.14.0,Log iteration
v0.14.0,"Generate controls, features, treatment and outcome"
v0.14.0,T and Y residuals to be used in later scripts
v0.14.0,Save generated dataset
v0.14.0,#################
v0.14.0,ORF parameters #
v0.14.0,#################
v0.14.0,######################################
v0.14.0,Train and evaluate treatment effect #
v0.14.0,######################################
v0.14.0,########
v0.14.0,Plots #
v0.14.0,########
v0.14.0,###############
v0.14.0,Save results #
v0.14.0,###############
v0.14.0,##############
v0.14.0,Run Rscript #
v0.14.0,##############
v0.14.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.14.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.14.0,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.14.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.14.0,def mlasso_model(): return MultiTaskLassoCV(
v0.14.0,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.14.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.14.0,heterogeneity
v0.14.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.14.0,heterogeneity
v0.14.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.14.0,heterogeneity
v0.14.0,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.14.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.14.0,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.14.0,subset of features that are exogenous and create heterogeneity
v0.14.0,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.14.0,subset of features wrt we estimate heterogeneity
v0.14.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.14.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,introspect the constructor arguments to find the model parameters
v0.14.0,to represent
v0.14.0,"if the argument is deprecated, ignore it"
v0.14.0,Extract and sort argument names excluding 'self'
v0.14.0,column names
v0.14.0,transfer input to numpy arrays
v0.14.0,transfer input to 2d arrays
v0.14.0,create dataframe
v0.14.0,currently dowhy only support single outcome and single treatment
v0.14.0,call dowhy
v0.14.0,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.14.0,cate estimator but not the effect.
v0.14.0,don't proxy special methods
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Check if model is sparse enough for this model
v0.14.0,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.14.0,TODO: any way to avoid creating a copy if the array was already dense?
v0.14.0,"the call is necessary if the input was something like a list, though"
v0.14.0,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.14.0,so convert to pydata sparse first
v0.14.0,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.14.0,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.14.0,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.14.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.14.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.14.0,For when checking input values is disabled
v0.14.0,Type to column extraction function
v0.14.0,if not all column names are strings
v0.14.0,coerce feature names to be strings
v0.14.0,Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
v0.14.0,"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
v0.14.0,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.14.0,Handles cases where the passed feature names create issues
v0.14.0,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.14.0,Get feature names using featurizer
v0.14.0,All attempts at retrieving transformed feature names have failed
v0.14.0,Delegate handling to downstream logic
v0.14.0,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.14.0,same number of input definitions as arrays
v0.14.0,input definitions have same number of dimensions as each array
v0.14.0,all result indices are unique
v0.14.0,all result indices must match at least one input index
v0.14.0,"map indices to all array, axis pairs for that index"
v0.14.0,each index has the same cardinality wherever it appears
v0.14.0,"State: list of (set of letters, list of (corresponding indices, value))"
v0.14.0,Algo: while list contains more than one entry
v0.14.0,take two entries
v0.14.0,sort both lists by intersection of their indices
v0.14.0,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.14.0,"take the union of indices and the product of values), stepping through each list linearly"
v0.14.0,TODO: might be faster to break into connected components first
v0.14.0,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.14.0,"so compute their content separately, then take cartesian product"
v0.14.0,this would save a few pointless sorts by empty tuples
v0.14.0,TODO: Consider investigating other performance ideas for these cases
v0.14.0,where the dense method beat the sparse method (usually sparse is faster)
v0.14.0,"e,facd,c->cfed"
v0.14.0,sparse: 0.0335489
v0.14.0,dense:  0.011465999999999997
v0.14.0,"gbd,da,egb->da"
v0.14.0,sparse: 0.0791625
v0.14.0,dense:  0.007319099999999995
v0.14.0,"dcc,d,faedb,c->abe"
v0.14.0,sparse: 1.2868097
v0.14.0,dense:  0.44605229999999985
v0.14.0,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.14.0,TODO: would using einsum's paths to optimize the order of merging help?
v0.14.0,assume that we should perform nested cross-validation if and only if
v0.14.0,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.14.0,logic copied from check_cv
v0.14.0,otherwise we will assume the user already set the cv attribute to something
v0.14.0,compatible with splitting with a 'groups' argument
v0.14.0,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.14.0,don't use the groups when calling split internally
v0.14.0,Normalize weights
v0.14.0,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.14.0,"if we're decorating a class, just update the __init__ method,"
v0.14.0,so that the result is still a class instead of a wrapper method
v0.14.0,"want to enforce that each bad_arg was either in kwargs,"
v0.14.0,or else it was in neither and is just taking its default value
v0.14.0,Any access should throw
v0.14.0,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.14.0,for every dimension of the treatment add some epsilon and observe change in featurized treatment
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.14.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.14.0,input feature name is already updated by cate_feature_names.
v0.14.0,define the index of d_x to filter for each given T
v0.14.0,filter X after broadcast with T for each given T
v0.14.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.14.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,
v0.14.0,This code contains some snippets of code from:
v0.14.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
v0.14.0,published under the following license and copyright:
v0.14.0,BSD 3-Clause License
v0.14.0,
v0.14.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.0,All rights reserved.
v0.14.0,make any access to matplotlib or plt throw an exception
v0.14.0,make any access to graphviz or plt throw an exception
v0.14.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.14.0,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.14.0,Initialize saturation & value; calculate chroma & value shift
v0.14.0,Calculate some intermediate values
v0.14.0,Initialize RGB with same hue & chroma as our color
v0.14.0,Shift the initial RGB values to match value and store
v0.14.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.14.0,clean way of achieving this
v0.14.0,make sure we don't accidentally escape anything in the substitution
v0.14.0,Fetch appropriate color for node
v0.14.0,"red for negative, green for positive"
v0.14.0,in multi-target use mean of targets
v0.14.0,Write node mean CATE
v0.14.0,Write node std of CATE
v0.14.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.14.0,Fetch appropriate color for node
v0.14.0,Write node mean CATE
v0.14.0,Write node mean CATE
v0.14.0,Write recommended treatment and value - cost
v0.14.0,Licensed under the MIT License.
v0.14.0,"since inference objects can be stateful, we must copy it before fitting;"
v0.14.0,otherwise this sequence wouldn't work:
v0.14.0,"est1.fit(..., inference=inf)"
v0.14.0,"est2.fit(..., inference=inf)"
v0.14.0,est1.effect_interval(...)
v0.14.0,because inf now stores state from fitting est2
v0.14.0,This flag is true when names are set in a child class instead
v0.14.0,"If names are set in a child class, add an attribute reflecting that"
v0.14.0,This works only if X is passed as a kwarg
v0.14.0,We plan to enforce X as kwarg only in future releases
v0.14.0,This checks if names have been set in a child class
v0.14.0,"If names were set in a child class, don't do it again"
v0.14.0,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.14.0,call the wrapped fit method
v0.14.0,NOTE: we call inference fit *after* calling the main fit method
v0.14.0,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.14.0,but tensordot can't be applied to this problem because we don't sum over m
v0.14.0,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.14.0,of rows of T was not taken into account
v0.14.0,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.14.0,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.14.0,"Treatment names is None, default to BaseCateEstimator"
v0.14.0,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.14.0,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.14.0,Get input names
v0.14.0,Summary
v0.14.0,add statsmodels to parent's options
v0.14.0,add debiasedlasso to parent's options
v0.14.0,add blb to parent's options
v0.14.0,TODO Share some logic with non-discrete version
v0.14.0,Get input names
v0.14.0,Note: we do not transform feature names since that is done within summary_frame
v0.14.0,Summary
v0.14.0,add statsmodels to parent's options
v0.14.0,add statsmodels to parent's options
v0.14.0,add blb to parent's options
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,remove None arguments
v0.14.0,"scores entries should be lists of scores, so make each entry a singleton list"
v0.14.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.14.0,generate an instance of the final model
v0.14.0,generate an instance of the nuisance model
v0.14.0,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.14.0,alteration even when we only want to fit_final.
v0.14.0,use a binary array to get stratified split in case of discrete treatment
v0.14.0,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.14.0,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.14.0,"however, sklearn doesn't support both stratifying and grouping (see"
v0.14.0,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.14.0,their own object that supports grouping if they want to use groups.
v0.14.0,for each mc iteration
v0.14.0,for each model under cross fit setting
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,
v0.14.0,This code contains snippets of code from
v0.14.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.14.0,published under the following license and copyright:
v0.14.0,BSD 3-Clause License
v0.14.0,
v0.14.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.0,All rights reserved.
v0.14.0,=============================================================================
v0.14.0,Policy Forest
v0.14.0,=============================================================================
v0.14.0,Remap output
v0.14.0,reshape is necessary to preserve the data contiguity against vs
v0.14.0,"[:, np.newaxis] that does not."
v0.14.0,Get subsample sample size
v0.14.0,Check parameters
v0.14.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.14.0,if this is the first `fit` call of the warm start mode.
v0.14.0,"Free allocated memory, if any"
v0.14.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.14.0,We draw from the random state to get the random state we
v0.14.0,would have got if we hadn't used a warm_start.
v0.14.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.14.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.14.0,but would still advance randomness enough so that tree subsamples will be different.
v0.14.0,Parallel loop: we prefer the threading backend as the Cython code
v0.14.0,for fitting the trees is internally releasing the Python GIL
v0.14.0,making threading more efficient than multiprocessing in
v0.14.0,"that case. However, for joblib 0.12+ we respect any"
v0.14.0,"parallel_backend contexts set at a higher level,"
v0.14.0,since correctness does not rely on using threads.
v0.14.0,Collect newly grown trees
v0.14.0,Check data
v0.14.0,Assign chunk of trees to jobs
v0.14.0,avoid storing the output of every estimator by summing them here
v0.14.0,Parallel loop
v0.14.0,Check data
v0.14.0,Assign chunk of trees to jobs
v0.14.0,avoid storing the output of every estimator by summing them here
v0.14.0,Parallel loop
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,
v0.14.0,This code contains snippets of code from:
v0.14.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.14.0,published under the following license and copyright:
v0.14.0,BSD 3-Clause License
v0.14.0,
v0.14.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.0,All rights reserved.
v0.14.0,=============================================================================
v0.14.0,Types and constants
v0.14.0,=============================================================================
v0.14.0,=============================================================================
v0.14.0,Base Policy tree
v0.14.0,=============================================================================
v0.14.0,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.14.0,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.14.0,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.14.0,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.14.0,checks that X is 2D array.
v0.14.0,"since we only allow single dimensional y, we could flatten the prediction"
v0.14.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.14.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.14.0,Handles the corner case when X=None but featurizer might be not None
v0.14.0,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.14.0,"Replacing this method which is invalid for this class, so that we make the"
v0.14.0,dosctring empty and not appear in the docs.
v0.14.0,TODO: support freq_weight and sample_var in debiased lasso
v0.14.0,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.14.0,Replacing to remove docstring
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"if both X and W are None, just return a column of ones"
v0.14.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.14.0,We need to go back to the label representation of the one-hot so as to call
v0.14.0,the classifier.
v0.14.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.14.0,We need to go back to the label representation of the one-hot so as to call
v0.14.0,the classifier.
v0.14.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.14.0,This works both with our without the weighting trick as the treatments T are unit vector
v0.14.0,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.14.0,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.14.0,both Parametric and Non Parametric DML.
v0.14.0,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.14.0,attribute so that the trained featurizer will be passed through
v0.14.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.14.0,for internal use by the library
v0.14.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.14.0,We need to use the rlearner's copy to retain the information from fitting
v0.14.0,Handles the corner case when X=None but featurizer might be not None
v0.14.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.14.0,since we clone it and fit separate copies
v0.14.0,override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
v0.14.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.14.0,TODO: support freq_weight and sample_var in debiased lasso
v0.14.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.14.0,since we clone it and fit separate copies
v0.14.0,add blb to parent's options
v0.14.0,override only so that we can update the docstring to indicate
v0.14.0,support for `GenericSingleTreatmentModelFinalInference`
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,note that groups are not passed to score because they are only used for fitting
v0.14.0,note that groups are not passed to score because they are only used for fitting
v0.14.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.14.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.14.0,NOTE: important to get parent's wrapped copy so that
v0.14.0,"after training wrapped featurizer is also trained, etc."
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.14.0,Fit a doubly robust average effect
v0.14.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.14.0,(which needs to have been expanded if there's a discrete treatment)
v0.14.0,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.14.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.14.0,since we clone it and fit separate copies
v0.14.0,"If custom param grid, check that only estimator parameters are being altered"
v0.14.0,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.14.0,override only so that we can update the docstring to indicate support for `blb`
v0.14.0,Get input names
v0.14.0,Summary
v0.14.0,Determine output settings
v0.14.0,"Important: This must be the first invocation of the random state at fit time, so that"
v0.14.0,train/test splits are re-generatable from an external object simply by knowing the
v0.14.0,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.14.0,linear predictions. Currently is also useful for testing.
v0.14.0,reshape is necessary to preserve the data contiguity against vs
v0.14.0,"[:, np.newaxis] that does not."
v0.14.0,Check parameters
v0.14.0,Set min_weight_leaf from min_weight_fraction_leaf
v0.14.0,Build tree
v0.14.0,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.14.0,hold. Used by criterion for memory space savings.
v0.14.0,Initialize the criterion object and the criterion_val object if honest.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,
v0.14.0,This code is a fork from:
v0.14.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
v0.14.0,published under the following license and copyright:
v0.14.0,BSD 3-Clause License
v0.14.0,
v0.14.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.0,All rights reserved.
v0.14.0,Set parameters
v0.14.0,Don't instantiate estimators now! Parameters of base_estimator might
v0.14.0,"still change. Eg., when grid-searching with the nested object syntax."
v0.14.0,self.estimators_ needs to be filled by the derived classes in fit.
v0.14.0,Compute the number of jobs
v0.14.0,Partition estimators between jobs
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,covariance matrix
v0.14.0,get eigen value and eigen vectors
v0.14.0,simulate eigen vectors
v0.14.0,keep the top 4 eigen value and corresponding eigen vector
v0.14.0,replace the negative eigen values
v0.14.0,generate a new covariance matrix
v0.14.0,get linear approximation of eigen values
v0.14.0,coefs
v0.14.0,get the indices of each group of features
v0.14.0,print(ind_same_proxy)
v0.14.0,demo
v0.14.0,same proxy
v0.14.0,residuals
v0.14.0,gmm
v0.14.0,log normal on outliers
v0.14.0,positive outliers
v0.14.0,negative outliers
v0.14.0,demean the new residual again
v0.14.0,generate data
v0.14.0,sample residuals
v0.14.0,get prediction for current investment
v0.14.0,get prediction for current proxy
v0.14.0,get first period prediction
v0.14.0,iterate the step ahead contruction
v0.14.0,prepare new x
v0.14.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.14.0,heterogeneity
v0.14.0,get new covariance matrix
v0.14.0,get coefs
v0.14.0,get residuals
v0.14.0,proxy 1 is the outcome
v0.14.0,make fixed residuals
v0.14.0,Remove children with nonwhite mothers from the treatment group
v0.14.0,Remove children with nonwhite mothers from the treatment group
v0.14.0,Select columns
v0.14.0,Scale the numeric variables
v0.14.0,"Change the binary variable 'first' takes values in {1,2}"
v0.14.0,Append a column of ones as intercept
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.14.0,(which needs to have been expanded if there's a discrete treatment)
v0.14.0,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.14.0,d_t=None here since we measure the effect across all Ts
v0.14.0,"y is a vector, rather than a 2D array"
v0.14.0,once the estimator has been fit
v0.14.0,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.14.0,an intercept even when bias is part of coef.
v0.14.0,We can write effect inference as a function of prediction and prediction standard error of
v0.14.0,the final method for linear models
v0.14.0,squeeze the first axis
v0.14.0,d_t=None here since we measure the effect across all Ts
v0.14.0,set the mean_pred_stderr
v0.14.0,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.14.0,squeeze the first axis
v0.14.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.14.0,(which needs to have been expanded if there's a discrete treatment)
v0.14.0,"send treatment to the end, pull bounds to the front"
v0.14.0,d_t=None here since we measure the effect across all Ts
v0.14.0,set the mean_pred_stderr
v0.14.0,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.14.0,d_t=None here since we measure the effect across all Ts
v0.14.0,d_t=None here since we measure the effect across all Ts
v0.14.0,need to set the fit args before the estimator is fit
v0.14.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.14.0,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.14.0,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.14.0,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.14.0,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.14.0,scale preds
v0.14.0,scale std errs
v0.14.0,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.14.0,offset preds
v0.14.0,"offset the distribution, too"
v0.14.0,scale preds
v0.14.0,"scale the distribution, too"
v0.14.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.14.0,1. Uncertainty of Mean Point Estimate
v0.14.0,2. Distribution of Point Estimate
v0.14.0,3. Total Variance of Point Estimate
v0.14.0,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.14.0,so bail out early; note that it might be possible to correct the algorithm for
v0.14.0,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.14.0,be clean
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,TODO: Add a __dir__ implementation?
v0.14.0,don't proxy special methods
v0.14.0,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.14.0,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.14.0,then we should be computing a confidence interval for the wrapped calls
v0.14.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.14.0,second level bootstrap which would be prohibitive computationally?
v0.14.0,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.14.0,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.14.0,can't import from econml.inference at top level without creating cyclical dependencies
v0.14.0,Note that inference results are always methods even if the inference is for a property
v0.14.0,(e.g. coef__inference() is a method but coef_ is a property)
v0.14.0,Therefore we must insert a lambda if getting inference for a non-callable
v0.14.0,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.14.0,"try to get interval/std first if appropriate,"
v0.14.0,since we don't prefer a wrapped method with this name
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,
v0.14.0,This code contains snippets of code from:
v0.14.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.14.0,published under the following license and copyright:
v0.14.0,BSD 3-Clause License
v0.14.0,
v0.14.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.0,All rights reserved.
v0.14.0,=============================================================================
v0.14.0,Types and constants
v0.14.0,=============================================================================
v0.14.0,=============================================================================
v0.14.0,Base GRF tree
v0.14.0,=============================================================================
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,=============================================================================
v0.14.0,A MultOutputWrapper for GRF classes
v0.14.0,=============================================================================
v0.14.0,=============================================================================
v0.14.0,Instantiations of Generalized Random Forest
v0.14.0,=============================================================================
v0.14.0,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.14.0,in front of the constant treatment is the intercept in the moment equation.
v0.14.0,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.14.0,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,
v0.14.0,This code contains snippets of code from
v0.14.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.14.0,published under the following license and copyright:
v0.14.0,BSD 3-Clause License
v0.14.0,
v0.14.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.14.0,All rights reserved.
v0.14.0,=============================================================================
v0.14.0,Base Generalized Random Forest
v0.14.0,=============================================================================
v0.14.0,TODO: support freq_weight and sample_var
v0.14.0,Remap output
v0.14.0,reshape is necessary to preserve the data contiguity against vs
v0.14.0,"[:, np.newaxis] that does not."
v0.14.0,reshape is necessary to preserve the data contiguity against vs
v0.14.0,"[:, np.newaxis] that does not."
v0.14.0,Get subsample sample size
v0.14.0,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.14.0,We calculate the min eigenvalue proxy that each criterion is considering
v0.14.0,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.14.0,Check parameters
v0.14.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.14.0,if this is the first `fit` call of the warm start mode.
v0.14.0,"Free allocated memory, if any"
v0.14.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.14.0,We draw from the random state to get the random state we
v0.14.0,would have got if we hadn't used a warm_start.
v0.14.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.14.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.14.0,but would still advance randomness enough so that tree subsamples will be different.
v0.14.0,Generating indices a priori before parallelism ended up being orders of magnitude
v0.14.0,faster than how sklearn does it. The reason is that random samplers do not release the
v0.14.0,gil it seems.
v0.14.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.14.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.14.0,but would still advance randomness enough so that tree subsamples will be different.
v0.14.0,Parallel loop: we prefer the threading backend as the Cython code
v0.14.0,for fitting the trees is internally releasing the Python GIL
v0.14.0,making threading more efficient than multiprocessing in
v0.14.0,"that case. However, for joblib 0.12+ we respect any"
v0.14.0,"parallel_backend contexts set at a higher level,"
v0.14.0,since correctness does not rely on using threads.
v0.14.0,Collect newly grown trees
v0.14.0,Check data
v0.14.0,Assign chunk of trees to jobs
v0.14.0,avoid storing the output of every estimator by summing them here
v0.14.0,Parallel loop
v0.14.0,Check data
v0.14.0,Assign chunk of trees to jobs
v0.14.0,Parallel loop
v0.14.0,Check data
v0.14.0,Assign chunk of trees to jobs
v0.14.0,Parallel loop
v0.14.0,####################
v0.14.0,Variance correction
v0.14.0,####################
v0.14.0,Subtract the average within bag variance. This ends up being equal to the
v0.14.0,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.14.0,The negative part is just sq_between.
v0.14.0,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.14.0,"The off diagonals we have no objective prior, so no correction is applied."
v0.14.0,Finally correcting the pred_cov or pred_var
v0.14.0,avoid storing the output of every estimator by summing them here
v0.14.0,Parallel loop
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,TODO: update docs
v0.14.0,"NOTE: sample weight, sample var are not passed in"
v0.14.0,Compose final model
v0.14.0,Calculate auxiliary quantities
v0.14.0,X ⨂ T_res
v0.14.0,"sum(model_final.predict(X, T_res))"
v0.14.0,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.14.0,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
v0.14.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.0,generate an instance of the final model
v0.14.0,generate an instance of the nuisance model
v0.14.0,Set _d_t to effective number of treatments
v0.14.0,Required for bootstrap inference
v0.14.0,for each mc iteration
v0.14.0,for each model under cross fit setting
v0.14.0,Handles the corner case when X=None but featurizer might be not None
v0.14.0,Expand treatments for each time period
v0.14.0,NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
v0.14.0,attribute so that the trained featurizer will be passed through
v0.14.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.14.0,for internal use by the library
v0.14.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.14.0,We need to use the _ortho_learner's copy to retain the information from fitting
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,testing importances
v0.14.0,testing heterogeneity importances
v0.14.0,Testing that all parameters do what they are supposed to
v0.14.0,"testing predict, apply and decision path"
v0.14.0,test that the subsampling scheme past to the trees is correct
v0.14.0,The sample size is chosen in particular to test rounding based error when subsampling
v0.14.0,test that the estimator calcualtes var correctly
v0.14.0,test api
v0.14.0,test accuracy
v0.14.0,test the projection functionality of forests
v0.14.0,test that the estimator calcualtes var correctly
v0.14.0,test api
v0.14.0,test that the estimator calcualtes var correctly
v0.14.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.14.0,test that we raise errors in mishandled situations.
v0.14.0,test that the subsampling scheme past to the trees is correct
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
v0.14.0,omit the lalonde notebook
v0.14.0,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.14.0,creating notebooks that are annoying for our users to actually run themselves
v0.14.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.14.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.14.0,"prior to calling interpret, can't plot, render, etc."
v0.14.0,can interpret without uncertainty
v0.14.0,can't interpret with uncertainty if inference wasn't used during fit
v0.14.0,can interpret with uncertainty if we refit
v0.14.0,can interpret without uncertainty
v0.14.0,can't treat before interpreting
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,for is_discrete in [False]:
v0.14.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.14.0,ensure we can serialize the unfit estimator
v0.14.0,ensure we can pickle the fit estimator
v0.14.0,make sure we can call the marginal_effect and effect methods
v0.14.0,test const marginal inference
v0.14.0,test effect inference
v0.14.0,test marginal effect inference
v0.14.0,test coef__inference and intercept__inference
v0.14.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.0,"make sure we can call effect with implied scalar treatments,"
v0.14.0,"no matter the dimensions of T, and also that we warn when there"
v0.14.0,are multiple treatments
v0.14.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.0,No heterogeneity
v0.14.0,Define indices to test
v0.14.0,Heterogeneous effects
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,simple DGP only for illustration
v0.14.0,Define the treatment model neural network architecture
v0.14.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.14.0,"so the input shape is (d_z + d_x,)"
v0.14.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.14.0,add extra layers on top for the mixture density network
v0.14.0,Define the response model neural network architecture
v0.14.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.14.0,"so the input shape is (d_t + d_x,)"
v0.14.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.14.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.14.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.14.0,so that the same weights will be reused in each instantiation
v0.14.0,number of samples to use in second estimate of the response
v0.14.0,(to make loss estimate unbiased)
v0.14.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.14.0,do something with predictions...
v0.14.0,also test vector t and y
v0.14.0,simple DGP only for illustration
v0.14.0,Define the treatment model neural network architecture
v0.14.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.14.0,"so the input shape is (d_z + d_x,)"
v0.14.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.14.0,add extra layers on top for the mixture density network
v0.14.0,Define the response model neural network architecture
v0.14.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.14.0,"so the input shape is (d_t + d_x,)"
v0.14.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.14.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.14.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.14.0,so that the same weights will be reused in each instantiation
v0.14.0,number of samples to use in second estimate of the response
v0.14.0,(to make loss estimate unbiased)
v0.14.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.14.0,do something with predictions...
v0.14.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.14.0,test = True ensures we draw test set images
v0.14.0,test = True ensures we draw test set images
v0.14.0,re-draw to get new independent treatment and implied response
v0.14.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.14.0,above is necesary so that reduced form doesn't win
v0.14.0,covariates: time and emotion
v0.14.0,random instrument
v0.14.0,z -> price
v0.14.0,true observable demand function
v0.14.0,errors
v0.14.0,response
v0.14.0,test = True ensures we draw test set images
v0.14.0,test = True ensures we draw test set images
v0.14.0,re-draw to get new independent treatment and implied response
v0.14.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.14.0,above is necesary so that reduced form doesn't win
v0.14.0,covariates: time and emotion
v0.14.0,random instrument
v0.14.0,z -> price
v0.14.0,true observable demand function
v0.14.0,errors
v0.14.0,response
v0.14.0,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.14.0,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.14.0,For some reason this doesn't work at all when run against the CNTK backend...
v0.14.0,"model.compile('nadam', loss=lambda _,l:l)"
v0.14.0,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.14.0,generate a valiation set
v0.14.0,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.14.0,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,DGP constants
v0.14.0,Generate data
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,identity featurization effect functions
v0.14.0,polynomial featurization effect functions
v0.14.0,1d polynomial featurization functions
v0.14.0,2d-to-1d featurization functions
v0.14.0,2d-to-1d vector featurization functions
v0.14.0,test that treatment names are assigned for the featurized treatment
v0.14.0,expected shapes
v0.14.0,check effects
v0.14.0,ate
v0.14.0,loose inference checks
v0.14.0,temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons
v0.14.0,effect inference
v0.14.0,marginal effect inference
v0.14.0,const marginal effect inference
v0.14.0,fit a dummy estimator first so the featurizer can be fit to the treatment
v0.14.0,edge case with transformer that only takes a vector treatment
v0.14.0,so far will always return None for cate_treatment_names
v0.14.0,assert proper handling of improper feature names passed to certain transformers
v0.14.0,"depending on sklearn version, bad feature names either throws error or only uses first relevant name"
v0.14.0,ensure alpha is passed
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,testing importances
v0.14.0,testing heterogeneity importances
v0.14.0,Testing that all parameters do what they are supposed to
v0.14.0,"testing predict, apply and decision path"
v0.14.0,initialize parameters
v0.14.0,initialize config wtih base config and overwite some values
v0.14.0,predict tree using config parameters and assert
v0.14.0,shape of trained tree is the same as y_test
v0.14.0,initialize config wtih base honest config and overwite some values
v0.14.0,predict tree using config parameters and assert
v0.14.0,shape of trained tree is the same as y_test
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.14.0,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.14.0,so we need to transpose the result
v0.14.0,1-d output
v0.14.0,2-d output
v0.14.0,Single dimensional output y
v0.14.0,compare with weight
v0.14.0,compare with weight
v0.14.0,compare with weight
v0.14.0,compare with weight
v0.14.0,Multi-dimensional output y
v0.14.0,1-d y
v0.14.0,compare when both sample_var and sample_weight exist
v0.14.0,multi-d y
v0.14.0,compare when both sample_var and sample_weight exist
v0.14.0,compare when both sample_var and sample_weight exist
v0.14.0,compare when both sample_var and sample_weight exist
v0.14.0,compare when both sample_var and sample_weight exist
v0.14.0,compare when both sample_var and sample_weight exist
v0.14.0,compare when both sample_var and sample_weight exist
v0.14.0,dgp
v0.14.0,StatsModels2SLS
v0.14.0,IV2SLS
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,test that we can fit with the same arguments as the base estimator
v0.14.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.14.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.0,with the same shape for the lower and upper bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,test that we can do the same thing once we provide percentile bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.14.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.0,with the same shape for the lower and upper bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,test that we can do the same thing once we provide percentile bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,test that we can fit with the same arguments as the base estimator
v0.14.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.14.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.0,with the same shape for the lower and upper bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,test that we can do the same thing once we provide percentile bounds
v0.14.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.14.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.0,with the same shape for the lower and upper bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,test that we can do the same thing once we provide percentile bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,test that we can fit with the same arguments as the base estimator
v0.14.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.14.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.0,with the same shape for the lower and upper bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,test that we can do the same thing once we provide percentile bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.14.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.0,with the same shape for the lower and upper bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,test that we can do the same thing once we provide percentile bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.0,with the same shape for the lower and upper bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,test that the estimated effect is usually within the bounds
v0.14.0,test that we can do the same thing once we provide alpha explicitly
v0.14.0,test that the lower and upper bounds differ
v0.14.0,test that the estimated effect is usually within the bounds
v0.14.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.14.0,with the same shape for the lower and upper bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,TODO: test that the estimated effect is usually within the bounds
v0.14.0,and that the true effect is also usually within the bounds
v0.14.0,test that we can do the same thing once we provide percentile bounds
v0.14.0,test that the lower and upper bounds differ
v0.14.0,TODO: test that the estimated effect is usually within the bounds
v0.14.0,and that the true effect is also usually within the bounds
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,test that the subsampling scheme past to the trees is correct
v0.14.0,test that the estimator calcualtes var correctly
v0.14.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.14.0,test that we raise errors in mishandled situations.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,DGP constants
v0.14.0,Generate data
v0.14.0,Test inference results when `cate_feature_names` doesn not exist
v0.14.0,Test inference results when `cate_feature_names` doesn not exist
v0.14.0,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.14.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.14.0,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.14.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.14.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.14.0,pvalue for second column should be greater than zero since some points are on either side
v0.14.0,of the tested value
v0.14.0,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.14.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.14.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.14.0,ensure alpha is passed
v0.14.0,only is not None when T1 is a constant or a list of constant
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.14.0,Test non keyword based calls to fit
v0.14.0,test non-array inputs
v0.14.0,Test custom splitter
v0.14.0,Test incomplete set of test folds
v0.14.0,"y scores should be positive, since W predicts Y somewhat"
v0.14.0,"t scores might not be, since W and T are uncorrelated"
v0.14.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,make sure cross product varies more slowly with first array
v0.14.0,and that vectors are okay as inputs
v0.14.0,number of inputs in specification must match number of inputs
v0.14.0,must have an output
v0.14.0,output indices must be unique
v0.14.0,output indices must be present in an input
v0.14.0,number of indices must match number of dimensions for each input
v0.14.0,repeated indices must always have consistent sizes
v0.14.0,transpose
v0.14.0,tensordot
v0.14.0,trace
v0.14.0,TODO: set up proper flag for this
v0.14.0,pick indices at random with replacement from the first 7 letters of the alphabet
v0.14.0,"of all of the distinct indices that appear in any input,"
v0.14.0,pick a random subset of them (of size at most 5) to appear in the output
v0.14.0,creating an instance should warn
v0.14.0,using the instance should not warn
v0.14.0,using the deprecated method should warn
v0.14.0,don't warn if b and c are passed by keyword
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,make any access to matplotlib or plt throw an exception
v0.14.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.14.0,heterogeneity
v0.14.0,Invert indices to match latest API
v0.14.0,Invert indices to match latest API
v0.14.0,The feature for heterogeneity stays constant
v0.14.0,Auxiliary function for adding xticks and vertical lines when plotting results
v0.14.0,for dynamic dml vs ground truth parameters.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Preprocess data
v0.14.0,Convert 'week' to a date
v0.14.0,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.14.0,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.14.0,Take log of price
v0.14.0,Make brand numeric
v0.14.0,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.14.0,which have all zero coeffs
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,test at least one estimator from each category
v0.14.0,test causal graph
v0.14.0,test refutation estimate
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.14.0,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.14.0,TODO: test something rather than just print...
v0.14.0,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.14.0,pick some arbitrary X
v0.14.0,pick some arbitrary T
v0.14.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.14.0,The average variance should be lower when using monte carlo iterations
v0.14.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.14.0,The average variance should be lower when using monte carlo iterations
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,ensure we can serialize unfit estimator
v0.14.0,ensure we can serialize fit estimator
v0.14.0,expected effect size
v0.14.0,test effect
v0.14.0,test inference
v0.14.0,only OrthoIV support inference other than bootstrap
v0.14.0,test summary
v0.14.0,test can run score
v0.14.0,test cate_feature_names
v0.14.0,test can run shap values
v0.14.0,dgp
v0.14.0,no heterogeneity
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.14.0,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.14.0,The __warningregistry__'s need to be in a pristine state for tests
v0.14.0,to work properly.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,parameter combinations to test
v0.14.0,TODO: serializing/deserializing for every combination -- is this necessary?
v0.14.0,ensure we can serialize unfit estimator
v0.14.0,ensure we can serialize fit estimator
v0.14.0,expected effect size
v0.14.0,assert calculated constant marginal effect shape is expected
v0.14.0,const_marginal effect is defined in LinearCateEstimator class
v0.14.0,assert calculated marginal effect shape is expected
v0.14.0,test inference
v0.14.0,test can run score
v0.14.0,test cate_feature_names
v0.14.0,test can run shap values
v0.14.0,"dgp (binary T, binary Z)"
v0.14.0,no heterogeneity
v0.14.0,with heterogeneity
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Set random seed
v0.14.0,Generate data
v0.14.0,DGP constants
v0.14.0,Test data
v0.14.0,Constant treatment effect
v0.14.0,Constant treatment with multi output Y
v0.14.0,Heterogeneous treatment
v0.14.0,Heterogeneous treatment with multi output Y
v0.14.0,TLearner test
v0.14.0,Instantiate TLearner
v0.14.0,Test inputs
v0.14.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.14.0,Instantiate SLearner
v0.14.0,Test inputs
v0.14.0,Test constant treatment effect
v0.14.0,Test constant treatment effect with multi output Y
v0.14.0,Test heterogeneous treatment effect
v0.14.0,Need interactions between T and features
v0.14.0,Test heterogeneous treatment effect with multi output Y
v0.14.0,Instantiate XLearner
v0.14.0,Test inputs
v0.14.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.14.0,Instantiate DomainAdaptationLearner
v0.14.0,Test inputs
v0.14.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.14.0,Get the true treatment effect
v0.14.0,Get the true treatment effect
v0.14.0,Fit learner and get the effect and marginal effect
v0.14.0,Compute treatment effect residuals (absolute)
v0.14.0,Check that at least 90% of predictions are within tolerance interval
v0.14.0,Check whether the output shape is right
v0.14.0,Check that one can pass in regular lists
v0.14.0,Check that it fails correctly if lists of different shape are passed in
v0.14.0,"Check that it works when T, Y have shape (n, 1)"
v0.14.0,Generate covariates
v0.14.0,Generate treatment
v0.14.0,Calculate outcome
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,DGP constants
v0.14.0,Generate data
v0.14.0,Test data
v0.14.0,Remove warnings that might be raised by the models passed into the ORF
v0.14.0,Generate data with continuous treatments
v0.14.0,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.14.0,does not work well with parallelism.
v0.14.0,Test inputs for continuous treatments
v0.14.0,--> Check that one can pass in regular lists
v0.14.0,--> Check that it fails correctly if lists of different shape are passed in
v0.14.0,Check that outputs have the correct shape
v0.14.0,Test continuous treatments with controls
v0.14.0,Test continuous treatments without controls
v0.14.0,Generate data with binary treatments
v0.14.0,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.14.0,does not work well with parallelism.
v0.14.0,Test inputs for binary treatments
v0.14.0,--> Check that one can pass in regular lists
v0.14.0,--> Check that it fails correctly if lists of different shape are passed in
v0.14.0,"--> Check that it works when T, Y have shape (n, 1)"
v0.14.0,"--> Check that it fails correctly when T has shape (n, 2)"
v0.14.0,--> Check that it fails correctly when the treatments are not numeric
v0.14.0,Check that outputs have the correct shape
v0.14.0,Test binary treatments with controls
v0.14.0,Test binary treatments without controls
v0.14.0,Only applicable to continuous treatments
v0.14.0,Generate data for 2 treatments
v0.14.0,Test multiple treatments with controls
v0.14.0,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.14.0,The rest for controls. Just as an example.
v0.14.0,Generating A/B test data
v0.14.0,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.14.0,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.14.0,Create a wrapper around Lasso that doesn't support weights
v0.14.0,since Lasso does natively support them starting in sklearn 0.23
v0.14.0,Generate data with continuous treatments
v0.14.0,Instantiate model with most of the default parameters
v0.14.0,Compute the treatment effect on test points
v0.14.0,Compute treatment effect residuals
v0.14.0,Multiple treatments
v0.14.0,Allow at most 10% test points to be outside of the tolerance interval
v0.14.0,Compute treatment effect residuals
v0.14.0,Multiple treatments
v0.14.0,Allow at most 20% test points to be outside of the confidence interval
v0.14.0,Check that the intervals are not too wide
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.14.0,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.14.0,ensure that we've got at least 6 of every element
v0.14.0,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.14.0,NOTE: this number may need to change if the default number of folds in
v0.14.0,WeightedStratifiedKFold changes
v0.14.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.14.0,ensure we can serialize the unfit estimator
v0.14.0,ensure we can pickle the fit estimator
v0.14.0,make sure we can call the marginal_effect and effect methods
v0.14.0,test const marginal inference
v0.14.0,test effect inference
v0.14.0,test marginal effect inference
v0.14.0,test coef__inference and intercept__inference
v0.14.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.0,"make sure we can call effect with implied scalar treatments,"
v0.14.0,"no matter the dimensions of T, and also that we warn when there"
v0.14.0,are multiple treatments
v0.14.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.0,ensure that we've got at least two of every element
v0.14.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.14.0,make sure we can call the marginal_effect and effect methods
v0.14.0,test const marginal inference
v0.14.0,test effect inference
v0.14.0,test marginal effect inference
v0.14.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.14.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.14.0,We concatenate the two copies data
v0.14.0,make sure we can get out post-fit stuff
v0.14.0,create a simple artificial setup where effect of moving from treatment
v0.14.0,"1 -> 2 is 2,"
v0.14.0,"1 -> 3 is 1, and"
v0.14.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.14.0,"Using an uneven number of examples from different classes,"
v0.14.0,"and having the treatments in non-lexicographic order,"
v0.14.0,Should rule out some basic issues.
v0.14.0,test that we can fit with a KFold instance
v0.14.0,test that we can fit with a train/test iterable
v0.14.0,predetermined splits ensure that all features are seen in each split
v0.14.0,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.14.0,(incorrectly) use a final model with an intercept
v0.14.0,"Because final model is fixed, actual values of T and Y don't matter"
v0.14.0,Ensure reproducibility
v0.14.0,Sparse DGP
v0.14.0,Treatment effect coef
v0.14.0,Other coefs
v0.14.0,Features and controls
v0.14.0,Test sparse estimator
v0.14.0,"--> test coef_, intercept_"
v0.14.0,--> test treatment effects
v0.14.0,Restrict x_test to vectors of norm < 1
v0.14.0,--> check inference
v0.14.0,Check that a majority of true effects lie in the 5-95% CI
v0.14.0,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.14.0,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.14.0,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.14.0,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.14.0,sparse test case: heterogeneous effect by product
v0.14.0,need at least as many rows in e_y as there are distinct columns
v0.14.0,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.14.0,create a simple artificial setup where effect of moving from treatment
v0.14.0,"a -> b is 2,"
v0.14.0,"a -> c is 1, and"
v0.14.0,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.14.0,"Using an uneven number of examples from different classes,"
v0.14.0,"and having the treatments in non-lexicographic order,"
v0.14.0,should rule out some basic issues.
v0.14.0,Note that explicitly specifying the dtype as object is necessary until
v0.14.0,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.14.0,estimated effects should be identical when treatment is explicitly given
v0.14.0,but const_marginal_effect should be reordered based on the explicit cagetories
v0.14.0,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.14.0,test outer grouping
v0.14.0,test nested grouping
v0.14.0,DML nested CV works via a 'cv' attribute
v0.14.0,"with 2-fold outer and 2-fold inner grouping, and six total groups,"
v0.14.0,should get 1 or 2 groups per split
v0.14.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.14.0,whichever groups we see
v0.14.0,test nested grouping
v0.14.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.14.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.14.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.14.0,"Try default, integer, and new user-passed treatment name"
v0.14.0,FunctionTransformers are agnostic to passed treatment names
v0.14.0,Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Set random seed
v0.14.0,Generate data
v0.14.0,DGP constants
v0.14.0,Test data
v0.14.0,Constant treatment effect and propensity
v0.14.0,Heterogeneous treatment and propensity
v0.14.0,ensure that we've got at least two of every element
v0.14.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.14.0,ensure that we can serialize unfit estimator
v0.14.0,ensure that we can serialize fit estimator
v0.14.0,make sure we can call the marginal_effect and effect methods
v0.14.0,test const marginal inference
v0.14.0,test effect inference
v0.14.0,test marginal effect inference
v0.14.0,test coef_ and intercept_ inference
v0.14.0,verify we can generate the summary
v0.14.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.14.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.14.0,create a simple artificial setup where effect of moving from treatment
v0.14.0,"1 -> 2 is 2,"
v0.14.0,"1 -> 3 is 1, and"
v0.14.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.14.0,"Using an uneven number of examples from different classes,"
v0.14.0,"and having the treatments in non-lexicographic order,"
v0.14.0,Should rule out some basic issues.
v0.14.0,test that we can fit with a KFold instance
v0.14.0,test that we can fit with a train/test iterable
v0.14.0,"for at least some of the examples, the CI should have nonzero width"
v0.14.0,"for at least some of the examples, the CI should have nonzero width"
v0.14.0,"for at least some of the examples, the CI should have nonzero width"
v0.14.0,test coef__inference function works
v0.14.0,test intercept__inference function works
v0.14.0,test summary function works
v0.14.0,Test inputs
v0.14.0,self._test_inputs(DR_learner)
v0.14.0,Test constant treatment effect
v0.14.0,Test heterogeneous treatment effect
v0.14.0,Test heterogenous treatment effect for W =/= None
v0.14.0,Sparse DGP
v0.14.0,Treatment effect coef
v0.14.0,Other coefs
v0.14.0,Features and controls
v0.14.0,Test sparse estimator
v0.14.0,"--> test coef_, intercept_"
v0.14.0,--> test treatment effects
v0.14.0,Restrict x_test to vectors of norm < 1
v0.14.0,--> check inference
v0.14.0,Check that a majority of true effects lie in the 5-95% CI
v0.14.0,test outer grouping
v0.14.0,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.14.0,test nested grouping
v0.14.0,DML nested CV works via a 'cv' attribute
v0.14.0,"with 2-fold outer and 2-fold inner grouping, and six total groups,"
v0.14.0,should get 1 or 2 groups per split
v0.14.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.14.0,whichever groups we see
v0.14.0,test nested grouping
v0.14.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.14.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.14.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.14.0,helper class
v0.14.0,Fit learner and get the effect
v0.14.0,Get the true treatment effect
v0.14.0,Compute treatment effect residuals (absolute)
v0.14.0,Check that at least 90% of predictions are within tolerance interval
v0.14.0,Only for heterogeneous TE
v0.14.0,Fit learner on X and W and get the effect
v0.14.0,Get the true treatment effect
v0.14.0,Compute treatment effect residuals (absolute)
v0.14.0,Check that at least 90% of predictions are within tolerance interval
v0.14.0,Check that one can pass in regular lists
v0.14.0,Check that it fails correctly if lists of different shape are passed in
v0.14.0,Check that it fails when T contains values other than 0 and 1
v0.14.0,"Check that it works when T, Y have shape (n, 1)"
v0.14.0,Generate covariates
v0.14.0,Generate treatment
v0.14.0,Calculate outcome
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,DGP constants
v0.14.0,DGP coefficients
v0.14.0,Generated outcomes
v0.14.0,################
v0.14.0,WeightedLasso #
v0.14.0,################
v0.14.0,Define weights
v0.14.0,Define extended datasets
v0.14.0,Range of alphas
v0.14.0,Compare with Lasso
v0.14.0,--> No intercept
v0.14.0,--> With intercept
v0.14.0,When DGP has no intercept
v0.14.0,When DGP has intercept
v0.14.0,--> Coerce coefficients to be positive
v0.14.0,--> Toggle max_iter & tol
v0.14.0,Define weights
v0.14.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.14.0,Mixed DGP scenario.
v0.14.0,Define extended datasets
v0.14.0,Define weights
v0.14.0,Define multioutput
v0.14.0,##################
v0.14.0,WeightedLassoCV #
v0.14.0,##################
v0.14.0,Define alphas to test
v0.14.0,Compare with LassoCV
v0.14.0,--> No intercept
v0.14.0,--> With intercept
v0.14.0,--> Force parameters to be positive
v0.14.0,Choose a smaller n to speed-up process
v0.14.0,Compare fold weights
v0.14.0,Define weights
v0.14.0,Define extended datasets
v0.14.0,Define splitters
v0.14.0,WeightedKFold splitter
v0.14.0,Map weighted splitter to an extended splitter
v0.14.0,Define alphas to test
v0.14.0,Compare with LassoCV
v0.14.0,--> No intercept
v0.14.0,--> With intercept
v0.14.0,--> Force parameters to be positive
v0.14.0,###########################
v0.14.0,MultiTaskWeightedLassoCV #
v0.14.0,###########################
v0.14.0,Define alphas to test
v0.14.0,Define splitter
v0.14.0,Compare with MultiTaskLassoCV
v0.14.0,--> No intercept
v0.14.0,--> With intercept
v0.14.0,Define weights
v0.14.0,Define extended datasets
v0.14.0,Define splitters
v0.14.0,WeightedKFold splitter
v0.14.0,Map weighted splitter to an extended splitter
v0.14.0,Define alphas to test
v0.14.0,Compare with LassoCV
v0.14.0,--> No intercept
v0.14.0,--> With intercept
v0.14.0,#########################
v0.14.0,WeightedLassoCVWrapper #
v0.14.0,#########################
v0.14.0,perform 1D fit
v0.14.0,perform 2D fit
v0.14.0,################
v0.14.0,DebiasedLasso #
v0.14.0,################
v0.14.0,Test DebiasedLasso without weights
v0.14.0,--> Check debiased coeffcients without intercept
v0.14.0,--> Check debiased coeffcients with intercept
v0.14.0,--> Check 5-95 CI coverage for unit vectors
v0.14.0,Test DebiasedLasso with weights for one DGP
v0.14.0,Define weights
v0.14.0,Define extended datasets
v0.14.0,--> Check debiased coefficients
v0.14.0,Define weights
v0.14.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.14.0,--> Check debiased coeffcients
v0.14.0,Test that attributes propagate correctly
v0.14.0,Test MultiOutputDebiasedLasso without weights
v0.14.0,--> Check debiased coeffcients without intercept
v0.14.0,--> Check debiased coeffcients with intercept
v0.14.0,--> Check CI coverage
v0.14.0,Test MultiOutputDebiasedLasso with weights
v0.14.0,Define weights
v0.14.0,Define extended datasets
v0.14.0,--> Check debiased coefficients
v0.14.0,Unit vectors
v0.14.0,Unit vectors
v0.14.0,Check coeffcients and intercept are the same within tolerance
v0.14.0,Check results are similar with tolerance 1e-6
v0.14.0,Check if multitask
v0.14.0,Check that same alpha is chosen
v0.14.0,Check that the coefficients are similar
v0.14.0,selective ridge has a simple implementation that we can test against
v0.14.0,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.14.0,"it should be the case that when we set fit_intercept to true,"
v0.14.0,it doesn't matter whether the penalized model also fits an intercept or not
v0.14.0,create an extra copy of rows with weight 2
v0.14.0,"instead of a slice, explicitly return an array of indices"
v0.14.0,_penalized_inds is only set during fitting
v0.14.0,cv exists on penalized model
v0.14.0,now we can access _penalized_inds
v0.14.0,check that we can read the cv attribute back out from the underlying model
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"global and cohort data should have exactly the same structure, but different values"
v0.14.0,local index should have as many times entries as global as there were rows passed in
v0.14.0,continuous treatments have typical treatment values equal to
v0.14.0,the mean of the absolute value of non-zero entries
v0.14.0,discrete treatments have typical treatment value 1
v0.14.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.14.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.14.0,policy value should exceed always treating with any treatment
v0.14.0,"global shape is (d_y, sum(d_t))"
v0.14.0,global and cohort row-wise dicts have d_y * d_t entries
v0.14.0,local dictionary is flattened to n_rows * d_y * d_t
v0.14.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.0,features; for categoricals they should appear #cats-1 times each
v0.14.0,"global and cohort data should have exactly the same structure, but different values"
v0.14.0,local index should have as many times entries as global as there were rows passed in
v0.14.0,features; for categoricals they should appear #cats-1 times each
v0.14.0,"global shape is (d_y, sum(d_t))"
v0.14.0,global and cohort row-wise dicts have d_y * d_t entries
v0.14.0,local dictionary is flattened to n_rows * d_y * d_t
v0.14.0,continuous treatments have typical treatment values equal to
v0.14.0,the mean of the absolute value of non-zero entries
v0.14.0,discrete treatments have typical treatment value 1
v0.14.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.14.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.14.0,policy value should exceed always treating with any treatment
v0.14.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.0,"global and cohort data should have exactly the same structure, but different values"
v0.14.0,local index should have as many times entries as global as there were rows passed in
v0.14.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.14.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.14.0,policy value should exceed always treating with any treatment
v0.14.0,"global shape is (d_y, sum(d_t))"
v0.14.0,global and cohort row-wise dicts have d_y * d_t entries
v0.14.0,local dictionary is flattened to n_rows * d_y * d_t
v0.14.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.0,features; for categoricals they should appear #cats-1 times each
v0.14.0,make sure we don't run into problems dropping every index
v0.14.0,"global and cohort data should have exactly the same structure, but different values"
v0.14.0,local index should have as many times entries as global as there were rows passed in
v0.14.0,"global shape is (d_y, sum(d_t))"
v0.14.0,global and cohort row-wise dicts have d_y * d_t entries
v0.14.0,local dictionary is flattened to n_rows * d_y * d_t
v0.14.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.14.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.14.0,policy value should exceed always treating with any treatment
v0.14.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.14.0,"global and cohort data should have exactly the same structure, but different values"
v0.14.0,local index should have as many times entries as global as there were rows passed in
v0.14.0,features; for categoricals they should appear #cats-1 times each
v0.14.0,"global shape is (d_y, sum(d_t))"
v0.14.0,global and cohort row-wise dicts have d_y * d_t entries
v0.14.0,local dictionary is flattened to n_rows * d_y * d_t
v0.14.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.14.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.14.0,policy value should exceed always treating with any treatment
v0.14.0,dgp
v0.14.0,model
v0.14.0,model
v0.14.0,"columns 'd', 'e', 'h' have too many values"
v0.14.0,"columns 'd', 'e' have too many values"
v0.14.0,lowering bound shouldn't affect already fit columns when warm starting
v0.14.0,"column d is now okay, too"
v0.14.0,verify that we can use a scalar treatment cost
v0.14.0,verify that we can specify per-treatment costs for each sample
v0.14.0,verify that using the same state returns the same results each time
v0.14.0,set the categories for column 'd' explicitly so that b is default
v0.14.0,"first column: 10 ones, this is fine"
v0.14.0,"second column: 6 categories, plenty of random instances of each"
v0.14.0,this is fine only if we increase the cateogry limit
v0.14.0,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.14.0,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.14.0,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.14.0,forest heterogeneity won't work
v0.14.0,"sixth column: just 1 one, not enough even without check"
v0.14.0,increase bound on cat expansion
v0.14.0,skip checks (reducing folds accordingly)
v0.14.0,"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
v0.14.0,"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
v0.14.0,Pass an example where W is irrelevant and X is confounder
v0.14.0,"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
v0.14.0,zeroed out and the test will fail
v0.14.0,"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
v0.14.0,rescaling X shouldn't affect the first stage models because they normalize the inputs
v0.14.0,"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
v0.14.0,cross terms
v0.14.0,scale by 1000 to match the input to this model:
v0.14.0,"the scale of X does matter for the final model, which keeps results in user-denominated units"
v0.14.0,rescaling X still shouldn't affect the first stage models
v0.14.0,TODO: we don't recover the correct values with enough accuracy to enable this assertion
v0.14.0,is there a different way to verify that we are learning the correct coefficients?
v0.14.0,"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,DGP constants
v0.14.0,Define data features
v0.14.0,Added `_df`to names to be different from the default cate_estimator names
v0.14.0,Generate data
v0.14.0,################################
v0.14.0,Single treatment and outcome #
v0.14.0,################################
v0.14.0,Test LinearDML
v0.14.0,|--> Test featurizers
v0.14.0,"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
v0.14.0,|--> Test re-fit
v0.14.0,Test SparseLinearDML
v0.14.0,Test ForestDML
v0.14.0,###################################
v0.14.0,Mutiple treatments and outcomes #
v0.14.0,###################################
v0.14.0,Test LinearDML
v0.14.0,Test SparseLinearDML
v0.14.0,"Single outcome only, ORF does not support multiple outcomes"
v0.14.0,Test DMLOrthoForest
v0.14.0,Test DROrthoForest
v0.14.0,Test XLearner
v0.14.0,Skipping population summary names test because bootstrap inference is too slow
v0.14.0,Test SLearner
v0.14.0,Test TLearner
v0.14.0,Test LinearDRLearner
v0.14.0,Test SparseLinearDRLearner
v0.14.0,Test ForestDRLearner
v0.14.0,Test LinearIntentToTreatDRIV
v0.14.0,Test DeepIV
v0.14.0,Test categorical treatments
v0.14.0,Check refit
v0.14.0,Check refit after setting categories
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Linear models are required for parametric dml
v0.14.0,sample weighting models are required for nonparametric dml
v0.14.0,Test values
v0.14.0,TLearner test
v0.14.0,Instantiate TLearner
v0.14.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.14.0,Test constant treatment effect with multi output Y
v0.14.0,Test heterogeneous treatment effect
v0.14.0,Need interactions between T and features
v0.14.0,Test heterogeneous treatment effect with multi output Y
v0.14.0,Instantiate DomainAdaptationLearner
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,test base values equals to mean of constant marginal effect
v0.14.0,test shape of shap values output is as expected
v0.14.0,test shape of attribute of explanation object is as expected
v0.14.0,test length of feature names equals to shap values shape
v0.14.0,test base values equals to mean of constant marginal effect
v0.14.0,test shape of shap values output is as expected
v0.14.0,test shape of attribute of explanation object is as expected
v0.14.0,test length of feature names equals to shap values shape
v0.14.0,Treatment effect function
v0.14.0,Outcome support
v0.14.0,Treatment support
v0.14.0,"Generate controls, covariates, treatments and outcomes"
v0.14.0,Heterogeneous treatment effects
v0.14.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.14.0,through shap package.
v0.14.0,test shap could generate the plot from the shap_values
v0.14.0,"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Check inputs
v0.14.0,Check inputs
v0.14.0,Check inputs
v0.14.0,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.14.0,"Thus, we append the controls column before the one-hot-encoded T"
v0.14.0,"We might want to revisit, though, since it's linearly determined by the others"
v0.14.0,Check inputs
v0.14.0,Check inputs
v0.14.0,Estimate response function
v0.14.0,Check inputs
v0.14.0,Train model on controls. Assign higher weight to units resembling
v0.14.0,treated units.
v0.14.0,Train model on the treated. Assign higher weight to units resembling
v0.14.0,control units.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,TODO: make sure to use random seeds wherever necessary
v0.14.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.14.0,"unfortunately with the Theano and Tensorflow backends,"
v0.14.0,the straightforward use of K.stop_gradient can cause an error
v0.14.0,because the parameters of the intermediate layers are now disconnected from the loss;
v0.14.0,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.14.0,so that those layers remain connected but with 0 gradient
v0.14.0,|| t - mu_i || ^2
v0.14.0,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.14.0,Use logsumexp for numeric stability:
v0.14.0,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.14.0,TODO: does the numeric stability actually make any difference?
v0.14.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.14.0,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.14.0,generate cumulative sum via matrix multiplication
v0.14.0,"Generate standard uniform values in shape (batch_size,1)"
v0.14.0,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.14.0,we use uniform_like instead with an input of an appropriate shape)
v0.14.0,convert to floats and multiply to perform equivalent of logical AND
v0.14.0,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.14.0,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.14.0,we use normal_like instead with an input of an appropriate shape)
v0.14.0,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.14.0,prevent gradient from passing through sampling
v0.14.0,three options: biased or upper-bound loss require a single number of samples;
v0.14.0,unbiased can take different numbers for the network and its gradient
v0.14.0,"sample: (() -> Layer, int) -> Layer"
v0.14.0,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.14.0,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.14.0,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.14.0,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.14.0,the dimensionality of the output of the network
v0.14.0,TODO: is there a more robust way to do this?
v0.14.0,TODO: do we need to give the user more control over other arguments to fit?
v0.14.0,"subtle point: we need to build a new model each time,"
v0.14.0,because each model encapsulates its randomness
v0.14.0,TODO: do we need to give the user more control over other arguments to fit?
v0.14.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.14.0,not a general tensor (because of how backprop works in every framework)
v0.14.0,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.14.0,but this seems annoying...)
v0.14.0,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.14.0,TODO: any way to get this to work on batches of arbitrary size?
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.14.0,"fit on projected Z: E[T * E[T|X,Z]|X]"
v0.14.0,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.14.0,"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
v0.14.0,"shape (n,)"
v0.14.0,"shape (n,)"
v0.14.0,"shape(n,)"
v0.14.0,TODO: prel_model_effect could allow sample_var and freq_weight?
v0.14.0,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.14.0,"we need to undo the one-hot encoding for calling effect,"
v0.14.0,since it expects raw values
v0.14.0,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.14.0,target will be discrete and will be inversed from FirstStageWrapper
v0.14.0,"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
v0.14.0,reshape the predictions
v0.14.0,concat W and Z
v0.14.0,check nuisances outcome shape
v0.14.0,Y_res could be a vector or 1-dimensional 2d-array
v0.14.0,"all could be reshaped to vector since Y, T, Z are all single dimensional."
v0.14.0,Estimate final model of theta(X) by minimizing the square loss:
v0.14.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.14.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.14.0,at the expense of some small bias. For points with very small covariance we revert
v0.14.0,to the model-based preliminary estimate and do not add the correction term.
v0.14.0,A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
v0.14.0,Used by both DRIV and IntentToTreatDRIV.
v0.14.0,Maggie: I think that would be the case?
v0.14.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.14.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.14.0,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.14.0,attribute so that the trained featurizer will be passed through
v0.14.0,Handles the corner case when X=None but featurizer might be not None
v0.14.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.14.0,this is a regression model since proj_t is probability
v0.14.0,outcome is continuous since proj_t is probability
v0.14.0,Define the data generation functions
v0.14.0,Define the data generation functions
v0.14.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.14.0,Define the data generation functions
v0.14.0,TODO: support freq_weight and sample_var in debiased lasso
v0.14.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.14.0,Define the data generation functions
v0.14.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.14.0,concat W and Z
v0.14.0,"we need to undo the one-hot encoding for calling effect,"
v0.14.0,since it expects raw values
v0.14.0,concat W and Z
v0.14.0,"we need to undo the one-hot encoding for calling effect,"
v0.14.0,since it expects raw values
v0.14.0,reshape the predictions
v0.14.0,"T_res, Z_res, beta expect shape to be (n,1)"
v0.14.0,Define the data generation functions
v0.14.0,maybe shouldn't expose fit_cate_intercept in this class?
v0.14.0,Define the data generation functions
v0.14.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.14.0,TODO: do correct adjustment for sample_var
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,concat W and Z
v0.14.0,concat W and Z
v0.14.0,concat W and Z
v0.14.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.14.0,Define the data generation functions
v0.14.0,"train E[T|X,W,Z]"
v0.14.0,"train [Z|X,W]"
v0.14.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.14.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.14.0,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.14.0,attribute so that the trained featurizer will be passed through
v0.14.0,Handles the corner case when X=None but featurizer might be not None
v0.14.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.14.0,concat W and Z
v0.14.0,note that groups are not passed to score because they are only used for fitting
v0.14.0,concat W and Z
v0.14.0,note that sample_weight and groups are not passed to predict because they are only used for fitting
v0.14.0,concat W and Z
v0.14.0,A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
v0.14.0,Used by both Parametric and Non Parametric DMLIV.
v0.14.0,override only so that we can enforce Z to be required
v0.14.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.14.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.14.0,for internal use by the library
v0.14.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.14.0,Handles the corner case when X=None but featurizer might be not None
v0.14.0,Define the data generation functions
v0.14.0,Get input names
v0.14.0,Summary
v0.14.0,coefficient
v0.14.0,intercept
v0.14.0,Define the data generation functions
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,"this will have dimension (d,) + shape(X)"
v0.14.0,send the first dimension to the end
v0.14.0,columns are featurized independently; partial derivatives are only non-zero
v0.14.0,when taken with respect to the same column each time
v0.14.0,don't fit intercept; manually add column of ones to the data instead;
v0.14.0,this allows us to ignore the intercept when computing marginal effects
v0.14.0,make T 2D if if was a vector
v0.14.0,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.14.0,two stage approximation
v0.14.0,"first, get basis expansions of T, X, and Z"
v0.14.0,TODO: is it right that the effective number of intruments is the
v0.14.0,"product of ft_X and ft_Z, not just ft_Z?"
v0.14.0,"regress T expansion on X,Z expansions concatenated with W"
v0.14.0,"predict ft_T from interacted ft_X, ft_Z"
v0.14.0,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.14.0,dT may be only 2-dimensional)
v0.14.0,promote dT to 3D if necessary (e.g. if T was a vector)
v0.14.0,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,TODO: this utility is documented but internal; reimplement?
v0.14.0,TODO: this utility is even less public...
v0.14.0,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.14.0,use same Cs as would be used by default by LogisticRegressionCV
v0.14.0,NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
v0.14.0,which could affect how many times each distinct Y value needs to be present in the data
v0.14.0,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.14.0,but also supports get_feature_names with expected signature
v0.14.0,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.14.0,NOTE: we rely on the passthrough columns coming first in the concatenated X;W
v0.14.0,"when we pipeline scaling with our first stage models later, so the order here is important"
v0.14.0,Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
v0.14.0,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.14.0,Convert SingleTreeInterpreter to a python dictionary
v0.14.0,named tuple type for storing results inside CausalAnalysis class;
v0.14.0,must be lifted to module level to enable pickling
v0.14.0,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
v0.14.0,"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
v0.14.0,"when running the first stage models, but don't want to scale the X columns when running the final model,"
v0.14.0,since then our coefficients will have odd units and our trees will also have decisions using those units.
v0.14.0,
v0.14.0,"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
v0.14.0,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.14.0,Controls are all other columns of X
v0.14.0,"can't use X[:, feat_ind] when X is a DataFrame"
v0.14.0,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.14.0,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.14.0,so that the user can opt-in to allowing unseen treatment values
v0.14.0,(and return NaN or something in that case)
v0.14.0,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
v0.14.0,and so we can just peel the first columns off of that combined array for rescaling in the pipeline
v0.14.0,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
v0.14.0,"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
v0.14.0,becomes a valid approach to handling this
v0.14.0,array checking routines don't accept 0-width arrays
v0.14.0,perform model selection
v0.14.0,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.14.0,convert to NormalInferenceResults for consistency
v0.14.0,Set the dictionary values shared between local and global summaries
v0.14.0,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.14.0,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.14.0,required to fit a discrete DML model
v0.14.0,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
v0.14.0,sub-cases of models or also integrate with azure autoML. (post-MVP)
v0.14.0,"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
v0.14.0,"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
v0.14.0,TODO: Enable multi-class classification (post-MVP)
v0.14.0,Validate inputs
v0.14.0,TODO: check compatibility of X and Y lengths
v0.14.0,"no previous fit, cancel warm start"
v0.14.0,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.14.0,"if heterogeneity_inds is 1D, repeat it"
v0.14.0,heterogeneity inds should be a 2D list of length same as train_inds
v0.14.0,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.14.0,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.14.0,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.14.0,train the Y model
v0.14.0,"perform model selection for the Y model using all X, not on a per-column basis"
v0.14.0,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.14.0,work with the regression wrapper
v0.14.0,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.14.0,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.14.0,since otherwise we'll have too many columns to be able to train a classifier
v0.14.0,start with empty results and default shared insights
v0.14.0,convert categorical indicators to numeric indices
v0.14.0,check for indices over the categorical expansion bound
v0.14.0,assume we'll be able to train former failures this time; we'll add them back if not
v0.14.0,"can't remove in place while iterating over new_inds, so store in separate list"
v0.14.0,"train the model, but warn"
v0.14.0,no model can be trained in this case since we need more folds
v0.14.0,"don't train a model, but suggest workaround since there are enough instances of least"
v0.14.0,populated class
v0.14.0,also remove from train_inds so we don't try to access the result later
v0.14.0,extract subset of names matching new columns
v0.14.0,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.14.0,don't want to cache this failed result
v0.14.0,properties to return from effect InferenceResults
v0.14.0,properties to return from PopulationSummaryResults
v0.14.0,Converts strings to property lookups or method calls as a convenience so that the
v0.14.0,_point_props and _summary_props above can be applied to an inference object
v0.14.0,Create a summary combining all results into a single output; this is used
v0.14.0,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.14.0,"or a dictionary, respectively, based on the summary function passed into this method"
v0.14.0,"ensure array has shape (m,y,t)"
v0.14.0,population summary is missing sample dimension; add it for consistency
v0.14.0,outcome dimension is missing; add it for consistency
v0.14.0,add singleton treatment dimension if missing
v0.14.0,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.14.0,"each attr has dimension (m,y) or (m,y,t)"
v0.14.0,concatenate along treatment dimension
v0.14.0,"for dictionary representation, want to remove unneeded sample dimension"
v0.14.0,in cohort and global results
v0.14.0,TODO: enrich outcome logic for multi-class classification when that is supported
v0.14.0,There is no actual sample level in this data
v0.14.0,can't drop only level
v0.14.0,should be serialization-ready and contain no numpy arrays
v0.14.0,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.14.0,TODO: Note that there's no column metadata for the sample number - should there be?
v0.14.0,"need to replicate the column info for each sample, then remove from the shared data"
v0.14.0,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.14.0,which may need to be revisited once we support multiclass
v0.14.0,get the length of the list corresponding to the first dictionary key
v0.14.0,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.14.0,a global inference indicates the effect of that one feature on the outcome
v0.14.0,need to reshape the output to match the input
v0.14.0,we want to offset the inference object by the baseline estimate of y
v0.14.0,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.14.0,get the length of the list corresponding to the first dictionary key
v0.14.0,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.14.0,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.14.0,because then scaling the difference between treatment value and treatment costs is the
v0.14.0,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.14.0,
v0.14.0,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.14.0,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.14.0,(rather than just not treating at all)
v0.14.0,
v0.14.0,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.14.0,2 * policy_value - always_treat
v0.14.0,includes exactly their contribution because policy_value and always_treat both include it
v0.14.0,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.14.0,2 * policy_value - always-treat
v0.14.0,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.14.0,is zero and the contribution to always_treat is negative
v0.14.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.14.0,"however, the tree can't store the feature and treatment names we compute here..."
v0.14.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.14.0,"however, the tree can't store the feature and treatment names we compute here..."
v0.14.0,get dataframe with all but selected column
v0.14.0,apply 10% of a typical treatment for this feature
v0.14.0,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.14.0,set the effect bounds; for positive treatments these agree with
v0.14.0,"the estimates; for negative treatments, we need to invert the interval"
v0.14.0,the effect is now always positive since we decrease treatment when negative
v0.14.0,"for discrete treatment, stack a zero result in front for control"
v0.14.0,we need to call effect_inference to get the correct CI between the two treatment options
v0.14.0,we now need to construct the delta in the cost between the two treatments and translate the effect
v0.14.0,remove third dimenions potentially added
v0.14.0,"find cost of current treatment: equality creates a 2d array with True on each row,"
v0.14.0,only if its the location of the current treatment. Then we take the corresponding cost.
v0.14.0,construct index of current treatment
v0.14.0,add second dimension if needed for broadcasting during translation of effect
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,TODO: conisder working around relying on sklearn implementation details
v0.14.0,"Found a good split, return."
v0.14.0,Record all splits in case the stratification by weight yeilds a worse partition
v0.14.0,Reseed random generator and try again
v0.14.0,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.14.0,"Found a good split, return."
v0.14.0,Did not find a good split
v0.14.0,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.14.0,Return most weight-balanced partition
v0.14.0,Weight stratification algorithm
v0.14.0,Sort weights for weight strata search
v0.14.0,There are some leftover indices that have yet to be assigned
v0.14.0,Append stratum splits to overall splits
v0.14.0,"If classification methods produce multiple columns of output,"
v0.14.0,we need to manually encode classes to ensure consistent column ordering.
v0.14.0,We clone the estimator to make sure that all the folds are
v0.14.0,"independent, and that it is pickle-able."
v0.14.0,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.14.0,`predictions` is a list of method outputs from each fold.
v0.14.0,"If each of those is also a list, then treat this as a"
v0.14.0,multioutput-multiclass task. We need to separately concatenate
v0.14.0,the method outputs for each label into an `n_labels` long list.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Our classes that derive from sklearn ones sometimes include
v0.14.0,inherited docstrings that have embedded doctests; we need the following imports
v0.14.0,so that they don't break.
v0.14.0,TODO: consider working around relying on sklearn implementation details
v0.14.0,"TODO: once we drop support for sklearn < 1.0, we can remove this"
v0.14.0,"if we're decorating a class, just update the __init__ method,"
v0.14.0,so that the result is still a class instead of a wrapper method
v0.14.0,normalize was deprecated or removed; don't need to do anything
v0.14.0,"Convert X, y into numpy arrays"
v0.14.0,Define fit parameters
v0.14.0,Some algorithms don't have a check_input option
v0.14.0,Check weights array
v0.14.0,Check that weights are size-compatible
v0.14.0,Normalize inputs
v0.14.0,Weight inputs
v0.14.0,Fit base class without intercept
v0.14.0,Fit Lasso
v0.14.0,Reset intercept
v0.14.0,The intercept is not calculated properly due the sqrt(weights) factor
v0.14.0,so it must be recomputed
v0.14.0,Fit lasso without weights
v0.14.0,Make weighted splitter
v0.14.0,Fit weighted model
v0.14.0,Make weighted splitter
v0.14.0,Fit weighted model
v0.14.0,Call weighted lasso on reduced design matrix
v0.14.0,Weighted tau
v0.14.0,Select optimal penalty
v0.14.0,Warn about consistency
v0.14.0,"Convert X, y into numpy arrays"
v0.14.0,Fit weighted lasso with user input
v0.14.0,"Center X, y"
v0.14.0,Calculate quantities that will be used later on. Account for centered data
v0.14.0,Calculate coefficient and error variance
v0.14.0,Add coefficient correction
v0.14.0,Set coefficients and intercept standard errors
v0.14.0,Set intercept
v0.14.0,Return alpha to 'auto' state
v0.14.0,"Note that in the case of no intercept, X_offset is 0"
v0.14.0,Calculate the variance of the predictions
v0.14.0,Calculate prediction confidence intervals
v0.14.0,Assumes flattened y
v0.14.0,Compute weighted residuals
v0.14.0,To be done once per target. Assumes y can be flattened.
v0.14.0,Assumes that X has already been offset
v0.14.0,Special case: n_features=1
v0.14.0,Compute Lasso coefficients for the columns of the design matrix
v0.14.0,Compute C_hat
v0.14.0,Compute theta_hat
v0.14.0,Allow for single output as well
v0.14.0,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.14.0,Set coef_ attribute
v0.14.0,Set intercept_ attribute
v0.14.0,Set selected_alpha_ attribute
v0.14.0,Set coef_stderr_
v0.14.0,intercept_stderr_
v0.14.0,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.14.0,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.14.0,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.14.0,set intercept_ attribute
v0.14.0,set coef_ attribute
v0.14.0,set alpha_ attribute
v0.14.0,set alphas_ attribute
v0.14.0,set n_iter_ attribute
v0.14.0,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.14.0,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.14.0,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.14.0,now regress X1 on y - X2 * beta2 to learn beta1
v0.14.0,set coef_ and intercept_ attributes
v0.14.0,Note that the penalized model should *not* have an intercept
v0.14.0,don't proxy special methods
v0.14.0,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.14.0,regressor incorrectly
v0.14.0,"Note: for known attributes that have been set this method will not be called,"
v0.14.0,so we should just throw here because this is an attribute belonging to this class
v0.14.0,but which hasn't yet been set on this instance
v0.14.0,set default values for None
v0.14.0,check freq_weight should be integer and should be accompanied by sample_var
v0.14.0,check array shape
v0.14.0,weight X and y and sample_var
v0.14.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.14.0,set default values for None
v0.14.0,check array shape
v0.14.0,check dimension of instruments is more than dimension of treatments
v0.14.0,weight X and y
v0.14.0,learn point estimate
v0.14.0,solve first stage linear regression E[T|Z]
v0.14.0,"""that"" means T̂"
v0.14.0,solve second stage linear regression E[Y|that]
v0.14.0,(T̂.T*T̂)^{-1}
v0.14.0,learn cov(theta)
v0.14.0,(T̂.T*T̂)^{-1}
v0.14.0,sigma^2
v0.14.0,reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,AzureML
v0.14.0,helper imports
v0.14.0,write the details of the workspace to a configuration file to the notebook library
v0.14.0,if y is a multioutput model
v0.14.0,Make sure second dimension has 1 or more item
v0.14.0,switch _inner Model to a MultiOutputRegressor
v0.14.0,flatten array as automl only takes vectors for y
v0.14.0,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.14.0,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.14.0,as an sklearn estimator
v0.14.0,fit implementation for a single output model.
v0.14.0,Create experiment for specified workspace
v0.14.0,Configure automl_config with training set information.
v0.14.0,"Wait for remote run to complete, the set the model"
v0.14.0,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.14.0,create model and pass model into final.
v0.14.0,"If item is an automl config, get its corresponding"
v0.14.0,AutomatedML Model and add it to new_Args
v0.14.0,"If item is an automl config, get its corresponding"
v0.14.0,AutomatedML Model and set it for this key in
v0.14.0,kwargs
v0.14.0,takes in either automated_ml config and instantiates
v0.14.0,an AutomatedMLModel
v0.14.0,The prefix can only be 18 characters long
v0.14.0,"because prefixes come from kwarg_names, we must ensure they are"
v0.14.0,short enough.
v0.14.0,Get workspace from config file.
v0.14.0,Take the intersect of the white for sample
v0.14.0,weights and linear models
v0.14.0,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,average the outcome dimension if it exists and ensure 2d y_pred
v0.14.0,get index of best treatment
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,TODO: consider working around relying on sklearn implementation details
v0.14.0,Create splits of causal tree
v0.14.0,Make sure the correct exception is being rethrown
v0.14.0,Must make sure indices are merged correctly
v0.14.0,Convert rows to columns
v0.14.0,Require group assignment t to be one-hot-encoded
v0.14.0,Get predictions for the 2 splits
v0.14.0,Must make sure indices are merged correctly
v0.14.0,Crossfitting
v0.14.0,Compute weighted nuisance estimates
v0.14.0,-------------------------------------------------------------------------------
v0.14.0,Calculate the covariance matrix corresponding to the BLB inference
v0.14.0,
v0.14.0,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.14.0,2. Calculate the weighted moments for each tree slice to create a matrix
v0.14.0,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.14.0,in that slice from the overall parameter estimate.
v0.14.0,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.14.0,-------------------------------------------------------------------------------
v0.14.0,Calclulate covariance matrix through BLB
v0.14.0,Estimators
v0.14.0,OrthoForest parameters
v0.14.0,Sub-forests
v0.14.0,Auxiliary attributes
v0.14.0,Fit check
v0.14.0,TODO: Check performance
v0.14.0,Must normalize weights
v0.14.0,Override the CATE inference options
v0.14.0,Add blb inference to parent's options
v0.14.0,Generate subsample indices
v0.14.0,Build trees in parallel
v0.14.0,Bootstraping has repetitions in tree sample
v0.14.0,Similar for `a` weights
v0.14.0,Bootstraping has repetitions in tree sample
v0.14.0,Define subsample size
v0.14.0,Safety check
v0.14.0,Draw points to create little bags
v0.14.0,Copy and/or define models
v0.14.0,Define nuisance estimators
v0.14.0,Define parameter estimators
v0.14.0,Define
v0.14.0,Need to redefine fit here for auto inference to work due to a quirk in how
v0.14.0,wrap_fit is defined
v0.14.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.14.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.14.0,Override to flatten output if T is flat
v0.14.0,Check that all discrete treatments are represented
v0.14.0,Nuissance estimates evaluated with cross-fitting
v0.14.0,Define 2-fold iterator
v0.14.0,Check if there is only one example of some class
v0.14.0,Define 2-fold iterator
v0.14.0,need safe=False when cloning for WeightedModelWrapper
v0.14.0,Compute residuals
v0.14.0,Compute coefficient by OLS on residuals
v0.14.0,"Parameter returned by LinearRegression is (d_T, )"
v0.14.0,Compute residuals
v0.14.0,Compute coefficient by OLS on residuals
v0.14.0,ell_2 regularization
v0.14.0,Ridge regression estimate
v0.14.0,"Parameter returned is of shape (d_T, )"
v0.14.0,Return moments and gradients
v0.14.0,Compute residuals
v0.14.0,Compute moments
v0.14.0,"Moments shape is (n, d_T)"
v0.14.0,Compute moment gradients
v0.14.0,returns shape-conforming residuals
v0.14.0,Copy and/or define models
v0.14.0,Define parameter estimators
v0.14.0,Define moment and mean gradient estimator
v0.14.0,"Check that T is shape (n, )"
v0.14.0,Check T is numeric
v0.14.0,Train label encoder
v0.14.0,Call `fit` from parent class
v0.14.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.14.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.14.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.0,Override to flatten output if T is flat
v0.14.0,override only so that we can exclude treatment featurization verbiage in docstring
v0.14.0,Expand one-hot encoding to include the zero treatment
v0.14.0,"Test that T contains all treatments. If not, return None"
v0.14.0,Nuissance estimates evaluated with cross-fitting
v0.14.0,Define 2-fold iterator
v0.14.0,Check if there is only one example of some class
v0.14.0,No need to crossfit for internal nodes
v0.14.0,Compute partial moments
v0.14.0,"If any of the values in the parameter estimate is nan, return None"
v0.14.0,Compute partial moments
v0.14.0,Compute coefficient by OLS on residuals
v0.14.0,ell_2 regularization
v0.14.0,Ridge regression estimate
v0.14.0,"Parameter returned is of shape (d_T, )"
v0.14.0,Return moments and gradients
v0.14.0,Compute partial moments
v0.14.0,Compute moments
v0.14.0,"Moments shape is (n, d_T-1)"
v0.14.0,Compute moment gradients
v0.14.0,Need to calculate this in an elegant way for when propensity is 0
v0.14.0,This will flatten T
v0.14.0,Check that T is numeric
v0.14.0,Test whether the input estimator is supported
v0.14.0,Calculate confidence intervals for the parameter (marginal effect)
v0.14.0,Calculate confidence intervals for the effect
v0.14.0,Calculate the effects
v0.14.0,Calculate the standard deviations for the effects
v0.14.0,d_t=None here since we measure the effect across all Ts
v0.14.0,conditionally expand jacobian dimensions to align with einsum str
v0.14.0,Calculate the effects
v0.14.0,Calculate the standard deviations for the effects
v0.14.0,"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Causal tree parameters
v0.14.0,Tree structure
v0.14.0,No need for a random split since the data is already
v0.14.0,a random subsample from the original input
v0.14.0,node list stores the nodes that are yet to be splitted
v0.14.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.14.0,Create local sample set
v0.14.0,Compute nuisance estimates for the current node
v0.14.0,Nuisance estimate cannot be calculated
v0.14.0,Estimate parameter for current node
v0.14.0,Node estimate cannot be calculated
v0.14.0,Calculate moments and gradient of moments for current data
v0.14.0,Calculate inverse gradient
v0.14.0,The gradient matrix is not invertible.
v0.14.0,No good split can be found
v0.14.0,Calculate point-wise pseudo-outcomes rho
v0.14.0,a split is determined by a feature and a sample pair
v0.14.0,the number of possible splits is at most (number of features) * (number of node samples)
v0.14.0,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.14.0,parse row and column of random pair
v0.14.0,the sample of the pair is the integer division of the random number with n_feats
v0.14.0,calculate the binary indicator of whether sample i is on the left or the right
v0.14.0,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.14.0,calculate the number of samples on the left child for each proposed split
v0.14.0,calculate the analogous binary indicator for the samples in the estimation set
v0.14.0,calculate the number of estimation samples on the left child of each proposed split
v0.14.0,find the upper and lower bound on the size of the left split for the split
v0.14.0,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.14.0,on each side.
v0.14.0,similarly for the estimation sample set
v0.14.0,if there is no valid split then don't create any children
v0.14.0,filter only the valid splits
v0.14.0,calculate the average influence vector of the samples in the left child
v0.14.0,calculate the average influence vector of the samples in the right child
v0.14.0,take the square of each of the entries of the influence vectors and normalize
v0.14.0,by size of each child
v0.14.0,calculate the vector score of each candidate split as the average of left and right
v0.14.0,influence vectors
v0.14.0,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.14.0,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.14.0,where there might be large discontinuities in some parameter as the conditioning set varies
v0.14.0,calculate the scalar score of each split by aggregating across the vector of scores
v0.14.0,Find split that minimizes criterion
v0.14.0,Create child nodes with corresponding subsamples
v0.14.0,add the created children to the list of not yet split nodes
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.14.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.14.0,Licensed under the MIT License.
v0.13.1,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.13.1,configuration is all pulled from setup.cfg
v0.13.1,-*- coding: utf-8 -*-
v0.13.1,
v0.13.1,Configuration file for the Sphinx documentation builder.
v0.13.1,
v0.13.1,This file does only contain a selection of the most common options. For a
v0.13.1,full list see the documentation:
v0.13.1,http://www.sphinx-doc.org/en/main/config
v0.13.1,-- Path setup --------------------------------------------------------------
v0.13.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.13.1,add these directories to sys.path here. If the directory is relative to the
v0.13.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.13.1,
v0.13.1,-- Project information -----------------------------------------------------
v0.13.1,-- General configuration ---------------------------------------------------
v0.13.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.13.1,
v0.13.1,needs_sphinx = '1.0'
v0.13.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.13.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.13.1,ones.
v0.13.1,"Add any paths that contain templates here, relative to this directory."
v0.13.1,The suffix(es) of source filenames.
v0.13.1,You can specify multiple suffix as a list of string:
v0.13.1,
v0.13.1,"source_suffix = ['.rst', '.md']"
v0.13.1,The root toctree document.
v0.13.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.13.1,for a list of supported languages.
v0.13.1,
v0.13.1,This is also used if you do content translation via gettext catalogs.
v0.13.1,"Usually you set ""language"" from the command line for these cases."
v0.13.1,"List of patterns, relative to source directory, that match files and"
v0.13.1,directories to ignore when looking for source files.
v0.13.1,This pattern also affects html_static_path and html_extra_path.
v0.13.1,The name of the Pygments (syntax highlighting) style to use.
v0.13.1,-- Options for HTML output -------------------------------------------------
v0.13.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.13.1,a list of builtin themes.
v0.13.1,
v0.13.1,Theme options are theme-specific and customize the look and feel of a theme
v0.13.1,"further.  For a list of options available for each theme, see the"
v0.13.1,documentation.
v0.13.1,
v0.13.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.13.1,"relative to this directory. They are copied after the builtin static files,"
v0.13.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.13.1,html_static_path = ['_static']
v0.13.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.13.1,to template names.
v0.13.1,
v0.13.1,The default sidebars (for documents that don't match any pattern) are
v0.13.1,defined by theme itself.  Builtin themes are using these templates by
v0.13.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.13.1,'searchbox.html']``.
v0.13.1,
v0.13.1,html_sidebars = {}
v0.13.1,-- Options for HTMLHelp output ---------------------------------------------
v0.13.1,Output file base name for HTML help builder.
v0.13.1,-- Options for LaTeX output ------------------------------------------------
v0.13.1,The paper size ('letterpaper' or 'a4paper').
v0.13.1,
v0.13.1,"'papersize': 'letterpaper',"
v0.13.1,"The font size ('10pt', '11pt' or '12pt')."
v0.13.1,
v0.13.1,"'pointsize': '10pt',"
v0.13.1,Additional stuff for the LaTeX preamble.
v0.13.1,
v0.13.1,"'preamble': '',"
v0.13.1,Latex figure (float) alignment
v0.13.1,
v0.13.1,"'figure_align': 'htbp',"
v0.13.1,Grouping the document tree into LaTeX files. List of tuples
v0.13.1,"(source start file, target name, title,"
v0.13.1,"author, documentclass [howto, manual, or own class])."
v0.13.1,-- Options for manual page output ------------------------------------------
v0.13.1,One entry per manual page. List of tuples
v0.13.1,"(source start file, name, description, authors, manual section)."
v0.13.1,-- Options for Texinfo output ----------------------------------------------
v0.13.1,Grouping the document tree into Texinfo files. List of tuples
v0.13.1,"(source start file, target name, title, author,"
v0.13.1,"dir menu entry, description, category)"
v0.13.1,-- Options for Epub output -------------------------------------------------
v0.13.1,Bibliographic Dublin Core info.
v0.13.1,The unique identifier of the text. This can be a ISBN number
v0.13.1,or the project homepage.
v0.13.1,
v0.13.1,epub_identifier = ''
v0.13.1,A unique identification for the text.
v0.13.1,
v0.13.1,epub_uid = ''
v0.13.1,A list of files that should not be packed into the epub file.
v0.13.1,-- Extension configuration -------------------------------------------------
v0.13.1,-- Options for intersphinx extension ---------------------------------------
v0.13.1,Example configuration for intersphinx: refer to the Python standard library.
v0.13.1,-- Options for todo extension ----------------------------------------------
v0.13.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.13.1,-- Options for doctest extension -------------------------------------------
v0.13.1,we can document otherwise excluded entities here by returning False
v0.13.1,or skip otherwise included entities by returning True
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Calculate residuals
v0.13.1,Estimate E[T_res | Z_res]
v0.13.1,TODO. Deal with multi-class instrument
v0.13.1,Calculate nuisances
v0.13.1,Estimate E[T_res | Z_res]
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.13.1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.13.1,TODO. Deal with multi-class instrument
v0.13.1,Estimate final model of theta(X) by minimizing the square loss:
v0.13.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.13.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.13.1,at the expense of some small bias. For points with very small covariance we revert
v0.13.1,to the model-based preliminary estimate and do not add the correction term.
v0.13.1,Estimate preliminary theta in cross fitting manner
v0.13.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.13.1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.13.1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.13.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.13.1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.13.1,TODO. The solution below is not really a valid cross-fitting
v0.13.1,as the test data are used to create the proj_t on the train
v0.13.1,which in the second train-test loop is used to create the nuisance
v0.13.1,cov on the test data. Hence the T variable of some sample
v0.13.1,"is implicitly correlated with its cov nuisance, through this flow"
v0.13.1,"of information. However, this seems a rather weak correlation."
v0.13.1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.13.1,model.
v0.13.1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.13.1,Estimate preliminary theta in cross fitting manner
v0.13.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.13.1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.13.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.13.1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.13.1,#############################################################################
v0.13.1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.13.1,A/B test
v0.13.1,#############################################################################
v0.13.1,Estimate preliminary theta in cross fitting manner
v0.13.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.13.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.13.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.13.1,We can use statsmodel for all hypothesis testing capabilities
v0.13.1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.13.1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.13.1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.13.1,model_T_XZ = lambda: model_clf()
v0.13.1,#'days_visited': lambda:
v0.13.1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.13.1,Turn strings into categories for numeric mapping
v0.13.1,### Defining some generic regressors and classifiers
v0.13.1,This a generic non-parametric regressor
v0.13.1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.13.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.13.1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.13.1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.13.1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.13.1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.13.1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.13.1,model = lambda: LinearRegression(n_jobs=-1)
v0.13.1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.13.1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.13.1,underlying model whenever predict is called.
v0.13.1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.13.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.13.1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.13.1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.13.1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.13.1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.13.1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.13.1,We need to specify models to be used for each of these residualizations
v0.13.1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.13.1,"E[T | X, Z]"
v0.13.1,E[TZ | X]
v0.13.1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.13.1,n_splits determines the number of splits to be used for cross-fitting.
v0.13.1,# Algorithm 2 - Current Method
v0.13.1,In[121]:
v0.13.1,# Algorithm 3 - DRIV ATE
v0.13.1,dmliv_model_effect = lambda: model()
v0.13.1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.13.1,"dmliv_model_effect(),"
v0.13.1,n_splits=1)
v0.13.1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.13.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.13.1,"Once multiple treatments are supported, we'll need to fix this"
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.13.1,We can use statsmodel for all hypothesis testing capabilities
v0.13.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.13.1,We can use statsmodel for all hypothesis testing capabilities
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,TODO. Deal with multi-class instrument/treatment
v0.13.1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.13.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.13.1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.13.1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.13.1,##################
v0.13.1,Global settings #
v0.13.1,##################
v0.13.1,Global plotting controls
v0.13.1,"Control for support size, can control for more"
v0.13.1,#################
v0.13.1,File utilities #
v0.13.1,#################
v0.13.1,#################
v0.13.1,Plotting utils #
v0.13.1,#################
v0.13.1,bias
v0.13.1,var
v0.13.1,rmse
v0.13.1,r2
v0.13.1,Infer feature dimension
v0.13.1,Metrics by support plots
v0.13.1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.13.1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.13.1,Steven Wu <zhiww@microsoft.com>
v0.13.1,Initialize causal tree parameters
v0.13.1,Create splits of causal tree
v0.13.1,Estimate treatment effects at the leafs
v0.13.1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.13.1,the corresponding split and associating the effect computed on that leaf
v0.13.1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.13.1,Safety check
v0.13.1,Weighted linear regression
v0.13.1,Calculates weights
v0.13.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.13.1,over all indices
v0.13.1,Similar for `a` weights
v0.13.1,Doesn't have sample weights
v0.13.1,Is a linear model
v0.13.1,Weighted linear regression
v0.13.1,Calculates weights
v0.13.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.13.1,over all indices
v0.13.1,Similar for `a` weights
v0.13.1,normalize weights
v0.13.1,"Split the data in half, train and test"
v0.13.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.13.1,"a function of W, using only the train fold"
v0.13.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.13.1,"Split the data in half, train and test"
v0.13.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.13.1,"a function of W, using only the train fold"
v0.13.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.13.1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.13.1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.13.1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.13.1,"Split the data in half, train and test"
v0.13.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.13.1,"a function of x, using only the train fold"
v0.13.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.13.1,Compute coefficient by OLS on residuals
v0.13.1,"Split the data in half, train and test"
v0.13.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.13.1,"a function of x, using only the train fold"
v0.13.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.13.1,Estimate multipliers for second order orthogonal method
v0.13.1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.13.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.13.1,Create local sample set
v0.13.1,compute the base estimate for the current node using double ml or second order double ml
v0.13.1,compute the influence functions here that are used for the criterion
v0.13.1,generate random proposals of dimensions to split
v0.13.1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.13.1,compute criterion for each proposal
v0.13.1,if splitting creates valid leafs in terms of mean leaf size
v0.13.1,Calculate criterion for split
v0.13.1,Else set criterion to infinity so that this split is not chosen
v0.13.1,If no good split was found
v0.13.1,Find split that minimizes criterion
v0.13.1,Set the split attributes at the node
v0.13.1,Create child nodes with corresponding subsamples
v0.13.1,Recursively split children
v0.13.1,Return parent node
v0.13.1,estimate the local parameter at the leaf using the estimate data
v0.13.1,###################
v0.13.1,Argument parsing #
v0.13.1,###################
v0.13.1,#########################################
v0.13.1,Parameters constant across experiments #
v0.13.1,#########################################
v0.13.1,Outcome support
v0.13.1,Treatment support
v0.13.1,Evaluation grid
v0.13.1,Treatment effects array
v0.13.1,Other variables
v0.13.1,##########################
v0.13.1,Data Generating Process #
v0.13.1,##########################
v0.13.1,Log iteration
v0.13.1,"Generate controls, features, treatment and outcome"
v0.13.1,T and Y residuals to be used in later scripts
v0.13.1,Save generated dataset
v0.13.1,#################
v0.13.1,ORF parameters #
v0.13.1,#################
v0.13.1,######################################
v0.13.1,Train and evaluate treatment effect #
v0.13.1,######################################
v0.13.1,########
v0.13.1,Plots #
v0.13.1,########
v0.13.1,###############
v0.13.1,Save results #
v0.13.1,###############
v0.13.1,##############
v0.13.1,Run Rscript #
v0.13.1,##############
v0.13.1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.13.1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.13.1,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.13.1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.13.1,def mlasso_model(): return MultiTaskLassoCV(
v0.13.1,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.13.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.13.1,heterogeneity
v0.13.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.13.1,heterogeneity
v0.13.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.13.1,heterogeneity
v0.13.1,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.13.1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.13.1,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.13.1,subset of features that are exogenous and create heterogeneity
v0.13.1,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.13.1,subset of features wrt we estimate heterogeneity
v0.13.1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.13.1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,introspect the constructor arguments to find the model parameters
v0.13.1,to represent
v0.13.1,"if the argument is deprecated, ignore it"
v0.13.1,Extract and sort argument names excluding 'self'
v0.13.1,column names
v0.13.1,transfer input to numpy arrays
v0.13.1,transfer input to 2d arrays
v0.13.1,create dataframe
v0.13.1,currently dowhy only support single outcome and single treatment
v0.13.1,call dowhy
v0.13.1,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.13.1,cate estimator but not the effect.
v0.13.1,don't proxy special methods
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Check if model is sparse enough for this model
v0.13.1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.13.1,TODO: any way to avoid creating a copy if the array was already dense?
v0.13.1,"the call is necessary if the input was something like a list, though"
v0.13.1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.13.1,so convert to pydata sparse first
v0.13.1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.13.1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.13.1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.13.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.13.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.13.1,For when checking input values is disabled
v0.13.1,Type to column extraction function
v0.13.1,Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
v0.13.1,"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
v0.13.1,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.13.1,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.13.1,Get feature names using featurizer
v0.13.1,All attempts at retrieving transformed feature names have failed
v0.13.1,Delegate handling to downstream logic
v0.13.1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.13.1,same number of input definitions as arrays
v0.13.1,input definitions have same number of dimensions as each array
v0.13.1,all result indices are unique
v0.13.1,all result indices must match at least one input index
v0.13.1,"map indices to all array, axis pairs for that index"
v0.13.1,each index has the same cardinality wherever it appears
v0.13.1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.13.1,Algo: while list contains more than one entry
v0.13.1,take two entries
v0.13.1,sort both lists by intersection of their indices
v0.13.1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.13.1,"take the union of indices and the product of values), stepping through each list linearly"
v0.13.1,TODO: might be faster to break into connected components first
v0.13.1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.13.1,"so compute their content separately, then take cartesian product"
v0.13.1,this would save a few pointless sorts by empty tuples
v0.13.1,TODO: Consider investigating other performance ideas for these cases
v0.13.1,where the dense method beat the sparse method (usually sparse is faster)
v0.13.1,"e,facd,c->cfed"
v0.13.1,sparse: 0.0335489
v0.13.1,dense:  0.011465999999999997
v0.13.1,"gbd,da,egb->da"
v0.13.1,sparse: 0.0791625
v0.13.1,dense:  0.007319099999999995
v0.13.1,"dcc,d,faedb,c->abe"
v0.13.1,sparse: 1.2868097
v0.13.1,dense:  0.44605229999999985
v0.13.1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.13.1,TODO: would using einsum's paths to optimize the order of merging help?
v0.13.1,assume that we should perform nested cross-validation if and only if
v0.13.1,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.13.1,logic copied from check_cv
v0.13.1,otherwise we will assume the user already set the cv attribute to something
v0.13.1,compatible with splitting with a 'groups' argument
v0.13.1,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.13.1,don't use the groups when calling split internally
v0.13.1,Normalize weights
v0.13.1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.13.1,"if we're decorating a class, just update the __init__ method,"
v0.13.1,so that the result is still a class instead of a wrapper method
v0.13.1,"want to enforce that each bad_arg was either in kwargs,"
v0.13.1,or else it was in neither and is just taking its default value
v0.13.1,Any access should throw
v0.13.1,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.13.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.13.1,input feature name is already updated by cate_feature_names.
v0.13.1,define the index of d_x to filter for each given T
v0.13.1,filter X after broadcast with T for each given T
v0.13.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.13.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,
v0.13.1,This code contains some snippets of code from:
v0.13.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
v0.13.1,published under the following license and copyright:
v0.13.1,BSD 3-Clause License
v0.13.1,
v0.13.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.1,All rights reserved.
v0.13.1,make any access to matplotlib or plt throw an exception
v0.13.1,make any access to graphviz or plt throw an exception
v0.13.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.13.1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.13.1,Initialize saturation & value; calculate chroma & value shift
v0.13.1,Calculate some intermediate values
v0.13.1,Initialize RGB with same hue & chroma as our color
v0.13.1,Shift the initial RGB values to match value and store
v0.13.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.13.1,clean way of achieving this
v0.13.1,make sure we don't accidentally escape anything in the substitution
v0.13.1,Fetch appropriate color for node
v0.13.1,"red for negative, green for positive"
v0.13.1,in multi-target use mean of targets
v0.13.1,Write node mean CATE
v0.13.1,Write node std of CATE
v0.13.1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.13.1,Fetch appropriate color for node
v0.13.1,Write node mean CATE
v0.13.1,Write node mean CATE
v0.13.1,Write recommended treatment and value - cost
v0.13.1,Licensed under the MIT License.
v0.13.1,"since inference objects can be stateful, we must copy it before fitting;"
v0.13.1,otherwise this sequence wouldn't work:
v0.13.1,"est1.fit(..., inference=inf)"
v0.13.1,"est2.fit(..., inference=inf)"
v0.13.1,est1.effect_interval(...)
v0.13.1,because inf now stores state from fitting est2
v0.13.1,This flag is true when names are set in a child class instead
v0.13.1,"If names are set in a child class, add an attribute reflecting that"
v0.13.1,This works only if X is passed as a kwarg
v0.13.1,We plan to enforce X as kwarg only in future releases
v0.13.1,This checks if names have been set in a child class
v0.13.1,"If names were set in a child class, don't do it again"
v0.13.1,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.13.1,call the wrapped fit method
v0.13.1,NOTE: we call inference fit *after* calling the main fit method
v0.13.1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.13.1,but tensordot can't be applied to this problem because we don't sum over m
v0.13.1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.13.1,of rows of T was not taken into account
v0.13.1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.13.1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.13.1,"Treatment names is None, default to BaseCateEstimator"
v0.13.1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.13.1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.13.1,Get input names
v0.13.1,Summary
v0.13.1,add statsmodels to parent's options
v0.13.1,add debiasedlasso to parent's options
v0.13.1,add blb to parent's options
v0.13.1,TODO Share some logic with non-discrete version
v0.13.1,Get input names
v0.13.1,Note: we do not transform feature names since that is done within summary_frame
v0.13.1,Summary
v0.13.1,add statsmodels to parent's options
v0.13.1,add statsmodels to parent's options
v0.13.1,add blb to parent's options
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,remove None arguments
v0.13.1,"scores entries should be lists of scores, so make each entry a singleton list"
v0.13.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.13.1,generate an instance of the final model
v0.13.1,generate an instance of the nuisance model
v0.13.1,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.13.1,alteration even when we only want to fit_final.
v0.13.1,use a binary array to get stratified split in case of discrete treatment
v0.13.1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.13.1,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.13.1,"however, sklearn doesn't support both stratifying and grouping (see"
v0.13.1,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.13.1,their own object that supports grouping if they want to use groups.
v0.13.1,for each mc iteration
v0.13.1,for each model under cross fit setting
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,
v0.13.1,This code contains snippets of code from
v0.13.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.13.1,published under the following license and copyright:
v0.13.1,BSD 3-Clause License
v0.13.1,
v0.13.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.1,All rights reserved.
v0.13.1,=============================================================================
v0.13.1,Policy Forest
v0.13.1,=============================================================================
v0.13.1,Remap output
v0.13.1,reshape is necessary to preserve the data contiguity against vs
v0.13.1,"[:, np.newaxis] that does not."
v0.13.1,Get subsample sample size
v0.13.1,Check parameters
v0.13.1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.13.1,if this is the first `fit` call of the warm start mode.
v0.13.1,"Free allocated memory, if any"
v0.13.1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.13.1,We draw from the random state to get the random state we
v0.13.1,would have got if we hadn't used a warm_start.
v0.13.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.13.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.13.1,but would still advance randomness enough so that tree subsamples will be different.
v0.13.1,Parallel loop: we prefer the threading backend as the Cython code
v0.13.1,for fitting the trees is internally releasing the Python GIL
v0.13.1,making threading more efficient than multiprocessing in
v0.13.1,"that case. However, for joblib 0.12+ we respect any"
v0.13.1,"parallel_backend contexts set at a higher level,"
v0.13.1,since correctness does not rely on using threads.
v0.13.1,Collect newly grown trees
v0.13.1,Check data
v0.13.1,Assign chunk of trees to jobs
v0.13.1,avoid storing the output of every estimator by summing them here
v0.13.1,Parallel loop
v0.13.1,Check data
v0.13.1,Assign chunk of trees to jobs
v0.13.1,avoid storing the output of every estimator by summing them here
v0.13.1,Parallel loop
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,
v0.13.1,This code contains snippets of code from:
v0.13.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.13.1,published under the following license and copyright:
v0.13.1,BSD 3-Clause License
v0.13.1,
v0.13.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.1,All rights reserved.
v0.13.1,=============================================================================
v0.13.1,Types and constants
v0.13.1,=============================================================================
v0.13.1,=============================================================================
v0.13.1,Base Policy tree
v0.13.1,=============================================================================
v0.13.1,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.13.1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.13.1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.13.1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.13.1,checks that X is 2D array.
v0.13.1,"since we only allow single dimensional y, we could flatten the prediction"
v0.13.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.13.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.13.1,Handles the corner case when X=None but featurizer might be not None
v0.13.1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.13.1,"Replacing this method which is invalid for this class, so that we make the"
v0.13.1,dosctring empty and not appear in the docs.
v0.13.1,TODO: support freq_weight and sample_var in debiased lasso
v0.13.1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.13.1,Replacing to remove docstring
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"if both X and W are None, just return a column of ones"
v0.13.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.13.1,We need to go back to the label representation of the one-hot so as to call
v0.13.1,the classifier.
v0.13.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.13.1,We need to go back to the label representation of the one-hot so as to call
v0.13.1,the classifier.
v0.13.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.13.1,This works both with our without the weighting trick as the treatments T are unit vector
v0.13.1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.13.1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.13.1,both Parametric and Non Parametric DML.
v0.13.1,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.13.1,attribute so that the trained featurizer will be passed through
v0.13.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.13.1,for internal use by the library
v0.13.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.13.1,We need to use the rlearner's copy to retain the information from fitting
v0.13.1,Handles the corner case when X=None but featurizer might be not None
v0.13.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.13.1,since we clone it and fit separate copies
v0.13.1,override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
v0.13.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.13.1,TODO: support freq_weight and sample_var in debiased lasso
v0.13.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.13.1,since we clone it and fit separate copies
v0.13.1,add blb to parent's options
v0.13.1,override only so that we can update the docstring to indicate
v0.13.1,support for `GenericSingleTreatmentModelFinalInference`
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,note that groups are not passed to score because they are only used for fitting
v0.13.1,note that groups are not passed to score because they are only used for fitting
v0.13.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.13.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.13.1,NOTE: important to get parent's wrapped copy so that
v0.13.1,"after training wrapped featurizer is also trained, etc."
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.13.1,Fit a doubly robust average effect
v0.13.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.13.1,(which needs to have been expanded if there's a discrete treatment)
v0.13.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.13.1,since we clone it and fit separate copies
v0.13.1,"If custom param grid, check that only estimator parameters are being altered"
v0.13.1,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.13.1,override only so that we can update the docstring to indicate support for `blb`
v0.13.1,Get input names
v0.13.1,Summary
v0.13.1,Determine output settings
v0.13.1,"Important: This must be the first invocation of the random state at fit time, so that"
v0.13.1,train/test splits are re-generatable from an external object simply by knowing the
v0.13.1,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.13.1,linear predictions. Currently is also useful for testing.
v0.13.1,reshape is necessary to preserve the data contiguity against vs
v0.13.1,"[:, np.newaxis] that does not."
v0.13.1,Check parameters
v0.13.1,Set min_weight_leaf from min_weight_fraction_leaf
v0.13.1,Build tree
v0.13.1,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.13.1,hold. Used by criterion for memory space savings.
v0.13.1,Initialize the criterion object and the criterion_val object if honest.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,
v0.13.1,This code is a fork from:
v0.13.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
v0.13.1,published under the following license and copyright:
v0.13.1,BSD 3-Clause License
v0.13.1,
v0.13.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.1,All rights reserved.
v0.13.1,Set parameters
v0.13.1,Don't instantiate estimators now! Parameters of base_estimator might
v0.13.1,"still change. Eg., when grid-searching with the nested object syntax."
v0.13.1,self.estimators_ needs to be filled by the derived classes in fit.
v0.13.1,Compute the number of jobs
v0.13.1,Partition estimators between jobs
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,covariance matrix
v0.13.1,get eigen value and eigen vectors
v0.13.1,simulate eigen vectors
v0.13.1,keep the top 4 eigen value and corresponding eigen vector
v0.13.1,replace the negative eigen values
v0.13.1,generate a new covariance matrix
v0.13.1,get linear approximation of eigen values
v0.13.1,coefs
v0.13.1,get the indices of each group of features
v0.13.1,print(ind_same_proxy)
v0.13.1,demo
v0.13.1,same proxy
v0.13.1,residuals
v0.13.1,gmm
v0.13.1,log normal on outliers
v0.13.1,positive outliers
v0.13.1,negative outliers
v0.13.1,demean the new residual again
v0.13.1,generate data
v0.13.1,sample residuals
v0.13.1,get prediction for current investment
v0.13.1,get prediction for current proxy
v0.13.1,get first period prediction
v0.13.1,iterate the step ahead contruction
v0.13.1,prepare new x
v0.13.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.13.1,heterogeneity
v0.13.1,get new covariance matrix
v0.13.1,get coefs
v0.13.1,get residuals
v0.13.1,proxy 1 is the outcome
v0.13.1,make fixed residuals
v0.13.1,Remove children with nonwhite mothers from the treatment group
v0.13.1,Remove children with nonwhite mothers from the treatment group
v0.13.1,Select columns
v0.13.1,Scale the numeric variables
v0.13.1,"Change the binary variable 'first' takes values in {1,2}"
v0.13.1,Append a column of ones as intercept
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.13.1,(which needs to have been expanded if there's a discrete treatment)
v0.13.1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.13.1,d_t=None here since we measure the effect across all Ts
v0.13.1,once the estimator has been fit
v0.13.1,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.13.1,an intercept even when bias is part of coef.
v0.13.1,We can write effect inference as a function of prediction and prediction standard error of
v0.13.1,the final method for linear models
v0.13.1,squeeze the first axis
v0.13.1,d_t=None here since we measure the effect across all Ts
v0.13.1,set the mean_pred_stderr
v0.13.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.13.1,(which needs to have been expanded if there's a discrete treatment)
v0.13.1,"send treatment to the end, pull bounds to the front"
v0.13.1,d_t=None here since we measure the effect across all Ts
v0.13.1,set the mean_pred_stderr
v0.13.1,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.13.1,d_t=None here since we measure the effect across all Ts
v0.13.1,d_t=None here since we measure the effect across all Ts
v0.13.1,need to set the fit args before the estimator is fit
v0.13.1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.13.1,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.13.1,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.13.1,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.13.1,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.13.1,scale preds
v0.13.1,scale std errs
v0.13.1,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.13.1,offset preds
v0.13.1,"offset the distribution, too"
v0.13.1,scale preds
v0.13.1,"scale the distribution, too"
v0.13.1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.13.1,1. Uncertainty of Mean Point Estimate
v0.13.1,2. Distribution of Point Estimate
v0.13.1,3. Total Variance of Point Estimate
v0.13.1,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.13.1,so bail out early; note that it might be possible to correct the algorithm for
v0.13.1,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.13.1,be clean
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,TODO: Add a __dir__ implementation?
v0.13.1,don't proxy special methods
v0.13.1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.13.1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.13.1,then we should be computing a confidence interval for the wrapped calls
v0.13.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.13.1,second level bootstrap which would be prohibitive computationally?
v0.13.1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.13.1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.13.1,can't import from econml.inference at top level without creating cyclical dependencies
v0.13.1,Note that inference results are always methods even if the inference is for a property
v0.13.1,(e.g. coef__inference() is a method but coef_ is a property)
v0.13.1,Therefore we must insert a lambda if getting inference for a non-callable
v0.13.1,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.13.1,"try to get interval/std first if appropriate,"
v0.13.1,since we don't prefer a wrapped method with this name
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,
v0.13.1,This code contains snippets of code from:
v0.13.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.13.1,published under the following license and copyright:
v0.13.1,BSD 3-Clause License
v0.13.1,
v0.13.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.1,All rights reserved.
v0.13.1,=============================================================================
v0.13.1,Types and constants
v0.13.1,=============================================================================
v0.13.1,=============================================================================
v0.13.1,Base GRF tree
v0.13.1,=============================================================================
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,=============================================================================
v0.13.1,A MultOutputWrapper for GRF classes
v0.13.1,=============================================================================
v0.13.1,=============================================================================
v0.13.1,Instantiations of Generalized Random Forest
v0.13.1,=============================================================================
v0.13.1,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.13.1,in front of the constant treatment is the intercept in the moment equation.
v0.13.1,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.13.1,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,
v0.13.1,This code contains snippets of code from
v0.13.1,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.13.1,published under the following license and copyright:
v0.13.1,BSD 3-Clause License
v0.13.1,
v0.13.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.1,All rights reserved.
v0.13.1,=============================================================================
v0.13.1,Base Generalized Random Forest
v0.13.1,=============================================================================
v0.13.1,TODO: support freq_weight and sample_var
v0.13.1,Remap output
v0.13.1,reshape is necessary to preserve the data contiguity against vs
v0.13.1,"[:, np.newaxis] that does not."
v0.13.1,reshape is necessary to preserve the data contiguity against vs
v0.13.1,"[:, np.newaxis] that does not."
v0.13.1,Get subsample sample size
v0.13.1,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.13.1,We calculate the min eigenvalue proxy that each criterion is considering
v0.13.1,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.13.1,Check parameters
v0.13.1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.13.1,if this is the first `fit` call of the warm start mode.
v0.13.1,"Free allocated memory, if any"
v0.13.1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.13.1,We draw from the random state to get the random state we
v0.13.1,would have got if we hadn't used a warm_start.
v0.13.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.13.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.13.1,but would still advance randomness enough so that tree subsamples will be different.
v0.13.1,Generating indices a priori before parallelism ended up being orders of magnitude
v0.13.1,faster than how sklearn does it. The reason is that random samplers do not release the
v0.13.1,gil it seems.
v0.13.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.13.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.13.1,but would still advance randomness enough so that tree subsamples will be different.
v0.13.1,Parallel loop: we prefer the threading backend as the Cython code
v0.13.1,for fitting the trees is internally releasing the Python GIL
v0.13.1,making threading more efficient than multiprocessing in
v0.13.1,"that case. However, for joblib 0.12+ we respect any"
v0.13.1,"parallel_backend contexts set at a higher level,"
v0.13.1,since correctness does not rely on using threads.
v0.13.1,Collect newly grown trees
v0.13.1,Check data
v0.13.1,Assign chunk of trees to jobs
v0.13.1,avoid storing the output of every estimator by summing them here
v0.13.1,Parallel loop
v0.13.1,Check data
v0.13.1,Assign chunk of trees to jobs
v0.13.1,Parallel loop
v0.13.1,Check data
v0.13.1,Assign chunk of trees to jobs
v0.13.1,Parallel loop
v0.13.1,####################
v0.13.1,Variance correction
v0.13.1,####################
v0.13.1,Subtract the average within bag variance. This ends up being equal to the
v0.13.1,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.13.1,The negative part is just sq_between.
v0.13.1,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.13.1,"The off diagonals we have no objective prior, so no correction is applied."
v0.13.1,Finally correcting the pred_cov or pred_var
v0.13.1,avoid storing the output of every estimator by summing them here
v0.13.1,Parallel loop
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,testing importances
v0.13.1,testing heterogeneity importances
v0.13.1,Testing that all parameters do what they are supposed to
v0.13.1,"testing predict, apply and decision path"
v0.13.1,test that the subsampling scheme past to the trees is correct
v0.13.1,The sample size is chosen in particular to test rounding based error when subsampling
v0.13.1,test that the estimator calcualtes var correctly
v0.13.1,test api
v0.13.1,test accuracy
v0.13.1,test the projection functionality of forests
v0.13.1,test that the estimator calcualtes var correctly
v0.13.1,test api
v0.13.1,test that the estimator calcualtes var correctly
v0.13.1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.13.1,test that we raise errors in mishandled situations.
v0.13.1,test that the subsampling scheme past to the trees is correct
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
v0.13.1,omit the lalonde notebook
v0.13.1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.13.1,creating notebooks that are annoying for our users to actually run themselves
v0.13.1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.13.1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.13.1,"prior to calling interpret, can't plot, render, etc."
v0.13.1,can interpret without uncertainty
v0.13.1,can't interpret with uncertainty if inference wasn't used during fit
v0.13.1,can interpret with uncertainty if we refit
v0.13.1,can interpret without uncertainty
v0.13.1,can't treat before interpreting
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,for is_discrete in [False]:
v0.13.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.13.1,ensure we can serialize the unfit estimator
v0.13.1,ensure we can pickle the fit estimator
v0.13.1,make sure we can call the marginal_effect and effect methods
v0.13.1,test const marginal inference
v0.13.1,test effect inference
v0.13.1,test marginal effect inference
v0.13.1,test coef__inference and intercept__inference
v0.13.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.1,"make sure we can call effect with implied scalar treatments,"
v0.13.1,"no matter the dimensions of T, and also that we warn when there"
v0.13.1,are multiple treatments
v0.13.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.1,No heterogeneity
v0.13.1,Define indices to test
v0.13.1,Heterogeneous effects
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,simple DGP only for illustration
v0.13.1,Define the treatment model neural network architecture
v0.13.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.13.1,"so the input shape is (d_z + d_x,)"
v0.13.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.13.1,add extra layers on top for the mixture density network
v0.13.1,Define the response model neural network architecture
v0.13.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.13.1,"so the input shape is (d_t + d_x,)"
v0.13.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.13.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.13.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.13.1,so that the same weights will be reused in each instantiation
v0.13.1,number of samples to use in second estimate of the response
v0.13.1,(to make loss estimate unbiased)
v0.13.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.13.1,do something with predictions...
v0.13.1,also test vector t and y
v0.13.1,simple DGP only for illustration
v0.13.1,Define the treatment model neural network architecture
v0.13.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.13.1,"so the input shape is (d_z + d_x,)"
v0.13.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.13.1,add extra layers on top for the mixture density network
v0.13.1,Define the response model neural network architecture
v0.13.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.13.1,"so the input shape is (d_t + d_x,)"
v0.13.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.13.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.13.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.13.1,so that the same weights will be reused in each instantiation
v0.13.1,number of samples to use in second estimate of the response
v0.13.1,(to make loss estimate unbiased)
v0.13.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.13.1,do something with predictions...
v0.13.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.13.1,test = True ensures we draw test set images
v0.13.1,test = True ensures we draw test set images
v0.13.1,re-draw to get new independent treatment and implied response
v0.13.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.13.1,above is necesary so that reduced form doesn't win
v0.13.1,covariates: time and emotion
v0.13.1,random instrument
v0.13.1,z -> price
v0.13.1,true observable demand function
v0.13.1,errors
v0.13.1,response
v0.13.1,test = True ensures we draw test set images
v0.13.1,test = True ensures we draw test set images
v0.13.1,re-draw to get new independent treatment and implied response
v0.13.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.13.1,above is necesary so that reduced form doesn't win
v0.13.1,covariates: time and emotion
v0.13.1,random instrument
v0.13.1,z -> price
v0.13.1,true observable demand function
v0.13.1,errors
v0.13.1,response
v0.13.1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.13.1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.13.1,For some reason this doesn't work at all when run against the CNTK backend...
v0.13.1,"model.compile('nadam', loss=lambda _,l:l)"
v0.13.1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.13.1,generate a valiation set
v0.13.1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.13.1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,DGP constants
v0.13.1,Generate data
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,testing importances
v0.13.1,testing heterogeneity importances
v0.13.1,Testing that all parameters do what they are supposed to
v0.13.1,"testing predict, apply and decision path"
v0.13.1,initialize parameters
v0.13.1,initialize config wtih base config and overwite some values
v0.13.1,predict tree using config parameters and assert
v0.13.1,shape of trained tree is the same as y_test
v0.13.1,initialize config wtih base honest config and overwite some values
v0.13.1,predict tree using config parameters and assert
v0.13.1,shape of trained tree is the same as y_test
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.13.1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.13.1,so we need to transpose the result
v0.13.1,1-d output
v0.13.1,2-d output
v0.13.1,Single dimensional output y
v0.13.1,compare with weight
v0.13.1,compare with weight
v0.13.1,compare with weight
v0.13.1,compare with weight
v0.13.1,Multi-dimensional output y
v0.13.1,1-d y
v0.13.1,compare when both sample_var and sample_weight exist
v0.13.1,multi-d y
v0.13.1,compare when both sample_var and sample_weight exist
v0.13.1,compare when both sample_var and sample_weight exist
v0.13.1,compare when both sample_var and sample_weight exist
v0.13.1,compare when both sample_var and sample_weight exist
v0.13.1,compare when both sample_var and sample_weight exist
v0.13.1,compare when both sample_var and sample_weight exist
v0.13.1,dgp
v0.13.1,StatsModels2SLS
v0.13.1,IV2SLS
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,test that we can fit with the same arguments as the base estimator
v0.13.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.13.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.1,with the same shape for the lower and upper bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,test that we can do the same thing once we provide percentile bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.13.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.1,with the same shape for the lower and upper bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,test that we can do the same thing once we provide percentile bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,test that we can fit with the same arguments as the base estimator
v0.13.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.13.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.1,with the same shape for the lower and upper bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,test that we can do the same thing once we provide percentile bounds
v0.13.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.13.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.1,with the same shape for the lower and upper bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,test that we can do the same thing once we provide percentile bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,test that we can fit with the same arguments as the base estimator
v0.13.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.13.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.1,with the same shape for the lower and upper bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,test that we can do the same thing once we provide percentile bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.13.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.1,with the same shape for the lower and upper bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,test that we can do the same thing once we provide percentile bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.1,with the same shape for the lower and upper bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,test that the estimated effect is usually within the bounds
v0.13.1,test that we can do the same thing once we provide alpha explicitly
v0.13.1,test that the lower and upper bounds differ
v0.13.1,test that the estimated effect is usually within the bounds
v0.13.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.1,with the same shape for the lower and upper bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,TODO: test that the estimated effect is usually within the bounds
v0.13.1,and that the true effect is also usually within the bounds
v0.13.1,test that we can do the same thing once we provide percentile bounds
v0.13.1,test that the lower and upper bounds differ
v0.13.1,TODO: test that the estimated effect is usually within the bounds
v0.13.1,and that the true effect is also usually within the bounds
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,test that the subsampling scheme past to the trees is correct
v0.13.1,test that the estimator calcualtes var correctly
v0.13.1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.13.1,test that we raise errors in mishandled situations.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,DGP constants
v0.13.1,Generate data
v0.13.1,Test inference results when `cate_feature_names` doesn not exist
v0.13.1,Test inference results when `cate_feature_names` doesn not exist
v0.13.1,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.13.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.13.1,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.13.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.13.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.13.1,pvalue for second column should be greater than zero since some points are on either side
v0.13.1,of the tested value
v0.13.1,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.13.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.13.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.13.1,ensure alpha is passed
v0.13.1,only is not None when T1 is a constant or a list of constant
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.13.1,Test non keyword based calls to fit
v0.13.1,test non-array inputs
v0.13.1,Test custom splitter
v0.13.1,Test incomplete set of test folds
v0.13.1,"y scores should be positive, since W predicts Y somewhat"
v0.13.1,"t scores might not be, since W and T are uncorrelated"
v0.13.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,make sure cross product varies more slowly with first array
v0.13.1,and that vectors are okay as inputs
v0.13.1,number of inputs in specification must match number of inputs
v0.13.1,must have an output
v0.13.1,output indices must be unique
v0.13.1,output indices must be present in an input
v0.13.1,number of indices must match number of dimensions for each input
v0.13.1,repeated indices must always have consistent sizes
v0.13.1,transpose
v0.13.1,tensordot
v0.13.1,trace
v0.13.1,TODO: set up proper flag for this
v0.13.1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.13.1,"of all of the distinct indices that appear in any input,"
v0.13.1,pick a random subset of them (of size at most 5) to appear in the output
v0.13.1,creating an instance should warn
v0.13.1,using the instance should not warn
v0.13.1,using the deprecated method should warn
v0.13.1,don't warn if b and c are passed by keyword
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,make any access to matplotlib or plt throw an exception
v0.13.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.13.1,heterogeneity
v0.13.1,Invert indices to match latest API
v0.13.1,Invert indices to match latest API
v0.13.1,The feature for heterogeneity stays constant
v0.13.1,Auxiliary function for adding xticks and vertical lines when plotting results
v0.13.1,for dynamic dml vs ground truth parameters.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Preprocess data
v0.13.1,Convert 'week' to a date
v0.13.1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.13.1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.13.1,Take log of price
v0.13.1,Make brand numeric
v0.13.1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.13.1,which have all zero coeffs
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,test at least one estimator from each category
v0.13.1,test causal graph
v0.13.1,test refutation estimate
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.13.1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.13.1,TODO: test something rather than just print...
v0.13.1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.13.1,pick some arbitrary X
v0.13.1,pick some arbitrary T
v0.13.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.13.1,The average variance should be lower when using monte carlo iterations
v0.13.1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.13.1,The average variance should be lower when using monte carlo iterations
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,ensure we can serialize unfit estimator
v0.13.1,ensure we can serialize fit estimator
v0.13.1,expected effect size
v0.13.1,test effect
v0.13.1,test inference
v0.13.1,only OrthoIV support inference other than bootstrap
v0.13.1,test summary
v0.13.1,test can run score
v0.13.1,test cate_feature_names
v0.13.1,test can run shap values
v0.13.1,dgp
v0.13.1,no heterogeneity
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.13.1,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.13.1,The __warningregistry__'s need to be in a pristine state for tests
v0.13.1,to work properly.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,parameter combinations to test
v0.13.1,TODO: serializing/deserializing for every combination -- is this necessary?
v0.13.1,ensure we can serialize unfit estimator
v0.13.1,ensure we can serialize fit estimator
v0.13.1,expected effect size
v0.13.1,assert calculated constant marginal effect shape is expected
v0.13.1,const_marginal effect is defined in LinearCateEstimator class
v0.13.1,assert calculated marginal effect shape is expected
v0.13.1,test inference
v0.13.1,test can run score
v0.13.1,test cate_feature_names
v0.13.1,test can run shap values
v0.13.1,"dgp (binary T, binary Z)"
v0.13.1,no heterogeneity
v0.13.1,with heterogeneity
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Set random seed
v0.13.1,Generate data
v0.13.1,DGP constants
v0.13.1,Test data
v0.13.1,Constant treatment effect
v0.13.1,Constant treatment with multi output Y
v0.13.1,Heterogeneous treatment
v0.13.1,Heterogeneous treatment with multi output Y
v0.13.1,TLearner test
v0.13.1,Instantiate TLearner
v0.13.1,Test inputs
v0.13.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.13.1,Instantiate SLearner
v0.13.1,Test inputs
v0.13.1,Test constant treatment effect
v0.13.1,Test constant treatment effect with multi output Y
v0.13.1,Test heterogeneous treatment effect
v0.13.1,Need interactions between T and features
v0.13.1,Test heterogeneous treatment effect with multi output Y
v0.13.1,Instantiate XLearner
v0.13.1,Test inputs
v0.13.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.13.1,Instantiate DomainAdaptationLearner
v0.13.1,Test inputs
v0.13.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.13.1,Get the true treatment effect
v0.13.1,Get the true treatment effect
v0.13.1,Fit learner and get the effect and marginal effect
v0.13.1,Compute treatment effect residuals (absolute)
v0.13.1,Check that at least 90% of predictions are within tolerance interval
v0.13.1,Check whether the output shape is right
v0.13.1,Check that one can pass in regular lists
v0.13.1,Check that it fails correctly if lists of different shape are passed in
v0.13.1,"Check that it works when T, Y have shape (n, 1)"
v0.13.1,Generate covariates
v0.13.1,Generate treatment
v0.13.1,Calculate outcome
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,DGP constants
v0.13.1,Generate data
v0.13.1,Test data
v0.13.1,Remove warnings that might be raised by the models passed into the ORF
v0.13.1,Generate data with continuous treatments
v0.13.1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.13.1,does not work well with parallelism.
v0.13.1,Test inputs for continuous treatments
v0.13.1,--> Check that one can pass in regular lists
v0.13.1,--> Check that it fails correctly if lists of different shape are passed in
v0.13.1,Check that outputs have the correct shape
v0.13.1,Test continuous treatments with controls
v0.13.1,Test continuous treatments without controls
v0.13.1,Generate data with binary treatments
v0.13.1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.13.1,does not work well with parallelism.
v0.13.1,Test inputs for binary treatments
v0.13.1,--> Check that one can pass in regular lists
v0.13.1,--> Check that it fails correctly if lists of different shape are passed in
v0.13.1,"--> Check that it works when T, Y have shape (n, 1)"
v0.13.1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.13.1,--> Check that it fails correctly when the treatments are not numeric
v0.13.1,Check that outputs have the correct shape
v0.13.1,Test binary treatments with controls
v0.13.1,Test binary treatments without controls
v0.13.1,Only applicable to continuous treatments
v0.13.1,Generate data for 2 treatments
v0.13.1,Test multiple treatments with controls
v0.13.1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.13.1,The rest for controls. Just as an example.
v0.13.1,Generating A/B test data
v0.13.1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.13.1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.13.1,Create a wrapper around Lasso that doesn't support weights
v0.13.1,since Lasso does natively support them starting in sklearn 0.23
v0.13.1,Generate data with continuous treatments
v0.13.1,Instantiate model with most of the default parameters
v0.13.1,Compute the treatment effect on test points
v0.13.1,Compute treatment effect residuals
v0.13.1,Multiple treatments
v0.13.1,Allow at most 10% test points to be outside of the tolerance interval
v0.13.1,Compute treatment effect residuals
v0.13.1,Multiple treatments
v0.13.1,Allow at most 20% test points to be outside of the confidence interval
v0.13.1,Check that the intervals are not too wide
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.13.1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.13.1,ensure that we've got at least 6 of every element
v0.13.1,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.13.1,NOTE: this number may need to change if the default number of folds in
v0.13.1,WeightedStratifiedKFold changes
v0.13.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.13.1,ensure we can serialize the unfit estimator
v0.13.1,ensure we can pickle the fit estimator
v0.13.1,make sure we can call the marginal_effect and effect methods
v0.13.1,test const marginal inference
v0.13.1,test effect inference
v0.13.1,test marginal effect inference
v0.13.1,test coef__inference and intercept__inference
v0.13.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.1,"make sure we can call effect with implied scalar treatments,"
v0.13.1,"no matter the dimensions of T, and also that we warn when there"
v0.13.1,are multiple treatments
v0.13.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.1,ensure that we've got at least two of every element
v0.13.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.13.1,make sure we can call the marginal_effect and effect methods
v0.13.1,test const marginal inference
v0.13.1,test effect inference
v0.13.1,test marginal effect inference
v0.13.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.13.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.13.1,We concatenate the two copies data
v0.13.1,make sure we can get out post-fit stuff
v0.13.1,create a simple artificial setup where effect of moving from treatment
v0.13.1,"1 -> 2 is 2,"
v0.13.1,"1 -> 3 is 1, and"
v0.13.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.13.1,"Using an uneven number of examples from different classes,"
v0.13.1,"and having the treatments in non-lexicographic order,"
v0.13.1,Should rule out some basic issues.
v0.13.1,test that we can fit with a KFold instance
v0.13.1,test that we can fit with a train/test iterable
v0.13.1,predetermined splits ensure that all features are seen in each split
v0.13.1,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.13.1,(incorrectly) use a final model with an intercept
v0.13.1,"Because final model is fixed, actual values of T and Y don't matter"
v0.13.1,Ensure reproducibility
v0.13.1,Sparse DGP
v0.13.1,Treatment effect coef
v0.13.1,Other coefs
v0.13.1,Features and controls
v0.13.1,Test sparse estimator
v0.13.1,"--> test coef_, intercept_"
v0.13.1,--> test treatment effects
v0.13.1,Restrict x_test to vectors of norm < 1
v0.13.1,--> check inference
v0.13.1,Check that a majority of true effects lie in the 5-95% CI
v0.13.1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.13.1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.13.1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.13.1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.13.1,sparse test case: heterogeneous effect by product
v0.13.1,need at least as many rows in e_y as there are distinct columns
v0.13.1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.13.1,create a simple artificial setup where effect of moving from treatment
v0.13.1,"a -> b is 2,"
v0.13.1,"a -> c is 1, and"
v0.13.1,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.13.1,"Using an uneven number of examples from different classes,"
v0.13.1,"and having the treatments in non-lexicographic order,"
v0.13.1,should rule out some basic issues.
v0.13.1,Note that explicitly specifying the dtype as object is necessary until
v0.13.1,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.13.1,estimated effects should be identical when treatment is explicitly given
v0.13.1,but const_marginal_effect should be reordered based on the explicit cagetories
v0.13.1,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.13.1,test outer grouping
v0.13.1,test nested grouping
v0.13.1,DML nested CV works via a 'cv' attribute
v0.13.1,"with 2-fold outer and 2-fold inner grouping, and six total groups,"
v0.13.1,should get 1 or 2 groups per split
v0.13.1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.13.1,whichever groups we see
v0.13.1,test nested grouping
v0.13.1,"by default, we use 5 split cross-validation for our T and Y models"
v0.13.1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.13.1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Set random seed
v0.13.1,Generate data
v0.13.1,DGP constants
v0.13.1,Test data
v0.13.1,Constant treatment effect and propensity
v0.13.1,Heterogeneous treatment and propensity
v0.13.1,ensure that we've got at least two of every element
v0.13.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.13.1,ensure that we can serialize unfit estimator
v0.13.1,ensure that we can serialize fit estimator
v0.13.1,make sure we can call the marginal_effect and effect methods
v0.13.1,test const marginal inference
v0.13.1,test effect inference
v0.13.1,test marginal effect inference
v0.13.1,test coef_ and intercept_ inference
v0.13.1,verify we can generate the summary
v0.13.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.13.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.13.1,create a simple artificial setup where effect of moving from treatment
v0.13.1,"1 -> 2 is 2,"
v0.13.1,"1 -> 3 is 1, and"
v0.13.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.13.1,"Using an uneven number of examples from different classes,"
v0.13.1,"and having the treatments in non-lexicographic order,"
v0.13.1,Should rule out some basic issues.
v0.13.1,test that we can fit with a KFold instance
v0.13.1,test that we can fit with a train/test iterable
v0.13.1,"for at least some of the examples, the CI should have nonzero width"
v0.13.1,"for at least some of the examples, the CI should have nonzero width"
v0.13.1,"for at least some of the examples, the CI should have nonzero width"
v0.13.1,test coef__inference function works
v0.13.1,test intercept__inference function works
v0.13.1,test summary function works
v0.13.1,Test inputs
v0.13.1,self._test_inputs(DR_learner)
v0.13.1,Test constant treatment effect
v0.13.1,Test heterogeneous treatment effect
v0.13.1,Test heterogenous treatment effect for W =/= None
v0.13.1,Sparse DGP
v0.13.1,Treatment effect coef
v0.13.1,Other coefs
v0.13.1,Features and controls
v0.13.1,Test sparse estimator
v0.13.1,"--> test coef_, intercept_"
v0.13.1,--> test treatment effects
v0.13.1,Restrict x_test to vectors of norm < 1
v0.13.1,--> check inference
v0.13.1,Check that a majority of true effects lie in the 5-95% CI
v0.13.1,test outer grouping
v0.13.1,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.13.1,test nested grouping
v0.13.1,DML nested CV works via a 'cv' attribute
v0.13.1,"with 2-fold outer and 2-fold inner grouping, and six total groups,"
v0.13.1,should get 1 or 2 groups per split
v0.13.1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.13.1,whichever groups we see
v0.13.1,test nested grouping
v0.13.1,"by default, we use 5 split cross-validation for our T and Y models"
v0.13.1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.13.1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.13.1,helper class
v0.13.1,Fit learner and get the effect
v0.13.1,Get the true treatment effect
v0.13.1,Compute treatment effect residuals (absolute)
v0.13.1,Check that at least 90% of predictions are within tolerance interval
v0.13.1,Only for heterogeneous TE
v0.13.1,Fit learner on X and W and get the effect
v0.13.1,Get the true treatment effect
v0.13.1,Compute treatment effect residuals (absolute)
v0.13.1,Check that at least 90% of predictions are within tolerance interval
v0.13.1,Check that one can pass in regular lists
v0.13.1,Check that it fails correctly if lists of different shape are passed in
v0.13.1,Check that it fails when T contains values other than 0 and 1
v0.13.1,"Check that it works when T, Y have shape (n, 1)"
v0.13.1,Generate covariates
v0.13.1,Generate treatment
v0.13.1,Calculate outcome
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,DGP constants
v0.13.1,DGP coefficients
v0.13.1,Generated outcomes
v0.13.1,################
v0.13.1,WeightedLasso #
v0.13.1,################
v0.13.1,Define weights
v0.13.1,Define extended datasets
v0.13.1,Range of alphas
v0.13.1,Compare with Lasso
v0.13.1,--> No intercept
v0.13.1,--> With intercept
v0.13.1,When DGP has no intercept
v0.13.1,When DGP has intercept
v0.13.1,--> Coerce coefficients to be positive
v0.13.1,--> Toggle max_iter & tol
v0.13.1,Define weights
v0.13.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.13.1,Mixed DGP scenario.
v0.13.1,Define extended datasets
v0.13.1,Define weights
v0.13.1,Define multioutput
v0.13.1,##################
v0.13.1,WeightedLassoCV #
v0.13.1,##################
v0.13.1,Define alphas to test
v0.13.1,Compare with LassoCV
v0.13.1,--> No intercept
v0.13.1,--> With intercept
v0.13.1,--> Force parameters to be positive
v0.13.1,Choose a smaller n to speed-up process
v0.13.1,Compare fold weights
v0.13.1,Define weights
v0.13.1,Define extended datasets
v0.13.1,Define splitters
v0.13.1,WeightedKFold splitter
v0.13.1,Map weighted splitter to an extended splitter
v0.13.1,Define alphas to test
v0.13.1,Compare with LassoCV
v0.13.1,--> No intercept
v0.13.1,--> With intercept
v0.13.1,--> Force parameters to be positive
v0.13.1,###########################
v0.13.1,MultiTaskWeightedLassoCV #
v0.13.1,###########################
v0.13.1,Define alphas to test
v0.13.1,Define splitter
v0.13.1,Compare with MultiTaskLassoCV
v0.13.1,--> No intercept
v0.13.1,--> With intercept
v0.13.1,Define weights
v0.13.1,Define extended datasets
v0.13.1,Define splitters
v0.13.1,WeightedKFold splitter
v0.13.1,Map weighted splitter to an extended splitter
v0.13.1,Define alphas to test
v0.13.1,Compare with LassoCV
v0.13.1,--> No intercept
v0.13.1,--> With intercept
v0.13.1,#########################
v0.13.1,WeightedLassoCVWrapper #
v0.13.1,#########################
v0.13.1,perform 1D fit
v0.13.1,perform 2D fit
v0.13.1,################
v0.13.1,DebiasedLasso #
v0.13.1,################
v0.13.1,Test DebiasedLasso without weights
v0.13.1,--> Check debiased coeffcients without intercept
v0.13.1,--> Check debiased coeffcients with intercept
v0.13.1,--> Check 5-95 CI coverage for unit vectors
v0.13.1,Test DebiasedLasso with weights for one DGP
v0.13.1,Define weights
v0.13.1,Define extended datasets
v0.13.1,--> Check debiased coefficients
v0.13.1,Define weights
v0.13.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.13.1,--> Check debiased coeffcients
v0.13.1,Test that attributes propagate correctly
v0.13.1,Test MultiOutputDebiasedLasso without weights
v0.13.1,--> Check debiased coeffcients without intercept
v0.13.1,--> Check debiased coeffcients with intercept
v0.13.1,--> Check CI coverage
v0.13.1,Test MultiOutputDebiasedLasso with weights
v0.13.1,Define weights
v0.13.1,Define extended datasets
v0.13.1,--> Check debiased coefficients
v0.13.1,Unit vectors
v0.13.1,Unit vectors
v0.13.1,Check coeffcients and intercept are the same within tolerance
v0.13.1,Check results are similar with tolerance 1e-6
v0.13.1,Check if multitask
v0.13.1,Check that same alpha is chosen
v0.13.1,Check that the coefficients are similar
v0.13.1,selective ridge has a simple implementation that we can test against
v0.13.1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.13.1,"it should be the case that when we set fit_intercept to true,"
v0.13.1,it doesn't matter whether the penalized model also fits an intercept or not
v0.13.1,create an extra copy of rows with weight 2
v0.13.1,"instead of a slice, explicitly return an array of indices"
v0.13.1,_penalized_inds is only set during fitting
v0.13.1,cv exists on penalized model
v0.13.1,now we can access _penalized_inds
v0.13.1,check that we can read the cv attribute back out from the underlying model
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"global and cohort data should have exactly the same structure, but different values"
v0.13.1,local index should have as many times entries as global as there were rows passed in
v0.13.1,continuous treatments have typical treatment values equal to
v0.13.1,the mean of the absolute value of non-zero entries
v0.13.1,discrete treatments have typical treatment value 1
v0.13.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.13.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.13.1,policy value should exceed always treating with any treatment
v0.13.1,"global shape is (d_y, sum(d_t))"
v0.13.1,global and cohort row-wise dicts have d_y * d_t entries
v0.13.1,local dictionary is flattened to n_rows * d_y * d_t
v0.13.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.1,features; for categoricals they should appear #cats-1 times each
v0.13.1,"global and cohort data should have exactly the same structure, but different values"
v0.13.1,local index should have as many times entries as global as there were rows passed in
v0.13.1,features; for categoricals they should appear #cats-1 times each
v0.13.1,"global shape is (d_y, sum(d_t))"
v0.13.1,global and cohort row-wise dicts have d_y * d_t entries
v0.13.1,local dictionary is flattened to n_rows * d_y * d_t
v0.13.1,continuous treatments have typical treatment values equal to
v0.13.1,the mean of the absolute value of non-zero entries
v0.13.1,discrete treatments have typical treatment value 1
v0.13.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.13.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.13.1,policy value should exceed always treating with any treatment
v0.13.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.1,"global and cohort data should have exactly the same structure, but different values"
v0.13.1,local index should have as many times entries as global as there were rows passed in
v0.13.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.13.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.13.1,policy value should exceed always treating with any treatment
v0.13.1,"global shape is (d_y, sum(d_t))"
v0.13.1,global and cohort row-wise dicts have d_y * d_t entries
v0.13.1,local dictionary is flattened to n_rows * d_y * d_t
v0.13.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.1,features; for categoricals they should appear #cats-1 times each
v0.13.1,make sure we don't run into problems dropping every index
v0.13.1,"global and cohort data should have exactly the same structure, but different values"
v0.13.1,local index should have as many times entries as global as there were rows passed in
v0.13.1,"global shape is (d_y, sum(d_t))"
v0.13.1,global and cohort row-wise dicts have d_y * d_t entries
v0.13.1,local dictionary is flattened to n_rows * d_y * d_t
v0.13.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.13.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.13.1,policy value should exceed always treating with any treatment
v0.13.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.1,"global and cohort data should have exactly the same structure, but different values"
v0.13.1,local index should have as many times entries as global as there were rows passed in
v0.13.1,features; for categoricals they should appear #cats-1 times each
v0.13.1,"global shape is (d_y, sum(d_t))"
v0.13.1,global and cohort row-wise dicts have d_y * d_t entries
v0.13.1,local dictionary is flattened to n_rows * d_y * d_t
v0.13.1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.13.1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.13.1,policy value should exceed always treating with any treatment
v0.13.1,dgp
v0.13.1,model
v0.13.1,model
v0.13.1,"columns 'd', 'e', 'h' have too many values"
v0.13.1,"columns 'd', 'e' have too many values"
v0.13.1,lowering bound shouldn't affect already fit columns when warm starting
v0.13.1,"column d is now okay, too"
v0.13.1,verify that we can use a scalar treatment cost
v0.13.1,verify that we can specify per-treatment costs for each sample
v0.13.1,verify that using the same state returns the same results each time
v0.13.1,set the categories for column 'd' explicitly so that b is default
v0.13.1,"first column: 10 ones, this is fine"
v0.13.1,"second column: 6 categories, plenty of random instances of each"
v0.13.1,this is fine only if we increase the cateogry limit
v0.13.1,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.13.1,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.13.1,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.13.1,forest heterogeneity won't work
v0.13.1,"sixth column: just 1 one, not enough even without check"
v0.13.1,increase bound on cat expansion
v0.13.1,skip checks (reducing folds accordingly)
v0.13.1,"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
v0.13.1,"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
v0.13.1,Pass an example where W is irrelevant and X is confounder
v0.13.1,"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
v0.13.1,zeroed out and the test will fail
v0.13.1,"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
v0.13.1,rescaling X shouldn't affect the first stage models because they normalize the inputs
v0.13.1,"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
v0.13.1,cross terms
v0.13.1,scale by 1000 to match the input to this model:
v0.13.1,"the scale of X does matter for the final model, which keeps results in user-denominated units"
v0.13.1,rescaling X still shouldn't affect the first stage models
v0.13.1,TODO: we don't recover the correct values with enough accuracy to enable this assertion
v0.13.1,is there a different way to verify that we are learning the correct coefficients?
v0.13.1,"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,DGP constants
v0.13.1,Define data features
v0.13.1,Added `_df`to names to be different from the default cate_estimator names
v0.13.1,Generate data
v0.13.1,################################
v0.13.1,Single treatment and outcome #
v0.13.1,################################
v0.13.1,Test LinearDML
v0.13.1,|--> Test featurizers
v0.13.1,"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
v0.13.1,|--> Test re-fit
v0.13.1,Test SparseLinearDML
v0.13.1,Test ForestDML
v0.13.1,###################################
v0.13.1,Mutiple treatments and outcomes #
v0.13.1,###################################
v0.13.1,Test LinearDML
v0.13.1,Test SparseLinearDML
v0.13.1,"Single outcome only, ORF does not support multiple outcomes"
v0.13.1,Test DMLOrthoForest
v0.13.1,Test DROrthoForest
v0.13.1,Test XLearner
v0.13.1,Skipping population summary names test because bootstrap inference is too slow
v0.13.1,Test SLearner
v0.13.1,Test TLearner
v0.13.1,Test LinearDRLearner
v0.13.1,Test SparseLinearDRLearner
v0.13.1,Test ForestDRLearner
v0.13.1,Test LinearIntentToTreatDRIV
v0.13.1,Test DeepIV
v0.13.1,Test categorical treatments
v0.13.1,Check refit
v0.13.1,Check refit after setting categories
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Linear models are required for parametric dml
v0.13.1,sample weighting models are required for nonparametric dml
v0.13.1,Test values
v0.13.1,TLearner test
v0.13.1,Instantiate TLearner
v0.13.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.13.1,Test constant treatment effect with multi output Y
v0.13.1,Test heterogeneous treatment effect
v0.13.1,Need interactions between T and features
v0.13.1,Test heterogeneous treatment effect with multi output Y
v0.13.1,Instantiate DomainAdaptationLearner
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,test base values equals to mean of constant marginal effect
v0.13.1,test shape of shap values output is as expected
v0.13.1,test shape of attribute of explanation object is as expected
v0.13.1,test length of feature names equals to shap values shape
v0.13.1,test base values equals to mean of constant marginal effect
v0.13.1,test shape of shap values output is as expected
v0.13.1,test shape of attribute of explanation object is as expected
v0.13.1,test length of feature names equals to shap values shape
v0.13.1,Treatment effect function
v0.13.1,Outcome support
v0.13.1,Treatment support
v0.13.1,"Generate controls, covariates, treatments and outcomes"
v0.13.1,Heterogeneous treatment effects
v0.13.1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.13.1,through shap package.
v0.13.1,test shap could generate the plot from the shap_values
v0.13.1,"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Check inputs
v0.13.1,Check inputs
v0.13.1,Check inputs
v0.13.1,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.13.1,"Thus, we append the controls column before the one-hot-encoded T"
v0.13.1,"We might want to revisit, though, since it's linearly determined by the others"
v0.13.1,Check inputs
v0.13.1,Check inputs
v0.13.1,Estimate response function
v0.13.1,Check inputs
v0.13.1,Train model on controls. Assign higher weight to units resembling
v0.13.1,treated units.
v0.13.1,Train model on the treated. Assign higher weight to units resembling
v0.13.1,control units.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,TODO: make sure to use random seeds wherever necessary
v0.13.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.13.1,"unfortunately with the Theano and Tensorflow backends,"
v0.13.1,the straightforward use of K.stop_gradient can cause an error
v0.13.1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.13.1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.13.1,so that those layers remain connected but with 0 gradient
v0.13.1,|| t - mu_i || ^2
v0.13.1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.13.1,Use logsumexp for numeric stability:
v0.13.1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.13.1,TODO: does the numeric stability actually make any difference?
v0.13.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.13.1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.13.1,generate cumulative sum via matrix multiplication
v0.13.1,"Generate standard uniform values in shape (batch_size,1)"
v0.13.1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.13.1,we use uniform_like instead with an input of an appropriate shape)
v0.13.1,convert to floats and multiply to perform equivalent of logical AND
v0.13.1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.13.1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.13.1,we use normal_like instead with an input of an appropriate shape)
v0.13.1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.13.1,prevent gradient from passing through sampling
v0.13.1,three options: biased or upper-bound loss require a single number of samples;
v0.13.1,unbiased can take different numbers for the network and its gradient
v0.13.1,"sample: (() -> Layer, int) -> Layer"
v0.13.1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.13.1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.13.1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.13.1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.13.1,the dimensionality of the output of the network
v0.13.1,TODO: is there a more robust way to do this?
v0.13.1,TODO: do we need to give the user more control over other arguments to fit?
v0.13.1,"subtle point: we need to build a new model each time,"
v0.13.1,because each model encapsulates its randomness
v0.13.1,TODO: do we need to give the user more control over other arguments to fit?
v0.13.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.13.1,not a general tensor (because of how backprop works in every framework)
v0.13.1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.13.1,but this seems annoying...)
v0.13.1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.13.1,TODO: any way to get this to work on batches of arbitrary size?
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.13.1,"fit on projected Z: E[T * E[T|X,Z]|X]"
v0.13.1,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.13.1,"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
v0.13.1,"shape (n,)"
v0.13.1,"shape (n,)"
v0.13.1,"shape(n,)"
v0.13.1,TODO: prel_model_effect could allow sample_var and freq_weight?
v0.13.1,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.13.1,"we need to undo the one-hot encoding for calling effect,"
v0.13.1,since it expects raw values
v0.13.1,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.13.1,target will be discrete and will be inversed from FirstStageWrapper
v0.13.1,"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
v0.13.1,reshape the predictions
v0.13.1,concat W and Z
v0.13.1,check nuisances outcome shape
v0.13.1,Y_res could be a vector or 1-dimensional 2d-array
v0.13.1,"all could be reshaped to vector since Y, T, Z are all single dimensional."
v0.13.1,Estimate final model of theta(X) by minimizing the square loss:
v0.13.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.13.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.13.1,at the expense of some small bias. For points with very small covariance we revert
v0.13.1,to the model-based preliminary estimate and do not add the correction term.
v0.13.1,A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
v0.13.1,Used by both DRIV and IntentToTreatDRIV.
v0.13.1,Maggie: I think that would be the case?
v0.13.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.13.1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.13.1,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.13.1,attribute so that the trained featurizer will be passed through
v0.13.1,Handles the corner case when X=None but featurizer might be not None
v0.13.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.13.1,this is a regression model since proj_t is probability
v0.13.1,outcome is continuous since proj_t is probability
v0.13.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.13.1,TODO: support freq_weight and sample_var in debiased lasso
v0.13.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.13.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.13.1,concat W and Z
v0.13.1,"we need to undo the one-hot encoding for calling effect,"
v0.13.1,since it expects raw values
v0.13.1,concat W and Z
v0.13.1,"we need to undo the one-hot encoding for calling effect,"
v0.13.1,since it expects raw values
v0.13.1,reshape the predictions
v0.13.1,"T_res, Z_res, beta expect shape to be (n,1)"
v0.13.1,maybe shouldn't expose fit_cate_intercept in this class?
v0.13.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.13.1,TODO: do correct adjustment for sample_var
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,concat W and Z
v0.13.1,concat W and Z
v0.13.1,concat W and Z
v0.13.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.13.1,"train E[T|X,W,Z]"
v0.13.1,"train [Z|X,W]"
v0.13.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.13.1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.13.1,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.13.1,attribute so that the trained featurizer will be passed through
v0.13.1,Handles the corner case when X=None but featurizer might be not None
v0.13.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.13.1,concat W and Z
v0.13.1,note that groups are not passed to score because they are only used for fitting
v0.13.1,concat W and Z
v0.13.1,note that sample_weight and groups are not passed to predict because they are only used for fitting
v0.13.1,concat W and Z
v0.13.1,A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
v0.13.1,Used by both Parametric and Non Parametric DMLIV.
v0.13.1,override only so that we can enforce Z to be required
v0.13.1,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.13.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.13.1,for internal use by the library
v0.13.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.13.1,Handles the corner case when X=None but featurizer might be not None
v0.13.1,Get input names
v0.13.1,Summary
v0.13.1,coefficient
v0.13.1,intercept
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,"this will have dimension (d,) + shape(X)"
v0.13.1,send the first dimension to the end
v0.13.1,columns are featurized independently; partial derivatives are only non-zero
v0.13.1,when taken with respect to the same column each time
v0.13.1,don't fit intercept; manually add column of ones to the data instead;
v0.13.1,this allows us to ignore the intercept when computing marginal effects
v0.13.1,make T 2D if if was a vector
v0.13.1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.13.1,two stage approximation
v0.13.1,"first, get basis expansions of T, X, and Z"
v0.13.1,TODO: is it right that the effective number of intruments is the
v0.13.1,"product of ft_X and ft_Z, not just ft_Z?"
v0.13.1,"regress T expansion on X,Z expansions concatenated with W"
v0.13.1,"predict ft_T from interacted ft_X, ft_Z"
v0.13.1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.13.1,dT may be only 2-dimensional)
v0.13.1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.13.1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,TODO: this utility is documented but internal; reimplement?
v0.13.1,TODO: this utility is even less public...
v0.13.1,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.13.1,use same Cs as would be used by default by LogisticRegressionCV
v0.13.1,NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
v0.13.1,which could affect how many times each distinct Y value needs to be present in the data
v0.13.1,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.13.1,but also supports get_feature_names with expected signature
v0.13.1,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.13.1,NOTE: we rely on the passthrough columns coming first in the concatenated X;W
v0.13.1,"when we pipeline scaling with our first stage models later, so the order here is important"
v0.13.1,Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
v0.13.1,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.13.1,Convert SingleTreeInterpreter to a python dictionary
v0.13.1,named tuple type for storing results inside CausalAnalysis class;
v0.13.1,must be lifted to module level to enable pickling
v0.13.1,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
v0.13.1,"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
v0.13.1,"when running the first stage models, but don't want to scale the X columns when running the final model,"
v0.13.1,since then our coefficients will have odd units and our trees will also have decisions using those units.
v0.13.1,
v0.13.1,"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
v0.13.1,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.13.1,Controls are all other columns of X
v0.13.1,"can't use X[:, feat_ind] when X is a DataFrame"
v0.13.1,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.13.1,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.13.1,so that the user can opt-in to allowing unseen treatment values
v0.13.1,(and return NaN or something in that case)
v0.13.1,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
v0.13.1,and so we can just peel the first columns off of that combined array for rescaling in the pipeline
v0.13.1,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
v0.13.1,"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
v0.13.1,becomes a valid approach to handling this
v0.13.1,array checking routines don't accept 0-width arrays
v0.13.1,perform model selection
v0.13.1,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.13.1,convert to NormalInferenceResults for consistency
v0.13.1,Set the dictionary values shared between local and global summaries
v0.13.1,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.13.1,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.13.1,required to fit a discrete DML model
v0.13.1,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
v0.13.1,sub-cases of models or also integrate with azure autoML. (post-MVP)
v0.13.1,"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
v0.13.1,"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
v0.13.1,TODO: Enable multi-class classification (post-MVP)
v0.13.1,Validate inputs
v0.13.1,TODO: check compatibility of X and Y lengths
v0.13.1,"no previous fit, cancel warm start"
v0.13.1,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.13.1,"if heterogeneity_inds is 1D, repeat it"
v0.13.1,heterogeneity inds should be a 2D list of length same as train_inds
v0.13.1,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.13.1,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.13.1,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.13.1,train the Y model
v0.13.1,"perform model selection for the Y model using all X, not on a per-column basis"
v0.13.1,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.13.1,work with the regression wrapper
v0.13.1,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.13.1,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.13.1,since otherwise we'll have too many columns to be able to train a classifier
v0.13.1,start with empty results and default shared insights
v0.13.1,convert categorical indicators to numeric indices
v0.13.1,check for indices over the categorical expansion bound
v0.13.1,assume we'll be able to train former failures this time; we'll add them back if not
v0.13.1,"can't remove in place while iterating over new_inds, so store in separate list"
v0.13.1,"train the model, but warn"
v0.13.1,no model can be trained in this case since we need more folds
v0.13.1,"don't train a model, but suggest workaround since there are enough instances of least"
v0.13.1,populated class
v0.13.1,also remove from train_inds so we don't try to access the result later
v0.13.1,extract subset of names matching new columns
v0.13.1,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.13.1,don't want to cache this failed result
v0.13.1,properties to return from effect InferenceResults
v0.13.1,properties to return from PopulationSummaryResults
v0.13.1,Converts strings to property lookups or method calls as a convenience so that the
v0.13.1,_point_props and _summary_props above can be applied to an inference object
v0.13.1,Create a summary combining all results into a single output; this is used
v0.13.1,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.13.1,"or a dictionary, respectively, based on the summary function passed into this method"
v0.13.1,"ensure array has shape (m,y,t)"
v0.13.1,population summary is missing sample dimension; add it for consistency
v0.13.1,outcome dimension is missing; add it for consistency
v0.13.1,add singleton treatment dimension if missing
v0.13.1,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.13.1,"each attr has dimension (m,y) or (m,y,t)"
v0.13.1,concatenate along treatment dimension
v0.13.1,"for dictionary representation, want to remove unneeded sample dimension"
v0.13.1,in cohort and global results
v0.13.1,TODO: enrich outcome logic for multi-class classification when that is supported
v0.13.1,There is no actual sample level in this data
v0.13.1,can't drop only level
v0.13.1,should be serialization-ready and contain no numpy arrays
v0.13.1,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.13.1,TODO: Note that there's no column metadata for the sample number - should there be?
v0.13.1,"need to replicate the column info for each sample, then remove from the shared data"
v0.13.1,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.13.1,which may need to be revisited once we support multiclass
v0.13.1,get the length of the list corresponding to the first dictionary key
v0.13.1,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.13.1,a global inference indicates the effect of that one feature on the outcome
v0.13.1,need to reshape the output to match the input
v0.13.1,we want to offset the inference object by the baseline estimate of y
v0.13.1,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.13.1,get the length of the list corresponding to the first dictionary key
v0.13.1,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.13.1,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.13.1,because then scaling the difference between treatment value and treatment costs is the
v0.13.1,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.13.1,
v0.13.1,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.13.1,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.13.1,(rather than just not treating at all)
v0.13.1,
v0.13.1,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.13.1,2 * policy_value - always_treat
v0.13.1,includes exactly their contribution because policy_value and always_treat both include it
v0.13.1,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.13.1,2 * policy_value - always-treat
v0.13.1,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.13.1,is zero and the contribution to always_treat is negative
v0.13.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.13.1,"however, the tree can't store the feature and treatment names we compute here..."
v0.13.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.13.1,"however, the tree can't store the feature and treatment names we compute here..."
v0.13.1,get dataframe with all but selected column
v0.13.1,apply 10% of a typical treatment for this feature
v0.13.1,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.13.1,set the effect bounds; for positive treatments these agree with
v0.13.1,"the estimates; for negative treatments, we need to invert the interval"
v0.13.1,the effect is now always positive since we decrease treatment when negative
v0.13.1,"for discrete treatment, stack a zero result in front for control"
v0.13.1,we need to call effect_inference to get the correct CI between the two treatment options
v0.13.1,we now need to construct the delta in the cost between the two treatments and translate the effect
v0.13.1,remove third dimenions potentially added
v0.13.1,"find cost of current treatment: equality creates a 2d array with True on each row,"
v0.13.1,only if its the location of the current treatment. Then we take the corresponding cost.
v0.13.1,construct index of current treatment
v0.13.1,add second dimension if needed for broadcasting during translation of effect
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,TODO: conisder working around relying on sklearn implementation details
v0.13.1,"Found a good split, return."
v0.13.1,Record all splits in case the stratification by weight yeilds a worse partition
v0.13.1,Reseed random generator and try again
v0.13.1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.13.1,"Found a good split, return."
v0.13.1,Did not find a good split
v0.13.1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.13.1,Return most weight-balanced partition
v0.13.1,Weight stratification algorithm
v0.13.1,Sort weights for weight strata search
v0.13.1,There are some leftover indices that have yet to be assigned
v0.13.1,Append stratum splits to overall splits
v0.13.1,"If classification methods produce multiple columns of output,"
v0.13.1,we need to manually encode classes to ensure consistent column ordering.
v0.13.1,We clone the estimator to make sure that all the folds are
v0.13.1,"independent, and that it is pickle-able."
v0.13.1,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.13.1,`predictions` is a list of method outputs from each fold.
v0.13.1,"If each of those is also a list, then treat this as a"
v0.13.1,multioutput-multiclass task. We need to separately concatenate
v0.13.1,the method outputs for each label into an `n_labels` long list.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Our classes that derive from sklearn ones sometimes include
v0.13.1,inherited docstrings that have embedded doctests; we need the following imports
v0.13.1,so that they don't break.
v0.13.1,TODO: consider working around relying on sklearn implementation details
v0.13.1,"TODO: once we drop support for sklearn < 1.0, we can remove this"
v0.13.1,"if we're decorating a class, just update the __init__ method,"
v0.13.1,so that the result is still a class instead of a wrapper method
v0.13.1,normalize was deprecated or removed; don't need to do anything
v0.13.1,"Convert X, y into numpy arrays"
v0.13.1,Define fit parameters
v0.13.1,Some algorithms don't have a check_input option
v0.13.1,Check weights array
v0.13.1,Check that weights are size-compatible
v0.13.1,Normalize inputs
v0.13.1,Weight inputs
v0.13.1,Fit base class without intercept
v0.13.1,Fit Lasso
v0.13.1,Reset intercept
v0.13.1,The intercept is not calculated properly due the sqrt(weights) factor
v0.13.1,so it must be recomputed
v0.13.1,Fit lasso without weights
v0.13.1,Make weighted splitter
v0.13.1,Fit weighted model
v0.13.1,Make weighted splitter
v0.13.1,Fit weighted model
v0.13.1,Call weighted lasso on reduced design matrix
v0.13.1,Weighted tau
v0.13.1,Select optimal penalty
v0.13.1,Warn about consistency
v0.13.1,"Convert X, y into numpy arrays"
v0.13.1,Fit weighted lasso with user input
v0.13.1,"Center X, y"
v0.13.1,Calculate quantities that will be used later on. Account for centered data
v0.13.1,Calculate coefficient and error variance
v0.13.1,Add coefficient correction
v0.13.1,Set coefficients and intercept standard errors
v0.13.1,Set intercept
v0.13.1,Return alpha to 'auto' state
v0.13.1,"Note that in the case of no intercept, X_offset is 0"
v0.13.1,Calculate the variance of the predictions
v0.13.1,Calculate prediction confidence intervals
v0.13.1,Assumes flattened y
v0.13.1,Compute weighted residuals
v0.13.1,To be done once per target. Assumes y can be flattened.
v0.13.1,Assumes that X has already been offset
v0.13.1,Special case: n_features=1
v0.13.1,Compute Lasso coefficients for the columns of the design matrix
v0.13.1,Compute C_hat
v0.13.1,Compute theta_hat
v0.13.1,Allow for single output as well
v0.13.1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.13.1,Set coef_ attribute
v0.13.1,Set intercept_ attribute
v0.13.1,Set selected_alpha_ attribute
v0.13.1,Set coef_stderr_
v0.13.1,intercept_stderr_
v0.13.1,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.13.1,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.13.1,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.13.1,set intercept_ attribute
v0.13.1,set coef_ attribute
v0.13.1,set alpha_ attribute
v0.13.1,set alphas_ attribute
v0.13.1,set n_iter_ attribute
v0.13.1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.13.1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.13.1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.13.1,now regress X1 on y - X2 * beta2 to learn beta1
v0.13.1,set coef_ and intercept_ attributes
v0.13.1,Note that the penalized model should *not* have an intercept
v0.13.1,don't proxy special methods
v0.13.1,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.13.1,regressor incorrectly
v0.13.1,"Note: for known attributes that have been set this method will not be called,"
v0.13.1,so we should just throw here because this is an attribute belonging to this class
v0.13.1,but which hasn't yet been set on this instance
v0.13.1,set default values for None
v0.13.1,check freq_weight should be integer and should be accompanied by sample_var
v0.13.1,check array shape
v0.13.1,weight X and y and sample_var
v0.13.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.13.1,set default values for None
v0.13.1,check array shape
v0.13.1,check dimension of instruments is more than dimension of treatments
v0.13.1,weight X and y
v0.13.1,learn point estimate
v0.13.1,solve first stage linear regression E[T|Z]
v0.13.1,"""that"" means T̂"
v0.13.1,solve second stage linear regression E[Y|that]
v0.13.1,(T̂.T*T̂)^{-1}
v0.13.1,learn cov(theta)
v0.13.1,(T̂.T*T̂)^{-1}
v0.13.1,sigma^2
v0.13.1,reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,AzureML
v0.13.1,helper imports
v0.13.1,write the details of the workspace to a configuration file to the notebook library
v0.13.1,if y is a multioutput model
v0.13.1,Make sure second dimension has 1 or more item
v0.13.1,switch _inner Model to a MultiOutputRegressor
v0.13.1,flatten array as automl only takes vectors for y
v0.13.1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.13.1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.13.1,as an sklearn estimator
v0.13.1,fit implementation for a single output model.
v0.13.1,Create experiment for specified workspace
v0.13.1,Configure automl_config with training set information.
v0.13.1,"Wait for remote run to complete, the set the model"
v0.13.1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.13.1,create model and pass model into final.
v0.13.1,"If item is an automl config, get its corresponding"
v0.13.1,AutomatedML Model and add it to new_Args
v0.13.1,"If item is an automl config, get its corresponding"
v0.13.1,AutomatedML Model and set it for this key in
v0.13.1,kwargs
v0.13.1,takes in either automated_ml config and instantiates
v0.13.1,an AutomatedMLModel
v0.13.1,The prefix can only be 18 characters long
v0.13.1,"because prefixes come from kwarg_names, we must ensure they are"
v0.13.1,short enough.
v0.13.1,Get workspace from config file.
v0.13.1,Take the intersect of the white for sample
v0.13.1,weights and linear models
v0.13.1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,average the outcome dimension if it exists and ensure 2d y_pred
v0.13.1,get index of best treatment
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,TODO: consider working around relying on sklearn implementation details
v0.13.1,Create splits of causal tree
v0.13.1,Make sure the correct exception is being rethrown
v0.13.1,Must make sure indices are merged correctly
v0.13.1,Convert rows to columns
v0.13.1,Require group assignment t to be one-hot-encoded
v0.13.1,Get predictions for the 2 splits
v0.13.1,Must make sure indices are merged correctly
v0.13.1,Crossfitting
v0.13.1,Compute weighted nuisance estimates
v0.13.1,-------------------------------------------------------------------------------
v0.13.1,Calculate the covariance matrix corresponding to the BLB inference
v0.13.1,
v0.13.1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.13.1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.13.1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.13.1,in that slice from the overall parameter estimate.
v0.13.1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.13.1,-------------------------------------------------------------------------------
v0.13.1,Calclulate covariance matrix through BLB
v0.13.1,Estimators
v0.13.1,OrthoForest parameters
v0.13.1,Sub-forests
v0.13.1,Auxiliary attributes
v0.13.1,Fit check
v0.13.1,TODO: Check performance
v0.13.1,Must normalize weights
v0.13.1,Override the CATE inference options
v0.13.1,Add blb inference to parent's options
v0.13.1,Generate subsample indices
v0.13.1,Build trees in parallel
v0.13.1,Bootstraping has repetitions in tree sample
v0.13.1,Similar for `a` weights
v0.13.1,Bootstraping has repetitions in tree sample
v0.13.1,Define subsample size
v0.13.1,Safety check
v0.13.1,Draw points to create little bags
v0.13.1,Copy and/or define models
v0.13.1,Define nuisance estimators
v0.13.1,Define parameter estimators
v0.13.1,Define
v0.13.1,Need to redefine fit here for auto inference to work due to a quirk in how
v0.13.1,wrap_fit is defined
v0.13.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.13.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.13.1,Override to flatten output if T is flat
v0.13.1,Check that all discrete treatments are represented
v0.13.1,Nuissance estimates evaluated with cross-fitting
v0.13.1,Define 2-fold iterator
v0.13.1,Check if there is only one example of some class
v0.13.1,Define 2-fold iterator
v0.13.1,need safe=False when cloning for WeightedModelWrapper
v0.13.1,Compute residuals
v0.13.1,Compute coefficient by OLS on residuals
v0.13.1,"Parameter returned by LinearRegression is (d_T, )"
v0.13.1,Compute residuals
v0.13.1,Compute coefficient by OLS on residuals
v0.13.1,ell_2 regularization
v0.13.1,Ridge regression estimate
v0.13.1,"Parameter returned is of shape (d_T, )"
v0.13.1,Return moments and gradients
v0.13.1,Compute residuals
v0.13.1,Compute moments
v0.13.1,"Moments shape is (n, d_T)"
v0.13.1,Compute moment gradients
v0.13.1,returns shape-conforming residuals
v0.13.1,Copy and/or define models
v0.13.1,Define parameter estimators
v0.13.1,Define moment and mean gradient estimator
v0.13.1,"Check that T is shape (n, )"
v0.13.1,Check T is numeric
v0.13.1,Train label encoder
v0.13.1,Call `fit` from parent class
v0.13.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.13.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.13.1,Override to flatten output if T is flat
v0.13.1,Expand one-hot encoding to include the zero treatment
v0.13.1,"Test that T contains all treatments. If not, return None"
v0.13.1,Nuissance estimates evaluated with cross-fitting
v0.13.1,Define 2-fold iterator
v0.13.1,Check if there is only one example of some class
v0.13.1,No need to crossfit for internal nodes
v0.13.1,Compute partial moments
v0.13.1,"If any of the values in the parameter estimate is nan, return None"
v0.13.1,Compute partial moments
v0.13.1,Compute coefficient by OLS on residuals
v0.13.1,ell_2 regularization
v0.13.1,Ridge regression estimate
v0.13.1,"Parameter returned is of shape (d_T, )"
v0.13.1,Return moments and gradients
v0.13.1,Compute partial moments
v0.13.1,Compute moments
v0.13.1,"Moments shape is (n, d_T-1)"
v0.13.1,Compute moment gradients
v0.13.1,Need to calculate this in an elegant way for when propensity is 0
v0.13.1,This will flatten T
v0.13.1,Check that T is numeric
v0.13.1,Test whether the input estimator is supported
v0.13.1,Calculate confidence intervals for the parameter (marginal effect)
v0.13.1,Calculate confidence intervals for the effect
v0.13.1,Calculate the effects
v0.13.1,Calculate the standard deviations for the effects
v0.13.1,d_t=None here since we measure the effect across all Ts
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Causal tree parameters
v0.13.1,Tree structure
v0.13.1,No need for a random split since the data is already
v0.13.1,a random subsample from the original input
v0.13.1,node list stores the nodes that are yet to be splitted
v0.13.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.13.1,Create local sample set
v0.13.1,Compute nuisance estimates for the current node
v0.13.1,Nuisance estimate cannot be calculated
v0.13.1,Estimate parameter for current node
v0.13.1,Node estimate cannot be calculated
v0.13.1,Calculate moments and gradient of moments for current data
v0.13.1,Calculate inverse gradient
v0.13.1,The gradient matrix is not invertible.
v0.13.1,No good split can be found
v0.13.1,Calculate point-wise pseudo-outcomes rho
v0.13.1,a split is determined by a feature and a sample pair
v0.13.1,the number of possible splits is at most (number of features) * (number of node samples)
v0.13.1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.13.1,parse row and column of random pair
v0.13.1,the sample of the pair is the integer division of the random number with n_feats
v0.13.1,calculate the binary indicator of whether sample i is on the left or the right
v0.13.1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.13.1,calculate the number of samples on the left child for each proposed split
v0.13.1,calculate the analogous binary indicator for the samples in the estimation set
v0.13.1,calculate the number of estimation samples on the left child of each proposed split
v0.13.1,find the upper and lower bound on the size of the left split for the split
v0.13.1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.13.1,on each side.
v0.13.1,similarly for the estimation sample set
v0.13.1,if there is no valid split then don't create any children
v0.13.1,filter only the valid splits
v0.13.1,calculate the average influence vector of the samples in the left child
v0.13.1,calculate the average influence vector of the samples in the right child
v0.13.1,take the square of each of the entries of the influence vectors and normalize
v0.13.1,by size of each child
v0.13.1,calculate the vector score of each candidate split as the average of left and right
v0.13.1,influence vectors
v0.13.1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.13.1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.13.1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.13.1,calculate the scalar score of each split by aggregating across the vector of scores
v0.13.1,Find split that minimizes criterion
v0.13.1,Create child nodes with corresponding subsamples
v0.13.1,add the created children to the list of not yet split nodes
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.1,Licensed under the MIT License.
v0.13.1,TODO: update docs
v0.13.1,"NOTE: sample weight, sample var are not passed in"
v0.13.1,Compose final model
v0.13.1,Calculate auxiliary quantities
v0.13.1,X ⨂ T_res
v0.13.1,"sum(model_final.predict(X, T_res))"
v0.13.1,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.13.1,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
v0.13.1,generate an instance of the final model
v0.13.1,generate an instance of the nuisance model
v0.13.1,Set _d_t to effective number of treatments
v0.13.1,Required for bootstrap inference
v0.13.1,for each mc iteration
v0.13.1,for each model under cross fit setting
v0.13.1,Handles the corner case when X=None but featurizer might be not None
v0.13.1,Expand treatments for each time period
v0.13.1,NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
v0.13.1,attribute so that the trained featurizer will be passed through
v0.13.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.13.1,for internal use by the library
v0.13.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.13.1,We need to use the _ortho_learner's copy to retain the information from fitting
v0.13.0,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.13.0,configuration is all pulled from setup.cfg
v0.13.0,-*- coding: utf-8 -*-
v0.13.0,
v0.13.0,Configuration file for the Sphinx documentation builder.
v0.13.0,
v0.13.0,This file does only contain a selection of the most common options. For a
v0.13.0,full list see the documentation:
v0.13.0,http://www.sphinx-doc.org/en/main/config
v0.13.0,-- Path setup --------------------------------------------------------------
v0.13.0,"If extensions (or modules to document with autodoc) are in another directory,"
v0.13.0,add these directories to sys.path here. If the directory is relative to the
v0.13.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.13.0,
v0.13.0,-- Project information -----------------------------------------------------
v0.13.0,-- General configuration ---------------------------------------------------
v0.13.0,"If your documentation needs a minimal Sphinx version, state it here."
v0.13.0,
v0.13.0,needs_sphinx = '1.0'
v0.13.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.13.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.13.0,ones.
v0.13.0,"Add any paths that contain templates here, relative to this directory."
v0.13.0,The suffix(es) of source filenames.
v0.13.0,You can specify multiple suffix as a list of string:
v0.13.0,
v0.13.0,"source_suffix = ['.rst', '.md']"
v0.13.0,The root toctree document.
v0.13.0,The language for content autogenerated by Sphinx. Refer to documentation
v0.13.0,for a list of supported languages.
v0.13.0,
v0.13.0,This is also used if you do content translation via gettext catalogs.
v0.13.0,"Usually you set ""language"" from the command line for these cases."
v0.13.0,"List of patterns, relative to source directory, that match files and"
v0.13.0,directories to ignore when looking for source files.
v0.13.0,This pattern also affects html_static_path and html_extra_path.
v0.13.0,The name of the Pygments (syntax highlighting) style to use.
v0.13.0,-- Options for HTML output -------------------------------------------------
v0.13.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.13.0,a list of builtin themes.
v0.13.0,
v0.13.0,Theme options are theme-specific and customize the look and feel of a theme
v0.13.0,"further.  For a list of options available for each theme, see the"
v0.13.0,documentation.
v0.13.0,
v0.13.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.13.0,"relative to this directory. They are copied after the builtin static files,"
v0.13.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.13.0,html_static_path = ['_static']
v0.13.0,"Custom sidebar templates, must be a dictionary that maps document names"
v0.13.0,to template names.
v0.13.0,
v0.13.0,The default sidebars (for documents that don't match any pattern) are
v0.13.0,defined by theme itself.  Builtin themes are using these templates by
v0.13.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.13.0,'searchbox.html']``.
v0.13.0,
v0.13.0,html_sidebars = {}
v0.13.0,-- Options for HTMLHelp output ---------------------------------------------
v0.13.0,Output file base name for HTML help builder.
v0.13.0,-- Options for LaTeX output ------------------------------------------------
v0.13.0,The paper size ('letterpaper' or 'a4paper').
v0.13.0,
v0.13.0,"'papersize': 'letterpaper',"
v0.13.0,"The font size ('10pt', '11pt' or '12pt')."
v0.13.0,
v0.13.0,"'pointsize': '10pt',"
v0.13.0,Additional stuff for the LaTeX preamble.
v0.13.0,
v0.13.0,"'preamble': '',"
v0.13.0,Latex figure (float) alignment
v0.13.0,
v0.13.0,"'figure_align': 'htbp',"
v0.13.0,Grouping the document tree into LaTeX files. List of tuples
v0.13.0,"(source start file, target name, title,"
v0.13.0,"author, documentclass [howto, manual, or own class])."
v0.13.0,-- Options for manual page output ------------------------------------------
v0.13.0,One entry per manual page. List of tuples
v0.13.0,"(source start file, name, description, authors, manual section)."
v0.13.0,-- Options for Texinfo output ----------------------------------------------
v0.13.0,Grouping the document tree into Texinfo files. List of tuples
v0.13.0,"(source start file, target name, title, author,"
v0.13.0,"dir menu entry, description, category)"
v0.13.0,-- Options for Epub output -------------------------------------------------
v0.13.0,Bibliographic Dublin Core info.
v0.13.0,The unique identifier of the text. This can be a ISBN number
v0.13.0,or the project homepage.
v0.13.0,
v0.13.0,epub_identifier = ''
v0.13.0,A unique identification for the text.
v0.13.0,
v0.13.0,epub_uid = ''
v0.13.0,A list of files that should not be packed into the epub file.
v0.13.0,-- Extension configuration -------------------------------------------------
v0.13.0,-- Options for intersphinx extension ---------------------------------------
v0.13.0,Example configuration for intersphinx: refer to the Python standard library.
v0.13.0,-- Options for todo extension ----------------------------------------------
v0.13.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.13.0,-- Options for doctest extension -------------------------------------------
v0.13.0,we can document otherwise excluded entities here by returning False
v0.13.0,or skip otherwise included entities by returning True
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Calculate residuals
v0.13.0,Estimate E[T_res | Z_res]
v0.13.0,TODO. Deal with multi-class instrument
v0.13.0,Calculate nuisances
v0.13.0,Estimate E[T_res | Z_res]
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"We do a three way split, as typically a preliminary theta estimator would require"
v0.13.0,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.13.0,TODO. Deal with multi-class instrument
v0.13.0,Estimate final model of theta(X) by minimizing the square loss:
v0.13.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.13.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.13.0,at the expense of some small bias. For points with very small covariance we revert
v0.13.0,to the model-based preliminary estimate and do not add the correction term.
v0.13.0,Estimate preliminary theta in cross fitting manner
v0.13.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.13.0,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.13.0,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.13.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.13.0,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.13.0,TODO. The solution below is not really a valid cross-fitting
v0.13.0,as the test data are used to create the proj_t on the train
v0.13.0,which in the second train-test loop is used to create the nuisance
v0.13.0,cov on the test data. Hence the T variable of some sample
v0.13.0,"is implicitly correlated with its cov nuisance, through this flow"
v0.13.0,"of information. However, this seems a rather weak correlation."
v0.13.0,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.13.0,model.
v0.13.0,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.13.0,Estimate preliminary theta in cross fitting manner
v0.13.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.13.0,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.13.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.13.0,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.13.0,#############################################################################
v0.13.0,Classes for the DRIV implementation for the special case of intent-to-treat
v0.13.0,A/B test
v0.13.0,#############################################################################
v0.13.0,Estimate preliminary theta in cross fitting manner
v0.13.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.13.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.13.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.13.0,We can use statsmodel for all hypothesis testing capabilities
v0.13.0,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.13.0,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.13.0,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.13.0,model_T_XZ = lambda: model_clf()
v0.13.0,#'days_visited': lambda:
v0.13.0,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.13.0,Turn strings into categories for numeric mapping
v0.13.0,### Defining some generic regressors and classifiers
v0.13.0,This a generic non-parametric regressor
v0.13.0,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.13.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.13.0,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.13.0,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.13.0,model = lambda: RandomForestRegressor(n_estimators=100)
v0.13.0,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.13.0,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.13.0,model = lambda: LinearRegression(n_jobs=-1)
v0.13.0,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.13.0,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.13.0,underlying model whenever predict is called.
v0.13.0,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.13.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.13.0,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.13.0,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.13.0,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.13.0,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.13.0,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.13.0,We need to specify models to be used for each of these residualizations
v0.13.0,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.13.0,"E[T | X, Z]"
v0.13.0,E[TZ | X]
v0.13.0,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.13.0,n_splits determines the number of splits to be used for cross-fitting.
v0.13.0,# Algorithm 2 - Current Method
v0.13.0,In[121]:
v0.13.0,# Algorithm 3 - DRIV ATE
v0.13.0,dmliv_model_effect = lambda: model()
v0.13.0,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.13.0,"dmliv_model_effect(),"
v0.13.0,n_splits=1)
v0.13.0,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.13.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.13.0,"Once multiple treatments are supported, we'll need to fix this"
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.13.0,We can use statsmodel for all hypothesis testing capabilities
v0.13.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.13.0,We can use statsmodel for all hypothesis testing capabilities
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,TODO. Deal with multi-class instrument/treatment
v0.13.0,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.13.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.13.0,Estimate p(X) = E[T | X] in cross-fitting manner
v0.13.0,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.13.0,##################
v0.13.0,Global settings #
v0.13.0,##################
v0.13.0,Global plotting controls
v0.13.0,"Control for support size, can control for more"
v0.13.0,#################
v0.13.0,File utilities #
v0.13.0,#################
v0.13.0,#################
v0.13.0,Plotting utils #
v0.13.0,#################
v0.13.0,bias
v0.13.0,var
v0.13.0,rmse
v0.13.0,r2
v0.13.0,Infer feature dimension
v0.13.0,Metrics by support plots
v0.13.0,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.13.0,Vasilis Syrgkanis <vasy@microsoft.com>
v0.13.0,Steven Wu <zhiww@microsoft.com>
v0.13.0,Initialize causal tree parameters
v0.13.0,Create splits of causal tree
v0.13.0,Estimate treatment effects at the leafs
v0.13.0,Compute heterogeneous treatement effect for x's in x_list by finding
v0.13.0,the corresponding split and associating the effect computed on that leaf
v0.13.0,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.13.0,Safety check
v0.13.0,Weighted linear regression
v0.13.0,Calculates weights
v0.13.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.13.0,over all indices
v0.13.0,Similar for `a` weights
v0.13.0,Doesn't have sample weights
v0.13.0,Is a linear model
v0.13.0,Weighted linear regression
v0.13.0,Calculates weights
v0.13.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.13.0,over all indices
v0.13.0,Similar for `a` weights
v0.13.0,normalize weights
v0.13.0,"Split the data in half, train and test"
v0.13.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.13.0,"a function of W, using only the train fold"
v0.13.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.13.0,"Split the data in half, train and test"
v0.13.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.13.0,"a function of W, using only the train fold"
v0.13.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.13.0,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.13.0,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.13.0,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.13.0,"Split the data in half, train and test"
v0.13.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.13.0,"a function of x, using only the train fold"
v0.13.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.13.0,Compute coefficient by OLS on residuals
v0.13.0,"Split the data in half, train and test"
v0.13.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.13.0,"a function of x, using only the train fold"
v0.13.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.13.0,Estimate multipliers for second order orthogonal method
v0.13.0,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.13.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.13.0,Create local sample set
v0.13.0,compute the base estimate for the current node using double ml or second order double ml
v0.13.0,compute the influence functions here that are used for the criterion
v0.13.0,generate random proposals of dimensions to split
v0.13.0,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.13.0,compute criterion for each proposal
v0.13.0,if splitting creates valid leafs in terms of mean leaf size
v0.13.0,Calculate criterion for split
v0.13.0,Else set criterion to infinity so that this split is not chosen
v0.13.0,If no good split was found
v0.13.0,Find split that minimizes criterion
v0.13.0,Set the split attributes at the node
v0.13.0,Create child nodes with corresponding subsamples
v0.13.0,Recursively split children
v0.13.0,Return parent node
v0.13.0,estimate the local parameter at the leaf using the estimate data
v0.13.0,###################
v0.13.0,Argument parsing #
v0.13.0,###################
v0.13.0,#########################################
v0.13.0,Parameters constant across experiments #
v0.13.0,#########################################
v0.13.0,Outcome support
v0.13.0,Treatment support
v0.13.0,Evaluation grid
v0.13.0,Treatment effects array
v0.13.0,Other variables
v0.13.0,##########################
v0.13.0,Data Generating Process #
v0.13.0,##########################
v0.13.0,Log iteration
v0.13.0,"Generate controls, features, treatment and outcome"
v0.13.0,T and Y residuals to be used in later scripts
v0.13.0,Save generated dataset
v0.13.0,#################
v0.13.0,ORF parameters #
v0.13.0,#################
v0.13.0,######################################
v0.13.0,Train and evaluate treatment effect #
v0.13.0,######################################
v0.13.0,########
v0.13.0,Plots #
v0.13.0,########
v0.13.0,###############
v0.13.0,Save results #
v0.13.0,###############
v0.13.0,##############
v0.13.0,Run Rscript #
v0.13.0,##############
v0.13.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.13.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.13.0,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.13.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.13.0,def mlasso_model(): return MultiTaskLassoCV(
v0.13.0,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.13.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.13.0,heterogeneity
v0.13.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.13.0,heterogeneity
v0.13.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.13.0,heterogeneity
v0.13.0,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.13.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.13.0,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.13.0,subset of features that are exogenous and create heterogeneity
v0.13.0,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.13.0,subset of features wrt we estimate heterogeneity
v0.13.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.13.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,introspect the constructor arguments to find the model parameters
v0.13.0,to represent
v0.13.0,"if the argument is deprecated, ignore it"
v0.13.0,Extract and sort argument names excluding 'self'
v0.13.0,column names
v0.13.0,transfer input to numpy arrays
v0.13.0,transfer input to 2d arrays
v0.13.0,create dataframe
v0.13.0,currently dowhy only support single outcome and single treatment
v0.13.0,call dowhy
v0.13.0,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.13.0,cate estimator but not the effect.
v0.13.0,don't proxy special methods
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Check if model is sparse enough for this model
v0.13.0,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.13.0,TODO: any way to avoid creating a copy if the array was already dense?
v0.13.0,"the call is necessary if the input was something like a list, though"
v0.13.0,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.13.0,so convert to pydata sparse first
v0.13.0,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.13.0,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.13.0,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.13.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.13.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.13.0,For when checking input values is disabled
v0.13.0,Type to column extraction function
v0.13.0,Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
v0.13.0,"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
v0.13.0,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.13.0,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.13.0,Get feature names using featurizer
v0.13.0,All attempts at retrieving transformed feature names have failed
v0.13.0,Delegate handling to downstream logic
v0.13.0,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.13.0,same number of input definitions as arrays
v0.13.0,input definitions have same number of dimensions as each array
v0.13.0,all result indices are unique
v0.13.0,all result indices must match at least one input index
v0.13.0,"map indices to all array, axis pairs for that index"
v0.13.0,each index has the same cardinality wherever it appears
v0.13.0,"State: list of (set of letters, list of (corresponding indices, value))"
v0.13.0,Algo: while list contains more than one entry
v0.13.0,take two entries
v0.13.0,sort both lists by intersection of their indices
v0.13.0,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.13.0,"take the union of indices and the product of values), stepping through each list linearly"
v0.13.0,TODO: might be faster to break into connected components first
v0.13.0,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.13.0,"so compute their content separately, then take cartesian product"
v0.13.0,this would save a few pointless sorts by empty tuples
v0.13.0,TODO: Consider investigating other performance ideas for these cases
v0.13.0,where the dense method beat the sparse method (usually sparse is faster)
v0.13.0,"e,facd,c->cfed"
v0.13.0,sparse: 0.0335489
v0.13.0,dense:  0.011465999999999997
v0.13.0,"gbd,da,egb->da"
v0.13.0,sparse: 0.0791625
v0.13.0,dense:  0.007319099999999995
v0.13.0,"dcc,d,faedb,c->abe"
v0.13.0,sparse: 1.2868097
v0.13.0,dense:  0.44605229999999985
v0.13.0,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.13.0,TODO: would using einsum's paths to optimize the order of merging help?
v0.13.0,assume that we should perform nested cross-validation if and only if
v0.13.0,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.13.0,logic copied from check_cv
v0.13.0,otherwise we will assume the user already set the cv attribute to something
v0.13.0,compatible with splitting with a 'groups' argument
v0.13.0,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.13.0,don't use the groups when calling split internally
v0.13.0,Normalize weights
v0.13.0,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.13.0,"if we're decorating a class, just update the __init__ method,"
v0.13.0,so that the result is still a class instead of a wrapper method
v0.13.0,"want to enforce that each bad_arg was either in kwargs,"
v0.13.0,or else it was in neither and is just taking its default value
v0.13.0,Any access should throw
v0.13.0,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.13.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.13.0,input feature name is already updated by cate_feature_names.
v0.13.0,define the index of d_x to filter for each given T
v0.13.0,filter X after broadcast with T for each given T
v0.13.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.13.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,
v0.13.0,This code contains some snippets of code from:
v0.13.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
v0.13.0,published under the following license and copyright:
v0.13.0,BSD 3-Clause License
v0.13.0,
v0.13.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.0,All rights reserved.
v0.13.0,make any access to matplotlib or plt throw an exception
v0.13.0,make any access to graphviz or plt throw an exception
v0.13.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.13.0,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.13.0,Initialize saturation & value; calculate chroma & value shift
v0.13.0,Calculate some intermediate values
v0.13.0,Initialize RGB with same hue & chroma as our color
v0.13.0,Shift the initial RGB values to match value and store
v0.13.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.13.0,clean way of achieving this
v0.13.0,make sure we don't accidentally escape anything in the substitution
v0.13.0,Fetch appropriate color for node
v0.13.0,"red for negative, green for positive"
v0.13.0,in multi-target use mean of targets
v0.13.0,Write node mean CATE
v0.13.0,Write node std of CATE
v0.13.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.13.0,Fetch appropriate color for node
v0.13.0,Write node mean CATE
v0.13.0,Write node mean CATE
v0.13.0,Write recommended treatment and value - cost
v0.13.0,Licensed under the MIT License.
v0.13.0,"since inference objects can be stateful, we must copy it before fitting;"
v0.13.0,otherwise this sequence wouldn't work:
v0.13.0,"est1.fit(..., inference=inf)"
v0.13.0,"est2.fit(..., inference=inf)"
v0.13.0,est1.effect_interval(...)
v0.13.0,because inf now stores state from fitting est2
v0.13.0,This flag is true when names are set in a child class instead
v0.13.0,"If names are set in a child class, add an attribute reflecting that"
v0.13.0,This works only if X is passed as a kwarg
v0.13.0,We plan to enforce X as kwarg only in future releases
v0.13.0,This checks if names have been set in a child class
v0.13.0,"If names were set in a child class, don't do it again"
v0.13.0,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.13.0,call the wrapped fit method
v0.13.0,NOTE: we call inference fit *after* calling the main fit method
v0.13.0,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.13.0,but tensordot can't be applied to this problem because we don't sum over m
v0.13.0,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.13.0,of rows of T was not taken into account
v0.13.0,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.13.0,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.13.0,"Treatment names is None, default to BaseCateEstimator"
v0.13.0,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.13.0,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.13.0,Get input names
v0.13.0,Summary
v0.13.0,add statsmodels to parent's options
v0.13.0,add debiasedlasso to parent's options
v0.13.0,add blb to parent's options
v0.13.0,TODO Share some logic with non-discrete version
v0.13.0,Get input names
v0.13.0,Note: we do not transform feature names since that is done within summary_frame
v0.13.0,Summary
v0.13.0,add statsmodels to parent's options
v0.13.0,add statsmodels to parent's options
v0.13.0,add blb to parent's options
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,remove None arguments
v0.13.0,"scores entries should be lists of scores, so make each entry a singleton list"
v0.13.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.13.0,generate an instance of the final model
v0.13.0,generate an instance of the nuisance model
v0.13.0,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.13.0,alteration even when we only want to fit_final.
v0.13.0,use a binary array to get stratified split in case of discrete treatment
v0.13.0,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.13.0,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.13.0,"however, sklearn doesn't support both stratifying and grouping (see"
v0.13.0,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.13.0,their own object that supports grouping if they want to use groups.
v0.13.0,for each mc iteration
v0.13.0,for each model under cross fit setting
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,
v0.13.0,This code contains snippets of code from
v0.13.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.13.0,published under the following license and copyright:
v0.13.0,BSD 3-Clause License
v0.13.0,
v0.13.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.0,All rights reserved.
v0.13.0,=============================================================================
v0.13.0,Policy Forest
v0.13.0,=============================================================================
v0.13.0,Remap output
v0.13.0,reshape is necessary to preserve the data contiguity against vs
v0.13.0,"[:, np.newaxis] that does not."
v0.13.0,Get subsample sample size
v0.13.0,Check parameters
v0.13.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.13.0,if this is the first `fit` call of the warm start mode.
v0.13.0,"Free allocated memory, if any"
v0.13.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.13.0,We draw from the random state to get the random state we
v0.13.0,would have got if we hadn't used a warm_start.
v0.13.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.13.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.13.0,but would still advance randomness enough so that tree subsamples will be different.
v0.13.0,Parallel loop: we prefer the threading backend as the Cython code
v0.13.0,for fitting the trees is internally releasing the Python GIL
v0.13.0,making threading more efficient than multiprocessing in
v0.13.0,"that case. However, for joblib 0.12+ we respect any"
v0.13.0,"parallel_backend contexts set at a higher level,"
v0.13.0,since correctness does not rely on using threads.
v0.13.0,Collect newly grown trees
v0.13.0,Check data
v0.13.0,Assign chunk of trees to jobs
v0.13.0,avoid storing the output of every estimator by summing them here
v0.13.0,Parallel loop
v0.13.0,Check data
v0.13.0,Assign chunk of trees to jobs
v0.13.0,avoid storing the output of every estimator by summing them here
v0.13.0,Parallel loop
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,
v0.13.0,This code contains snippets of code from:
v0.13.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.13.0,published under the following license and copyright:
v0.13.0,BSD 3-Clause License
v0.13.0,
v0.13.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.0,All rights reserved.
v0.13.0,=============================================================================
v0.13.0,Types and constants
v0.13.0,=============================================================================
v0.13.0,=============================================================================
v0.13.0,Base Policy tree
v0.13.0,=============================================================================
v0.13.0,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.13.0,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.13.0,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.13.0,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.13.0,checks that X is 2D array.
v0.13.0,"since we only allow single dimensional y, we could flatten the prediction"
v0.13.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.13.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.13.0,Handles the corner case when X=None but featurizer might be not None
v0.13.0,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.13.0,"Replacing this method which is invalid for this class, so that we make the"
v0.13.0,dosctring empty and not appear in the docs.
v0.13.0,TODO: support freq_weight and sample_var in debiased lasso
v0.13.0,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.13.0,Replacing to remove docstring
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"if both X and W are None, just return a column of ones"
v0.13.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.13.0,We need to go back to the label representation of the one-hot so as to call
v0.13.0,the classifier.
v0.13.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.13.0,We need to go back to the label representation of the one-hot so as to call
v0.13.0,the classifier.
v0.13.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.13.0,This works both with our without the weighting trick as the treatments T are unit vector
v0.13.0,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.13.0,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.13.0,both Parametric and Non Parametric DML.
v0.13.0,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.13.0,attribute so that the trained featurizer will be passed through
v0.13.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.13.0,for internal use by the library
v0.13.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.13.0,We need to use the rlearner's copy to retain the information from fitting
v0.13.0,Handles the corner case when X=None but featurizer might be not None
v0.13.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.13.0,since we clone it and fit separate copies
v0.13.0,override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
v0.13.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.13.0,TODO: support freq_weight and sample_var in debiased lasso
v0.13.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.13.0,since we clone it and fit separate copies
v0.13.0,add blb to parent's options
v0.13.0,override only so that we can update the docstring to indicate
v0.13.0,support for `GenericSingleTreatmentModelFinalInference`
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,note that groups are not passed to score because they are only used for fitting
v0.13.0,note that groups are not passed to score because they are only used for fitting
v0.13.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.13.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.13.0,NOTE: important to get parent's wrapped copy so that
v0.13.0,"after training wrapped featurizer is also trained, etc."
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.13.0,Fit a doubly robust average effect
v0.13.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.13.0,(which needs to have been expanded if there's a discrete treatment)
v0.13.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.13.0,since we clone it and fit separate copies
v0.13.0,"If custom param grid, check that only estimator parameters are being altered"
v0.13.0,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.13.0,override only so that we can update the docstring to indicate support for `blb`
v0.13.0,Get input names
v0.13.0,Summary
v0.13.0,Determine output settings
v0.13.0,"Important: This must be the first invocation of the random state at fit time, so that"
v0.13.0,train/test splits are re-generatable from an external object simply by knowing the
v0.13.0,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.13.0,linear predictions. Currently is also useful for testing.
v0.13.0,reshape is necessary to preserve the data contiguity against vs
v0.13.0,"[:, np.newaxis] that does not."
v0.13.0,Check parameters
v0.13.0,Set min_weight_leaf from min_weight_fraction_leaf
v0.13.0,Build tree
v0.13.0,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.13.0,hold. Used by criterion for memory space savings.
v0.13.0,Initialize the criterion object and the criterion_val object if honest.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,
v0.13.0,This code is a fork from:
v0.13.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
v0.13.0,published under the following license and copyright:
v0.13.0,BSD 3-Clause License
v0.13.0,
v0.13.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.0,All rights reserved.
v0.13.0,Set parameters
v0.13.0,Don't instantiate estimators now! Parameters of base_estimator might
v0.13.0,"still change. Eg., when grid-searching with the nested object syntax."
v0.13.0,self.estimators_ needs to be filled by the derived classes in fit.
v0.13.0,Compute the number of jobs
v0.13.0,Partition estimators between jobs
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,covariance matrix
v0.13.0,get eigen value and eigen vectors
v0.13.0,simulate eigen vectors
v0.13.0,keep the top 4 eigen value and corresponding eigen vector
v0.13.0,replace the negative eigen values
v0.13.0,generate a new covariance matrix
v0.13.0,get linear approximation of eigen values
v0.13.0,coefs
v0.13.0,get the indices of each group of features
v0.13.0,print(ind_same_proxy)
v0.13.0,demo
v0.13.0,same proxy
v0.13.0,residuals
v0.13.0,gmm
v0.13.0,log normal on outliers
v0.13.0,positive outliers
v0.13.0,negative outliers
v0.13.0,demean the new residual again
v0.13.0,generate data
v0.13.0,sample residuals
v0.13.0,get prediction for current investment
v0.13.0,get prediction for current proxy
v0.13.0,get first period prediction
v0.13.0,iterate the step ahead contruction
v0.13.0,prepare new x
v0.13.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.13.0,heterogeneity
v0.13.0,get new covariance matrix
v0.13.0,get coefs
v0.13.0,get residuals
v0.13.0,proxy 1 is the outcome
v0.13.0,make fixed residuals
v0.13.0,Remove children with nonwhite mothers from the treatment group
v0.13.0,Remove children with nonwhite mothers from the treatment group
v0.13.0,Select columns
v0.13.0,Scale the numeric variables
v0.13.0,"Change the binary variable 'first' takes values in {1,2}"
v0.13.0,Append a column of ones as intercept
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.13.0,(which needs to have been expanded if there's a discrete treatment)
v0.13.0,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.13.0,d_t=None here since we measure the effect across all Ts
v0.13.0,once the estimator has been fit
v0.13.0,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.13.0,an intercept even when bias is part of coef.
v0.13.0,We can write effect inference as a function of prediction and prediction standard error of
v0.13.0,the final method for linear models
v0.13.0,squeeze the first axis
v0.13.0,d_t=None here since we measure the effect across all Ts
v0.13.0,set the mean_pred_stderr
v0.13.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.13.0,(which needs to have been expanded if there's a discrete treatment)
v0.13.0,"send treatment to the end, pull bounds to the front"
v0.13.0,d_t=None here since we measure the effect across all Ts
v0.13.0,set the mean_pred_stderr
v0.13.0,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.13.0,d_t=None here since we measure the effect across all Ts
v0.13.0,d_t=None here since we measure the effect across all Ts
v0.13.0,need to set the fit args before the estimator is fit
v0.13.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.13.0,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.13.0,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.13.0,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.13.0,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.13.0,scale preds
v0.13.0,scale std errs
v0.13.0,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.13.0,offset preds
v0.13.0,"offset the distribution, too"
v0.13.0,scale preds
v0.13.0,"scale the distribution, too"
v0.13.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.13.0,1. Uncertainty of Mean Point Estimate
v0.13.0,2. Distribution of Point Estimate
v0.13.0,3. Total Variance of Point Estimate
v0.13.0,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.13.0,so bail out early; note that it might be possible to correct the algorithm for
v0.13.0,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.13.0,be clean
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,TODO: Add a __dir__ implementation?
v0.13.0,don't proxy special methods
v0.13.0,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.13.0,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.13.0,then we should be computing a confidence interval for the wrapped calls
v0.13.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.13.0,second level bootstrap which would be prohibitive computationally?
v0.13.0,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.13.0,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.13.0,can't import from econml.inference at top level without creating cyclical dependencies
v0.13.0,Note that inference results are always methods even if the inference is for a property
v0.13.0,(e.g. coef__inference() is a method but coef_ is a property)
v0.13.0,Therefore we must insert a lambda if getting inference for a non-callable
v0.13.0,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.13.0,"try to get interval/std first if appropriate,"
v0.13.0,since we don't prefer a wrapped method with this name
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,
v0.13.0,This code contains snippets of code from:
v0.13.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
v0.13.0,published under the following license and copyright:
v0.13.0,BSD 3-Clause License
v0.13.0,
v0.13.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.0,All rights reserved.
v0.13.0,=============================================================================
v0.13.0,Types and constants
v0.13.0,=============================================================================
v0.13.0,=============================================================================
v0.13.0,Base GRF tree
v0.13.0,=============================================================================
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,=============================================================================
v0.13.0,A MultOutputWrapper for GRF classes
v0.13.0,=============================================================================
v0.13.0,=============================================================================
v0.13.0,Instantiations of Generalized Random Forest
v0.13.0,=============================================================================
v0.13.0,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.13.0,in front of the constant treatment is the intercept in the moment equation.
v0.13.0,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.13.0,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,
v0.13.0,This code contains snippets of code from
v0.13.0,https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
v0.13.0,published under the following license and copyright:
v0.13.0,BSD 3-Clause License
v0.13.0,
v0.13.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.13.0,All rights reserved.
v0.13.0,=============================================================================
v0.13.0,Base Generalized Random Forest
v0.13.0,=============================================================================
v0.13.0,TODO: support freq_weight and sample_var
v0.13.0,Remap output
v0.13.0,reshape is necessary to preserve the data contiguity against vs
v0.13.0,"[:, np.newaxis] that does not."
v0.13.0,reshape is necessary to preserve the data contiguity against vs
v0.13.0,"[:, np.newaxis] that does not."
v0.13.0,Get subsample sample size
v0.13.0,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.13.0,We calculate the min eigenvalue proxy that each criterion is considering
v0.13.0,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.13.0,Check parameters
v0.13.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.13.0,if this is the first `fit` call of the warm start mode.
v0.13.0,"Free allocated memory, if any"
v0.13.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.13.0,We draw from the random state to get the random state we
v0.13.0,would have got if we hadn't used a warm_start.
v0.13.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.13.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.13.0,but would still advance randomness enough so that tree subsamples will be different.
v0.13.0,Generating indices a priori before parallelism ended up being orders of magnitude
v0.13.0,faster than how sklearn does it. The reason is that random samplers do not release the
v0.13.0,gil it seems.
v0.13.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.13.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.13.0,but would still advance randomness enough so that tree subsamples will be different.
v0.13.0,Parallel loop: we prefer the threading backend as the Cython code
v0.13.0,for fitting the trees is internally releasing the Python GIL
v0.13.0,making threading more efficient than multiprocessing in
v0.13.0,"that case. However, for joblib 0.12+ we respect any"
v0.13.0,"parallel_backend contexts set at a higher level,"
v0.13.0,since correctness does not rely on using threads.
v0.13.0,Collect newly grown trees
v0.13.0,Check data
v0.13.0,Assign chunk of trees to jobs
v0.13.0,avoid storing the output of every estimator by summing them here
v0.13.0,Parallel loop
v0.13.0,Check data
v0.13.0,Assign chunk of trees to jobs
v0.13.0,Parallel loop
v0.13.0,Check data
v0.13.0,Assign chunk of trees to jobs
v0.13.0,Parallel loop
v0.13.0,####################
v0.13.0,Variance correction
v0.13.0,####################
v0.13.0,Subtract the average within bag variance. This ends up being equal to the
v0.13.0,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.13.0,The negative part is just sq_between.
v0.13.0,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.13.0,"The off diagonals we have no objective prior, so no correction is applied."
v0.13.0,Finally correcting the pred_cov or pred_var
v0.13.0,avoid storing the output of every estimator by summing them here
v0.13.0,Parallel loop
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,testing importances
v0.13.0,testing heterogeneity importances
v0.13.0,Testing that all parameters do what they are supposed to
v0.13.0,"testing predict, apply and decision path"
v0.13.0,test that the subsampling scheme past to the trees is correct
v0.13.0,The sample size is chosen in particular to test rounding based error when subsampling
v0.13.0,test that the estimator calcualtes var correctly
v0.13.0,test api
v0.13.0,test accuracy
v0.13.0,test the projection functionality of forests
v0.13.0,test that the estimator calcualtes var correctly
v0.13.0,test api
v0.13.0,test that the estimator calcualtes var correctly
v0.13.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.13.0,test that we raise errors in mishandled situations.
v0.13.0,test that the subsampling scheme past to the trees is correct
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
v0.13.0,omit the lalonde notebook
v0.13.0,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.13.0,creating notebooks that are annoying for our users to actually run themselves
v0.13.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.13.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.13.0,"prior to calling interpret, can't plot, render, etc."
v0.13.0,can interpret without uncertainty
v0.13.0,can't interpret with uncertainty if inference wasn't used during fit
v0.13.0,can interpret with uncertainty if we refit
v0.13.0,can interpret without uncertainty
v0.13.0,can't treat before interpreting
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,for is_discrete in [False]:
v0.13.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.13.0,ensure we can serialize the unfit estimator
v0.13.0,ensure we can pickle the fit estimator
v0.13.0,make sure we can call the marginal_effect and effect methods
v0.13.0,test const marginal inference
v0.13.0,test effect inference
v0.13.0,test marginal effect inference
v0.13.0,test coef__inference and intercept__inference
v0.13.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.0,"make sure we can call effect with implied scalar treatments,"
v0.13.0,"no matter the dimensions of T, and also that we warn when there"
v0.13.0,are multiple treatments
v0.13.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.0,No heterogeneity
v0.13.0,Define indices to test
v0.13.0,Heterogeneous effects
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,simple DGP only for illustration
v0.13.0,Define the treatment model neural network architecture
v0.13.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.13.0,"so the input shape is (d_z + d_x,)"
v0.13.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.13.0,add extra layers on top for the mixture density network
v0.13.0,Define the response model neural network architecture
v0.13.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.13.0,"so the input shape is (d_t + d_x,)"
v0.13.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.13.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.13.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.13.0,so that the same weights will be reused in each instantiation
v0.13.0,number of samples to use in second estimate of the response
v0.13.0,(to make loss estimate unbiased)
v0.13.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.13.0,do something with predictions...
v0.13.0,also test vector t and y
v0.13.0,simple DGP only for illustration
v0.13.0,Define the treatment model neural network architecture
v0.13.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.13.0,"so the input shape is (d_z + d_x,)"
v0.13.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.13.0,add extra layers on top for the mixture density network
v0.13.0,Define the response model neural network architecture
v0.13.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.13.0,"so the input shape is (d_t + d_x,)"
v0.13.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.13.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.13.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.13.0,so that the same weights will be reused in each instantiation
v0.13.0,number of samples to use in second estimate of the response
v0.13.0,(to make loss estimate unbiased)
v0.13.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.13.0,do something with predictions...
v0.13.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.13.0,test = True ensures we draw test set images
v0.13.0,test = True ensures we draw test set images
v0.13.0,re-draw to get new independent treatment and implied response
v0.13.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.13.0,above is necesary so that reduced form doesn't win
v0.13.0,covariates: time and emotion
v0.13.0,random instrument
v0.13.0,z -> price
v0.13.0,true observable demand function
v0.13.0,errors
v0.13.0,response
v0.13.0,test = True ensures we draw test set images
v0.13.0,test = True ensures we draw test set images
v0.13.0,re-draw to get new independent treatment and implied response
v0.13.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.13.0,above is necesary so that reduced form doesn't win
v0.13.0,covariates: time and emotion
v0.13.0,random instrument
v0.13.0,z -> price
v0.13.0,true observable demand function
v0.13.0,errors
v0.13.0,response
v0.13.0,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.13.0,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.13.0,For some reason this doesn't work at all when run against the CNTK backend...
v0.13.0,"model.compile('nadam', loss=lambda _,l:l)"
v0.13.0,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.13.0,generate a valiation set
v0.13.0,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.13.0,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,DGP constants
v0.13.0,Generate data
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,testing importances
v0.13.0,testing heterogeneity importances
v0.13.0,Testing that all parameters do what they are supposed to
v0.13.0,"testing predict, apply and decision path"
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.13.0,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.13.0,so we need to transpose the result
v0.13.0,1-d output
v0.13.0,2-d output
v0.13.0,Single dimensional output y
v0.13.0,compare with weight
v0.13.0,compare with weight
v0.13.0,compare with weight
v0.13.0,compare with weight
v0.13.0,Multi-dimensional output y
v0.13.0,1-d y
v0.13.0,compare when both sample_var and sample_weight exist
v0.13.0,multi-d y
v0.13.0,compare when both sample_var and sample_weight exist
v0.13.0,compare when both sample_var and sample_weight exist
v0.13.0,compare when both sample_var and sample_weight exist
v0.13.0,compare when both sample_var and sample_weight exist
v0.13.0,compare when both sample_var and sample_weight exist
v0.13.0,compare when both sample_var and sample_weight exist
v0.13.0,dgp
v0.13.0,StatsModels2SLS
v0.13.0,IV2SLS
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,test that we can fit with the same arguments as the base estimator
v0.13.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.13.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.0,with the same shape for the lower and upper bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,test that we can do the same thing once we provide percentile bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.13.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.0,with the same shape for the lower and upper bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,test that we can do the same thing once we provide percentile bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,test that we can fit with the same arguments as the base estimator
v0.13.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.13.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.0,with the same shape for the lower and upper bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,test that we can do the same thing once we provide percentile bounds
v0.13.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.13.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.0,with the same shape for the lower and upper bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,test that we can do the same thing once we provide percentile bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,test that we can fit with the same arguments as the base estimator
v0.13.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.13.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.0,with the same shape for the lower and upper bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,test that we can do the same thing once we provide percentile bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.13.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.0,with the same shape for the lower and upper bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,test that we can do the same thing once we provide percentile bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.0,with the same shape for the lower and upper bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,test that the estimated effect is usually within the bounds
v0.13.0,test that we can do the same thing once we provide alpha explicitly
v0.13.0,test that the lower and upper bounds differ
v0.13.0,test that the estimated effect is usually within the bounds
v0.13.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.13.0,with the same shape for the lower and upper bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,TODO: test that the estimated effect is usually within the bounds
v0.13.0,and that the true effect is also usually within the bounds
v0.13.0,test that we can do the same thing once we provide percentile bounds
v0.13.0,test that the lower and upper bounds differ
v0.13.0,TODO: test that the estimated effect is usually within the bounds
v0.13.0,and that the true effect is also usually within the bounds
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,test that the subsampling scheme past to the trees is correct
v0.13.0,test that the estimator calcualtes var correctly
v0.13.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.13.0,test that we raise errors in mishandled situations.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,DGP constants
v0.13.0,Generate data
v0.13.0,Test inference results when `cate_feature_names` doesn not exist
v0.13.0,Test inference results when `cate_feature_names` doesn not exist
v0.13.0,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.13.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.13.0,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.13.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.13.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.13.0,pvalue for second column should be greater than zero since some points are on either side
v0.13.0,of the tested value
v0.13.0,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.13.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.13.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.13.0,ensure alpha is passed
v0.13.0,only is not None when T1 is a constant or a list of constant
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.13.0,Test non keyword based calls to fit
v0.13.0,test non-array inputs
v0.13.0,Test custom splitter
v0.13.0,Test incomplete set of test folds
v0.13.0,"y scores should be positive, since W predicts Y somewhat"
v0.13.0,"t scores might not be, since W and T are uncorrelated"
v0.13.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,make sure cross product varies more slowly with first array
v0.13.0,and that vectors are okay as inputs
v0.13.0,number of inputs in specification must match number of inputs
v0.13.0,must have an output
v0.13.0,output indices must be unique
v0.13.0,output indices must be present in an input
v0.13.0,number of indices must match number of dimensions for each input
v0.13.0,repeated indices must always have consistent sizes
v0.13.0,transpose
v0.13.0,tensordot
v0.13.0,trace
v0.13.0,TODO: set up proper flag for this
v0.13.0,pick indices at random with replacement from the first 7 letters of the alphabet
v0.13.0,"of all of the distinct indices that appear in any input,"
v0.13.0,pick a random subset of them (of size at most 5) to appear in the output
v0.13.0,creating an instance should warn
v0.13.0,using the instance should not warn
v0.13.0,using the deprecated method should warn
v0.13.0,don't warn if b and c are passed by keyword
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,make any access to matplotlib or plt throw an exception
v0.13.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.13.0,heterogeneity
v0.13.0,Invert indices to match latest API
v0.13.0,Invert indices to match latest API
v0.13.0,The feature for heterogeneity stays constant
v0.13.0,Auxiliary function for adding xticks and vertical lines when plotting results
v0.13.0,for dynamic dml vs ground truth parameters.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Preprocess data
v0.13.0,Convert 'week' to a date
v0.13.0,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.13.0,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.13.0,Take log of price
v0.13.0,Make brand numeric
v0.13.0,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.13.0,which have all zero coeffs
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,test at least one estimator from each category
v0.13.0,test causal graph
v0.13.0,test refutation estimate
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.13.0,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.13.0,TODO: test something rather than just print...
v0.13.0,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.13.0,pick some arbitrary X
v0.13.0,pick some arbitrary T
v0.13.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.13.0,The average variance should be lower when using monte carlo iterations
v0.13.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.13.0,The average variance should be lower when using monte carlo iterations
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,ensure we can serialize unfit estimator
v0.13.0,ensure we can serialize fit estimator
v0.13.0,expected effect size
v0.13.0,test effect
v0.13.0,test inference
v0.13.0,only OrthoIV support inference other than bootstrap
v0.13.0,test summary
v0.13.0,test can run score
v0.13.0,test cate_feature_names
v0.13.0,test can run shap values
v0.13.0,dgp
v0.13.0,no heterogeneity
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.13.0,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.13.0,The __warningregistry__'s need to be in a pristine state for tests
v0.13.0,to work properly.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,ensure we can serialize unfit estimator
v0.13.0,ensure we can serialize fit estimator
v0.13.0,expected effect size
v0.13.0,test effect
v0.13.0,test inference
v0.13.0,test can run score
v0.13.0,test cate_feature_names
v0.13.0,test can run shap values
v0.13.0,"dgp (binary T, binary Z)"
v0.13.0,no heterogeneity
v0.13.0,with heterogeneity
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Set random seed
v0.13.0,Generate data
v0.13.0,DGP constants
v0.13.0,Test data
v0.13.0,Constant treatment effect
v0.13.0,Constant treatment with multi output Y
v0.13.0,Heterogeneous treatment
v0.13.0,Heterogeneous treatment with multi output Y
v0.13.0,TLearner test
v0.13.0,Instantiate TLearner
v0.13.0,Test inputs
v0.13.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.13.0,Instantiate SLearner
v0.13.0,Test inputs
v0.13.0,Test constant treatment effect
v0.13.0,Test constant treatment effect with multi output Y
v0.13.0,Test heterogeneous treatment effect
v0.13.0,Need interactions between T and features
v0.13.0,Test heterogeneous treatment effect with multi output Y
v0.13.0,Instantiate XLearner
v0.13.0,Test inputs
v0.13.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.13.0,Instantiate DomainAdaptationLearner
v0.13.0,Test inputs
v0.13.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.13.0,Get the true treatment effect
v0.13.0,Get the true treatment effect
v0.13.0,Fit learner and get the effect and marginal effect
v0.13.0,Compute treatment effect residuals (absolute)
v0.13.0,Check that at least 90% of predictions are within tolerance interval
v0.13.0,Check whether the output shape is right
v0.13.0,Check that one can pass in regular lists
v0.13.0,Check that it fails correctly if lists of different shape are passed in
v0.13.0,"Check that it works when T, Y have shape (n, 1)"
v0.13.0,Generate covariates
v0.13.0,Generate treatment
v0.13.0,Calculate outcome
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,DGP constants
v0.13.0,Generate data
v0.13.0,Test data
v0.13.0,Remove warnings that might be raised by the models passed into the ORF
v0.13.0,Generate data with continuous treatments
v0.13.0,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.13.0,does not work well with parallelism.
v0.13.0,Test inputs for continuous treatments
v0.13.0,--> Check that one can pass in regular lists
v0.13.0,--> Check that it fails correctly if lists of different shape are passed in
v0.13.0,Check that outputs have the correct shape
v0.13.0,Test continuous treatments with controls
v0.13.0,Test continuous treatments without controls
v0.13.0,Generate data with binary treatments
v0.13.0,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.13.0,does not work well with parallelism.
v0.13.0,Test inputs for binary treatments
v0.13.0,--> Check that one can pass in regular lists
v0.13.0,--> Check that it fails correctly if lists of different shape are passed in
v0.13.0,"--> Check that it works when T, Y have shape (n, 1)"
v0.13.0,"--> Check that it fails correctly when T has shape (n, 2)"
v0.13.0,--> Check that it fails correctly when the treatments are not numeric
v0.13.0,Check that outputs have the correct shape
v0.13.0,Test binary treatments with controls
v0.13.0,Test binary treatments without controls
v0.13.0,Only applicable to continuous treatments
v0.13.0,Generate data for 2 treatments
v0.13.0,Test multiple treatments with controls
v0.13.0,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.13.0,The rest for controls. Just as an example.
v0.13.0,Generating A/B test data
v0.13.0,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.13.0,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.13.0,Create a wrapper around Lasso that doesn't support weights
v0.13.0,since Lasso does natively support them starting in sklearn 0.23
v0.13.0,Generate data with continuous treatments
v0.13.0,Instantiate model with most of the default parameters
v0.13.0,Compute the treatment effect on test points
v0.13.0,Compute treatment effect residuals
v0.13.0,Multiple treatments
v0.13.0,Allow at most 10% test points to be outside of the tolerance interval
v0.13.0,Compute treatment effect residuals
v0.13.0,Multiple treatments
v0.13.0,Allow at most 20% test points to be outside of the confidence interval
v0.13.0,Check that the intervals are not too wide
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.13.0,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.13.0,ensure that we've got at least 6 of every element
v0.13.0,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.13.0,NOTE: this number may need to change if the default number of folds in
v0.13.0,WeightedStratifiedKFold changes
v0.13.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.13.0,ensure we can serialize the unfit estimator
v0.13.0,ensure we can pickle the fit estimator
v0.13.0,make sure we can call the marginal_effect and effect methods
v0.13.0,test const marginal inference
v0.13.0,test effect inference
v0.13.0,test marginal effect inference
v0.13.0,test coef__inference and intercept__inference
v0.13.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.0,"make sure we can call effect with implied scalar treatments,"
v0.13.0,"no matter the dimensions of T, and also that we warn when there"
v0.13.0,are multiple treatments
v0.13.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.0,ensure that we've got at least two of every element
v0.13.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.13.0,make sure we can call the marginal_effect and effect methods
v0.13.0,test const marginal inference
v0.13.0,test effect inference
v0.13.0,test marginal effect inference
v0.13.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.13.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.13.0,We concatenate the two copies data
v0.13.0,make sure we can get out post-fit stuff
v0.13.0,create a simple artificial setup where effect of moving from treatment
v0.13.0,"1 -> 2 is 2,"
v0.13.0,"1 -> 3 is 1, and"
v0.13.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.13.0,"Using an uneven number of examples from different classes,"
v0.13.0,"and having the treatments in non-lexicographic order,"
v0.13.0,Should rule out some basic issues.
v0.13.0,test that we can fit with a KFold instance
v0.13.0,test that we can fit with a train/test iterable
v0.13.0,predetermined splits ensure that all features are seen in each split
v0.13.0,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.13.0,(incorrectly) use a final model with an intercept
v0.13.0,"Because final model is fixed, actual values of T and Y don't matter"
v0.13.0,Ensure reproducibility
v0.13.0,Sparse DGP
v0.13.0,Treatment effect coef
v0.13.0,Other coefs
v0.13.0,Features and controls
v0.13.0,Test sparse estimator
v0.13.0,"--> test coef_, intercept_"
v0.13.0,--> test treatment effects
v0.13.0,Restrict x_test to vectors of norm < 1
v0.13.0,--> check inference
v0.13.0,Check that a majority of true effects lie in the 5-95% CI
v0.13.0,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.13.0,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.13.0,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.13.0,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.13.0,sparse test case: heterogeneous effect by product
v0.13.0,need at least as many rows in e_y as there are distinct columns
v0.13.0,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.13.0,create a simple artificial setup where effect of moving from treatment
v0.13.0,"a -> b is 2,"
v0.13.0,"a -> c is 1, and"
v0.13.0,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.13.0,"Using an uneven number of examples from different classes,"
v0.13.0,"and having the treatments in non-lexicographic order,"
v0.13.0,should rule out some basic issues.
v0.13.0,Note that explicitly specifying the dtype as object is necessary until
v0.13.0,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.13.0,estimated effects should be identical when treatment is explicitly given
v0.13.0,but const_marginal_effect should be reordered based on the explicit cagetories
v0.13.0,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.13.0,test outer grouping
v0.13.0,test nested grouping
v0.13.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.13.0,whichever groups we saw
v0.13.0,test nested grouping
v0.13.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.13.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.13.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Set random seed
v0.13.0,Generate data
v0.13.0,DGP constants
v0.13.0,Test data
v0.13.0,Constant treatment effect and propensity
v0.13.0,Heterogeneous treatment and propensity
v0.13.0,ensure that we've got at least two of every element
v0.13.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.13.0,ensure that we can serialize unfit estimator
v0.13.0,ensure that we can serialize fit estimator
v0.13.0,make sure we can call the marginal_effect and effect methods
v0.13.0,test const marginal inference
v0.13.0,test effect inference
v0.13.0,test marginal effect inference
v0.13.0,test coef_ and intercept_ inference
v0.13.0,verify we can generate the summary
v0.13.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.13.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.13.0,create a simple artificial setup where effect of moving from treatment
v0.13.0,"1 -> 2 is 2,"
v0.13.0,"1 -> 3 is 1, and"
v0.13.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.13.0,"Using an uneven number of examples from different classes,"
v0.13.0,"and having the treatments in non-lexicographic order,"
v0.13.0,Should rule out some basic issues.
v0.13.0,test that we can fit with a KFold instance
v0.13.0,test that we can fit with a train/test iterable
v0.13.0,"for at least some of the examples, the CI should have nonzero width"
v0.13.0,"for at least some of the examples, the CI should have nonzero width"
v0.13.0,"for at least some of the examples, the CI should have nonzero width"
v0.13.0,test coef__inference function works
v0.13.0,test intercept__inference function works
v0.13.0,test summary function works
v0.13.0,Test inputs
v0.13.0,self._test_inputs(DR_learner)
v0.13.0,Test constant treatment effect
v0.13.0,Test heterogeneous treatment effect
v0.13.0,Test heterogenous treatment effect for W =/= None
v0.13.0,Sparse DGP
v0.13.0,Treatment effect coef
v0.13.0,Other coefs
v0.13.0,Features and controls
v0.13.0,Test sparse estimator
v0.13.0,"--> test coef_, intercept_"
v0.13.0,--> test treatment effects
v0.13.0,Restrict x_test to vectors of norm < 1
v0.13.0,--> check inference
v0.13.0,Check that a majority of true effects lie in the 5-95% CI
v0.13.0,test outer grouping
v0.13.0,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.13.0,test nested grouping
v0.13.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.13.0,whichever groups we saw
v0.13.0,test nested grouping
v0.13.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.13.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.13.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.13.0,helper class
v0.13.0,Fit learner and get the effect
v0.13.0,Get the true treatment effect
v0.13.0,Compute treatment effect residuals (absolute)
v0.13.0,Check that at least 90% of predictions are within tolerance interval
v0.13.0,Only for heterogeneous TE
v0.13.0,Fit learner on X and W and get the effect
v0.13.0,Get the true treatment effect
v0.13.0,Compute treatment effect residuals (absolute)
v0.13.0,Check that at least 90% of predictions are within tolerance interval
v0.13.0,Check that one can pass in regular lists
v0.13.0,Check that it fails correctly if lists of different shape are passed in
v0.13.0,Check that it fails when T contains values other than 0 and 1
v0.13.0,"Check that it works when T, Y have shape (n, 1)"
v0.13.0,Generate covariates
v0.13.0,Generate treatment
v0.13.0,Calculate outcome
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,DGP constants
v0.13.0,DGP coefficients
v0.13.0,Generated outcomes
v0.13.0,################
v0.13.0,WeightedLasso #
v0.13.0,################
v0.13.0,Define weights
v0.13.0,Define extended datasets
v0.13.0,Range of alphas
v0.13.0,Compare with Lasso
v0.13.0,--> No intercept
v0.13.0,--> With intercept
v0.13.0,When DGP has no intercept
v0.13.0,When DGP has intercept
v0.13.0,--> Coerce coefficients to be positive
v0.13.0,--> Toggle max_iter & tol
v0.13.0,Define weights
v0.13.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.13.0,Mixed DGP scenario.
v0.13.0,Define extended datasets
v0.13.0,Define weights
v0.13.0,Define multioutput
v0.13.0,##################
v0.13.0,WeightedLassoCV #
v0.13.0,##################
v0.13.0,Define alphas to test
v0.13.0,Compare with LassoCV
v0.13.0,--> No intercept
v0.13.0,--> With intercept
v0.13.0,--> Force parameters to be positive
v0.13.0,Choose a smaller n to speed-up process
v0.13.0,Compare fold weights
v0.13.0,Define weights
v0.13.0,Define extended datasets
v0.13.0,Define splitters
v0.13.0,WeightedKFold splitter
v0.13.0,Map weighted splitter to an extended splitter
v0.13.0,Define alphas to test
v0.13.0,Compare with LassoCV
v0.13.0,--> No intercept
v0.13.0,--> With intercept
v0.13.0,--> Force parameters to be positive
v0.13.0,###########################
v0.13.0,MultiTaskWeightedLassoCV #
v0.13.0,###########################
v0.13.0,Define alphas to test
v0.13.0,Define splitter
v0.13.0,Compare with MultiTaskLassoCV
v0.13.0,--> No intercept
v0.13.0,--> With intercept
v0.13.0,Define weights
v0.13.0,Define extended datasets
v0.13.0,Define splitters
v0.13.0,WeightedKFold splitter
v0.13.0,Map weighted splitter to an extended splitter
v0.13.0,Define alphas to test
v0.13.0,Compare with LassoCV
v0.13.0,--> No intercept
v0.13.0,--> With intercept
v0.13.0,#########################
v0.13.0,WeightedLassoCVWrapper #
v0.13.0,#########################
v0.13.0,perform 1D fit
v0.13.0,perform 2D fit
v0.13.0,################
v0.13.0,DebiasedLasso #
v0.13.0,################
v0.13.0,Test DebiasedLasso without weights
v0.13.0,--> Check debiased coeffcients without intercept
v0.13.0,--> Check debiased coeffcients with intercept
v0.13.0,--> Check 5-95 CI coverage for unit vectors
v0.13.0,Test DebiasedLasso with weights for one DGP
v0.13.0,Define weights
v0.13.0,Define extended datasets
v0.13.0,--> Check debiased coefficients
v0.13.0,Define weights
v0.13.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.13.0,--> Check debiased coeffcients
v0.13.0,Test that attributes propagate correctly
v0.13.0,Test MultiOutputDebiasedLasso without weights
v0.13.0,--> Check debiased coeffcients without intercept
v0.13.0,--> Check debiased coeffcients with intercept
v0.13.0,--> Check CI coverage
v0.13.0,Test MultiOutputDebiasedLasso with weights
v0.13.0,Define weights
v0.13.0,Define extended datasets
v0.13.0,--> Check debiased coefficients
v0.13.0,Unit vectors
v0.13.0,Unit vectors
v0.13.0,Check coeffcients and intercept are the same within tolerance
v0.13.0,Check results are similar with tolerance 1e-6
v0.13.0,Check if multitask
v0.13.0,Check that same alpha is chosen
v0.13.0,Check that the coefficients are similar
v0.13.0,selective ridge has a simple implementation that we can test against
v0.13.0,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.13.0,"it should be the case that when we set fit_intercept to true,"
v0.13.0,it doesn't matter whether the penalized model also fits an intercept or not
v0.13.0,create an extra copy of rows with weight 2
v0.13.0,"instead of a slice, explicitly return an array of indices"
v0.13.0,_penalized_inds is only set during fitting
v0.13.0,cv exists on penalized model
v0.13.0,now we can access _penalized_inds
v0.13.0,check that we can read the cv attribute back out from the underlying model
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"global and cohort data should have exactly the same structure, but different values"
v0.13.0,local index should have as many times entries as global as there were rows passed in
v0.13.0,continuous treatments have typical treatment values equal to
v0.13.0,the mean of the absolute value of non-zero entries
v0.13.0,discrete treatments have typical treatment value 1
v0.13.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.13.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.13.0,policy value should exceed always treating with any treatment
v0.13.0,"global shape is (d_y, sum(d_t))"
v0.13.0,global and cohort row-wise dicts have d_y * d_t entries
v0.13.0,local dictionary is flattened to n_rows * d_y * d_t
v0.13.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.0,features; for categoricals they should appear #cats-1 times each
v0.13.0,"global and cohort data should have exactly the same structure, but different values"
v0.13.0,local index should have as many times entries as global as there were rows passed in
v0.13.0,features; for categoricals they should appear #cats-1 times each
v0.13.0,"global shape is (d_y, sum(d_t))"
v0.13.0,global and cohort row-wise dicts have d_y * d_t entries
v0.13.0,local dictionary is flattened to n_rows * d_y * d_t
v0.13.0,continuous treatments have typical treatment values equal to
v0.13.0,the mean of the absolute value of non-zero entries
v0.13.0,discrete treatments have typical treatment value 1
v0.13.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.13.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.13.0,policy value should exceed always treating with any treatment
v0.13.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.0,"global and cohort data should have exactly the same structure, but different values"
v0.13.0,local index should have as many times entries as global as there were rows passed in
v0.13.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.13.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.13.0,policy value should exceed always treating with any treatment
v0.13.0,"global shape is (d_y, sum(d_t))"
v0.13.0,global and cohort row-wise dicts have d_y * d_t entries
v0.13.0,local dictionary is flattened to n_rows * d_y * d_t
v0.13.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.0,features; for categoricals they should appear #cats-1 times each
v0.13.0,make sure we don't run into problems dropping every index
v0.13.0,"global and cohort data should have exactly the same structure, but different values"
v0.13.0,local index should have as many times entries as global as there were rows passed in
v0.13.0,"global shape is (d_y, sum(d_t))"
v0.13.0,global and cohort row-wise dicts have d_y * d_t entries
v0.13.0,local dictionary is flattened to n_rows * d_y * d_t
v0.13.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.13.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.13.0,policy value should exceed always treating with any treatment
v0.13.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.13.0,"global and cohort data should have exactly the same structure, but different values"
v0.13.0,local index should have as many times entries as global as there were rows passed in
v0.13.0,features; for categoricals they should appear #cats-1 times each
v0.13.0,"global shape is (d_y, sum(d_t))"
v0.13.0,global and cohort row-wise dicts have d_y * d_t entries
v0.13.0,local dictionary is flattened to n_rows * d_y * d_t
v0.13.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.13.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.13.0,policy value should exceed always treating with any treatment
v0.13.0,dgp
v0.13.0,model
v0.13.0,model
v0.13.0,"columns 'd', 'e', 'h' have too many values"
v0.13.0,"columns 'd', 'e' have too many values"
v0.13.0,lowering bound shouldn't affect already fit columns when warm starting
v0.13.0,"column d is now okay, too"
v0.13.0,verify that we can use a scalar treatment cost
v0.13.0,verify that we can specify per-treatment costs for each sample
v0.13.0,verify that using the same state returns the same results each time
v0.13.0,set the categories for column 'd' explicitly so that b is default
v0.13.0,"first column: 10 ones, this is fine"
v0.13.0,"second column: 6 categories, plenty of random instances of each"
v0.13.0,this is fine only if we increase the cateogry limit
v0.13.0,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.13.0,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.13.0,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.13.0,forest heterogeneity won't work
v0.13.0,"sixth column: just 1 one, not enough even without check"
v0.13.0,increase bound on cat expansion
v0.13.0,skip checks (reducing folds accordingly)
v0.13.0,"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
v0.13.0,"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
v0.13.0,Pass an example where W is irrelevant and X is confounder
v0.13.0,"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
v0.13.0,zeroed out and the test will fail
v0.13.0,"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
v0.13.0,rescaling X shouldn't affect the first stage models because they normalize the inputs
v0.13.0,"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
v0.13.0,cross terms
v0.13.0,scale by 1000 to match the input to this model:
v0.13.0,"the scale of X does matter for the final model, which keeps results in user-denominated units"
v0.13.0,rescaling X still shouldn't affect the first stage models
v0.13.0,TODO: we don't recover the correct values with enough accuracy to enable this assertion
v0.13.0,is there a different way to verify that we are learning the correct coefficients?
v0.13.0,"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,DGP constants
v0.13.0,Define data features
v0.13.0,Added `_df`to names to be different from the default cate_estimator names
v0.13.0,Generate data
v0.13.0,################################
v0.13.0,Single treatment and outcome #
v0.13.0,################################
v0.13.0,Test LinearDML
v0.13.0,|--> Test featurizers
v0.13.0,"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
v0.13.0,|--> Test re-fit
v0.13.0,Test SparseLinearDML
v0.13.0,Test ForestDML
v0.13.0,###################################
v0.13.0,Mutiple treatments and outcomes #
v0.13.0,###################################
v0.13.0,Test LinearDML
v0.13.0,Test SparseLinearDML
v0.13.0,"Single outcome only, ORF does not support multiple outcomes"
v0.13.0,Test DMLOrthoForest
v0.13.0,Test DROrthoForest
v0.13.0,Test XLearner
v0.13.0,Skipping population summary names test because bootstrap inference is too slow
v0.13.0,Test SLearner
v0.13.0,Test TLearner
v0.13.0,Test LinearDRLearner
v0.13.0,Test SparseLinearDRLearner
v0.13.0,Test ForestDRLearner
v0.13.0,Test LinearIntentToTreatDRIV
v0.13.0,Test DeepIV
v0.13.0,Test categorical treatments
v0.13.0,Check refit
v0.13.0,Check refit after setting categories
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Linear models are required for parametric dml
v0.13.0,sample weighting models are required for nonparametric dml
v0.13.0,Test values
v0.13.0,TLearner test
v0.13.0,Instantiate TLearner
v0.13.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.13.0,Test constant treatment effect with multi output Y
v0.13.0,Test heterogeneous treatment effect
v0.13.0,Need interactions between T and features
v0.13.0,Test heterogeneous treatment effect with multi output Y
v0.13.0,Instantiate DomainAdaptationLearner
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,test base values equals to mean of constant marginal effect
v0.13.0,test shape of shap values output is as expected
v0.13.0,test shape of attribute of explanation object is as expected
v0.13.0,test length of feature names equals to shap values shape
v0.13.0,test base values equals to mean of constant marginal effect
v0.13.0,test shape of shap values output is as expected
v0.13.0,test shape of attribute of explanation object is as expected
v0.13.0,test length of feature names equals to shap values shape
v0.13.0,Treatment effect function
v0.13.0,Outcome support
v0.13.0,Treatment support
v0.13.0,"Generate controls, covariates, treatments and outcomes"
v0.13.0,Heterogeneous treatment effects
v0.13.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.13.0,through shap package.
v0.13.0,test shap could generate the plot from the shap_values
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Check inputs
v0.13.0,Check inputs
v0.13.0,Check inputs
v0.13.0,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.13.0,"Thus, we append the controls column before the one-hot-encoded T"
v0.13.0,"We might want to revisit, though, since it's linearly determined by the others"
v0.13.0,Check inputs
v0.13.0,Check inputs
v0.13.0,Estimate response function
v0.13.0,Check inputs
v0.13.0,Train model on controls. Assign higher weight to units resembling
v0.13.0,treated units.
v0.13.0,Train model on the treated. Assign higher weight to units resembling
v0.13.0,control units.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,TODO: make sure to use random seeds wherever necessary
v0.13.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.13.0,"unfortunately with the Theano and Tensorflow backends,"
v0.13.0,the straightforward use of K.stop_gradient can cause an error
v0.13.0,because the parameters of the intermediate layers are now disconnected from the loss;
v0.13.0,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.13.0,so that those layers remain connected but with 0 gradient
v0.13.0,|| t - mu_i || ^2
v0.13.0,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.13.0,Use logsumexp for numeric stability:
v0.13.0,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.13.0,TODO: does the numeric stability actually make any difference?
v0.13.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.13.0,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.13.0,generate cumulative sum via matrix multiplication
v0.13.0,"Generate standard uniform values in shape (batch_size,1)"
v0.13.0,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.13.0,we use uniform_like instead with an input of an appropriate shape)
v0.13.0,convert to floats and multiply to perform equivalent of logical AND
v0.13.0,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.13.0,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.13.0,we use normal_like instead with an input of an appropriate shape)
v0.13.0,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.13.0,prevent gradient from passing through sampling
v0.13.0,three options: biased or upper-bound loss require a single number of samples;
v0.13.0,unbiased can take different numbers for the network and its gradient
v0.13.0,"sample: (() -> Layer, int) -> Layer"
v0.13.0,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.13.0,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.13.0,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.13.0,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.13.0,the dimensionality of the output of the network
v0.13.0,TODO: is there a more robust way to do this?
v0.13.0,TODO: do we need to give the user more control over other arguments to fit?
v0.13.0,"subtle point: we need to build a new model each time,"
v0.13.0,because each model encapsulates its randomness
v0.13.0,TODO: do we need to give the user more control over other arguments to fit?
v0.13.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.13.0,not a general tensor (because of how backprop works in every framework)
v0.13.0,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.13.0,but this seems annoying...)
v0.13.0,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.13.0,TODO: any way to get this to work on batches of arbitrary size?
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.13.0,"fit on projected Z: E[T * E[T|X,Z]|X]"
v0.13.0,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.13.0,"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
v0.13.0,"shape (n,)"
v0.13.0,"shape (n,)"
v0.13.0,"shape(n,)"
v0.13.0,TODO: prel_model_effect could allow sample_var and freq_weight?
v0.13.0,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.13.0,"we need to undo the one-hot encoding for calling effect,"
v0.13.0,since it expects raw values
v0.13.0,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.13.0,target will be discrete and will be inversed from FirstStageWrapper
v0.13.0,"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
v0.13.0,reshape the predictions
v0.13.0,concat W and Z
v0.13.0,check nuisances outcome shape
v0.13.0,Y_res could be a vector or 1-dimensional 2d-array
v0.13.0,"all could be reshaped to vector since Y, T, Z are all single dimensional."
v0.13.0,Estimate final model of theta(X) by minimizing the square loss:
v0.13.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.13.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.13.0,at the expense of some small bias. For points with very small covariance we revert
v0.13.0,to the model-based preliminary estimate and do not add the correction term.
v0.13.0,A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
v0.13.0,Used by both DRIV and IntentToTreatDRIV.
v0.13.0,Maggie: I think that would be the case?
v0.13.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.13.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.13.0,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.13.0,attribute so that the trained featurizer will be passed through
v0.13.0,Handles the corner case when X=None but featurizer might be not None
v0.13.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.13.0,this is a regression model since proj_t is probability
v0.13.0,outcome is continuous since proj_t is probability
v0.13.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.13.0,TODO: support freq_weight and sample_var in debiased lasso
v0.13.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.13.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.13.0,concat W and Z
v0.13.0,"we need to undo the one-hot encoding for calling effect,"
v0.13.0,since it expects raw values
v0.13.0,concat W and Z
v0.13.0,"we need to undo the one-hot encoding for calling effect,"
v0.13.0,since it expects raw values
v0.13.0,reshape the predictions
v0.13.0,"T_res, Z_res, beta expect shape to be (n,1)"
v0.13.0,maybe shouldn't expose fit_cate_intercept in this class?
v0.13.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.13.0,TODO: do correct adjustment for sample_var
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,concat W and Z
v0.13.0,concat W and Z
v0.13.0,concat W and Z
v0.13.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.13.0,"train E[T|X,W,Z]"
v0.13.0,"train [Z|X,W]"
v0.13.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.13.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.13.0,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.13.0,attribute so that the trained featurizer will be passed through
v0.13.0,Handles the corner case when X=None but featurizer might be not None
v0.13.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.13.0,concat W and Z
v0.13.0,note that groups are not passed to score because they are only used for fitting
v0.13.0,concat W and Z
v0.13.0,note that sample_weight and groups are not passed to predict because they are only used for fitting
v0.13.0,concat W and Z
v0.13.0,A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
v0.13.0,Used by both Parametric and Non Parametric DMLIV.
v0.13.0,override only so that we can enforce Z to be required
v0.13.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.13.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.13.0,for internal use by the library
v0.13.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.13.0,Handles the corner case when X=None but featurizer might be not None
v0.13.0,Get input names
v0.13.0,Summary
v0.13.0,coefficient
v0.13.0,intercept
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,"this will have dimension (d,) + shape(X)"
v0.13.0,send the first dimension to the end
v0.13.0,columns are featurized independently; partial derivatives are only non-zero
v0.13.0,when taken with respect to the same column each time
v0.13.0,don't fit intercept; manually add column of ones to the data instead;
v0.13.0,this allows us to ignore the intercept when computing marginal effects
v0.13.0,make T 2D if if was a vector
v0.13.0,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.13.0,two stage approximation
v0.13.0,"first, get basis expansions of T, X, and Z"
v0.13.0,TODO: is it right that the effective number of intruments is the
v0.13.0,"product of ft_X and ft_Z, not just ft_Z?"
v0.13.0,"regress T expansion on X,Z expansions concatenated with W"
v0.13.0,"predict ft_T from interacted ft_X, ft_Z"
v0.13.0,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.13.0,dT may be only 2-dimensional)
v0.13.0,promote dT to 3D if necessary (e.g. if T was a vector)
v0.13.0,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,TODO: this utility is documented but internal; reimplement?
v0.13.0,TODO: this utility is even less public...
v0.13.0,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.13.0,use same Cs as would be used by default by LogisticRegressionCV
v0.13.0,NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
v0.13.0,which could affect how many times each distinct Y value needs to be present in the data
v0.13.0,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.13.0,but also supports get_feature_names with expected signature
v0.13.0,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.13.0,NOTE: we rely on the passthrough columns coming first in the concatenated X;W
v0.13.0,"when we pipeline scaling with our first stage models later, so the order here is important"
v0.13.0,Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
v0.13.0,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.13.0,Convert SingleTreeInterpreter to a python dictionary
v0.13.0,named tuple type for storing results inside CausalAnalysis class;
v0.13.0,must be lifted to module level to enable pickling
v0.13.0,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
v0.13.0,"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
v0.13.0,"when running the first stage models, but don't want to scale the X columns when running the final model,"
v0.13.0,since then our coefficients will have odd units and our trees will also have decisions using those units.
v0.13.0,
v0.13.0,"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
v0.13.0,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.13.0,Controls are all other columns of X
v0.13.0,"can't use X[:, feat_ind] when X is a DataFrame"
v0.13.0,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.13.0,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.13.0,so that the user can opt-in to allowing unseen treatment values
v0.13.0,(and return NaN or something in that case)
v0.13.0,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
v0.13.0,and so we can just peel the first columns off of that combined array for rescaling in the pipeline
v0.13.0,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
v0.13.0,"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
v0.13.0,becomes a valid approach to handling this
v0.13.0,array checking routines don't accept 0-width arrays
v0.13.0,perform model selection
v0.13.0,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.13.0,convert to NormalInferenceResults for consistency
v0.13.0,Set the dictionary values shared between local and global summaries
v0.13.0,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.13.0,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.13.0,required to fit a discrete DML model
v0.13.0,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
v0.13.0,sub-cases of models or also integrate with azure autoML. (post-MVP)
v0.13.0,"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
v0.13.0,"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
v0.13.0,TODO: Enable multi-class classification (post-MVP)
v0.13.0,Validate inputs
v0.13.0,TODO: check compatibility of X and Y lengths
v0.13.0,"no previous fit, cancel warm start"
v0.13.0,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.13.0,"if heterogeneity_inds is 1D, repeat it"
v0.13.0,heterogeneity inds should be a 2D list of length same as train_inds
v0.13.0,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.13.0,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.13.0,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.13.0,train the Y model
v0.13.0,"perform model selection for the Y model using all X, not on a per-column basis"
v0.13.0,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.13.0,work with the regression wrapper
v0.13.0,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.13.0,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.13.0,since otherwise we'll have too many columns to be able to train a classifier
v0.13.0,start with empty results and default shared insights
v0.13.0,convert categorical indicators to numeric indices
v0.13.0,check for indices over the categorical expansion bound
v0.13.0,assume we'll be able to train former failures this time; we'll add them back if not
v0.13.0,"can't remove in place while iterating over new_inds, so store in separate list"
v0.13.0,"train the model, but warn"
v0.13.0,no model can be trained in this case since we need more folds
v0.13.0,"don't train a model, but suggest workaround since there are enough instances of least"
v0.13.0,populated class
v0.13.0,also remove from train_inds so we don't try to access the result later
v0.13.0,extract subset of names matching new columns
v0.13.0,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.13.0,don't want to cache this failed result
v0.13.0,properties to return from effect InferenceResults
v0.13.0,properties to return from PopulationSummaryResults
v0.13.0,Converts strings to property lookups or method calls as a convenience so that the
v0.13.0,_point_props and _summary_props above can be applied to an inference object
v0.13.0,Create a summary combining all results into a single output; this is used
v0.13.0,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.13.0,"or a dictionary, respectively, based on the summary function passed into this method"
v0.13.0,"ensure array has shape (m,y,t)"
v0.13.0,population summary is missing sample dimension; add it for consistency
v0.13.0,outcome dimension is missing; add it for consistency
v0.13.0,add singleton treatment dimension if missing
v0.13.0,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.13.0,"each attr has dimension (m,y) or (m,y,t)"
v0.13.0,concatenate along treatment dimension
v0.13.0,"for dictionary representation, want to remove unneeded sample dimension"
v0.13.0,in cohort and global results
v0.13.0,TODO: enrich outcome logic for multi-class classification when that is supported
v0.13.0,There is no actual sample level in this data
v0.13.0,can't drop only level
v0.13.0,should be serialization-ready and contain no numpy arrays
v0.13.0,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.13.0,TODO: Note that there's no column metadata for the sample number - should there be?
v0.13.0,"need to replicate the column info for each sample, then remove from the shared data"
v0.13.0,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.13.0,which may need to be revisited once we support multiclass
v0.13.0,get the length of the list corresponding to the first dictionary key
v0.13.0,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.13.0,a global inference indicates the effect of that one feature on the outcome
v0.13.0,need to reshape the output to match the input
v0.13.0,we want to offset the inference object by the baseline estimate of y
v0.13.0,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.13.0,get the length of the list corresponding to the first dictionary key
v0.13.0,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.13.0,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.13.0,because then scaling the difference between treatment value and treatment costs is the
v0.13.0,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.13.0,
v0.13.0,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.13.0,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.13.0,(rather than just not treating at all)
v0.13.0,
v0.13.0,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.13.0,2 * policy_value - always_treat
v0.13.0,includes exactly their contribution because policy_value and always_treat both include it
v0.13.0,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.13.0,2 * policy_value - always-treat
v0.13.0,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.13.0,is zero and the contribution to always_treat is negative
v0.13.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.13.0,"however, the tree can't store the feature and treatment names we compute here..."
v0.13.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.13.0,"however, the tree can't store the feature and treatment names we compute here..."
v0.13.0,get dataframe with all but selected column
v0.13.0,apply 10% of a typical treatment for this feature
v0.13.0,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.13.0,set the effect bounds; for positive treatments these agree with
v0.13.0,"the estimates; for negative treatments, we need to invert the interval"
v0.13.0,the effect is now always positive since we decrease treatment when negative
v0.13.0,"for discrete treatment, stack a zero result in front for control"
v0.13.0,we need to call effect_inference to get the correct CI between the two treatment options
v0.13.0,we now need to construct the delta in the cost between the two treatments and translate the effect
v0.13.0,remove third dimenions potentially added
v0.13.0,"find cost of current treatment: equality creates a 2d array with True on each row,"
v0.13.0,only if its the location of the current treatment. Then we take the corresponding cost.
v0.13.0,construct index of current treatment
v0.13.0,add second dimension if needed for broadcasting during translation of effect
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,TODO: conisder working around relying on sklearn implementation details
v0.13.0,"Found a good split, return."
v0.13.0,Record all splits in case the stratification by weight yeilds a worse partition
v0.13.0,Reseed random generator and try again
v0.13.0,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.13.0,"Found a good split, return."
v0.13.0,Did not find a good split
v0.13.0,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.13.0,Return most weight-balanced partition
v0.13.0,Weight stratification algorithm
v0.13.0,Sort weights for weight strata search
v0.13.0,There are some leftover indices that have yet to be assigned
v0.13.0,Append stratum splits to overall splits
v0.13.0,"If classification methods produce multiple columns of output,"
v0.13.0,we need to manually encode classes to ensure consistent column ordering.
v0.13.0,We clone the estimator to make sure that all the folds are
v0.13.0,"independent, and that it is pickle-able."
v0.13.0,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.13.0,`predictions` is a list of method outputs from each fold.
v0.13.0,"If each of those is also a list, then treat this as a"
v0.13.0,multioutput-multiclass task. We need to separately concatenate
v0.13.0,the method outputs for each label into an `n_labels` long list.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Our classes that derive from sklearn ones sometimes include
v0.13.0,inherited docstrings that have embedded doctests; we need the following imports
v0.13.0,so that they don't break.
v0.13.0,TODO: consider working around relying on sklearn implementation details
v0.13.0,"Convert X, y into numpy arrays"
v0.13.0,Define fit parameters
v0.13.0,Some algorithms don't have a check_input option
v0.13.0,Check weights array
v0.13.0,Check that weights are size-compatible
v0.13.0,Normalize inputs
v0.13.0,Weight inputs
v0.13.0,Fit base class without intercept
v0.13.0,Fit Lasso
v0.13.0,Reset intercept
v0.13.0,The intercept is not calculated properly due the sqrt(weights) factor
v0.13.0,so it must be recomputed
v0.13.0,Fit lasso without weights
v0.13.0,Make weighted splitter
v0.13.0,Fit weighted model
v0.13.0,Make weighted splitter
v0.13.0,Fit weighted model
v0.13.0,Call weighted lasso on reduced design matrix
v0.13.0,Weighted tau
v0.13.0,Select optimal penalty
v0.13.0,Warn about consistency
v0.13.0,"Convert X, y into numpy arrays"
v0.13.0,Fit weighted lasso with user input
v0.13.0,"Center X, y"
v0.13.0,Calculate quantities that will be used later on. Account for centered data
v0.13.0,Calculate coefficient and error variance
v0.13.0,Add coefficient correction
v0.13.0,Set coefficients and intercept standard errors
v0.13.0,Set intercept
v0.13.0,Return alpha to 'auto' state
v0.13.0,"Note that in the case of no intercept, X_offset is 0"
v0.13.0,Calculate the variance of the predictions
v0.13.0,Calculate prediction confidence intervals
v0.13.0,Assumes flattened y
v0.13.0,Compute weighted residuals
v0.13.0,To be done once per target. Assumes y can be flattened.
v0.13.0,Assumes that X has already been offset
v0.13.0,Special case: n_features=1
v0.13.0,Compute Lasso coefficients for the columns of the design matrix
v0.13.0,Compute C_hat
v0.13.0,Compute theta_hat
v0.13.0,Allow for single output as well
v0.13.0,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.13.0,Set coef_ attribute
v0.13.0,Set intercept_ attribute
v0.13.0,Set selected_alpha_ attribute
v0.13.0,Set coef_stderr_
v0.13.0,intercept_stderr_
v0.13.0,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.13.0,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.13.0,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.13.0,set intercept_ attribute
v0.13.0,set coef_ attribute
v0.13.0,set alpha_ attribute
v0.13.0,set alphas_ attribute
v0.13.0,set n_iter_ attribute
v0.13.0,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.13.0,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.13.0,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.13.0,now regress X1 on y - X2 * beta2 to learn beta1
v0.13.0,set coef_ and intercept_ attributes
v0.13.0,Note that the penalized model should *not* have an intercept
v0.13.0,don't proxy special methods
v0.13.0,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.13.0,regressor incorrectly
v0.13.0,"Note: for known attributes that have been set this method will not be called,"
v0.13.0,so we should just throw here because this is an attribute belonging to this class
v0.13.0,but which hasn't yet been set on this instance
v0.13.0,set default values for None
v0.13.0,check freq_weight should be integer and should be accompanied by sample_var
v0.13.0,check array shape
v0.13.0,weight X and y and sample_var
v0.13.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.13.0,set default values for None
v0.13.0,check array shape
v0.13.0,check dimension of instruments is more than dimension of treatments
v0.13.0,weight X and y
v0.13.0,learn point estimate
v0.13.0,solve first stage linear regression E[T|Z]
v0.13.0,"""that"" means T̂"
v0.13.0,solve second stage linear regression E[Y|that]
v0.13.0,(T̂.T*T̂)^{-1}
v0.13.0,learn cov(theta)
v0.13.0,(T̂.T*T̂)^{-1}
v0.13.0,sigma^2
v0.13.0,reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,AzureML
v0.13.0,helper imports
v0.13.0,write the details of the workspace to a configuration file to the notebook library
v0.13.0,if y is a multioutput model
v0.13.0,Make sure second dimension has 1 or more item
v0.13.0,switch _inner Model to a MultiOutputRegressor
v0.13.0,flatten array as automl only takes vectors for y
v0.13.0,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.13.0,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.13.0,as an sklearn estimator
v0.13.0,fit implementation for a single output model.
v0.13.0,Create experiment for specified workspace
v0.13.0,Configure automl_config with training set information.
v0.13.0,"Wait for remote run to complete, the set the model"
v0.13.0,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.13.0,create model and pass model into final.
v0.13.0,"If item is an automl config, get its corresponding"
v0.13.0,AutomatedML Model and add it to new_Args
v0.13.0,"If item is an automl config, get its corresponding"
v0.13.0,AutomatedML Model and set it for this key in
v0.13.0,kwargs
v0.13.0,takes in either automated_ml config and instantiates
v0.13.0,an AutomatedMLModel
v0.13.0,The prefix can only be 18 characters long
v0.13.0,"because prefixes come from kwarg_names, we must ensure they are"
v0.13.0,short enough.
v0.13.0,Get workspace from config file.
v0.13.0,Take the intersect of the white for sample
v0.13.0,weights and linear models
v0.13.0,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,average the outcome dimension if it exists and ensure 2d y_pred
v0.13.0,get index of best treatment
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,TODO: consider working around relying on sklearn implementation details
v0.13.0,Create splits of causal tree
v0.13.0,Make sure the correct exception is being rethrown
v0.13.0,Must make sure indices are merged correctly
v0.13.0,Convert rows to columns
v0.13.0,Require group assignment t to be one-hot-encoded
v0.13.0,Get predictions for the 2 splits
v0.13.0,Must make sure indices are merged correctly
v0.13.0,Crossfitting
v0.13.0,Compute weighted nuisance estimates
v0.13.0,-------------------------------------------------------------------------------
v0.13.0,Calculate the covariance matrix corresponding to the BLB inference
v0.13.0,
v0.13.0,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.13.0,2. Calculate the weighted moments for each tree slice to create a matrix
v0.13.0,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.13.0,in that slice from the overall parameter estimate.
v0.13.0,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.13.0,-------------------------------------------------------------------------------
v0.13.0,Calclulate covariance matrix through BLB
v0.13.0,Estimators
v0.13.0,OrthoForest parameters
v0.13.0,Sub-forests
v0.13.0,Auxiliary attributes
v0.13.0,Fit check
v0.13.0,TODO: Check performance
v0.13.0,Must normalize weights
v0.13.0,Override the CATE inference options
v0.13.0,Add blb inference to parent's options
v0.13.0,Generate subsample indices
v0.13.0,Build trees in parallel
v0.13.0,Bootstraping has repetitions in tree sample
v0.13.0,Similar for `a` weights
v0.13.0,Bootstraping has repetitions in tree sample
v0.13.0,Define subsample size
v0.13.0,Safety check
v0.13.0,Draw points to create little bags
v0.13.0,Copy and/or define models
v0.13.0,Define nuisance estimators
v0.13.0,Define parameter estimators
v0.13.0,Define
v0.13.0,Need to redefine fit here for auto inference to work due to a quirk in how
v0.13.0,wrap_fit is defined
v0.13.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.13.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.13.0,Override to flatten output if T is flat
v0.13.0,Check that all discrete treatments are represented
v0.13.0,Nuissance estimates evaluated with cross-fitting
v0.13.0,Define 2-fold iterator
v0.13.0,Check if there is only one example of some class
v0.13.0,Define 2-fold iterator
v0.13.0,need safe=False when cloning for WeightedModelWrapper
v0.13.0,Compute residuals
v0.13.0,Compute coefficient by OLS on residuals
v0.13.0,"Parameter returned by LinearRegression is (d_T, )"
v0.13.0,Compute residuals
v0.13.0,Compute coefficient by OLS on residuals
v0.13.0,ell_2 regularization
v0.13.0,Ridge regression estimate
v0.13.0,"Parameter returned is of shape (d_T, )"
v0.13.0,Return moments and gradients
v0.13.0,Compute residuals
v0.13.0,Compute moments
v0.13.0,"Moments shape is (n, d_T)"
v0.13.0,Compute moment gradients
v0.13.0,returns shape-conforming residuals
v0.13.0,Copy and/or define models
v0.13.0,Define parameter estimators
v0.13.0,Define moment and mean gradient estimator
v0.13.0,"Check that T is shape (n, )"
v0.13.0,Check T is numeric
v0.13.0,Train label encoder
v0.13.0,Call `fit` from parent class
v0.13.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.13.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.13.0,Override to flatten output if T is flat
v0.13.0,Expand one-hot encoding to include the zero treatment
v0.13.0,"Test that T contains all treatments. If not, return None"
v0.13.0,Nuissance estimates evaluated with cross-fitting
v0.13.0,Define 2-fold iterator
v0.13.0,Check if there is only one example of some class
v0.13.0,No need to crossfit for internal nodes
v0.13.0,Compute partial moments
v0.13.0,"If any of the values in the parameter estimate is nan, return None"
v0.13.0,Compute partial moments
v0.13.0,Compute coefficient by OLS on residuals
v0.13.0,ell_2 regularization
v0.13.0,Ridge regression estimate
v0.13.0,"Parameter returned is of shape (d_T, )"
v0.13.0,Return moments and gradients
v0.13.0,Compute partial moments
v0.13.0,Compute moments
v0.13.0,"Moments shape is (n, d_T-1)"
v0.13.0,Compute moment gradients
v0.13.0,Need to calculate this in an elegant way for when propensity is 0
v0.13.0,This will flatten T
v0.13.0,Check that T is numeric
v0.13.0,Test whether the input estimator is supported
v0.13.0,Calculate confidence intervals for the parameter (marginal effect)
v0.13.0,Calculate confidence intervals for the effect
v0.13.0,Calculate the effects
v0.13.0,Calculate the standard deviations for the effects
v0.13.0,d_t=None here since we measure the effect across all Ts
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Causal tree parameters
v0.13.0,Tree structure
v0.13.0,No need for a random split since the data is already
v0.13.0,a random subsample from the original input
v0.13.0,node list stores the nodes that are yet to be splitted
v0.13.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.13.0,Create local sample set
v0.13.0,Compute nuisance estimates for the current node
v0.13.0,Nuisance estimate cannot be calculated
v0.13.0,Estimate parameter for current node
v0.13.0,Node estimate cannot be calculated
v0.13.0,Calculate moments and gradient of moments for current data
v0.13.0,Calculate inverse gradient
v0.13.0,The gradient matrix is not invertible.
v0.13.0,No good split can be found
v0.13.0,Calculate point-wise pseudo-outcomes rho
v0.13.0,a split is determined by a feature and a sample pair
v0.13.0,the number of possible splits is at most (number of features) * (number of node samples)
v0.13.0,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.13.0,parse row and column of random pair
v0.13.0,the sample of the pair is the integer division of the random number with n_feats
v0.13.0,calculate the binary indicator of whether sample i is on the left or the right
v0.13.0,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.13.0,calculate the number of samples on the left child for each proposed split
v0.13.0,calculate the analogous binary indicator for the samples in the estimation set
v0.13.0,calculate the number of estimation samples on the left child of each proposed split
v0.13.0,find the upper and lower bound on the size of the left split for the split
v0.13.0,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.13.0,on each side.
v0.13.0,similarly for the estimation sample set
v0.13.0,if there is no valid split then don't create any children
v0.13.0,filter only the valid splits
v0.13.0,calculate the average influence vector of the samples in the left child
v0.13.0,calculate the average influence vector of the samples in the right child
v0.13.0,take the square of each of the entries of the influence vectors and normalize
v0.13.0,by size of each child
v0.13.0,calculate the vector score of each candidate split as the average of left and right
v0.13.0,influence vectors
v0.13.0,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.13.0,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.13.0,where there might be large discontinuities in some parameter as the conditioning set varies
v0.13.0,calculate the scalar score of each split by aggregating across the vector of scores
v0.13.0,Find split that minimizes criterion
v0.13.0,Create child nodes with corresponding subsamples
v0.13.0,add the created children to the list of not yet split nodes
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.13.0,Licensed under the MIT License.
v0.13.0,TODO: update docs
v0.13.0,"NOTE: sample weight, sample var are not passed in"
v0.13.0,Compose final model
v0.13.0,Calculate auxiliary quantities
v0.13.0,X ⨂ T_res
v0.13.0,"sum(model_final.predict(X, T_res))"
v0.13.0,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.13.0,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
v0.13.0,generate an instance of the final model
v0.13.0,generate an instance of the nuisance model
v0.13.0,Set _d_t to effective number of treatments
v0.13.0,Required for bootstrap inference
v0.13.0,for each mc iteration
v0.13.0,for each model under cross fit setting
v0.13.0,Handles the corner case when X=None but featurizer might be not None
v0.13.0,Expand treatments for each time period
v0.13.0,NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
v0.13.0,attribute so that the trained featurizer will be passed through
v0.13.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.13.0,for internal use by the library
v0.13.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.13.0,We need to use the _ortho_learner's copy to retain the information from fitting
v0.12.0,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.12.0,configuration is all pulled from setup.cfg
v0.12.0,-*- coding: utf-8 -*-
v0.12.0,
v0.12.0,Configuration file for the Sphinx documentation builder.
v0.12.0,
v0.12.0,This file does only contain a selection of the most common options. For a
v0.12.0,full list see the documentation:
v0.12.0,http://www.sphinx-doc.org/en/master/config
v0.12.0,-- Path setup --------------------------------------------------------------
v0.12.0,"If extensions (or modules to document with autodoc) are in another directory,"
v0.12.0,add these directories to sys.path here. If the directory is relative to the
v0.12.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.12.0,
v0.12.0,-- Project information -----------------------------------------------------
v0.12.0,-- General configuration ---------------------------------------------------
v0.12.0,"If your documentation needs a minimal Sphinx version, state it here."
v0.12.0,
v0.12.0,needs_sphinx = '1.0'
v0.12.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.12.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.12.0,ones.
v0.12.0,"Add any paths that contain templates here, relative to this directory."
v0.12.0,The suffix(es) of source filenames.
v0.12.0,You can specify multiple suffix as a list of string:
v0.12.0,
v0.12.0,"source_suffix = ['.rst', '.md']"
v0.12.0,The master toctree document.
v0.12.0,The language for content autogenerated by Sphinx. Refer to documentation
v0.12.0,for a list of supported languages.
v0.12.0,
v0.12.0,This is also used if you do content translation via gettext catalogs.
v0.12.0,"Usually you set ""language"" from the command line for these cases."
v0.12.0,"List of patterns, relative to source directory, that match files and"
v0.12.0,directories to ignore when looking for source files.
v0.12.0,This pattern also affects html_static_path and html_extra_path.
v0.12.0,The name of the Pygments (syntax highlighting) style to use.
v0.12.0,-- Options for HTML output -------------------------------------------------
v0.12.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.12.0,a list of builtin themes.
v0.12.0,
v0.12.0,Theme options are theme-specific and customize the look and feel of a theme
v0.12.0,"further.  For a list of options available for each theme, see the"
v0.12.0,documentation.
v0.12.0,
v0.12.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.12.0,"relative to this directory. They are copied after the builtin static files,"
v0.12.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.12.0,html_static_path = ['_static']
v0.12.0,"Custom sidebar templates, must be a dictionary that maps document names"
v0.12.0,to template names.
v0.12.0,
v0.12.0,The default sidebars (for documents that don't match any pattern) are
v0.12.0,defined by theme itself.  Builtin themes are using these templates by
v0.12.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.12.0,'searchbox.html']``.
v0.12.0,
v0.12.0,html_sidebars = {}
v0.12.0,-- Options for HTMLHelp output ---------------------------------------------
v0.12.0,Output file base name for HTML help builder.
v0.12.0,-- Options for LaTeX output ------------------------------------------------
v0.12.0,The paper size ('letterpaper' or 'a4paper').
v0.12.0,
v0.12.0,"'papersize': 'letterpaper',"
v0.12.0,"The font size ('10pt', '11pt' or '12pt')."
v0.12.0,
v0.12.0,"'pointsize': '10pt',"
v0.12.0,Additional stuff for the LaTeX preamble.
v0.12.0,
v0.12.0,"'preamble': '',"
v0.12.0,Latex figure (float) alignment
v0.12.0,
v0.12.0,"'figure_align': 'htbp',"
v0.12.0,Grouping the document tree into LaTeX files. List of tuples
v0.12.0,"(source start file, target name, title,"
v0.12.0,"author, documentclass [howto, manual, or own class])."
v0.12.0,-- Options for manual page output ------------------------------------------
v0.12.0,One entry per manual page. List of tuples
v0.12.0,"(source start file, name, description, authors, manual section)."
v0.12.0,-- Options for Texinfo output ----------------------------------------------
v0.12.0,Grouping the document tree into Texinfo files. List of tuples
v0.12.0,"(source start file, target name, title, author,"
v0.12.0,"dir menu entry, description, category)"
v0.12.0,-- Options for Epub output -------------------------------------------------
v0.12.0,Bibliographic Dublin Core info.
v0.12.0,The unique identifier of the text. This can be a ISBN number
v0.12.0,or the project homepage.
v0.12.0,
v0.12.0,epub_identifier = ''
v0.12.0,A unique identification for the text.
v0.12.0,
v0.12.0,epub_uid = ''
v0.12.0,A list of files that should not be packed into the epub file.
v0.12.0,-- Extension configuration -------------------------------------------------
v0.12.0,-- Options for intersphinx extension ---------------------------------------
v0.12.0,Example configuration for intersphinx: refer to the Python standard library.
v0.12.0,-- Options for todo extension ----------------------------------------------
v0.12.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.12.0,-- Options for doctest extension -------------------------------------------
v0.12.0,we can document otherwise excluded entities here by returning False
v0.12.0,or skip otherwise included entities by returning True
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Calculate residuals
v0.12.0,Estimate E[T_res | Z_res]
v0.12.0,TODO. Deal with multi-class instrument
v0.12.0,Calculate nuisances
v0.12.0,Estimate E[T_res | Z_res]
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"We do a three way split, as typically a preliminary theta estimator would require"
v0.12.0,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.12.0,TODO. Deal with multi-class instrument
v0.12.0,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0,at the expense of some small bias. For points with very small covariance we revert
v0.12.0,to the model-based preliminary estimate and do not add the correction term.
v0.12.0,Estimate preliminary theta in cross fitting manner
v0.12.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.12.0,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.12.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.12.0,TODO. The solution below is not really a valid cross-fitting
v0.12.0,as the test data are used to create the proj_t on the train
v0.12.0,which in the second train-test loop is used to create the nuisance
v0.12.0,cov on the test data. Hence the T variable of some sample
v0.12.0,"is implicitly correlated with its cov nuisance, through this flow"
v0.12.0,"of information. However, this seems a rather weak correlation."
v0.12.0,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.12.0,model.
v0.12.0,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.12.0,Estimate preliminary theta in cross fitting manner
v0.12.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.12.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.12.0,#############################################################################
v0.12.0,Classes for the DRIV implementation for the special case of intent-to-treat
v0.12.0,A/B test
v0.12.0,#############################################################################
v0.12.0,Estimate preliminary theta in cross fitting manner
v0.12.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0,We can use statsmodel for all hypothesis testing capabilities
v0.12.0,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.12.0,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.12.0,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.12.0,model_T_XZ = lambda: model_clf()
v0.12.0,#'days_visited': lambda:
v0.12.0,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.12.0,Turn strings into categories for numeric mapping
v0.12.0,### Defining some generic regressors and classifiers
v0.12.0,This a generic non-parametric regressor
v0.12.0,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.12.0,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.12.0,model = lambda: RandomForestRegressor(n_estimators=100)
v0.12.0,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.12.0,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.12.0,model = lambda: LinearRegression(n_jobs=-1)
v0.12.0,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.12.0,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.12.0,underlying model whenever predict is called.
v0.12.0,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.12.0,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.12.0,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.12.0,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.12.0,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.12.0,We need to specify models to be used for each of these residualizations
v0.12.0,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.12.0,"E[T | X, Z]"
v0.12.0,E[TZ | X]
v0.12.0,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.12.0,n_splits determines the number of splits to be used for cross-fitting.
v0.12.0,# Algorithm 2 - Current Method
v0.12.0,In[121]:
v0.12.0,# Algorithm 3 - DRIV ATE
v0.12.0,dmliv_model_effect = lambda: model()
v0.12.0,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.12.0,"dmliv_model_effect(),"
v0.12.0,n_splits=1)
v0.12.0,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.12.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.12.0,"Once multiple treatments are supported, we'll need to fix this"
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0,We can use statsmodel for all hypothesis testing capabilities
v0.12.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0,We can use statsmodel for all hypothesis testing capabilities
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,TODO. Deal with multi-class instrument/treatment
v0.12.0,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.12.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.12.0,Estimate p(X) = E[T | X] in cross-fitting manner
v0.12.0,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.12.0,##################
v0.12.0,Global settings #
v0.12.0,##################
v0.12.0,Global plotting controls
v0.12.0,"Control for support size, can control for more"
v0.12.0,#################
v0.12.0,File utilities #
v0.12.0,#################
v0.12.0,#################
v0.12.0,Plotting utils #
v0.12.0,#################
v0.12.0,bias
v0.12.0,var
v0.12.0,rmse
v0.12.0,r2
v0.12.0,Infer feature dimension
v0.12.0,Metrics by support plots
v0.12.0,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.12.0,Vasilis Syrgkanis <vasy@microsoft.com>
v0.12.0,Steven Wu <zhiww@microsoft.com>
v0.12.0,Initialize causal tree parameters
v0.12.0,Create splits of causal tree
v0.12.0,Estimate treatment effects at the leafs
v0.12.0,Compute heterogeneous treatement effect for x's in x_list by finding
v0.12.0,the corresponding split and associating the effect computed on that leaf
v0.12.0,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.12.0,Safety check
v0.12.0,Weighted linear regression
v0.12.0,Calculates weights
v0.12.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0,over all indices
v0.12.0,Similar for `a` weights
v0.12.0,Doesn't have sample weights
v0.12.0,Is a linear model
v0.12.0,Weighted linear regression
v0.12.0,Calculates weights
v0.12.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0,over all indices
v0.12.0,Similar for `a` weights
v0.12.0,normalize weights
v0.12.0,"Split the data in half, train and test"
v0.12.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0,"a function of W, using only the train fold"
v0.12.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0,"Split the data in half, train and test"
v0.12.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0,"a function of W, using only the train fold"
v0.12.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.12.0,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.12.0,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.12.0,"Split the data in half, train and test"
v0.12.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0,"a function of x, using only the train fold"
v0.12.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0,Compute coefficient by OLS on residuals
v0.12.0,"Split the data in half, train and test"
v0.12.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0,"a function of x, using only the train fold"
v0.12.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0,Estimate multipliers for second order orthogonal method
v0.12.0,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.12.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0,Create local sample set
v0.12.0,compute the base estimate for the current node using double ml or second order double ml
v0.12.0,compute the influence functions here that are used for the criterion
v0.12.0,generate random proposals of dimensions to split
v0.12.0,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.12.0,compute criterion for each proposal
v0.12.0,if splitting creates valid leafs in terms of mean leaf size
v0.12.0,Calculate criterion for split
v0.12.0,Else set criterion to infinity so that this split is not chosen
v0.12.0,If no good split was found
v0.12.0,Find split that minimizes criterion
v0.12.0,Set the split attributes at the node
v0.12.0,Create child nodes with corresponding subsamples
v0.12.0,Recursively split children
v0.12.0,Return parent node
v0.12.0,estimate the local parameter at the leaf using the estimate data
v0.12.0,###################
v0.12.0,Argument parsing #
v0.12.0,###################
v0.12.0,#########################################
v0.12.0,Parameters constant across experiments #
v0.12.0,#########################################
v0.12.0,Outcome support
v0.12.0,Treatment support
v0.12.0,Evaluation grid
v0.12.0,Treatment effects array
v0.12.0,Other variables
v0.12.0,##########################
v0.12.0,Data Generating Process #
v0.12.0,##########################
v0.12.0,Log iteration
v0.12.0,"Generate controls, features, treatment and outcome"
v0.12.0,T and Y residuals to be used in later scripts
v0.12.0,Save generated dataset
v0.12.0,#################
v0.12.0,ORF parameters #
v0.12.0,#################
v0.12.0,######################################
v0.12.0,Train and evaluate treatment effect #
v0.12.0,######################################
v0.12.0,########
v0.12.0,Plots #
v0.12.0,########
v0.12.0,###############
v0.12.0,Save results #
v0.12.0,###############
v0.12.0,##############
v0.12.0,Run Rscript #
v0.12.0,##############
v0.12.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.12.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0,def mlasso_model(): return MultiTaskLassoCV(
v0.12.0,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0,heterogeneity
v0.12.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0,heterogeneity
v0.12.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0,heterogeneity
v0.12.0,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.12.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.12.0,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.12.0,subset of features that are exogenous and create heterogeneity
v0.12.0,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.12.0,subset of features wrt we estimate heterogeneity
v0.12.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,introspect the constructor arguments to find the model parameters
v0.12.0,to represent
v0.12.0,"if the argument is deprecated, ignore it"
v0.12.0,Extract and sort argument names excluding 'self'
v0.12.0,column names
v0.12.0,transfer input to numpy arrays
v0.12.0,transfer input to 2d arrays
v0.12.0,create dataframe
v0.12.0,currently dowhy only support single outcome and single treatment
v0.12.0,call dowhy
v0.12.0,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.12.0,cate estimator but not the effect.
v0.12.0,don't proxy special methods
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Check if model is sparse enough for this model
v0.12.0,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.12.0,TODO: any way to avoid creating a copy if the array was already dense?
v0.12.0,"the call is necessary if the input was something like a list, though"
v0.12.0,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.12.0,so convert to pydata sparse first
v0.12.0,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.12.0,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.12.0,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.12.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0,For when checking input values is disabled
v0.12.0,Type to column extraction function
v0.12.0,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.12.0,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.12.0,Get feature names using featurizer
v0.12.0,All attempts at retrieving transformed feature names have failed
v0.12.0,Delegate handling to downstream logic
v0.12.0,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.12.0,same number of input definitions as arrays
v0.12.0,input definitions have same number of dimensions as each array
v0.12.0,all result indices are unique
v0.12.0,all result indices must match at least one input index
v0.12.0,"map indices to all array, axis pairs for that index"
v0.12.0,each index has the same cardinality wherever it appears
v0.12.0,"State: list of (set of letters, list of (corresponding indices, value))"
v0.12.0,Algo: while list contains more than one entry
v0.12.0,take two entries
v0.12.0,sort both lists by intersection of their indices
v0.12.0,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.12.0,"take the union of indices and the product of values), stepping through each list linearly"
v0.12.0,TODO: might be faster to break into connected components first
v0.12.0,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.12.0,"so compute their content separately, then take cartesian product"
v0.12.0,this would save a few pointless sorts by empty tuples
v0.12.0,TODO: Consider investigating other performance ideas for these cases
v0.12.0,where the dense method beat the sparse method (usually sparse is faster)
v0.12.0,"e,facd,c->cfed"
v0.12.0,sparse: 0.0335489
v0.12.0,dense:  0.011465999999999997
v0.12.0,"gbd,da,egb->da"
v0.12.0,sparse: 0.0791625
v0.12.0,dense:  0.007319099999999995
v0.12.0,"dcc,d,faedb,c->abe"
v0.12.0,sparse: 1.2868097
v0.12.0,dense:  0.44605229999999985
v0.12.0,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.12.0,TODO: would using einsum's paths to optimize the order of merging help?
v0.12.0,assume that we should perform nested cross-validation if and only if
v0.12.0,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.12.0,logic copied from check_cv
v0.12.0,otherwise we will assume the user already set the cv attribute to something
v0.12.0,compatible with splitting with a 'groups' argument
v0.12.0,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.12.0,don't use the groups when calling split internally
v0.12.0,Normalize weights
v0.12.0,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.12.0,"if we're decorating a class, just update the __init__ method,"
v0.12.0,so that the result is still a class instead of a wrapper method
v0.12.0,"want to enforce that each bad_arg was either in kwargs,"
v0.12.0,or else it was in neither and is just taking its default value
v0.12.0,Any access should throw
v0.12.0,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0,input feature name is already updated by cate_feature_names.
v0.12.0,define the index of d_x to filter for each given T
v0.12.0,filter X after broadcast with T for each given T
v0.12.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,
v0.12.0,This code contains some snippets of code from:
v0.12.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
v0.12.0,published under the following license and copyright:
v0.12.0,BSD 3-Clause License
v0.12.0,
v0.12.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0,All rights reserved.
v0.12.0,make any access to matplotlib or plt throw an exception
v0.12.0,make any access to graphviz or plt throw an exception
v0.12.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.12.0,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.12.0,Initialize saturation & value; calculate chroma & value shift
v0.12.0,Calculate some intermediate values
v0.12.0,Initialize RGB with same hue & chroma as our color
v0.12.0,Shift the initial RGB values to match value and store
v0.12.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.12.0,clean way of achieving this
v0.12.0,make sure we don't accidentally escape anything in the substitution
v0.12.0,Fetch appropriate color for node
v0.12.0,"red for negative, green for positive"
v0.12.0,in multi-target use mean of targets
v0.12.0,Write node mean CATE
v0.12.0,Write node std of CATE
v0.12.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.12.0,Fetch appropriate color for node
v0.12.0,Write node mean CATE
v0.12.0,Write node mean CATE
v0.12.0,Write recommended treatment and value - cost
v0.12.0,Licensed under the MIT License.
v0.12.0,"since inference objects can be stateful, we must copy it before fitting;"
v0.12.0,otherwise this sequence wouldn't work:
v0.12.0,"est1.fit(..., inference=inf)"
v0.12.0,"est2.fit(..., inference=inf)"
v0.12.0,est1.effect_interval(...)
v0.12.0,because inf now stores state from fitting est2
v0.12.0,This flag is true when names are set in a child class instead
v0.12.0,"If names are set in a child class, add an attribute reflecting that"
v0.12.0,This works only if X is passed as a kwarg
v0.12.0,We plan to enforce X as kwarg only in future releases
v0.12.0,This checks if names have been set in a child class
v0.12.0,"If names were set in a child class, don't do it again"
v0.12.0,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.12.0,call the wrapped fit method
v0.12.0,NOTE: we call inference fit *after* calling the main fit method
v0.12.0,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.12.0,but tensordot can't be applied to this problem because we don't sum over m
v0.12.0,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.12.0,of rows of T was not taken into account
v0.12.0,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.12.0,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.12.0,"Treatment names is None, default to BaseCateEstimator"
v0.12.0,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.12.0,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.12.0,Get input names
v0.12.0,Summary
v0.12.0,add statsmodels to parent's options
v0.12.0,add debiasedlasso to parent's options
v0.12.0,add blb to parent's options
v0.12.0,TODO Share some logic with non-discrete version
v0.12.0,Get input names
v0.12.0,Summary
v0.12.0,add statsmodels to parent's options
v0.12.0,add statsmodels to parent's options
v0.12.0,add blb to parent's options
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,remove None arguments
v0.12.0,"scores entries should be lists of scores, so make each entry a singleton list"
v0.12.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0,generate an instance of the final model
v0.12.0,generate an instance of the nuisance model
v0.12.0,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.12.0,alteration even when we only want to fit_final.
v0.12.0,use a binary array to get stratified split in case of discrete treatment
v0.12.0,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.12.0,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.12.0,"however, sklearn doesn't support both stratifying and grouping (see"
v0.12.0,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.12.0,their own object that supports grouping if they want to use groups.
v0.12.0,for each mc iteration
v0.12.0,for each model under cross fit setting
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,
v0.12.0,This code contains snippets of code from
v0.12.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0,published under the following license and copyright:
v0.12.0,BSD 3-Clause License
v0.12.0,
v0.12.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0,All rights reserved.
v0.12.0,=============================================================================
v0.12.0,Policy Forest
v0.12.0,=============================================================================
v0.12.0,Remap output
v0.12.0,reshape is necessary to preserve the data contiguity against vs
v0.12.0,"[:, np.newaxis] that does not."
v0.12.0,Get subsample sample size
v0.12.0,Check parameters
v0.12.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0,if this is the first `fit` call of the warm start mode.
v0.12.0,"Free allocated memory, if any"
v0.12.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0,We draw from the random state to get the random state we
v0.12.0,would have got if we hadn't used a warm_start.
v0.12.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0,for fitting the trees is internally releasing the Python GIL
v0.12.0,making threading more efficient than multiprocessing in
v0.12.0,"that case. However, for joblib 0.12+ we respect any"
v0.12.0,"parallel_backend contexts set at a higher level,"
v0.12.0,since correctness does not rely on using threads.
v0.12.0,Collect newly grown trees
v0.12.0,Check data
v0.12.0,Assign chunk of trees to jobs
v0.12.0,avoid storing the output of every estimator by summing them here
v0.12.0,Parallel loop
v0.12.0,Check data
v0.12.0,Assign chunk of trees to jobs
v0.12.0,avoid storing the output of every estimator by summing them here
v0.12.0,Parallel loop
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,
v0.12.0,This code contains snippets of code from:
v0.12.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0,published under the following license and copyright:
v0.12.0,BSD 3-Clause License
v0.12.0,
v0.12.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0,All rights reserved.
v0.12.0,=============================================================================
v0.12.0,Types and constants
v0.12.0,=============================================================================
v0.12.0,=============================================================================
v0.12.0,Base Policy tree
v0.12.0,=============================================================================
v0.12.0,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.12.0,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.12.0,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.12.0,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.12.0,checks that X is 2D array.
v0.12.0,"since we only allow single dimensional y, we could flatten the prediction"
v0.12.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0,Handles the corner case when X=None but featurizer might be not None
v0.12.0,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.12.0,"Replacing this method which is invalid for this class, so that we make the"
v0.12.0,dosctring empty and not appear in the docs.
v0.12.0,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.12.0,Replacing to remove docstring
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"if both X and W are None, just return a column of ones"
v0.12.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0,We need to go back to the label representation of the one-hot so as to call
v0.12.0,the classifier.
v0.12.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0,We need to go back to the label representation of the one-hot so as to call
v0.12.0,the classifier.
v0.12.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0,This works both with our without the weighting trick as the treatments T are unit vector
v0.12.0,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.12.0,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.12.0,both Parametric and Non Parametric DML.
v0.12.0,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.12.0,attribute so that the trained featurizer will be passed through
v0.12.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0,for internal use by the library
v0.12.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0,We need to use the rlearner's copy to retain the information from fitting
v0.12.0,Handles the corner case when X=None but featurizer might be not None
v0.12.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0,since we clone it and fit separate copies
v0.12.0,override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
v0.12.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0,since we clone it and fit separate copies
v0.12.0,add blb to parent's options
v0.12.0,override only so that we can update the docstring to indicate
v0.12.0,support for `GenericSingleTreatmentModelFinalInference`
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,note that groups are not passed to score because they are only used for fitting
v0.12.0,note that groups are not passed to score because they are only used for fitting
v0.12.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0,NOTE: important to get parent's wrapped copy so that
v0.12.0,"after training wrapped featurizer is also trained, etc."
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0,Fit a doubly robust average effect
v0.12.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0,(which needs to have been expanded if there's a discrete treatment)
v0.12.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0,since we clone it and fit separate copies
v0.12.0,"If custom param grid, check that only estimator parameters are being altered"
v0.12.0,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.12.0,override only so that we can update the docstring to indicate support for `blb`
v0.12.0,Get input names
v0.12.0,Summary
v0.12.0,Determine output settings
v0.12.0,"Important: This must be the first invocation of the random state at fit time, so that"
v0.12.0,train/test splits are re-generatable from an external object simply by knowing the
v0.12.0,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.12.0,linear predictions. Currently is also useful for testing.
v0.12.0,reshape is necessary to preserve the data contiguity against vs
v0.12.0,"[:, np.newaxis] that does not."
v0.12.0,Check parameters
v0.12.0,Set min_weight_leaf from min_weight_fraction_leaf
v0.12.0,Build tree
v0.12.0,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.12.0,hold. Used by criterion for memory space savings.
v0.12.0,Initialize the criterion object and the criterion_val object if honest.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,
v0.12.0,This code is a fork from:
v0.12.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.12.0,published under the following license and copyright:
v0.12.0,BSD 3-Clause License
v0.12.0,
v0.12.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0,All rights reserved.
v0.12.0,Set parameters
v0.12.0,Don't instantiate estimators now! Parameters of base_estimator might
v0.12.0,"still change. Eg., when grid-searching with the nested object syntax."
v0.12.0,self.estimators_ needs to be filled by the derived classes in fit.
v0.12.0,Compute the number of jobs
v0.12.0,Partition estimators between jobs
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,covariance matrix
v0.12.0,get eigen value and eigen vectors
v0.12.0,simulate eigen vectors
v0.12.0,keep the top 4 eigen value and corresponding eigen vector
v0.12.0,replace the negative eigen values
v0.12.0,generate a new covariance matrix
v0.12.0,get linear approximation of eigen values
v0.12.0,coefs
v0.12.0,get the indices of each group of features
v0.12.0,print(ind_same_proxy)
v0.12.0,demo
v0.12.0,same proxy
v0.12.0,residuals
v0.12.0,gmm
v0.12.0,log normal on outliers
v0.12.0,positive outliers
v0.12.0,negative outliers
v0.12.0,demean the new residual again
v0.12.0,generate data
v0.12.0,sample residuals
v0.12.0,get prediction for current investment
v0.12.0,get prediction for current proxy
v0.12.0,get first period prediction
v0.12.0,iterate the step ahead contruction
v0.12.0,prepare new x
v0.12.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0,heterogeneity
v0.12.0,get new covariance matrix
v0.12.0,get coefs
v0.12.0,get residuals
v0.12.0,proxy 1 is the outcome
v0.12.0,make fixed residuals
v0.12.0,Remove children with nonwhite mothers from the treatment group
v0.12.0,Remove children with nonwhite mothers from the treatment group
v0.12.0,Select columns
v0.12.0,Scale the numeric variables
v0.12.0,"Change the binary variable 'first' takes values in {1,2}"
v0.12.0,Append a column of ones as intercept
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0,(which needs to have been expanded if there's a discrete treatment)
v0.12.0,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.12.0,d_t=None here since we measure the effect across all Ts
v0.12.0,once the estimator has been fit
v0.12.0,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.12.0,an intercept even when bias is part of coef.
v0.12.0,We can write effect inference as a function of prediction and prediction standard error of
v0.12.0,the final method for linear models
v0.12.0,squeeze the first axis
v0.12.0,d_t=None here since we measure the effect across all Ts
v0.12.0,set the mean_pred_stderr
v0.12.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0,(which needs to have been expanded if there's a discrete treatment)
v0.12.0,"send treatment to the end, pull bounds to the front"
v0.12.0,d_t=None here since we measure the effect across all Ts
v0.12.0,set the mean_pred_stderr
v0.12.0,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.12.0,d_t=None here since we measure the effect across all Ts
v0.12.0,d_t=None here since we measure the effect across all Ts
v0.12.0,need to set the fit args before the estimator is fit
v0.12.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.12.0,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.12.0,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.12.0,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.12.0,scale preds
v0.12.0,scale std errs
v0.12.0,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.12.0,offset preds
v0.12.0,"offset the distribution, too"
v0.12.0,scale preds
v0.12.0,"scale the distribution, too"
v0.12.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0,1. Uncertainty of Mean Point Estimate
v0.12.0,2. Distribution of Point Estimate
v0.12.0,3. Total Variance of Point Estimate
v0.12.0,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.12.0,so bail out early; note that it might be possible to correct the algorithm for
v0.12.0,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.12.0,be clean
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,TODO: Add a __dir__ implementation?
v0.12.0,don't proxy special methods
v0.12.0,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.12.0,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.12.0,then we should be computing a confidence interval for the wrapped calls
v0.12.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.12.0,second level bootstrap which would be prohibitive computationally?
v0.12.0,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.12.0,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.12.0,can't import from econml.inference at top level without creating cyclical dependencies
v0.12.0,Note that inference results are always methods even if the inference is for a property
v0.12.0,(e.g. coef__inference() is a method but coef_ is a property)
v0.12.0,Therefore we must insert a lambda if getting inference for a non-callable
v0.12.0,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.12.0,"try to get interval/std first if appropriate,"
v0.12.0,since we don't prefer a wrapped method with this name
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,
v0.12.0,This code contains snippets of code from:
v0.12.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0,published under the following license and copyright:
v0.12.0,BSD 3-Clause License
v0.12.0,
v0.12.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0,All rights reserved.
v0.12.0,=============================================================================
v0.12.0,Types and constants
v0.12.0,=============================================================================
v0.12.0,=============================================================================
v0.12.0,Base GRF tree
v0.12.0,=============================================================================
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,=============================================================================
v0.12.0,A MultOutputWrapper for GRF classes
v0.12.0,=============================================================================
v0.12.0,=============================================================================
v0.12.0,Instantiations of Generalized Random Forest
v0.12.0,=============================================================================
v0.12.0,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.12.0,in front of the constant treatment is the intercept in the moment equation.
v0.12.0,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.12.0,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,
v0.12.0,This code contains snippets of code from
v0.12.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0,published under the following license and copyright:
v0.12.0,BSD 3-Clause License
v0.12.0,
v0.12.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0,All rights reserved.
v0.12.0,=============================================================================
v0.12.0,Base Generalized Random Forest
v0.12.0,=============================================================================
v0.12.0,TODO: support freq_weight and sample_var
v0.12.0,Remap output
v0.12.0,reshape is necessary to preserve the data contiguity against vs
v0.12.0,"[:, np.newaxis] that does not."
v0.12.0,reshape is necessary to preserve the data contiguity against vs
v0.12.0,"[:, np.newaxis] that does not."
v0.12.0,Get subsample sample size
v0.12.0,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.12.0,We calculate the min eigenvalue proxy that each criterion is considering
v0.12.0,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.12.0,Check parameters
v0.12.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0,if this is the first `fit` call of the warm start mode.
v0.12.0,"Free allocated memory, if any"
v0.12.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0,We draw from the random state to get the random state we
v0.12.0,would have got if we hadn't used a warm_start.
v0.12.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0,Generating indices a priori before parallelism ended up being orders of magnitude
v0.12.0,faster than how sklearn does it. The reason is that random samplers do not release the
v0.12.0,gil it seems.
v0.12.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0,for fitting the trees is internally releasing the Python GIL
v0.12.0,making threading more efficient than multiprocessing in
v0.12.0,"that case. However, for joblib 0.12+ we respect any"
v0.12.0,"parallel_backend contexts set at a higher level,"
v0.12.0,since correctness does not rely on using threads.
v0.12.0,Collect newly grown trees
v0.12.0,Check data
v0.12.0,Assign chunk of trees to jobs
v0.12.0,avoid storing the output of every estimator by summing them here
v0.12.0,Parallel loop
v0.12.0,Check data
v0.12.0,Assign chunk of trees to jobs
v0.12.0,Parallel loop
v0.12.0,Check data
v0.12.0,Assign chunk of trees to jobs
v0.12.0,Parallel loop
v0.12.0,####################
v0.12.0,Variance correction
v0.12.0,####################
v0.12.0,Subtract the average within bag variance. This ends up being equal to the
v0.12.0,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.12.0,The negative part is just sq_between.
v0.12.0,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.12.0,"The off diagonals we have no objective prior, so no correction is applied."
v0.12.0,Finally correcting the pred_cov or pred_var
v0.12.0,avoid storing the output of every estimator by summing them here
v0.12.0,Parallel loop
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,testing importances
v0.12.0,testing heterogeneity importances
v0.12.0,Testing that all parameters do what they are supposed to
v0.12.0,"testing predict, apply and decision path"
v0.12.0,test that the subsampling scheme past to the trees is correct
v0.12.0,The sample size is chosen in particular to test rounding based error when subsampling
v0.12.0,test that the estimator calcualtes var correctly
v0.12.0,test api
v0.12.0,test accuracy
v0.12.0,test the projection functionality of forests
v0.12.0,test that the estimator calcualtes var correctly
v0.12.0,test api
v0.12.0,test that the estimator calcualtes var correctly
v0.12.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0,test that we raise errors in mishandled situations.
v0.12.0,test that the subsampling scheme past to the trees is correct
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,omit the lalonde notebook
v0.12.0,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.12.0,creating notebooks that are annoying for our users to actually run themselves
v0.12.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.12.0,"prior to calling interpret, can't plot, render, etc."
v0.12.0,can interpret without uncertainty
v0.12.0,can't interpret with uncertainty if inference wasn't used during fit
v0.12.0,can interpret with uncertainty if we refit
v0.12.0,can interpret without uncertainty
v0.12.0,can't treat before interpreting
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,for is_discrete in [False]:
v0.12.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0,ensure we can serialize the unfit estimator
v0.12.0,ensure we can pickle the fit estimator
v0.12.0,make sure we can call the marginal_effect and effect methods
v0.12.0,test const marginal inference
v0.12.0,test effect inference
v0.12.0,test marginal effect inference
v0.12.0,test coef__inference and intercept__inference
v0.12.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0,"make sure we can call effect with implied scalar treatments,"
v0.12.0,"no matter the dimensions of T, and also that we warn when there"
v0.12.0,are multiple treatments
v0.12.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0,No heterogeneity
v0.12.0,Define indices to test
v0.12.0,Heterogeneous effects
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,simple DGP only for illustration
v0.12.0,Define the treatment model neural network architecture
v0.12.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0,"so the input shape is (d_z + d_x,)"
v0.12.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0,add extra layers on top for the mixture density network
v0.12.0,Define the response model neural network architecture
v0.12.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0,"so the input shape is (d_t + d_x,)"
v0.12.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0,so that the same weights will be reused in each instantiation
v0.12.0,number of samples to use in second estimate of the response
v0.12.0,(to make loss estimate unbiased)
v0.12.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0,do something with predictions...
v0.12.0,also test vector t and y
v0.12.0,simple DGP only for illustration
v0.12.0,Define the treatment model neural network architecture
v0.12.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0,"so the input shape is (d_z + d_x,)"
v0.12.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0,add extra layers on top for the mixture density network
v0.12.0,Define the response model neural network architecture
v0.12.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0,"so the input shape is (d_t + d_x,)"
v0.12.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0,so that the same weights will be reused in each instantiation
v0.12.0,number of samples to use in second estimate of the response
v0.12.0,(to make loss estimate unbiased)
v0.12.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0,do something with predictions...
v0.12.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.12.0,test = True ensures we draw test set images
v0.12.0,test = True ensures we draw test set images
v0.12.0,re-draw to get new independent treatment and implied response
v0.12.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0,above is necesary so that reduced form doesn't win
v0.12.0,covariates: time and emotion
v0.12.0,random instrument
v0.12.0,z -> price
v0.12.0,true observable demand function
v0.12.0,errors
v0.12.0,response
v0.12.0,test = True ensures we draw test set images
v0.12.0,test = True ensures we draw test set images
v0.12.0,re-draw to get new independent treatment and implied response
v0.12.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0,above is necesary so that reduced form doesn't win
v0.12.0,covariates: time and emotion
v0.12.0,random instrument
v0.12.0,z -> price
v0.12.0,true observable demand function
v0.12.0,errors
v0.12.0,response
v0.12.0,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.12.0,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.12.0,For some reason this doesn't work at all when run against the CNTK backend...
v0.12.0,"model.compile('nadam', loss=lambda _,l:l)"
v0.12.0,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.12.0,generate a valiation set
v0.12.0,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.12.0,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,DGP constants
v0.12.0,Generate data
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,testing importances
v0.12.0,testing heterogeneity importances
v0.12.0,Testing that all parameters do what they are supposed to
v0.12.0,"testing predict, apply and decision path"
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.12.0,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.12.0,so we need to transpose the result
v0.12.0,1-d output
v0.12.0,2-d output
v0.12.0,Single dimensional output y
v0.12.0,compare with weight
v0.12.0,compare with weight
v0.12.0,compare with weight
v0.12.0,compare with weight
v0.12.0,Multi-dimensional output y
v0.12.0,1-d y
v0.12.0,compare when both sample_var and sample_weight exist
v0.12.0,multi-d y
v0.12.0,compare when both sample_var and sample_weight exist
v0.12.0,compare when both sample_var and sample_weight exist
v0.12.0,compare when both sample_var and sample_weight exist
v0.12.0,compare when both sample_var and sample_weight exist
v0.12.0,compare when both sample_var and sample_weight exist
v0.12.0,compare when both sample_var and sample_weight exist
v0.12.0,dgp
v0.12.0,StatsModels2SLS
v0.12.0,IV2SLS
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,test that we can fit with the same arguments as the base estimator
v0.12.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0,with the same shape for the lower and upper bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,test that we can do the same thing once we provide percentile bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0,with the same shape for the lower and upper bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,test that we can do the same thing once we provide percentile bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,test that we can fit with the same arguments as the base estimator
v0.12.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0,with the same shape for the lower and upper bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,test that we can do the same thing once we provide percentile bounds
v0.12.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0,with the same shape for the lower and upper bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,test that we can do the same thing once we provide percentile bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,test that we can fit with the same arguments as the base estimator
v0.12.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0,with the same shape for the lower and upper bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,test that we can do the same thing once we provide percentile bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0,with the same shape for the lower and upper bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,test that we can do the same thing once we provide percentile bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0,with the same shape for the lower and upper bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,test that the estimated effect is usually within the bounds
v0.12.0,test that we can do the same thing once we provide alpha explicitly
v0.12.0,test that the lower and upper bounds differ
v0.12.0,test that the estimated effect is usually within the bounds
v0.12.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0,with the same shape for the lower and upper bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,TODO: test that the estimated effect is usually within the bounds
v0.12.0,and that the true effect is also usually within the bounds
v0.12.0,test that we can do the same thing once we provide percentile bounds
v0.12.0,test that the lower and upper bounds differ
v0.12.0,TODO: test that the estimated effect is usually within the bounds
v0.12.0,and that the true effect is also usually within the bounds
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,test that the subsampling scheme past to the trees is correct
v0.12.0,test that the estimator calcualtes var correctly
v0.12.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0,test that we raise errors in mishandled situations.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,DGP constants
v0.12.0,Generate data
v0.12.0,Test inference results when `cate_feature_names` doesn not exist
v0.12.0,Test inference results when `cate_feature_names` doesn not exist
v0.12.0,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.12.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.12.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0,pvalue for second column should be greater than zero since some points are on either side
v0.12.0,of the tested value
v0.12.0,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.12.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0,ensure alpha is passed
v0.12.0,only is not None when T1 is a constant or a list of constant
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.12.0,Test non keyword based calls to fit
v0.12.0,test non-array inputs
v0.12.0,Test custom splitter
v0.12.0,Test incomplete set of test folds
v0.12.0,"y scores should be positive, since W predicts Y somewhat"
v0.12.0,"t scores might not be, since W and T are uncorrelated"
v0.12.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,make sure cross product varies more slowly with first array
v0.12.0,and that vectors are okay as inputs
v0.12.0,number of inputs in specification must match number of inputs
v0.12.0,must have an output
v0.12.0,output indices must be unique
v0.12.0,output indices must be present in an input
v0.12.0,number of indices must match number of dimensions for each input
v0.12.0,repeated indices must always have consistent sizes
v0.12.0,transpose
v0.12.0,tensordot
v0.12.0,trace
v0.12.0,TODO: set up proper flag for this
v0.12.0,pick indices at random with replacement from the first 7 letters of the alphabet
v0.12.0,"of all of the distinct indices that appear in any input,"
v0.12.0,pick a random subset of them (of size at most 5) to appear in the output
v0.12.0,creating an instance should warn
v0.12.0,using the instance should not warn
v0.12.0,using the deprecated method should warn
v0.12.0,don't warn if b and c are passed by keyword
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,make any access to matplotlib or plt throw an exception
v0.12.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0,heterogeneity
v0.12.0,Invert indices to match latest API
v0.12.0,Invert indices to match latest API
v0.12.0,The feature for heterogeneity stays constant
v0.12.0,Auxiliary function for adding xticks and vertical lines when plotting results
v0.12.0,for dynamic dml vs ground truth parameters.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Preprocess data
v0.12.0,Convert 'week' to a date
v0.12.0,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.12.0,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.12.0,Take log of price
v0.12.0,Make brand numeric
v0.12.0,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.12.0,which have all zero coeffs
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,test at least one estimator from each category
v0.12.0,test causal graph
v0.12.0,test refutation estimate
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.12.0,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.12.0,TODO: test something rather than just print...
v0.12.0,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.12.0,pick some arbitrary X
v0.12.0,pick some arbitrary T
v0.12.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0,The average variance should be lower when using monte carlo iterations
v0.12.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0,The average variance should be lower when using monte carlo iterations
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,ensure we can serialize unfit estimator
v0.12.0,ensure we can serialize fit estimator
v0.12.0,expected effect size
v0.12.0,test effect
v0.12.0,test inference
v0.12.0,only OrthoIV support inference other than bootstrap
v0.12.0,test summary
v0.12.0,test can run score
v0.12.0,test cate_feature_names
v0.12.0,test can run shap values
v0.12.0,dgp
v0.12.0,no heterogeneity
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.12.0,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.12.0,The __warningregistry__'s need to be in a pristine state for tests
v0.12.0,to work properly.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,ensure we can serialize unfit estimator
v0.12.0,ensure we can serialize fit estimator
v0.12.0,expected effect size
v0.12.0,test effect
v0.12.0,test inference
v0.12.0,test can run score
v0.12.0,test cate_feature_names
v0.12.0,test can run shap values
v0.12.0,"dgp (binary T, binary Z)"
v0.12.0,no heterogeneity
v0.12.0,with heterogeneity
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Set random seed
v0.12.0,Generate data
v0.12.0,DGP constants
v0.12.0,Test data
v0.12.0,Constant treatment effect
v0.12.0,Constant treatment with multi output Y
v0.12.0,Heterogeneous treatment
v0.12.0,Heterogeneous treatment with multi output Y
v0.12.0,TLearner test
v0.12.0,Instantiate TLearner
v0.12.0,Test inputs
v0.12.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0,Instantiate SLearner
v0.12.0,Test inputs
v0.12.0,Test constant treatment effect
v0.12.0,Test constant treatment effect with multi output Y
v0.12.0,Test heterogeneous treatment effect
v0.12.0,Need interactions between T and features
v0.12.0,Test heterogeneous treatment effect with multi output Y
v0.12.0,Instantiate XLearner
v0.12.0,Test inputs
v0.12.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0,Instantiate DomainAdaptationLearner
v0.12.0,Test inputs
v0.12.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0,Get the true treatment effect
v0.12.0,Get the true treatment effect
v0.12.0,Fit learner and get the effect and marginal effect
v0.12.0,Compute treatment effect residuals (absolute)
v0.12.0,Check that at least 90% of predictions are within tolerance interval
v0.12.0,Check whether the output shape is right
v0.12.0,Check that one can pass in regular lists
v0.12.0,Check that it fails correctly if lists of different shape are passed in
v0.12.0,"Check that it works when T, Y have shape (n, 1)"
v0.12.0,Generate covariates
v0.12.0,Generate treatment
v0.12.0,Calculate outcome
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,DGP constants
v0.12.0,Generate data
v0.12.0,Test data
v0.12.0,Remove warnings that might be raised by the models passed into the ORF
v0.12.0,Generate data with continuous treatments
v0.12.0,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.12.0,does not work well with parallelism.
v0.12.0,Test inputs for continuous treatments
v0.12.0,--> Check that one can pass in regular lists
v0.12.0,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0,Check that outputs have the correct shape
v0.12.0,Test continuous treatments with controls
v0.12.0,Test continuous treatments without controls
v0.12.0,Generate data with binary treatments
v0.12.0,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.12.0,does not work well with parallelism.
v0.12.0,Test inputs for binary treatments
v0.12.0,--> Check that one can pass in regular lists
v0.12.0,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0,"--> Check that it works when T, Y have shape (n, 1)"
v0.12.0,"--> Check that it fails correctly when T has shape (n, 2)"
v0.12.0,--> Check that it fails correctly when the treatments are not numeric
v0.12.0,Check that outputs have the correct shape
v0.12.0,Test binary treatments with controls
v0.12.0,Test binary treatments without controls
v0.12.0,Only applicable to continuous treatments
v0.12.0,Generate data for 2 treatments
v0.12.0,Test multiple treatments with controls
v0.12.0,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.12.0,The rest for controls. Just as an example.
v0.12.0,Generating A/B test data
v0.12.0,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.12.0,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.12.0,Create a wrapper around Lasso that doesn't support weights
v0.12.0,since Lasso does natively support them starting in sklearn 0.23
v0.12.0,Generate data with continuous treatments
v0.12.0,Instantiate model with most of the default parameters
v0.12.0,Compute the treatment effect on test points
v0.12.0,Compute treatment effect residuals
v0.12.0,Multiple treatments
v0.12.0,Allow at most 10% test points to be outside of the tolerance interval
v0.12.0,Compute treatment effect residuals
v0.12.0,Multiple treatments
v0.12.0,Allow at most 20% test points to be outside of the confidence interval
v0.12.0,Check that the intervals are not too wide
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.12.0,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.12.0,ensure that we've got at least 6 of every element
v0.12.0,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.12.0,NOTE: this number may need to change if the default number of folds in
v0.12.0,WeightedStratifiedKFold changes
v0.12.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0,ensure we can serialize the unfit estimator
v0.12.0,ensure we can pickle the fit estimator
v0.12.0,make sure we can call the marginal_effect and effect methods
v0.12.0,test const marginal inference
v0.12.0,test effect inference
v0.12.0,test marginal effect inference
v0.12.0,test coef__inference and intercept__inference
v0.12.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0,"make sure we can call effect with implied scalar treatments,"
v0.12.0,"no matter the dimensions of T, and also that we warn when there"
v0.12.0,are multiple treatments
v0.12.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0,ensure that we've got at least two of every element
v0.12.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0,make sure we can call the marginal_effect and effect methods
v0.12.0,test const marginal inference
v0.12.0,test effect inference
v0.12.0,test marginal effect inference
v0.12.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0,We concatenate the two copies data
v0.12.0,make sure we can get out post-fit stuff
v0.12.0,create a simple artificial setup where effect of moving from treatment
v0.12.0,"1 -> 2 is 2,"
v0.12.0,"1 -> 3 is 1, and"
v0.12.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0,"Using an uneven number of examples from different classes,"
v0.12.0,"and having the treatments in non-lexicographic order,"
v0.12.0,Should rule out some basic issues.
v0.12.0,test that we can fit with a KFold instance
v0.12.0,test that we can fit with a train/test iterable
v0.12.0,predetermined splits ensure that all features are seen in each split
v0.12.0,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.12.0,(incorrectly) use a final model with an intercept
v0.12.0,"Because final model is fixed, actual values of T and Y don't matter"
v0.12.0,Ensure reproducibility
v0.12.0,Sparse DGP
v0.12.0,Treatment effect coef
v0.12.0,Other coefs
v0.12.0,Features and controls
v0.12.0,Test sparse estimator
v0.12.0,"--> test coef_, intercept_"
v0.12.0,--> test treatment effects
v0.12.0,Restrict x_test to vectors of norm < 1
v0.12.0,--> check inference
v0.12.0,Check that a majority of true effects lie in the 5-95% CI
v0.12.0,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.12.0,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.12.0,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.12.0,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.12.0,sparse test case: heterogeneous effect by product
v0.12.0,need at least as many rows in e_y as there are distinct columns
v0.12.0,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.12.0,create a simple artificial setup where effect of moving from treatment
v0.12.0,"a -> b is 2,"
v0.12.0,"a -> c is 1, and"
v0.12.0,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.12.0,"Using an uneven number of examples from different classes,"
v0.12.0,"and having the treatments in non-lexicographic order,"
v0.12.0,should rule out some basic issues.
v0.12.0,Note that explicitly specifying the dtype as object is necessary until
v0.12.0,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.12.0,estimated effects should be identical when treatment is explicitly given
v0.12.0,but const_marginal_effect should be reordered based on the explicit cagetories
v0.12.0,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.12.0,test outer grouping
v0.12.0,test nested grouping
v0.12.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0,whichever groups we saw
v0.12.0,test nested grouping
v0.12.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Set random seed
v0.12.0,Generate data
v0.12.0,DGP constants
v0.12.0,Test data
v0.12.0,Constant treatment effect and propensity
v0.12.0,Heterogeneous treatment and propensity
v0.12.0,ensure that we've got at least two of every element
v0.12.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0,ensure that we can serialize unfit estimator
v0.12.0,ensure that we can serialize fit estimator
v0.12.0,make sure we can call the marginal_effect and effect methods
v0.12.0,test const marginal inference
v0.12.0,test effect inference
v0.12.0,test marginal effect inference
v0.12.0,test coef_ and intercept_ inference
v0.12.0,verify we can generate the summary
v0.12.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0,create a simple artificial setup where effect of moving from treatment
v0.12.0,"1 -> 2 is 2,"
v0.12.0,"1 -> 3 is 1, and"
v0.12.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0,"Using an uneven number of examples from different classes,"
v0.12.0,"and having the treatments in non-lexicographic order,"
v0.12.0,Should rule out some basic issues.
v0.12.0,test that we can fit with a KFold instance
v0.12.0,test that we can fit with a train/test iterable
v0.12.0,"for at least some of the examples, the CI should have nonzero width"
v0.12.0,"for at least some of the examples, the CI should have nonzero width"
v0.12.0,"for at least some of the examples, the CI should have nonzero width"
v0.12.0,test coef__inference function works
v0.12.0,test intercept__inference function works
v0.12.0,test summary function works
v0.12.0,Test inputs
v0.12.0,self._test_inputs(DR_learner)
v0.12.0,Test constant treatment effect
v0.12.0,Test heterogeneous treatment effect
v0.12.0,Test heterogenous treatment effect for W =/= None
v0.12.0,Sparse DGP
v0.12.0,Treatment effect coef
v0.12.0,Other coefs
v0.12.0,Features and controls
v0.12.0,Test sparse estimator
v0.12.0,"--> test coef_, intercept_"
v0.12.0,--> test treatment effects
v0.12.0,Restrict x_test to vectors of norm < 1
v0.12.0,--> check inference
v0.12.0,Check that a majority of true effects lie in the 5-95% CI
v0.12.0,test outer grouping
v0.12.0,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.12.0,test nested grouping
v0.12.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0,whichever groups we saw
v0.12.0,test nested grouping
v0.12.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0,helper class
v0.12.0,Fit learner and get the effect
v0.12.0,Get the true treatment effect
v0.12.0,Compute treatment effect residuals (absolute)
v0.12.0,Check that at least 90% of predictions are within tolerance interval
v0.12.0,Only for heterogeneous TE
v0.12.0,Fit learner on X and W and get the effect
v0.12.0,Get the true treatment effect
v0.12.0,Compute treatment effect residuals (absolute)
v0.12.0,Check that at least 90% of predictions are within tolerance interval
v0.12.0,Check that one can pass in regular lists
v0.12.0,Check that it fails correctly if lists of different shape are passed in
v0.12.0,Check that it fails when T contains values other than 0 and 1
v0.12.0,"Check that it works when T, Y have shape (n, 1)"
v0.12.0,Generate covariates
v0.12.0,Generate treatment
v0.12.0,Calculate outcome
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,DGP constants
v0.12.0,DGP coefficients
v0.12.0,Generated outcomes
v0.12.0,################
v0.12.0,WeightedLasso #
v0.12.0,################
v0.12.0,Define weights
v0.12.0,Define extended datasets
v0.12.0,Range of alphas
v0.12.0,Compare with Lasso
v0.12.0,--> No intercept
v0.12.0,--> With intercept
v0.12.0,When DGP has no intercept
v0.12.0,When DGP has intercept
v0.12.0,--> Coerce coefficients to be positive
v0.12.0,--> Toggle max_iter & tol
v0.12.0,Define weights
v0.12.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0,Mixed DGP scenario.
v0.12.0,Define extended datasets
v0.12.0,Define weights
v0.12.0,Define multioutput
v0.12.0,##################
v0.12.0,WeightedLassoCV #
v0.12.0,##################
v0.12.0,Define alphas to test
v0.12.0,Compare with LassoCV
v0.12.0,--> No intercept
v0.12.0,--> With intercept
v0.12.0,--> Force parameters to be positive
v0.12.0,Choose a smaller n to speed-up process
v0.12.0,Compare fold weights
v0.12.0,Define weights
v0.12.0,Define extended datasets
v0.12.0,Define splitters
v0.12.0,WeightedKFold splitter
v0.12.0,Map weighted splitter to an extended splitter
v0.12.0,Define alphas to test
v0.12.0,Compare with LassoCV
v0.12.0,--> No intercept
v0.12.0,--> With intercept
v0.12.0,--> Force parameters to be positive
v0.12.0,###########################
v0.12.0,MultiTaskWeightedLassoCV #
v0.12.0,###########################
v0.12.0,Define alphas to test
v0.12.0,Define splitter
v0.12.0,Compare with MultiTaskLassoCV
v0.12.0,--> No intercept
v0.12.0,--> With intercept
v0.12.0,Define weights
v0.12.0,Define extended datasets
v0.12.0,Define splitters
v0.12.0,WeightedKFold splitter
v0.12.0,Map weighted splitter to an extended splitter
v0.12.0,Define alphas to test
v0.12.0,Compare with LassoCV
v0.12.0,--> No intercept
v0.12.0,--> With intercept
v0.12.0,#########################
v0.12.0,WeightedLassoCVWrapper #
v0.12.0,#########################
v0.12.0,perform 1D fit
v0.12.0,perform 2D fit
v0.12.0,################
v0.12.0,DebiasedLasso #
v0.12.0,################
v0.12.0,Test DebiasedLasso without weights
v0.12.0,--> Check debiased coeffcients without intercept
v0.12.0,--> Check debiased coeffcients with intercept
v0.12.0,--> Check 5-95 CI coverage for unit vectors
v0.12.0,Test DebiasedLasso with weights for one DGP
v0.12.0,Define weights
v0.12.0,Define extended datasets
v0.12.0,--> Check debiased coefficients
v0.12.0,Define weights
v0.12.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0,--> Check debiased coeffcients
v0.12.0,Test that attributes propagate correctly
v0.12.0,Test MultiOutputDebiasedLasso without weights
v0.12.0,--> Check debiased coeffcients without intercept
v0.12.0,--> Check debiased coeffcients with intercept
v0.12.0,--> Check CI coverage
v0.12.0,Test MultiOutputDebiasedLasso with weights
v0.12.0,Define weights
v0.12.0,Define extended datasets
v0.12.0,--> Check debiased coefficients
v0.12.0,Unit vectors
v0.12.0,Unit vectors
v0.12.0,Check coeffcients and intercept are the same within tolerance
v0.12.0,Check results are similar with tolerance 1e-6
v0.12.0,Check if multitask
v0.12.0,Check that same alpha is chosen
v0.12.0,Check that the coefficients are similar
v0.12.0,selective ridge has a simple implementation that we can test against
v0.12.0,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.12.0,"it should be the case that when we set fit_intercept to true,"
v0.12.0,it doesn't matter whether the penalized model also fits an intercept or not
v0.12.0,create an extra copy of rows with weight 2
v0.12.0,"instead of a slice, explicitly return an array of indices"
v0.12.0,_penalized_inds is only set during fitting
v0.12.0,cv exists on penalized model
v0.12.0,now we can access _penalized_inds
v0.12.0,check that we can read the cv attribute back out from the underlying model
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"global and cohort data should have exactly the same structure, but different values"
v0.12.0,local index should have as many times entries as global as there were rows passed in
v0.12.0,continuous treatments have typical treatment values equal to
v0.12.0,the mean of the absolute value of non-zero entries
v0.12.0,discrete treatments have typical treatment value 1
v0.12.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0,policy value should exceed always treating with any treatment
v0.12.0,"global shape is (d_y, sum(d_t))"
v0.12.0,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0,features; for categoricals they should appear #cats-1 times each
v0.12.0,"global and cohort data should have exactly the same structure, but different values"
v0.12.0,local index should have as many times entries as global as there were rows passed in
v0.12.0,features; for categoricals they should appear #cats-1 times each
v0.12.0,"global shape is (d_y, sum(d_t))"
v0.12.0,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0,continuous treatments have typical treatment values equal to
v0.12.0,the mean of the absolute value of non-zero entries
v0.12.0,discrete treatments have typical treatment value 1
v0.12.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0,policy value should exceed always treating with any treatment
v0.12.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0,"global and cohort data should have exactly the same structure, but different values"
v0.12.0,local index should have as many times entries as global as there were rows passed in
v0.12.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0,policy value should exceed always treating with any treatment
v0.12.0,"global shape is (d_y, sum(d_t))"
v0.12.0,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0,features; for categoricals they should appear #cats-1 times each
v0.12.0,make sure we don't run into problems dropping every index
v0.12.0,"global and cohort data should have exactly the same structure, but different values"
v0.12.0,local index should have as many times entries as global as there were rows passed in
v0.12.0,"global shape is (d_y, sum(d_t))"
v0.12.0,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0,policy value should exceed always treating with any treatment
v0.12.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0,"global and cohort data should have exactly the same structure, but different values"
v0.12.0,local index should have as many times entries as global as there were rows passed in
v0.12.0,features; for categoricals they should appear #cats-1 times each
v0.12.0,"global shape is (d_y, sum(d_t))"
v0.12.0,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0,policy value should exceed always treating with any treatment
v0.12.0,dgp
v0.12.0,model
v0.12.0,model
v0.12.0,"columns 'd', 'e', 'h' have too many values"
v0.12.0,"columns 'd', 'e' have too many values"
v0.12.0,lowering bound shouldn't affect already fit columns when warm starting
v0.12.0,"column d is now okay, too"
v0.12.0,verify that we can use a scalar treatment cost
v0.12.0,verify that we can specify per-treatment costs for each sample
v0.12.0,verify that using the same state returns the same results each time
v0.12.0,set the categories for column 'd' explicitly so that b is default
v0.12.0,"first column: 10 ones, this is fine"
v0.12.0,"second column: 6 categories, plenty of random instances of each"
v0.12.0,this is fine only if we increase the cateogry limit
v0.12.0,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.12.0,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.12.0,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.12.0,forest heterogeneity won't work
v0.12.0,"sixth column: just 1 one, not enough even without check"
v0.12.0,increase bound on cat expansion
v0.12.0,skip checks (reducing folds accordingly)
v0.12.0,"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
v0.12.0,"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
v0.12.0,Pass an example where W is irrelevant and X is confounder
v0.12.0,"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
v0.12.0,zeroed out and the test will fail
v0.12.0,"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
v0.12.0,rescaling X shouldn't affect the first stage models because they normalize the inputs
v0.12.0,"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
v0.12.0,cross terms
v0.12.0,scale by 1000 to match the input to this model:
v0.12.0,"the scale of X does matter for the final model, which keeps results in user-denominated units"
v0.12.0,rescaling X still shouldn't affect the first stage models
v0.12.0,TODO: we don't recover the correct values with enough accuracy to enable this assertion
v0.12.0,is there a different way to verify that we are learning the correct coefficients?
v0.12.0,"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,DGP constants
v0.12.0,Define data features
v0.12.0,Added `_df`to names to be different from the default cate_estimator names
v0.12.0,Generate data
v0.12.0,################################
v0.12.0,Single treatment and outcome #
v0.12.0,################################
v0.12.0,Test LinearDML
v0.12.0,|--> Test featurizers
v0.12.0,ColumnTransformer doesn't propagate column names
v0.12.0,|--> Test re-fit
v0.12.0,Test SparseLinearDML
v0.12.0,Test ForestDML
v0.12.0,###################################
v0.12.0,Mutiple treatments and outcomes #
v0.12.0,###################################
v0.12.0,Test LinearDML
v0.12.0,Test SparseLinearDML
v0.12.0,"Single outcome only, ORF does not support multiple outcomes"
v0.12.0,Test DMLOrthoForest
v0.12.0,Test DROrthoForest
v0.12.0,Test XLearner
v0.12.0,Skipping population summary names test because bootstrap inference is too slow
v0.12.0,Test SLearner
v0.12.0,Test TLearner
v0.12.0,Test LinearDRLearner
v0.12.0,Test SparseLinearDRLearner
v0.12.0,Test ForestDRLearner
v0.12.0,Test LinearIntentToTreatDRIV
v0.12.0,Test DeepIV
v0.12.0,Test categorical treatments
v0.12.0,Check refit
v0.12.0,Check refit after setting categories
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Linear models are required for parametric dml
v0.12.0,sample weighting models are required for nonparametric dml
v0.12.0,Test values
v0.12.0,TLearner test
v0.12.0,Instantiate TLearner
v0.12.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0,Test constant treatment effect with multi output Y
v0.12.0,Test heterogeneous treatment effect
v0.12.0,Need interactions between T and features
v0.12.0,Test heterogeneous treatment effect with multi output Y
v0.12.0,Instantiate DomainAdaptationLearner
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,test base values equals to mean of constant marginal effect
v0.12.0,test shape of shap values output is as expected
v0.12.0,test shape of attribute of explanation object is as expected
v0.12.0,test length of feature names equals to shap values shape
v0.12.0,test base values equals to mean of constant marginal effect
v0.12.0,test shape of shap values output is as expected
v0.12.0,test shape of attribute of explanation object is as expected
v0.12.0,test length of feature names equals to shap values shape
v0.12.0,Treatment effect function
v0.12.0,Outcome support
v0.12.0,Treatment support
v0.12.0,"Generate controls, covariates, treatments and outcomes"
v0.12.0,Heterogeneous treatment effects
v0.12.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.12.0,through shap package.
v0.12.0,test shap could generate the plot from the shap_values
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Check inputs
v0.12.0,Check inputs
v0.12.0,Check inputs
v0.12.0,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.12.0,"Thus, we append the controls column before the one-hot-encoded T"
v0.12.0,"We might want to revisit, though, since it's linearly determined by the others"
v0.12.0,Check inputs
v0.12.0,Check inputs
v0.12.0,Estimate response function
v0.12.0,Check inputs
v0.12.0,Train model on controls. Assign higher weight to units resembling
v0.12.0,treated units.
v0.12.0,Train model on the treated. Assign higher weight to units resembling
v0.12.0,control units.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,TODO: make sure to use random seeds wherever necessary
v0.12.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.12.0,"unfortunately with the Theano and Tensorflow backends,"
v0.12.0,the straightforward use of K.stop_gradient can cause an error
v0.12.0,because the parameters of the intermediate layers are now disconnected from the loss;
v0.12.0,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.12.0,so that those layers remain connected but with 0 gradient
v0.12.0,|| t - mu_i || ^2
v0.12.0,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.12.0,Use logsumexp for numeric stability:
v0.12.0,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.12.0,TODO: does the numeric stability actually make any difference?
v0.12.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.12.0,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.12.0,generate cumulative sum via matrix multiplication
v0.12.0,"Generate standard uniform values in shape (batch_size,1)"
v0.12.0,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.12.0,we use uniform_like instead with an input of an appropriate shape)
v0.12.0,convert to floats and multiply to perform equivalent of logical AND
v0.12.0,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.12.0,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.12.0,we use normal_like instead with an input of an appropriate shape)
v0.12.0,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.12.0,prevent gradient from passing through sampling
v0.12.0,three options: biased or upper-bound loss require a single number of samples;
v0.12.0,unbiased can take different numbers for the network and its gradient
v0.12.0,"sample: (() -> Layer, int) -> Layer"
v0.12.0,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.12.0,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.12.0,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.12.0,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.12.0,the dimensionality of the output of the network
v0.12.0,TODO: is there a more robust way to do this?
v0.12.0,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0,"subtle point: we need to build a new model each time,"
v0.12.0,because each model encapsulates its randomness
v0.12.0,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.12.0,not a general tensor (because of how backprop works in every framework)
v0.12.0,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.12.0,but this seems annoying...)
v0.12.0,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.12.0,TODO: any way to get this to work on batches of arbitrary size?
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.12.0,"fit on projected Z: E[T * E[T|X,Z]|X]"
v0.12.0,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.12.0,"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
v0.12.0,"shape (n,)"
v0.12.0,"shape (n,)"
v0.12.0,"shape(n,)"
v0.12.0,TODO: prel_model_effect could allow sample_var and freq_weight?
v0.12.0,"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
v0.12.0,"we need to undo the one-hot encoding for calling effect,"
v0.12.0,since it expects raw values
v0.12.0,"if discrete, return shape (n,1); if continuous return shape (n,)"
v0.12.0,target will be discrete and will be inversed from FirstStageWrapper
v0.12.0,"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
v0.12.0,reshape the predictions
v0.12.0,concat W and Z
v0.12.0,check nuisances outcome shape
v0.12.0,Y_res could be a vector or 1-dimensional 2d-array
v0.12.0,"all could be reshaped to vector since Y, T, Z are all single dimensional."
v0.12.0,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0,at the expense of some small bias. For points with very small covariance we revert
v0.12.0,to the model-based preliminary estimate and do not add the correction term.
v0.12.0,A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
v0.12.0,Used by both DRIV and IntentToTreatDRIV.
v0.12.0,Maggie: I think that would be the case?
v0.12.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.12.0,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.12.0,attribute so that the trained featurizer will be passed through
v0.12.0,Handles the corner case when X=None but featurizer might be not None
v0.12.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0,this is a regression model since proj_t is probability
v0.12.0,outcome is continuous since proj_t is probability
v0.12.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0,concat W and Z
v0.12.0,"we need to undo the one-hot encoding for calling effect,"
v0.12.0,since it expects raw values
v0.12.0,concat W and Z
v0.12.0,"we need to undo the one-hot encoding for calling effect,"
v0.12.0,since it expects raw values
v0.12.0,reshape the predictions
v0.12.0,"T_res, Z_res, beta expect shape to be (n,1)"
v0.12.0,maybe shouldn't expose fit_cate_intercept in this class?
v0.12.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0,TODO: do correct adjustment for sample_var
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,concat W and Z
v0.12.0,concat W and Z
v0.12.0,concat W and Z
v0.12.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0,"train E[T|X,W,Z]"
v0.12.0,"train [Z|X,W]"
v0.12.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.12.0,NOTE: important to use the ortho_learner_model_final_ attribute instead of the
v0.12.0,attribute so that the trained featurizer will be passed through
v0.12.0,Handles the corner case when X=None but featurizer might be not None
v0.12.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0,concat W and Z
v0.12.0,note that groups are not passed to score because they are only used for fitting
v0.12.0,concat W and Z
v0.12.0,note that sample_weight and groups are not passed to predict because they are only used for fitting
v0.12.0,concat W and Z
v0.12.0,A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
v0.12.0,Used by both Parametric and Non Parametric DMLIV.
v0.12.0,override only so that we can enforce Z to be required
v0.12.0,"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
v0.12.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0,for internal use by the library
v0.12.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0,Handles the corner case when X=None but featurizer might be not None
v0.12.0,Get input names
v0.12.0,Summary
v0.12.0,coefficient
v0.12.0,intercept
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,"this will have dimension (d,) + shape(X)"
v0.12.0,send the first dimension to the end
v0.12.0,columns are featurized independently; partial derivatives are only non-zero
v0.12.0,when taken with respect to the same column each time
v0.12.0,don't fit intercept; manually add column of ones to the data instead;
v0.12.0,this allows us to ignore the intercept when computing marginal effects
v0.12.0,make T 2D if if was a vector
v0.12.0,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.12.0,two stage approximation
v0.12.0,"first, get basis expansions of T, X, and Z"
v0.12.0,TODO: is it right that the effective number of intruments is the
v0.12.0,"product of ft_X and ft_Z, not just ft_Z?"
v0.12.0,"regress T expansion on X,Z expansions concatenated with W"
v0.12.0,"predict ft_T from interacted ft_X, ft_Z"
v0.12.0,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.12.0,dT may be only 2-dimensional)
v0.12.0,promote dT to 3D if necessary (e.g. if T was a vector)
v0.12.0,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,TODO: this utility is documented but internal; reimplement?
v0.12.0,TODO: this utility is even less public...
v0.12.0,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.12.0,use same Cs as would be used by default by LogisticRegressionCV
v0.12.0,NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
v0.12.0,which could affect how many times each distinct Y value needs to be present in the data
v0.12.0,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.12.0,but also supports get_feature_names with expected signature
v0.12.0,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.12.0,NOTE: we rely on the passthrough columns coming first in the concatenated X;W
v0.12.0,"when we pipeline scaling with our first stage models later, so the order here is important"
v0.12.0,Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
v0.12.0,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.12.0,Convert SingleTreeInterpreter to a python dictionary
v0.12.0,named tuple type for storing results inside CausalAnalysis class;
v0.12.0,must be lifted to module level to enable pickling
v0.12.0,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
v0.12.0,"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
v0.12.0,"when running the first stage models, but don't want to scale the X columns when running the final model,"
v0.12.0,since then our coefficients will have odd units and our trees will also have decisions using those units.
v0.12.0,
v0.12.0,"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
v0.12.0,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.12.0,Controls are all other columns of X
v0.12.0,"can't use X[:, feat_ind] when X is a DataFrame"
v0.12.0,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.12.0,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.12.0,so that the user can opt-in to allowing unseen treatment values
v0.12.0,(and return NaN or something in that case)
v0.12.0,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
v0.12.0,and so we can just peel the first columns off of that combined array for rescaling in the pipeline
v0.12.0,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
v0.12.0,"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
v0.12.0,becomes a valid approach to handling this
v0.12.0,array checking routines don't accept 0-width arrays
v0.12.0,perform model selection
v0.12.0,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.12.0,convert to NormalInferenceResults for consistency
v0.12.0,Set the dictionary values shared between local and global summaries
v0.12.0,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.12.0,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.12.0,required to fit a discrete DML model
v0.12.0,Validate inputs
v0.12.0,TODO: check compatibility of X and Y lengths
v0.12.0,"no previous fit, cancel warm start"
v0.12.0,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.12.0,"if heterogeneity_inds is 1D, repeat it"
v0.12.0,heterogeneity inds should be a 2D list of length same as train_inds
v0.12.0,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.12.0,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.12.0,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.12.0,train the Y model
v0.12.0,"perform model selection for the Y model using all X, not on a per-column basis"
v0.12.0,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.12.0,work with the regression wrapper
v0.12.0,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.12.0,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.12.0,since otherwise we'll have too many columns to be able to train a classifier
v0.12.0,start with empty results and default shared insights
v0.12.0,convert categorical indicators to numeric indices
v0.12.0,check for indices over the categorical expansion bound
v0.12.0,assume we'll be able to train former failures this time; we'll add them back if not
v0.12.0,"can't remove in place while iterating over new_inds, so store in separate list"
v0.12.0,"train the model, but warn"
v0.12.0,no model can be trained in this case since we need more folds
v0.12.0,"don't train a model, but suggest workaround since there are enough instances of least"
v0.12.0,populated class
v0.12.0,also remove from train_inds so we don't try to access the result later
v0.12.0,extract subset of names matching new columns
v0.12.0,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.12.0,don't want to cache this failed result
v0.12.0,properties to return from effect InferenceResults
v0.12.0,properties to return from PopulationSummaryResults
v0.12.0,Converts strings to property lookups or method calls as a convenience so that the
v0.12.0,_point_props and _summary_props above can be applied to an inference object
v0.12.0,Create a summary combining all results into a single output; this is used
v0.12.0,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.12.0,"or a dictionary, respectively, based on the summary function passed into this method"
v0.12.0,"ensure array has shape (m,y,t)"
v0.12.0,population summary is missing sample dimension; add it for consistency
v0.12.0,outcome dimension is missing; add it for consistency
v0.12.0,add singleton treatment dimension if missing
v0.12.0,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.12.0,"each attr has dimension (m,y) or (m,y,t)"
v0.12.0,concatenate along treatment dimension
v0.12.0,"for dictionary representation, want to remove unneeded sample dimension"
v0.12.0,in cohort and global results
v0.12.0,TODO: enrich outcome logic for multi-class classification when that is supported
v0.12.0,There is no actual sample level in this data
v0.12.0,can't drop only level
v0.12.0,should be serialization-ready and contain no numpy arrays
v0.12.0,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.12.0,TODO: Note that there's no column metadata for the sample number - should there be?
v0.12.0,"need to replicate the column info for each sample, then remove from the shared data"
v0.12.0,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.12.0,which may need to be revisited once we support multiclass
v0.12.0,get the length of the list corresponding to the first dictionary key
v0.12.0,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.12.0,a global inference indicates the effect of that one feature on the outcome
v0.12.0,need to reshape the output to match the input
v0.12.0,we want to offset the inference object by the baseline estimate of y
v0.12.0,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.12.0,get the length of the list corresponding to the first dictionary key
v0.12.0,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.12.0,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.12.0,because then scaling the difference between treatment value and treatment costs is the
v0.12.0,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.12.0,
v0.12.0,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.12.0,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.12.0,(rather than just not treating at all)
v0.12.0,
v0.12.0,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.12.0,2 * policy_value - always_treat
v0.12.0,includes exactly their contribution because policy_value and always_treat both include it
v0.12.0,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.12.0,2 * policy_value - always-treat
v0.12.0,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.12.0,is zero and the contribution to always_treat is negative
v0.12.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0,get dataframe with all but selected column
v0.12.0,apply 10% of a typical treatment for this feature
v0.12.0,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.12.0,set the effect bounds; for positive treatments these agree with
v0.12.0,"the estimates; for negative treatments, we need to invert the interval"
v0.12.0,the effect is now always positive since we decrease treatment when negative
v0.12.0,"for discrete treatment, stack a zero result in front for control"
v0.12.0,we need to call effect_inference to get the correct CI between the two treatment options
v0.12.0,we now need to construct the delta in the cost between the two treatments and translate the effect
v0.12.0,remove third dimenions potentially added
v0.12.0,"find cost of current treatment: equality creates a 2d array with True on each row,"
v0.12.0,only if its the location of the current treatment. Then we take the corresponding cost.
v0.12.0,construct index of current treatment
v0.12.0,add second dimension if needed for broadcasting during translation of effect
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,TODO: conisder working around relying on sklearn implementation details
v0.12.0,"Found a good split, return."
v0.12.0,Record all splits in case the stratification by weight yeilds a worse partition
v0.12.0,Reseed random generator and try again
v0.12.0,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.12.0,"Found a good split, return."
v0.12.0,Did not find a good split
v0.12.0,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.12.0,Return most weight-balanced partition
v0.12.0,Weight stratification algorithm
v0.12.0,Sort weights for weight strata search
v0.12.0,There are some leftover indices that have yet to be assigned
v0.12.0,Append stratum splits to overall splits
v0.12.0,"If classification methods produce multiple columns of output,"
v0.12.0,we need to manually encode classes to ensure consistent column ordering.
v0.12.0,We clone the estimator to make sure that all the folds are
v0.12.0,"independent, and that it is pickle-able."
v0.12.0,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.12.0,`predictions` is a list of method outputs from each fold.
v0.12.0,"If each of those is also a list, then treat this as a"
v0.12.0,multioutput-multiclass task. We need to separately concatenate
v0.12.0,the method outputs for each label into an `n_labels` long list.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Our classes that derive from sklearn ones sometimes include
v0.12.0,inherited docstrings that have embedded doctests; we need the following imports
v0.12.0,so that they don't break.
v0.12.0,TODO: consider working around relying on sklearn implementation details
v0.12.0,"Convert X, y into numpy arrays"
v0.12.0,Define fit parameters
v0.12.0,Some algorithms don't have a check_input option
v0.12.0,Check weights array
v0.12.0,Check that weights are size-compatible
v0.12.0,Normalize inputs
v0.12.0,Weight inputs
v0.12.0,Fit base class without intercept
v0.12.0,Fit Lasso
v0.12.0,Reset intercept
v0.12.0,The intercept is not calculated properly due the sqrt(weights) factor
v0.12.0,so it must be recomputed
v0.12.0,Fit lasso without weights
v0.12.0,Make weighted splitter
v0.12.0,Fit weighted model
v0.12.0,Make weighted splitter
v0.12.0,Fit weighted model
v0.12.0,Call weighted lasso on reduced design matrix
v0.12.0,Weighted tau
v0.12.0,Select optimal penalty
v0.12.0,Warn about consistency
v0.12.0,"Convert X, y into numpy arrays"
v0.12.0,Fit weighted lasso with user input
v0.12.0,"Center X, y"
v0.12.0,Calculate quantities that will be used later on. Account for centered data
v0.12.0,Calculate coefficient and error variance
v0.12.0,Add coefficient correction
v0.12.0,Set coefficients and intercept standard errors
v0.12.0,Set intercept
v0.12.0,Return alpha to 'auto' state
v0.12.0,"Note that in the case of no intercept, X_offset is 0"
v0.12.0,Calculate the variance of the predictions
v0.12.0,Calculate prediction confidence intervals
v0.12.0,Assumes flattened y
v0.12.0,Compute weighted residuals
v0.12.0,To be done once per target. Assumes y can be flattened.
v0.12.0,Assumes that X has already been offset
v0.12.0,Special case: n_features=1
v0.12.0,Compute Lasso coefficients for the columns of the design matrix
v0.12.0,Compute C_hat
v0.12.0,Compute theta_hat
v0.12.0,Allow for single output as well
v0.12.0,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.12.0,Set coef_ attribute
v0.12.0,Set intercept_ attribute
v0.12.0,Set selected_alpha_ attribute
v0.12.0,Set coef_stderr_
v0.12.0,intercept_stderr_
v0.12.0,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.12.0,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.12.0,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.12.0,set intercept_ attribute
v0.12.0,set coef_ attribute
v0.12.0,set alpha_ attribute
v0.12.0,set alphas_ attribute
v0.12.0,set n_iter_ attribute
v0.12.0,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.12.0,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.12.0,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.12.0,now regress X1 on y - X2 * beta2 to learn beta1
v0.12.0,set coef_ and intercept_ attributes
v0.12.0,Note that the penalized model should *not* have an intercept
v0.12.0,don't proxy special methods
v0.12.0,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.12.0,regressor incorrectly
v0.12.0,"Note: for known attributes that have been set this method will not be called,"
v0.12.0,so we should just throw here because this is an attribute belonging to this class
v0.12.0,but which hasn't yet been set on this instance
v0.12.0,set default values for None
v0.12.0,check freq_weight should be integer and should be accompanied by sample_var
v0.12.0,check array shape
v0.12.0,weight X and y and sample_var
v0.12.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.12.0,set default values for None
v0.12.0,check array shape
v0.12.0,check dimension of instruments is more than dimension of treatments
v0.12.0,weight X and y
v0.12.0,learn point estimate
v0.12.0,solve first stage linear regression E[T|Z]
v0.12.0,"""that"" means T̂"
v0.12.0,solve second stage linear regression E[Y|that]
v0.12.0,(T̂.T*T̂)^{-1}
v0.12.0,learn cov(theta)
v0.12.0,(T̂.T*T̂)^{-1}
v0.12.0,sigma^2
v0.12.0,reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,AzureML
v0.12.0,helper imports
v0.12.0,write the details of the workspace to a configuration file to the notebook library
v0.12.0,if y is a multioutput model
v0.12.0,Make sure second dimension has 1 or more item
v0.12.0,switch _inner Model to a MultiOutputRegressor
v0.12.0,flatten array as automl only takes vectors for y
v0.12.0,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.12.0,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.12.0,as an sklearn estimator
v0.12.0,fit implementation for a single output model.
v0.12.0,Create experiment for specified workspace
v0.12.0,Configure automl_config with training set information.
v0.12.0,"Wait for remote run to complete, the set the model"
v0.12.0,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.12.0,create model and pass model into final.
v0.12.0,"If item is an automl config, get its corresponding"
v0.12.0,AutomatedML Model and add it to new_Args
v0.12.0,"If item is an automl config, get its corresponding"
v0.12.0,AutomatedML Model and set it for this key in
v0.12.0,kwargs
v0.12.0,takes in either automated_ml config and instantiates
v0.12.0,an AutomatedMLModel
v0.12.0,The prefix can only be 18 characters long
v0.12.0,"because prefixes come from kwarg_names, we must ensure they are"
v0.12.0,short enough.
v0.12.0,Get workspace from config file.
v0.12.0,Take the intersect of the white for sample
v0.12.0,weights and linear models
v0.12.0,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,average the outcome dimension if it exists and ensure 2d y_pred
v0.12.0,get index of best treatment
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,TODO: consider working around relying on sklearn implementation details
v0.12.0,Create splits of causal tree
v0.12.0,Make sure the correct exception is being rethrown
v0.12.0,Must make sure indices are merged correctly
v0.12.0,Convert rows to columns
v0.12.0,Require group assignment t to be one-hot-encoded
v0.12.0,Get predictions for the 2 splits
v0.12.0,Must make sure indices are merged correctly
v0.12.0,Crossfitting
v0.12.0,Compute weighted nuisance estimates
v0.12.0,-------------------------------------------------------------------------------
v0.12.0,Calculate the covariance matrix corresponding to the BLB inference
v0.12.0,
v0.12.0,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.12.0,2. Calculate the weighted moments for each tree slice to create a matrix
v0.12.0,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.12.0,in that slice from the overall parameter estimate.
v0.12.0,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.12.0,-------------------------------------------------------------------------------
v0.12.0,Calclulate covariance matrix through BLB
v0.12.0,Estimators
v0.12.0,OrthoForest parameters
v0.12.0,Sub-forests
v0.12.0,Auxiliary attributes
v0.12.0,Fit check
v0.12.0,TODO: Check performance
v0.12.0,Must normalize weights
v0.12.0,Override the CATE inference options
v0.12.0,Add blb inference to parent's options
v0.12.0,Generate subsample indices
v0.12.0,Build trees in parallel
v0.12.0,Bootstraping has repetitions in tree sample
v0.12.0,Similar for `a` weights
v0.12.0,Bootstraping has repetitions in tree sample
v0.12.0,Define subsample size
v0.12.0,Safety check
v0.12.0,Draw points to create little bags
v0.12.0,Copy and/or define models
v0.12.0,Define nuisance estimators
v0.12.0,Define parameter estimators
v0.12.0,Define
v0.12.0,Need to redefine fit here for auto inference to work due to a quirk in how
v0.12.0,wrap_fit is defined
v0.12.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0,Override to flatten output if T is flat
v0.12.0,Check that all discrete treatments are represented
v0.12.0,Nuissance estimates evaluated with cross-fitting
v0.12.0,Define 2-fold iterator
v0.12.0,Check if there is only one example of some class
v0.12.0,Define 2-fold iterator
v0.12.0,need safe=False when cloning for WeightedModelWrapper
v0.12.0,Compute residuals
v0.12.0,Compute coefficient by OLS on residuals
v0.12.0,"Parameter returned by LinearRegression is (d_T, )"
v0.12.0,Compute residuals
v0.12.0,Compute coefficient by OLS on residuals
v0.12.0,ell_2 regularization
v0.12.0,Ridge regression estimate
v0.12.0,"Parameter returned is of shape (d_T, )"
v0.12.0,Return moments and gradients
v0.12.0,Compute residuals
v0.12.0,Compute moments
v0.12.0,"Moments shape is (n, d_T)"
v0.12.0,Compute moment gradients
v0.12.0,returns shape-conforming residuals
v0.12.0,Copy and/or define models
v0.12.0,Define parameter estimators
v0.12.0,Define moment and mean gradient estimator
v0.12.0,"Check that T is shape (n, )"
v0.12.0,Check T is numeric
v0.12.0,Train label encoder
v0.12.0,Call `fit` from parent class
v0.12.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0,Override to flatten output if T is flat
v0.12.0,Expand one-hot encoding to include the zero treatment
v0.12.0,"Test that T contains all treatments. If not, return None"
v0.12.0,Nuissance estimates evaluated with cross-fitting
v0.12.0,Define 2-fold iterator
v0.12.0,Check if there is only one example of some class
v0.12.0,No need to crossfit for internal nodes
v0.12.0,Compute partial moments
v0.12.0,"If any of the values in the parameter estimate is nan, return None"
v0.12.0,Compute partial moments
v0.12.0,Compute coefficient by OLS on residuals
v0.12.0,ell_2 regularization
v0.12.0,Ridge regression estimate
v0.12.0,"Parameter returned is of shape (d_T, )"
v0.12.0,Return moments and gradients
v0.12.0,Compute partial moments
v0.12.0,Compute moments
v0.12.0,"Moments shape is (n, d_T-1)"
v0.12.0,Compute moment gradients
v0.12.0,Need to calculate this in an elegant way for when propensity is 0
v0.12.0,This will flatten T
v0.12.0,Check that T is numeric
v0.12.0,Test whether the input estimator is supported
v0.12.0,Calculate confidence intervals for the parameter (marginal effect)
v0.12.0,Calculate confidence intervals for the effect
v0.12.0,Calculate the effects
v0.12.0,Calculate the standard deviations for the effects
v0.12.0,d_t=None here since we measure the effect across all Ts
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Causal tree parameters
v0.12.0,Tree structure
v0.12.0,No need for a random split since the data is already
v0.12.0,a random subsample from the original input
v0.12.0,node list stores the nodes that are yet to be splitted
v0.12.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0,Create local sample set
v0.12.0,Compute nuisance estimates for the current node
v0.12.0,Nuisance estimate cannot be calculated
v0.12.0,Estimate parameter for current node
v0.12.0,Node estimate cannot be calculated
v0.12.0,Calculate moments and gradient of moments for current data
v0.12.0,Calculate inverse gradient
v0.12.0,The gradient matrix is not invertible.
v0.12.0,No good split can be found
v0.12.0,Calculate point-wise pseudo-outcomes rho
v0.12.0,a split is determined by a feature and a sample pair
v0.12.0,the number of possible splits is at most (number of features) * (number of node samples)
v0.12.0,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.12.0,parse row and column of random pair
v0.12.0,the sample of the pair is the integer division of the random number with n_feats
v0.12.0,calculate the binary indicator of whether sample i is on the left or the right
v0.12.0,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.12.0,calculate the number of samples on the left child for each proposed split
v0.12.0,calculate the analogous binary indicator for the samples in the estimation set
v0.12.0,calculate the number of estimation samples on the left child of each proposed split
v0.12.0,find the upper and lower bound on the size of the left split for the split
v0.12.0,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.12.0,on each side.
v0.12.0,similarly for the estimation sample set
v0.12.0,if there is no valid split then don't create any children
v0.12.0,filter only the valid splits
v0.12.0,calculate the average influence vector of the samples in the left child
v0.12.0,calculate the average influence vector of the samples in the right child
v0.12.0,take the square of each of the entries of the influence vectors and normalize
v0.12.0,by size of each child
v0.12.0,calculate the vector score of each candidate split as the average of left and right
v0.12.0,influence vectors
v0.12.0,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.12.0,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.12.0,where there might be large discontinuities in some parameter as the conditioning set varies
v0.12.0,calculate the scalar score of each split by aggregating across the vector of scores
v0.12.0,Find split that minimizes criterion
v0.12.0,Create child nodes with corresponding subsamples
v0.12.0,add the created children to the list of not yet split nodes
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0,Licensed under the MIT License.
v0.12.0,TODO: update docs
v0.12.0,"NOTE: sample weight, sample var are not passed in"
v0.12.0,Compose final model
v0.12.0,Calculate auxiliary quantities
v0.12.0,X ⨂ T_res
v0.12.0,"sum(model_final.predict(X, T_res))"
v0.12.0,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0,"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
v0.12.0,generate an instance of the final model
v0.12.0,generate an instance of the nuisance model
v0.12.0,Set _d_t to effective number of treatments
v0.12.0,Required for bootstrap inference
v0.12.0,for each mc iteration
v0.12.0,for each model under cross fit setting
v0.12.0,Handles the corner case when X=None but featurizer might be not None
v0.12.0,Expand treatments for each time period
v0.12.0,NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
v0.12.0,attribute so that the trained featurizer will be passed through
v0.12.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0,for internal use by the library
v0.12.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0,We need to use the _ortho_learner's copy to retain the information from fitting
v0.12.0b6,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.12.0b6,configuration is all pulled from setup.cfg
v0.12.0b6,-*- coding: utf-8 -*-
v0.12.0b6,
v0.12.0b6,Configuration file for the Sphinx documentation builder.
v0.12.0b6,
v0.12.0b6,This file does only contain a selection of the most common options. For a
v0.12.0b6,full list see the documentation:
v0.12.0b6,http://www.sphinx-doc.org/en/master/config
v0.12.0b6,-- Path setup --------------------------------------------------------------
v0.12.0b6,"If extensions (or modules to document with autodoc) are in another directory,"
v0.12.0b6,add these directories to sys.path here. If the directory is relative to the
v0.12.0b6,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.12.0b6,
v0.12.0b6,-- Project information -----------------------------------------------------
v0.12.0b6,-- General configuration ---------------------------------------------------
v0.12.0b6,"If your documentation needs a minimal Sphinx version, state it here."
v0.12.0b6,
v0.12.0b6,needs_sphinx = '1.0'
v0.12.0b6,"Add any Sphinx extension module names here, as strings. They can be"
v0.12.0b6,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.12.0b6,ones.
v0.12.0b6,"Add any paths that contain templates here, relative to this directory."
v0.12.0b6,The suffix(es) of source filenames.
v0.12.0b6,You can specify multiple suffix as a list of string:
v0.12.0b6,
v0.12.0b6,"source_suffix = ['.rst', '.md']"
v0.12.0b6,The master toctree document.
v0.12.0b6,The language for content autogenerated by Sphinx. Refer to documentation
v0.12.0b6,for a list of supported languages.
v0.12.0b6,
v0.12.0b6,This is also used if you do content translation via gettext catalogs.
v0.12.0b6,"Usually you set ""language"" from the command line for these cases."
v0.12.0b6,"List of patterns, relative to source directory, that match files and"
v0.12.0b6,directories to ignore when looking for source files.
v0.12.0b6,This pattern also affects html_static_path and html_extra_path.
v0.12.0b6,The name of the Pygments (syntax highlighting) style to use.
v0.12.0b6,-- Options for HTML output -------------------------------------------------
v0.12.0b6,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.12.0b6,a list of builtin themes.
v0.12.0b6,
v0.12.0b6,Theme options are theme-specific and customize the look and feel of a theme
v0.12.0b6,"further.  For a list of options available for each theme, see the"
v0.12.0b6,documentation.
v0.12.0b6,
v0.12.0b6,"Add any paths that contain custom static files (such as style sheets) here,"
v0.12.0b6,"relative to this directory. They are copied after the builtin static files,"
v0.12.0b6,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.12.0b6,html_static_path = ['_static']
v0.12.0b6,"Custom sidebar templates, must be a dictionary that maps document names"
v0.12.0b6,to template names.
v0.12.0b6,
v0.12.0b6,The default sidebars (for documents that don't match any pattern) are
v0.12.0b6,defined by theme itself.  Builtin themes are using these templates by
v0.12.0b6,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.12.0b6,'searchbox.html']``.
v0.12.0b6,
v0.12.0b6,html_sidebars = {}
v0.12.0b6,-- Options for HTMLHelp output ---------------------------------------------
v0.12.0b6,Output file base name for HTML help builder.
v0.12.0b6,-- Options for LaTeX output ------------------------------------------------
v0.12.0b6,The paper size ('letterpaper' or 'a4paper').
v0.12.0b6,
v0.12.0b6,"'papersize': 'letterpaper',"
v0.12.0b6,"The font size ('10pt', '11pt' or '12pt')."
v0.12.0b6,
v0.12.0b6,"'pointsize': '10pt',"
v0.12.0b6,Additional stuff for the LaTeX preamble.
v0.12.0b6,
v0.12.0b6,"'preamble': '',"
v0.12.0b6,Latex figure (float) alignment
v0.12.0b6,
v0.12.0b6,"'figure_align': 'htbp',"
v0.12.0b6,Grouping the document tree into LaTeX files. List of tuples
v0.12.0b6,"(source start file, target name, title,"
v0.12.0b6,"author, documentclass [howto, manual, or own class])."
v0.12.0b6,-- Options for manual page output ------------------------------------------
v0.12.0b6,One entry per manual page. List of tuples
v0.12.0b6,"(source start file, name, description, authors, manual section)."
v0.12.0b6,-- Options for Texinfo output ----------------------------------------------
v0.12.0b6,Grouping the document tree into Texinfo files. List of tuples
v0.12.0b6,"(source start file, target name, title, author,"
v0.12.0b6,"dir menu entry, description, category)"
v0.12.0b6,-- Options for Epub output -------------------------------------------------
v0.12.0b6,Bibliographic Dublin Core info.
v0.12.0b6,The unique identifier of the text. This can be a ISBN number
v0.12.0b6,or the project homepage.
v0.12.0b6,
v0.12.0b6,epub_identifier = ''
v0.12.0b6,A unique identification for the text.
v0.12.0b6,
v0.12.0b6,epub_uid = ''
v0.12.0b6,A list of files that should not be packed into the epub file.
v0.12.0b6,-- Extension configuration -------------------------------------------------
v0.12.0b6,-- Options for intersphinx extension ---------------------------------------
v0.12.0b6,Example configuration for intersphinx: refer to the Python standard library.
v0.12.0b6,-- Options for todo extension ----------------------------------------------
v0.12.0b6,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.12.0b6,-- Options for doctest extension -------------------------------------------
v0.12.0b6,we can document otherwise excluded entities here by returning False
v0.12.0b6,or skip otherwise included entities by returning True
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Calculate residuals
v0.12.0b6,Estimate E[T_res | Z_res]
v0.12.0b6,TODO. Deal with multi-class instrument
v0.12.0b6,Calculate nuisances
v0.12.0b6,Estimate E[T_res | Z_res]
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"We do a three way split, as typically a preliminary theta estimator would require"
v0.12.0b6,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.12.0b6,TODO. Deal with multi-class instrument
v0.12.0b6,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b6,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b6,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b6,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b6,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b6,Estimate preliminary theta in cross fitting manner
v0.12.0b6,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b6,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.12.0b6,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.12.0b6,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b6,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.12.0b6,TODO. The solution below is not really a valid cross-fitting
v0.12.0b6,as the test data are used to create the proj_t on the train
v0.12.0b6,which in the second train-test loop is used to create the nuisance
v0.12.0b6,cov on the test data. Hence the T variable of some sample
v0.12.0b6,"is implicitly correlated with its cov nuisance, through this flow"
v0.12.0b6,"of information. However, this seems a rather weak correlation."
v0.12.0b6,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.12.0b6,model.
v0.12.0b6,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.12.0b6,Estimate preliminary theta in cross fitting manner
v0.12.0b6,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b6,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.12.0b6,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b6,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.12.0b6,#############################################################################
v0.12.0b6,Classes for the DRIV implementation for the special case of intent-to-treat
v0.12.0b6,A/B test
v0.12.0b6,#############################################################################
v0.12.0b6,Estimate preliminary theta in cross fitting manner
v0.12.0b6,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b6,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b6,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b6,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b6,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.12.0b6,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.12.0b6,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.12.0b6,model_T_XZ = lambda: model_clf()
v0.12.0b6,#'days_visited': lambda:
v0.12.0b6,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.12.0b6,Turn strings into categories for numeric mapping
v0.12.0b6,### Defining some generic regressors and classifiers
v0.12.0b6,This a generic non-parametric regressor
v0.12.0b6,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b6,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.12.0b6,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b6,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.12.0b6,model = lambda: RandomForestRegressor(n_estimators=100)
v0.12.0b6,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.12.0b6,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.12.0b6,model = lambda: LinearRegression(n_jobs=-1)
v0.12.0b6,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.12.0b6,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.12.0b6,underlying model whenever predict is called.
v0.12.0b6,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b6,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.12.0b6,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b6,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.12.0b6,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.12.0b6,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.12.0b6,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.12.0b6,We need to specify models to be used for each of these residualizations
v0.12.0b6,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.12.0b6,"E[T | X, Z]"
v0.12.0b6,E[TZ | X]
v0.12.0b6,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.12.0b6,n_splits determines the number of splits to be used for cross-fitting.
v0.12.0b6,# Algorithm 2 - Current Method
v0.12.0b6,In[121]:
v0.12.0b6,# Algorithm 3 - DRIV ATE
v0.12.0b6,dmliv_model_effect = lambda: model()
v0.12.0b6,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.12.0b6,"dmliv_model_effect(),"
v0.12.0b6,n_splits=1)
v0.12.0b6,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.12.0b6,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.12.0b6,"Once multiple treatments are supported, we'll need to fix this"
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b6,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b6,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b6,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,TODO. Deal with multi-class instrument/treatment
v0.12.0b6,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.12.0b6,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.12.0b6,Estimate p(X) = E[T | X] in cross-fitting manner
v0.12.0b6,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.12.0b6,##################
v0.12.0b6,Global settings #
v0.12.0b6,##################
v0.12.0b6,Global plotting controls
v0.12.0b6,"Control for support size, can control for more"
v0.12.0b6,#################
v0.12.0b6,File utilities #
v0.12.0b6,#################
v0.12.0b6,#################
v0.12.0b6,Plotting utils #
v0.12.0b6,#################
v0.12.0b6,bias
v0.12.0b6,var
v0.12.0b6,rmse
v0.12.0b6,r2
v0.12.0b6,Infer feature dimension
v0.12.0b6,Metrics by support plots
v0.12.0b6,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.12.0b6,Vasilis Syrgkanis <vasy@microsoft.com>
v0.12.0b6,Steven Wu <zhiww@microsoft.com>
v0.12.0b6,Initialize causal tree parameters
v0.12.0b6,Create splits of causal tree
v0.12.0b6,Estimate treatment effects at the leafs
v0.12.0b6,Compute heterogeneous treatement effect for x's in x_list by finding
v0.12.0b6,the corresponding split and associating the effect computed on that leaf
v0.12.0b6,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.12.0b6,Safety check
v0.12.0b6,Weighted linear regression
v0.12.0b6,Calculates weights
v0.12.0b6,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b6,over all indices
v0.12.0b6,Similar for `a` weights
v0.12.0b6,Doesn't have sample weights
v0.12.0b6,Is a linear model
v0.12.0b6,Weighted linear regression
v0.12.0b6,Calculates weights
v0.12.0b6,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b6,over all indices
v0.12.0b6,Similar for `a` weights
v0.12.0b6,normalize weights
v0.12.0b6,"Split the data in half, train and test"
v0.12.0b6,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b6,"a function of W, using only the train fold"
v0.12.0b6,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b6,"Split the data in half, train and test"
v0.12.0b6,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b6,"a function of W, using only the train fold"
v0.12.0b6,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b6,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.12.0b6,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.12.0b6,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.12.0b6,"Split the data in half, train and test"
v0.12.0b6,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b6,"a function of x, using only the train fold"
v0.12.0b6,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b6,Compute coefficient by OLS on residuals
v0.12.0b6,"Split the data in half, train and test"
v0.12.0b6,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b6,"a function of x, using only the train fold"
v0.12.0b6,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b6,Estimate multipliers for second order orthogonal method
v0.12.0b6,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.12.0b6,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b6,Create local sample set
v0.12.0b6,compute the base estimate for the current node using double ml or second order double ml
v0.12.0b6,compute the influence functions here that are used for the criterion
v0.12.0b6,generate random proposals of dimensions to split
v0.12.0b6,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.12.0b6,compute criterion for each proposal
v0.12.0b6,if splitting creates valid leafs in terms of mean leaf size
v0.12.0b6,Calculate criterion for split
v0.12.0b6,Else set criterion to infinity so that this split is not chosen
v0.12.0b6,If no good split was found
v0.12.0b6,Find split that minimizes criterion
v0.12.0b6,Set the split attributes at the node
v0.12.0b6,Create child nodes with corresponding subsamples
v0.12.0b6,Recursively split children
v0.12.0b6,Return parent node
v0.12.0b6,estimate the local parameter at the leaf using the estimate data
v0.12.0b6,###################
v0.12.0b6,Argument parsing #
v0.12.0b6,###################
v0.12.0b6,#########################################
v0.12.0b6,Parameters constant across experiments #
v0.12.0b6,#########################################
v0.12.0b6,Outcome support
v0.12.0b6,Treatment support
v0.12.0b6,Evaluation grid
v0.12.0b6,Treatment effects array
v0.12.0b6,Other variables
v0.12.0b6,##########################
v0.12.0b6,Data Generating Process #
v0.12.0b6,##########################
v0.12.0b6,Log iteration
v0.12.0b6,"Generate controls, features, treatment and outcome"
v0.12.0b6,T and Y residuals to be used in later scripts
v0.12.0b6,Save generated dataset
v0.12.0b6,#################
v0.12.0b6,ORF parameters #
v0.12.0b6,#################
v0.12.0b6,######################################
v0.12.0b6,Train and evaluate treatment effect #
v0.12.0b6,######################################
v0.12.0b6,########
v0.12.0b6,Plots #
v0.12.0b6,########
v0.12.0b6,###############
v0.12.0b6,Save results #
v0.12.0b6,###############
v0.12.0b6,##############
v0.12.0b6,Run Rscript #
v0.12.0b6,##############
v0.12.0b6,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b6,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b6,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.12.0b6,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b6,def mlasso_model(): return MultiTaskLassoCV(
v0.12.0b6,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b6,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b6,heterogeneity
v0.12.0b6,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b6,heterogeneity
v0.12.0b6,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b6,heterogeneity
v0.12.0b6,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.12.0b6,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b6,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b6,subset of features that are exogenous and create heterogeneity
v0.12.0b6,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.12.0b6,subset of features wrt we estimate heterogeneity
v0.12.0b6,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b6,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,introspect the constructor arguments to find the model parameters
v0.12.0b6,to represent
v0.12.0b6,"if the argument is deprecated, ignore it"
v0.12.0b6,Extract and sort argument names excluding 'self'
v0.12.0b6,column names
v0.12.0b6,transfer input to numpy arrays
v0.12.0b6,transfer input to 2d arrays
v0.12.0b6,create dataframe
v0.12.0b6,currently dowhy only support single outcome and single treatment
v0.12.0b6,call dowhy
v0.12.0b6,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.12.0b6,cate estimator but not the effect.
v0.12.0b6,don't proxy special methods
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Check if model is sparse enough for this model
v0.12.0b6,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.12.0b6,TODO: any way to avoid creating a copy if the array was already dense?
v0.12.0b6,"the call is necessary if the input was something like a list, though"
v0.12.0b6,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.12.0b6,so convert to pydata sparse first
v0.12.0b6,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.12.0b6,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.12.0b6,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.12.0b6,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b6,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b6,For when checking input values is disabled
v0.12.0b6,Type to column extraction function
v0.12.0b6,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.12.0b6,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.12.0b6,Get feature names using featurizer
v0.12.0b6,All attempts at retrieving transformed feature names have failed
v0.12.0b6,Delegate handling to downstream logic
v0.12.0b6,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.12.0b6,same number of input definitions as arrays
v0.12.0b6,input definitions have same number of dimensions as each array
v0.12.0b6,all result indices are unique
v0.12.0b6,all result indices must match at least one input index
v0.12.0b6,"map indices to all array, axis pairs for that index"
v0.12.0b6,each index has the same cardinality wherever it appears
v0.12.0b6,"State: list of (set of letters, list of (corresponding indices, value))"
v0.12.0b6,Algo: while list contains more than one entry
v0.12.0b6,take two entries
v0.12.0b6,sort both lists by intersection of their indices
v0.12.0b6,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.12.0b6,"take the union of indices and the product of values), stepping through each list linearly"
v0.12.0b6,TODO: might be faster to break into connected components first
v0.12.0b6,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.12.0b6,"so compute their content separately, then take cartesian product"
v0.12.0b6,this would save a few pointless sorts by empty tuples
v0.12.0b6,TODO: Consider investigating other performance ideas for these cases
v0.12.0b6,where the dense method beat the sparse method (usually sparse is faster)
v0.12.0b6,"e,facd,c->cfed"
v0.12.0b6,sparse: 0.0335489
v0.12.0b6,dense:  0.011465999999999997
v0.12.0b6,"gbd,da,egb->da"
v0.12.0b6,sparse: 0.0791625
v0.12.0b6,dense:  0.007319099999999995
v0.12.0b6,"dcc,d,faedb,c->abe"
v0.12.0b6,sparse: 1.2868097
v0.12.0b6,dense:  0.44605229999999985
v0.12.0b6,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.12.0b6,TODO: would using einsum's paths to optimize the order of merging help?
v0.12.0b6,assume that we should perform nested cross-validation if and only if
v0.12.0b6,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.12.0b6,logic copied from check_cv
v0.12.0b6,otherwise we will assume the user already set the cv attribute to something
v0.12.0b6,compatible with splitting with a 'groups' argument
v0.12.0b6,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.12.0b6,don't use the groups when calling split internally
v0.12.0b6,Normalize weights
v0.12.0b6,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.12.0b6,"if we're decorating a class, just update the __init__ method,"
v0.12.0b6,so that the result is still a class instead of a wrapper method
v0.12.0b6,"want to enforce that each bad_arg was either in kwargs,"
v0.12.0b6,or else it was in neither and is just taking its default value
v0.12.0b6,Any access should throw
v0.12.0b6,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b6,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b6,input feature name is already updated by cate_feature_names.
v0.12.0b6,define the index of d_x to filter for each given T
v0.12.0b6,filter X after broadcast with T for each given T
v0.12.0b6,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b6,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,
v0.12.0b6,This code contains some snippets of code from:
v0.12.0b6,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
v0.12.0b6,published under the following license and copyright:
v0.12.0b6,BSD 3-Clause License
v0.12.0b6,
v0.12.0b6,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b6,All rights reserved.
v0.12.0b6,make any access to matplotlib or plt throw an exception
v0.12.0b6,make any access to graphviz or plt throw an exception
v0.12.0b6,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.12.0b6,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.12.0b6,Initialize saturation & value; calculate chroma & value shift
v0.12.0b6,Calculate some intermediate values
v0.12.0b6,Initialize RGB with same hue & chroma as our color
v0.12.0b6,Shift the initial RGB values to match value and store
v0.12.0b6,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.12.0b6,clean way of achieving this
v0.12.0b6,make sure we don't accidentally escape anything in the substitution
v0.12.0b6,Fetch appropriate color for node
v0.12.0b6,"red for negative, green for positive"
v0.12.0b6,in multi-target use mean of targets
v0.12.0b6,Write node mean CATE
v0.12.0b6,Write node std of CATE
v0.12.0b6,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.12.0b6,Fetch appropriate color for node
v0.12.0b6,Write node mean CATE
v0.12.0b6,Write node mean CATE
v0.12.0b6,Write recommended treatment and value - cost
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"since inference objects can be stateful, we must copy it before fitting;"
v0.12.0b6,otherwise this sequence wouldn't work:
v0.12.0b6,"est1.fit(..., inference=inf)"
v0.12.0b6,"est2.fit(..., inference=inf)"
v0.12.0b6,est1.effect_interval(...)
v0.12.0b6,because inf now stores state from fitting est2
v0.12.0b6,This flag is true when names are set in a child class instead
v0.12.0b6,"If names are set in a child class, add an attribute reflecting that"
v0.12.0b6,This works only if X is passed as a kwarg
v0.12.0b6,We plan to enforce X as kwarg only in future releases
v0.12.0b6,This checks if names have been set in a child class
v0.12.0b6,"If names were set in a child class, don't do it again"
v0.12.0b6,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.12.0b6,call the wrapped fit method
v0.12.0b6,NOTE: we call inference fit *after* calling the main fit method
v0.12.0b6,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.12.0b6,but tensordot can't be applied to this problem because we don't sum over m
v0.12.0b6,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.12.0b6,of rows of T was not taken into account
v0.12.0b6,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.12.0b6,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.12.0b6,"Treatment names is None, default to BaseCateEstimator"
v0.12.0b6,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.12.0b6,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.12.0b6,Get input names
v0.12.0b6,Summary
v0.12.0b6,add statsmodels to parent's options
v0.12.0b6,add debiasedlasso to parent's options
v0.12.0b6,add blb to parent's options
v0.12.0b6,TODO Share some logic with non-discrete version
v0.12.0b6,Get input names
v0.12.0b6,Summary
v0.12.0b6,add statsmodels to parent's options
v0.12.0b6,add statsmodels to parent's options
v0.12.0b6,add blb to parent's options
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,remove None arguments
v0.12.0b6,"scores entries should be lists of scores, so make each entry a singleton list"
v0.12.0b6,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b6,generate an instance of the final model
v0.12.0b6,generate an instance of the nuisance model
v0.12.0b6,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.12.0b6,alteration even when we only want to fit_final.
v0.12.0b6,use a binary array to get stratified split in case of discrete treatment
v0.12.0b6,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.12.0b6,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.12.0b6,"however, sklearn doesn't support both stratifying and grouping (see"
v0.12.0b6,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.12.0b6,their own object that supports grouping if they want to use groups.
v0.12.0b6,for each mc iteration
v0.12.0b6,for each model under cross fit setting
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,
v0.12.0b6,This code contains snippets of code from
v0.12.0b6,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b6,published under the following license and copyright:
v0.12.0b6,BSD 3-Clause License
v0.12.0b6,
v0.12.0b6,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b6,All rights reserved.
v0.12.0b6,=============================================================================
v0.12.0b6,Policy Forest
v0.12.0b6,=============================================================================
v0.12.0b6,Remap output
v0.12.0b6,reshape is necessary to preserve the data contiguity against vs
v0.12.0b6,"[:, np.newaxis] that does not."
v0.12.0b6,Get subsample sample size
v0.12.0b6,Check parameters
v0.12.0b6,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b6,if this is the first `fit` call of the warm start mode.
v0.12.0b6,"Free allocated memory, if any"
v0.12.0b6,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b6,We draw from the random state to get the random state we
v0.12.0b6,would have got if we hadn't used a warm_start.
v0.12.0b6,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b6,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b6,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b6,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b6,for fitting the trees is internally releasing the Python GIL
v0.12.0b6,making threading more efficient than multiprocessing in
v0.12.0b6,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b6,"parallel_backend contexts set at a higher level,"
v0.12.0b6,since correctness does not rely on using threads.
v0.12.0b6,Collect newly grown trees
v0.12.0b6,Check data
v0.12.0b6,Assign chunk of trees to jobs
v0.12.0b6,avoid storing the output of every estimator by summing them here
v0.12.0b6,Parallel loop
v0.12.0b6,Check data
v0.12.0b6,Assign chunk of trees to jobs
v0.12.0b6,avoid storing the output of every estimator by summing them here
v0.12.0b6,Parallel loop
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,
v0.12.0b6,This code contains snippets of code from:
v0.12.0b6,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b6,published under the following license and copyright:
v0.12.0b6,BSD 3-Clause License
v0.12.0b6,
v0.12.0b6,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b6,All rights reserved.
v0.12.0b6,=============================================================================
v0.12.0b6,Types and constants
v0.12.0b6,=============================================================================
v0.12.0b6,=============================================================================
v0.12.0b6,Base Policy tree
v0.12.0b6,=============================================================================
v0.12.0b6,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.12.0b6,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.12.0b6,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.12.0b6,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.12.0b6,checks that X is 2D array.
v0.12.0b6,"since we only allow single dimensional y, we could flatten the prediction"
v0.12.0b6,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b6,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b6,Handles the corner case when X=None but featurizer might be not None
v0.12.0b6,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.12.0b6,"Replacing this method which is invalid for this class, so that we make the"
v0.12.0b6,dosctring empty and not appear in the docs.
v0.12.0b6,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b6,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.12.0b6,Replacing to remove docstring
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"if both X and W are None, just return a column of ones"
v0.12.0b6,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b6,We need to go back to the label representation of the one-hot so as to call
v0.12.0b6,the classifier.
v0.12.0b6,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b6,We need to go back to the label representation of the one-hot so as to call
v0.12.0b6,the classifier.
v0.12.0b6,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b6,This works both with our without the weighting trick as the treatments T are unit vector
v0.12.0b6,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.12.0b6,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.12.0b6,both Parametric and Non Parametric DML.
v0.12.0b6,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.12.0b6,attribute so that the trained featurizer will be passed through
v0.12.0b6,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b6,for internal use by the library
v0.12.0b6,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b6,We need to use the rlearner's copy to retain the information from fitting
v0.12.0b6,Handles the corner case when X=None but featurizer might be not None
v0.12.0b6,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b6,since we clone it and fit separate copies
v0.12.0b6,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b6,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b6,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b6,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b6,since we clone it and fit separate copies
v0.12.0b6,add blb to parent's options
v0.12.0b6,override only so that we can update the docstring to indicate
v0.12.0b6,support for `GenericSingleTreatmentModelFinalInference`
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,note that groups are not passed to score because they are only used for fitting
v0.12.0b6,note that groups are not passed to score because they are only used for fitting
v0.12.0b6,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b6,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b6,NOTE: important to get parent's wrapped copy so that
v0.12.0b6,"after training wrapped featurizer is also trained, etc."
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b6,Fit a doubly robust average effect
v0.12.0b6,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b6,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b6,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b6,since we clone it and fit separate copies
v0.12.0b6,"If custom param grid, check that only estimator parameters are being altered"
v0.12.0b6,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.12.0b6,override only so that we can update the docstring to indicate support for `blb`
v0.12.0b6,Get input names
v0.12.0b6,Summary
v0.12.0b6,Determine output settings
v0.12.0b6,"Important: This must be the first invocation of the random state at fit time, so that"
v0.12.0b6,train/test splits are re-generatable from an external object simply by knowing the
v0.12.0b6,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.12.0b6,linear predictions. Currently is also useful for testing.
v0.12.0b6,reshape is necessary to preserve the data contiguity against vs
v0.12.0b6,"[:, np.newaxis] that does not."
v0.12.0b6,Check parameters
v0.12.0b6,Set min_weight_leaf from min_weight_fraction_leaf
v0.12.0b6,Build tree
v0.12.0b6,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.12.0b6,hold. Used by criterion for memory space savings.
v0.12.0b6,Initialize the criterion object and the criterion_val object if honest.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,
v0.12.0b6,This code is a fork from:
v0.12.0b6,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.12.0b6,published under the following license and copyright:
v0.12.0b6,BSD 3-Clause License
v0.12.0b6,
v0.12.0b6,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b6,All rights reserved.
v0.12.0b6,Set parameters
v0.12.0b6,Don't instantiate estimators now! Parameters of base_estimator might
v0.12.0b6,"still change. Eg., when grid-searching with the nested object syntax."
v0.12.0b6,self.estimators_ needs to be filled by the derived classes in fit.
v0.12.0b6,Compute the number of jobs
v0.12.0b6,Partition estimators between jobs
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Remove children with nonwhite mothers from the treatment group
v0.12.0b6,Remove children with nonwhite mothers from the treatment group
v0.12.0b6,Select columns
v0.12.0b6,Scale the numeric variables
v0.12.0b6,"Change the binary variable 'first' takes values in {1,2}"
v0.12.0b6,Append a column of ones as intercept
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b6,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b6,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.12.0b6,d_t=None here since we measure the effect across all Ts
v0.12.0b6,once the estimator has been fit
v0.12.0b6,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.12.0b6,an intercept even when bias is part of coef.
v0.12.0b6,We can write effect inference as a function of prediction and prediction standard error of
v0.12.0b6,the final method for linear models
v0.12.0b6,squeeze the first axis
v0.12.0b6,d_t=None here since we measure the effect across all Ts
v0.12.0b6,set the mean_pred_stderr
v0.12.0b6,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b6,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b6,"send treatment to the end, pull bounds to the front"
v0.12.0b6,d_t=None here since we measure the effect across all Ts
v0.12.0b6,set the mean_pred_stderr
v0.12.0b6,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.12.0b6,d_t=None here since we measure the effect across all Ts
v0.12.0b6,d_t=None here since we measure the effect across all Ts
v0.12.0b6,need to set the fit args before the estimator is fit
v0.12.0b6,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b6,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.12.0b6,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.12.0b6,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.12.0b6,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.12.0b6,scale preds
v0.12.0b6,scale std errs
v0.12.0b6,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.12.0b6,offset preds
v0.12.0b6,"offset the distribution, too"
v0.12.0b6,scale preds
v0.12.0b6,"scale the distribution, too"
v0.12.0b6,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b6,1. Uncertainty of Mean Point Estimate
v0.12.0b6,2. Distribution of Point Estimate
v0.12.0b6,3. Total Variance of Point Estimate
v0.12.0b6,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.12.0b6,so bail out early; note that it might be possible to correct the algorithm for
v0.12.0b6,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.12.0b6,be clean
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,TODO: Add a __dir__ implementation?
v0.12.0b6,don't proxy special methods
v0.12.0b6,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.12.0b6,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.12.0b6,then we should be computing a confidence interval for the wrapped calls
v0.12.0b6,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.12.0b6,second level bootstrap which would be prohibitive computationally?
v0.12.0b6,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.12.0b6,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.12.0b6,can't import from econml.inference at top level without creating cyclical dependencies
v0.12.0b6,Note that inference results are always methods even if the inference is for a property
v0.12.0b6,(e.g. coef__inference() is a method but coef_ is a property)
v0.12.0b6,Therefore we must insert a lambda if getting inference for a non-callable
v0.12.0b6,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.12.0b6,"try to get interval/std first if appropriate,"
v0.12.0b6,since we don't prefer a wrapped method with this name
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,
v0.12.0b6,This code contains snippets of code from:
v0.12.0b6,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b6,published under the following license and copyright:
v0.12.0b6,BSD 3-Clause License
v0.12.0b6,
v0.12.0b6,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b6,All rights reserved.
v0.12.0b6,=============================================================================
v0.12.0b6,Types and constants
v0.12.0b6,=============================================================================
v0.12.0b6,=============================================================================
v0.12.0b6,Base GRF tree
v0.12.0b6,=============================================================================
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,=============================================================================
v0.12.0b6,A MultOutputWrapper for GRF classes
v0.12.0b6,=============================================================================
v0.12.0b6,=============================================================================
v0.12.0b6,Instantiations of Generalized Random Forest
v0.12.0b6,=============================================================================
v0.12.0b6,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.12.0b6,in front of the constant treatment is the intercept in the moment equation.
v0.12.0b6,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.12.0b6,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,
v0.12.0b6,This code contains snippets of code from
v0.12.0b6,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b6,published under the following license and copyright:
v0.12.0b6,BSD 3-Clause License
v0.12.0b6,
v0.12.0b6,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b6,All rights reserved.
v0.12.0b6,=============================================================================
v0.12.0b6,Base Generalized Random Forest
v0.12.0b6,=============================================================================
v0.12.0b6,TODO: support freq_weight and sample_var
v0.12.0b6,Remap output
v0.12.0b6,reshape is necessary to preserve the data contiguity against vs
v0.12.0b6,"[:, np.newaxis] that does not."
v0.12.0b6,reshape is necessary to preserve the data contiguity against vs
v0.12.0b6,"[:, np.newaxis] that does not."
v0.12.0b6,Get subsample sample size
v0.12.0b6,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.12.0b6,We calculate the min eigenvalue proxy that each criterion is considering
v0.12.0b6,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.12.0b6,Check parameters
v0.12.0b6,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b6,if this is the first `fit` call of the warm start mode.
v0.12.0b6,"Free allocated memory, if any"
v0.12.0b6,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b6,We draw from the random state to get the random state we
v0.12.0b6,would have got if we hadn't used a warm_start.
v0.12.0b6,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b6,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b6,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b6,Generating indices a priori before parallelism ended up being orders of magnitude
v0.12.0b6,faster than how sklearn does it. The reason is that random samplers do not release the
v0.12.0b6,gil it seems.
v0.12.0b6,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b6,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b6,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b6,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b6,for fitting the trees is internally releasing the Python GIL
v0.12.0b6,making threading more efficient than multiprocessing in
v0.12.0b6,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b6,"parallel_backend contexts set at a higher level,"
v0.12.0b6,since correctness does not rely on using threads.
v0.12.0b6,Collect newly grown trees
v0.12.0b6,Check data
v0.12.0b6,Assign chunk of trees to jobs
v0.12.0b6,avoid storing the output of every estimator by summing them here
v0.12.0b6,Parallel loop
v0.12.0b6,Check data
v0.12.0b6,Assign chunk of trees to jobs
v0.12.0b6,Parallel loop
v0.12.0b6,Check data
v0.12.0b6,Assign chunk of trees to jobs
v0.12.0b6,Parallel loop
v0.12.0b6,####################
v0.12.0b6,Variance correction
v0.12.0b6,####################
v0.12.0b6,Subtract the average within bag variance. This ends up being equal to the
v0.12.0b6,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.12.0b6,The negative part is just sq_between.
v0.12.0b6,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.12.0b6,"The off diagonals we have no objective prior, so no correction is applied."
v0.12.0b6,Finally correcting the pred_cov or pred_var
v0.12.0b6,avoid storing the output of every estimator by summing them here
v0.12.0b6,Parallel loop
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,testing importances
v0.12.0b6,testing heterogeneity importances
v0.12.0b6,Testing that all parameters do what they are supposed to
v0.12.0b6,"testing predict, apply and decision path"
v0.12.0b6,test that the subsampling scheme past to the trees is correct
v0.12.0b6,The sample size is chosen in particular to test rounding based error when subsampling
v0.12.0b6,test that the estimator calcualtes var correctly
v0.12.0b6,test api
v0.12.0b6,test accuracy
v0.12.0b6,test the projection functionality of forests
v0.12.0b6,test that the estimator calcualtes var correctly
v0.12.0b6,test api
v0.12.0b6,test that the estimator calcualtes var correctly
v0.12.0b6,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b6,test that we raise errors in mishandled situations.
v0.12.0b6,test that the subsampling scheme past to the trees is correct
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,omit the lalonde notebook
v0.12.0b6,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.12.0b6,creating notebooks that are annoying for our users to actually run themselves
v0.12.0b6,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b6,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.12.0b6,"prior to calling interpret, can't plot, render, etc."
v0.12.0b6,can interpret without uncertainty
v0.12.0b6,can't interpret with uncertainty if inference wasn't used during fit
v0.12.0b6,can interpret with uncertainty if we refit
v0.12.0b6,can interpret without uncertainty
v0.12.0b6,can't treat before interpreting
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,simple DGP only for illustration
v0.12.0b6,Define the treatment model neural network architecture
v0.12.0b6,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b6,"so the input shape is (d_z + d_x,)"
v0.12.0b6,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b6,add extra layers on top for the mixture density network
v0.12.0b6,Define the response model neural network architecture
v0.12.0b6,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b6,"so the input shape is (d_t + d_x,)"
v0.12.0b6,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b6,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b6,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b6,so that the same weights will be reused in each instantiation
v0.12.0b6,number of samples to use in second estimate of the response
v0.12.0b6,(to make loss estimate unbiased)
v0.12.0b6,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b6,do something with predictions...
v0.12.0b6,also test vector t and y
v0.12.0b6,simple DGP only for illustration
v0.12.0b6,Define the treatment model neural network architecture
v0.12.0b6,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b6,"so the input shape is (d_z + d_x,)"
v0.12.0b6,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b6,add extra layers on top for the mixture density network
v0.12.0b6,Define the response model neural network architecture
v0.12.0b6,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b6,"so the input shape is (d_t + d_x,)"
v0.12.0b6,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b6,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b6,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b6,so that the same weights will be reused in each instantiation
v0.12.0b6,number of samples to use in second estimate of the response
v0.12.0b6,(to make loss estimate unbiased)
v0.12.0b6,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b6,do something with predictions...
v0.12.0b6,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.12.0b6,test = True ensures we draw test set images
v0.12.0b6,test = True ensures we draw test set images
v0.12.0b6,re-draw to get new independent treatment and implied response
v0.12.0b6,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b6,above is necesary so that reduced form doesn't win
v0.12.0b6,covariates: time and emotion
v0.12.0b6,random instrument
v0.12.0b6,z -> price
v0.12.0b6,true observable demand function
v0.12.0b6,errors
v0.12.0b6,response
v0.12.0b6,test = True ensures we draw test set images
v0.12.0b6,test = True ensures we draw test set images
v0.12.0b6,re-draw to get new independent treatment and implied response
v0.12.0b6,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b6,above is necesary so that reduced form doesn't win
v0.12.0b6,covariates: time and emotion
v0.12.0b6,random instrument
v0.12.0b6,z -> price
v0.12.0b6,true observable demand function
v0.12.0b6,errors
v0.12.0b6,response
v0.12.0b6,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.12.0b6,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.12.0b6,For some reason this doesn't work at all when run against the CNTK backend...
v0.12.0b6,"model.compile('nadam', loss=lambda _,l:l)"
v0.12.0b6,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.12.0b6,generate a valiation set
v0.12.0b6,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.12.0b6,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,DGP constants
v0.12.0b6,Generate data
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,testing importances
v0.12.0b6,testing heterogeneity importances
v0.12.0b6,Testing that all parameters do what they are supposed to
v0.12.0b6,"testing predict, apply and decision path"
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.12.0b6,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.12.0b6,so we need to transpose the result
v0.12.0b6,1-d output
v0.12.0b6,2-d output
v0.12.0b6,Single dimensional output y
v0.12.0b6,compare with weight
v0.12.0b6,compare with weight
v0.12.0b6,compare with weight
v0.12.0b6,compare with weight
v0.12.0b6,Multi-dimensional output y
v0.12.0b6,1-d y
v0.12.0b6,compare when both sample_var and sample_weight exist
v0.12.0b6,multi-d y
v0.12.0b6,compare when both sample_var and sample_weight exist
v0.12.0b6,compare when both sample_var and sample_weight exist
v0.12.0b6,compare when both sample_var and sample_weight exist
v0.12.0b6,compare when both sample_var and sample_weight exist
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,test that we can fit with the same arguments as the base estimator
v0.12.0b6,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b6,with the same shape for the lower and upper bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,test that we can do the same thing once we provide percentile bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b6,with the same shape for the lower and upper bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,test that we can do the same thing once we provide percentile bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,test that we can fit with the same arguments as the base estimator
v0.12.0b6,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b6,with the same shape for the lower and upper bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,test that we can do the same thing once we provide percentile bounds
v0.12.0b6,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b6,with the same shape for the lower and upper bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,test that we can do the same thing once we provide percentile bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,test that we can fit with the same arguments as the base estimator
v0.12.0b6,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b6,with the same shape for the lower and upper bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,test that we can do the same thing once we provide percentile bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b6,with the same shape for the lower and upper bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,test that we can do the same thing once we provide percentile bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b6,with the same shape for the lower and upper bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,test that the estimated effect is usually within the bounds
v0.12.0b6,test that we can do the same thing once we provide alpha explicitly
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,test that the estimated effect is usually within the bounds
v0.12.0b6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b6,with the same shape for the lower and upper bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,TODO: test that the estimated effect is usually within the bounds
v0.12.0b6,and that the true effect is also usually within the bounds
v0.12.0b6,test that we can do the same thing once we provide percentile bounds
v0.12.0b6,test that the lower and upper bounds differ
v0.12.0b6,TODO: test that the estimated effect is usually within the bounds
v0.12.0b6,and that the true effect is also usually within the bounds
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,test that the subsampling scheme past to the trees is correct
v0.12.0b6,test that the estimator calcualtes var correctly
v0.12.0b6,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b6,test that we raise errors in mishandled situations.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,DGP constants
v0.12.0b6,Generate data
v0.12.0b6,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b6,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b6,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.12.0b6,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b6,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.12.0b6,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b6,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b6,pvalue for second column should be greater than zero since some points are on either side
v0.12.0b6,of the tested value
v0.12.0b6,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.12.0b6,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b6,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b6,only is not None when T1 is a constant or a list of constant
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.12.0b6,Test non keyword based calls to fit
v0.12.0b6,test non-array inputs
v0.12.0b6,Test custom splitter
v0.12.0b6,Test incomplete set of test folds
v0.12.0b6,"y scores should be positive, since W predicts Y somewhat"
v0.12.0b6,"t scores might not be, since W and T are uncorrelated"
v0.12.0b6,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,make sure cross product varies more slowly with first array
v0.12.0b6,and that vectors are okay as inputs
v0.12.0b6,number of inputs in specification must match number of inputs
v0.12.0b6,must have an output
v0.12.0b6,output indices must be unique
v0.12.0b6,output indices must be present in an input
v0.12.0b6,number of indices must match number of dimensions for each input
v0.12.0b6,repeated indices must always have consistent sizes
v0.12.0b6,transpose
v0.12.0b6,tensordot
v0.12.0b6,trace
v0.12.0b6,TODO: set up proper flag for this
v0.12.0b6,pick indices at random with replacement from the first 7 letters of the alphabet
v0.12.0b6,"of all of the distinct indices that appear in any input,"
v0.12.0b6,pick a random subset of them (of size at most 5) to appear in the output
v0.12.0b6,creating an instance should warn
v0.12.0b6,using the instance should not warn
v0.12.0b6,using the deprecated method should warn
v0.12.0b6,don't warn if b and c are passed by keyword
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Preprocess data
v0.12.0b6,Convert 'week' to a date
v0.12.0b6,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.12.0b6,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.12.0b6,Take log of price
v0.12.0b6,Make brand numeric
v0.12.0b6,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.12.0b6,which have all zero coeffs
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,test at least one estimator from each category
v0.12.0b6,test causal graph
v0.12.0b6,test refutation estimate
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.12.0b6,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.12.0b6,TODO: test something rather than just print...
v0.12.0b6,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.12.0b6,pick some arbitrary X
v0.12.0b6,pick some arbitrary T
v0.12.0b6,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b6,The average variance should be lower when using monte carlo iterations
v0.12.0b6,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b6,The average variance should be lower when using monte carlo iterations
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,ensure that we've got at least two of every row
v0.12.0b6,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b6,need to make sure we get all *joint* combinations
v0.12.0b6,IntentToTreat only supports binary treatments/instruments
v0.12.0b6,IntentToTreat only supports binary treatments/instruments
v0.12.0b6,IntentToTreat requires X
v0.12.0b6,ensure we can serialize unfit estimator
v0.12.0b6,these support only W but not X
v0.12.0b6,"these support only binary, not general discrete T and Z"
v0.12.0b6,ensure we can serialize fit estimator
v0.12.0b6,make sure we can call the marginal_effect and effect methods
v0.12.0b6,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b6,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b6,"make sure we can call effect with implied scalar treatments,"
v0.12.0b6,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b6,are multiple treatments
v0.12.0b6,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b6,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.12.0b6,"however, with custom splits the checking happens in the first stage wrapper"
v0.12.0b6,where we don't have all of the required information to do this;
v0.12.0b6,we'd probably need to add it to _crossfit instead
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.12.0b6,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.12.0b6,The __warningregistry__'s need to be in a pristine state for tests
v0.12.0b6,to work properly.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Set random seed
v0.12.0b6,Generate data
v0.12.0b6,DGP constants
v0.12.0b6,Test data
v0.12.0b6,Constant treatment effect
v0.12.0b6,Constant treatment with multi output Y
v0.12.0b6,Heterogeneous treatment
v0.12.0b6,Heterogeneous treatment with multi output Y
v0.12.0b6,TLearner test
v0.12.0b6,Instantiate TLearner
v0.12.0b6,Test inputs
v0.12.0b6,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b6,Instantiate SLearner
v0.12.0b6,Test inputs
v0.12.0b6,Test constant treatment effect
v0.12.0b6,Test constant treatment effect with multi output Y
v0.12.0b6,Test heterogeneous treatment effect
v0.12.0b6,Need interactions between T and features
v0.12.0b6,Test heterogeneous treatment effect with multi output Y
v0.12.0b6,Instantiate XLearner
v0.12.0b6,Test inputs
v0.12.0b6,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b6,Instantiate DomainAdaptationLearner
v0.12.0b6,Test inputs
v0.12.0b6,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b6,Get the true treatment effect
v0.12.0b6,Get the true treatment effect
v0.12.0b6,Fit learner and get the effect and marginal effect
v0.12.0b6,Compute treatment effect residuals (absolute)
v0.12.0b6,Check that at least 90% of predictions are within tolerance interval
v0.12.0b6,Check whether the output shape is right
v0.12.0b6,Check that one can pass in regular lists
v0.12.0b6,Check that it fails correctly if lists of different shape are passed in
v0.12.0b6,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b6,Generate covariates
v0.12.0b6,Generate treatment
v0.12.0b6,Calculate outcome
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,DGP constants
v0.12.0b6,Generate data
v0.12.0b6,Test data
v0.12.0b6,Remove warnings that might be raised by the models passed into the ORF
v0.12.0b6,Generate data with continuous treatments
v0.12.0b6,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.12.0b6,does not work well with parallelism.
v0.12.0b6,Test inputs for continuous treatments
v0.12.0b6,--> Check that one can pass in regular lists
v0.12.0b6,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b6,Check that outputs have the correct shape
v0.12.0b6,Test continuous treatments with controls
v0.12.0b6,Test continuous treatments without controls
v0.12.0b6,Generate data with binary treatments
v0.12.0b6,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.12.0b6,does not work well with parallelism.
v0.12.0b6,Test inputs for binary treatments
v0.12.0b6,--> Check that one can pass in regular lists
v0.12.0b6,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b6,"--> Check that it works when T, Y have shape (n, 1)"
v0.12.0b6,"--> Check that it fails correctly when T has shape (n, 2)"
v0.12.0b6,--> Check that it fails correctly when the treatments are not numeric
v0.12.0b6,Check that outputs have the correct shape
v0.12.0b6,Test binary treatments with controls
v0.12.0b6,Test binary treatments without controls
v0.12.0b6,Only applicable to continuous treatments
v0.12.0b6,Generate data for 2 treatments
v0.12.0b6,Test multiple treatments with controls
v0.12.0b6,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.12.0b6,The rest for controls. Just as an example.
v0.12.0b6,Generating A/B test data
v0.12.0b6,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.12.0b6,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.12.0b6,Create a wrapper around Lasso that doesn't support weights
v0.12.0b6,since Lasso does natively support them starting in sklearn 0.23
v0.12.0b6,Generate data with continuous treatments
v0.12.0b6,Instantiate model with most of the default parameters
v0.12.0b6,Compute the treatment effect on test points
v0.12.0b6,Compute treatment effect residuals
v0.12.0b6,Multiple treatments
v0.12.0b6,Allow at most 10% test points to be outside of the tolerance interval
v0.12.0b6,Compute treatment effect residuals
v0.12.0b6,Multiple treatments
v0.12.0b6,Allow at most 20% test points to be outside of the confidence interval
v0.12.0b6,Check that the intervals are not too wide
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.12.0b6,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.12.0b6,ensure that we've got at least 6 of every element
v0.12.0b6,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.12.0b6,NOTE: this number may need to change if the default number of folds in
v0.12.0b6,WeightedStratifiedKFold changes
v0.12.0b6,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b6,ensure we can serialize the unfit estimator
v0.12.0b6,ensure we can pickle the fit estimator
v0.12.0b6,make sure we can call the marginal_effect and effect methods
v0.12.0b6,test const marginal inference
v0.12.0b6,test effect inference
v0.12.0b6,test marginal effect inference
v0.12.0b6,test coef__inference and intercept__inference
v0.12.0b6,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b6,"make sure we can call effect with implied scalar treatments,"
v0.12.0b6,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b6,are multiple treatments
v0.12.0b6,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b6,ensure that we've got at least two of every element
v0.12.0b6,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b6,make sure we can call the marginal_effect and effect methods
v0.12.0b6,test const marginal inference
v0.12.0b6,test effect inference
v0.12.0b6,test marginal effect inference
v0.12.0b6,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b6,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b6,We concatenate the two copies data
v0.12.0b6,make sure we can get out post-fit stuff
v0.12.0b6,create a simple artificial setup where effect of moving from treatment
v0.12.0b6,"1 -> 2 is 2,"
v0.12.0b6,"1 -> 3 is 1, and"
v0.12.0b6,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b6,"Using an uneven number of examples from different classes,"
v0.12.0b6,"and having the treatments in non-lexicographic order,"
v0.12.0b6,Should rule out some basic issues.
v0.12.0b6,test that we can fit with a KFold instance
v0.12.0b6,test that we can fit with a train/test iterable
v0.12.0b6,predetermined splits ensure that all features are seen in each split
v0.12.0b6,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.12.0b6,(incorrectly) use a final model with an intercept
v0.12.0b6,"Because final model is fixed, actual values of T and Y don't matter"
v0.12.0b6,Ensure reproducibility
v0.12.0b6,Sparse DGP
v0.12.0b6,Treatment effect coef
v0.12.0b6,Other coefs
v0.12.0b6,Features and controls
v0.12.0b6,Test sparse estimator
v0.12.0b6,"--> test coef_, intercept_"
v0.12.0b6,--> test treatment effects
v0.12.0b6,Restrict x_test to vectors of norm < 1
v0.12.0b6,--> check inference
v0.12.0b6,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b6,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.12.0b6,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.12.0b6,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.12.0b6,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.12.0b6,sparse test case: heterogeneous effect by product
v0.12.0b6,need at least as many rows in e_y as there are distinct columns
v0.12.0b6,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.12.0b6,create a simple artificial setup where effect of moving from treatment
v0.12.0b6,"a -> b is 2,"
v0.12.0b6,"a -> c is 1, and"
v0.12.0b6,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.12.0b6,"Using an uneven number of examples from different classes,"
v0.12.0b6,"and having the treatments in non-lexicographic order,"
v0.12.0b6,should rule out some basic issues.
v0.12.0b6,Note that explicitly specifying the dtype as object is necessary until
v0.12.0b6,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.12.0b6,estimated effects should be identical when treatment is explicitly given
v0.12.0b6,but const_marginal_effect should be reordered based on the explicit cagetories
v0.12.0b6,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.12.0b6,test outer grouping
v0.12.0b6,test nested grouping
v0.12.0b6,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b6,whichever groups we saw
v0.12.0b6,test nested grouping
v0.12.0b6,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b6,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b6,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Set random seed
v0.12.0b6,Generate data
v0.12.0b6,DGP constants
v0.12.0b6,Test data
v0.12.0b6,Constant treatment effect and propensity
v0.12.0b6,Heterogeneous treatment and propensity
v0.12.0b6,ensure that we've got at least two of every element
v0.12.0b6,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b6,ensure that we can serialize unfit estimator
v0.12.0b6,ensure that we can serialize fit estimator
v0.12.0b6,make sure we can call the marginal_effect and effect methods
v0.12.0b6,test const marginal inference
v0.12.0b6,test effect inference
v0.12.0b6,test marginal effect inference
v0.12.0b6,test coef_ and intercept_ inference
v0.12.0b6,verify we can generate the summary
v0.12.0b6,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b6,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b6,create a simple artificial setup where effect of moving from treatment
v0.12.0b6,"1 -> 2 is 2,"
v0.12.0b6,"1 -> 3 is 1, and"
v0.12.0b6,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b6,"Using an uneven number of examples from different classes,"
v0.12.0b6,"and having the treatments in non-lexicographic order,"
v0.12.0b6,Should rule out some basic issues.
v0.12.0b6,test that we can fit with a KFold instance
v0.12.0b6,test that we can fit with a train/test iterable
v0.12.0b6,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b6,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b6,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b6,test coef__inference function works
v0.12.0b6,test intercept__inference function works
v0.12.0b6,test summary function works
v0.12.0b6,Test inputs
v0.12.0b6,self._test_inputs(DR_learner)
v0.12.0b6,Test constant treatment effect
v0.12.0b6,Test heterogeneous treatment effect
v0.12.0b6,Test heterogenous treatment effect for W =/= None
v0.12.0b6,Sparse DGP
v0.12.0b6,Treatment effect coef
v0.12.0b6,Other coefs
v0.12.0b6,Features and controls
v0.12.0b6,Test sparse estimator
v0.12.0b6,"--> test coef_, intercept_"
v0.12.0b6,--> test treatment effects
v0.12.0b6,Restrict x_test to vectors of norm < 1
v0.12.0b6,--> check inference
v0.12.0b6,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b6,test outer grouping
v0.12.0b6,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.12.0b6,test nested grouping
v0.12.0b6,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b6,whichever groups we saw
v0.12.0b6,test nested grouping
v0.12.0b6,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b6,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b6,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b6,helper class
v0.12.0b6,Fit learner and get the effect
v0.12.0b6,Get the true treatment effect
v0.12.0b6,Compute treatment effect residuals (absolute)
v0.12.0b6,Check that at least 90% of predictions are within tolerance interval
v0.12.0b6,Only for heterogeneous TE
v0.12.0b6,Fit learner on X and W and get the effect
v0.12.0b6,Get the true treatment effect
v0.12.0b6,Compute treatment effect residuals (absolute)
v0.12.0b6,Check that at least 90% of predictions are within tolerance interval
v0.12.0b6,Check that one can pass in regular lists
v0.12.0b6,Check that it fails correctly if lists of different shape are passed in
v0.12.0b6,Check that it fails when T contains values other than 0 and 1
v0.12.0b6,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b6,Generate covariates
v0.12.0b6,Generate treatment
v0.12.0b6,Calculate outcome
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,DGP constants
v0.12.0b6,DGP coefficients
v0.12.0b6,Generated outcomes
v0.12.0b6,################
v0.12.0b6,WeightedLasso #
v0.12.0b6,################
v0.12.0b6,Define weights
v0.12.0b6,Define extended datasets
v0.12.0b6,Range of alphas
v0.12.0b6,Compare with Lasso
v0.12.0b6,--> No intercept
v0.12.0b6,--> With intercept
v0.12.0b6,When DGP has no intercept
v0.12.0b6,When DGP has intercept
v0.12.0b6,--> Coerce coefficients to be positive
v0.12.0b6,--> Toggle max_iter & tol
v0.12.0b6,Define weights
v0.12.0b6,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b6,Mixed DGP scenario.
v0.12.0b6,Define extended datasets
v0.12.0b6,Define weights
v0.12.0b6,Define multioutput
v0.12.0b6,##################
v0.12.0b6,WeightedLassoCV #
v0.12.0b6,##################
v0.12.0b6,Define alphas to test
v0.12.0b6,Compare with LassoCV
v0.12.0b6,--> No intercept
v0.12.0b6,--> With intercept
v0.12.0b6,--> Force parameters to be positive
v0.12.0b6,Choose a smaller n to speed-up process
v0.12.0b6,Compare fold weights
v0.12.0b6,Define weights
v0.12.0b6,Define extended datasets
v0.12.0b6,Define splitters
v0.12.0b6,WeightedKFold splitter
v0.12.0b6,Map weighted splitter to an extended splitter
v0.12.0b6,Define alphas to test
v0.12.0b6,Compare with LassoCV
v0.12.0b6,--> No intercept
v0.12.0b6,--> With intercept
v0.12.0b6,--> Force parameters to be positive
v0.12.0b6,###########################
v0.12.0b6,MultiTaskWeightedLassoCV #
v0.12.0b6,###########################
v0.12.0b6,Define alphas to test
v0.12.0b6,Define splitter
v0.12.0b6,Compare with MultiTaskLassoCV
v0.12.0b6,--> No intercept
v0.12.0b6,--> With intercept
v0.12.0b6,Define weights
v0.12.0b6,Define extended datasets
v0.12.0b6,Define splitters
v0.12.0b6,WeightedKFold splitter
v0.12.0b6,Map weighted splitter to an extended splitter
v0.12.0b6,Define alphas to test
v0.12.0b6,Compare with LassoCV
v0.12.0b6,--> No intercept
v0.12.0b6,--> With intercept
v0.12.0b6,#########################
v0.12.0b6,WeightedLassoCVWrapper #
v0.12.0b6,#########################
v0.12.0b6,perform 1D fit
v0.12.0b6,perform 2D fit
v0.12.0b6,################
v0.12.0b6,DebiasedLasso #
v0.12.0b6,################
v0.12.0b6,Test DebiasedLasso without weights
v0.12.0b6,--> Check debiased coeffcients without intercept
v0.12.0b6,--> Check debiased coeffcients with intercept
v0.12.0b6,--> Check 5-95 CI coverage for unit vectors
v0.12.0b6,Test DebiasedLasso with weights for one DGP
v0.12.0b6,Define weights
v0.12.0b6,Define extended datasets
v0.12.0b6,--> Check debiased coefficients
v0.12.0b6,Define weights
v0.12.0b6,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b6,--> Check debiased coeffcients
v0.12.0b6,Test that attributes propagate correctly
v0.12.0b6,Test MultiOutputDebiasedLasso without weights
v0.12.0b6,--> Check debiased coeffcients without intercept
v0.12.0b6,--> Check debiased coeffcients with intercept
v0.12.0b6,--> Check CI coverage
v0.12.0b6,Test MultiOutputDebiasedLasso with weights
v0.12.0b6,Define weights
v0.12.0b6,Define extended datasets
v0.12.0b6,--> Check debiased coefficients
v0.12.0b6,Unit vectors
v0.12.0b6,Unit vectors
v0.12.0b6,Check coeffcients and intercept are the same within tolerance
v0.12.0b6,Check results are similar with tolerance 1e-6
v0.12.0b6,Check if multitask
v0.12.0b6,Check that same alpha is chosen
v0.12.0b6,Check that the coefficients are similar
v0.12.0b6,selective ridge has a simple implementation that we can test against
v0.12.0b6,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.12.0b6,"it should be the case that when we set fit_intercept to true,"
v0.12.0b6,it doesn't matter whether the penalized model also fits an intercept or not
v0.12.0b6,create an extra copy of rows with weight 2
v0.12.0b6,"instead of a slice, explicitly return an array of indices"
v0.12.0b6,_penalized_inds is only set during fitting
v0.12.0b6,cv exists on penalized model
v0.12.0b6,now we can access _penalized_inds
v0.12.0b6,check that we can read the cv attribute back out from the underlying model
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b6,local index should have as many times entries as global as there were rows passed in
v0.12.0b6,continuous treatments have typical treatment values equal to
v0.12.0b6,the mean of the absolute value of non-zero entries
v0.12.0b6,discrete treatments have typical treatment value 1
v0.12.0b6,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b6,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b6,policy value should exceed always treating with any treatment
v0.12.0b6,"global shape is (d_y, sum(d_t))"
v0.12.0b6,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b6,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b6,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b6,features; for categoricals they should appear #cats-1 times each
v0.12.0b6,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b6,local index should have as many times entries as global as there were rows passed in
v0.12.0b6,features; for categoricals they should appear #cats-1 times each
v0.12.0b6,"global shape is (d_y, sum(d_t))"
v0.12.0b6,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b6,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b6,continuous treatments have typical treatment values equal to
v0.12.0b6,the mean of the absolute value of non-zero entries
v0.12.0b6,discrete treatments have typical treatment value 1
v0.12.0b6,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b6,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b6,policy value should exceed always treating with any treatment
v0.12.0b6,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b6,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b6,local index should have as many times entries as global as there were rows passed in
v0.12.0b6,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b6,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b6,policy value should exceed always treating with any treatment
v0.12.0b6,"global shape is (d_y, sum(d_t))"
v0.12.0b6,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b6,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b6,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b6,features; for categoricals they should appear #cats-1 times each
v0.12.0b6,make sure we don't run into problems dropping every index
v0.12.0b6,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b6,local index should have as many times entries as global as there were rows passed in
v0.12.0b6,"global shape is (d_y, sum(d_t))"
v0.12.0b6,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b6,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b6,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b6,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b6,policy value should exceed always treating with any treatment
v0.12.0b6,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b6,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b6,local index should have as many times entries as global as there were rows passed in
v0.12.0b6,features; for categoricals they should appear #cats-1 times each
v0.12.0b6,"global shape is (d_y, sum(d_t))"
v0.12.0b6,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b6,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b6,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b6,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b6,policy value should exceed always treating with any treatment
v0.12.0b6,dgp
v0.12.0b6,model
v0.12.0b6,model
v0.12.0b6,"columns 'd', 'e', 'h' have too many values"
v0.12.0b6,"columns 'd', 'e' have too many values"
v0.12.0b6,lowering bound shouldn't affect already fit columns when warm starting
v0.12.0b6,"column d is now okay, too"
v0.12.0b6,verify that we can use a scalar treatment cost
v0.12.0b6,verify that we can specify per-treatment costs for each sample
v0.12.0b6,verify that using the same state returns the same results each time
v0.12.0b6,set the categories for column 'd' explicitly so that b is default
v0.12.0b6,"first column: 10 ones, this is fine"
v0.12.0b6,"second column: 6 categories, plenty of random instances of each"
v0.12.0b6,this is fine only if we increase the cateogry limit
v0.12.0b6,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.12.0b6,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.12.0b6,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.12.0b6,forest heterogeneity won't work
v0.12.0b6,"sixth column: just 1 one, not enough even without check"
v0.12.0b6,increase bound on cat expansion
v0.12.0b6,skip checks (reducing folds accordingly)
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,DGP constants
v0.12.0b6,Define data features
v0.12.0b6,Added `_df`to names to be different from the default cate_estimator names
v0.12.0b6,Generate data
v0.12.0b6,################################
v0.12.0b6,Single treatment and outcome #
v0.12.0b6,################################
v0.12.0b6,Test LinearDML
v0.12.0b6,|--> Test featurizers
v0.12.0b6,ColumnTransformer doesn't propagate column names
v0.12.0b6,|--> Test re-fit
v0.12.0b6,Test SparseLinearDML
v0.12.0b6,Test ForestDML
v0.12.0b6,###################################
v0.12.0b6,Mutiple treatments and outcomes #
v0.12.0b6,###################################
v0.12.0b6,Test LinearDML
v0.12.0b6,Test SparseLinearDML
v0.12.0b6,"Single outcome only, ORF does not support multiple outcomes"
v0.12.0b6,Test DMLOrthoForest
v0.12.0b6,Test DROrthoForest
v0.12.0b6,Test XLearner
v0.12.0b6,Skipping population summary names test because bootstrap inference is too slow
v0.12.0b6,Test SLearner
v0.12.0b6,Test TLearner
v0.12.0b6,Test LinearDRLearner
v0.12.0b6,Test SparseLinearDRLearner
v0.12.0b6,Test ForestDRLearner
v0.12.0b6,Test LinearIntentToTreatDRIV
v0.12.0b6,Test DeepIV
v0.12.0b6,Test categorical treatments
v0.12.0b6,Check refit
v0.12.0b6,Check refit after setting categories
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Linear models are required for parametric dml
v0.12.0b6,sample weighting models are required for nonparametric dml
v0.12.0b6,Test values
v0.12.0b6,TLearner test
v0.12.0b6,Instantiate TLearner
v0.12.0b6,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b6,Test constant treatment effect with multi output Y
v0.12.0b6,Test heterogeneous treatment effect
v0.12.0b6,Need interactions between T and features
v0.12.0b6,Test heterogeneous treatment effect with multi output Y
v0.12.0b6,Instantiate DomainAdaptationLearner
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,test base values equals to mean of constant marginal effect
v0.12.0b6,test shape of shap values output is as expected
v0.12.0b6,test shape of attribute of explanation object is as expected
v0.12.0b6,test length of feature names equals to shap values shape
v0.12.0b6,test base values equals to mean of constant marginal effect
v0.12.0b6,test shape of shap values output is as expected
v0.12.0b6,test shape of attribute of explanation object is as expected
v0.12.0b6,test length of feature names equals to shap values shape
v0.12.0b6,Treatment effect function
v0.12.0b6,Outcome support
v0.12.0b6,Treatment support
v0.12.0b6,"Generate controls, covariates, treatments and outcomes"
v0.12.0b6,Heterogeneous treatment effects
v0.12.0b6,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.12.0b6,through shap package.
v0.12.0b6,test shap could generate the plot from the shap_values
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Check inputs
v0.12.0b6,Check inputs
v0.12.0b6,Check inputs
v0.12.0b6,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.12.0b6,"Thus, we append the controls column before the one-hot-encoded T"
v0.12.0b6,"We might want to revisit, though, since it's linearly determined by the others"
v0.12.0b6,Check inputs
v0.12.0b6,Check inputs
v0.12.0b6,Estimate response function
v0.12.0b6,Check inputs
v0.12.0b6,Train model on controls. Assign higher weight to units resembling
v0.12.0b6,treated units.
v0.12.0b6,Train model on the treated. Assign higher weight to units resembling
v0.12.0b6,control units.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.12.0b6,output is
v0.12.0b6,"* a column of ones if X, W, and Z are all None"
v0.12.0b6,* just X or W or Z if both of the others are None
v0.12.0b6,* hstack([arrs]) for whatever subset are not None otherwise
v0.12.0b6,ensure Z is 2D
v0.12.0b6,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b6,We need to go back to the label representation of the one-hot so as to call
v0.12.0b6,the classifier.
v0.12.0b6,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b6,We need to go back to the label representation of the one-hot so as to call
v0.12.0b6,the classifier.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,TODO: make sure to use random seeds wherever necessary
v0.12.0b6,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.12.0b6,"unfortunately with the Theano and Tensorflow backends,"
v0.12.0b6,the straightforward use of K.stop_gradient can cause an error
v0.12.0b6,because the parameters of the intermediate layers are now disconnected from the loss;
v0.12.0b6,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.12.0b6,so that those layers remain connected but with 0 gradient
v0.12.0b6,|| t - mu_i || ^2
v0.12.0b6,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.12.0b6,Use logsumexp for numeric stability:
v0.12.0b6,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.12.0b6,TODO: does the numeric stability actually make any difference?
v0.12.0b6,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.12.0b6,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.12.0b6,generate cumulative sum via matrix multiplication
v0.12.0b6,"Generate standard uniform values in shape (batch_size,1)"
v0.12.0b6,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.12.0b6,we use uniform_like instead with an input of an appropriate shape)
v0.12.0b6,convert to floats and multiply to perform equivalent of logical AND
v0.12.0b6,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.12.0b6,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.12.0b6,we use normal_like instead with an input of an appropriate shape)
v0.12.0b6,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.12.0b6,prevent gradient from passing through sampling
v0.12.0b6,three options: biased or upper-bound loss require a single number of samples;
v0.12.0b6,unbiased can take different numbers for the network and its gradient
v0.12.0b6,"sample: (() -> Layer, int) -> Layer"
v0.12.0b6,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.12.0b6,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.12.0b6,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.12.0b6,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.12.0b6,the dimensionality of the output of the network
v0.12.0b6,TODO: is there a more robust way to do this?
v0.12.0b6,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b6,"subtle point: we need to build a new model each time,"
v0.12.0b6,because each model encapsulates its randomness
v0.12.0b6,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b6,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.12.0b6,not a general tensor (because of how backprop works in every framework)
v0.12.0b6,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.12.0b6,but this seems annoying...)
v0.12.0b6,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.12.0b6,TODO: any way to get this to work on batches of arbitrary size?
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b6,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b6,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b6,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b6,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b6,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.12.0b6,"instruments, and outcomes"
v0.12.0b6,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b6,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b6,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b6,for internal use by the library
v0.12.0b6,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b6,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b6,since it expects raw values
v0.12.0b6,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b6,since it expects raw values
v0.12.0b6,"TODO: check that Y, T, Z do not have multiple columns"
v0.12.0b6,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b6,TODO: do correct adjustment for sample_var
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.12.0b6,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.12.0b6,since we would actually be calculating a CATE rather than ATE.
v0.12.0b6,TODO: allow the final model to actually use X?
v0.12.0b6,TODO: allow the final model to actually use X?
v0.12.0b6,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b6,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.12.0b6,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.12.0b6,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b6,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b6,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b6,for internal use by the library
v0.12.0b6,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,"this will have dimension (d,) + shape(X)"
v0.12.0b6,send the first dimension to the end
v0.12.0b6,columns are featurized independently; partial derivatives are only non-zero
v0.12.0b6,when taken with respect to the same column each time
v0.12.0b6,don't fit intercept; manually add column of ones to the data instead;
v0.12.0b6,this allows us to ignore the intercept when computing marginal effects
v0.12.0b6,make T 2D if if was a vector
v0.12.0b6,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.12.0b6,two stage approximation
v0.12.0b6,"first, get basis expansions of T, X, and Z"
v0.12.0b6,TODO: is it right that the effective number of intruments is the
v0.12.0b6,"product of ft_X and ft_Z, not just ft_Z?"
v0.12.0b6,"regress T expansion on X,Z expansions concatenated with W"
v0.12.0b6,"predict ft_T from interacted ft_X, ft_Z"
v0.12.0b6,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.12.0b6,dT may be only 2-dimensional)
v0.12.0b6,promote dT to 3D if necessary (e.g. if T was a vector)
v0.12.0b6,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,TODO: this utility is documented but internal; reimplement?
v0.12.0b6,TODO: this utility is even less public...
v0.12.0b6,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.12.0b6,use same Cs as would be used by default by LogisticRegressionCV
v0.12.0b6,NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
v0.12.0b6,which could affect how many times each distinct Y value needs to be present in the data
v0.12.0b6,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.12.0b6,but also supports get_feature_names with expected signature
v0.12.0b6,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.12.0b6,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.12.0b6,Convert SingleTreeInterpreter to a python dictionary
v0.12.0b6,named tuple type for storing results inside CausalAnalysis class;
v0.12.0b6,must be lifted to module level to enable pickling
v0.12.0b6,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.12.0b6,Controls are all other columns of X
v0.12.0b6,"can't use X[:, feat_ind] when X is a DataFrame"
v0.12.0b6,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.12.0b6,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.12.0b6,so that the user can opt-in to allowing unseen treatment values
v0.12.0b6,(and return NaN or something in that case)
v0.12.0b6,array checking routines don't accept 0-width arrays
v0.12.0b6,perform model selection
v0.12.0b6,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.12.0b6,convert to NormalInferenceResults for consistency
v0.12.0b6,Set the dictionary values shared between local and global summaries
v0.12.0b6,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.12.0b6,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.12.0b6,required to fit a discrete DML model
v0.12.0b6,Validate inputs
v0.12.0b6,TODO: check compatibility of X and Y lengths
v0.12.0b6,"no previous fit, cancel warm start"
v0.12.0b6,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.12.0b6,"if heterogeneity_inds is 1D, repeat it"
v0.12.0b6,heterogeneity inds should be a 2D list of length same as train_inds
v0.12.0b6,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.12.0b6,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.12.0b6,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.12.0b6,train the Y model
v0.12.0b6,"perform model selection for the Y model using all X, not on a per-column basis"
v0.12.0b6,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.12.0b6,work with the regression wrapper
v0.12.0b6,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.12.0b6,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.12.0b6,since otherwise we'll have too many columns to be able to train a classifier
v0.12.0b6,start with empty results and default shared insights
v0.12.0b6,convert categorical indicators to numeric indices
v0.12.0b6,check for indices over the categorical expansion bound
v0.12.0b6,assume we'll be able to train former failures this time; we'll add them back if not
v0.12.0b6,"can't remove in place while iterating over new_inds, so store in separate list"
v0.12.0b6,"train the model, but warn"
v0.12.0b6,no model can be trained in this case since we need more folds
v0.12.0b6,"don't train a model, but suggest workaround since there are enough instances of least"
v0.12.0b6,populated class
v0.12.0b6,also remove from train_inds so we don't try to access the result later
v0.12.0b6,extract subset of names matching new columns
v0.12.0b6,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.12.0b6,don't want to cache this failed result
v0.12.0b6,properties to return from effect InferenceResults
v0.12.0b6,properties to return from PopulationSummaryResults
v0.12.0b6,Converts strings to property lookups or method calls as a convenience so that the
v0.12.0b6,_point_props and _summary_props above can be applied to an inference object
v0.12.0b6,Create a summary combining all results into a single output; this is used
v0.12.0b6,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.12.0b6,"or a dictionary, respectively, based on the summary function passed into this method"
v0.12.0b6,"ensure array has shape (m,y,t)"
v0.12.0b6,population summary is missing sample dimension; add it for consistency
v0.12.0b6,outcome dimension is missing; add it for consistency
v0.12.0b6,add singleton treatment dimension if missing
v0.12.0b6,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.12.0b6,"each attr has dimension (m,y) or (m,y,t)"
v0.12.0b6,concatenate along treatment dimension
v0.12.0b6,"for dictionary representation, want to remove unneeded sample dimension"
v0.12.0b6,in cohort and global results
v0.12.0b6,TODO: enrich outcome logic for multi-class classification when that is supported
v0.12.0b6,There is no actual sample level in this data
v0.12.0b6,can't drop only level
v0.12.0b6,should be serialization-ready and contain no numpy arrays
v0.12.0b6,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.12.0b6,TODO: Note that there's no column metadata for the sample number - should there be?
v0.12.0b6,"need to replicate the column info for each sample, then remove from the shared data"
v0.12.0b6,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.12.0b6,which may need to be revisited once we support multiclass
v0.12.0b6,get the length of the list corresponding to the first dictionary key
v0.12.0b6,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.12.0b6,a global inference indicates the effect of that one feature on the outcome
v0.12.0b6,need to reshape the output to match the input
v0.12.0b6,we want to offset the inference object by the baseline estimate of y
v0.12.0b6,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.12.0b6,get the length of the list corresponding to the first dictionary key
v0.12.0b6,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.12.0b6,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.12.0b6,because then scaling the difference between treatment value and treatment costs is the
v0.12.0b6,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.12.0b6,
v0.12.0b6,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.12.0b6,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.12.0b6,(rather than just not treating at all)
v0.12.0b6,
v0.12.0b6,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.12.0b6,2 * policy_value - always_treat
v0.12.0b6,includes exactly their contribution because policy_value and always_treat both include it
v0.12.0b6,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.12.0b6,2 * policy_value - always-treat
v0.12.0b6,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.12.0b6,is zero and the contribution to always_treat is negative
v0.12.0b6,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b6,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b6,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b6,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b6,get dataframe with all but selected column
v0.12.0b6,apply 10% of a typical treatment for this feature
v0.12.0b6,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.12.0b6,set the effect bounds; for positive treatments these agree with
v0.12.0b6,"the estimates; for negative treatments, we need to invert the interval"
v0.12.0b6,the effect is now always positive since we decrease treatment when negative
v0.12.0b6,"for discrete treatment, stack a zero result in front for control"
v0.12.0b6,we need to call effect_inference to get the correct CI between the two treatment options
v0.12.0b6,we now need to construct the delta in the cost between the two treatments and translate the effect
v0.12.0b6,remove third dimenions potentially added
v0.12.0b6,"find cost of current treatment: equality creates a 2d array with True on each row,"
v0.12.0b6,only if its the location of the current treatment. Then we take the corresponding cost.
v0.12.0b6,construct index of current treatment
v0.12.0b6,add second dimension if needed for broadcasting during translation of effect
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,TODO: conisder working around relying on sklearn implementation details
v0.12.0b6,"Found a good split, return."
v0.12.0b6,Record all splits in case the stratification by weight yeilds a worse partition
v0.12.0b6,Reseed random generator and try again
v0.12.0b6,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.12.0b6,"Found a good split, return."
v0.12.0b6,Did not find a good split
v0.12.0b6,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.12.0b6,Return most weight-balanced partition
v0.12.0b6,Weight stratification algorithm
v0.12.0b6,Sort weights for weight strata search
v0.12.0b6,There are some leftover indices that have yet to be assigned
v0.12.0b6,Append stratum splits to overall splits
v0.12.0b6,"If classification methods produce multiple columns of output,"
v0.12.0b6,we need to manually encode classes to ensure consistent column ordering.
v0.12.0b6,We clone the estimator to make sure that all the folds are
v0.12.0b6,"independent, and that it is pickle-able."
v0.12.0b6,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.12.0b6,`predictions` is a list of method outputs from each fold.
v0.12.0b6,"If each of those is also a list, then treat this as a"
v0.12.0b6,multioutput-multiclass task. We need to separately concatenate
v0.12.0b6,the method outputs for each label into an `n_labels` long list.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Our classes that derive from sklearn ones sometimes include
v0.12.0b6,inherited docstrings that have embedded doctests; we need the following imports
v0.12.0b6,so that they don't break.
v0.12.0b6,TODO: consider working around relying on sklearn implementation details
v0.12.0b6,"Convert X, y into numpy arrays"
v0.12.0b6,Define fit parameters
v0.12.0b6,Some algorithms don't have a check_input option
v0.12.0b6,Check weights array
v0.12.0b6,Check that weights are size-compatible
v0.12.0b6,Normalize inputs
v0.12.0b6,Weight inputs
v0.12.0b6,Fit base class without intercept
v0.12.0b6,Fit Lasso
v0.12.0b6,Reset intercept
v0.12.0b6,The intercept is not calculated properly due the sqrt(weights) factor
v0.12.0b6,so it must be recomputed
v0.12.0b6,Fit lasso without weights
v0.12.0b6,Make weighted splitter
v0.12.0b6,Fit weighted model
v0.12.0b6,Make weighted splitter
v0.12.0b6,Fit weighted model
v0.12.0b6,Call weighted lasso on reduced design matrix
v0.12.0b6,Weighted tau
v0.12.0b6,Select optimal penalty
v0.12.0b6,Warn about consistency
v0.12.0b6,"Convert X, y into numpy arrays"
v0.12.0b6,Fit weighted lasso with user input
v0.12.0b6,"Center X, y"
v0.12.0b6,Calculate quantities that will be used later on. Account for centered data
v0.12.0b6,Calculate coefficient and error variance
v0.12.0b6,Add coefficient correction
v0.12.0b6,Set coefficients and intercept standard errors
v0.12.0b6,Set intercept
v0.12.0b6,Return alpha to 'auto' state
v0.12.0b6,"Note that in the case of no intercept, X_offset is 0"
v0.12.0b6,Calculate the variance of the predictions
v0.12.0b6,Calculate prediction confidence intervals
v0.12.0b6,Assumes flattened y
v0.12.0b6,Compute weighted residuals
v0.12.0b6,To be done once per target. Assumes y can be flattened.
v0.12.0b6,Assumes that X has already been offset
v0.12.0b6,Special case: n_features=1
v0.12.0b6,Compute Lasso coefficients for the columns of the design matrix
v0.12.0b6,Compute C_hat
v0.12.0b6,Compute theta_hat
v0.12.0b6,Allow for single output as well
v0.12.0b6,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.12.0b6,Set coef_ attribute
v0.12.0b6,Set intercept_ attribute
v0.12.0b6,Set selected_alpha_ attribute
v0.12.0b6,Set coef_stderr_
v0.12.0b6,intercept_stderr_
v0.12.0b6,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.12.0b6,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.12.0b6,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.12.0b6,set intercept_ attribute
v0.12.0b6,set coef_ attribute
v0.12.0b6,set alpha_ attribute
v0.12.0b6,set alphas_ attribute
v0.12.0b6,set n_iter_ attribute
v0.12.0b6,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.12.0b6,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.12.0b6,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.12.0b6,now regress X1 on y - X2 * beta2 to learn beta1
v0.12.0b6,set coef_ and intercept_ attributes
v0.12.0b6,Note that the penalized model should *not* have an intercept
v0.12.0b6,don't proxy special methods
v0.12.0b6,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.12.0b6,regressor incorrectly
v0.12.0b6,"Note: for known attributes that have been set this method will not be called,"
v0.12.0b6,so we should just throw here because this is an attribute belonging to this class
v0.12.0b6,but which hasn't yet been set on this instance
v0.12.0b6,set default values for None
v0.12.0b6,check freq_weight should be integer and should be accompanied by sample_var
v0.12.0b6,check array shape
v0.12.0b6,weight X and y and sample_var
v0.12.0b6,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,AzureML
v0.12.0b6,helper imports
v0.12.0b6,write the details of the workspace to a configuration file to the notebook library
v0.12.0b6,if y is a multioutput model
v0.12.0b6,Make sure second dimension has 1 or more item
v0.12.0b6,switch _inner Model to a MultiOutputRegressor
v0.12.0b6,flatten array as automl only takes vectors for y
v0.12.0b6,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.12.0b6,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.12.0b6,as an sklearn estimator
v0.12.0b6,fit implementation for a single output model.
v0.12.0b6,Create experiment for specified workspace
v0.12.0b6,Configure automl_config with training set information.
v0.12.0b6,"Wait for remote run to complete, the set the model"
v0.12.0b6,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.12.0b6,create model and pass model into final.
v0.12.0b6,"If item is an automl config, get its corresponding"
v0.12.0b6,AutomatedML Model and add it to new_Args
v0.12.0b6,"If item is an automl config, get its corresponding"
v0.12.0b6,AutomatedML Model and set it for this key in
v0.12.0b6,kwargs
v0.12.0b6,takes in either automated_ml config and instantiates
v0.12.0b6,an AutomatedMLModel
v0.12.0b6,The prefix can only be 18 characters long
v0.12.0b6,"because prefixes come from kwarg_names, we must ensure they are"
v0.12.0b6,short enough.
v0.12.0b6,Get workspace from config file.
v0.12.0b6,Take the intersect of the white for sample
v0.12.0b6,weights and linear models
v0.12.0b6,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,average the outcome dimension if it exists and ensure 2d y_pred
v0.12.0b6,get index of best treatment
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,TODO: consider working around relying on sklearn implementation details
v0.12.0b6,Create splits of causal tree
v0.12.0b6,Make sure the correct exception is being rethrown
v0.12.0b6,Must make sure indices are merged correctly
v0.12.0b6,Convert rows to columns
v0.12.0b6,Require group assignment t to be one-hot-encoded
v0.12.0b6,Get predictions for the 2 splits
v0.12.0b6,Must make sure indices are merged correctly
v0.12.0b6,Crossfitting
v0.12.0b6,Compute weighted nuisance estimates
v0.12.0b6,-------------------------------------------------------------------------------
v0.12.0b6,Calculate the covariance matrix corresponding to the BLB inference
v0.12.0b6,
v0.12.0b6,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.12.0b6,2. Calculate the weighted moments for each tree slice to create a matrix
v0.12.0b6,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.12.0b6,in that slice from the overall parameter estimate.
v0.12.0b6,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.12.0b6,-------------------------------------------------------------------------------
v0.12.0b6,Calclulate covariance matrix through BLB
v0.12.0b6,Estimators
v0.12.0b6,OrthoForest parameters
v0.12.0b6,Sub-forests
v0.12.0b6,Auxiliary attributes
v0.12.0b6,Fit check
v0.12.0b6,TODO: Check performance
v0.12.0b6,Must normalize weights
v0.12.0b6,Override the CATE inference options
v0.12.0b6,Add blb inference to parent's options
v0.12.0b6,Generate subsample indices
v0.12.0b6,Build trees in parallel
v0.12.0b6,Bootstraping has repetitions in tree sample
v0.12.0b6,Similar for `a` weights
v0.12.0b6,Bootstraping has repetitions in tree sample
v0.12.0b6,Define subsample size
v0.12.0b6,Safety check
v0.12.0b6,Draw points to create little bags
v0.12.0b6,Copy and/or define models
v0.12.0b6,Define nuisance estimators
v0.12.0b6,Define parameter estimators
v0.12.0b6,Define
v0.12.0b6,Need to redefine fit here for auto inference to work due to a quirk in how
v0.12.0b6,wrap_fit is defined
v0.12.0b6,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b6,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b6,Override to flatten output if T is flat
v0.12.0b6,Check that all discrete treatments are represented
v0.12.0b6,Nuissance estimates evaluated with cross-fitting
v0.12.0b6,Define 2-fold iterator
v0.12.0b6,Check if there is only one example of some class
v0.12.0b6,Define 2-fold iterator
v0.12.0b6,need safe=False when cloning for WeightedModelWrapper
v0.12.0b6,Compute residuals
v0.12.0b6,Compute coefficient by OLS on residuals
v0.12.0b6,"Parameter returned by LinearRegression is (d_T, )"
v0.12.0b6,Compute residuals
v0.12.0b6,Compute coefficient by OLS on residuals
v0.12.0b6,ell_2 regularization
v0.12.0b6,Ridge regression estimate
v0.12.0b6,"Parameter returned is of shape (d_T, )"
v0.12.0b6,Return moments and gradients
v0.12.0b6,Compute residuals
v0.12.0b6,Compute moments
v0.12.0b6,"Moments shape is (n, d_T)"
v0.12.0b6,Compute moment gradients
v0.12.0b6,returns shape-conforming residuals
v0.12.0b6,Copy and/or define models
v0.12.0b6,Define parameter estimators
v0.12.0b6,Define moment and mean gradient estimator
v0.12.0b6,"Check that T is shape (n, )"
v0.12.0b6,Check T is numeric
v0.12.0b6,Train label encoder
v0.12.0b6,Call `fit` from parent class
v0.12.0b6,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b6,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b6,Override to flatten output if T is flat
v0.12.0b6,Expand one-hot encoding to include the zero treatment
v0.12.0b6,"Test that T contains all treatments. If not, return None"
v0.12.0b6,Nuissance estimates evaluated with cross-fitting
v0.12.0b6,Define 2-fold iterator
v0.12.0b6,Check if there is only one example of some class
v0.12.0b6,No need to crossfit for internal nodes
v0.12.0b6,Compute partial moments
v0.12.0b6,"If any of the values in the parameter estimate is nan, return None"
v0.12.0b6,Compute partial moments
v0.12.0b6,Compute coefficient by OLS on residuals
v0.12.0b6,ell_2 regularization
v0.12.0b6,Ridge regression estimate
v0.12.0b6,"Parameter returned is of shape (d_T, )"
v0.12.0b6,Return moments and gradients
v0.12.0b6,Compute partial moments
v0.12.0b6,Compute moments
v0.12.0b6,"Moments shape is (n, d_T-1)"
v0.12.0b6,Compute moment gradients
v0.12.0b6,Need to calculate this in an elegant way for when propensity is 0
v0.12.0b6,This will flatten T
v0.12.0b6,Check that T is numeric
v0.12.0b6,Test whether the input estimator is supported
v0.12.0b6,Calculate confidence intervals for the parameter (marginal effect)
v0.12.0b6,Calculate confidence intervals for the effect
v0.12.0b6,Calculate the effects
v0.12.0b6,Calculate the standard deviations for the effects
v0.12.0b6,d_t=None here since we measure the effect across all Ts
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b6,Licensed under the MIT License.
v0.12.0b6,Causal tree parameters
v0.12.0b6,Tree structure
v0.12.0b6,No need for a random split since the data is already
v0.12.0b6,a random subsample from the original input
v0.12.0b6,node list stores the nodes that are yet to be splitted
v0.12.0b6,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b6,Create local sample set
v0.12.0b6,Compute nuisance estimates for the current node
v0.12.0b6,Nuisance estimate cannot be calculated
v0.12.0b6,Estimate parameter for current node
v0.12.0b6,Node estimate cannot be calculated
v0.12.0b6,Calculate moments and gradient of moments for current data
v0.12.0b6,Calculate inverse gradient
v0.12.0b6,The gradient matrix is not invertible.
v0.12.0b6,No good split can be found
v0.12.0b6,Calculate point-wise pseudo-outcomes rho
v0.12.0b6,a split is determined by a feature and a sample pair
v0.12.0b6,the number of possible splits is at most (number of features) * (number of node samples)
v0.12.0b6,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.12.0b6,parse row and column of random pair
v0.12.0b6,the sample of the pair is the integer division of the random number with n_feats
v0.12.0b6,calculate the binary indicator of whether sample i is on the left or the right
v0.12.0b6,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.12.0b6,calculate the number of samples on the left child for each proposed split
v0.12.0b6,calculate the analogous binary indicator for the samples in the estimation set
v0.12.0b6,calculate the number of estimation samples on the left child of each proposed split
v0.12.0b6,find the upper and lower bound on the size of the left split for the split
v0.12.0b6,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.12.0b6,on each side.
v0.12.0b6,similarly for the estimation sample set
v0.12.0b6,if there is no valid split then don't create any children
v0.12.0b6,filter only the valid splits
v0.12.0b6,calculate the average influence vector of the samples in the left child
v0.12.0b6,calculate the average influence vector of the samples in the right child
v0.12.0b6,take the square of each of the entries of the influence vectors and normalize
v0.12.0b6,by size of each child
v0.12.0b6,calculate the vector score of each candidate split as the average of left and right
v0.12.0b6,influence vectors
v0.12.0b6,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.12.0b6,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.12.0b6,where there might be large discontinuities in some parameter as the conditioning set varies
v0.12.0b6,calculate the scalar score of each split by aggregating across the vector of scores
v0.12.0b6,Find split that minimizes criterion
v0.12.0b6,Create child nodes with corresponding subsamples
v0.12.0b6,add the created children to the list of not yet split nodes
v0.12.0b5,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.12.0b5,configuration is all pulled from setup.cfg
v0.12.0b5,-*- coding: utf-8 -*-
v0.12.0b5,
v0.12.0b5,Configuration file for the Sphinx documentation builder.
v0.12.0b5,
v0.12.0b5,This file does only contain a selection of the most common options. For a
v0.12.0b5,full list see the documentation:
v0.12.0b5,http://www.sphinx-doc.org/en/master/config
v0.12.0b5,-- Path setup --------------------------------------------------------------
v0.12.0b5,"If extensions (or modules to document with autodoc) are in another directory,"
v0.12.0b5,add these directories to sys.path here. If the directory is relative to the
v0.12.0b5,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.12.0b5,
v0.12.0b5,-- Project information -----------------------------------------------------
v0.12.0b5,-- General configuration ---------------------------------------------------
v0.12.0b5,"If your documentation needs a minimal Sphinx version, state it here."
v0.12.0b5,
v0.12.0b5,needs_sphinx = '1.0'
v0.12.0b5,"Add any Sphinx extension module names here, as strings. They can be"
v0.12.0b5,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.12.0b5,ones.
v0.12.0b5,"Add any paths that contain templates here, relative to this directory."
v0.12.0b5,The suffix(es) of source filenames.
v0.12.0b5,You can specify multiple suffix as a list of string:
v0.12.0b5,
v0.12.0b5,"source_suffix = ['.rst', '.md']"
v0.12.0b5,The master toctree document.
v0.12.0b5,The language for content autogenerated by Sphinx. Refer to documentation
v0.12.0b5,for a list of supported languages.
v0.12.0b5,
v0.12.0b5,This is also used if you do content translation via gettext catalogs.
v0.12.0b5,"Usually you set ""language"" from the command line for these cases."
v0.12.0b5,"List of patterns, relative to source directory, that match files and"
v0.12.0b5,directories to ignore when looking for source files.
v0.12.0b5,This pattern also affects html_static_path and html_extra_path.
v0.12.0b5,The name of the Pygments (syntax highlighting) style to use.
v0.12.0b5,-- Options for HTML output -------------------------------------------------
v0.12.0b5,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.12.0b5,a list of builtin themes.
v0.12.0b5,
v0.12.0b5,Theme options are theme-specific and customize the look and feel of a theme
v0.12.0b5,"further.  For a list of options available for each theme, see the"
v0.12.0b5,documentation.
v0.12.0b5,
v0.12.0b5,"Add any paths that contain custom static files (such as style sheets) here,"
v0.12.0b5,"relative to this directory. They are copied after the builtin static files,"
v0.12.0b5,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.12.0b5,html_static_path = ['_static']
v0.12.0b5,"Custom sidebar templates, must be a dictionary that maps document names"
v0.12.0b5,to template names.
v0.12.0b5,
v0.12.0b5,The default sidebars (for documents that don't match any pattern) are
v0.12.0b5,defined by theme itself.  Builtin themes are using these templates by
v0.12.0b5,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.12.0b5,'searchbox.html']``.
v0.12.0b5,
v0.12.0b5,html_sidebars = {}
v0.12.0b5,-- Options for HTMLHelp output ---------------------------------------------
v0.12.0b5,Output file base name for HTML help builder.
v0.12.0b5,-- Options for LaTeX output ------------------------------------------------
v0.12.0b5,The paper size ('letterpaper' or 'a4paper').
v0.12.0b5,
v0.12.0b5,"'papersize': 'letterpaper',"
v0.12.0b5,"The font size ('10pt', '11pt' or '12pt')."
v0.12.0b5,
v0.12.0b5,"'pointsize': '10pt',"
v0.12.0b5,Additional stuff for the LaTeX preamble.
v0.12.0b5,
v0.12.0b5,"'preamble': '',"
v0.12.0b5,Latex figure (float) alignment
v0.12.0b5,
v0.12.0b5,"'figure_align': 'htbp',"
v0.12.0b5,Grouping the document tree into LaTeX files. List of tuples
v0.12.0b5,"(source start file, target name, title,"
v0.12.0b5,"author, documentclass [howto, manual, or own class])."
v0.12.0b5,-- Options for manual page output ------------------------------------------
v0.12.0b5,One entry per manual page. List of tuples
v0.12.0b5,"(source start file, name, description, authors, manual section)."
v0.12.0b5,-- Options for Texinfo output ----------------------------------------------
v0.12.0b5,Grouping the document tree into Texinfo files. List of tuples
v0.12.0b5,"(source start file, target name, title, author,"
v0.12.0b5,"dir menu entry, description, category)"
v0.12.0b5,-- Options for Epub output -------------------------------------------------
v0.12.0b5,Bibliographic Dublin Core info.
v0.12.0b5,The unique identifier of the text. This can be a ISBN number
v0.12.0b5,or the project homepage.
v0.12.0b5,
v0.12.0b5,epub_identifier = ''
v0.12.0b5,A unique identification for the text.
v0.12.0b5,
v0.12.0b5,epub_uid = ''
v0.12.0b5,A list of files that should not be packed into the epub file.
v0.12.0b5,-- Extension configuration -------------------------------------------------
v0.12.0b5,-- Options for intersphinx extension ---------------------------------------
v0.12.0b5,Example configuration for intersphinx: refer to the Python standard library.
v0.12.0b5,-- Options for todo extension ----------------------------------------------
v0.12.0b5,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.12.0b5,-- Options for doctest extension -------------------------------------------
v0.12.0b5,we can document otherwise excluded entities here by returning False
v0.12.0b5,or skip otherwise included entities by returning True
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Calculate residuals
v0.12.0b5,Estimate E[T_res | Z_res]
v0.12.0b5,TODO. Deal with multi-class instrument
v0.12.0b5,Calculate nuisances
v0.12.0b5,Estimate E[T_res | Z_res]
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"We do a three way split, as typically a preliminary theta estimator would require"
v0.12.0b5,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.12.0b5,TODO. Deal with multi-class instrument
v0.12.0b5,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b5,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b5,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b5,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b5,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b5,Estimate preliminary theta in cross fitting manner
v0.12.0b5,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b5,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.12.0b5,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.12.0b5,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b5,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.12.0b5,TODO. The solution below is not really a valid cross-fitting
v0.12.0b5,as the test data are used to create the proj_t on the train
v0.12.0b5,which in the second train-test loop is used to create the nuisance
v0.12.0b5,cov on the test data. Hence the T variable of some sample
v0.12.0b5,"is implicitly correlated with its cov nuisance, through this flow"
v0.12.0b5,"of information. However, this seems a rather weak correlation."
v0.12.0b5,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.12.0b5,model.
v0.12.0b5,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.12.0b5,Estimate preliminary theta in cross fitting manner
v0.12.0b5,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b5,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.12.0b5,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b5,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.12.0b5,#############################################################################
v0.12.0b5,Classes for the DRIV implementation for the special case of intent-to-treat
v0.12.0b5,A/B test
v0.12.0b5,#############################################################################
v0.12.0b5,Estimate preliminary theta in cross fitting manner
v0.12.0b5,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b5,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b5,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b5,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b5,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.12.0b5,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.12.0b5,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.12.0b5,model_T_XZ = lambda: model_clf()
v0.12.0b5,#'days_visited': lambda:
v0.12.0b5,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.12.0b5,Turn strings into categories for numeric mapping
v0.12.0b5,### Defining some generic regressors and classifiers
v0.12.0b5,This a generic non-parametric regressor
v0.12.0b5,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b5,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.12.0b5,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b5,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.12.0b5,model = lambda: RandomForestRegressor(n_estimators=100)
v0.12.0b5,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.12.0b5,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.12.0b5,model = lambda: LinearRegression(n_jobs=-1)
v0.12.0b5,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.12.0b5,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.12.0b5,underlying model whenever predict is called.
v0.12.0b5,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b5,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.12.0b5,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b5,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.12.0b5,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.12.0b5,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.12.0b5,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.12.0b5,We need to specify models to be used for each of these residualizations
v0.12.0b5,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.12.0b5,"E[T | X, Z]"
v0.12.0b5,E[TZ | X]
v0.12.0b5,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.12.0b5,n_splits determines the number of splits to be used for cross-fitting.
v0.12.0b5,# Algorithm 2 - Current Method
v0.12.0b5,In[121]:
v0.12.0b5,# Algorithm 3 - DRIV ATE
v0.12.0b5,dmliv_model_effect = lambda: model()
v0.12.0b5,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.12.0b5,"dmliv_model_effect(),"
v0.12.0b5,n_splits=1)
v0.12.0b5,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.12.0b5,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.12.0b5,"Once multiple treatments are supported, we'll need to fix this"
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b5,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b5,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b5,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,TODO. Deal with multi-class instrument/treatment
v0.12.0b5,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.12.0b5,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.12.0b5,Estimate p(X) = E[T | X] in cross-fitting manner
v0.12.0b5,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.12.0b5,##################
v0.12.0b5,Global settings #
v0.12.0b5,##################
v0.12.0b5,Global plotting controls
v0.12.0b5,"Control for support size, can control for more"
v0.12.0b5,#################
v0.12.0b5,File utilities #
v0.12.0b5,#################
v0.12.0b5,#################
v0.12.0b5,Plotting utils #
v0.12.0b5,#################
v0.12.0b5,bias
v0.12.0b5,var
v0.12.0b5,rmse
v0.12.0b5,r2
v0.12.0b5,Infer feature dimension
v0.12.0b5,Metrics by support plots
v0.12.0b5,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.12.0b5,Vasilis Syrgkanis <vasy@microsoft.com>
v0.12.0b5,Steven Wu <zhiww@microsoft.com>
v0.12.0b5,Initialize causal tree parameters
v0.12.0b5,Create splits of causal tree
v0.12.0b5,Estimate treatment effects at the leafs
v0.12.0b5,Compute heterogeneous treatement effect for x's in x_list by finding
v0.12.0b5,the corresponding split and associating the effect computed on that leaf
v0.12.0b5,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.12.0b5,Safety check
v0.12.0b5,Weighted linear regression
v0.12.0b5,Calculates weights
v0.12.0b5,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b5,over all indices
v0.12.0b5,Similar for `a` weights
v0.12.0b5,Doesn't have sample weights
v0.12.0b5,Is a linear model
v0.12.0b5,Weighted linear regression
v0.12.0b5,Calculates weights
v0.12.0b5,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b5,over all indices
v0.12.0b5,Similar for `a` weights
v0.12.0b5,normalize weights
v0.12.0b5,"Split the data in half, train and test"
v0.12.0b5,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b5,"a function of W, using only the train fold"
v0.12.0b5,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b5,"Split the data in half, train and test"
v0.12.0b5,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b5,"a function of W, using only the train fold"
v0.12.0b5,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b5,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.12.0b5,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.12.0b5,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.12.0b5,"Split the data in half, train and test"
v0.12.0b5,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b5,"a function of x, using only the train fold"
v0.12.0b5,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b5,Compute coefficient by OLS on residuals
v0.12.0b5,"Split the data in half, train and test"
v0.12.0b5,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b5,"a function of x, using only the train fold"
v0.12.0b5,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b5,Estimate multipliers for second order orthogonal method
v0.12.0b5,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.12.0b5,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b5,Create local sample set
v0.12.0b5,compute the base estimate for the current node using double ml or second order double ml
v0.12.0b5,compute the influence functions here that are used for the criterion
v0.12.0b5,generate random proposals of dimensions to split
v0.12.0b5,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.12.0b5,compute criterion for each proposal
v0.12.0b5,if splitting creates valid leafs in terms of mean leaf size
v0.12.0b5,Calculate criterion for split
v0.12.0b5,Else set criterion to infinity so that this split is not chosen
v0.12.0b5,If no good split was found
v0.12.0b5,Find split that minimizes criterion
v0.12.0b5,Set the split attributes at the node
v0.12.0b5,Create child nodes with corresponding subsamples
v0.12.0b5,Recursively split children
v0.12.0b5,Return parent node
v0.12.0b5,estimate the local parameter at the leaf using the estimate data
v0.12.0b5,###################
v0.12.0b5,Argument parsing #
v0.12.0b5,###################
v0.12.0b5,#########################################
v0.12.0b5,Parameters constant across experiments #
v0.12.0b5,#########################################
v0.12.0b5,Outcome support
v0.12.0b5,Treatment support
v0.12.0b5,Evaluation grid
v0.12.0b5,Treatment effects array
v0.12.0b5,Other variables
v0.12.0b5,##########################
v0.12.0b5,Data Generating Process #
v0.12.0b5,##########################
v0.12.0b5,Log iteration
v0.12.0b5,"Generate controls, features, treatment and outcome"
v0.12.0b5,T and Y residuals to be used in later scripts
v0.12.0b5,Save generated dataset
v0.12.0b5,#################
v0.12.0b5,ORF parameters #
v0.12.0b5,#################
v0.12.0b5,######################################
v0.12.0b5,Train and evaluate treatment effect #
v0.12.0b5,######################################
v0.12.0b5,########
v0.12.0b5,Plots #
v0.12.0b5,########
v0.12.0b5,###############
v0.12.0b5,Save results #
v0.12.0b5,###############
v0.12.0b5,##############
v0.12.0b5,Run Rscript #
v0.12.0b5,##############
v0.12.0b5,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b5,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b5,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.12.0b5,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b5,def mlasso_model(): return MultiTaskLassoCV(
v0.12.0b5,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b5,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b5,heterogeneity
v0.12.0b5,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b5,heterogeneity
v0.12.0b5,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b5,heterogeneity
v0.12.0b5,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.12.0b5,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b5,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b5,subset of features that are exogenous and create heterogeneity
v0.12.0b5,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.12.0b5,subset of features wrt we estimate heterogeneity
v0.12.0b5,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b5,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,introspect the constructor arguments to find the model parameters
v0.12.0b5,to represent
v0.12.0b5,"if the argument is deprecated, ignore it"
v0.12.0b5,Extract and sort argument names excluding 'self'
v0.12.0b5,column names
v0.12.0b5,transfer input to numpy arrays
v0.12.0b5,transfer input to 2d arrays
v0.12.0b5,create dataframe
v0.12.0b5,currently dowhy only support single outcome and single treatment
v0.12.0b5,call dowhy
v0.12.0b5,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.12.0b5,cate estimator but not the effect.
v0.12.0b5,don't proxy special methods
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Check if model is sparse enough for this model
v0.12.0b5,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.12.0b5,TODO: any way to avoid creating a copy if the array was already dense?
v0.12.0b5,"the call is necessary if the input was something like a list, though"
v0.12.0b5,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.12.0b5,so convert to pydata sparse first
v0.12.0b5,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.12.0b5,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.12.0b5,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.12.0b5,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b5,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b5,For when checking input values is disabled
v0.12.0b5,Type to column extraction function
v0.12.0b5,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.12.0b5,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.12.0b5,Get feature names using featurizer
v0.12.0b5,All attempts at retrieving transformed feature names have failed
v0.12.0b5,Delegate handling to downstream logic
v0.12.0b5,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.12.0b5,same number of input definitions as arrays
v0.12.0b5,input definitions have same number of dimensions as each array
v0.12.0b5,all result indices are unique
v0.12.0b5,all result indices must match at least one input index
v0.12.0b5,"map indices to all array, axis pairs for that index"
v0.12.0b5,each index has the same cardinality wherever it appears
v0.12.0b5,"State: list of (set of letters, list of (corresponding indices, value))"
v0.12.0b5,Algo: while list contains more than one entry
v0.12.0b5,take two entries
v0.12.0b5,sort both lists by intersection of their indices
v0.12.0b5,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.12.0b5,"take the union of indices and the product of values), stepping through each list linearly"
v0.12.0b5,TODO: might be faster to break into connected components first
v0.12.0b5,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.12.0b5,"so compute their content separately, then take cartesian product"
v0.12.0b5,this would save a few pointless sorts by empty tuples
v0.12.0b5,TODO: Consider investigating other performance ideas for these cases
v0.12.0b5,where the dense method beat the sparse method (usually sparse is faster)
v0.12.0b5,"e,facd,c->cfed"
v0.12.0b5,sparse: 0.0335489
v0.12.0b5,dense:  0.011465999999999997
v0.12.0b5,"gbd,da,egb->da"
v0.12.0b5,sparse: 0.0791625
v0.12.0b5,dense:  0.007319099999999995
v0.12.0b5,"dcc,d,faedb,c->abe"
v0.12.0b5,sparse: 1.2868097
v0.12.0b5,dense:  0.44605229999999985
v0.12.0b5,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.12.0b5,TODO: would using einsum's paths to optimize the order of merging help?
v0.12.0b5,assume that we should perform nested cross-validation if and only if
v0.12.0b5,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.12.0b5,logic copied from check_cv
v0.12.0b5,otherwise we will assume the user already set the cv attribute to something
v0.12.0b5,compatible with splitting with a 'groups' argument
v0.12.0b5,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.12.0b5,don't use the groups when calling split internally
v0.12.0b5,Normalize weights
v0.12.0b5,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.12.0b5,"if we're decorating a class, just update the __init__ method,"
v0.12.0b5,so that the result is still a class instead of a wrapper method
v0.12.0b5,"want to enforce that each bad_arg was either in kwargs,"
v0.12.0b5,or else it was in neither and is just taking its default value
v0.12.0b5,Any access should throw
v0.12.0b5,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b5,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b5,input feature name is already updated by cate_feature_names.
v0.12.0b5,define the index of d_x to filter for each given T
v0.12.0b5,filter X after broadcast with T for each given T
v0.12.0b5,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b5,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,
v0.12.0b5,This code contains some snippets of code from:
v0.12.0b5,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
v0.12.0b5,published under the following license and copyright:
v0.12.0b5,BSD 3-Clause License
v0.12.0b5,
v0.12.0b5,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b5,All rights reserved.
v0.12.0b5,make any access to matplotlib or plt throw an exception
v0.12.0b5,make any access to graphviz or plt throw an exception
v0.12.0b5,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.12.0b5,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.12.0b5,Initialize saturation & value; calculate chroma & value shift
v0.12.0b5,Calculate some intermediate values
v0.12.0b5,Initialize RGB with same hue & chroma as our color
v0.12.0b5,Shift the initial RGB values to match value and store
v0.12.0b5,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.12.0b5,clean way of achieving this
v0.12.0b5,make sure we don't accidentally escape anything in the substitution
v0.12.0b5,Fetch appropriate color for node
v0.12.0b5,"red for negative, green for positive"
v0.12.0b5,in multi-target use mean of targets
v0.12.0b5,Write node mean CATE
v0.12.0b5,Write node std of CATE
v0.12.0b5,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.12.0b5,Fetch appropriate color for node
v0.12.0b5,Write node mean CATE
v0.12.0b5,Write node mean CATE
v0.12.0b5,Write recommended treatment and value - cost
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"since inference objects can be stateful, we must copy it before fitting;"
v0.12.0b5,otherwise this sequence wouldn't work:
v0.12.0b5,"est1.fit(..., inference=inf)"
v0.12.0b5,"est2.fit(..., inference=inf)"
v0.12.0b5,est1.effect_interval(...)
v0.12.0b5,because inf now stores state from fitting est2
v0.12.0b5,This flag is true when names are set in a child class instead
v0.12.0b5,"If names are set in a child class, add an attribute reflecting that"
v0.12.0b5,This works only if X is passed as a kwarg
v0.12.0b5,We plan to enforce X as kwarg only in future releases
v0.12.0b5,This checks if names have been set in a child class
v0.12.0b5,"If names were set in a child class, don't do it again"
v0.12.0b5,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.12.0b5,call the wrapped fit method
v0.12.0b5,NOTE: we call inference fit *after* calling the main fit method
v0.12.0b5,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.12.0b5,but tensordot can't be applied to this problem because we don't sum over m
v0.12.0b5,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.12.0b5,of rows of T was not taken into account
v0.12.0b5,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.12.0b5,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.12.0b5,"Treatment names is None, default to BaseCateEstimator"
v0.12.0b5,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.12.0b5,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.12.0b5,Get input names
v0.12.0b5,Summary
v0.12.0b5,add statsmodels to parent's options
v0.12.0b5,add debiasedlasso to parent's options
v0.12.0b5,add blb to parent's options
v0.12.0b5,TODO Share some logic with non-discrete version
v0.12.0b5,Get input names
v0.12.0b5,Summary
v0.12.0b5,add statsmodels to parent's options
v0.12.0b5,add statsmodels to parent's options
v0.12.0b5,add blb to parent's options
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,remove None arguments
v0.12.0b5,"scores entries should be lists of scores, so make each entry a singleton list"
v0.12.0b5,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b5,generate an instance of the final model
v0.12.0b5,generate an instance of the nuisance model
v0.12.0b5,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.12.0b5,alteration even when we only want to fit_final.
v0.12.0b5,use a binary array to get stratified split in case of discrete treatment
v0.12.0b5,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.12.0b5,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.12.0b5,"however, sklearn doesn't support both stratifying and grouping (see"
v0.12.0b5,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.12.0b5,their own object that supports grouping if they want to use groups.
v0.12.0b5,for each mc iteration
v0.12.0b5,for each model under cross fit setting
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,
v0.12.0b5,This code contains snippets of code from
v0.12.0b5,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b5,published under the following license and copyright:
v0.12.0b5,BSD 3-Clause License
v0.12.0b5,
v0.12.0b5,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b5,All rights reserved.
v0.12.0b5,=============================================================================
v0.12.0b5,Policy Forest
v0.12.0b5,=============================================================================
v0.12.0b5,Remap output
v0.12.0b5,reshape is necessary to preserve the data contiguity against vs
v0.12.0b5,"[:, np.newaxis] that does not."
v0.12.0b5,Get subsample sample size
v0.12.0b5,Check parameters
v0.12.0b5,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b5,if this is the first `fit` call of the warm start mode.
v0.12.0b5,"Free allocated memory, if any"
v0.12.0b5,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b5,We draw from the random state to get the random state we
v0.12.0b5,would have got if we hadn't used a warm_start.
v0.12.0b5,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b5,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b5,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b5,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b5,for fitting the trees is internally releasing the Python GIL
v0.12.0b5,making threading more efficient than multiprocessing in
v0.12.0b5,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b5,"parallel_backend contexts set at a higher level,"
v0.12.0b5,since correctness does not rely on using threads.
v0.12.0b5,Collect newly grown trees
v0.12.0b5,Check data
v0.12.0b5,Assign chunk of trees to jobs
v0.12.0b5,avoid storing the output of every estimator by summing them here
v0.12.0b5,Parallel loop
v0.12.0b5,Check data
v0.12.0b5,Assign chunk of trees to jobs
v0.12.0b5,avoid storing the output of every estimator by summing them here
v0.12.0b5,Parallel loop
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,
v0.12.0b5,This code contains snippets of code from:
v0.12.0b5,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b5,published under the following license and copyright:
v0.12.0b5,BSD 3-Clause License
v0.12.0b5,
v0.12.0b5,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b5,All rights reserved.
v0.12.0b5,=============================================================================
v0.12.0b5,Types and constants
v0.12.0b5,=============================================================================
v0.12.0b5,=============================================================================
v0.12.0b5,Base Policy tree
v0.12.0b5,=============================================================================
v0.12.0b5,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.12.0b5,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.12.0b5,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.12.0b5,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.12.0b5,checks that X is 2D array.
v0.12.0b5,"since we only allow single dimensional y, we could flatten the prediction"
v0.12.0b5,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b5,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b5,Handles the corner case when X=None but featurizer might be not None
v0.12.0b5,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.12.0b5,"Replacing this method which is invalid for this class, so that we make the"
v0.12.0b5,dosctring empty and not appear in the docs.
v0.12.0b5,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b5,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.12.0b5,Replacing to remove docstring
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"if both X and W are None, just return a column of ones"
v0.12.0b5,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b5,We need to go back to the label representation of the one-hot so as to call
v0.12.0b5,the classifier.
v0.12.0b5,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b5,We need to go back to the label representation of the one-hot so as to call
v0.12.0b5,the classifier.
v0.12.0b5,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b5,This works both with our without the weighting trick as the treatments T are unit vector
v0.12.0b5,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.12.0b5,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.12.0b5,both Parametric and Non Parametric DML.
v0.12.0b5,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.12.0b5,attribute so that the trained featurizer will be passed through
v0.12.0b5,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b5,for internal use by the library
v0.12.0b5,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b5,We need to use the rlearner's copy to retain the information from fitting
v0.12.0b5,Handles the corner case when X=None but featurizer might be not None
v0.12.0b5,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b5,since we clone it and fit separate copies
v0.12.0b5,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b5,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b5,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b5,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b5,since we clone it and fit separate copies
v0.12.0b5,add blb to parent's options
v0.12.0b5,override only so that we can update the docstring to indicate
v0.12.0b5,support for `GenericSingleTreatmentModelFinalInference`
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,note that groups are not passed to score because they are only used for fitting
v0.12.0b5,note that groups are not passed to score because they are only used for fitting
v0.12.0b5,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b5,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b5,NOTE: important to get parent's wrapped copy so that
v0.12.0b5,"after training wrapped featurizer is also trained, etc."
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b5,Fit a doubly robust average effect
v0.12.0b5,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b5,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b5,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b5,since we clone it and fit separate copies
v0.12.0b5,"If custom param grid, check that only estimator parameters are being altered"
v0.12.0b5,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.12.0b5,override only so that we can update the docstring to indicate support for `blb`
v0.12.0b5,Get input names
v0.12.0b5,Summary
v0.12.0b5,Determine output settings
v0.12.0b5,"Important: This must be the first invocation of the random state at fit time, so that"
v0.12.0b5,train/test splits are re-generatable from an external object simply by knowing the
v0.12.0b5,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.12.0b5,linear predictions. Currently is also useful for testing.
v0.12.0b5,reshape is necessary to preserve the data contiguity against vs
v0.12.0b5,"[:, np.newaxis] that does not."
v0.12.0b5,Check parameters
v0.12.0b5,Set min_weight_leaf from min_weight_fraction_leaf
v0.12.0b5,Build tree
v0.12.0b5,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.12.0b5,hold. Used by criterion for memory space savings.
v0.12.0b5,Initialize the criterion object and the criterion_val object if honest.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,
v0.12.0b5,This code is a fork from:
v0.12.0b5,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.12.0b5,published under the following license and copyright:
v0.12.0b5,BSD 3-Clause License
v0.12.0b5,
v0.12.0b5,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b5,All rights reserved.
v0.12.0b5,Set parameters
v0.12.0b5,Don't instantiate estimators now! Parameters of base_estimator might
v0.12.0b5,"still change. Eg., when grid-searching with the nested object syntax."
v0.12.0b5,self.estimators_ needs to be filled by the derived classes in fit.
v0.12.0b5,Compute the number of jobs
v0.12.0b5,Partition estimators between jobs
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Remove children with nonwhite mothers from the treatment group
v0.12.0b5,Remove children with nonwhite mothers from the treatment group
v0.12.0b5,Select columns
v0.12.0b5,Scale the numeric variables
v0.12.0b5,"Change the binary variable 'first' takes values in {1,2}"
v0.12.0b5,Append a column of ones as intercept
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b5,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b5,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.12.0b5,d_t=None here since we measure the effect across all Ts
v0.12.0b5,once the estimator has been fit
v0.12.0b5,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.12.0b5,an intercept even when bias is part of coef.
v0.12.0b5,We can write effect inference as a function of prediction and prediction standard error of
v0.12.0b5,the final method for linear models
v0.12.0b5,squeeze the first axis
v0.12.0b5,d_t=None here since we measure the effect across all Ts
v0.12.0b5,set the mean_pred_stderr
v0.12.0b5,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b5,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b5,"send treatment to the end, pull bounds to the front"
v0.12.0b5,d_t=None here since we measure the effect across all Ts
v0.12.0b5,set the mean_pred_stderr
v0.12.0b5,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.12.0b5,d_t=None here since we measure the effect across all Ts
v0.12.0b5,d_t=None here since we measure the effect across all Ts
v0.12.0b5,need to set the fit args before the estimator is fit
v0.12.0b5,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b5,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.12.0b5,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.12.0b5,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.12.0b5,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.12.0b5,scale preds
v0.12.0b5,scale std errs
v0.12.0b5,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.12.0b5,offset preds
v0.12.0b5,"offset the distribution, too"
v0.12.0b5,scale preds
v0.12.0b5,"scale the distribution, too"
v0.12.0b5,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b5,1. Uncertainty of Mean Point Estimate
v0.12.0b5,2. Distribution of Point Estimate
v0.12.0b5,3. Total Variance of Point Estimate
v0.12.0b5,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.12.0b5,so bail out early; note that it might be possible to correct the algorithm for
v0.12.0b5,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.12.0b5,be clean
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,TODO: Add a __dir__ implementation?
v0.12.0b5,don't proxy special methods
v0.12.0b5,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.12.0b5,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.12.0b5,then we should be computing a confidence interval for the wrapped calls
v0.12.0b5,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.12.0b5,second level bootstrap which would be prohibitive computationally?
v0.12.0b5,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.12.0b5,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.12.0b5,can't import from econml.inference at top level without creating cyclical dependencies
v0.12.0b5,Note that inference results are always methods even if the inference is for a property
v0.12.0b5,(e.g. coef__inference() is a method but coef_ is a property)
v0.12.0b5,Therefore we must insert a lambda if getting inference for a non-callable
v0.12.0b5,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.12.0b5,"try to get interval/std first if appropriate,"
v0.12.0b5,since we don't prefer a wrapped method with this name
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,
v0.12.0b5,This code contains snippets of code from:
v0.12.0b5,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b5,published under the following license and copyright:
v0.12.0b5,BSD 3-Clause License
v0.12.0b5,
v0.12.0b5,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b5,All rights reserved.
v0.12.0b5,=============================================================================
v0.12.0b5,Types and constants
v0.12.0b5,=============================================================================
v0.12.0b5,=============================================================================
v0.12.0b5,Base GRF tree
v0.12.0b5,=============================================================================
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,=============================================================================
v0.12.0b5,A MultOutputWrapper for GRF classes
v0.12.0b5,=============================================================================
v0.12.0b5,=============================================================================
v0.12.0b5,Instantiations of Generalized Random Forest
v0.12.0b5,=============================================================================
v0.12.0b5,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.12.0b5,in front of the constant treatment is the intercept in the moment equation.
v0.12.0b5,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.12.0b5,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,
v0.12.0b5,This code contains snippets of code from
v0.12.0b5,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b5,published under the following license and copyright:
v0.12.0b5,BSD 3-Clause License
v0.12.0b5,
v0.12.0b5,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b5,All rights reserved.
v0.12.0b5,=============================================================================
v0.12.0b5,Base Generalized Random Forest
v0.12.0b5,=============================================================================
v0.12.0b5,TODO: support freq_weight and sample_var
v0.12.0b5,Remap output
v0.12.0b5,reshape is necessary to preserve the data contiguity against vs
v0.12.0b5,"[:, np.newaxis] that does not."
v0.12.0b5,reshape is necessary to preserve the data contiguity against vs
v0.12.0b5,"[:, np.newaxis] that does not."
v0.12.0b5,Get subsample sample size
v0.12.0b5,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.12.0b5,We calculate the min eigenvalue proxy that each criterion is considering
v0.12.0b5,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.12.0b5,Check parameters
v0.12.0b5,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b5,if this is the first `fit` call of the warm start mode.
v0.12.0b5,"Free allocated memory, if any"
v0.12.0b5,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b5,We draw from the random state to get the random state we
v0.12.0b5,would have got if we hadn't used a warm_start.
v0.12.0b5,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b5,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b5,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b5,Generating indices a priori before parallelism ended up being orders of magnitude
v0.12.0b5,faster than how sklearn does it. The reason is that random samplers do not release the
v0.12.0b5,gil it seems.
v0.12.0b5,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b5,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b5,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b5,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b5,for fitting the trees is internally releasing the Python GIL
v0.12.0b5,making threading more efficient than multiprocessing in
v0.12.0b5,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b5,"parallel_backend contexts set at a higher level,"
v0.12.0b5,since correctness does not rely on using threads.
v0.12.0b5,Collect newly grown trees
v0.12.0b5,Check data
v0.12.0b5,Assign chunk of trees to jobs
v0.12.0b5,avoid storing the output of every estimator by summing them here
v0.12.0b5,Parallel loop
v0.12.0b5,Check data
v0.12.0b5,Assign chunk of trees to jobs
v0.12.0b5,Parallel loop
v0.12.0b5,Check data
v0.12.0b5,Assign chunk of trees to jobs
v0.12.0b5,Parallel loop
v0.12.0b5,####################
v0.12.0b5,Variance correction
v0.12.0b5,####################
v0.12.0b5,Subtract the average within bag variance. This ends up being equal to the
v0.12.0b5,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.12.0b5,The negative part is just sq_between.
v0.12.0b5,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.12.0b5,"The off diagonals we have no objective prior, so no correction is applied."
v0.12.0b5,Finally correcting the pred_cov or pred_var
v0.12.0b5,avoid storing the output of every estimator by summing them here
v0.12.0b5,Parallel loop
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,testing importances
v0.12.0b5,testing heterogeneity importances
v0.12.0b5,Testing that all parameters do what they are supposed to
v0.12.0b5,"testing predict, apply and decision path"
v0.12.0b5,test that the subsampling scheme past to the trees is correct
v0.12.0b5,The sample size is chosen in particular to test rounding based error when subsampling
v0.12.0b5,test that the estimator calcualtes var correctly
v0.12.0b5,test api
v0.12.0b5,test accuracy
v0.12.0b5,test the projection functionality of forests
v0.12.0b5,test that the estimator calcualtes var correctly
v0.12.0b5,test api
v0.12.0b5,test that the estimator calcualtes var correctly
v0.12.0b5,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b5,test that we raise errors in mishandled situations.
v0.12.0b5,test that the subsampling scheme past to the trees is correct
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,omit the lalonde notebook
v0.12.0b5,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.12.0b5,creating notebooks that are annoying for our users to actually run themselves
v0.12.0b5,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b5,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.12.0b5,"prior to calling interpret, can't plot, render, etc."
v0.12.0b5,can interpret without uncertainty
v0.12.0b5,can't interpret with uncertainty if inference wasn't used during fit
v0.12.0b5,can interpret with uncertainty if we refit
v0.12.0b5,can interpret without uncertainty
v0.12.0b5,can't treat before interpreting
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,simple DGP only for illustration
v0.12.0b5,Define the treatment model neural network architecture
v0.12.0b5,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b5,"so the input shape is (d_z + d_x,)"
v0.12.0b5,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b5,add extra layers on top for the mixture density network
v0.12.0b5,Define the response model neural network architecture
v0.12.0b5,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b5,"so the input shape is (d_t + d_x,)"
v0.12.0b5,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b5,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b5,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b5,so that the same weights will be reused in each instantiation
v0.12.0b5,number of samples to use in second estimate of the response
v0.12.0b5,(to make loss estimate unbiased)
v0.12.0b5,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b5,do something with predictions...
v0.12.0b5,also test vector t and y
v0.12.0b5,simple DGP only for illustration
v0.12.0b5,Define the treatment model neural network architecture
v0.12.0b5,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b5,"so the input shape is (d_z + d_x,)"
v0.12.0b5,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b5,add extra layers on top for the mixture density network
v0.12.0b5,Define the response model neural network architecture
v0.12.0b5,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b5,"so the input shape is (d_t + d_x,)"
v0.12.0b5,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b5,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b5,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b5,so that the same weights will be reused in each instantiation
v0.12.0b5,number of samples to use in second estimate of the response
v0.12.0b5,(to make loss estimate unbiased)
v0.12.0b5,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b5,do something with predictions...
v0.12.0b5,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.12.0b5,test = True ensures we draw test set images
v0.12.0b5,test = True ensures we draw test set images
v0.12.0b5,re-draw to get new independent treatment and implied response
v0.12.0b5,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b5,above is necesary so that reduced form doesn't win
v0.12.0b5,covariates: time and emotion
v0.12.0b5,random instrument
v0.12.0b5,z -> price
v0.12.0b5,true observable demand function
v0.12.0b5,errors
v0.12.0b5,response
v0.12.0b5,test = True ensures we draw test set images
v0.12.0b5,test = True ensures we draw test set images
v0.12.0b5,re-draw to get new independent treatment and implied response
v0.12.0b5,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b5,above is necesary so that reduced form doesn't win
v0.12.0b5,covariates: time and emotion
v0.12.0b5,random instrument
v0.12.0b5,z -> price
v0.12.0b5,true observable demand function
v0.12.0b5,errors
v0.12.0b5,response
v0.12.0b5,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.12.0b5,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.12.0b5,For some reason this doesn't work at all when run against the CNTK backend...
v0.12.0b5,"model.compile('nadam', loss=lambda _,l:l)"
v0.12.0b5,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.12.0b5,generate a valiation set
v0.12.0b5,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.12.0b5,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,DGP constants
v0.12.0b5,Generate data
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,testing importances
v0.12.0b5,testing heterogeneity importances
v0.12.0b5,Testing that all parameters do what they are supposed to
v0.12.0b5,"testing predict, apply and decision path"
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.12.0b5,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.12.0b5,so we need to transpose the result
v0.12.0b5,1-d output
v0.12.0b5,2-d output
v0.12.0b5,Single dimensional output y
v0.12.0b5,compare with weight
v0.12.0b5,compare with weight
v0.12.0b5,compare with weight
v0.12.0b5,compare with weight
v0.12.0b5,Multi-dimensional output y
v0.12.0b5,1-d y
v0.12.0b5,compare when both sample_var and sample_weight exist
v0.12.0b5,multi-d y
v0.12.0b5,compare when both sample_var and sample_weight exist
v0.12.0b5,compare when both sample_var and sample_weight exist
v0.12.0b5,compare when both sample_var and sample_weight exist
v0.12.0b5,compare when both sample_var and sample_weight exist
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,test that we can fit with the same arguments as the base estimator
v0.12.0b5,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b5,with the same shape for the lower and upper bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,test that we can do the same thing once we provide percentile bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b5,with the same shape for the lower and upper bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,test that we can do the same thing once we provide percentile bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,test that we can fit with the same arguments as the base estimator
v0.12.0b5,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b5,with the same shape for the lower and upper bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,test that we can do the same thing once we provide percentile bounds
v0.12.0b5,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b5,with the same shape for the lower and upper bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,test that we can do the same thing once we provide percentile bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,test that we can fit with the same arguments as the base estimator
v0.12.0b5,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b5,with the same shape for the lower and upper bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,test that we can do the same thing once we provide percentile bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b5,with the same shape for the lower and upper bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,test that we can do the same thing once we provide percentile bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b5,with the same shape for the lower and upper bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,test that the estimated effect is usually within the bounds
v0.12.0b5,test that we can do the same thing once we provide alpha explicitly
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,test that the estimated effect is usually within the bounds
v0.12.0b5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b5,with the same shape for the lower and upper bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,TODO: test that the estimated effect is usually within the bounds
v0.12.0b5,and that the true effect is also usually within the bounds
v0.12.0b5,test that we can do the same thing once we provide percentile bounds
v0.12.0b5,test that the lower and upper bounds differ
v0.12.0b5,TODO: test that the estimated effect is usually within the bounds
v0.12.0b5,and that the true effect is also usually within the bounds
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,test that the subsampling scheme past to the trees is correct
v0.12.0b5,test that the estimator calcualtes var correctly
v0.12.0b5,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b5,test that we raise errors in mishandled situations.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,DGP constants
v0.12.0b5,Generate data
v0.12.0b5,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b5,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b5,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.12.0b5,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b5,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.12.0b5,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b5,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b5,pvalue for second column should be greater than zero since some points are on either side
v0.12.0b5,of the tested value
v0.12.0b5,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.12.0b5,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b5,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b5,only is not None when T1 is a constant or a list of constant
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.12.0b5,Test non keyword based calls to fit
v0.12.0b5,test non-array inputs
v0.12.0b5,Test custom splitter
v0.12.0b5,Test incomplete set of test folds
v0.12.0b5,"y scores should be positive, since W predicts Y somewhat"
v0.12.0b5,"t scores might not be, since W and T are uncorrelated"
v0.12.0b5,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,make sure cross product varies more slowly with first array
v0.12.0b5,and that vectors are okay as inputs
v0.12.0b5,number of inputs in specification must match number of inputs
v0.12.0b5,must have an output
v0.12.0b5,output indices must be unique
v0.12.0b5,output indices must be present in an input
v0.12.0b5,number of indices must match number of dimensions for each input
v0.12.0b5,repeated indices must always have consistent sizes
v0.12.0b5,transpose
v0.12.0b5,tensordot
v0.12.0b5,trace
v0.12.0b5,TODO: set up proper flag for this
v0.12.0b5,pick indices at random with replacement from the first 7 letters of the alphabet
v0.12.0b5,"of all of the distinct indices that appear in any input,"
v0.12.0b5,pick a random subset of them (of size at most 5) to appear in the output
v0.12.0b5,creating an instance should warn
v0.12.0b5,using the instance should not warn
v0.12.0b5,using the deprecated method should warn
v0.12.0b5,don't warn if b and c are passed by keyword
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Preprocess data
v0.12.0b5,Convert 'week' to a date
v0.12.0b5,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.12.0b5,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.12.0b5,Take log of price
v0.12.0b5,Make brand numeric
v0.12.0b5,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.12.0b5,which have all zero coeffs
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,test at least one estimator from each category
v0.12.0b5,test causal graph
v0.12.0b5,test refutation estimate
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.12.0b5,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.12.0b5,TODO: test something rather than just print...
v0.12.0b5,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.12.0b5,pick some arbitrary X
v0.12.0b5,pick some arbitrary T
v0.12.0b5,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b5,The average variance should be lower when using monte carlo iterations
v0.12.0b5,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b5,The average variance should be lower when using monte carlo iterations
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,ensure that we've got at least two of every row
v0.12.0b5,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b5,need to make sure we get all *joint* combinations
v0.12.0b5,IntentToTreat only supports binary treatments/instruments
v0.12.0b5,IntentToTreat only supports binary treatments/instruments
v0.12.0b5,IntentToTreat requires X
v0.12.0b5,ensure we can serialize unfit estimator
v0.12.0b5,these support only W but not X
v0.12.0b5,"these support only binary, not general discrete T and Z"
v0.12.0b5,ensure we can serialize fit estimator
v0.12.0b5,make sure we can call the marginal_effect and effect methods
v0.12.0b5,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b5,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b5,"make sure we can call effect with implied scalar treatments,"
v0.12.0b5,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b5,are multiple treatments
v0.12.0b5,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b5,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.12.0b5,"however, with custom splits the checking happens in the first stage wrapper"
v0.12.0b5,where we don't have all of the required information to do this;
v0.12.0b5,we'd probably need to add it to _crossfit instead
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.12.0b5,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.12.0b5,The __warningregistry__'s need to be in a pristine state for tests
v0.12.0b5,to work properly.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Set random seed
v0.12.0b5,Generate data
v0.12.0b5,DGP constants
v0.12.0b5,Test data
v0.12.0b5,Constant treatment effect
v0.12.0b5,Constant treatment with multi output Y
v0.12.0b5,Heterogeneous treatment
v0.12.0b5,Heterogeneous treatment with multi output Y
v0.12.0b5,TLearner test
v0.12.0b5,Instantiate TLearner
v0.12.0b5,Test inputs
v0.12.0b5,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b5,Instantiate SLearner
v0.12.0b5,Test inputs
v0.12.0b5,Test constant treatment effect
v0.12.0b5,Test constant treatment effect with multi output Y
v0.12.0b5,Test heterogeneous treatment effect
v0.12.0b5,Need interactions between T and features
v0.12.0b5,Test heterogeneous treatment effect with multi output Y
v0.12.0b5,Instantiate XLearner
v0.12.0b5,Test inputs
v0.12.0b5,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b5,Instantiate DomainAdaptationLearner
v0.12.0b5,Test inputs
v0.12.0b5,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b5,Get the true treatment effect
v0.12.0b5,Get the true treatment effect
v0.12.0b5,Fit learner and get the effect and marginal effect
v0.12.0b5,Compute treatment effect residuals (absolute)
v0.12.0b5,Check that at least 90% of predictions are within tolerance interval
v0.12.0b5,Check whether the output shape is right
v0.12.0b5,Check that one can pass in regular lists
v0.12.0b5,Check that it fails correctly if lists of different shape are passed in
v0.12.0b5,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b5,Generate covariates
v0.12.0b5,Generate treatment
v0.12.0b5,Calculate outcome
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,DGP constants
v0.12.0b5,Generate data
v0.12.0b5,Test data
v0.12.0b5,Remove warnings that might be raised by the models passed into the ORF
v0.12.0b5,Generate data with continuous treatments
v0.12.0b5,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.12.0b5,does not work well with parallelism.
v0.12.0b5,Test inputs for continuous treatments
v0.12.0b5,--> Check that one can pass in regular lists
v0.12.0b5,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b5,Check that outputs have the correct shape
v0.12.0b5,Test continuous treatments with controls
v0.12.0b5,Test continuous treatments without controls
v0.12.0b5,Generate data with binary treatments
v0.12.0b5,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.12.0b5,does not work well with parallelism.
v0.12.0b5,Test inputs for binary treatments
v0.12.0b5,--> Check that one can pass in regular lists
v0.12.0b5,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b5,"--> Check that it works when T, Y have shape (n, 1)"
v0.12.0b5,"--> Check that it fails correctly when T has shape (n, 2)"
v0.12.0b5,--> Check that it fails correctly when the treatments are not numeric
v0.12.0b5,Check that outputs have the correct shape
v0.12.0b5,Test binary treatments with controls
v0.12.0b5,Test binary treatments without controls
v0.12.0b5,Only applicable to continuous treatments
v0.12.0b5,Generate data for 2 treatments
v0.12.0b5,Test multiple treatments with controls
v0.12.0b5,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.12.0b5,The rest for controls. Just as an example.
v0.12.0b5,Generating A/B test data
v0.12.0b5,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.12.0b5,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.12.0b5,Create a wrapper around Lasso that doesn't support weights
v0.12.0b5,since Lasso does natively support them starting in sklearn 0.23
v0.12.0b5,Generate data with continuous treatments
v0.12.0b5,Instantiate model with most of the default parameters
v0.12.0b5,Compute the treatment effect on test points
v0.12.0b5,Compute treatment effect residuals
v0.12.0b5,Multiple treatments
v0.12.0b5,Allow at most 10% test points to be outside of the tolerance interval
v0.12.0b5,Compute treatment effect residuals
v0.12.0b5,Multiple treatments
v0.12.0b5,Allow at most 20% test points to be outside of the confidence interval
v0.12.0b5,Check that the intervals are not too wide
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.12.0b5,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.12.0b5,ensure that we've got at least 6 of every element
v0.12.0b5,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.12.0b5,NOTE: this number may need to change if the default number of folds in
v0.12.0b5,WeightedStratifiedKFold changes
v0.12.0b5,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b5,ensure we can serialize the unfit estimator
v0.12.0b5,ensure we can pickle the fit estimator
v0.12.0b5,make sure we can call the marginal_effect and effect methods
v0.12.0b5,test const marginal inference
v0.12.0b5,test effect inference
v0.12.0b5,test marginal effect inference
v0.12.0b5,test coef__inference and intercept__inference
v0.12.0b5,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b5,"make sure we can call effect with implied scalar treatments,"
v0.12.0b5,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b5,are multiple treatments
v0.12.0b5,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b5,ensure that we've got at least two of every element
v0.12.0b5,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b5,make sure we can call the marginal_effect and effect methods
v0.12.0b5,test const marginal inference
v0.12.0b5,test effect inference
v0.12.0b5,test marginal effect inference
v0.12.0b5,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b5,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b5,We concatenate the two copies data
v0.12.0b5,make sure we can get out post-fit stuff
v0.12.0b5,create a simple artificial setup where effect of moving from treatment
v0.12.0b5,"1 -> 2 is 2,"
v0.12.0b5,"1 -> 3 is 1, and"
v0.12.0b5,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b5,"Using an uneven number of examples from different classes,"
v0.12.0b5,"and having the treatments in non-lexicographic order,"
v0.12.0b5,Should rule out some basic issues.
v0.12.0b5,test that we can fit with a KFold instance
v0.12.0b5,test that we can fit with a train/test iterable
v0.12.0b5,predetermined splits ensure that all features are seen in each split
v0.12.0b5,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.12.0b5,(incorrectly) use a final model with an intercept
v0.12.0b5,"Because final model is fixed, actual values of T and Y don't matter"
v0.12.0b5,Ensure reproducibility
v0.12.0b5,Sparse DGP
v0.12.0b5,Treatment effect coef
v0.12.0b5,Other coefs
v0.12.0b5,Features and controls
v0.12.0b5,Test sparse estimator
v0.12.0b5,"--> test coef_, intercept_"
v0.12.0b5,--> test treatment effects
v0.12.0b5,Restrict x_test to vectors of norm < 1
v0.12.0b5,--> check inference
v0.12.0b5,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b5,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.12.0b5,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.12.0b5,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.12.0b5,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.12.0b5,sparse test case: heterogeneous effect by product
v0.12.0b5,need at least as many rows in e_y as there are distinct columns
v0.12.0b5,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.12.0b5,create a simple artificial setup where effect of moving from treatment
v0.12.0b5,"a -> b is 2,"
v0.12.0b5,"a -> c is 1, and"
v0.12.0b5,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.12.0b5,"Using an uneven number of examples from different classes,"
v0.12.0b5,"and having the treatments in non-lexicographic order,"
v0.12.0b5,should rule out some basic issues.
v0.12.0b5,Note that explicitly specifying the dtype as object is necessary until
v0.12.0b5,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.12.0b5,estimated effects should be identical when treatment is explicitly given
v0.12.0b5,but const_marginal_effect should be reordered based on the explicit cagetories
v0.12.0b5,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.12.0b5,test outer grouping
v0.12.0b5,test nested grouping
v0.12.0b5,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b5,whichever groups we saw
v0.12.0b5,test nested grouping
v0.12.0b5,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b5,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b5,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Set random seed
v0.12.0b5,Generate data
v0.12.0b5,DGP constants
v0.12.0b5,Test data
v0.12.0b5,Constant treatment effect and propensity
v0.12.0b5,Heterogeneous treatment and propensity
v0.12.0b5,ensure that we've got at least two of every element
v0.12.0b5,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b5,ensure that we can serialize unfit estimator
v0.12.0b5,ensure that we can serialize fit estimator
v0.12.0b5,make sure we can call the marginal_effect and effect methods
v0.12.0b5,test const marginal inference
v0.12.0b5,test effect inference
v0.12.0b5,test marginal effect inference
v0.12.0b5,test coef_ and intercept_ inference
v0.12.0b5,verify we can generate the summary
v0.12.0b5,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b5,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b5,create a simple artificial setup where effect of moving from treatment
v0.12.0b5,"1 -> 2 is 2,"
v0.12.0b5,"1 -> 3 is 1, and"
v0.12.0b5,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b5,"Using an uneven number of examples from different classes,"
v0.12.0b5,"and having the treatments in non-lexicographic order,"
v0.12.0b5,Should rule out some basic issues.
v0.12.0b5,test that we can fit with a KFold instance
v0.12.0b5,test that we can fit with a train/test iterable
v0.12.0b5,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b5,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b5,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b5,test coef__inference function works
v0.12.0b5,test intercept__inference function works
v0.12.0b5,test summary function works
v0.12.0b5,Test inputs
v0.12.0b5,self._test_inputs(DR_learner)
v0.12.0b5,Test constant treatment effect
v0.12.0b5,Test heterogeneous treatment effect
v0.12.0b5,Test heterogenous treatment effect for W =/= None
v0.12.0b5,Sparse DGP
v0.12.0b5,Treatment effect coef
v0.12.0b5,Other coefs
v0.12.0b5,Features and controls
v0.12.0b5,Test sparse estimator
v0.12.0b5,"--> test coef_, intercept_"
v0.12.0b5,--> test treatment effects
v0.12.0b5,Restrict x_test to vectors of norm < 1
v0.12.0b5,--> check inference
v0.12.0b5,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b5,test outer grouping
v0.12.0b5,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.12.0b5,test nested grouping
v0.12.0b5,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b5,whichever groups we saw
v0.12.0b5,test nested grouping
v0.12.0b5,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b5,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b5,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b5,helper class
v0.12.0b5,Fit learner and get the effect
v0.12.0b5,Get the true treatment effect
v0.12.0b5,Compute treatment effect residuals (absolute)
v0.12.0b5,Check that at least 90% of predictions are within tolerance interval
v0.12.0b5,Only for heterogeneous TE
v0.12.0b5,Fit learner on X and W and get the effect
v0.12.0b5,Get the true treatment effect
v0.12.0b5,Compute treatment effect residuals (absolute)
v0.12.0b5,Check that at least 90% of predictions are within tolerance interval
v0.12.0b5,Check that one can pass in regular lists
v0.12.0b5,Check that it fails correctly if lists of different shape are passed in
v0.12.0b5,Check that it fails when T contains values other than 0 and 1
v0.12.0b5,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b5,Generate covariates
v0.12.0b5,Generate treatment
v0.12.0b5,Calculate outcome
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,DGP constants
v0.12.0b5,DGP coefficients
v0.12.0b5,Generated outcomes
v0.12.0b5,################
v0.12.0b5,WeightedLasso #
v0.12.0b5,################
v0.12.0b5,Define weights
v0.12.0b5,Define extended datasets
v0.12.0b5,Range of alphas
v0.12.0b5,Compare with Lasso
v0.12.0b5,--> No intercept
v0.12.0b5,--> With intercept
v0.12.0b5,When DGP has no intercept
v0.12.0b5,When DGP has intercept
v0.12.0b5,--> Coerce coefficients to be positive
v0.12.0b5,--> Toggle max_iter & tol
v0.12.0b5,Define weights
v0.12.0b5,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b5,Mixed DGP scenario.
v0.12.0b5,Define extended datasets
v0.12.0b5,Define weights
v0.12.0b5,Define multioutput
v0.12.0b5,##################
v0.12.0b5,WeightedLassoCV #
v0.12.0b5,##################
v0.12.0b5,Define alphas to test
v0.12.0b5,Compare with LassoCV
v0.12.0b5,--> No intercept
v0.12.0b5,--> With intercept
v0.12.0b5,--> Force parameters to be positive
v0.12.0b5,Choose a smaller n to speed-up process
v0.12.0b5,Compare fold weights
v0.12.0b5,Define weights
v0.12.0b5,Define extended datasets
v0.12.0b5,Define splitters
v0.12.0b5,WeightedKFold splitter
v0.12.0b5,Map weighted splitter to an extended splitter
v0.12.0b5,Define alphas to test
v0.12.0b5,Compare with LassoCV
v0.12.0b5,--> No intercept
v0.12.0b5,--> With intercept
v0.12.0b5,--> Force parameters to be positive
v0.12.0b5,###########################
v0.12.0b5,MultiTaskWeightedLassoCV #
v0.12.0b5,###########################
v0.12.0b5,Define alphas to test
v0.12.0b5,Define splitter
v0.12.0b5,Compare with MultiTaskLassoCV
v0.12.0b5,--> No intercept
v0.12.0b5,--> With intercept
v0.12.0b5,Define weights
v0.12.0b5,Define extended datasets
v0.12.0b5,Define splitters
v0.12.0b5,WeightedKFold splitter
v0.12.0b5,Map weighted splitter to an extended splitter
v0.12.0b5,Define alphas to test
v0.12.0b5,Compare with LassoCV
v0.12.0b5,--> No intercept
v0.12.0b5,--> With intercept
v0.12.0b5,#########################
v0.12.0b5,WeightedLassoCVWrapper #
v0.12.0b5,#########################
v0.12.0b5,perform 1D fit
v0.12.0b5,perform 2D fit
v0.12.0b5,################
v0.12.0b5,DebiasedLasso #
v0.12.0b5,################
v0.12.0b5,Test DebiasedLasso without weights
v0.12.0b5,--> Check debiased coeffcients without intercept
v0.12.0b5,--> Check debiased coeffcients with intercept
v0.12.0b5,--> Check 5-95 CI coverage for unit vectors
v0.12.0b5,Test DebiasedLasso with weights for one DGP
v0.12.0b5,Define weights
v0.12.0b5,Define extended datasets
v0.12.0b5,--> Check debiased coefficients
v0.12.0b5,Define weights
v0.12.0b5,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b5,--> Check debiased coeffcients
v0.12.0b5,Test that attributes propagate correctly
v0.12.0b5,Test MultiOutputDebiasedLasso without weights
v0.12.0b5,--> Check debiased coeffcients without intercept
v0.12.0b5,--> Check debiased coeffcients with intercept
v0.12.0b5,--> Check CI coverage
v0.12.0b5,Test MultiOutputDebiasedLasso with weights
v0.12.0b5,Define weights
v0.12.0b5,Define extended datasets
v0.12.0b5,--> Check debiased coefficients
v0.12.0b5,Unit vectors
v0.12.0b5,Unit vectors
v0.12.0b5,Check coeffcients and intercept are the same within tolerance
v0.12.0b5,Check results are similar with tolerance 1e-6
v0.12.0b5,Check if multitask
v0.12.0b5,Check that same alpha is chosen
v0.12.0b5,Check that the coefficients are similar
v0.12.0b5,selective ridge has a simple implementation that we can test against
v0.12.0b5,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.12.0b5,"it should be the case that when we set fit_intercept to true,"
v0.12.0b5,it doesn't matter whether the penalized model also fits an intercept or not
v0.12.0b5,create an extra copy of rows with weight 2
v0.12.0b5,"instead of a slice, explicitly return an array of indices"
v0.12.0b5,_penalized_inds is only set during fitting
v0.12.0b5,cv exists on penalized model
v0.12.0b5,now we can access _penalized_inds
v0.12.0b5,check that we can read the cv attribute back out from the underlying model
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b5,local index should have as many times entries as global as there were rows passed in
v0.12.0b5,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b5,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b5,policy value should exceed always treating with any treatment
v0.12.0b5,"global shape is (d_y, sum(d_t))"
v0.12.0b5,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b5,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b5,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b5,features; for categoricals they should appear #cats-1 times each
v0.12.0b5,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b5,local index should have as many times entries as global as there were rows passed in
v0.12.0b5,features; for categoricals they should appear #cats-1 times each
v0.12.0b5,"global shape is (d_y, sum(d_t))"
v0.12.0b5,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b5,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b5,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b5,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b5,policy value should exceed always treating with any treatment
v0.12.0b5,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b5,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b5,local index should have as many times entries as global as there were rows passed in
v0.12.0b5,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b5,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b5,policy value should exceed always treating with any treatment
v0.12.0b5,"global shape is (d_y, sum(d_t))"
v0.12.0b5,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b5,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b5,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b5,features; for categoricals they should appear #cats-1 times each
v0.12.0b5,make sure we don't run into problems dropping every index
v0.12.0b5,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b5,local index should have as many times entries as global as there were rows passed in
v0.12.0b5,"global shape is (d_y, sum(d_t))"
v0.12.0b5,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b5,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b5,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b5,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b5,policy value should exceed always treating with any treatment
v0.12.0b5,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b5,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b5,local index should have as many times entries as global as there were rows passed in
v0.12.0b5,features; for categoricals they should appear #cats-1 times each
v0.12.0b5,"global shape is (d_y, sum(d_t))"
v0.12.0b5,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b5,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b5,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b5,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b5,policy value should exceed always treating with any treatment
v0.12.0b5,dgp
v0.12.0b5,model
v0.12.0b5,model
v0.12.0b5,"columns 'd', 'e', 'h' have too many values"
v0.12.0b5,"columns 'd', 'e' have too many values"
v0.12.0b5,lowering bound shouldn't affect already fit columns when warm starting
v0.12.0b5,"column d is now okay, too"
v0.12.0b5,verify that we can use a scalar treatment cost
v0.12.0b5,verify that we can specify per-treatment costs for each sample
v0.12.0b5,verify that using the same state returns the same results each time
v0.12.0b5,set the categories for column 'd' explicitly so that b is default
v0.12.0b5,"first column: 10 ones, this is fine"
v0.12.0b5,"second column: 6 categories, plenty of random instances of each"
v0.12.0b5,this is fine only if we increase the cateogry limit
v0.12.0b5,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.12.0b5,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.12.0b5,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.12.0b5,forest heterogeneity won't work
v0.12.0b5,"sixth column: just 1 one, not enough even without check"
v0.12.0b5,increase bound on cat expansion
v0.12.0b5,skip checks (reducing folds accordingly)
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,DGP constants
v0.12.0b5,Define data features
v0.12.0b5,Added `_df`to names to be different from the default cate_estimator names
v0.12.0b5,Generate data
v0.12.0b5,################################
v0.12.0b5,Single treatment and outcome #
v0.12.0b5,################################
v0.12.0b5,Test LinearDML
v0.12.0b5,|--> Test featurizers
v0.12.0b5,ColumnTransformer doesn't propagate column names
v0.12.0b5,|--> Test re-fit
v0.12.0b5,Test SparseLinearDML
v0.12.0b5,Test ForestDML
v0.12.0b5,###################################
v0.12.0b5,Mutiple treatments and outcomes #
v0.12.0b5,###################################
v0.12.0b5,Test LinearDML
v0.12.0b5,Test SparseLinearDML
v0.12.0b5,"Single outcome only, ORF does not support multiple outcomes"
v0.12.0b5,Test DMLOrthoForest
v0.12.0b5,Test DROrthoForest
v0.12.0b5,Test XLearner
v0.12.0b5,Skipping population summary names test because bootstrap inference is too slow
v0.12.0b5,Test SLearner
v0.12.0b5,Test TLearner
v0.12.0b5,Test LinearDRLearner
v0.12.0b5,Test SparseLinearDRLearner
v0.12.0b5,Test ForestDRLearner
v0.12.0b5,Test LinearIntentToTreatDRIV
v0.12.0b5,Test DeepIV
v0.12.0b5,Test categorical treatments
v0.12.0b5,Check refit
v0.12.0b5,Check refit after setting categories
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Linear models are required for parametric dml
v0.12.0b5,sample weighting models are required for nonparametric dml
v0.12.0b5,Test values
v0.12.0b5,TLearner test
v0.12.0b5,Instantiate TLearner
v0.12.0b5,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b5,Test constant treatment effect with multi output Y
v0.12.0b5,Test heterogeneous treatment effect
v0.12.0b5,Need interactions between T and features
v0.12.0b5,Test heterogeneous treatment effect with multi output Y
v0.12.0b5,Instantiate DomainAdaptationLearner
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,test base values equals to mean of constant marginal effect
v0.12.0b5,test shape of shap values output is as expected
v0.12.0b5,test shape of attribute of explanation object is as expected
v0.12.0b5,test length of feature names equals to shap values shape
v0.12.0b5,test base values equals to mean of constant marginal effect
v0.12.0b5,test shape of shap values output is as expected
v0.12.0b5,test shape of attribute of explanation object is as expected
v0.12.0b5,test length of feature names equals to shap values shape
v0.12.0b5,Treatment effect function
v0.12.0b5,Outcome support
v0.12.0b5,Treatment support
v0.12.0b5,"Generate controls, covariates, treatments and outcomes"
v0.12.0b5,Heterogeneous treatment effects
v0.12.0b5,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.12.0b5,through shap package.
v0.12.0b5,test shap could generate the plot from the shap_values
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Check inputs
v0.12.0b5,Check inputs
v0.12.0b5,Check inputs
v0.12.0b5,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.12.0b5,"Thus, we append the controls column before the one-hot-encoded T"
v0.12.0b5,"We might want to revisit, though, since it's linearly determined by the others"
v0.12.0b5,Check inputs
v0.12.0b5,Check inputs
v0.12.0b5,Estimate response function
v0.12.0b5,Check inputs
v0.12.0b5,Train model on controls. Assign higher weight to units resembling
v0.12.0b5,treated units.
v0.12.0b5,Train model on the treated. Assign higher weight to units resembling
v0.12.0b5,control units.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.12.0b5,output is
v0.12.0b5,"* a column of ones if X, W, and Z are all None"
v0.12.0b5,* just X or W or Z if both of the others are None
v0.12.0b5,* hstack([arrs]) for whatever subset are not None otherwise
v0.12.0b5,ensure Z is 2D
v0.12.0b5,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b5,We need to go back to the label representation of the one-hot so as to call
v0.12.0b5,the classifier.
v0.12.0b5,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b5,We need to go back to the label representation of the one-hot so as to call
v0.12.0b5,the classifier.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,TODO: make sure to use random seeds wherever necessary
v0.12.0b5,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.12.0b5,"unfortunately with the Theano and Tensorflow backends,"
v0.12.0b5,the straightforward use of K.stop_gradient can cause an error
v0.12.0b5,because the parameters of the intermediate layers are now disconnected from the loss;
v0.12.0b5,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.12.0b5,so that those layers remain connected but with 0 gradient
v0.12.0b5,|| t - mu_i || ^2
v0.12.0b5,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.12.0b5,Use logsumexp for numeric stability:
v0.12.0b5,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.12.0b5,TODO: does the numeric stability actually make any difference?
v0.12.0b5,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.12.0b5,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.12.0b5,generate cumulative sum via matrix multiplication
v0.12.0b5,"Generate standard uniform values in shape (batch_size,1)"
v0.12.0b5,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.12.0b5,we use uniform_like instead with an input of an appropriate shape)
v0.12.0b5,convert to floats and multiply to perform equivalent of logical AND
v0.12.0b5,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.12.0b5,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.12.0b5,we use normal_like instead with an input of an appropriate shape)
v0.12.0b5,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.12.0b5,prevent gradient from passing through sampling
v0.12.0b5,three options: biased or upper-bound loss require a single number of samples;
v0.12.0b5,unbiased can take different numbers for the network and its gradient
v0.12.0b5,"sample: (() -> Layer, int) -> Layer"
v0.12.0b5,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.12.0b5,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.12.0b5,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.12.0b5,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.12.0b5,the dimensionality of the output of the network
v0.12.0b5,TODO: is there a more robust way to do this?
v0.12.0b5,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b5,"subtle point: we need to build a new model each time,"
v0.12.0b5,because each model encapsulates its randomness
v0.12.0b5,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b5,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.12.0b5,not a general tensor (because of how backprop works in every framework)
v0.12.0b5,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.12.0b5,but this seems annoying...)
v0.12.0b5,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.12.0b5,TODO: any way to get this to work on batches of arbitrary size?
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b5,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b5,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b5,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b5,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b5,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.12.0b5,"instruments, and outcomes"
v0.12.0b5,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b5,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b5,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b5,for internal use by the library
v0.12.0b5,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b5,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b5,since it expects raw values
v0.12.0b5,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b5,since it expects raw values
v0.12.0b5,"TODO: check that Y, T, Z do not have multiple columns"
v0.12.0b5,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b5,TODO: do correct adjustment for sample_var
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.12.0b5,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.12.0b5,since we would actually be calculating a CATE rather than ATE.
v0.12.0b5,TODO: allow the final model to actually use X?
v0.12.0b5,TODO: allow the final model to actually use X?
v0.12.0b5,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b5,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.12.0b5,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.12.0b5,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b5,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b5,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b5,for internal use by the library
v0.12.0b5,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,"this will have dimension (d,) + shape(X)"
v0.12.0b5,send the first dimension to the end
v0.12.0b5,columns are featurized independently; partial derivatives are only non-zero
v0.12.0b5,when taken with respect to the same column each time
v0.12.0b5,don't fit intercept; manually add column of ones to the data instead;
v0.12.0b5,this allows us to ignore the intercept when computing marginal effects
v0.12.0b5,make T 2D if if was a vector
v0.12.0b5,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.12.0b5,two stage approximation
v0.12.0b5,"first, get basis expansions of T, X, and Z"
v0.12.0b5,TODO: is it right that the effective number of intruments is the
v0.12.0b5,"product of ft_X and ft_Z, not just ft_Z?"
v0.12.0b5,"regress T expansion on X,Z expansions concatenated with W"
v0.12.0b5,"predict ft_T from interacted ft_X, ft_Z"
v0.12.0b5,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.12.0b5,dT may be only 2-dimensional)
v0.12.0b5,promote dT to 3D if necessary (e.g. if T was a vector)
v0.12.0b5,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,TODO: this utility is documented but internal; reimplement?
v0.12.0b5,TODO: this utility is even less public...
v0.12.0b5,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.12.0b5,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.12.0b5,but also supports get_feature_names with expected signature
v0.12.0b5,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.12.0b5,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.12.0b5,Convert SingleTreeInterpreter to a python dictionary
v0.12.0b5,named tuple type for storing results inside CausalAnalysis class;
v0.12.0b5,must be lifted to module level to enable pickling
v0.12.0b5,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.12.0b5,Controls are all other columns of X
v0.12.0b5,"can't use X[:, feat_ind] when X is a DataFrame"
v0.12.0b5,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.12.0b5,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.12.0b5,so that the user can opt-in to allowing unseen treatment values
v0.12.0b5,(and return NaN or something in that case)
v0.12.0b5,array checking routines don't accept 0-width arrays
v0.12.0b5,perform model selection
v0.12.0b5,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.12.0b5,convert to NormalInferenceResults for consistency
v0.12.0b5,Set the dictionary values shared between local and global summaries
v0.12.0b5,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.12.0b5,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.12.0b5,required to fit a discrete DML model
v0.12.0b5,Validate inputs
v0.12.0b5,TODO: check compatibility of X and Y lengths
v0.12.0b5,"no previous fit, cancel warm start"
v0.12.0b5,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.12.0b5,"if heterogeneity_inds is 1D, repeat it"
v0.12.0b5,heterogeneity inds should be a 2D list of length same as train_inds
v0.12.0b5,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.12.0b5,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.12.0b5,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.12.0b5,train the Y model
v0.12.0b5,"perform model selection for the Y model using all X, not on a per-column basis"
v0.12.0b5,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.12.0b5,work with the regression wrapper
v0.12.0b5,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.12.0b5,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.12.0b5,since otherwise we'll have too many columns to be able to train a classifier
v0.12.0b5,start with empty results and default shared insights
v0.12.0b5,convert categorical indicators to numeric indices
v0.12.0b5,check for indices over the categorical expansion bound
v0.12.0b5,assume we'll be able to train former failures this time; we'll add them back if not
v0.12.0b5,"can't remove in place while iterating over new_inds, so store in separate list"
v0.12.0b5,"train the model, but warn"
v0.12.0b5,no model can be trained in this case since we need more folds
v0.12.0b5,"don't train a model, but suggest workaround since there are enough instances of least"
v0.12.0b5,populated class
v0.12.0b5,also remove from train_inds so we don't try to access the result later
v0.12.0b5,extract subset of names matching new columns
v0.12.0b5,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.12.0b5,don't want to cache this failed result
v0.12.0b5,properties to return from effect InferenceResults
v0.12.0b5,properties to return from PopulationSummaryResults
v0.12.0b5,Converts strings to property lookups or method calls as a convenience so that the
v0.12.0b5,_point_props and _summary_props above can be applied to an inference object
v0.12.0b5,Create a summary combining all results into a single output; this is used
v0.12.0b5,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.12.0b5,"or a dictionary, respectively, based on the summary function passed into this method"
v0.12.0b5,"ensure array has shape (m,y,t)"
v0.12.0b5,population summary is missing sample dimension; add it for consistency
v0.12.0b5,outcome dimension is missing; add it for consistency
v0.12.0b5,add singleton treatment dimension if missing
v0.12.0b5,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.12.0b5,"each attr has dimension (m,y) or (m,y,t)"
v0.12.0b5,concatenate along treatment dimension
v0.12.0b5,"for dictionary representation, want to remove unneeded sample dimension"
v0.12.0b5,in cohort and global results
v0.12.0b5,TODO: enrich outcome logic for multi-class classification when that is supported
v0.12.0b5,There is no actual sample level in this data
v0.12.0b5,can't drop only level
v0.12.0b5,should be serialization-ready and contain no numpy arrays
v0.12.0b5,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.12.0b5,TODO: Note that there's no column metadata for the sample number - should there be?
v0.12.0b5,"need to replicate the column info for each sample, then remove from the shared data"
v0.12.0b5,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.12.0b5,which may need to be revisited once we support multiclass
v0.12.0b5,get the length of the list corresponding to the first dictionary key
v0.12.0b5,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.12.0b5,a global inference indicates the effect of that one feature on the outcome
v0.12.0b5,need to reshape the output to match the input
v0.12.0b5,we want to offset the inference object by the baseline estimate of y
v0.12.0b5,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.12.0b5,get the length of the list corresponding to the first dictionary key
v0.12.0b5,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.12.0b5,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.12.0b5,because then scaling the difference between treatment value and treatment costs is the
v0.12.0b5,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.12.0b5,
v0.12.0b5,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.12.0b5,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.12.0b5,(rather than just not treating at all)
v0.12.0b5,
v0.12.0b5,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.12.0b5,2 * policy_value - always_treat
v0.12.0b5,includes exactly their contribution because policy_value and always_treat both include it
v0.12.0b5,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.12.0b5,2 * policy_value - always-treat
v0.12.0b5,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.12.0b5,is zero and the contribution to always_treat is negative
v0.12.0b5,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b5,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b5,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b5,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b5,get dataframe with all but selected column
v0.12.0b5,apply 10% of a typical treatment for this feature
v0.12.0b5,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.12.0b5,set the effect bounds; for positive treatments these agree with
v0.12.0b5,"the estimates; for negative treatments, we need to invert the interval"
v0.12.0b5,the effect is now always positive since we decrease treatment when negative
v0.12.0b5,"for discrete treatment, stack a zero result in front for control"
v0.12.0b5,we need to call effect_inference to get the correct CI between the two treatment options
v0.12.0b5,we now need to construct the delta in the cost between the two treatments and translate the effect
v0.12.0b5,remove third dimenions potentially added
v0.12.0b5,"find cost of current treatment: equality creates a 2d array with True on each row,"
v0.12.0b5,only if its the location of the current treatment. Then we take the corresponding cost.
v0.12.0b5,construct index of current treatment
v0.12.0b5,add second dimension if needed for broadcasting during translation of effect
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,TODO: conisder working around relying on sklearn implementation details
v0.12.0b5,"Found a good split, return."
v0.12.0b5,Record all splits in case the stratification by weight yeilds a worse partition
v0.12.0b5,Reseed random generator and try again
v0.12.0b5,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.12.0b5,"Found a good split, return."
v0.12.0b5,Did not find a good split
v0.12.0b5,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.12.0b5,Return most weight-balanced partition
v0.12.0b5,Weight stratification algorithm
v0.12.0b5,Sort weights for weight strata search
v0.12.0b5,There are some leftover indices that have yet to be assigned
v0.12.0b5,Append stratum splits to overall splits
v0.12.0b5,"If classification methods produce multiple columns of output,"
v0.12.0b5,we need to manually encode classes to ensure consistent column ordering.
v0.12.0b5,We clone the estimator to make sure that all the folds are
v0.12.0b5,"independent, and that it is pickle-able."
v0.12.0b5,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.12.0b5,`predictions` is a list of method outputs from each fold.
v0.12.0b5,"If each of those is also a list, then treat this as a"
v0.12.0b5,multioutput-multiclass task. We need to separately concatenate
v0.12.0b5,the method outputs for each label into an `n_labels` long list.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Our classes that derive from sklearn ones sometimes include
v0.12.0b5,inherited docstrings that have embedded doctests; we need the following imports
v0.12.0b5,so that they don't break.
v0.12.0b5,TODO: consider working around relying on sklearn implementation details
v0.12.0b5,"Convert X, y into numpy arrays"
v0.12.0b5,Define fit parameters
v0.12.0b5,Some algorithms don't have a check_input option
v0.12.0b5,Check weights array
v0.12.0b5,Check that weights are size-compatible
v0.12.0b5,Normalize inputs
v0.12.0b5,Weight inputs
v0.12.0b5,Fit base class without intercept
v0.12.0b5,Fit Lasso
v0.12.0b5,Reset intercept
v0.12.0b5,The intercept is not calculated properly due the sqrt(weights) factor
v0.12.0b5,so it must be recomputed
v0.12.0b5,Fit lasso without weights
v0.12.0b5,Make weighted splitter
v0.12.0b5,Fit weighted model
v0.12.0b5,Make weighted splitter
v0.12.0b5,Fit weighted model
v0.12.0b5,Call weighted lasso on reduced design matrix
v0.12.0b5,Weighted tau
v0.12.0b5,Select optimal penalty
v0.12.0b5,Warn about consistency
v0.12.0b5,"Convert X, y into numpy arrays"
v0.12.0b5,Fit weighted lasso with user input
v0.12.0b5,"Center X, y"
v0.12.0b5,Calculate quantities that will be used later on. Account for centered data
v0.12.0b5,Calculate coefficient and error variance
v0.12.0b5,Add coefficient correction
v0.12.0b5,Set coefficients and intercept standard errors
v0.12.0b5,Set intercept
v0.12.0b5,Return alpha to 'auto' state
v0.12.0b5,"Note that in the case of no intercept, X_offset is 0"
v0.12.0b5,Calculate the variance of the predictions
v0.12.0b5,Calculate prediction confidence intervals
v0.12.0b5,Assumes flattened y
v0.12.0b5,Compute weighted residuals
v0.12.0b5,To be done once per target. Assumes y can be flattened.
v0.12.0b5,Assumes that X has already been offset
v0.12.0b5,Special case: n_features=1
v0.12.0b5,Compute Lasso coefficients for the columns of the design matrix
v0.12.0b5,Compute C_hat
v0.12.0b5,Compute theta_hat
v0.12.0b5,Allow for single output as well
v0.12.0b5,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.12.0b5,Set coef_ attribute
v0.12.0b5,Set intercept_ attribute
v0.12.0b5,Set selected_alpha_ attribute
v0.12.0b5,Set coef_stderr_
v0.12.0b5,intercept_stderr_
v0.12.0b5,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.12.0b5,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.12.0b5,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.12.0b5,set intercept_ attribute
v0.12.0b5,set coef_ attribute
v0.12.0b5,set alpha_ attribute
v0.12.0b5,set alphas_ attribute
v0.12.0b5,set n_iter_ attribute
v0.12.0b5,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.12.0b5,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.12.0b5,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.12.0b5,now regress X1 on y - X2 * beta2 to learn beta1
v0.12.0b5,set coef_ and intercept_ attributes
v0.12.0b5,Note that the penalized model should *not* have an intercept
v0.12.0b5,don't proxy special methods
v0.12.0b5,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.12.0b5,regressor incorrectly
v0.12.0b5,"Note: for known attributes that have been set this method will not be called,"
v0.12.0b5,so we should just throw here because this is an attribute belonging to this class
v0.12.0b5,but which hasn't yet been set on this instance
v0.12.0b5,set default values for None
v0.12.0b5,check freq_weight should be integer and should be accompanied by sample_var
v0.12.0b5,check array shape
v0.12.0b5,weight X and y and sample_var
v0.12.0b5,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,AzureML
v0.12.0b5,helper imports
v0.12.0b5,write the details of the workspace to a configuration file to the notebook library
v0.12.0b5,if y is a multioutput model
v0.12.0b5,Make sure second dimension has 1 or more item
v0.12.0b5,switch _inner Model to a MultiOutputRegressor
v0.12.0b5,flatten array as automl only takes vectors for y
v0.12.0b5,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.12.0b5,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.12.0b5,as an sklearn estimator
v0.12.0b5,fit implementation for a single output model.
v0.12.0b5,Create experiment for specified workspace
v0.12.0b5,Configure automl_config with training set information.
v0.12.0b5,"Wait for remote run to complete, the set the model"
v0.12.0b5,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.12.0b5,create model and pass model into final.
v0.12.0b5,"If item is an automl config, get its corresponding"
v0.12.0b5,AutomatedML Model and add it to new_Args
v0.12.0b5,"If item is an automl config, get its corresponding"
v0.12.0b5,AutomatedML Model and set it for this key in
v0.12.0b5,kwargs
v0.12.0b5,takes in either automated_ml config and instantiates
v0.12.0b5,an AutomatedMLModel
v0.12.0b5,The prefix can only be 18 characters long
v0.12.0b5,"because prefixes come from kwarg_names, we must ensure they are"
v0.12.0b5,short enough.
v0.12.0b5,Get workspace from config file.
v0.12.0b5,Take the intersect of the white for sample
v0.12.0b5,weights and linear models
v0.12.0b5,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,average the outcome dimension if it exists and ensure 2d y_pred
v0.12.0b5,get index of best treatment
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,TODO: consider working around relying on sklearn implementation details
v0.12.0b5,Create splits of causal tree
v0.12.0b5,Make sure the correct exception is being rethrown
v0.12.0b5,Must make sure indices are merged correctly
v0.12.0b5,Convert rows to columns
v0.12.0b5,Require group assignment t to be one-hot-encoded
v0.12.0b5,Get predictions for the 2 splits
v0.12.0b5,Must make sure indices are merged correctly
v0.12.0b5,Crossfitting
v0.12.0b5,Compute weighted nuisance estimates
v0.12.0b5,-------------------------------------------------------------------------------
v0.12.0b5,Calculate the covariance matrix corresponding to the BLB inference
v0.12.0b5,
v0.12.0b5,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.12.0b5,2. Calculate the weighted moments for each tree slice to create a matrix
v0.12.0b5,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.12.0b5,in that slice from the overall parameter estimate.
v0.12.0b5,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.12.0b5,-------------------------------------------------------------------------------
v0.12.0b5,Calclulate covariance matrix through BLB
v0.12.0b5,Estimators
v0.12.0b5,OrthoForest parameters
v0.12.0b5,Sub-forests
v0.12.0b5,Auxiliary attributes
v0.12.0b5,Fit check
v0.12.0b5,TODO: Check performance
v0.12.0b5,Must normalize weights
v0.12.0b5,Override the CATE inference options
v0.12.0b5,Add blb inference to parent's options
v0.12.0b5,Generate subsample indices
v0.12.0b5,Build trees in parallel
v0.12.0b5,Bootstraping has repetitions in tree sample
v0.12.0b5,Similar for `a` weights
v0.12.0b5,Bootstraping has repetitions in tree sample
v0.12.0b5,Define subsample size
v0.12.0b5,Safety check
v0.12.0b5,Draw points to create little bags
v0.12.0b5,Copy and/or define models
v0.12.0b5,Define nuisance estimators
v0.12.0b5,Define parameter estimators
v0.12.0b5,Define
v0.12.0b5,Need to redefine fit here for auto inference to work due to a quirk in how
v0.12.0b5,wrap_fit is defined
v0.12.0b5,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b5,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b5,Override to flatten output if T is flat
v0.12.0b5,Check that all discrete treatments are represented
v0.12.0b5,Nuissance estimates evaluated with cross-fitting
v0.12.0b5,Define 2-fold iterator
v0.12.0b5,Check if there is only one example of some class
v0.12.0b5,Define 2-fold iterator
v0.12.0b5,need safe=False when cloning for WeightedModelWrapper
v0.12.0b5,Compute residuals
v0.12.0b5,Compute coefficient by OLS on residuals
v0.12.0b5,"Parameter returned by LinearRegression is (d_T, )"
v0.12.0b5,Compute residuals
v0.12.0b5,Compute coefficient by OLS on residuals
v0.12.0b5,ell_2 regularization
v0.12.0b5,Ridge regression estimate
v0.12.0b5,"Parameter returned is of shape (d_T, )"
v0.12.0b5,Return moments and gradients
v0.12.0b5,Compute residuals
v0.12.0b5,Compute moments
v0.12.0b5,"Moments shape is (n, d_T)"
v0.12.0b5,Compute moment gradients
v0.12.0b5,returns shape-conforming residuals
v0.12.0b5,Copy and/or define models
v0.12.0b5,Define parameter estimators
v0.12.0b5,Define moment and mean gradient estimator
v0.12.0b5,"Check that T is shape (n, )"
v0.12.0b5,Check T is numeric
v0.12.0b5,Train label encoder
v0.12.0b5,Call `fit` from parent class
v0.12.0b5,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b5,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b5,Override to flatten output if T is flat
v0.12.0b5,Expand one-hot encoding to include the zero treatment
v0.12.0b5,"Test that T contains all treatments. If not, return None"
v0.12.0b5,Nuissance estimates evaluated with cross-fitting
v0.12.0b5,Define 2-fold iterator
v0.12.0b5,Check if there is only one example of some class
v0.12.0b5,No need to crossfit for internal nodes
v0.12.0b5,Compute partial moments
v0.12.0b5,"If any of the values in the parameter estimate is nan, return None"
v0.12.0b5,Compute partial moments
v0.12.0b5,Compute coefficient by OLS on residuals
v0.12.0b5,ell_2 regularization
v0.12.0b5,Ridge regression estimate
v0.12.0b5,"Parameter returned is of shape (d_T, )"
v0.12.0b5,Return moments and gradients
v0.12.0b5,Compute partial moments
v0.12.0b5,Compute moments
v0.12.0b5,"Moments shape is (n, d_T-1)"
v0.12.0b5,Compute moment gradients
v0.12.0b5,Need to calculate this in an elegant way for when propensity is 0
v0.12.0b5,This will flatten T
v0.12.0b5,Check that T is numeric
v0.12.0b5,Test whether the input estimator is supported
v0.12.0b5,Calculate confidence intervals for the parameter (marginal effect)
v0.12.0b5,Calculate confidence intervals for the effect
v0.12.0b5,Calculate the effects
v0.12.0b5,Calculate the standard deviations for the effects
v0.12.0b5,d_t=None here since we measure the effect across all Ts
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b5,Licensed under the MIT License.
v0.12.0b5,Causal tree parameters
v0.12.0b5,Tree structure
v0.12.0b5,No need for a random split since the data is already
v0.12.0b5,a random subsample from the original input
v0.12.0b5,node list stores the nodes that are yet to be splitted
v0.12.0b5,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b5,Create local sample set
v0.12.0b5,Compute nuisance estimates for the current node
v0.12.0b5,Nuisance estimate cannot be calculated
v0.12.0b5,Estimate parameter for current node
v0.12.0b5,Node estimate cannot be calculated
v0.12.0b5,Calculate moments and gradient of moments for current data
v0.12.0b5,Calculate inverse gradient
v0.12.0b5,The gradient matrix is not invertible.
v0.12.0b5,No good split can be found
v0.12.0b5,Calculate point-wise pseudo-outcomes rho
v0.12.0b5,a split is determined by a feature and a sample pair
v0.12.0b5,the number of possible splits is at most (number of features) * (number of node samples)
v0.12.0b5,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.12.0b5,parse row and column of random pair
v0.12.0b5,the sample of the pair is the integer division of the random number with n_feats
v0.12.0b5,calculate the binary indicator of whether sample i is on the left or the right
v0.12.0b5,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.12.0b5,calculate the number of samples on the left child for each proposed split
v0.12.0b5,calculate the analogous binary indicator for the samples in the estimation set
v0.12.0b5,calculate the number of estimation samples on the left child of each proposed split
v0.12.0b5,find the upper and lower bound on the size of the left split for the split
v0.12.0b5,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.12.0b5,on each side.
v0.12.0b5,similarly for the estimation sample set
v0.12.0b5,if there is no valid split then don't create any children
v0.12.0b5,filter only the valid splits
v0.12.0b5,calculate the average influence vector of the samples in the left child
v0.12.0b5,calculate the average influence vector of the samples in the right child
v0.12.0b5,take the square of each of the entries of the influence vectors and normalize
v0.12.0b5,by size of each child
v0.12.0b5,calculate the vector score of each candidate split as the average of left and right
v0.12.0b5,influence vectors
v0.12.0b5,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.12.0b5,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.12.0b5,where there might be large discontinuities in some parameter as the conditioning set varies
v0.12.0b5,calculate the scalar score of each split by aggregating across the vector of scores
v0.12.0b5,Find split that minimizes criterion
v0.12.0b5,Create child nodes with corresponding subsamples
v0.12.0b5,add the created children to the list of not yet split nodes
v0.12.0b4,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.12.0b4,configuration is all pulled from setup.cfg
v0.12.0b4,-*- coding: utf-8 -*-
v0.12.0b4,
v0.12.0b4,Configuration file for the Sphinx documentation builder.
v0.12.0b4,
v0.12.0b4,This file does only contain a selection of the most common options. For a
v0.12.0b4,full list see the documentation:
v0.12.0b4,http://www.sphinx-doc.org/en/master/config
v0.12.0b4,-- Path setup --------------------------------------------------------------
v0.12.0b4,"If extensions (or modules to document with autodoc) are in another directory,"
v0.12.0b4,add these directories to sys.path here. If the directory is relative to the
v0.12.0b4,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.12.0b4,
v0.12.0b4,-- Project information -----------------------------------------------------
v0.12.0b4,-- General configuration ---------------------------------------------------
v0.12.0b4,"If your documentation needs a minimal Sphinx version, state it here."
v0.12.0b4,
v0.12.0b4,needs_sphinx = '1.0'
v0.12.0b4,"Add any Sphinx extension module names here, as strings. They can be"
v0.12.0b4,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.12.0b4,ones.
v0.12.0b4,"Add any paths that contain templates here, relative to this directory."
v0.12.0b4,The suffix(es) of source filenames.
v0.12.0b4,You can specify multiple suffix as a list of string:
v0.12.0b4,
v0.12.0b4,"source_suffix = ['.rst', '.md']"
v0.12.0b4,The master toctree document.
v0.12.0b4,The language for content autogenerated by Sphinx. Refer to documentation
v0.12.0b4,for a list of supported languages.
v0.12.0b4,
v0.12.0b4,This is also used if you do content translation via gettext catalogs.
v0.12.0b4,"Usually you set ""language"" from the command line for these cases."
v0.12.0b4,"List of patterns, relative to source directory, that match files and"
v0.12.0b4,directories to ignore when looking for source files.
v0.12.0b4,This pattern also affects html_static_path and html_extra_path.
v0.12.0b4,The name of the Pygments (syntax highlighting) style to use.
v0.12.0b4,-- Options for HTML output -------------------------------------------------
v0.12.0b4,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.12.0b4,a list of builtin themes.
v0.12.0b4,
v0.12.0b4,Theme options are theme-specific and customize the look and feel of a theme
v0.12.0b4,"further.  For a list of options available for each theme, see the"
v0.12.0b4,documentation.
v0.12.0b4,
v0.12.0b4,"Add any paths that contain custom static files (such as style sheets) here,"
v0.12.0b4,"relative to this directory. They are copied after the builtin static files,"
v0.12.0b4,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.12.0b4,html_static_path = ['_static']
v0.12.0b4,"Custom sidebar templates, must be a dictionary that maps document names"
v0.12.0b4,to template names.
v0.12.0b4,
v0.12.0b4,The default sidebars (for documents that don't match any pattern) are
v0.12.0b4,defined by theme itself.  Builtin themes are using these templates by
v0.12.0b4,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.12.0b4,'searchbox.html']``.
v0.12.0b4,
v0.12.0b4,html_sidebars = {}
v0.12.0b4,-- Options for HTMLHelp output ---------------------------------------------
v0.12.0b4,Output file base name for HTML help builder.
v0.12.0b4,-- Options for LaTeX output ------------------------------------------------
v0.12.0b4,The paper size ('letterpaper' or 'a4paper').
v0.12.0b4,
v0.12.0b4,"'papersize': 'letterpaper',"
v0.12.0b4,"The font size ('10pt', '11pt' or '12pt')."
v0.12.0b4,
v0.12.0b4,"'pointsize': '10pt',"
v0.12.0b4,Additional stuff for the LaTeX preamble.
v0.12.0b4,
v0.12.0b4,"'preamble': '',"
v0.12.0b4,Latex figure (float) alignment
v0.12.0b4,
v0.12.0b4,"'figure_align': 'htbp',"
v0.12.0b4,Grouping the document tree into LaTeX files. List of tuples
v0.12.0b4,"(source start file, target name, title,"
v0.12.0b4,"author, documentclass [howto, manual, or own class])."
v0.12.0b4,-- Options for manual page output ------------------------------------------
v0.12.0b4,One entry per manual page. List of tuples
v0.12.0b4,"(source start file, name, description, authors, manual section)."
v0.12.0b4,-- Options for Texinfo output ----------------------------------------------
v0.12.0b4,Grouping the document tree into Texinfo files. List of tuples
v0.12.0b4,"(source start file, target name, title, author,"
v0.12.0b4,"dir menu entry, description, category)"
v0.12.0b4,-- Options for Epub output -------------------------------------------------
v0.12.0b4,Bibliographic Dublin Core info.
v0.12.0b4,The unique identifier of the text. This can be a ISBN number
v0.12.0b4,or the project homepage.
v0.12.0b4,
v0.12.0b4,epub_identifier = ''
v0.12.0b4,A unique identification for the text.
v0.12.0b4,
v0.12.0b4,epub_uid = ''
v0.12.0b4,A list of files that should not be packed into the epub file.
v0.12.0b4,-- Extension configuration -------------------------------------------------
v0.12.0b4,-- Options for intersphinx extension ---------------------------------------
v0.12.0b4,Example configuration for intersphinx: refer to the Python standard library.
v0.12.0b4,-- Options for todo extension ----------------------------------------------
v0.12.0b4,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.12.0b4,-- Options for doctest extension -------------------------------------------
v0.12.0b4,we can document otherwise excluded entities here by returning False
v0.12.0b4,or skip otherwise included entities by returning True
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Calculate residuals
v0.12.0b4,Estimate E[T_res | Z_res]
v0.12.0b4,TODO. Deal with multi-class instrument
v0.12.0b4,Calculate nuisances
v0.12.0b4,Estimate E[T_res | Z_res]
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"We do a three way split, as typically a preliminary theta estimator would require"
v0.12.0b4,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.12.0b4,TODO. Deal with multi-class instrument
v0.12.0b4,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b4,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b4,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b4,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b4,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b4,Estimate preliminary theta in cross fitting manner
v0.12.0b4,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b4,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.12.0b4,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.12.0b4,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b4,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.12.0b4,TODO. The solution below is not really a valid cross-fitting
v0.12.0b4,as the test data are used to create the proj_t on the train
v0.12.0b4,which in the second train-test loop is used to create the nuisance
v0.12.0b4,cov on the test data. Hence the T variable of some sample
v0.12.0b4,"is implicitly correlated with its cov nuisance, through this flow"
v0.12.0b4,"of information. However, this seems a rather weak correlation."
v0.12.0b4,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.12.0b4,model.
v0.12.0b4,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.12.0b4,Estimate preliminary theta in cross fitting manner
v0.12.0b4,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b4,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.12.0b4,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b4,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.12.0b4,#############################################################################
v0.12.0b4,Classes for the DRIV implementation for the special case of intent-to-treat
v0.12.0b4,A/B test
v0.12.0b4,#############################################################################
v0.12.0b4,Estimate preliminary theta in cross fitting manner
v0.12.0b4,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b4,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b4,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b4,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b4,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.12.0b4,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.12.0b4,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.12.0b4,model_T_XZ = lambda: model_clf()
v0.12.0b4,#'days_visited': lambda:
v0.12.0b4,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.12.0b4,Turn strings into categories for numeric mapping
v0.12.0b4,### Defining some generic regressors and classifiers
v0.12.0b4,This a generic non-parametric regressor
v0.12.0b4,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b4,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.12.0b4,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b4,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.12.0b4,model = lambda: RandomForestRegressor(n_estimators=100)
v0.12.0b4,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.12.0b4,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.12.0b4,model = lambda: LinearRegression(n_jobs=-1)
v0.12.0b4,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.12.0b4,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.12.0b4,underlying model whenever predict is called.
v0.12.0b4,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b4,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.12.0b4,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b4,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.12.0b4,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.12.0b4,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.12.0b4,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.12.0b4,We need to specify models to be used for each of these residualizations
v0.12.0b4,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.12.0b4,"E[T | X, Z]"
v0.12.0b4,E[TZ | X]
v0.12.0b4,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.12.0b4,n_splits determines the number of splits to be used for cross-fitting.
v0.12.0b4,# Algorithm 2 - Current Method
v0.12.0b4,In[121]:
v0.12.0b4,# Algorithm 3 - DRIV ATE
v0.12.0b4,dmliv_model_effect = lambda: model()
v0.12.0b4,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.12.0b4,"dmliv_model_effect(),"
v0.12.0b4,n_splits=1)
v0.12.0b4,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.12.0b4,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.12.0b4,"Once multiple treatments are supported, we'll need to fix this"
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b4,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b4,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b4,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,TODO. Deal with multi-class instrument/treatment
v0.12.0b4,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.12.0b4,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.12.0b4,Estimate p(X) = E[T | X] in cross-fitting manner
v0.12.0b4,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.12.0b4,##################
v0.12.0b4,Global settings #
v0.12.0b4,##################
v0.12.0b4,Global plotting controls
v0.12.0b4,"Control for support size, can control for more"
v0.12.0b4,#################
v0.12.0b4,File utilities #
v0.12.0b4,#################
v0.12.0b4,#################
v0.12.0b4,Plotting utils #
v0.12.0b4,#################
v0.12.0b4,bias
v0.12.0b4,var
v0.12.0b4,rmse
v0.12.0b4,r2
v0.12.0b4,Infer feature dimension
v0.12.0b4,Metrics by support plots
v0.12.0b4,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.12.0b4,Vasilis Syrgkanis <vasy@microsoft.com>
v0.12.0b4,Steven Wu <zhiww@microsoft.com>
v0.12.0b4,Initialize causal tree parameters
v0.12.0b4,Create splits of causal tree
v0.12.0b4,Estimate treatment effects at the leafs
v0.12.0b4,Compute heterogeneous treatement effect for x's in x_list by finding
v0.12.0b4,the corresponding split and associating the effect computed on that leaf
v0.12.0b4,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.12.0b4,Safety check
v0.12.0b4,Weighted linear regression
v0.12.0b4,Calculates weights
v0.12.0b4,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b4,over all indices
v0.12.0b4,Similar for `a` weights
v0.12.0b4,Doesn't have sample weights
v0.12.0b4,Is a linear model
v0.12.0b4,Weighted linear regression
v0.12.0b4,Calculates weights
v0.12.0b4,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b4,over all indices
v0.12.0b4,Similar for `a` weights
v0.12.0b4,normalize weights
v0.12.0b4,"Split the data in half, train and test"
v0.12.0b4,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b4,"a function of W, using only the train fold"
v0.12.0b4,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b4,"Split the data in half, train and test"
v0.12.0b4,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b4,"a function of W, using only the train fold"
v0.12.0b4,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b4,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.12.0b4,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.12.0b4,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.12.0b4,"Split the data in half, train and test"
v0.12.0b4,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b4,"a function of x, using only the train fold"
v0.12.0b4,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b4,Compute coefficient by OLS on residuals
v0.12.0b4,"Split the data in half, train and test"
v0.12.0b4,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b4,"a function of x, using only the train fold"
v0.12.0b4,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b4,Estimate multipliers for second order orthogonal method
v0.12.0b4,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.12.0b4,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b4,Create local sample set
v0.12.0b4,compute the base estimate for the current node using double ml or second order double ml
v0.12.0b4,compute the influence functions here that are used for the criterion
v0.12.0b4,generate random proposals of dimensions to split
v0.12.0b4,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.12.0b4,compute criterion for each proposal
v0.12.0b4,if splitting creates valid leafs in terms of mean leaf size
v0.12.0b4,Calculate criterion for split
v0.12.0b4,Else set criterion to infinity so that this split is not chosen
v0.12.0b4,If no good split was found
v0.12.0b4,Find split that minimizes criterion
v0.12.0b4,Set the split attributes at the node
v0.12.0b4,Create child nodes with corresponding subsamples
v0.12.0b4,Recursively split children
v0.12.0b4,Return parent node
v0.12.0b4,estimate the local parameter at the leaf using the estimate data
v0.12.0b4,###################
v0.12.0b4,Argument parsing #
v0.12.0b4,###################
v0.12.0b4,#########################################
v0.12.0b4,Parameters constant across experiments #
v0.12.0b4,#########################################
v0.12.0b4,Outcome support
v0.12.0b4,Treatment support
v0.12.0b4,Evaluation grid
v0.12.0b4,Treatment effects array
v0.12.0b4,Other variables
v0.12.0b4,##########################
v0.12.0b4,Data Generating Process #
v0.12.0b4,##########################
v0.12.0b4,Log iteration
v0.12.0b4,"Generate controls, features, treatment and outcome"
v0.12.0b4,T and Y residuals to be used in later scripts
v0.12.0b4,Save generated dataset
v0.12.0b4,#################
v0.12.0b4,ORF parameters #
v0.12.0b4,#################
v0.12.0b4,######################################
v0.12.0b4,Train and evaluate treatment effect #
v0.12.0b4,######################################
v0.12.0b4,########
v0.12.0b4,Plots #
v0.12.0b4,########
v0.12.0b4,###############
v0.12.0b4,Save results #
v0.12.0b4,###############
v0.12.0b4,##############
v0.12.0b4,Run Rscript #
v0.12.0b4,##############
v0.12.0b4,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b4,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b4,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.12.0b4,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b4,def mlasso_model(): return MultiTaskLassoCV(
v0.12.0b4,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b4,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b4,heterogeneity
v0.12.0b4,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b4,heterogeneity
v0.12.0b4,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b4,heterogeneity
v0.12.0b4,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.12.0b4,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b4,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b4,subset of features that are exogenous and create heterogeneity
v0.12.0b4,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.12.0b4,subset of features wrt we estimate heterogeneity
v0.12.0b4,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b4,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,introspect the constructor arguments to find the model parameters
v0.12.0b4,to represent
v0.12.0b4,"if the argument is deprecated, ignore it"
v0.12.0b4,Extract and sort argument names excluding 'self'
v0.12.0b4,column names
v0.12.0b4,transfer input to numpy arrays
v0.12.0b4,transfer input to 2d arrays
v0.12.0b4,create dataframe
v0.12.0b4,currently dowhy only support single outcome and single treatment
v0.12.0b4,call dowhy
v0.12.0b4,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.12.0b4,cate estimator but not the effect.
v0.12.0b4,don't proxy special methods
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Check if model is sparse enough for this model
v0.12.0b4,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.12.0b4,TODO: any way to avoid creating a copy if the array was already dense?
v0.12.0b4,"the call is necessary if the input was something like a list, though"
v0.12.0b4,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.12.0b4,so convert to pydata sparse first
v0.12.0b4,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.12.0b4,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.12.0b4,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.12.0b4,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b4,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b4,For when checking input values is disabled
v0.12.0b4,Type to column extraction function
v0.12.0b4,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.12.0b4,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.12.0b4,Get feature names using featurizer
v0.12.0b4,All attempts at retrieving transformed feature names have failed
v0.12.0b4,Delegate handling to downstream logic
v0.12.0b4,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.12.0b4,same number of input definitions as arrays
v0.12.0b4,input definitions have same number of dimensions as each array
v0.12.0b4,all result indices are unique
v0.12.0b4,all result indices must match at least one input index
v0.12.0b4,"map indices to all array, axis pairs for that index"
v0.12.0b4,each index has the same cardinality wherever it appears
v0.12.0b4,"State: list of (set of letters, list of (corresponding indices, value))"
v0.12.0b4,Algo: while list contains more than one entry
v0.12.0b4,take two entries
v0.12.0b4,sort both lists by intersection of their indices
v0.12.0b4,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.12.0b4,"take the union of indices and the product of values), stepping through each list linearly"
v0.12.0b4,TODO: might be faster to break into connected components first
v0.12.0b4,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.12.0b4,"so compute their content separately, then take cartesian product"
v0.12.0b4,this would save a few pointless sorts by empty tuples
v0.12.0b4,TODO: Consider investigating other performance ideas for these cases
v0.12.0b4,where the dense method beat the sparse method (usually sparse is faster)
v0.12.0b4,"e,facd,c->cfed"
v0.12.0b4,sparse: 0.0335489
v0.12.0b4,dense:  0.011465999999999997
v0.12.0b4,"gbd,da,egb->da"
v0.12.0b4,sparse: 0.0791625
v0.12.0b4,dense:  0.007319099999999995
v0.12.0b4,"dcc,d,faedb,c->abe"
v0.12.0b4,sparse: 1.2868097
v0.12.0b4,dense:  0.44605229999999985
v0.12.0b4,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.12.0b4,TODO: would using einsum's paths to optimize the order of merging help?
v0.12.0b4,assume that we should perform nested cross-validation if and only if
v0.12.0b4,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.12.0b4,logic copied from check_cv
v0.12.0b4,otherwise we will assume the user already set the cv attribute to something
v0.12.0b4,compatible with splitting with a 'groups' argument
v0.12.0b4,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.12.0b4,don't use the groups when calling split internally
v0.12.0b4,Normalize weights
v0.12.0b4,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.12.0b4,"if we're decorating a class, just update the __init__ method,"
v0.12.0b4,so that the result is still a class instead of a wrapper method
v0.12.0b4,"want to enforce that each bad_arg was either in kwargs,"
v0.12.0b4,or else it was in neither and is just taking its default value
v0.12.0b4,Any access should throw
v0.12.0b4,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b4,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b4,input feature name is already updated by cate_feature_names.
v0.12.0b4,define the index of d_x to filter for each given T
v0.12.0b4,filter X after broadcast with T for each given T
v0.12.0b4,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b4,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,
v0.12.0b4,This code contains some snippets of code from:
v0.12.0b4,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
v0.12.0b4,published under the following license and copyright:
v0.12.0b4,BSD 3-Clause License
v0.12.0b4,
v0.12.0b4,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b4,All rights reserved.
v0.12.0b4,make any access to matplotlib or plt throw an exception
v0.12.0b4,make any access to graphviz or plt throw an exception
v0.12.0b4,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.12.0b4,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.12.0b4,Initialize saturation & value; calculate chroma & value shift
v0.12.0b4,Calculate some intermediate values
v0.12.0b4,Initialize RGB with same hue & chroma as our color
v0.12.0b4,Shift the initial RGB values to match value and store
v0.12.0b4,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.12.0b4,clean way of achieving this
v0.12.0b4,make sure we don't accidentally escape anything in the substitution
v0.12.0b4,Fetch appropriate color for node
v0.12.0b4,"red for negative, green for positive"
v0.12.0b4,in multi-target use mean of targets
v0.12.0b4,Write node mean CATE
v0.12.0b4,Write node std of CATE
v0.12.0b4,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.12.0b4,Fetch appropriate color for node
v0.12.0b4,Write node mean CATE
v0.12.0b4,Write node mean CATE
v0.12.0b4,Write recommended treatment and value - cost
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"since inference objects can be stateful, we must copy it before fitting;"
v0.12.0b4,otherwise this sequence wouldn't work:
v0.12.0b4,"est1.fit(..., inference=inf)"
v0.12.0b4,"est2.fit(..., inference=inf)"
v0.12.0b4,est1.effect_interval(...)
v0.12.0b4,because inf now stores state from fitting est2
v0.12.0b4,This flag is true when names are set in a child class instead
v0.12.0b4,"If names are set in a child class, add an attribute reflecting that"
v0.12.0b4,This works only if X is passed as a kwarg
v0.12.0b4,We plan to enforce X as kwarg only in future releases
v0.12.0b4,This checks if names have been set in a child class
v0.12.0b4,"If names were set in a child class, don't do it again"
v0.12.0b4,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.12.0b4,call the wrapped fit method
v0.12.0b4,NOTE: we call inference fit *after* calling the main fit method
v0.12.0b4,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.12.0b4,but tensordot can't be applied to this problem because we don't sum over m
v0.12.0b4,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.12.0b4,of rows of T was not taken into account
v0.12.0b4,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.12.0b4,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.12.0b4,"Treatment names is None, default to BaseCateEstimator"
v0.12.0b4,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.12.0b4,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.12.0b4,Get input names
v0.12.0b4,Summary
v0.12.0b4,add statsmodels to parent's options
v0.12.0b4,add debiasedlasso to parent's options
v0.12.0b4,add blb to parent's options
v0.12.0b4,TODO Share some logic with non-discrete version
v0.12.0b4,Get input names
v0.12.0b4,Summary
v0.12.0b4,add statsmodels to parent's options
v0.12.0b4,add statsmodels to parent's options
v0.12.0b4,add blb to parent's options
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,remove None arguments
v0.12.0b4,"scores entries should be lists of scores, so make each entry a singleton list"
v0.12.0b4,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b4,generate an instance of the final model
v0.12.0b4,generate an instance of the nuisance model
v0.12.0b4,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.12.0b4,alteration even when we only want to fit_final.
v0.12.0b4,use a binary array to get stratified split in case of discrete treatment
v0.12.0b4,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.12.0b4,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.12.0b4,"however, sklearn doesn't support both stratifying and grouping (see"
v0.12.0b4,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.12.0b4,their own object that supports grouping if they want to use groups.
v0.12.0b4,for each mc iteration
v0.12.0b4,for each model under cross fit setting
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,
v0.12.0b4,This code contains snippets of code from
v0.12.0b4,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b4,published under the following license and copyright:
v0.12.0b4,BSD 3-Clause License
v0.12.0b4,
v0.12.0b4,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b4,All rights reserved.
v0.12.0b4,=============================================================================
v0.12.0b4,Policy Forest
v0.12.0b4,=============================================================================
v0.12.0b4,Remap output
v0.12.0b4,reshape is necessary to preserve the data contiguity against vs
v0.12.0b4,"[:, np.newaxis] that does not."
v0.12.0b4,Get subsample sample size
v0.12.0b4,Check parameters
v0.12.0b4,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b4,if this is the first `fit` call of the warm start mode.
v0.12.0b4,"Free allocated memory, if any"
v0.12.0b4,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b4,We draw from the random state to get the random state we
v0.12.0b4,would have got if we hadn't used a warm_start.
v0.12.0b4,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b4,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b4,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b4,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b4,for fitting the trees is internally releasing the Python GIL
v0.12.0b4,making threading more efficient than multiprocessing in
v0.12.0b4,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b4,"parallel_backend contexts set at a higher level,"
v0.12.0b4,since correctness does not rely on using threads.
v0.12.0b4,Collect newly grown trees
v0.12.0b4,Check data
v0.12.0b4,Assign chunk of trees to jobs
v0.12.0b4,avoid storing the output of every estimator by summing them here
v0.12.0b4,Parallel loop
v0.12.0b4,Check data
v0.12.0b4,Assign chunk of trees to jobs
v0.12.0b4,avoid storing the output of every estimator by summing them here
v0.12.0b4,Parallel loop
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,
v0.12.0b4,This code contains snippets of code from:
v0.12.0b4,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b4,published under the following license and copyright:
v0.12.0b4,BSD 3-Clause License
v0.12.0b4,
v0.12.0b4,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b4,All rights reserved.
v0.12.0b4,=============================================================================
v0.12.0b4,Types and constants
v0.12.0b4,=============================================================================
v0.12.0b4,=============================================================================
v0.12.0b4,Base Policy tree
v0.12.0b4,=============================================================================
v0.12.0b4,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.12.0b4,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.12.0b4,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.12.0b4,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.12.0b4,checks that X is 2D array.
v0.12.0b4,"since we only allow single dimensional y, we could flatten the prediction"
v0.12.0b4,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b4,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b4,Handles the corner case when X=None but featurizer might be not None
v0.12.0b4,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.12.0b4,"Replacing this method which is invalid for this class, so that we make the"
v0.12.0b4,dosctring empty and not appear in the docs.
v0.12.0b4,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b4,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.12.0b4,Replacing to remove docstring
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"if both X and W are None, just return a column of ones"
v0.12.0b4,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b4,We need to go back to the label representation of the one-hot so as to call
v0.12.0b4,the classifier.
v0.12.0b4,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b4,We need to go back to the label representation of the one-hot so as to call
v0.12.0b4,the classifier.
v0.12.0b4,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b4,This works both with our without the weighting trick as the treatments T are unit vector
v0.12.0b4,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.12.0b4,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.12.0b4,both Parametric and Non Parametric DML.
v0.12.0b4,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.12.0b4,attribute so that the trained featurizer will be passed through
v0.12.0b4,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b4,for internal use by the library
v0.12.0b4,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b4,We need to use the rlearner's copy to retain the information from fitting
v0.12.0b4,Handles the corner case when X=None but featurizer might be not None
v0.12.0b4,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b4,since we clone it and fit separate copies
v0.12.0b4,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b4,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b4,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b4,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b4,since we clone it and fit separate copies
v0.12.0b4,add blb to parent's options
v0.12.0b4,override only so that we can update the docstring to indicate
v0.12.0b4,support for `GenericSingleTreatmentModelFinalInference`
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,note that groups are not passed to score because they are only used for fitting
v0.12.0b4,note that groups are not passed to score because they are only used for fitting
v0.12.0b4,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b4,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b4,NOTE: important to get parent's wrapped copy so that
v0.12.0b4,"after training wrapped featurizer is also trained, etc."
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b4,Fit a doubly robust average effect
v0.12.0b4,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b4,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b4,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b4,since we clone it and fit separate copies
v0.12.0b4,"If custom param grid, check that only estimator parameters are being altered"
v0.12.0b4,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.12.0b4,override only so that we can update the docstring to indicate support for `blb`
v0.12.0b4,Get input names
v0.12.0b4,Summary
v0.12.0b4,Determine output settings
v0.12.0b4,"Important: This must be the first invocation of the random state at fit time, so that"
v0.12.0b4,train/test splits are re-generatable from an external object simply by knowing the
v0.12.0b4,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.12.0b4,linear predictions. Currently is also useful for testing.
v0.12.0b4,reshape is necessary to preserve the data contiguity against vs
v0.12.0b4,"[:, np.newaxis] that does not."
v0.12.0b4,Check parameters
v0.12.0b4,Set min_weight_leaf from min_weight_fraction_leaf
v0.12.0b4,Build tree
v0.12.0b4,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.12.0b4,hold. Used by criterion for memory space savings.
v0.12.0b4,Initialize the criterion object and the criterion_val object if honest.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,
v0.12.0b4,This code is a fork from:
v0.12.0b4,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.12.0b4,published under the following license and copyright:
v0.12.0b4,BSD 3-Clause License
v0.12.0b4,
v0.12.0b4,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b4,All rights reserved.
v0.12.0b4,Set parameters
v0.12.0b4,Don't instantiate estimators now! Parameters of base_estimator might
v0.12.0b4,"still change. Eg., when grid-searching with the nested object syntax."
v0.12.0b4,self.estimators_ needs to be filled by the derived classes in fit.
v0.12.0b4,Compute the number of jobs
v0.12.0b4,Partition estimators between jobs
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Remove children with nonwhite mothers from the treatment group
v0.12.0b4,Remove children with nonwhite mothers from the treatment group
v0.12.0b4,Select columns
v0.12.0b4,Scale the numeric variables
v0.12.0b4,"Change the binary variable 'first' takes values in {1,2}"
v0.12.0b4,Append a column of ones as intercept
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b4,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b4,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.12.0b4,d_t=None here since we measure the effect across all Ts
v0.12.0b4,once the estimator has been fit
v0.12.0b4,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.12.0b4,an intercept even when bias is part of coef.
v0.12.0b4,We can write effect inference as a function of prediction and prediction standard error of
v0.12.0b4,the final method for linear models
v0.12.0b4,squeeze the first axis
v0.12.0b4,d_t=None here since we measure the effect across all Ts
v0.12.0b4,set the mean_pred_stderr
v0.12.0b4,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b4,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b4,"send treatment to the end, pull bounds to the front"
v0.12.0b4,d_t=None here since we measure the effect across all Ts
v0.12.0b4,set the mean_pred_stderr
v0.12.0b4,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.12.0b4,d_t=None here since we measure the effect across all Ts
v0.12.0b4,d_t=None here since we measure the effect across all Ts
v0.12.0b4,need to set the fit args before the estimator is fit
v0.12.0b4,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b4,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.12.0b4,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.12.0b4,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.12.0b4,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.12.0b4,scale preds
v0.12.0b4,scale std errs
v0.12.0b4,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.12.0b4,offset preds
v0.12.0b4,"offset the distribution, too"
v0.12.0b4,scale preds
v0.12.0b4,"scale the distribution, too"
v0.12.0b4,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b4,1. Uncertainty of Mean Point Estimate
v0.12.0b4,2. Distribution of Point Estimate
v0.12.0b4,3. Total Variance of Point Estimate
v0.12.0b4,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.12.0b4,so bail out early; note that it might be possible to correct the algorithm for
v0.12.0b4,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.12.0b4,be clean
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,TODO: Add a __dir__ implementation?
v0.12.0b4,don't proxy special methods
v0.12.0b4,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.12.0b4,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.12.0b4,then we should be computing a confidence interval for the wrapped calls
v0.12.0b4,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.12.0b4,second level bootstrap which would be prohibitive computationally?
v0.12.0b4,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.12.0b4,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.12.0b4,can't import from econml.inference at top level without creating cyclical dependencies
v0.12.0b4,Note that inference results are always methods even if the inference is for a property
v0.12.0b4,(e.g. coef__inference() is a method but coef_ is a property)
v0.12.0b4,Therefore we must insert a lambda if getting inference for a non-callable
v0.12.0b4,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.12.0b4,"try to get interval/std first if appropriate,"
v0.12.0b4,since we don't prefer a wrapped method with this name
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,
v0.12.0b4,This code contains snippets of code from:
v0.12.0b4,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b4,published under the following license and copyright:
v0.12.0b4,BSD 3-Clause License
v0.12.0b4,
v0.12.0b4,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b4,All rights reserved.
v0.12.0b4,=============================================================================
v0.12.0b4,Types and constants
v0.12.0b4,=============================================================================
v0.12.0b4,=============================================================================
v0.12.0b4,Base GRF tree
v0.12.0b4,=============================================================================
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,=============================================================================
v0.12.0b4,A MultOutputWrapper for GRF classes
v0.12.0b4,=============================================================================
v0.12.0b4,=============================================================================
v0.12.0b4,Instantiations of Generalized Random Forest
v0.12.0b4,=============================================================================
v0.12.0b4,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.12.0b4,in front of the constant treatment is the intercept in the moment equation.
v0.12.0b4,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.12.0b4,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,
v0.12.0b4,This code contains snippets of code from
v0.12.0b4,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b4,published under the following license and copyright:
v0.12.0b4,BSD 3-Clause License
v0.12.0b4,
v0.12.0b4,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b4,All rights reserved.
v0.12.0b4,=============================================================================
v0.12.0b4,Base Generalized Random Forest
v0.12.0b4,=============================================================================
v0.12.0b4,TODO: support freq_weight and sample_var
v0.12.0b4,Remap output
v0.12.0b4,reshape is necessary to preserve the data contiguity against vs
v0.12.0b4,"[:, np.newaxis] that does not."
v0.12.0b4,reshape is necessary to preserve the data contiguity against vs
v0.12.0b4,"[:, np.newaxis] that does not."
v0.12.0b4,Get subsample sample size
v0.12.0b4,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.12.0b4,We calculate the min eigenvalue proxy that each criterion is considering
v0.12.0b4,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.12.0b4,Check parameters
v0.12.0b4,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b4,if this is the first `fit` call of the warm start mode.
v0.12.0b4,"Free allocated memory, if any"
v0.12.0b4,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b4,We draw from the random state to get the random state we
v0.12.0b4,would have got if we hadn't used a warm_start.
v0.12.0b4,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b4,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b4,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b4,Generating indices a priori before parallelism ended up being orders of magnitude
v0.12.0b4,faster than how sklearn does it. The reason is that random samplers do not release the
v0.12.0b4,gil it seems.
v0.12.0b4,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b4,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b4,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b4,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b4,for fitting the trees is internally releasing the Python GIL
v0.12.0b4,making threading more efficient than multiprocessing in
v0.12.0b4,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b4,"parallel_backend contexts set at a higher level,"
v0.12.0b4,since correctness does not rely on using threads.
v0.12.0b4,Collect newly grown trees
v0.12.0b4,Check data
v0.12.0b4,Assign chunk of trees to jobs
v0.12.0b4,avoid storing the output of every estimator by summing them here
v0.12.0b4,Parallel loop
v0.12.0b4,Check data
v0.12.0b4,Assign chunk of trees to jobs
v0.12.0b4,Parallel loop
v0.12.0b4,Check data
v0.12.0b4,Assign chunk of trees to jobs
v0.12.0b4,Parallel loop
v0.12.0b4,####################
v0.12.0b4,Variance correction
v0.12.0b4,####################
v0.12.0b4,Subtract the average within bag variance. This ends up being equal to the
v0.12.0b4,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.12.0b4,The negative part is just sq_between.
v0.12.0b4,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.12.0b4,"The off diagonals we have no objective prior, so no correction is applied."
v0.12.0b4,Finally correcting the pred_cov or pred_var
v0.12.0b4,avoid storing the output of every estimator by summing them here
v0.12.0b4,Parallel loop
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,testing importances
v0.12.0b4,testing heterogeneity importances
v0.12.0b4,Testing that all parameters do what they are supposed to
v0.12.0b4,"testing predict, apply and decision path"
v0.12.0b4,test that the subsampling scheme past to the trees is correct
v0.12.0b4,The sample size is chosen in particular to test rounding based error when subsampling
v0.12.0b4,test that the estimator calcualtes var correctly
v0.12.0b4,test api
v0.12.0b4,test accuracy
v0.12.0b4,test the projection functionality of forests
v0.12.0b4,test that the estimator calcualtes var correctly
v0.12.0b4,test api
v0.12.0b4,test that the estimator calcualtes var correctly
v0.12.0b4,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b4,test that we raise errors in mishandled situations.
v0.12.0b4,test that the subsampling scheme past to the trees is correct
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,omit the lalonde notebook
v0.12.0b4,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.12.0b4,creating notebooks that are annoying for our users to actually run themselves
v0.12.0b4,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b4,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.12.0b4,"prior to calling interpret, can't plot, render, etc."
v0.12.0b4,can interpret without uncertainty
v0.12.0b4,can't interpret with uncertainty if inference wasn't used during fit
v0.12.0b4,can interpret with uncertainty if we refit
v0.12.0b4,can interpret without uncertainty
v0.12.0b4,can't treat before interpreting
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,simple DGP only for illustration
v0.12.0b4,Define the treatment model neural network architecture
v0.12.0b4,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b4,"so the input shape is (d_z + d_x,)"
v0.12.0b4,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b4,add extra layers on top for the mixture density network
v0.12.0b4,Define the response model neural network architecture
v0.12.0b4,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b4,"so the input shape is (d_t + d_x,)"
v0.12.0b4,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b4,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b4,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b4,so that the same weights will be reused in each instantiation
v0.12.0b4,number of samples to use in second estimate of the response
v0.12.0b4,(to make loss estimate unbiased)
v0.12.0b4,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b4,do something with predictions...
v0.12.0b4,also test vector t and y
v0.12.0b4,simple DGP only for illustration
v0.12.0b4,Define the treatment model neural network architecture
v0.12.0b4,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b4,"so the input shape is (d_z + d_x,)"
v0.12.0b4,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b4,add extra layers on top for the mixture density network
v0.12.0b4,Define the response model neural network architecture
v0.12.0b4,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b4,"so the input shape is (d_t + d_x,)"
v0.12.0b4,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b4,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b4,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b4,so that the same weights will be reused in each instantiation
v0.12.0b4,number of samples to use in second estimate of the response
v0.12.0b4,(to make loss estimate unbiased)
v0.12.0b4,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b4,do something with predictions...
v0.12.0b4,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.12.0b4,test = True ensures we draw test set images
v0.12.0b4,test = True ensures we draw test set images
v0.12.0b4,re-draw to get new independent treatment and implied response
v0.12.0b4,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b4,above is necesary so that reduced form doesn't win
v0.12.0b4,covariates: time and emotion
v0.12.0b4,random instrument
v0.12.0b4,z -> price
v0.12.0b4,true observable demand function
v0.12.0b4,errors
v0.12.0b4,response
v0.12.0b4,test = True ensures we draw test set images
v0.12.0b4,test = True ensures we draw test set images
v0.12.0b4,re-draw to get new independent treatment and implied response
v0.12.0b4,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b4,above is necesary so that reduced form doesn't win
v0.12.0b4,covariates: time and emotion
v0.12.0b4,random instrument
v0.12.0b4,z -> price
v0.12.0b4,true observable demand function
v0.12.0b4,errors
v0.12.0b4,response
v0.12.0b4,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.12.0b4,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.12.0b4,For some reason this doesn't work at all when run against the CNTK backend...
v0.12.0b4,"model.compile('nadam', loss=lambda _,l:l)"
v0.12.0b4,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.12.0b4,generate a valiation set
v0.12.0b4,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.12.0b4,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,DGP constants
v0.12.0b4,Generate data
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,testing importances
v0.12.0b4,testing heterogeneity importances
v0.12.0b4,Testing that all parameters do what they are supposed to
v0.12.0b4,"testing predict, apply and decision path"
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.12.0b4,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.12.0b4,so we need to transpose the result
v0.12.0b4,1-d output
v0.12.0b4,2-d output
v0.12.0b4,Single dimensional output y
v0.12.0b4,compare with weight
v0.12.0b4,compare with weight
v0.12.0b4,compare with weight
v0.12.0b4,compare with weight
v0.12.0b4,Multi-dimensional output y
v0.12.0b4,1-d y
v0.12.0b4,compare when both sample_var and sample_weight exist
v0.12.0b4,multi-d y
v0.12.0b4,compare when both sample_var and sample_weight exist
v0.12.0b4,compare when both sample_var and sample_weight exist
v0.12.0b4,compare when both sample_var and sample_weight exist
v0.12.0b4,compare when both sample_var and sample_weight exist
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,test that we can fit with the same arguments as the base estimator
v0.12.0b4,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b4,with the same shape for the lower and upper bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,test that we can do the same thing once we provide percentile bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b4,with the same shape for the lower and upper bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,test that we can do the same thing once we provide percentile bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,test that we can fit with the same arguments as the base estimator
v0.12.0b4,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b4,with the same shape for the lower and upper bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,test that we can do the same thing once we provide percentile bounds
v0.12.0b4,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b4,with the same shape for the lower and upper bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,test that we can do the same thing once we provide percentile bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,test that we can fit with the same arguments as the base estimator
v0.12.0b4,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b4,with the same shape for the lower and upper bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,test that we can do the same thing once we provide percentile bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b4,with the same shape for the lower and upper bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,test that we can do the same thing once we provide percentile bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b4,with the same shape for the lower and upper bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,test that the estimated effect is usually within the bounds
v0.12.0b4,test that we can do the same thing once we provide alpha explicitly
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,test that the estimated effect is usually within the bounds
v0.12.0b4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b4,with the same shape for the lower and upper bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,TODO: test that the estimated effect is usually within the bounds
v0.12.0b4,and that the true effect is also usually within the bounds
v0.12.0b4,test that we can do the same thing once we provide percentile bounds
v0.12.0b4,test that the lower and upper bounds differ
v0.12.0b4,TODO: test that the estimated effect is usually within the bounds
v0.12.0b4,and that the true effect is also usually within the bounds
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,test that the subsampling scheme past to the trees is correct
v0.12.0b4,test that the estimator calcualtes var correctly
v0.12.0b4,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b4,test that we raise errors in mishandled situations.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,DGP constants
v0.12.0b4,Generate data
v0.12.0b4,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b4,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b4,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.12.0b4,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b4,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.12.0b4,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b4,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b4,pvalue for second column should be greater than zero since some points are on either side
v0.12.0b4,of the tested value
v0.12.0b4,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.12.0b4,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b4,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b4,only is not None when T1 is a constant or a list of constant
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.12.0b4,Test non keyword based calls to fit
v0.12.0b4,test non-array inputs
v0.12.0b4,Test custom splitter
v0.12.0b4,Test incomplete set of test folds
v0.12.0b4,"y scores should be positive, since W predicts Y somewhat"
v0.12.0b4,"t scores might not be, since W and T are uncorrelated"
v0.12.0b4,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,make sure cross product varies more slowly with first array
v0.12.0b4,and that vectors are okay as inputs
v0.12.0b4,number of inputs in specification must match number of inputs
v0.12.0b4,must have an output
v0.12.0b4,output indices must be unique
v0.12.0b4,output indices must be present in an input
v0.12.0b4,number of indices must match number of dimensions for each input
v0.12.0b4,repeated indices must always have consistent sizes
v0.12.0b4,transpose
v0.12.0b4,tensordot
v0.12.0b4,trace
v0.12.0b4,TODO: set up proper flag for this
v0.12.0b4,pick indices at random with replacement from the first 7 letters of the alphabet
v0.12.0b4,"of all of the distinct indices that appear in any input,"
v0.12.0b4,pick a random subset of them (of size at most 5) to appear in the output
v0.12.0b4,creating an instance should warn
v0.12.0b4,using the instance should not warn
v0.12.0b4,using the deprecated method should warn
v0.12.0b4,don't warn if b and c are passed by keyword
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Preprocess data
v0.12.0b4,Convert 'week' to a date
v0.12.0b4,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.12.0b4,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.12.0b4,Take log of price
v0.12.0b4,Make brand numeric
v0.12.0b4,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.12.0b4,which have all zero coeffs
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,test at least one estimator from each category
v0.12.0b4,test causal graph
v0.12.0b4,test refutation estimate
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.12.0b4,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.12.0b4,TODO: test something rather than just print...
v0.12.0b4,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.12.0b4,pick some arbitrary X
v0.12.0b4,pick some arbitrary T
v0.12.0b4,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b4,The average variance should be lower when using monte carlo iterations
v0.12.0b4,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b4,The average variance should be lower when using monte carlo iterations
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,ensure that we've got at least two of every row
v0.12.0b4,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b4,need to make sure we get all *joint* combinations
v0.12.0b4,IntentToTreat only supports binary treatments/instruments
v0.12.0b4,IntentToTreat only supports binary treatments/instruments
v0.12.0b4,IntentToTreat requires X
v0.12.0b4,ensure we can serialize unfit estimator
v0.12.0b4,these support only W but not X
v0.12.0b4,"these support only binary, not general discrete T and Z"
v0.12.0b4,ensure we can serialize fit estimator
v0.12.0b4,make sure we can call the marginal_effect and effect methods
v0.12.0b4,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b4,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b4,"make sure we can call effect with implied scalar treatments,"
v0.12.0b4,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b4,are multiple treatments
v0.12.0b4,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b4,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.12.0b4,"however, with custom splits the checking happens in the first stage wrapper"
v0.12.0b4,where we don't have all of the required information to do this;
v0.12.0b4,we'd probably need to add it to _crossfit instead
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.12.0b4,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.12.0b4,The __warningregistry__'s need to be in a pristine state for tests
v0.12.0b4,to work properly.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Set random seed
v0.12.0b4,Generate data
v0.12.0b4,DGP constants
v0.12.0b4,Test data
v0.12.0b4,Constant treatment effect
v0.12.0b4,Constant treatment with multi output Y
v0.12.0b4,Heterogeneous treatment
v0.12.0b4,Heterogeneous treatment with multi output Y
v0.12.0b4,TLearner test
v0.12.0b4,Instantiate TLearner
v0.12.0b4,Test inputs
v0.12.0b4,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b4,Instantiate SLearner
v0.12.0b4,Test inputs
v0.12.0b4,Test constant treatment effect
v0.12.0b4,Test constant treatment effect with multi output Y
v0.12.0b4,Test heterogeneous treatment effect
v0.12.0b4,Need interactions between T and features
v0.12.0b4,Test heterogeneous treatment effect with multi output Y
v0.12.0b4,Instantiate XLearner
v0.12.0b4,Test inputs
v0.12.0b4,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b4,Instantiate DomainAdaptationLearner
v0.12.0b4,Test inputs
v0.12.0b4,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b4,Get the true treatment effect
v0.12.0b4,Get the true treatment effect
v0.12.0b4,Fit learner and get the effect and marginal effect
v0.12.0b4,Compute treatment effect residuals (absolute)
v0.12.0b4,Check that at least 90% of predictions are within tolerance interval
v0.12.0b4,Check whether the output shape is right
v0.12.0b4,Check that one can pass in regular lists
v0.12.0b4,Check that it fails correctly if lists of different shape are passed in
v0.12.0b4,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b4,Generate covariates
v0.12.0b4,Generate treatment
v0.12.0b4,Calculate outcome
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,DGP constants
v0.12.0b4,Generate data
v0.12.0b4,Test data
v0.12.0b4,Remove warnings that might be raised by the models passed into the ORF
v0.12.0b4,Generate data with continuous treatments
v0.12.0b4,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.12.0b4,does not work well with parallelism.
v0.12.0b4,Test inputs for continuous treatments
v0.12.0b4,--> Check that one can pass in regular lists
v0.12.0b4,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b4,Check that outputs have the correct shape
v0.12.0b4,Test continuous treatments with controls
v0.12.0b4,Test continuous treatments without controls
v0.12.0b4,Generate data with binary treatments
v0.12.0b4,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.12.0b4,does not work well with parallelism.
v0.12.0b4,Test inputs for binary treatments
v0.12.0b4,--> Check that one can pass in regular lists
v0.12.0b4,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b4,"--> Check that it works when T, Y have shape (n, 1)"
v0.12.0b4,"--> Check that it fails correctly when T has shape (n, 2)"
v0.12.0b4,--> Check that it fails correctly when the treatments are not numeric
v0.12.0b4,Check that outputs have the correct shape
v0.12.0b4,Test binary treatments with controls
v0.12.0b4,Test binary treatments without controls
v0.12.0b4,Only applicable to continuous treatments
v0.12.0b4,Generate data for 2 treatments
v0.12.0b4,Test multiple treatments with controls
v0.12.0b4,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.12.0b4,The rest for controls. Just as an example.
v0.12.0b4,Generating A/B test data
v0.12.0b4,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.12.0b4,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.12.0b4,Create a wrapper around Lasso that doesn't support weights
v0.12.0b4,since Lasso does natively support them starting in sklearn 0.23
v0.12.0b4,Generate data with continuous treatments
v0.12.0b4,Instantiate model with most of the default parameters
v0.12.0b4,Compute the treatment effect on test points
v0.12.0b4,Compute treatment effect residuals
v0.12.0b4,Multiple treatments
v0.12.0b4,Allow at most 10% test points to be outside of the tolerance interval
v0.12.0b4,Compute treatment effect residuals
v0.12.0b4,Multiple treatments
v0.12.0b4,Allow at most 20% test points to be outside of the confidence interval
v0.12.0b4,Check that the intervals are not too wide
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.12.0b4,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.12.0b4,ensure that we've got at least 6 of every element
v0.12.0b4,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.12.0b4,NOTE: this number may need to change if the default number of folds in
v0.12.0b4,WeightedStratifiedKFold changes
v0.12.0b4,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b4,ensure we can serialize the unfit estimator
v0.12.0b4,ensure we can pickle the fit estimator
v0.12.0b4,make sure we can call the marginal_effect and effect methods
v0.12.0b4,test const marginal inference
v0.12.0b4,test effect inference
v0.12.0b4,test marginal effect inference
v0.12.0b4,test coef__inference and intercept__inference
v0.12.0b4,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b4,"make sure we can call effect with implied scalar treatments,"
v0.12.0b4,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b4,are multiple treatments
v0.12.0b4,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b4,ensure that we've got at least two of every element
v0.12.0b4,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b4,make sure we can call the marginal_effect and effect methods
v0.12.0b4,test const marginal inference
v0.12.0b4,test effect inference
v0.12.0b4,test marginal effect inference
v0.12.0b4,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b4,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b4,We concatenate the two copies data
v0.12.0b4,make sure we can get out post-fit stuff
v0.12.0b4,create a simple artificial setup where effect of moving from treatment
v0.12.0b4,"1 -> 2 is 2,"
v0.12.0b4,"1 -> 3 is 1, and"
v0.12.0b4,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b4,"Using an uneven number of examples from different classes,"
v0.12.0b4,"and having the treatments in non-lexicographic order,"
v0.12.0b4,Should rule out some basic issues.
v0.12.0b4,test that we can fit with a KFold instance
v0.12.0b4,test that we can fit with a train/test iterable
v0.12.0b4,predetermined splits ensure that all features are seen in each split
v0.12.0b4,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.12.0b4,(incorrectly) use a final model with an intercept
v0.12.0b4,"Because final model is fixed, actual values of T and Y don't matter"
v0.12.0b4,Ensure reproducibility
v0.12.0b4,Sparse DGP
v0.12.0b4,Treatment effect coef
v0.12.0b4,Other coefs
v0.12.0b4,Features and controls
v0.12.0b4,Test sparse estimator
v0.12.0b4,"--> test coef_, intercept_"
v0.12.0b4,--> test treatment effects
v0.12.0b4,Restrict x_test to vectors of norm < 1
v0.12.0b4,--> check inference
v0.12.0b4,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b4,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.12.0b4,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.12.0b4,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.12.0b4,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.12.0b4,sparse test case: heterogeneous effect by product
v0.12.0b4,need at least as many rows in e_y as there are distinct columns
v0.12.0b4,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.12.0b4,create a simple artificial setup where effect of moving from treatment
v0.12.0b4,"a -> b is 2,"
v0.12.0b4,"a -> c is 1, and"
v0.12.0b4,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.12.0b4,"Using an uneven number of examples from different classes,"
v0.12.0b4,"and having the treatments in non-lexicographic order,"
v0.12.0b4,should rule out some basic issues.
v0.12.0b4,Note that explicitly specifying the dtype as object is necessary until
v0.12.0b4,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.12.0b4,estimated effects should be identical when treatment is explicitly given
v0.12.0b4,but const_marginal_effect should be reordered based on the explicit cagetories
v0.12.0b4,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.12.0b4,test outer grouping
v0.12.0b4,test nested grouping
v0.12.0b4,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b4,whichever groups we saw
v0.12.0b4,test nested grouping
v0.12.0b4,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b4,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b4,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Set random seed
v0.12.0b4,Generate data
v0.12.0b4,DGP constants
v0.12.0b4,Test data
v0.12.0b4,Constant treatment effect and propensity
v0.12.0b4,Heterogeneous treatment and propensity
v0.12.0b4,ensure that we've got at least two of every element
v0.12.0b4,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b4,ensure that we can serialize unfit estimator
v0.12.0b4,ensure that we can serialize fit estimator
v0.12.0b4,make sure we can call the marginal_effect and effect methods
v0.12.0b4,test const marginal inference
v0.12.0b4,test effect inference
v0.12.0b4,test marginal effect inference
v0.12.0b4,test coef_ and intercept_ inference
v0.12.0b4,verify we can generate the summary
v0.12.0b4,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b4,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b4,create a simple artificial setup where effect of moving from treatment
v0.12.0b4,"1 -> 2 is 2,"
v0.12.0b4,"1 -> 3 is 1, and"
v0.12.0b4,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b4,"Using an uneven number of examples from different classes,"
v0.12.0b4,"and having the treatments in non-lexicographic order,"
v0.12.0b4,Should rule out some basic issues.
v0.12.0b4,test that we can fit with a KFold instance
v0.12.0b4,test that we can fit with a train/test iterable
v0.12.0b4,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b4,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b4,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b4,test coef__inference function works
v0.12.0b4,test intercept__inference function works
v0.12.0b4,test summary function works
v0.12.0b4,Test inputs
v0.12.0b4,self._test_inputs(DR_learner)
v0.12.0b4,Test constant treatment effect
v0.12.0b4,Test heterogeneous treatment effect
v0.12.0b4,Test heterogenous treatment effect for W =/= None
v0.12.0b4,Sparse DGP
v0.12.0b4,Treatment effect coef
v0.12.0b4,Other coefs
v0.12.0b4,Features and controls
v0.12.0b4,Test sparse estimator
v0.12.0b4,"--> test coef_, intercept_"
v0.12.0b4,--> test treatment effects
v0.12.0b4,Restrict x_test to vectors of norm < 1
v0.12.0b4,--> check inference
v0.12.0b4,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b4,test outer grouping
v0.12.0b4,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.12.0b4,test nested grouping
v0.12.0b4,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b4,whichever groups we saw
v0.12.0b4,test nested grouping
v0.12.0b4,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b4,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b4,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b4,helper class
v0.12.0b4,Fit learner and get the effect
v0.12.0b4,Get the true treatment effect
v0.12.0b4,Compute treatment effect residuals (absolute)
v0.12.0b4,Check that at least 90% of predictions are within tolerance interval
v0.12.0b4,Only for heterogeneous TE
v0.12.0b4,Fit learner on X and W and get the effect
v0.12.0b4,Get the true treatment effect
v0.12.0b4,Compute treatment effect residuals (absolute)
v0.12.0b4,Check that at least 90% of predictions are within tolerance interval
v0.12.0b4,Check that one can pass in regular lists
v0.12.0b4,Check that it fails correctly if lists of different shape are passed in
v0.12.0b4,Check that it fails when T contains values other than 0 and 1
v0.12.0b4,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b4,Generate covariates
v0.12.0b4,Generate treatment
v0.12.0b4,Calculate outcome
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,DGP constants
v0.12.0b4,DGP coefficients
v0.12.0b4,Generated outcomes
v0.12.0b4,################
v0.12.0b4,WeightedLasso #
v0.12.0b4,################
v0.12.0b4,Define weights
v0.12.0b4,Define extended datasets
v0.12.0b4,Range of alphas
v0.12.0b4,Compare with Lasso
v0.12.0b4,--> No intercept
v0.12.0b4,--> With intercept
v0.12.0b4,When DGP has no intercept
v0.12.0b4,When DGP has intercept
v0.12.0b4,--> Coerce coefficients to be positive
v0.12.0b4,--> Toggle max_iter & tol
v0.12.0b4,Define weights
v0.12.0b4,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b4,Mixed DGP scenario.
v0.12.0b4,Define extended datasets
v0.12.0b4,Define weights
v0.12.0b4,Define multioutput
v0.12.0b4,##################
v0.12.0b4,WeightedLassoCV #
v0.12.0b4,##################
v0.12.0b4,Define alphas to test
v0.12.0b4,Compare with LassoCV
v0.12.0b4,--> No intercept
v0.12.0b4,--> With intercept
v0.12.0b4,--> Force parameters to be positive
v0.12.0b4,Choose a smaller n to speed-up process
v0.12.0b4,Compare fold weights
v0.12.0b4,Define weights
v0.12.0b4,Define extended datasets
v0.12.0b4,Define splitters
v0.12.0b4,WeightedKFold splitter
v0.12.0b4,Map weighted splitter to an extended splitter
v0.12.0b4,Define alphas to test
v0.12.0b4,Compare with LassoCV
v0.12.0b4,--> No intercept
v0.12.0b4,--> With intercept
v0.12.0b4,--> Force parameters to be positive
v0.12.0b4,###########################
v0.12.0b4,MultiTaskWeightedLassoCV #
v0.12.0b4,###########################
v0.12.0b4,Define alphas to test
v0.12.0b4,Define splitter
v0.12.0b4,Compare with MultiTaskLassoCV
v0.12.0b4,--> No intercept
v0.12.0b4,--> With intercept
v0.12.0b4,Define weights
v0.12.0b4,Define extended datasets
v0.12.0b4,Define splitters
v0.12.0b4,WeightedKFold splitter
v0.12.0b4,Map weighted splitter to an extended splitter
v0.12.0b4,Define alphas to test
v0.12.0b4,Compare with LassoCV
v0.12.0b4,--> No intercept
v0.12.0b4,--> With intercept
v0.12.0b4,#########################
v0.12.0b4,WeightedLassoCVWrapper #
v0.12.0b4,#########################
v0.12.0b4,perform 1D fit
v0.12.0b4,perform 2D fit
v0.12.0b4,################
v0.12.0b4,DebiasedLasso #
v0.12.0b4,################
v0.12.0b4,Test DebiasedLasso without weights
v0.12.0b4,--> Check debiased coeffcients without intercept
v0.12.0b4,--> Check debiased coeffcients with intercept
v0.12.0b4,--> Check 5-95 CI coverage for unit vectors
v0.12.0b4,Test DebiasedLasso with weights for one DGP
v0.12.0b4,Define weights
v0.12.0b4,Define extended datasets
v0.12.0b4,--> Check debiased coefficients
v0.12.0b4,Define weights
v0.12.0b4,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b4,--> Check debiased coeffcients
v0.12.0b4,Test that attributes propagate correctly
v0.12.0b4,Test MultiOutputDebiasedLasso without weights
v0.12.0b4,--> Check debiased coeffcients without intercept
v0.12.0b4,--> Check debiased coeffcients with intercept
v0.12.0b4,--> Check CI coverage
v0.12.0b4,Test MultiOutputDebiasedLasso with weights
v0.12.0b4,Define weights
v0.12.0b4,Define extended datasets
v0.12.0b4,--> Check debiased coefficients
v0.12.0b4,Unit vectors
v0.12.0b4,Unit vectors
v0.12.0b4,Check coeffcients and intercept are the same within tolerance
v0.12.0b4,Check results are similar with tolerance 1e-6
v0.12.0b4,Check if multitask
v0.12.0b4,Check that same alpha is chosen
v0.12.0b4,Check that the coefficients are similar
v0.12.0b4,selective ridge has a simple implementation that we can test against
v0.12.0b4,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.12.0b4,"it should be the case that when we set fit_intercept to true,"
v0.12.0b4,it doesn't matter whether the penalized model also fits an intercept or not
v0.12.0b4,create an extra copy of rows with weight 2
v0.12.0b4,"instead of a slice, explicitly return an array of indices"
v0.12.0b4,_penalized_inds is only set during fitting
v0.12.0b4,cv exists on penalized model
v0.12.0b4,now we can access _penalized_inds
v0.12.0b4,check that we can read the cv attribute back out from the underlying model
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b4,local index should have as many times entries as global as there were rows passed in
v0.12.0b4,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b4,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b4,policy value should exceed always treating with any treatment
v0.12.0b4,"global shape is (d_y, sum(d_t))"
v0.12.0b4,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b4,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b4,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b4,features; for categoricals they should appear #cats-1 times each
v0.12.0b4,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b4,local index should have as many times entries as global as there were rows passed in
v0.12.0b4,features; for categoricals they should appear #cats-1 times each
v0.12.0b4,"global shape is (d_y, sum(d_t))"
v0.12.0b4,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b4,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b4,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b4,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b4,policy value should exceed always treating with any treatment
v0.12.0b4,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b4,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b4,local index should have as many times entries as global as there were rows passed in
v0.12.0b4,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b4,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b4,policy value should exceed always treating with any treatment
v0.12.0b4,"global shape is (d_y, sum(d_t))"
v0.12.0b4,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b4,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b4,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b4,features; for categoricals they should appear #cats-1 times each
v0.12.0b4,make sure we don't run into problems dropping every index
v0.12.0b4,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b4,local index should have as many times entries as global as there were rows passed in
v0.12.0b4,"global shape is (d_y, sum(d_t))"
v0.12.0b4,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b4,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b4,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b4,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b4,policy value should exceed always treating with any treatment
v0.12.0b4,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b4,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b4,local index should have as many times entries as global as there were rows passed in
v0.12.0b4,features; for categoricals they should appear #cats-1 times each
v0.12.0b4,"global shape is (d_y, sum(d_t))"
v0.12.0b4,global and cohort row-wise dicts have d_y * d_t entries
v0.12.0b4,local dictionary is flattened to n_rows * d_y * d_t
v0.12.0b4,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b4,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b4,policy value should exceed always treating with any treatment
v0.12.0b4,dgp
v0.12.0b4,model
v0.12.0b4,model
v0.12.0b4,"columns 'd', 'e', 'h' have too many values"
v0.12.0b4,"columns 'd', 'e' have too many values"
v0.12.0b4,lowering bound shouldn't affect already fit columns when warm starting
v0.12.0b4,"column d is now okay, too"
v0.12.0b4,verify that we can use a scalar treatment cost
v0.12.0b4,verify that we can specify per-treatment costs for each sample
v0.12.0b4,verify that using the same state returns the same results each time
v0.12.0b4,set the categories for column 'd' explicitly so that b is default
v0.12.0b4,"first column: 10 ones, this is fine"
v0.12.0b4,"second column: 6 categories, plenty of random instances of each"
v0.12.0b4,this is fine only if we increase the cateogry limit
v0.12.0b4,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.12.0b4,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.12.0b4,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.12.0b4,forest heterogeneity won't work
v0.12.0b4,"sixth column: just 1 one, not enough even without check"
v0.12.0b4,increase bound on cat expansion
v0.12.0b4,skip checks (reducing folds accordingly)
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,DGP constants
v0.12.0b4,Define data features
v0.12.0b4,Added `_df`to names to be different from the default cate_estimator names
v0.12.0b4,Generate data
v0.12.0b4,################################
v0.12.0b4,Single treatment and outcome #
v0.12.0b4,################################
v0.12.0b4,Test LinearDML
v0.12.0b4,|--> Test featurizers
v0.12.0b4,ColumnTransformer doesn't propagate column names
v0.12.0b4,|--> Test re-fit
v0.12.0b4,Test SparseLinearDML
v0.12.0b4,Test ForestDML
v0.12.0b4,###################################
v0.12.0b4,Mutiple treatments and outcomes #
v0.12.0b4,###################################
v0.12.0b4,Test LinearDML
v0.12.0b4,Test SparseLinearDML
v0.12.0b4,"Single outcome only, ORF does not support multiple outcomes"
v0.12.0b4,Test DMLOrthoForest
v0.12.0b4,Test DROrthoForest
v0.12.0b4,Test XLearner
v0.12.0b4,Skipping population summary names test because bootstrap inference is too slow
v0.12.0b4,Test SLearner
v0.12.0b4,Test TLearner
v0.12.0b4,Test LinearDRLearner
v0.12.0b4,Test SparseLinearDRLearner
v0.12.0b4,Test ForestDRLearner
v0.12.0b4,Test LinearIntentToTreatDRIV
v0.12.0b4,Test DeepIV
v0.12.0b4,Test categorical treatments
v0.12.0b4,Check refit
v0.12.0b4,Check refit after setting categories
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Linear models are required for parametric dml
v0.12.0b4,sample weighting models are required for nonparametric dml
v0.12.0b4,Test values
v0.12.0b4,TLearner test
v0.12.0b4,Instantiate TLearner
v0.12.0b4,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b4,Test constant treatment effect with multi output Y
v0.12.0b4,Test heterogeneous treatment effect
v0.12.0b4,Need interactions between T and features
v0.12.0b4,Test heterogeneous treatment effect with multi output Y
v0.12.0b4,Instantiate DomainAdaptationLearner
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,test base values equals to mean of constant marginal effect
v0.12.0b4,test shape of shap values output is as expected
v0.12.0b4,test shape of attribute of explanation object is as expected
v0.12.0b4,test length of feature names equals to shap values shape
v0.12.0b4,test base values equals to mean of constant marginal effect
v0.12.0b4,test shape of shap values output is as expected
v0.12.0b4,test shape of attribute of explanation object is as expected
v0.12.0b4,test length of feature names equals to shap values shape
v0.12.0b4,Treatment effect function
v0.12.0b4,Outcome support
v0.12.0b4,Treatment support
v0.12.0b4,"Generate controls, covariates, treatments and outcomes"
v0.12.0b4,Heterogeneous treatment effects
v0.12.0b4,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.12.0b4,through shap package.
v0.12.0b4,test shap could generate the plot from the shap_values
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Check inputs
v0.12.0b4,Check inputs
v0.12.0b4,Check inputs
v0.12.0b4,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.12.0b4,"Thus, we append the controls column before the one-hot-encoded T"
v0.12.0b4,"We might want to revisit, though, since it's linearly determined by the others"
v0.12.0b4,Check inputs
v0.12.0b4,Check inputs
v0.12.0b4,Estimate response function
v0.12.0b4,Check inputs
v0.12.0b4,Train model on controls. Assign higher weight to units resembling
v0.12.0b4,treated units.
v0.12.0b4,Train model on the treated. Assign higher weight to units resembling
v0.12.0b4,control units.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.12.0b4,output is
v0.12.0b4,"* a column of ones if X, W, and Z are all None"
v0.12.0b4,* just X or W or Z if both of the others are None
v0.12.0b4,* hstack([arrs]) for whatever subset are not None otherwise
v0.12.0b4,ensure Z is 2D
v0.12.0b4,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b4,We need to go back to the label representation of the one-hot so as to call
v0.12.0b4,the classifier.
v0.12.0b4,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b4,We need to go back to the label representation of the one-hot so as to call
v0.12.0b4,the classifier.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,TODO: make sure to use random seeds wherever necessary
v0.12.0b4,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.12.0b4,"unfortunately with the Theano and Tensorflow backends,"
v0.12.0b4,the straightforward use of K.stop_gradient can cause an error
v0.12.0b4,because the parameters of the intermediate layers are now disconnected from the loss;
v0.12.0b4,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.12.0b4,so that those layers remain connected but with 0 gradient
v0.12.0b4,|| t - mu_i || ^2
v0.12.0b4,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.12.0b4,Use logsumexp for numeric stability:
v0.12.0b4,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.12.0b4,TODO: does the numeric stability actually make any difference?
v0.12.0b4,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.12.0b4,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.12.0b4,generate cumulative sum via matrix multiplication
v0.12.0b4,"Generate standard uniform values in shape (batch_size,1)"
v0.12.0b4,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.12.0b4,we use uniform_like instead with an input of an appropriate shape)
v0.12.0b4,convert to floats and multiply to perform equivalent of logical AND
v0.12.0b4,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.12.0b4,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.12.0b4,we use normal_like instead with an input of an appropriate shape)
v0.12.0b4,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.12.0b4,prevent gradient from passing through sampling
v0.12.0b4,three options: biased or upper-bound loss require a single number of samples;
v0.12.0b4,unbiased can take different numbers for the network and its gradient
v0.12.0b4,"sample: (() -> Layer, int) -> Layer"
v0.12.0b4,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.12.0b4,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.12.0b4,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.12.0b4,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.12.0b4,the dimensionality of the output of the network
v0.12.0b4,TODO: is there a more robust way to do this?
v0.12.0b4,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b4,"subtle point: we need to build a new model each time,"
v0.12.0b4,because each model encapsulates its randomness
v0.12.0b4,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b4,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.12.0b4,not a general tensor (because of how backprop works in every framework)
v0.12.0b4,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.12.0b4,but this seems annoying...)
v0.12.0b4,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.12.0b4,TODO: any way to get this to work on batches of arbitrary size?
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b4,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b4,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b4,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b4,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b4,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.12.0b4,"instruments, and outcomes"
v0.12.0b4,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b4,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b4,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b4,for internal use by the library
v0.12.0b4,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b4,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b4,since it expects raw values
v0.12.0b4,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b4,since it expects raw values
v0.12.0b4,"TODO: check that Y, T, Z do not have multiple columns"
v0.12.0b4,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b4,TODO: do correct adjustment for sample_var
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.12.0b4,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.12.0b4,since we would actually be calculating a CATE rather than ATE.
v0.12.0b4,TODO: allow the final model to actually use X?
v0.12.0b4,TODO: allow the final model to actually use X?
v0.12.0b4,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b4,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.12.0b4,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.12.0b4,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b4,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b4,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b4,for internal use by the library
v0.12.0b4,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,"this will have dimension (d,) + shape(X)"
v0.12.0b4,send the first dimension to the end
v0.12.0b4,columns are featurized independently; partial derivatives are only non-zero
v0.12.0b4,when taken with respect to the same column each time
v0.12.0b4,don't fit intercept; manually add column of ones to the data instead;
v0.12.0b4,this allows us to ignore the intercept when computing marginal effects
v0.12.0b4,make T 2D if if was a vector
v0.12.0b4,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.12.0b4,two stage approximation
v0.12.0b4,"first, get basis expansions of T, X, and Z"
v0.12.0b4,TODO: is it right that the effective number of intruments is the
v0.12.0b4,"product of ft_X and ft_Z, not just ft_Z?"
v0.12.0b4,"regress T expansion on X,Z expansions concatenated with W"
v0.12.0b4,"predict ft_T from interacted ft_X, ft_Z"
v0.12.0b4,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.12.0b4,dT may be only 2-dimensional)
v0.12.0b4,promote dT to 3D if necessary (e.g. if T was a vector)
v0.12.0b4,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,TODO: this utility is documented but internal; reimplement?
v0.12.0b4,TODO: this utility is even less public...
v0.12.0b4,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.12.0b4,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.12.0b4,but also supports get_feature_names with expected signature
v0.12.0b4,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.12.0b4,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.12.0b4,Convert SingleTreeInterpreter to a python dictionary
v0.12.0b4,named tuple type for storing results inside CausalAnalysis class;
v0.12.0b4,must be lifted to module level to enable pickling
v0.12.0b4,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.12.0b4,Controls are all other columns of X
v0.12.0b4,"can't use X[:, feat_ind] when X is a DataFrame"
v0.12.0b4,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.12.0b4,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.12.0b4,so that the user can opt-in to allowing unseen treatment values
v0.12.0b4,(and return NaN or something in that case)
v0.12.0b4,array checking routines don't accept 0-width arrays
v0.12.0b4,perform model selection
v0.12.0b4,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.12.0b4,convert to NormalInferenceResults for consistency
v0.12.0b4,Set the dictionary values shared between local and global summaries
v0.12.0b4,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.12.0b4,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.12.0b4,required to fit a discrete DML model
v0.12.0b4,Validate inputs
v0.12.0b4,TODO: check compatibility of X and Y lengths
v0.12.0b4,"no previous fit, cancel warm start"
v0.12.0b4,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.12.0b4,"if heterogeneity_inds is 1D, repeat it"
v0.12.0b4,heterogeneity inds should be a 2D list of length same as train_inds
v0.12.0b4,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.12.0b4,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.12.0b4,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.12.0b4,train the Y model
v0.12.0b4,"perform model selection for the Y model using all X, not on a per-column basis"
v0.12.0b4,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.12.0b4,work with the regression wrapper
v0.12.0b4,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.12.0b4,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.12.0b4,since otherwise we'll have too many columns to be able to train a classifier
v0.12.0b4,start with empty results and default shared insights
v0.12.0b4,convert categorical indicators to numeric indices
v0.12.0b4,check for indices over the categorical expansion bound
v0.12.0b4,assume we'll be able to train former failures this time; we'll add them back if not
v0.12.0b4,"can't remove in place while iterating over new_inds, so store in separate list"
v0.12.0b4,"train the model, but warn"
v0.12.0b4,no model can be trained in this case since we need more folds
v0.12.0b4,"don't train a model, but suggest workaround since there are enough instances of least"
v0.12.0b4,populated class
v0.12.0b4,also remove from train_inds so we don't try to access the result later
v0.12.0b4,extract subset of names matching new columns
v0.12.0b4,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.12.0b4,don't want to cache this failed result
v0.12.0b4,properties to return from effect InferenceResults
v0.12.0b4,properties to return from PopulationSummaryResults
v0.12.0b4,Converts strings to property lookups or method calls as a convenience so that the
v0.12.0b4,_point_props and _summary_props above can be applied to an inference object
v0.12.0b4,Create a summary combining all results into a single output; this is used
v0.12.0b4,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.12.0b4,"or a dictionary, respectively, based on the summary function passed into this method"
v0.12.0b4,"ensure array has shape (m,y,t)"
v0.12.0b4,population summary is missing sample dimension; add it for consistency
v0.12.0b4,outcome dimension is missing; add it for consistency
v0.12.0b4,add singleton treatment dimension if missing
v0.12.0b4,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.12.0b4,"each attr has dimension (m,y) or (m,y,t)"
v0.12.0b4,concatenate along treatment dimension
v0.12.0b4,"for dictionary representation, want to remove unneeded sample dimension"
v0.12.0b4,in cohort and global results
v0.12.0b4,TODO: enrich outcome logic for multi-class classification when that is supported
v0.12.0b4,There is no actual sample level in this data
v0.12.0b4,can't drop only level
v0.12.0b4,should be serialization-ready and contain no numpy arrays
v0.12.0b4,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.12.0b4,TODO: Note that there's no column metadata for the sample number - should there be?
v0.12.0b4,"need to replicate the column info for each sample, then remove from the shared data"
v0.12.0b4,NOTE: the flattened order has the ouptut dimension before the feature dimension
v0.12.0b4,which may need to be revisited once we support multiclass
v0.12.0b4,get the length of the list corresponding to the first dictionary key
v0.12.0b4,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.12.0b4,a global inference indicates the effect of that one feature on the outcome
v0.12.0b4,need to reshape the output to match the input
v0.12.0b4,we want to offset the inference object by the baseline estimate of y
v0.12.0b4,"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
v0.12.0b4,get the length of the list corresponding to the first dictionary key
v0.12.0b4,"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
v0.12.0b4,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.12.0b4,because then scaling the difference between treatment value and treatment costs is the
v0.12.0b4,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.12.0b4,
v0.12.0b4,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.12.0b4,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.12.0b4,(rather than just not treating at all)
v0.12.0b4,
v0.12.0b4,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.12.0b4,2 * policy_value - always_treat
v0.12.0b4,includes exactly their contribution because policy_value and always_treat both include it
v0.12.0b4,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.12.0b4,2 * policy_value - always-treat
v0.12.0b4,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.12.0b4,is zero and the contribution to always_treat is negative
v0.12.0b4,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b4,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b4,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b4,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b4,get dataframe with all but selected column
v0.12.0b4,apply 10% of a typical treatment for this feature
v0.12.0b4,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.12.0b4,set the effect bounds; for positive treatments these agree with
v0.12.0b4,"the estimates; for negative treatments, we need to invert the interval"
v0.12.0b4,the effect is now always positive since we decrease treatment when negative
v0.12.0b4,"for discrete treatment, stack a zero result in front for control"
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,TODO: conisder working around relying on sklearn implementation details
v0.12.0b4,"Found a good split, return."
v0.12.0b4,Record all splits in case the stratification by weight yeilds a worse partition
v0.12.0b4,Reseed random generator and try again
v0.12.0b4,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.12.0b4,"Found a good split, return."
v0.12.0b4,Did not find a good split
v0.12.0b4,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.12.0b4,Return most weight-balanced partition
v0.12.0b4,Weight stratification algorithm
v0.12.0b4,Sort weights for weight strata search
v0.12.0b4,There are some leftover indices that have yet to be assigned
v0.12.0b4,Append stratum splits to overall splits
v0.12.0b4,"If classification methods produce multiple columns of output,"
v0.12.0b4,we need to manually encode classes to ensure consistent column ordering.
v0.12.0b4,We clone the estimator to make sure that all the folds are
v0.12.0b4,"independent, and that it is pickle-able."
v0.12.0b4,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.12.0b4,`predictions` is a list of method outputs from each fold.
v0.12.0b4,"If each of those is also a list, then treat this as a"
v0.12.0b4,multioutput-multiclass task. We need to separately concatenate
v0.12.0b4,the method outputs for each label into an `n_labels` long list.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Our classes that derive from sklearn ones sometimes include
v0.12.0b4,inherited docstrings that have embedded doctests; we need the following imports
v0.12.0b4,so that they don't break.
v0.12.0b4,TODO: consider working around relying on sklearn implementation details
v0.12.0b4,"Convert X, y into numpy arrays"
v0.12.0b4,Define fit parameters
v0.12.0b4,Some algorithms don't have a check_input option
v0.12.0b4,Check weights array
v0.12.0b4,Check that weights are size-compatible
v0.12.0b4,Normalize inputs
v0.12.0b4,Weight inputs
v0.12.0b4,Fit base class without intercept
v0.12.0b4,Fit Lasso
v0.12.0b4,Reset intercept
v0.12.0b4,The intercept is not calculated properly due the sqrt(weights) factor
v0.12.0b4,so it must be recomputed
v0.12.0b4,Fit lasso without weights
v0.12.0b4,Make weighted splitter
v0.12.0b4,Fit weighted model
v0.12.0b4,Make weighted splitter
v0.12.0b4,Fit weighted model
v0.12.0b4,Call weighted lasso on reduced design matrix
v0.12.0b4,Weighted tau
v0.12.0b4,Select optimal penalty
v0.12.0b4,Warn about consistency
v0.12.0b4,"Convert X, y into numpy arrays"
v0.12.0b4,Fit weighted lasso with user input
v0.12.0b4,"Center X, y"
v0.12.0b4,Calculate quantities that will be used later on. Account for centered data
v0.12.0b4,Calculate coefficient and error variance
v0.12.0b4,Add coefficient correction
v0.12.0b4,Set coefficients and intercept standard errors
v0.12.0b4,Set intercept
v0.12.0b4,Return alpha to 'auto' state
v0.12.0b4,"Note that in the case of no intercept, X_offset is 0"
v0.12.0b4,Calculate the variance of the predictions
v0.12.0b4,Calculate prediction confidence intervals
v0.12.0b4,Assumes flattened y
v0.12.0b4,Compute weighted residuals
v0.12.0b4,To be done once per target. Assumes y can be flattened.
v0.12.0b4,Assumes that X has already been offset
v0.12.0b4,Special case: n_features=1
v0.12.0b4,Compute Lasso coefficients for the columns of the design matrix
v0.12.0b4,Compute C_hat
v0.12.0b4,Compute theta_hat
v0.12.0b4,Allow for single output as well
v0.12.0b4,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.12.0b4,Set coef_ attribute
v0.12.0b4,Set intercept_ attribute
v0.12.0b4,Set selected_alpha_ attribute
v0.12.0b4,Set coef_stderr_
v0.12.0b4,intercept_stderr_
v0.12.0b4,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.12.0b4,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.12.0b4,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.12.0b4,set intercept_ attribute
v0.12.0b4,set coef_ attribute
v0.12.0b4,set alpha_ attribute
v0.12.0b4,set alphas_ attribute
v0.12.0b4,set n_iter_ attribute
v0.12.0b4,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.12.0b4,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.12.0b4,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.12.0b4,now regress X1 on y - X2 * beta2 to learn beta1
v0.12.0b4,set coef_ and intercept_ attributes
v0.12.0b4,Note that the penalized model should *not* have an intercept
v0.12.0b4,don't proxy special methods
v0.12.0b4,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.12.0b4,regressor incorrectly
v0.12.0b4,"Note: for known attributes that have been set this method will not be called,"
v0.12.0b4,so we should just throw here because this is an attribute belonging to this class
v0.12.0b4,but which hasn't yet been set on this instance
v0.12.0b4,set default values for None
v0.12.0b4,check freq_weight should be integer and should be accompanied by sample_var
v0.12.0b4,check array shape
v0.12.0b4,weight X and y and sample_var
v0.12.0b4,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,AzureML
v0.12.0b4,helper imports
v0.12.0b4,write the details of the workspace to a configuration file to the notebook library
v0.12.0b4,if y is a multioutput model
v0.12.0b4,Make sure second dimension has 1 or more item
v0.12.0b4,switch _inner Model to a MultiOutputRegressor
v0.12.0b4,flatten array as automl only takes vectors for y
v0.12.0b4,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.12.0b4,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.12.0b4,as an sklearn estimator
v0.12.0b4,fit implementation for a single output model.
v0.12.0b4,Create experiment for specified workspace
v0.12.0b4,Configure automl_config with training set information.
v0.12.0b4,"Wait for remote run to complete, the set the model"
v0.12.0b4,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.12.0b4,create model and pass model into final.
v0.12.0b4,"If item is an automl config, get its corresponding"
v0.12.0b4,AutomatedML Model and add it to new_Args
v0.12.0b4,"If item is an automl config, get its corresponding"
v0.12.0b4,AutomatedML Model and set it for this key in
v0.12.0b4,kwargs
v0.12.0b4,takes in either automated_ml config and instantiates
v0.12.0b4,an AutomatedMLModel
v0.12.0b4,The prefix can only be 18 characters long
v0.12.0b4,"because prefixes come from kwarg_names, we must ensure they are"
v0.12.0b4,short enough.
v0.12.0b4,Get workspace from config file.
v0.12.0b4,Take the intersect of the white for sample
v0.12.0b4,weights and linear models
v0.12.0b4,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,average the outcome dimension if it exists and ensure 2d y_pred
v0.12.0b4,get index of best treatment
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,TODO: consider working around relying on sklearn implementation details
v0.12.0b4,Create splits of causal tree
v0.12.0b4,Make sure the correct exception is being rethrown
v0.12.0b4,Must make sure indices are merged correctly
v0.12.0b4,Convert rows to columns
v0.12.0b4,Require group assignment t to be one-hot-encoded
v0.12.0b4,Get predictions for the 2 splits
v0.12.0b4,Must make sure indices are merged correctly
v0.12.0b4,Crossfitting
v0.12.0b4,Compute weighted nuisance estimates
v0.12.0b4,-------------------------------------------------------------------------------
v0.12.0b4,Calculate the covariance matrix corresponding to the BLB inference
v0.12.0b4,
v0.12.0b4,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.12.0b4,2. Calculate the weighted moments for each tree slice to create a matrix
v0.12.0b4,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.12.0b4,in that slice from the overall parameter estimate.
v0.12.0b4,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.12.0b4,-------------------------------------------------------------------------------
v0.12.0b4,Calclulate covariance matrix through BLB
v0.12.0b4,Estimators
v0.12.0b4,OrthoForest parameters
v0.12.0b4,Sub-forests
v0.12.0b4,Auxiliary attributes
v0.12.0b4,Fit check
v0.12.0b4,TODO: Check performance
v0.12.0b4,Must normalize weights
v0.12.0b4,Override the CATE inference options
v0.12.0b4,Add blb inference to parent's options
v0.12.0b4,Generate subsample indices
v0.12.0b4,Build trees in parallel
v0.12.0b4,Bootstraping has repetitions in tree sample
v0.12.0b4,Similar for `a` weights
v0.12.0b4,Bootstraping has repetitions in tree sample
v0.12.0b4,Define subsample size
v0.12.0b4,Safety check
v0.12.0b4,Draw points to create little bags
v0.12.0b4,Copy and/or define models
v0.12.0b4,Define nuisance estimators
v0.12.0b4,Define parameter estimators
v0.12.0b4,Define
v0.12.0b4,Need to redefine fit here for auto inference to work due to a quirk in how
v0.12.0b4,wrap_fit is defined
v0.12.0b4,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b4,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b4,Override to flatten output if T is flat
v0.12.0b4,Check that all discrete treatments are represented
v0.12.0b4,Nuissance estimates evaluated with cross-fitting
v0.12.0b4,Define 2-fold iterator
v0.12.0b4,Check if there is only one example of some class
v0.12.0b4,Define 2-fold iterator
v0.12.0b4,need safe=False when cloning for WeightedModelWrapper
v0.12.0b4,Compute residuals
v0.12.0b4,Compute coefficient by OLS on residuals
v0.12.0b4,"Parameter returned by LinearRegression is (d_T, )"
v0.12.0b4,Compute residuals
v0.12.0b4,Compute coefficient by OLS on residuals
v0.12.0b4,ell_2 regularization
v0.12.0b4,Ridge regression estimate
v0.12.0b4,"Parameter returned is of shape (d_T, )"
v0.12.0b4,Return moments and gradients
v0.12.0b4,Compute residuals
v0.12.0b4,Compute moments
v0.12.0b4,"Moments shape is (n, d_T)"
v0.12.0b4,Compute moment gradients
v0.12.0b4,returns shape-conforming residuals
v0.12.0b4,Copy and/or define models
v0.12.0b4,Define parameter estimators
v0.12.0b4,Define moment and mean gradient estimator
v0.12.0b4,"Check that T is shape (n, )"
v0.12.0b4,Check T is numeric
v0.12.0b4,Train label encoder
v0.12.0b4,Call `fit` from parent class
v0.12.0b4,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b4,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b4,Override to flatten output if T is flat
v0.12.0b4,Expand one-hot encoding to include the zero treatment
v0.12.0b4,"Test that T contains all treatments. If not, return None"
v0.12.0b4,Nuissance estimates evaluated with cross-fitting
v0.12.0b4,Define 2-fold iterator
v0.12.0b4,Check if there is only one example of some class
v0.12.0b4,No need to crossfit for internal nodes
v0.12.0b4,Compute partial moments
v0.12.0b4,"If any of the values in the parameter estimate is nan, return None"
v0.12.0b4,Compute partial moments
v0.12.0b4,Compute coefficient by OLS on residuals
v0.12.0b4,ell_2 regularization
v0.12.0b4,Ridge regression estimate
v0.12.0b4,"Parameter returned is of shape (d_T, )"
v0.12.0b4,Return moments and gradients
v0.12.0b4,Compute partial moments
v0.12.0b4,Compute moments
v0.12.0b4,"Moments shape is (n, d_T-1)"
v0.12.0b4,Compute moment gradients
v0.12.0b4,Need to calculate this in an elegant way for when propensity is 0
v0.12.0b4,This will flatten T
v0.12.0b4,Check that T is numeric
v0.12.0b4,Test whether the input estimator is supported
v0.12.0b4,Calculate confidence intervals for the parameter (marginal effect)
v0.12.0b4,Calculate confidence intervals for the effect
v0.12.0b4,Calculate the effects
v0.12.0b4,Calculate the standard deviations for the effects
v0.12.0b4,d_t=None here since we measure the effect across all Ts
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b4,Licensed under the MIT License.
v0.12.0b4,Causal tree parameters
v0.12.0b4,Tree structure
v0.12.0b4,No need for a random split since the data is already
v0.12.0b4,a random subsample from the original input
v0.12.0b4,node list stores the nodes that are yet to be splitted
v0.12.0b4,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b4,Create local sample set
v0.12.0b4,Compute nuisance estimates for the current node
v0.12.0b4,Nuisance estimate cannot be calculated
v0.12.0b4,Estimate parameter for current node
v0.12.0b4,Node estimate cannot be calculated
v0.12.0b4,Calculate moments and gradient of moments for current data
v0.12.0b4,Calculate inverse gradient
v0.12.0b4,The gradient matrix is not invertible.
v0.12.0b4,No good split can be found
v0.12.0b4,Calculate point-wise pseudo-outcomes rho
v0.12.0b4,a split is determined by a feature and a sample pair
v0.12.0b4,the number of possible splits is at most (number of features) * (number of node samples)
v0.12.0b4,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.12.0b4,parse row and column of random pair
v0.12.0b4,the sample of the pair is the integer division of the random number with n_feats
v0.12.0b4,calculate the binary indicator of whether sample i is on the left or the right
v0.12.0b4,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.12.0b4,calculate the number of samples on the left child for each proposed split
v0.12.0b4,calculate the analogous binary indicator for the samples in the estimation set
v0.12.0b4,calculate the number of estimation samples on the left child of each proposed split
v0.12.0b4,find the upper and lower bound on the size of the left split for the split
v0.12.0b4,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.12.0b4,on each side.
v0.12.0b4,similarly for the estimation sample set
v0.12.0b4,if there is no valid split then don't create any children
v0.12.0b4,filter only the valid splits
v0.12.0b4,calculate the average influence vector of the samples in the left child
v0.12.0b4,calculate the average influence vector of the samples in the right child
v0.12.0b4,take the square of each of the entries of the influence vectors and normalize
v0.12.0b4,by size of each child
v0.12.0b4,calculate the vector score of each candidate split as the average of left and right
v0.12.0b4,influence vectors
v0.12.0b4,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.12.0b4,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.12.0b4,where there might be large discontinuities in some parameter as the conditioning set varies
v0.12.0b4,calculate the scalar score of each split by aggregating across the vector of scores
v0.12.0b4,Find split that minimizes criterion
v0.12.0b4,Create child nodes with corresponding subsamples
v0.12.0b4,add the created children to the list of not yet split nodes
v0.12.0b3,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.12.0b3,configuration is all pulled from setup.cfg
v0.12.0b3,-*- coding: utf-8 -*-
v0.12.0b3,
v0.12.0b3,Configuration file for the Sphinx documentation builder.
v0.12.0b3,
v0.12.0b3,This file does only contain a selection of the most common options. For a
v0.12.0b3,full list see the documentation:
v0.12.0b3,http://www.sphinx-doc.org/en/master/config
v0.12.0b3,-- Path setup --------------------------------------------------------------
v0.12.0b3,"If extensions (or modules to document with autodoc) are in another directory,"
v0.12.0b3,add these directories to sys.path here. If the directory is relative to the
v0.12.0b3,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.12.0b3,
v0.12.0b3,-- Project information -----------------------------------------------------
v0.12.0b3,-- General configuration ---------------------------------------------------
v0.12.0b3,"If your documentation needs a minimal Sphinx version, state it here."
v0.12.0b3,
v0.12.0b3,needs_sphinx = '1.0'
v0.12.0b3,"Add any Sphinx extension module names here, as strings. They can be"
v0.12.0b3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.12.0b3,ones.
v0.12.0b3,"Add any paths that contain templates here, relative to this directory."
v0.12.0b3,The suffix(es) of source filenames.
v0.12.0b3,You can specify multiple suffix as a list of string:
v0.12.0b3,
v0.12.0b3,"source_suffix = ['.rst', '.md']"
v0.12.0b3,The master toctree document.
v0.12.0b3,The language for content autogenerated by Sphinx. Refer to documentation
v0.12.0b3,for a list of supported languages.
v0.12.0b3,
v0.12.0b3,This is also used if you do content translation via gettext catalogs.
v0.12.0b3,"Usually you set ""language"" from the command line for these cases."
v0.12.0b3,"List of patterns, relative to source directory, that match files and"
v0.12.0b3,directories to ignore when looking for source files.
v0.12.0b3,This pattern also affects html_static_path and html_extra_path.
v0.12.0b3,The name of the Pygments (syntax highlighting) style to use.
v0.12.0b3,-- Options for HTML output -------------------------------------------------
v0.12.0b3,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.12.0b3,a list of builtin themes.
v0.12.0b3,
v0.12.0b3,Theme options are theme-specific and customize the look and feel of a theme
v0.12.0b3,"further.  For a list of options available for each theme, see the"
v0.12.0b3,documentation.
v0.12.0b3,
v0.12.0b3,"Add any paths that contain custom static files (such as style sheets) here,"
v0.12.0b3,"relative to this directory. They are copied after the builtin static files,"
v0.12.0b3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.12.0b3,html_static_path = ['_static']
v0.12.0b3,"Custom sidebar templates, must be a dictionary that maps document names"
v0.12.0b3,to template names.
v0.12.0b3,
v0.12.0b3,The default sidebars (for documents that don't match any pattern) are
v0.12.0b3,defined by theme itself.  Builtin themes are using these templates by
v0.12.0b3,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.12.0b3,'searchbox.html']``.
v0.12.0b3,
v0.12.0b3,html_sidebars = {}
v0.12.0b3,-- Options for HTMLHelp output ---------------------------------------------
v0.12.0b3,Output file base name for HTML help builder.
v0.12.0b3,-- Options for LaTeX output ------------------------------------------------
v0.12.0b3,The paper size ('letterpaper' or 'a4paper').
v0.12.0b3,
v0.12.0b3,"'papersize': 'letterpaper',"
v0.12.0b3,"The font size ('10pt', '11pt' or '12pt')."
v0.12.0b3,
v0.12.0b3,"'pointsize': '10pt',"
v0.12.0b3,Additional stuff for the LaTeX preamble.
v0.12.0b3,
v0.12.0b3,"'preamble': '',"
v0.12.0b3,Latex figure (float) alignment
v0.12.0b3,
v0.12.0b3,"'figure_align': 'htbp',"
v0.12.0b3,Grouping the document tree into LaTeX files. List of tuples
v0.12.0b3,"(source start file, target name, title,"
v0.12.0b3,"author, documentclass [howto, manual, or own class])."
v0.12.0b3,-- Options for manual page output ------------------------------------------
v0.12.0b3,One entry per manual page. List of tuples
v0.12.0b3,"(source start file, name, description, authors, manual section)."
v0.12.0b3,-- Options for Texinfo output ----------------------------------------------
v0.12.0b3,Grouping the document tree into Texinfo files. List of tuples
v0.12.0b3,"(source start file, target name, title, author,"
v0.12.0b3,"dir menu entry, description, category)"
v0.12.0b3,-- Options for Epub output -------------------------------------------------
v0.12.0b3,Bibliographic Dublin Core info.
v0.12.0b3,The unique identifier of the text. This can be a ISBN number
v0.12.0b3,or the project homepage.
v0.12.0b3,
v0.12.0b3,epub_identifier = ''
v0.12.0b3,A unique identification for the text.
v0.12.0b3,
v0.12.0b3,epub_uid = ''
v0.12.0b3,A list of files that should not be packed into the epub file.
v0.12.0b3,-- Extension configuration -------------------------------------------------
v0.12.0b3,-- Options for intersphinx extension ---------------------------------------
v0.12.0b3,Example configuration for intersphinx: refer to the Python standard library.
v0.12.0b3,-- Options for todo extension ----------------------------------------------
v0.12.0b3,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.12.0b3,-- Options for doctest extension -------------------------------------------
v0.12.0b3,we can document otherwise excluded entities here by returning False
v0.12.0b3,or skip otherwise included entities by returning True
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Calculate residuals
v0.12.0b3,Estimate E[T_res | Z_res]
v0.12.0b3,TODO. Deal with multi-class instrument
v0.12.0b3,Calculate nuisances
v0.12.0b3,Estimate E[T_res | Z_res]
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"We do a three way split, as typically a preliminary theta estimator would require"
v0.12.0b3,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.12.0b3,TODO. Deal with multi-class instrument
v0.12.0b3,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b3,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b3,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b3,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b3,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b3,Estimate preliminary theta in cross fitting manner
v0.12.0b3,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b3,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.12.0b3,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.12.0b3,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b3,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.12.0b3,TODO. The solution below is not really a valid cross-fitting
v0.12.0b3,as the test data are used to create the proj_t on the train
v0.12.0b3,which in the second train-test loop is used to create the nuisance
v0.12.0b3,cov on the test data. Hence the T variable of some sample
v0.12.0b3,"is implicitly correlated with its cov nuisance, through this flow"
v0.12.0b3,"of information. However, this seems a rather weak correlation."
v0.12.0b3,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.12.0b3,model.
v0.12.0b3,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.12.0b3,Estimate preliminary theta in cross fitting manner
v0.12.0b3,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b3,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.12.0b3,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b3,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.12.0b3,#############################################################################
v0.12.0b3,Classes for the DRIV implementation for the special case of intent-to-treat
v0.12.0b3,A/B test
v0.12.0b3,#############################################################################
v0.12.0b3,Estimate preliminary theta in cross fitting manner
v0.12.0b3,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b3,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b3,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b3,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b3,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.12.0b3,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.12.0b3,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.12.0b3,model_T_XZ = lambda: model_clf()
v0.12.0b3,#'days_visited': lambda:
v0.12.0b3,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.12.0b3,Turn strings into categories for numeric mapping
v0.12.0b3,### Defining some generic regressors and classifiers
v0.12.0b3,This a generic non-parametric regressor
v0.12.0b3,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b3,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.12.0b3,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b3,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.12.0b3,model = lambda: RandomForestRegressor(n_estimators=100)
v0.12.0b3,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.12.0b3,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.12.0b3,model = lambda: LinearRegression(n_jobs=-1)
v0.12.0b3,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.12.0b3,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.12.0b3,underlying model whenever predict is called.
v0.12.0b3,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b3,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.12.0b3,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b3,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.12.0b3,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.12.0b3,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.12.0b3,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.12.0b3,We need to specify models to be used for each of these residualizations
v0.12.0b3,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.12.0b3,"E[T | X, Z]"
v0.12.0b3,E[TZ | X]
v0.12.0b3,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.12.0b3,n_splits determines the number of splits to be used for cross-fitting.
v0.12.0b3,# Algorithm 2 - Current Method
v0.12.0b3,In[121]:
v0.12.0b3,# Algorithm 3 - DRIV ATE
v0.12.0b3,dmliv_model_effect = lambda: model()
v0.12.0b3,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.12.0b3,"dmliv_model_effect(),"
v0.12.0b3,n_splits=1)
v0.12.0b3,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.12.0b3,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.12.0b3,"Once multiple treatments are supported, we'll need to fix this"
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b3,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b3,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b3,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,TODO. Deal with multi-class instrument/treatment
v0.12.0b3,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.12.0b3,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.12.0b3,Estimate p(X) = E[T | X] in cross-fitting manner
v0.12.0b3,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.12.0b3,##################
v0.12.0b3,Global settings #
v0.12.0b3,##################
v0.12.0b3,Global plotting controls
v0.12.0b3,"Control for support size, can control for more"
v0.12.0b3,#################
v0.12.0b3,File utilities #
v0.12.0b3,#################
v0.12.0b3,#################
v0.12.0b3,Plotting utils #
v0.12.0b3,#################
v0.12.0b3,bias
v0.12.0b3,var
v0.12.0b3,rmse
v0.12.0b3,r2
v0.12.0b3,Infer feature dimension
v0.12.0b3,Metrics by support plots
v0.12.0b3,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.12.0b3,Vasilis Syrgkanis <vasy@microsoft.com>
v0.12.0b3,Steven Wu <zhiww@microsoft.com>
v0.12.0b3,Initialize causal tree parameters
v0.12.0b3,Create splits of causal tree
v0.12.0b3,Estimate treatment effects at the leafs
v0.12.0b3,Compute heterogeneous treatement effect for x's in x_list by finding
v0.12.0b3,the corresponding split and associating the effect computed on that leaf
v0.12.0b3,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.12.0b3,Safety check
v0.12.0b3,Weighted linear regression
v0.12.0b3,Calculates weights
v0.12.0b3,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b3,over all indices
v0.12.0b3,Similar for `a` weights
v0.12.0b3,Doesn't have sample weights
v0.12.0b3,Is a linear model
v0.12.0b3,Weighted linear regression
v0.12.0b3,Calculates weights
v0.12.0b3,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b3,over all indices
v0.12.0b3,Similar for `a` weights
v0.12.0b3,normalize weights
v0.12.0b3,"Split the data in half, train and test"
v0.12.0b3,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b3,"a function of W, using only the train fold"
v0.12.0b3,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b3,"Split the data in half, train and test"
v0.12.0b3,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b3,"a function of W, using only the train fold"
v0.12.0b3,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b3,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.12.0b3,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.12.0b3,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.12.0b3,"Split the data in half, train and test"
v0.12.0b3,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b3,"a function of x, using only the train fold"
v0.12.0b3,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b3,Compute coefficient by OLS on residuals
v0.12.0b3,"Split the data in half, train and test"
v0.12.0b3,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b3,"a function of x, using only the train fold"
v0.12.0b3,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b3,Estimate multipliers for second order orthogonal method
v0.12.0b3,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.12.0b3,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b3,Create local sample set
v0.12.0b3,compute the base estimate for the current node using double ml or second order double ml
v0.12.0b3,compute the influence functions here that are used for the criterion
v0.12.0b3,generate random proposals of dimensions to split
v0.12.0b3,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.12.0b3,compute criterion for each proposal
v0.12.0b3,if splitting creates valid leafs in terms of mean leaf size
v0.12.0b3,Calculate criterion for split
v0.12.0b3,Else set criterion to infinity so that this split is not chosen
v0.12.0b3,If no good split was found
v0.12.0b3,Find split that minimizes criterion
v0.12.0b3,Set the split attributes at the node
v0.12.0b3,Create child nodes with corresponding subsamples
v0.12.0b3,Recursively split children
v0.12.0b3,Return parent node
v0.12.0b3,estimate the local parameter at the leaf using the estimate data
v0.12.0b3,###################
v0.12.0b3,Argument parsing #
v0.12.0b3,###################
v0.12.0b3,#########################################
v0.12.0b3,Parameters constant across experiments #
v0.12.0b3,#########################################
v0.12.0b3,Outcome support
v0.12.0b3,Treatment support
v0.12.0b3,Evaluation grid
v0.12.0b3,Treatment effects array
v0.12.0b3,Other variables
v0.12.0b3,##########################
v0.12.0b3,Data Generating Process #
v0.12.0b3,##########################
v0.12.0b3,Log iteration
v0.12.0b3,"Generate controls, features, treatment and outcome"
v0.12.0b3,T and Y residuals to be used in later scripts
v0.12.0b3,Save generated dataset
v0.12.0b3,#################
v0.12.0b3,ORF parameters #
v0.12.0b3,#################
v0.12.0b3,######################################
v0.12.0b3,Train and evaluate treatment effect #
v0.12.0b3,######################################
v0.12.0b3,########
v0.12.0b3,Plots #
v0.12.0b3,########
v0.12.0b3,###############
v0.12.0b3,Save results #
v0.12.0b3,###############
v0.12.0b3,##############
v0.12.0b3,Run Rscript #
v0.12.0b3,##############
v0.12.0b3,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b3,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b3,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.12.0b3,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b3,def mlasso_model(): return MultiTaskLassoCV(
v0.12.0b3,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b3,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b3,heterogeneity
v0.12.0b3,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b3,heterogeneity
v0.12.0b3,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b3,heterogeneity
v0.12.0b3,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.12.0b3,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b3,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b3,subset of features that are exogenous and create heterogeneity
v0.12.0b3,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.12.0b3,subset of features wrt we estimate heterogeneity
v0.12.0b3,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b3,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,introspect the constructor arguments to find the model parameters
v0.12.0b3,to represent
v0.12.0b3,"if the argument is deprecated, ignore it"
v0.12.0b3,Extract and sort argument names excluding 'self'
v0.12.0b3,column names
v0.12.0b3,transfer input to numpy arrays
v0.12.0b3,transfer input to 2d arrays
v0.12.0b3,create dataframe
v0.12.0b3,currently dowhy only support single outcome and single treatment
v0.12.0b3,call dowhy
v0.12.0b3,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.12.0b3,cate estimator but not the effect.
v0.12.0b3,don't proxy special methods
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Check if model is sparse enough for this model
v0.12.0b3,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.12.0b3,TODO: any way to avoid creating a copy if the array was already dense?
v0.12.0b3,"the call is necessary if the input was something like a list, though"
v0.12.0b3,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.12.0b3,so convert to pydata sparse first
v0.12.0b3,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.12.0b3,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.12.0b3,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.12.0b3,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b3,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b3,For when checking input values is disabled
v0.12.0b3,Type to column extraction function
v0.12.0b3,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.12.0b3,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.12.0b3,Get feature names using featurizer
v0.12.0b3,All attempts at retrieving transformed feature names have failed
v0.12.0b3,Delegate handling to downstream logic
v0.12.0b3,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.12.0b3,same number of input definitions as arrays
v0.12.0b3,input definitions have same number of dimensions as each array
v0.12.0b3,all result indices are unique
v0.12.0b3,all result indices must match at least one input index
v0.12.0b3,"map indices to all array, axis pairs for that index"
v0.12.0b3,each index has the same cardinality wherever it appears
v0.12.0b3,"State: list of (set of letters, list of (corresponding indices, value))"
v0.12.0b3,Algo: while list contains more than one entry
v0.12.0b3,take two entries
v0.12.0b3,sort both lists by intersection of their indices
v0.12.0b3,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.12.0b3,"take the union of indices and the product of values), stepping through each list linearly"
v0.12.0b3,TODO: might be faster to break into connected components first
v0.12.0b3,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.12.0b3,"so compute their content separately, then take cartesian product"
v0.12.0b3,this would save a few pointless sorts by empty tuples
v0.12.0b3,TODO: Consider investigating other performance ideas for these cases
v0.12.0b3,where the dense method beat the sparse method (usually sparse is faster)
v0.12.0b3,"e,facd,c->cfed"
v0.12.0b3,sparse: 0.0335489
v0.12.0b3,dense:  0.011465999999999997
v0.12.0b3,"gbd,da,egb->da"
v0.12.0b3,sparse: 0.0791625
v0.12.0b3,dense:  0.007319099999999995
v0.12.0b3,"dcc,d,faedb,c->abe"
v0.12.0b3,sparse: 1.2868097
v0.12.0b3,dense:  0.44605229999999985
v0.12.0b3,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.12.0b3,TODO: would using einsum's paths to optimize the order of merging help?
v0.12.0b3,assume that we should perform nested cross-validation if and only if
v0.12.0b3,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.12.0b3,logic copied from check_cv
v0.12.0b3,otherwise we will assume the user already set the cv attribute to something
v0.12.0b3,compatible with splitting with a 'groups' argument
v0.12.0b3,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.12.0b3,don't use the groups when calling split internally
v0.12.0b3,Normalize weights
v0.12.0b3,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.12.0b3,"if we're decorating a class, just update the __init__ method,"
v0.12.0b3,so that the result is still a class instead of a wrapper method
v0.12.0b3,"want to enforce that each bad_arg was either in kwargs,"
v0.12.0b3,or else it was in neither and is just taking its default value
v0.12.0b3,Any access should throw
v0.12.0b3,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b3,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b3,input feature name is already updated by cate_feature_names.
v0.12.0b3,define the index of d_x to filter for each given T
v0.12.0b3,filter X after broadcast with T for each given T
v0.12.0b3,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b3,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,
v0.12.0b3,This code contains some snippets of code from:
v0.12.0b3,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
v0.12.0b3,published under the following license and copyright:
v0.12.0b3,BSD 3-Clause License
v0.12.0b3,
v0.12.0b3,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b3,All rights reserved.
v0.12.0b3,make any access to matplotlib or plt throw an exception
v0.12.0b3,make any access to graphviz or plt throw an exception
v0.12.0b3,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.12.0b3,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.12.0b3,Initialize saturation & value; calculate chroma & value shift
v0.12.0b3,Calculate some intermediate values
v0.12.0b3,Initialize RGB with same hue & chroma as our color
v0.12.0b3,Shift the initial RGB values to match value and store
v0.12.0b3,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.12.0b3,clean way of achieving this
v0.12.0b3,make sure we don't accidentally escape anything in the substitution
v0.12.0b3,Fetch appropriate color for node
v0.12.0b3,"red for negative, green for positive"
v0.12.0b3,in multi-target use mean of targets
v0.12.0b3,Write node mean CATE
v0.12.0b3,Write node std of CATE
v0.12.0b3,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.12.0b3,Fetch appropriate color for node
v0.12.0b3,Write node mean CATE
v0.12.0b3,Write node mean CATE
v0.12.0b3,Write recommended treatment and value - cost
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"since inference objects can be stateful, we must copy it before fitting;"
v0.12.0b3,otherwise this sequence wouldn't work:
v0.12.0b3,"est1.fit(..., inference=inf)"
v0.12.0b3,"est2.fit(..., inference=inf)"
v0.12.0b3,est1.effect_interval(...)
v0.12.0b3,because inf now stores state from fitting est2
v0.12.0b3,This flag is true when names are set in a child class instead
v0.12.0b3,"If names are set in a child class, add an attribute reflecting that"
v0.12.0b3,This works only if X is passed as a kwarg
v0.12.0b3,We plan to enforce X as kwarg only in future releases
v0.12.0b3,This checks if names have been set in a child class
v0.12.0b3,"If names were set in a child class, don't do it again"
v0.12.0b3,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.12.0b3,call the wrapped fit method
v0.12.0b3,NOTE: we call inference fit *after* calling the main fit method
v0.12.0b3,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.12.0b3,but tensordot can't be applied to this problem because we don't sum over m
v0.12.0b3,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.12.0b3,of rows of T was not taken into account
v0.12.0b3,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.12.0b3,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.12.0b3,"Treatment names is None, default to BaseCateEstimator"
v0.12.0b3,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.12.0b3,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.12.0b3,Get input names
v0.12.0b3,Summary
v0.12.0b3,add statsmodels to parent's options
v0.12.0b3,add debiasedlasso to parent's options
v0.12.0b3,add blb to parent's options
v0.12.0b3,TODO Share some logic with non-discrete version
v0.12.0b3,Get input names
v0.12.0b3,Summary
v0.12.0b3,add statsmodels to parent's options
v0.12.0b3,add statsmodels to parent's options
v0.12.0b3,add blb to parent's options
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,remove None arguments
v0.12.0b3,"scores entries should be lists of scores, so make each entry a singleton list"
v0.12.0b3,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b3,generate an instance of the final model
v0.12.0b3,generate an instance of the nuisance model
v0.12.0b3,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.12.0b3,alteration even when we only want to fit_final.
v0.12.0b3,use a binary array to get stratified split in case of discrete treatment
v0.12.0b3,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.12.0b3,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.12.0b3,"however, sklearn doesn't support both stratifying and grouping (see"
v0.12.0b3,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.12.0b3,their own object that supports grouping if they want to use groups.
v0.12.0b3,for each mc iteration
v0.12.0b3,for each model under cross fit setting
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,
v0.12.0b3,This code contains snippets of code from
v0.12.0b3,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b3,published under the following license and copyright:
v0.12.0b3,BSD 3-Clause License
v0.12.0b3,
v0.12.0b3,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b3,All rights reserved.
v0.12.0b3,=============================================================================
v0.12.0b3,Policy Forest
v0.12.0b3,=============================================================================
v0.12.0b3,Remap output
v0.12.0b3,reshape is necessary to preserve the data contiguity against vs
v0.12.0b3,"[:, np.newaxis] that does not."
v0.12.0b3,Get subsample sample size
v0.12.0b3,Check parameters
v0.12.0b3,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b3,if this is the first `fit` call of the warm start mode.
v0.12.0b3,"Free allocated memory, if any"
v0.12.0b3,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b3,We draw from the random state to get the random state we
v0.12.0b3,would have got if we hadn't used a warm_start.
v0.12.0b3,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b3,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b3,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b3,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b3,for fitting the trees is internally releasing the Python GIL
v0.12.0b3,making threading more efficient than multiprocessing in
v0.12.0b3,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b3,"parallel_backend contexts set at a higher level,"
v0.12.0b3,since correctness does not rely on using threads.
v0.12.0b3,Collect newly grown trees
v0.12.0b3,Check data
v0.12.0b3,Assign chunk of trees to jobs
v0.12.0b3,avoid storing the output of every estimator by summing them here
v0.12.0b3,Parallel loop
v0.12.0b3,Check data
v0.12.0b3,Assign chunk of trees to jobs
v0.12.0b3,avoid storing the output of every estimator by summing them here
v0.12.0b3,Parallel loop
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,
v0.12.0b3,This code contains snippets of code from:
v0.12.0b3,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b3,published under the following license and copyright:
v0.12.0b3,BSD 3-Clause License
v0.12.0b3,
v0.12.0b3,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b3,All rights reserved.
v0.12.0b3,=============================================================================
v0.12.0b3,Types and constants
v0.12.0b3,=============================================================================
v0.12.0b3,=============================================================================
v0.12.0b3,Base Policy tree
v0.12.0b3,=============================================================================
v0.12.0b3,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.12.0b3,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.12.0b3,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.12.0b3,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.12.0b3,checks that X is 2D array.
v0.12.0b3,"since we only allow single dimensional y, we could flatten the prediction"
v0.12.0b3,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b3,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b3,Handles the corner case when X=None but featurizer might be not None
v0.12.0b3,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.12.0b3,"Replacing this method which is invalid for this class, so that we make the"
v0.12.0b3,dosctring empty and not appear in the docs.
v0.12.0b3,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b3,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.12.0b3,Replacing to remove docstring
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"if both X and W are None, just return a column of ones"
v0.12.0b3,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b3,We need to go back to the label representation of the one-hot so as to call
v0.12.0b3,the classifier.
v0.12.0b3,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b3,We need to go back to the label representation of the one-hot so as to call
v0.12.0b3,the classifier.
v0.12.0b3,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b3,This works both with our without the weighting trick as the treatments T are unit vector
v0.12.0b3,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.12.0b3,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.12.0b3,both Parametric and Non Parametric DML.
v0.12.0b3,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.12.0b3,attribute so that the trained featurizer will be passed through
v0.12.0b3,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b3,for internal use by the library
v0.12.0b3,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b3,We need to use the rlearner's copy to retain the information from fitting
v0.12.0b3,Handles the corner case when X=None but featurizer might be not None
v0.12.0b3,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b3,since we clone it and fit separate copies
v0.12.0b3,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b3,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b3,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b3,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b3,since we clone it and fit separate copies
v0.12.0b3,add blb to parent's options
v0.12.0b3,override only so that we can update the docstring to indicate
v0.12.0b3,support for `GenericSingleTreatmentModelFinalInference`
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,note that groups are not passed to score because they are only used for fitting
v0.12.0b3,note that groups are not passed to score because they are only used for fitting
v0.12.0b3,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b3,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b3,NOTE: important to get parent's wrapped copy so that
v0.12.0b3,"after training wrapped featurizer is also trained, etc."
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b3,Fit a doubly robust average effect
v0.12.0b3,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b3,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b3,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b3,since we clone it and fit separate copies
v0.12.0b3,"If custom param grid, check that only estimator parameters are being altered"
v0.12.0b3,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.12.0b3,override only so that we can update the docstring to indicate support for `blb`
v0.12.0b3,Get input names
v0.12.0b3,Summary
v0.12.0b3,Determine output settings
v0.12.0b3,"Important: This must be the first invocation of the random state at fit time, so that"
v0.12.0b3,train/test splits are re-generatable from an external object simply by knowing the
v0.12.0b3,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.12.0b3,linear predictions. Currently is also useful for testing.
v0.12.0b3,reshape is necessary to preserve the data contiguity against vs
v0.12.0b3,"[:, np.newaxis] that does not."
v0.12.0b3,Check parameters
v0.12.0b3,Set min_weight_leaf from min_weight_fraction_leaf
v0.12.0b3,Build tree
v0.12.0b3,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.12.0b3,hold. Used by criterion for memory space savings.
v0.12.0b3,Initialize the criterion object and the criterion_val object if honest.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,
v0.12.0b3,This code is a fork from:
v0.12.0b3,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.12.0b3,published under the following license and copyright:
v0.12.0b3,BSD 3-Clause License
v0.12.0b3,
v0.12.0b3,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b3,All rights reserved.
v0.12.0b3,Set parameters
v0.12.0b3,Don't instantiate estimators now! Parameters of base_estimator might
v0.12.0b3,"still change. Eg., when grid-searching with the nested object syntax."
v0.12.0b3,self.estimators_ needs to be filled by the derived classes in fit.
v0.12.0b3,Compute the number of jobs
v0.12.0b3,Partition estimators between jobs
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Remove children with nonwhite mothers from the treatment group
v0.12.0b3,Remove children with nonwhite mothers from the treatment group
v0.12.0b3,Select columns
v0.12.0b3,Scale the numeric variables
v0.12.0b3,"Change the binary variable 'first' takes values in {1,2}"
v0.12.0b3,Append a column of ones as intercept
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b3,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b3,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.12.0b3,d_t=None here since we measure the effect across all Ts
v0.12.0b3,once the estimator has been fit
v0.12.0b3,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.12.0b3,an intercept even when bias is part of coef.
v0.12.0b3,We can write effect inference as a function of prediction and prediction standard error of
v0.12.0b3,the final method for linear models
v0.12.0b3,squeeze the first axis
v0.12.0b3,d_t=None here since we measure the effect across all Ts
v0.12.0b3,set the mean_pred_stderr
v0.12.0b3,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b3,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b3,"send treatment to the end, pull bounds to the front"
v0.12.0b3,d_t=None here since we measure the effect across all Ts
v0.12.0b3,set the mean_pred_stderr
v0.12.0b3,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.12.0b3,d_t=None here since we measure the effect across all Ts
v0.12.0b3,d_t=None here since we measure the effect across all Ts
v0.12.0b3,need to set the fit args before the estimator is fit
v0.12.0b3,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b3,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.12.0b3,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.12.0b3,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.12.0b3,Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
v0.12.0b3,scale preds
v0.12.0b3,scale std errs
v0.12.0b3,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.12.0b3,offset preds
v0.12.0b3,"offset the distribution, too"
v0.12.0b3,scale preds
v0.12.0b3,"scale the distribution, too"
v0.12.0b3,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b3,1. Uncertainty of Mean Point Estimate
v0.12.0b3,2. Distribution of Point Estimate
v0.12.0b3,3. Total Variance of Point Estimate
v0.12.0b3,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.12.0b3,so bail out early; note that it might be possible to correct the algorithm for
v0.12.0b3,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.12.0b3,be clean
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,TODO: Add a __dir__ implementation?
v0.12.0b3,don't proxy special methods
v0.12.0b3,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.12.0b3,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.12.0b3,then we should be computing a confidence interval for the wrapped calls
v0.12.0b3,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.12.0b3,second level bootstrap which would be prohibitive computationally?
v0.12.0b3,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.12.0b3,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.12.0b3,can't import from econml.inference at top level without creating cyclical dependencies
v0.12.0b3,Note that inference results are always methods even if the inference is for a property
v0.12.0b3,(e.g. coef__inference() is a method but coef_ is a property)
v0.12.0b3,Therefore we must insert a lambda if getting inference for a non-callable
v0.12.0b3,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.12.0b3,"try to get interval/std first if appropriate,"
v0.12.0b3,since we don't prefer a wrapped method with this name
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,
v0.12.0b3,This code contains snippets of code from:
v0.12.0b3,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b3,published under the following license and copyright:
v0.12.0b3,BSD 3-Clause License
v0.12.0b3,
v0.12.0b3,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b3,All rights reserved.
v0.12.0b3,=============================================================================
v0.12.0b3,Types and constants
v0.12.0b3,=============================================================================
v0.12.0b3,=============================================================================
v0.12.0b3,Base GRF tree
v0.12.0b3,=============================================================================
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,=============================================================================
v0.12.0b3,A MultOutputWrapper for GRF classes
v0.12.0b3,=============================================================================
v0.12.0b3,=============================================================================
v0.12.0b3,Instantiations of Generalized Random Forest
v0.12.0b3,=============================================================================
v0.12.0b3,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.12.0b3,in front of the constant treatment is the intercept in the moment equation.
v0.12.0b3,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.12.0b3,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,
v0.12.0b3,This code contains snippets of code from
v0.12.0b3,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b3,published under the following license and copyright:
v0.12.0b3,BSD 3-Clause License
v0.12.0b3,
v0.12.0b3,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b3,All rights reserved.
v0.12.0b3,=============================================================================
v0.12.0b3,Base Generalized Random Forest
v0.12.0b3,=============================================================================
v0.12.0b3,TODO: support freq_weight and sample_var
v0.12.0b3,Remap output
v0.12.0b3,reshape is necessary to preserve the data contiguity against vs
v0.12.0b3,"[:, np.newaxis] that does not."
v0.12.0b3,reshape is necessary to preserve the data contiguity against vs
v0.12.0b3,"[:, np.newaxis] that does not."
v0.12.0b3,Get subsample sample size
v0.12.0b3,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.12.0b3,We calculate the min eigenvalue proxy that each criterion is considering
v0.12.0b3,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.12.0b3,Check parameters
v0.12.0b3,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b3,if this is the first `fit` call of the warm start mode.
v0.12.0b3,"Free allocated memory, if any"
v0.12.0b3,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b3,We draw from the random state to get the random state we
v0.12.0b3,would have got if we hadn't used a warm_start.
v0.12.0b3,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b3,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b3,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b3,Generating indices a priori before parallelism ended up being orders of magnitude
v0.12.0b3,faster than how sklearn does it. The reason is that random samplers do not release the
v0.12.0b3,gil it seems.
v0.12.0b3,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b3,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b3,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b3,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b3,for fitting the trees is internally releasing the Python GIL
v0.12.0b3,making threading more efficient than multiprocessing in
v0.12.0b3,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b3,"parallel_backend contexts set at a higher level,"
v0.12.0b3,since correctness does not rely on using threads.
v0.12.0b3,Collect newly grown trees
v0.12.0b3,Check data
v0.12.0b3,Assign chunk of trees to jobs
v0.12.0b3,avoid storing the output of every estimator by summing them here
v0.12.0b3,Parallel loop
v0.12.0b3,Check data
v0.12.0b3,Assign chunk of trees to jobs
v0.12.0b3,Parallel loop
v0.12.0b3,Check data
v0.12.0b3,Assign chunk of trees to jobs
v0.12.0b3,Parallel loop
v0.12.0b3,####################
v0.12.0b3,Variance correction
v0.12.0b3,####################
v0.12.0b3,Subtract the average within bag variance. This ends up being equal to the
v0.12.0b3,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.12.0b3,The negative part is just sq_between.
v0.12.0b3,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.12.0b3,"The off diagonals we have no objective prior, so no correction is applied."
v0.12.0b3,Finally correcting the pred_cov or pred_var
v0.12.0b3,avoid storing the output of every estimator by summing them here
v0.12.0b3,Parallel loop
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,testing importances
v0.12.0b3,testing heterogeneity importances
v0.12.0b3,Testing that all parameters do what they are supposed to
v0.12.0b3,"testing predict, apply and decision path"
v0.12.0b3,test that the subsampling scheme past to the trees is correct
v0.12.0b3,The sample size is chosen in particular to test rounding based error when subsampling
v0.12.0b3,test that the estimator calcualtes var correctly
v0.12.0b3,test api
v0.12.0b3,test accuracy
v0.12.0b3,test the projection functionality of forests
v0.12.0b3,test that the estimator calcualtes var correctly
v0.12.0b3,test api
v0.12.0b3,test that the estimator calcualtes var correctly
v0.12.0b3,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b3,test that we raise errors in mishandled situations.
v0.12.0b3,test that the subsampling scheme past to the trees is correct
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,omit the lalonde notebook
v0.12.0b3,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.12.0b3,creating notebooks that are annoying for our users to actually run themselves
v0.12.0b3,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b3,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.12.0b3,"prior to calling interpret, can't plot, render, etc."
v0.12.0b3,can interpret without uncertainty
v0.12.0b3,can't interpret with uncertainty if inference wasn't used during fit
v0.12.0b3,can interpret with uncertainty if we refit
v0.12.0b3,can interpret without uncertainty
v0.12.0b3,can't treat before interpreting
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,simple DGP only for illustration
v0.12.0b3,Define the treatment model neural network architecture
v0.12.0b3,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b3,"so the input shape is (d_z + d_x,)"
v0.12.0b3,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b3,add extra layers on top for the mixture density network
v0.12.0b3,Define the response model neural network architecture
v0.12.0b3,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b3,"so the input shape is (d_t + d_x,)"
v0.12.0b3,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b3,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b3,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b3,so that the same weights will be reused in each instantiation
v0.12.0b3,number of samples to use in second estimate of the response
v0.12.0b3,(to make loss estimate unbiased)
v0.12.0b3,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b3,do something with predictions...
v0.12.0b3,also test vector t and y
v0.12.0b3,simple DGP only for illustration
v0.12.0b3,Define the treatment model neural network architecture
v0.12.0b3,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b3,"so the input shape is (d_z + d_x,)"
v0.12.0b3,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b3,add extra layers on top for the mixture density network
v0.12.0b3,Define the response model neural network architecture
v0.12.0b3,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b3,"so the input shape is (d_t + d_x,)"
v0.12.0b3,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b3,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b3,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b3,so that the same weights will be reused in each instantiation
v0.12.0b3,number of samples to use in second estimate of the response
v0.12.0b3,(to make loss estimate unbiased)
v0.12.0b3,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b3,do something with predictions...
v0.12.0b3,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.12.0b3,test = True ensures we draw test set images
v0.12.0b3,test = True ensures we draw test set images
v0.12.0b3,re-draw to get new independent treatment and implied response
v0.12.0b3,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b3,above is necesary so that reduced form doesn't win
v0.12.0b3,covariates: time and emotion
v0.12.0b3,random instrument
v0.12.0b3,z -> price
v0.12.0b3,true observable demand function
v0.12.0b3,errors
v0.12.0b3,response
v0.12.0b3,test = True ensures we draw test set images
v0.12.0b3,test = True ensures we draw test set images
v0.12.0b3,re-draw to get new independent treatment and implied response
v0.12.0b3,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b3,above is necesary so that reduced form doesn't win
v0.12.0b3,covariates: time and emotion
v0.12.0b3,random instrument
v0.12.0b3,z -> price
v0.12.0b3,true observable demand function
v0.12.0b3,errors
v0.12.0b3,response
v0.12.0b3,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.12.0b3,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.12.0b3,For some reason this doesn't work at all when run against the CNTK backend...
v0.12.0b3,"model.compile('nadam', loss=lambda _,l:l)"
v0.12.0b3,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.12.0b3,generate a valiation set
v0.12.0b3,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.12.0b3,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,DGP constants
v0.12.0b3,Generate data
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,testing importances
v0.12.0b3,testing heterogeneity importances
v0.12.0b3,Testing that all parameters do what they are supposed to
v0.12.0b3,"testing predict, apply and decision path"
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.12.0b3,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.12.0b3,so we need to transpose the result
v0.12.0b3,1-d output
v0.12.0b3,2-d output
v0.12.0b3,Single dimensional output y
v0.12.0b3,compare with weight
v0.12.0b3,compare with weight
v0.12.0b3,compare with weight
v0.12.0b3,compare with weight
v0.12.0b3,Multi-dimensional output y
v0.12.0b3,1-d y
v0.12.0b3,compare when both sample_var and sample_weight exist
v0.12.0b3,multi-d y
v0.12.0b3,compare when both sample_var and sample_weight exist
v0.12.0b3,compare when both sample_var and sample_weight exist
v0.12.0b3,compare when both sample_var and sample_weight exist
v0.12.0b3,compare when both sample_var and sample_weight exist
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,test that we can fit with the same arguments as the base estimator
v0.12.0b3,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b3,with the same shape for the lower and upper bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,test that we can do the same thing once we provide percentile bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b3,with the same shape for the lower and upper bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,test that we can do the same thing once we provide percentile bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,test that we can fit with the same arguments as the base estimator
v0.12.0b3,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b3,with the same shape for the lower and upper bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,test that we can do the same thing once we provide percentile bounds
v0.12.0b3,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b3,with the same shape for the lower and upper bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,test that we can do the same thing once we provide percentile bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,test that we can fit with the same arguments as the base estimator
v0.12.0b3,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b3,with the same shape for the lower and upper bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,test that we can do the same thing once we provide percentile bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b3,with the same shape for the lower and upper bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,test that we can do the same thing once we provide percentile bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b3,with the same shape for the lower and upper bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,test that the estimated effect is usually within the bounds
v0.12.0b3,test that we can do the same thing once we provide alpha explicitly
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,test that the estimated effect is usually within the bounds
v0.12.0b3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b3,with the same shape for the lower and upper bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,TODO: test that the estimated effect is usually within the bounds
v0.12.0b3,and that the true effect is also usually within the bounds
v0.12.0b3,test that we can do the same thing once we provide percentile bounds
v0.12.0b3,test that the lower and upper bounds differ
v0.12.0b3,TODO: test that the estimated effect is usually within the bounds
v0.12.0b3,and that the true effect is also usually within the bounds
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,test that the subsampling scheme past to the trees is correct
v0.12.0b3,test that the estimator calcualtes var correctly
v0.12.0b3,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b3,test that we raise errors in mishandled situations.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,DGP constants
v0.12.0b3,Generate data
v0.12.0b3,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b3,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b3,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.12.0b3,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b3,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.12.0b3,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b3,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b3,pvalue for second column should be greater than zero since some points are on either side
v0.12.0b3,of the tested value
v0.12.0b3,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.12.0b3,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b3,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b3,only is not None when T1 is a constant or a list of constant
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.12.0b3,Test non keyword based calls to fit
v0.12.0b3,test non-array inputs
v0.12.0b3,Test custom splitter
v0.12.0b3,Test incomplete set of test folds
v0.12.0b3,"y scores should be positive, since W predicts Y somewhat"
v0.12.0b3,"t scores might not be, since W and T are uncorrelated"
v0.12.0b3,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,make sure cross product varies more slowly with first array
v0.12.0b3,and that vectors are okay as inputs
v0.12.0b3,number of inputs in specification must match number of inputs
v0.12.0b3,must have an output
v0.12.0b3,output indices must be unique
v0.12.0b3,output indices must be present in an input
v0.12.0b3,number of indices must match number of dimensions for each input
v0.12.0b3,repeated indices must always have consistent sizes
v0.12.0b3,transpose
v0.12.0b3,tensordot
v0.12.0b3,trace
v0.12.0b3,TODO: set up proper flag for this
v0.12.0b3,pick indices at random with replacement from the first 7 letters of the alphabet
v0.12.0b3,"of all of the distinct indices that appear in any input,"
v0.12.0b3,pick a random subset of them (of size at most 5) to appear in the output
v0.12.0b3,creating an instance should warn
v0.12.0b3,using the instance should not warn
v0.12.0b3,using the deprecated method should warn
v0.12.0b3,don't warn if b and c are passed by keyword
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Preprocess data
v0.12.0b3,Convert 'week' to a date
v0.12.0b3,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.12.0b3,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.12.0b3,Take log of price
v0.12.0b3,Make brand numeric
v0.12.0b3,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.12.0b3,which have all zero coeffs
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,test at least one estimator from each category
v0.12.0b3,test causal graph
v0.12.0b3,test refutation estimate
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.12.0b3,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.12.0b3,TODO: test something rather than just print...
v0.12.0b3,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.12.0b3,pick some arbitrary X
v0.12.0b3,pick some arbitrary T
v0.12.0b3,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b3,The average variance should be lower when using monte carlo iterations
v0.12.0b3,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b3,The average variance should be lower when using monte carlo iterations
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,ensure that we've got at least two of every row
v0.12.0b3,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b3,need to make sure we get all *joint* combinations
v0.12.0b3,IntentToTreat only supports binary treatments/instruments
v0.12.0b3,IntentToTreat only supports binary treatments/instruments
v0.12.0b3,IntentToTreat requires X
v0.12.0b3,ensure we can serialize unfit estimator
v0.12.0b3,these support only W but not X
v0.12.0b3,"these support only binary, not general discrete T and Z"
v0.12.0b3,ensure we can serialize fit estimator
v0.12.0b3,make sure we can call the marginal_effect and effect methods
v0.12.0b3,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b3,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b3,"make sure we can call effect with implied scalar treatments,"
v0.12.0b3,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b3,are multiple treatments
v0.12.0b3,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b3,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.12.0b3,"however, with custom splits the checking happens in the first stage wrapper"
v0.12.0b3,where we don't have all of the required information to do this;
v0.12.0b3,we'd probably need to add it to _crossfit instead
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.12.0b3,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.12.0b3,The __warningregistry__'s need to be in a pristine state for tests
v0.12.0b3,to work properly.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Set random seed
v0.12.0b3,Generate data
v0.12.0b3,DGP constants
v0.12.0b3,Test data
v0.12.0b3,Constant treatment effect
v0.12.0b3,Constant treatment with multi output Y
v0.12.0b3,Heterogeneous treatment
v0.12.0b3,Heterogeneous treatment with multi output Y
v0.12.0b3,TLearner test
v0.12.0b3,Instantiate TLearner
v0.12.0b3,Test inputs
v0.12.0b3,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b3,Instantiate SLearner
v0.12.0b3,Test inputs
v0.12.0b3,Test constant treatment effect
v0.12.0b3,Test constant treatment effect with multi output Y
v0.12.0b3,Test heterogeneous treatment effect
v0.12.0b3,Need interactions between T and features
v0.12.0b3,Test heterogeneous treatment effect with multi output Y
v0.12.0b3,Instantiate XLearner
v0.12.0b3,Test inputs
v0.12.0b3,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b3,Instantiate DomainAdaptationLearner
v0.12.0b3,Test inputs
v0.12.0b3,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b3,Get the true treatment effect
v0.12.0b3,Get the true treatment effect
v0.12.0b3,Fit learner and get the effect and marginal effect
v0.12.0b3,Compute treatment effect residuals (absolute)
v0.12.0b3,Check that at least 90% of predictions are within tolerance interval
v0.12.0b3,Check whether the output shape is right
v0.12.0b3,Check that one can pass in regular lists
v0.12.0b3,Check that it fails correctly if lists of different shape are passed in
v0.12.0b3,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b3,Generate covariates
v0.12.0b3,Generate treatment
v0.12.0b3,Calculate outcome
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,DGP constants
v0.12.0b3,Generate data
v0.12.0b3,Test data
v0.12.0b3,Remove warnings that might be raised by the models passed into the ORF
v0.12.0b3,Generate data with continuous treatments
v0.12.0b3,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.12.0b3,does not work well with parallelism.
v0.12.0b3,Test inputs for continuous treatments
v0.12.0b3,--> Check that one can pass in regular lists
v0.12.0b3,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b3,Check that outputs have the correct shape
v0.12.0b3,Test continuous treatments with controls
v0.12.0b3,Test continuous treatments without controls
v0.12.0b3,Generate data with binary treatments
v0.12.0b3,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.12.0b3,does not work well with parallelism.
v0.12.0b3,Test inputs for binary treatments
v0.12.0b3,--> Check that one can pass in regular lists
v0.12.0b3,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b3,"--> Check that it works when T, Y have shape (n, 1)"
v0.12.0b3,"--> Check that it fails correctly when T has shape (n, 2)"
v0.12.0b3,--> Check that it fails correctly when the treatments are not numeric
v0.12.0b3,Check that outputs have the correct shape
v0.12.0b3,Test binary treatments with controls
v0.12.0b3,Test binary treatments without controls
v0.12.0b3,Only applicable to continuous treatments
v0.12.0b3,Generate data for 2 treatments
v0.12.0b3,Test multiple treatments with controls
v0.12.0b3,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.12.0b3,The rest for controls. Just as an example.
v0.12.0b3,Generating A/B test data
v0.12.0b3,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.12.0b3,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.12.0b3,Create a wrapper around Lasso that doesn't support weights
v0.12.0b3,since Lasso does natively support them starting in sklearn 0.23
v0.12.0b3,Generate data with continuous treatments
v0.12.0b3,Instantiate model with most of the default parameters
v0.12.0b3,Compute the treatment effect on test points
v0.12.0b3,Compute treatment effect residuals
v0.12.0b3,Multiple treatments
v0.12.0b3,Allow at most 10% test points to be outside of the tolerance interval
v0.12.0b3,Compute treatment effect residuals
v0.12.0b3,Multiple treatments
v0.12.0b3,Allow at most 20% test points to be outside of the confidence interval
v0.12.0b3,Check that the intervals are not too wide
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.12.0b3,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.12.0b3,ensure that we've got at least 6 of every element
v0.12.0b3,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.12.0b3,NOTE: this number may need to change if the default number of folds in
v0.12.0b3,WeightedStratifiedKFold changes
v0.12.0b3,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b3,ensure we can serialize the unfit estimator
v0.12.0b3,ensure we can pickle the fit estimator
v0.12.0b3,make sure we can call the marginal_effect and effect methods
v0.12.0b3,test const marginal inference
v0.12.0b3,test effect inference
v0.12.0b3,test marginal effect inference
v0.12.0b3,test coef__inference and intercept__inference
v0.12.0b3,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b3,"make sure we can call effect with implied scalar treatments,"
v0.12.0b3,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b3,are multiple treatments
v0.12.0b3,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b3,ensure that we've got at least two of every element
v0.12.0b3,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b3,make sure we can call the marginal_effect and effect methods
v0.12.0b3,test const marginal inference
v0.12.0b3,test effect inference
v0.12.0b3,test marginal effect inference
v0.12.0b3,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b3,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b3,We concatenate the two copies data
v0.12.0b3,make sure we can get out post-fit stuff
v0.12.0b3,create a simple artificial setup where effect of moving from treatment
v0.12.0b3,"1 -> 2 is 2,"
v0.12.0b3,"1 -> 3 is 1, and"
v0.12.0b3,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b3,"Using an uneven number of examples from different classes,"
v0.12.0b3,"and having the treatments in non-lexicographic order,"
v0.12.0b3,Should rule out some basic issues.
v0.12.0b3,test that we can fit with a KFold instance
v0.12.0b3,test that we can fit with a train/test iterable
v0.12.0b3,predetermined splits ensure that all features are seen in each split
v0.12.0b3,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.12.0b3,(incorrectly) use a final model with an intercept
v0.12.0b3,"Because final model is fixed, actual values of T and Y don't matter"
v0.12.0b3,Ensure reproducibility
v0.12.0b3,Sparse DGP
v0.12.0b3,Treatment effect coef
v0.12.0b3,Other coefs
v0.12.0b3,Features and controls
v0.12.0b3,Test sparse estimator
v0.12.0b3,"--> test coef_, intercept_"
v0.12.0b3,--> test treatment effects
v0.12.0b3,Restrict x_test to vectors of norm < 1
v0.12.0b3,--> check inference
v0.12.0b3,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b3,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.12.0b3,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.12.0b3,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.12.0b3,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.12.0b3,sparse test case: heterogeneous effect by product
v0.12.0b3,need at least as many rows in e_y as there are distinct columns
v0.12.0b3,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.12.0b3,create a simple artificial setup where effect of moving from treatment
v0.12.0b3,"a -> b is 2,"
v0.12.0b3,"a -> c is 1, and"
v0.12.0b3,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.12.0b3,"Using an uneven number of examples from different classes,"
v0.12.0b3,"and having the treatments in non-lexicographic order,"
v0.12.0b3,should rule out some basic issues.
v0.12.0b3,Note that explicitly specifying the dtype as object is necessary until
v0.12.0b3,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.12.0b3,estimated effects should be identical when treatment is explicitly given
v0.12.0b3,but const_marginal_effect should be reordered based on the explicit cagetories
v0.12.0b3,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.12.0b3,test outer grouping
v0.12.0b3,test nested grouping
v0.12.0b3,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b3,whichever groups we saw
v0.12.0b3,test nested grouping
v0.12.0b3,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b3,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b3,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Set random seed
v0.12.0b3,Generate data
v0.12.0b3,DGP constants
v0.12.0b3,Test data
v0.12.0b3,Constant treatment effect and propensity
v0.12.0b3,Heterogeneous treatment and propensity
v0.12.0b3,ensure that we've got at least two of every element
v0.12.0b3,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b3,ensure that we can serialize unfit estimator
v0.12.0b3,ensure that we can serialize fit estimator
v0.12.0b3,make sure we can call the marginal_effect and effect methods
v0.12.0b3,test const marginal inference
v0.12.0b3,test effect inference
v0.12.0b3,test marginal effect inference
v0.12.0b3,test coef_ and intercept_ inference
v0.12.0b3,verify we can generate the summary
v0.12.0b3,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b3,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b3,create a simple artificial setup where effect of moving from treatment
v0.12.0b3,"1 -> 2 is 2,"
v0.12.0b3,"1 -> 3 is 1, and"
v0.12.0b3,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b3,"Using an uneven number of examples from different classes,"
v0.12.0b3,"and having the treatments in non-lexicographic order,"
v0.12.0b3,Should rule out some basic issues.
v0.12.0b3,test that we can fit with a KFold instance
v0.12.0b3,test that we can fit with a train/test iterable
v0.12.0b3,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b3,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b3,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b3,test coef__inference function works
v0.12.0b3,test intercept__inference function works
v0.12.0b3,test summary function works
v0.12.0b3,Test inputs
v0.12.0b3,self._test_inputs(DR_learner)
v0.12.0b3,Test constant treatment effect
v0.12.0b3,Test heterogeneous treatment effect
v0.12.0b3,Test heterogenous treatment effect for W =/= None
v0.12.0b3,Sparse DGP
v0.12.0b3,Treatment effect coef
v0.12.0b3,Other coefs
v0.12.0b3,Features and controls
v0.12.0b3,Test sparse estimator
v0.12.0b3,"--> test coef_, intercept_"
v0.12.0b3,--> test treatment effects
v0.12.0b3,Restrict x_test to vectors of norm < 1
v0.12.0b3,--> check inference
v0.12.0b3,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b3,test outer grouping
v0.12.0b3,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.12.0b3,test nested grouping
v0.12.0b3,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b3,whichever groups we saw
v0.12.0b3,test nested grouping
v0.12.0b3,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b3,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b3,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b3,helper class
v0.12.0b3,Fit learner and get the effect
v0.12.0b3,Get the true treatment effect
v0.12.0b3,Compute treatment effect residuals (absolute)
v0.12.0b3,Check that at least 90% of predictions are within tolerance interval
v0.12.0b3,Only for heterogeneous TE
v0.12.0b3,Fit learner on X and W and get the effect
v0.12.0b3,Get the true treatment effect
v0.12.0b3,Compute treatment effect residuals (absolute)
v0.12.0b3,Check that at least 90% of predictions are within tolerance interval
v0.12.0b3,Check that one can pass in regular lists
v0.12.0b3,Check that it fails correctly if lists of different shape are passed in
v0.12.0b3,Check that it fails when T contains values other than 0 and 1
v0.12.0b3,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b3,Generate covariates
v0.12.0b3,Generate treatment
v0.12.0b3,Calculate outcome
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,DGP constants
v0.12.0b3,DGP coefficients
v0.12.0b3,Generated outcomes
v0.12.0b3,################
v0.12.0b3,WeightedLasso #
v0.12.0b3,################
v0.12.0b3,Define weights
v0.12.0b3,Define extended datasets
v0.12.0b3,Range of alphas
v0.12.0b3,Compare with Lasso
v0.12.0b3,--> No intercept
v0.12.0b3,--> With intercept
v0.12.0b3,When DGP has no intercept
v0.12.0b3,When DGP has intercept
v0.12.0b3,--> Coerce coefficients to be positive
v0.12.0b3,--> Toggle max_iter & tol
v0.12.0b3,Define weights
v0.12.0b3,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b3,Mixed DGP scenario.
v0.12.0b3,Define extended datasets
v0.12.0b3,Define weights
v0.12.0b3,Define multioutput
v0.12.0b3,##################
v0.12.0b3,WeightedLassoCV #
v0.12.0b3,##################
v0.12.0b3,Define alphas to test
v0.12.0b3,Compare with LassoCV
v0.12.0b3,--> No intercept
v0.12.0b3,--> With intercept
v0.12.0b3,--> Force parameters to be positive
v0.12.0b3,Choose a smaller n to speed-up process
v0.12.0b3,Compare fold weights
v0.12.0b3,Define weights
v0.12.0b3,Define extended datasets
v0.12.0b3,Define splitters
v0.12.0b3,WeightedKFold splitter
v0.12.0b3,Map weighted splitter to an extended splitter
v0.12.0b3,Define alphas to test
v0.12.0b3,Compare with LassoCV
v0.12.0b3,--> No intercept
v0.12.0b3,--> With intercept
v0.12.0b3,--> Force parameters to be positive
v0.12.0b3,###########################
v0.12.0b3,MultiTaskWeightedLassoCV #
v0.12.0b3,###########################
v0.12.0b3,Define alphas to test
v0.12.0b3,Define splitter
v0.12.0b3,Compare with MultiTaskLassoCV
v0.12.0b3,--> No intercept
v0.12.0b3,--> With intercept
v0.12.0b3,Define weights
v0.12.0b3,Define extended datasets
v0.12.0b3,Define splitters
v0.12.0b3,WeightedKFold splitter
v0.12.0b3,Map weighted splitter to an extended splitter
v0.12.0b3,Define alphas to test
v0.12.0b3,Compare with LassoCV
v0.12.0b3,--> No intercept
v0.12.0b3,--> With intercept
v0.12.0b3,#########################
v0.12.0b3,WeightedLassoCVWrapper #
v0.12.0b3,#########################
v0.12.0b3,perform 1D fit
v0.12.0b3,perform 2D fit
v0.12.0b3,################
v0.12.0b3,DebiasedLasso #
v0.12.0b3,################
v0.12.0b3,Test DebiasedLasso without weights
v0.12.0b3,--> Check debiased coeffcients without intercept
v0.12.0b3,--> Check debiased coeffcients with intercept
v0.12.0b3,--> Check 5-95 CI coverage for unit vectors
v0.12.0b3,Test DebiasedLasso with weights for one DGP
v0.12.0b3,Define weights
v0.12.0b3,Define extended datasets
v0.12.0b3,--> Check debiased coefficients
v0.12.0b3,Define weights
v0.12.0b3,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b3,--> Check debiased coeffcients
v0.12.0b3,Test that attributes propagate correctly
v0.12.0b3,Test MultiOutputDebiasedLasso without weights
v0.12.0b3,--> Check debiased coeffcients without intercept
v0.12.0b3,--> Check debiased coeffcients with intercept
v0.12.0b3,--> Check CI coverage
v0.12.0b3,Test MultiOutputDebiasedLasso with weights
v0.12.0b3,Define weights
v0.12.0b3,Define extended datasets
v0.12.0b3,--> Check debiased coefficients
v0.12.0b3,Unit vectors
v0.12.0b3,Unit vectors
v0.12.0b3,Check coeffcients and intercept are the same within tolerance
v0.12.0b3,Check results are similar with tolerance 1e-6
v0.12.0b3,Check if multitask
v0.12.0b3,Check that same alpha is chosen
v0.12.0b3,Check that the coefficients are similar
v0.12.0b3,selective ridge has a simple implementation that we can test against
v0.12.0b3,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.12.0b3,"it should be the case that when we set fit_intercept to true,"
v0.12.0b3,it doesn't matter whether the penalized model also fits an intercept or not
v0.12.0b3,create an extra copy of rows with weight 2
v0.12.0b3,"instead of a slice, explicitly return an array of indices"
v0.12.0b3,_penalized_inds is only set during fitting
v0.12.0b3,cv exists on penalized model
v0.12.0b3,now we can access _penalized_inds
v0.12.0b3,check that we can read the cv attribute back out from the underlying model
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b3,local index should have as many times entries as global as there were rows passed in
v0.12.0b3,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b3,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b3,policy value should exceed always treating with any treatment
v0.12.0b3,"global shape is (d_y, sum(d_t))"
v0.12.0b3,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b3,features; for categoricals they should appear #cats-1 times each
v0.12.0b3,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b3,local index should have as many times entries as global as there were rows passed in
v0.12.0b3,features; for categoricals they should appear #cats-1 times each
v0.12.0b3,"global shape is (d_y, sum(d_t))"
v0.12.0b3,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b3,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b3,policy value should exceed always treating with any treatment
v0.12.0b3,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b3,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b3,local index should have as many times entries as global as there were rows passed in
v0.12.0b3,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b3,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b3,policy value should exceed always treating with any treatment
v0.12.0b3,"global shape is (d_y, sum(d_t))"
v0.12.0b3,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b3,features; for categoricals they should appear #cats-1 times each
v0.12.0b3,make sure we don't run into problems dropping every index
v0.12.0b3,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b3,local index should have as many times entries as global as there were rows passed in
v0.12.0b3,"global shape is (d_y, sum(d_t))"
v0.12.0b3,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b3,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b3,policy value should exceed always treating with any treatment
v0.12.0b3,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b3,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b3,local index should have as many times entries as global as there were rows passed in
v0.12.0b3,features; for categoricals they should appear #cats-1 times each
v0.12.0b3,"global shape is (d_y, sum(d_t))"
v0.12.0b3,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b3,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b3,policy value should exceed always treating with any treatment
v0.12.0b3,dgp
v0.12.0b3,model
v0.12.0b3,model
v0.12.0b3,"columns 'd', 'e', 'h' have too many values"
v0.12.0b3,"columns 'd', 'e' have too many values"
v0.12.0b3,lowering bound shouldn't affect already fit columns when warm starting
v0.12.0b3,"column d is now okay, too"
v0.12.0b3,verify that we can use a scalar treatment cost
v0.12.0b3,verify that we can specify per-treatment costs for each sample
v0.12.0b3,verify that using the same state returns the same results each time
v0.12.0b3,set the categories for column 'd' explicitly so that b is default
v0.12.0b3,"first column: 10 ones, this is fine"
v0.12.0b3,"second column: 6 categories, plenty of random instances of each"
v0.12.0b3,this is fine only if we increase the cateogry limit
v0.12.0b3,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.12.0b3,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.12.0b3,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.12.0b3,forest heterogeneity won't work
v0.12.0b3,"sixth column: just 1 one, not enough even without check"
v0.12.0b3,increase bound on cat expansion
v0.12.0b3,skip checks (reducing folds accordingly)
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,DGP constants
v0.12.0b3,Define data features
v0.12.0b3,Added `_df`to names to be different from the default cate_estimator names
v0.12.0b3,Generate data
v0.12.0b3,################################
v0.12.0b3,Single treatment and outcome #
v0.12.0b3,################################
v0.12.0b3,Test LinearDML
v0.12.0b3,|--> Test featurizers
v0.12.0b3,ColumnTransformer doesn't propagate column names
v0.12.0b3,|--> Test re-fit
v0.12.0b3,Test SparseLinearDML
v0.12.0b3,Test ForestDML
v0.12.0b3,###################################
v0.12.0b3,Mutiple treatments and outcomes #
v0.12.0b3,###################################
v0.12.0b3,Test LinearDML
v0.12.0b3,Test SparseLinearDML
v0.12.0b3,"Single outcome only, ORF does not support multiple outcomes"
v0.12.0b3,Test DMLOrthoForest
v0.12.0b3,Test DROrthoForest
v0.12.0b3,Test XLearner
v0.12.0b3,Skipping population summary names test because bootstrap inference is too slow
v0.12.0b3,Test SLearner
v0.12.0b3,Test TLearner
v0.12.0b3,Test LinearDRLearner
v0.12.0b3,Test SparseLinearDRLearner
v0.12.0b3,Test ForestDRLearner
v0.12.0b3,Test LinearIntentToTreatDRIV
v0.12.0b3,Test DeepIV
v0.12.0b3,Test categorical treatments
v0.12.0b3,Check refit
v0.12.0b3,Check refit after setting categories
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Linear models are required for parametric dml
v0.12.0b3,sample weighting models are required for nonparametric dml
v0.12.0b3,Test values
v0.12.0b3,TLearner test
v0.12.0b3,Instantiate TLearner
v0.12.0b3,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b3,Test constant treatment effect with multi output Y
v0.12.0b3,Test heterogeneous treatment effect
v0.12.0b3,Need interactions between T and features
v0.12.0b3,Test heterogeneous treatment effect with multi output Y
v0.12.0b3,Instantiate DomainAdaptationLearner
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,test base values equals to mean of constant marginal effect
v0.12.0b3,test shape of shap values output is as expected
v0.12.0b3,test shape of attribute of explanation object is as expected
v0.12.0b3,test length of feature names equals to shap values shape
v0.12.0b3,test base values equals to mean of constant marginal effect
v0.12.0b3,test shape of shap values output is as expected
v0.12.0b3,test shape of attribute of explanation object is as expected
v0.12.0b3,test length of feature names equals to shap values shape
v0.12.0b3,Treatment effect function
v0.12.0b3,Outcome support
v0.12.0b3,Treatment support
v0.12.0b3,"Generate controls, covariates, treatments and outcomes"
v0.12.0b3,Heterogeneous treatment effects
v0.12.0b3,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.12.0b3,through shap package.
v0.12.0b3,test shap could generate the plot from the shap_values
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Check inputs
v0.12.0b3,Check inputs
v0.12.0b3,Check inputs
v0.12.0b3,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.12.0b3,"Thus, we append the controls column before the one-hot-encoded T"
v0.12.0b3,"We might want to revisit, though, since it's linearly determined by the others"
v0.12.0b3,Check inputs
v0.12.0b3,Check inputs
v0.12.0b3,Estimate response function
v0.12.0b3,Check inputs
v0.12.0b3,Train model on controls. Assign higher weight to units resembling
v0.12.0b3,treated units.
v0.12.0b3,Train model on the treated. Assign higher weight to units resembling
v0.12.0b3,control units.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.12.0b3,output is
v0.12.0b3,"* a column of ones if X, W, and Z are all None"
v0.12.0b3,* just X or W or Z if both of the others are None
v0.12.0b3,* hstack([arrs]) for whatever subset are not None otherwise
v0.12.0b3,ensure Z is 2D
v0.12.0b3,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b3,We need to go back to the label representation of the one-hot so as to call
v0.12.0b3,the classifier.
v0.12.0b3,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b3,We need to go back to the label representation of the one-hot so as to call
v0.12.0b3,the classifier.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,TODO: make sure to use random seeds wherever necessary
v0.12.0b3,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.12.0b3,"unfortunately with the Theano and Tensorflow backends,"
v0.12.0b3,the straightforward use of K.stop_gradient can cause an error
v0.12.0b3,because the parameters of the intermediate layers are now disconnected from the loss;
v0.12.0b3,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.12.0b3,so that those layers remain connected but with 0 gradient
v0.12.0b3,|| t - mu_i || ^2
v0.12.0b3,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.12.0b3,Use logsumexp for numeric stability:
v0.12.0b3,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.12.0b3,TODO: does the numeric stability actually make any difference?
v0.12.0b3,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.12.0b3,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.12.0b3,generate cumulative sum via matrix multiplication
v0.12.0b3,"Generate standard uniform values in shape (batch_size,1)"
v0.12.0b3,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.12.0b3,we use uniform_like instead with an input of an appropriate shape)
v0.12.0b3,convert to floats and multiply to perform equivalent of logical AND
v0.12.0b3,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.12.0b3,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.12.0b3,we use normal_like instead with an input of an appropriate shape)
v0.12.0b3,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.12.0b3,prevent gradient from passing through sampling
v0.12.0b3,three options: biased or upper-bound loss require a single number of samples;
v0.12.0b3,unbiased can take different numbers for the network and its gradient
v0.12.0b3,"sample: (() -> Layer, int) -> Layer"
v0.12.0b3,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.12.0b3,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.12.0b3,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.12.0b3,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.12.0b3,the dimensionality of the output of the network
v0.12.0b3,TODO: is there a more robust way to do this?
v0.12.0b3,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b3,"subtle point: we need to build a new model each time,"
v0.12.0b3,because each model encapsulates its randomness
v0.12.0b3,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b3,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.12.0b3,not a general tensor (because of how backprop works in every framework)
v0.12.0b3,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.12.0b3,but this seems annoying...)
v0.12.0b3,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.12.0b3,TODO: any way to get this to work on batches of arbitrary size?
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b3,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b3,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b3,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b3,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b3,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.12.0b3,"instruments, and outcomes"
v0.12.0b3,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b3,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b3,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b3,for internal use by the library
v0.12.0b3,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b3,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b3,since it expects raw values
v0.12.0b3,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b3,since it expects raw values
v0.12.0b3,"TODO: check that Y, T, Z do not have multiple columns"
v0.12.0b3,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b3,TODO: do correct adjustment for sample_var
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.12.0b3,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.12.0b3,since we would actually be calculating a CATE rather than ATE.
v0.12.0b3,TODO: allow the final model to actually use X?
v0.12.0b3,TODO: allow the final model to actually use X?
v0.12.0b3,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b3,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.12.0b3,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.12.0b3,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b3,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b3,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b3,for internal use by the library
v0.12.0b3,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,"this will have dimension (d,) + shape(X)"
v0.12.0b3,send the first dimension to the end
v0.12.0b3,columns are featurized independently; partial derivatives are only non-zero
v0.12.0b3,when taken with respect to the same column each time
v0.12.0b3,don't fit intercept; manually add column of ones to the data instead;
v0.12.0b3,this allows us to ignore the intercept when computing marginal effects
v0.12.0b3,make T 2D if if was a vector
v0.12.0b3,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.12.0b3,two stage approximation
v0.12.0b3,"first, get basis expansions of T, X, and Z"
v0.12.0b3,TODO: is it right that the effective number of intruments is the
v0.12.0b3,"product of ft_X and ft_Z, not just ft_Z?"
v0.12.0b3,"regress T expansion on X,Z expansions concatenated with W"
v0.12.0b3,"predict ft_T from interacted ft_X, ft_Z"
v0.12.0b3,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.12.0b3,dT may be only 2-dimensional)
v0.12.0b3,promote dT to 3D if necessary (e.g. if T was a vector)
v0.12.0b3,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,TODO: this utility is documented but internal; reimplement?
v0.12.0b3,TODO: this utility is even less public...
v0.12.0b3,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.12.0b3,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.12.0b3,but also supports get_feature_names with expected signature
v0.12.0b3,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.12.0b3,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.12.0b3,Convert SingleTreeInterpreter to a python dictionary
v0.12.0b3,named tuple type for storing results inside CausalAnalysis class;
v0.12.0b3,must be lifted to module level to enable pickling
v0.12.0b3,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.12.0b3,Controls are all other columns of X
v0.12.0b3,"can't use X[:, feat_ind] when X is a DataFrame"
v0.12.0b3,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.12.0b3,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.12.0b3,so that the user can opt-in to allowing unseen treatment values
v0.12.0b3,(and return NaN or something in that case)
v0.12.0b3,array checking routines don't accept 0-width arrays
v0.12.0b3,perform model selection
v0.12.0b3,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.12.0b3,convert to NormalInferenceResults for consistency
v0.12.0b3,Set the dictionary values shared between local and global summaries
v0.12.0b3,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.12.0b3,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.12.0b3,required to fit a discrete DML model
v0.12.0b3,Validate inputs
v0.12.0b3,TODO: check compatibility of X and Y lengths
v0.12.0b3,"no previous fit, cancel warm start"
v0.12.0b3,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.12.0b3,"if heterogeneity_inds is 1D, repeat it"
v0.12.0b3,heterogeneity inds should be a 2D list of length same as train_inds
v0.12.0b3,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.12.0b3,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.12.0b3,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.12.0b3,train the Y model
v0.12.0b3,"perform model selection for the Y model using all X, not on a per-column basis"
v0.12.0b3,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.12.0b3,work with the regression wrapper
v0.12.0b3,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.12.0b3,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.12.0b3,since otherwise we'll have too many columns to be able to train a classifier
v0.12.0b3,start with empty results and default shared insights
v0.12.0b3,convert categorical indicators to numeric indices
v0.12.0b3,check for indices over the categorical expansion bound
v0.12.0b3,assume we'll be able to train former failures this time; we'll add them back if not
v0.12.0b3,"can't remove in place while iterating over new_inds, so store in separate list"
v0.12.0b3,"train the model, but warn"
v0.12.0b3,no model can be trained in this case since we need more folds
v0.12.0b3,"don't train a model, but suggest workaround since there are enough instances of least"
v0.12.0b3,populated class
v0.12.0b3,also remove from train_inds so we don't try to access the result later
v0.12.0b3,extract subset of names matching new columns
v0.12.0b3,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.12.0b3,don't want to cache this failed result
v0.12.0b3,properties to return from effect InferenceResults
v0.12.0b3,properties to return from PopulationSummaryResults
v0.12.0b3,Converts strings to property lookups or method calls as a convenience so that the
v0.12.0b3,_point_props and _summary_props above can be applied to an inference object
v0.12.0b3,Create a summary combining all results into a single output; this is used
v0.12.0b3,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.12.0b3,"or a dictionary, respectively, based on the summary function passed into this method"
v0.12.0b3,"ensure array has shape (m,y,t)"
v0.12.0b3,population summary is missing sample dimension; add it for consistency
v0.12.0b3,outcome dimension is missing; add it for consistency
v0.12.0b3,add singleton treatment dimension if missing
v0.12.0b3,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.12.0b3,"each attr has dimension (m,y) or (m,y,t)"
v0.12.0b3,concatenate along treatment dimension
v0.12.0b3,"for dictionary representation, want to remove unneeded sample dimension"
v0.12.0b3,in cohort and global results
v0.12.0b3,TODO: enrich outcome logic for multi-class classification when that is supported
v0.12.0b3,can't drop only level
v0.12.0b3,should be serialization-ready and contain no numpy arrays
v0.12.0b3,a global inference indicates the effect of that one feature on the outcome
v0.12.0b3,need to reshape the output to match the input
v0.12.0b3,we want to offset the inference object by the baseline estimate of y
v0.12.0b3,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.12.0b3,because then scaling the difference between treatment value and treatment costs is the
v0.12.0b3,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.12.0b3,
v0.12.0b3,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.12.0b3,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.12.0b3,(rather than just not treating at all)
v0.12.0b3,
v0.12.0b3,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.12.0b3,2 * policy_value - always_treat
v0.12.0b3,includes exactly their contribution because policy_value and always_treat both include it
v0.12.0b3,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.12.0b3,2 * policy_value - always-treat
v0.12.0b3,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.12.0b3,is zero and the contribution to always_treat is negative
v0.12.0b3,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b3,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b3,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b3,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b3,get dataframe with all but selected column
v0.12.0b3,apply 10% of a typical treatment for this feature
v0.12.0b3,"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
v0.12.0b3,set the effect bounds; for positive treatments these agree with
v0.12.0b3,"the estimates; for negative treatments, we need to invert the interval"
v0.12.0b3,the effect is now always positive since we decrease treatment when negative
v0.12.0b3,"for discrete treatment, stack a zero result in front for control"
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,TODO: conisder working around relying on sklearn implementation details
v0.12.0b3,"Found a good split, return."
v0.12.0b3,Record all splits in case the stratification by weight yeilds a worse partition
v0.12.0b3,Reseed random generator and try again
v0.12.0b3,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.12.0b3,"Found a good split, return."
v0.12.0b3,Did not find a good split
v0.12.0b3,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.12.0b3,Return most weight-balanced partition
v0.12.0b3,Weight stratification algorithm
v0.12.0b3,Sort weights for weight strata search
v0.12.0b3,There are some leftover indices that have yet to be assigned
v0.12.0b3,Append stratum splits to overall splits
v0.12.0b3,"If classification methods produce multiple columns of output,"
v0.12.0b3,we need to manually encode classes to ensure consistent column ordering.
v0.12.0b3,We clone the estimator to make sure that all the folds are
v0.12.0b3,"independent, and that it is pickle-able."
v0.12.0b3,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.12.0b3,`predictions` is a list of method outputs from each fold.
v0.12.0b3,"If each of those is also a list, then treat this as a"
v0.12.0b3,multioutput-multiclass task. We need to separately concatenate
v0.12.0b3,the method outputs for each label into an `n_labels` long list.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Our classes that derive from sklearn ones sometimes include
v0.12.0b3,inherited docstrings that have embedded doctests; we need the following imports
v0.12.0b3,so that they don't break.
v0.12.0b3,TODO: consider working around relying on sklearn implementation details
v0.12.0b3,"Convert X, y into numpy arrays"
v0.12.0b3,Define fit parameters
v0.12.0b3,Some algorithms don't have a check_input option
v0.12.0b3,Check weights array
v0.12.0b3,Check that weights are size-compatible
v0.12.0b3,Normalize inputs
v0.12.0b3,Weight inputs
v0.12.0b3,Fit base class without intercept
v0.12.0b3,Fit Lasso
v0.12.0b3,Reset intercept
v0.12.0b3,The intercept is not calculated properly due the sqrt(weights) factor
v0.12.0b3,so it must be recomputed
v0.12.0b3,Fit lasso without weights
v0.12.0b3,Make weighted splitter
v0.12.0b3,Fit weighted model
v0.12.0b3,Make weighted splitter
v0.12.0b3,Fit weighted model
v0.12.0b3,Call weighted lasso on reduced design matrix
v0.12.0b3,Weighted tau
v0.12.0b3,Select optimal penalty
v0.12.0b3,Warn about consistency
v0.12.0b3,"Convert X, y into numpy arrays"
v0.12.0b3,Fit weighted lasso with user input
v0.12.0b3,"Center X, y"
v0.12.0b3,Calculate quantities that will be used later on. Account for centered data
v0.12.0b3,Calculate coefficient and error variance
v0.12.0b3,Add coefficient correction
v0.12.0b3,Set coefficients and intercept standard errors
v0.12.0b3,Set intercept
v0.12.0b3,Return alpha to 'auto' state
v0.12.0b3,"Note that in the case of no intercept, X_offset is 0"
v0.12.0b3,Calculate the variance of the predictions
v0.12.0b3,Calculate prediction confidence intervals
v0.12.0b3,Assumes flattened y
v0.12.0b3,Compute weighted residuals
v0.12.0b3,To be done once per target. Assumes y can be flattened.
v0.12.0b3,Assumes that X has already been offset
v0.12.0b3,Special case: n_features=1
v0.12.0b3,Compute Lasso coefficients for the columns of the design matrix
v0.12.0b3,Compute C_hat
v0.12.0b3,Compute theta_hat
v0.12.0b3,Allow for single output as well
v0.12.0b3,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.12.0b3,Set coef_ attribute
v0.12.0b3,Set intercept_ attribute
v0.12.0b3,Set selected_alpha_ attribute
v0.12.0b3,Set coef_stderr_
v0.12.0b3,intercept_stderr_
v0.12.0b3,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.12.0b3,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.12.0b3,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.12.0b3,set intercept_ attribute
v0.12.0b3,set coef_ attribute
v0.12.0b3,set alpha_ attribute
v0.12.0b3,set alphas_ attribute
v0.12.0b3,set n_iter_ attribute
v0.12.0b3,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.12.0b3,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.12.0b3,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.12.0b3,now regress X1 on y - X2 * beta2 to learn beta1
v0.12.0b3,set coef_ and intercept_ attributes
v0.12.0b3,Note that the penalized model should *not* have an intercept
v0.12.0b3,don't proxy special methods
v0.12.0b3,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.12.0b3,regressor incorrectly
v0.12.0b3,"Note: for known attributes that have been set this method will not be called,"
v0.12.0b3,so we should just throw here because this is an attribute belonging to this class
v0.12.0b3,but which hasn't yet been set on this instance
v0.12.0b3,set default values for None
v0.12.0b3,check freq_weight should be integer and should be accompanied by sample_var
v0.12.0b3,check array shape
v0.12.0b3,weight X and y and sample_var
v0.12.0b3,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,AzureML
v0.12.0b3,helper imports
v0.12.0b3,write the details of the workspace to a configuration file to the notebook library
v0.12.0b3,if y is a multioutput model
v0.12.0b3,Make sure second dimension has 1 or more item
v0.12.0b3,switch _inner Model to a MultiOutputRegressor
v0.12.0b3,flatten array as automl only takes vectors for y
v0.12.0b3,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.12.0b3,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.12.0b3,as an sklearn estimator
v0.12.0b3,fit implementation for a single output model.
v0.12.0b3,Create experiment for specified workspace
v0.12.0b3,Configure automl_config with training set information.
v0.12.0b3,"Wait for remote run to complete, the set the model"
v0.12.0b3,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.12.0b3,create model and pass model into final.
v0.12.0b3,"If item is an automl config, get its corresponding"
v0.12.0b3,AutomatedML Model and add it to new_Args
v0.12.0b3,"If item is an automl config, get its corresponding"
v0.12.0b3,AutomatedML Model and set it for this key in
v0.12.0b3,kwargs
v0.12.0b3,takes in either automated_ml config and instantiates
v0.12.0b3,an AutomatedMLModel
v0.12.0b3,The prefix can only be 18 characters long
v0.12.0b3,"because prefixes come from kwarg_names, we must ensure they are"
v0.12.0b3,short enough.
v0.12.0b3,Get workspace from config file.
v0.12.0b3,Take the intersect of the white for sample
v0.12.0b3,weights and linear models
v0.12.0b3,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,average the outcome dimension if it exists and ensure 2d y_pred
v0.12.0b3,get index of best treatment
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,TODO: consider working around relying on sklearn implementation details
v0.12.0b3,Create splits of causal tree
v0.12.0b3,Make sure the correct exception is being rethrown
v0.12.0b3,Must make sure indices are merged correctly
v0.12.0b3,Convert rows to columns
v0.12.0b3,Require group assignment t to be one-hot-encoded
v0.12.0b3,Get predictions for the 2 splits
v0.12.0b3,Must make sure indices are merged correctly
v0.12.0b3,Crossfitting
v0.12.0b3,Compute weighted nuisance estimates
v0.12.0b3,-------------------------------------------------------------------------------
v0.12.0b3,Calculate the covariance matrix corresponding to the BLB inference
v0.12.0b3,
v0.12.0b3,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.12.0b3,2. Calculate the weighted moments for each tree slice to create a matrix
v0.12.0b3,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.12.0b3,in that slice from the overall parameter estimate.
v0.12.0b3,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.12.0b3,-------------------------------------------------------------------------------
v0.12.0b3,Calclulate covariance matrix through BLB
v0.12.0b3,Estimators
v0.12.0b3,OrthoForest parameters
v0.12.0b3,Sub-forests
v0.12.0b3,Auxiliary attributes
v0.12.0b3,Fit check
v0.12.0b3,TODO: Check performance
v0.12.0b3,Must normalize weights
v0.12.0b3,Override the CATE inference options
v0.12.0b3,Add blb inference to parent's options
v0.12.0b3,Generate subsample indices
v0.12.0b3,Build trees in parallel
v0.12.0b3,Bootstraping has repetitions in tree sample
v0.12.0b3,Similar for `a` weights
v0.12.0b3,Bootstraping has repetitions in tree sample
v0.12.0b3,Define subsample size
v0.12.0b3,Safety check
v0.12.0b3,Draw points to create little bags
v0.12.0b3,Copy and/or define models
v0.12.0b3,Define nuisance estimators
v0.12.0b3,Define parameter estimators
v0.12.0b3,Define
v0.12.0b3,Need to redefine fit here for auto inference to work due to a quirk in how
v0.12.0b3,wrap_fit is defined
v0.12.0b3,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b3,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b3,Override to flatten output if T is flat
v0.12.0b3,Check that all discrete treatments are represented
v0.12.0b3,Nuissance estimates evaluated with cross-fitting
v0.12.0b3,Define 2-fold iterator
v0.12.0b3,Check if there is only one example of some class
v0.12.0b3,Define 2-fold iterator
v0.12.0b3,need safe=False when cloning for WeightedModelWrapper
v0.12.0b3,Compute residuals
v0.12.0b3,Compute coefficient by OLS on residuals
v0.12.0b3,"Parameter returned by LinearRegression is (d_T, )"
v0.12.0b3,Compute residuals
v0.12.0b3,Compute coefficient by OLS on residuals
v0.12.0b3,ell_2 regularization
v0.12.0b3,Ridge regression estimate
v0.12.0b3,"Parameter returned is of shape (d_T, )"
v0.12.0b3,Return moments and gradients
v0.12.0b3,Compute residuals
v0.12.0b3,Compute moments
v0.12.0b3,"Moments shape is (n, d_T)"
v0.12.0b3,Compute moment gradients
v0.12.0b3,returns shape-conforming residuals
v0.12.0b3,Copy and/or define models
v0.12.0b3,Define parameter estimators
v0.12.0b3,Define moment and mean gradient estimator
v0.12.0b3,"Check that T is shape (n, )"
v0.12.0b3,Check T is numeric
v0.12.0b3,Train label encoder
v0.12.0b3,Call `fit` from parent class
v0.12.0b3,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b3,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b3,Override to flatten output if T is flat
v0.12.0b3,Expand one-hot encoding to include the zero treatment
v0.12.0b3,"Test that T contains all treatments. If not, return None"
v0.12.0b3,Nuissance estimates evaluated with cross-fitting
v0.12.0b3,Define 2-fold iterator
v0.12.0b3,Check if there is only one example of some class
v0.12.0b3,No need to crossfit for internal nodes
v0.12.0b3,Compute partial moments
v0.12.0b3,"If any of the values in the parameter estimate is nan, return None"
v0.12.0b3,Compute partial moments
v0.12.0b3,Compute coefficient by OLS on residuals
v0.12.0b3,ell_2 regularization
v0.12.0b3,Ridge regression estimate
v0.12.0b3,"Parameter returned is of shape (d_T, )"
v0.12.0b3,Return moments and gradients
v0.12.0b3,Compute partial moments
v0.12.0b3,Compute moments
v0.12.0b3,"Moments shape is (n, d_T-1)"
v0.12.0b3,Compute moment gradients
v0.12.0b3,Need to calculate this in an elegant way for when propensity is 0
v0.12.0b3,This will flatten T
v0.12.0b3,Check that T is numeric
v0.12.0b3,Test whether the input estimator is supported
v0.12.0b3,Calculate confidence intervals for the parameter (marginal effect)
v0.12.0b3,Calculate confidence intervals for the effect
v0.12.0b3,Calculate the effects
v0.12.0b3,Calculate the standard deviations for the effects
v0.12.0b3,d_t=None here since we measure the effect across all Ts
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b3,Licensed under the MIT License.
v0.12.0b3,Causal tree parameters
v0.12.0b3,Tree structure
v0.12.0b3,No need for a random split since the data is already
v0.12.0b3,a random subsample from the original input
v0.12.0b3,node list stores the nodes that are yet to be splitted
v0.12.0b3,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b3,Create local sample set
v0.12.0b3,Compute nuisance estimates for the current node
v0.12.0b3,Nuisance estimate cannot be calculated
v0.12.0b3,Estimate parameter for current node
v0.12.0b3,Node estimate cannot be calculated
v0.12.0b3,Calculate moments and gradient of moments for current data
v0.12.0b3,Calculate inverse gradient
v0.12.0b3,The gradient matrix is not invertible.
v0.12.0b3,No good split can be found
v0.12.0b3,Calculate point-wise pseudo-outcomes rho
v0.12.0b3,a split is determined by a feature and a sample pair
v0.12.0b3,the number of possible splits is at most (number of features) * (number of node samples)
v0.12.0b3,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.12.0b3,parse row and column of random pair
v0.12.0b3,the sample of the pair is the integer division of the random number with n_feats
v0.12.0b3,calculate the binary indicator of whether sample i is on the left or the right
v0.12.0b3,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.12.0b3,calculate the number of samples on the left child for each proposed split
v0.12.0b3,calculate the analogous binary indicator for the samples in the estimation set
v0.12.0b3,calculate the number of estimation samples on the left child of each proposed split
v0.12.0b3,find the upper and lower bound on the size of the left split for the split
v0.12.0b3,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.12.0b3,on each side.
v0.12.0b3,similarly for the estimation sample set
v0.12.0b3,if there is no valid split then don't create any children
v0.12.0b3,filter only the valid splits
v0.12.0b3,calculate the average influence vector of the samples in the left child
v0.12.0b3,calculate the average influence vector of the samples in the right child
v0.12.0b3,take the square of each of the entries of the influence vectors and normalize
v0.12.0b3,by size of each child
v0.12.0b3,calculate the vector score of each candidate split as the average of left and right
v0.12.0b3,influence vectors
v0.12.0b3,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.12.0b3,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.12.0b3,where there might be large discontinuities in some parameter as the conditioning set varies
v0.12.0b3,calculate the scalar score of each split by aggregating across the vector of scores
v0.12.0b3,Find split that minimizes criterion
v0.12.0b3,Create child nodes with corresponding subsamples
v0.12.0b3,add the created children to the list of not yet split nodes
v0.12.0b2,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.12.0b2,configuration is all pulled from setup.cfg
v0.12.0b2,-*- coding: utf-8 -*-
v0.12.0b2,
v0.12.0b2,Configuration file for the Sphinx documentation builder.
v0.12.0b2,
v0.12.0b2,This file does only contain a selection of the most common options. For a
v0.12.0b2,full list see the documentation:
v0.12.0b2,http://www.sphinx-doc.org/en/master/config
v0.12.0b2,-- Path setup --------------------------------------------------------------
v0.12.0b2,"If extensions (or modules to document with autodoc) are in another directory,"
v0.12.0b2,add these directories to sys.path here. If the directory is relative to the
v0.12.0b2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.12.0b2,
v0.12.0b2,-- Project information -----------------------------------------------------
v0.12.0b2,-- General configuration ---------------------------------------------------
v0.12.0b2,"If your documentation needs a minimal Sphinx version, state it here."
v0.12.0b2,
v0.12.0b2,needs_sphinx = '1.0'
v0.12.0b2,"Add any Sphinx extension module names here, as strings. They can be"
v0.12.0b2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.12.0b2,ones.
v0.12.0b2,"Add any paths that contain templates here, relative to this directory."
v0.12.0b2,The suffix(es) of source filenames.
v0.12.0b2,You can specify multiple suffix as a list of string:
v0.12.0b2,
v0.12.0b2,"source_suffix = ['.rst', '.md']"
v0.12.0b2,The master toctree document.
v0.12.0b2,The language for content autogenerated by Sphinx. Refer to documentation
v0.12.0b2,for a list of supported languages.
v0.12.0b2,
v0.12.0b2,This is also used if you do content translation via gettext catalogs.
v0.12.0b2,"Usually you set ""language"" from the command line for these cases."
v0.12.0b2,"List of patterns, relative to source directory, that match files and"
v0.12.0b2,directories to ignore when looking for source files.
v0.12.0b2,This pattern also affects html_static_path and html_extra_path.
v0.12.0b2,The name of the Pygments (syntax highlighting) style to use.
v0.12.0b2,-- Options for HTML output -------------------------------------------------
v0.12.0b2,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.12.0b2,a list of builtin themes.
v0.12.0b2,
v0.12.0b2,Theme options are theme-specific and customize the look and feel of a theme
v0.12.0b2,"further.  For a list of options available for each theme, see the"
v0.12.0b2,documentation.
v0.12.0b2,
v0.12.0b2,"Add any paths that contain custom static files (such as style sheets) here,"
v0.12.0b2,"relative to this directory. They are copied after the builtin static files,"
v0.12.0b2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.12.0b2,html_static_path = ['_static']
v0.12.0b2,"Custom sidebar templates, must be a dictionary that maps document names"
v0.12.0b2,to template names.
v0.12.0b2,
v0.12.0b2,The default sidebars (for documents that don't match any pattern) are
v0.12.0b2,defined by theme itself.  Builtin themes are using these templates by
v0.12.0b2,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.12.0b2,'searchbox.html']``.
v0.12.0b2,
v0.12.0b2,html_sidebars = {}
v0.12.0b2,-- Options for HTMLHelp output ---------------------------------------------
v0.12.0b2,Output file base name for HTML help builder.
v0.12.0b2,-- Options for LaTeX output ------------------------------------------------
v0.12.0b2,The paper size ('letterpaper' or 'a4paper').
v0.12.0b2,
v0.12.0b2,"'papersize': 'letterpaper',"
v0.12.0b2,"The font size ('10pt', '11pt' or '12pt')."
v0.12.0b2,
v0.12.0b2,"'pointsize': '10pt',"
v0.12.0b2,Additional stuff for the LaTeX preamble.
v0.12.0b2,
v0.12.0b2,"'preamble': '',"
v0.12.0b2,Latex figure (float) alignment
v0.12.0b2,
v0.12.0b2,"'figure_align': 'htbp',"
v0.12.0b2,Grouping the document tree into LaTeX files. List of tuples
v0.12.0b2,"(source start file, target name, title,"
v0.12.0b2,"author, documentclass [howto, manual, or own class])."
v0.12.0b2,-- Options for manual page output ------------------------------------------
v0.12.0b2,One entry per manual page. List of tuples
v0.12.0b2,"(source start file, name, description, authors, manual section)."
v0.12.0b2,-- Options for Texinfo output ----------------------------------------------
v0.12.0b2,Grouping the document tree into Texinfo files. List of tuples
v0.12.0b2,"(source start file, target name, title, author,"
v0.12.0b2,"dir menu entry, description, category)"
v0.12.0b2,-- Options for Epub output -------------------------------------------------
v0.12.0b2,Bibliographic Dublin Core info.
v0.12.0b2,The unique identifier of the text. This can be a ISBN number
v0.12.0b2,or the project homepage.
v0.12.0b2,
v0.12.0b2,epub_identifier = ''
v0.12.0b2,A unique identification for the text.
v0.12.0b2,
v0.12.0b2,epub_uid = ''
v0.12.0b2,A list of files that should not be packed into the epub file.
v0.12.0b2,-- Extension configuration -------------------------------------------------
v0.12.0b2,-- Options for intersphinx extension ---------------------------------------
v0.12.0b2,Example configuration for intersphinx: refer to the Python standard library.
v0.12.0b2,-- Options for todo extension ----------------------------------------------
v0.12.0b2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.12.0b2,-- Options for doctest extension -------------------------------------------
v0.12.0b2,we can document otherwise excluded entities here by returning False
v0.12.0b2,or skip otherwise included entities by returning True
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Calculate residuals
v0.12.0b2,Estimate E[T_res | Z_res]
v0.12.0b2,TODO. Deal with multi-class instrument
v0.12.0b2,Calculate nuisances
v0.12.0b2,Estimate E[T_res | Z_res]
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"We do a three way split, as typically a preliminary theta estimator would require"
v0.12.0b2,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.12.0b2,TODO. Deal with multi-class instrument
v0.12.0b2,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b2,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b2,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b2,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b2,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b2,Estimate preliminary theta in cross fitting manner
v0.12.0b2,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b2,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.12.0b2,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.12.0b2,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b2,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.12.0b2,TODO. The solution below is not really a valid cross-fitting
v0.12.0b2,as the test data are used to create the proj_t on the train
v0.12.0b2,which in the second train-test loop is used to create the nuisance
v0.12.0b2,cov on the test data. Hence the T variable of some sample
v0.12.0b2,"is implicitly correlated with its cov nuisance, through this flow"
v0.12.0b2,"of information. However, this seems a rather weak correlation."
v0.12.0b2,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.12.0b2,model.
v0.12.0b2,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.12.0b2,Estimate preliminary theta in cross fitting manner
v0.12.0b2,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b2,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.12.0b2,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b2,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.12.0b2,#############################################################################
v0.12.0b2,Classes for the DRIV implementation for the special case of intent-to-treat
v0.12.0b2,A/B test
v0.12.0b2,#############################################################################
v0.12.0b2,Estimate preliminary theta in cross fitting manner
v0.12.0b2,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b2,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b2,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b2,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b2,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.12.0b2,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.12.0b2,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.12.0b2,model_T_XZ = lambda: model_clf()
v0.12.0b2,#'days_visited': lambda:
v0.12.0b2,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.12.0b2,Turn strings into categories for numeric mapping
v0.12.0b2,### Defining some generic regressors and classifiers
v0.12.0b2,This a generic non-parametric regressor
v0.12.0b2,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b2,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.12.0b2,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b2,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.12.0b2,model = lambda: RandomForestRegressor(n_estimators=100)
v0.12.0b2,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.12.0b2,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.12.0b2,model = lambda: LinearRegression(n_jobs=-1)
v0.12.0b2,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.12.0b2,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.12.0b2,underlying model whenever predict is called.
v0.12.0b2,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b2,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.12.0b2,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b2,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.12.0b2,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.12.0b2,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.12.0b2,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.12.0b2,We need to specify models to be used for each of these residualizations
v0.12.0b2,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.12.0b2,"E[T | X, Z]"
v0.12.0b2,E[TZ | X]
v0.12.0b2,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.12.0b2,n_splits determines the number of splits to be used for cross-fitting.
v0.12.0b2,# Algorithm 2 - Current Method
v0.12.0b2,In[121]:
v0.12.0b2,# Algorithm 3 - DRIV ATE
v0.12.0b2,dmliv_model_effect = lambda: model()
v0.12.0b2,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.12.0b2,"dmliv_model_effect(),"
v0.12.0b2,n_splits=1)
v0.12.0b2,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.12.0b2,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.12.0b2,"Once multiple treatments are supported, we'll need to fix this"
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b2,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b2,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b2,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,TODO. Deal with multi-class instrument/treatment
v0.12.0b2,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.12.0b2,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.12.0b2,Estimate p(X) = E[T | X] in cross-fitting manner
v0.12.0b2,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.12.0b2,##################
v0.12.0b2,Global settings #
v0.12.0b2,##################
v0.12.0b2,Global plotting controls
v0.12.0b2,"Control for support size, can control for more"
v0.12.0b2,#################
v0.12.0b2,File utilities #
v0.12.0b2,#################
v0.12.0b2,#################
v0.12.0b2,Plotting utils #
v0.12.0b2,#################
v0.12.0b2,bias
v0.12.0b2,var
v0.12.0b2,rmse
v0.12.0b2,r2
v0.12.0b2,Infer feature dimension
v0.12.0b2,Metrics by support plots
v0.12.0b2,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.12.0b2,Vasilis Syrgkanis <vasy@microsoft.com>
v0.12.0b2,Steven Wu <zhiww@microsoft.com>
v0.12.0b2,Initialize causal tree parameters
v0.12.0b2,Create splits of causal tree
v0.12.0b2,Estimate treatment effects at the leafs
v0.12.0b2,Compute heterogeneous treatement effect for x's in x_list by finding
v0.12.0b2,the corresponding split and associating the effect computed on that leaf
v0.12.0b2,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.12.0b2,Safety check
v0.12.0b2,Weighted linear regression
v0.12.0b2,Calculates weights
v0.12.0b2,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b2,over all indices
v0.12.0b2,Similar for `a` weights
v0.12.0b2,Doesn't have sample weights
v0.12.0b2,Is a linear model
v0.12.0b2,Weighted linear regression
v0.12.0b2,Calculates weights
v0.12.0b2,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b2,over all indices
v0.12.0b2,Similar for `a` weights
v0.12.0b2,normalize weights
v0.12.0b2,"Split the data in half, train and test"
v0.12.0b2,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b2,"a function of W, using only the train fold"
v0.12.0b2,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b2,"Split the data in half, train and test"
v0.12.0b2,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b2,"a function of W, using only the train fold"
v0.12.0b2,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b2,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.12.0b2,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.12.0b2,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.12.0b2,"Split the data in half, train and test"
v0.12.0b2,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b2,"a function of x, using only the train fold"
v0.12.0b2,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b2,Compute coefficient by OLS on residuals
v0.12.0b2,"Split the data in half, train and test"
v0.12.0b2,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b2,"a function of x, using only the train fold"
v0.12.0b2,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b2,Estimate multipliers for second order orthogonal method
v0.12.0b2,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.12.0b2,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b2,Create local sample set
v0.12.0b2,compute the base estimate for the current node using double ml or second order double ml
v0.12.0b2,compute the influence functions here that are used for the criterion
v0.12.0b2,generate random proposals of dimensions to split
v0.12.0b2,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.12.0b2,compute criterion for each proposal
v0.12.0b2,if splitting creates valid leafs in terms of mean leaf size
v0.12.0b2,Calculate criterion for split
v0.12.0b2,Else set criterion to infinity so that this split is not chosen
v0.12.0b2,If no good split was found
v0.12.0b2,Find split that minimizes criterion
v0.12.0b2,Set the split attributes at the node
v0.12.0b2,Create child nodes with corresponding subsamples
v0.12.0b2,Recursively split children
v0.12.0b2,Return parent node
v0.12.0b2,estimate the local parameter at the leaf using the estimate data
v0.12.0b2,###################
v0.12.0b2,Argument parsing #
v0.12.0b2,###################
v0.12.0b2,#########################################
v0.12.0b2,Parameters constant across experiments #
v0.12.0b2,#########################################
v0.12.0b2,Outcome support
v0.12.0b2,Treatment support
v0.12.0b2,Evaluation grid
v0.12.0b2,Treatment effects array
v0.12.0b2,Other variables
v0.12.0b2,##########################
v0.12.0b2,Data Generating Process #
v0.12.0b2,##########################
v0.12.0b2,Log iteration
v0.12.0b2,"Generate controls, features, treatment and outcome"
v0.12.0b2,T and Y residuals to be used in later scripts
v0.12.0b2,Save generated dataset
v0.12.0b2,#################
v0.12.0b2,ORF parameters #
v0.12.0b2,#################
v0.12.0b2,######################################
v0.12.0b2,Train and evaluate treatment effect #
v0.12.0b2,######################################
v0.12.0b2,########
v0.12.0b2,Plots #
v0.12.0b2,########
v0.12.0b2,###############
v0.12.0b2,Save results #
v0.12.0b2,###############
v0.12.0b2,##############
v0.12.0b2,Run Rscript #
v0.12.0b2,##############
v0.12.0b2,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b2,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b2,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.12.0b2,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b2,def mlasso_model(): return MultiTaskLassoCV(
v0.12.0b2,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b2,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b2,heterogeneity
v0.12.0b2,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b2,heterogeneity
v0.12.0b2,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b2,heterogeneity
v0.12.0b2,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.12.0b2,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b2,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b2,subset of features that are exogenous and create heterogeneity
v0.12.0b2,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.12.0b2,subset of features wrt we estimate heterogeneity
v0.12.0b2,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b2,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,introspect the constructor arguments to find the model parameters
v0.12.0b2,to represent
v0.12.0b2,"if the argument is deprecated, ignore it"
v0.12.0b2,Extract and sort argument names excluding 'self'
v0.12.0b2,column names
v0.12.0b2,transfer input to numpy arrays
v0.12.0b2,transfer input to 2d arrays
v0.12.0b2,create dataframe
v0.12.0b2,currently dowhy only support single outcome and single treatment
v0.12.0b2,call dowhy
v0.12.0b2,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.12.0b2,cate estimator but not the effect.
v0.12.0b2,don't proxy special methods
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Check if model is sparse enough for this model
v0.12.0b2,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.12.0b2,TODO: any way to avoid creating a copy if the array was already dense?
v0.12.0b2,"the call is necessary if the input was something like a list, though"
v0.12.0b2,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.12.0b2,so convert to pydata sparse first
v0.12.0b2,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.12.0b2,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.12.0b2,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.12.0b2,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b2,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b2,For when checking input values is disabled
v0.12.0b2,Type to column extraction function
v0.12.0b2,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.12.0b2,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.12.0b2,Get feature names using featurizer
v0.12.0b2,All attempts at retrieving transformed feature names have failed
v0.12.0b2,Delegate handling to downstream logic
v0.12.0b2,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.12.0b2,same number of input definitions as arrays
v0.12.0b2,input definitions have same number of dimensions as each array
v0.12.0b2,all result indices are unique
v0.12.0b2,all result indices must match at least one input index
v0.12.0b2,"map indices to all array, axis pairs for that index"
v0.12.0b2,each index has the same cardinality wherever it appears
v0.12.0b2,"State: list of (set of letters, list of (corresponding indices, value))"
v0.12.0b2,Algo: while list contains more than one entry
v0.12.0b2,take two entries
v0.12.0b2,sort both lists by intersection of their indices
v0.12.0b2,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.12.0b2,"take the union of indices and the product of values), stepping through each list linearly"
v0.12.0b2,TODO: might be faster to break into connected components first
v0.12.0b2,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.12.0b2,"so compute their content separately, then take cartesian product"
v0.12.0b2,this would save a few pointless sorts by empty tuples
v0.12.0b2,TODO: Consider investigating other performance ideas for these cases
v0.12.0b2,where the dense method beat the sparse method (usually sparse is faster)
v0.12.0b2,"e,facd,c->cfed"
v0.12.0b2,sparse: 0.0335489
v0.12.0b2,dense:  0.011465999999999997
v0.12.0b2,"gbd,da,egb->da"
v0.12.0b2,sparse: 0.0791625
v0.12.0b2,dense:  0.007319099999999995
v0.12.0b2,"dcc,d,faedb,c->abe"
v0.12.0b2,sparse: 1.2868097
v0.12.0b2,dense:  0.44605229999999985
v0.12.0b2,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.12.0b2,TODO: would using einsum's paths to optimize the order of merging help?
v0.12.0b2,assume that we should perform nested cross-validation if and only if
v0.12.0b2,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.12.0b2,logic copied from check_cv
v0.12.0b2,otherwise we will assume the user already set the cv attribute to something
v0.12.0b2,compatible with splitting with a 'groups' argument
v0.12.0b2,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.12.0b2,don't use the groups when calling split internally
v0.12.0b2,Normalize weights
v0.12.0b2,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.12.0b2,"if we're decorating a class, just update the __init__ method,"
v0.12.0b2,so that the result is still a class instead of a wrapper method
v0.12.0b2,"want to enforce that each bad_arg was either in kwargs,"
v0.12.0b2,or else it was in neither and is just taking its default value
v0.12.0b2,Any access should throw
v0.12.0b2,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b2,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b2,input feature name is already updated by cate_feature_names.
v0.12.0b2,define the index of d_x to filter for each given T
v0.12.0b2,filter X after broadcast with T for each given T
v0.12.0b2,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b2,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,
v0.12.0b2,This code contains some snippets of code from:
v0.12.0b2,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
v0.12.0b2,published under the following license and copyright:
v0.12.0b2,BSD 3-Clause License
v0.12.0b2,
v0.12.0b2,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b2,All rights reserved.
v0.12.0b2,make any access to matplotlib or plt throw an exception
v0.12.0b2,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.12.0b2,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.12.0b2,Initialize saturation & value; calculate chroma & value shift
v0.12.0b2,Calculate some intermediate values
v0.12.0b2,Initialize RGB with same hue & chroma as our color
v0.12.0b2,Shift the initial RGB values to match value and store
v0.12.0b2,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.12.0b2,clean way of achieving this
v0.12.0b2,make sure we don't accidentally escape anything in the substitution
v0.12.0b2,Fetch appropriate color for node
v0.12.0b2,"red for negative, green for positive"
v0.12.0b2,in multi-target use mean of targets
v0.12.0b2,Write node mean CATE
v0.12.0b2,Write node std of CATE
v0.12.0b2,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.12.0b2,Fetch appropriate color for node
v0.12.0b2,Write node mean CATE
v0.12.0b2,Write node mean CATE
v0.12.0b2,Write recommended treatment and value - cost
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"since inference objects can be stateful, we must copy it before fitting;"
v0.12.0b2,otherwise this sequence wouldn't work:
v0.12.0b2,"est1.fit(..., inference=inf)"
v0.12.0b2,"est2.fit(..., inference=inf)"
v0.12.0b2,est1.effect_interval(...)
v0.12.0b2,because inf now stores state from fitting est2
v0.12.0b2,This flag is true when names are set in a child class instead
v0.12.0b2,"If names are set in a child class, add an attribute reflecting that"
v0.12.0b2,This works only if X is passed as a kwarg
v0.12.0b2,We plan to enforce X as kwarg only in future releases
v0.12.0b2,This checks if names have been set in a child class
v0.12.0b2,"If names were set in a child class, don't do it again"
v0.12.0b2,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.12.0b2,call the wrapped fit method
v0.12.0b2,NOTE: we call inference fit *after* calling the main fit method
v0.12.0b2,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.12.0b2,but tensordot can't be applied to this problem because we don't sum over m
v0.12.0b2,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.12.0b2,of rows of T was not taken into account
v0.12.0b2,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.12.0b2,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.12.0b2,"Treatment names is None, default to BaseCateEstimator"
v0.12.0b2,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.12.0b2,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.12.0b2,Get input names
v0.12.0b2,Summary
v0.12.0b2,add statsmodels to parent's options
v0.12.0b2,add debiasedlasso to parent's options
v0.12.0b2,add blb to parent's options
v0.12.0b2,TODO Share some logic with non-discrete version
v0.12.0b2,Get input names
v0.12.0b2,Summary
v0.12.0b2,add statsmodels to parent's options
v0.12.0b2,add statsmodels to parent's options
v0.12.0b2,add blb to parent's options
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,remove None arguments
v0.12.0b2,"scores entries should be lists of scores, so make each entry a singleton list"
v0.12.0b2,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b2,generate an instance of the final model
v0.12.0b2,generate an instance of the nuisance model
v0.12.0b2,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.12.0b2,alteration even when we only want to fit_final.
v0.12.0b2,use a binary array to get stratified split in case of discrete treatment
v0.12.0b2,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.12.0b2,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.12.0b2,"however, sklearn doesn't support both stratifying and grouping (see"
v0.12.0b2,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.12.0b2,their own object that supports grouping if they want to use groups.
v0.12.0b2,for each mc iteration
v0.12.0b2,for each model under cross fit setting
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,
v0.12.0b2,This code contains snippets of code from
v0.12.0b2,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b2,published under the following license and copyright:
v0.12.0b2,BSD 3-Clause License
v0.12.0b2,
v0.12.0b2,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b2,All rights reserved.
v0.12.0b2,=============================================================================
v0.12.0b2,Policy Forest
v0.12.0b2,=============================================================================
v0.12.0b2,Remap output
v0.12.0b2,reshape is necessary to preserve the data contiguity against vs
v0.12.0b2,"[:, np.newaxis] that does not."
v0.12.0b2,Get subsample sample size
v0.12.0b2,Check parameters
v0.12.0b2,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b2,if this is the first `fit` call of the warm start mode.
v0.12.0b2,"Free allocated memory, if any"
v0.12.0b2,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b2,We draw from the random state to get the random state we
v0.12.0b2,would have got if we hadn't used a warm_start.
v0.12.0b2,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b2,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b2,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b2,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b2,for fitting the trees is internally releasing the Python GIL
v0.12.0b2,making threading more efficient than multiprocessing in
v0.12.0b2,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b2,"parallel_backend contexts set at a higher level,"
v0.12.0b2,since correctness does not rely on using threads.
v0.12.0b2,Collect newly grown trees
v0.12.0b2,Check data
v0.12.0b2,Assign chunk of trees to jobs
v0.12.0b2,avoid storing the output of every estimator by summing them here
v0.12.0b2,Parallel loop
v0.12.0b2,Check data
v0.12.0b2,Assign chunk of trees to jobs
v0.12.0b2,avoid storing the output of every estimator by summing them here
v0.12.0b2,Parallel loop
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,
v0.12.0b2,This code contains snippets of code from:
v0.12.0b2,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b2,published under the following license and copyright:
v0.12.0b2,BSD 3-Clause License
v0.12.0b2,
v0.12.0b2,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b2,All rights reserved.
v0.12.0b2,=============================================================================
v0.12.0b2,Types and constants
v0.12.0b2,=============================================================================
v0.12.0b2,=============================================================================
v0.12.0b2,Base Policy tree
v0.12.0b2,=============================================================================
v0.12.0b2,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.12.0b2,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.12.0b2,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.12.0b2,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.12.0b2,checks that X is 2D array.
v0.12.0b2,"since we only allow single dimensional y, we could flatten the prediction"
v0.12.0b2,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b2,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b2,Handles the corner case when X=None but featurizer might be not None
v0.12.0b2,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.12.0b2,"Replacing this method which is invalid for this class, so that we make the"
v0.12.0b2,dosctring empty and not appear in the docs.
v0.12.0b2,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b2,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.12.0b2,Replacing to remove docstring
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"if both X and W are None, just return a column of ones"
v0.12.0b2,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b2,We need to go back to the label representation of the one-hot so as to call
v0.12.0b2,the classifier.
v0.12.0b2,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b2,We need to go back to the label representation of the one-hot so as to call
v0.12.0b2,the classifier.
v0.12.0b2,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b2,This works both with our without the weighting trick as the treatments T are unit vector
v0.12.0b2,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.12.0b2,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.12.0b2,both Parametric and Non Parametric DML.
v0.12.0b2,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.12.0b2,attribute so that the trained featurizer will be passed through
v0.12.0b2,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b2,for internal use by the library
v0.12.0b2,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b2,We need to use the rlearner's copy to retain the information from fitting
v0.12.0b2,Handles the corner case when X=None but featurizer might be not None
v0.12.0b2,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b2,since we clone it and fit separate copies
v0.12.0b2,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b2,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b2,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b2,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b2,since we clone it and fit separate copies
v0.12.0b2,add blb to parent's options
v0.12.0b2,override only so that we can update the docstring to indicate
v0.12.0b2,support for `GenericSingleTreatmentModelFinalInference`
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,note that groups are not passed to score because they are only used for fitting
v0.12.0b2,note that groups are not passed to score because they are only used for fitting
v0.12.0b2,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b2,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b2,NOTE: important to get parent's wrapped copy so that
v0.12.0b2,"after training wrapped featurizer is also trained, etc."
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b2,Fit a doubly robust average effect
v0.12.0b2,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b2,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b2,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b2,since we clone it and fit separate copies
v0.12.0b2,"If custom param grid, check that only estimator parameters are being altered"
v0.12.0b2,"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
v0.12.0b2,override only so that we can update the docstring to indicate support for `blb`
v0.12.0b2,Get input names
v0.12.0b2,Summary
v0.12.0b2,Determine output settings
v0.12.0b2,"Important: This must be the first invocation of the random state at fit time, so that"
v0.12.0b2,train/test splits are re-generatable from an external object simply by knowing the
v0.12.0b2,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.12.0b2,linear predictions. Currently is also useful for testing.
v0.12.0b2,reshape is necessary to preserve the data contiguity against vs
v0.12.0b2,"[:, np.newaxis] that does not."
v0.12.0b2,Check parameters
v0.12.0b2,Set min_weight_leaf from min_weight_fraction_leaf
v0.12.0b2,Build tree
v0.12.0b2,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.12.0b2,hold. Used by criterion for memory space savings.
v0.12.0b2,Initialize the criterion object and the criterion_val object if honest.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,
v0.12.0b2,This code is a fork from:
v0.12.0b2,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.12.0b2,published under the following license and copyright:
v0.12.0b2,BSD 3-Clause License
v0.12.0b2,
v0.12.0b2,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b2,All rights reserved.
v0.12.0b2,Set parameters
v0.12.0b2,Don't instantiate estimators now! Parameters of base_estimator might
v0.12.0b2,"still change. Eg., when grid-searching with the nested object syntax."
v0.12.0b2,self.estimators_ needs to be filled by the derived classes in fit.
v0.12.0b2,Compute the number of jobs
v0.12.0b2,Partition estimators between jobs
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Remove children with nonwhite mothers from the treatment group
v0.12.0b2,Remove children with nonwhite mothers from the treatment group
v0.12.0b2,Select columns
v0.12.0b2,Scale the numeric variables
v0.12.0b2,"Change the binary variable 'first' takes values in {1,2}"
v0.12.0b2,Append a column of ones as intercept
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b2,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b2,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.12.0b2,d_t=None here since we measure the effect across all Ts
v0.12.0b2,once the estimator has been fit
v0.12.0b2,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.12.0b2,an intercept even when bias is part of coef.
v0.12.0b2,We can write effect inference as a function of prediction and prediction standard error of
v0.12.0b2,the final method for linear models
v0.12.0b2,squeeze the first axis
v0.12.0b2,d_t=None here since we measure the effect across all Ts
v0.12.0b2,set the mean_pred_stderr
v0.12.0b2,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b2,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b2,"send treatment to the end, pull bounds to the front"
v0.12.0b2,d_t=None here since we measure the effect across all Ts
v0.12.0b2,set the mean_pred_stderr
v0.12.0b2,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.12.0b2,d_t=None here since we measure the effect across all Ts
v0.12.0b2,d_t=None here since we measure the effect across all Ts
v0.12.0b2,need to set the fit args before the estimator is fit
v0.12.0b2,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b2,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.12.0b2,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.12.0b2,NOTE: use np.asarray(offset) because if offset is a pd.Series direct addition would make the sum
v0.12.0b2,"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
v0.12.0b2,NOTE: use np.asarray(factor) because if offset is a pd.Series direct addition would make the product
v0.12.0b2,"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
v0.12.0b2,scale preds
v0.12.0b2,scale std errs
v0.12.0b2,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.12.0b2,offset preds
v0.12.0b2,"offset the distribution, too"
v0.12.0b2,scale preds
v0.12.0b2,"scale the distribution, too"
v0.12.0b2,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b2,1. Uncertainty of Mean Point Estimate
v0.12.0b2,2. Distribution of Point Estimate
v0.12.0b2,3. Total Variance of Point Estimate
v0.12.0b2,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.12.0b2,so bail out early; note that it might be possible to correct the algorithm for
v0.12.0b2,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.12.0b2,be clean
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,TODO: Add a __dir__ implementation?
v0.12.0b2,don't proxy special methods
v0.12.0b2,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.12.0b2,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.12.0b2,then we should be computing a confidence interval for the wrapped calls
v0.12.0b2,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.12.0b2,second level bootstrap which would be prohibitive computationally?
v0.12.0b2,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.12.0b2,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.12.0b2,can't import from econml.inference at top level without creating cyclical dependencies
v0.12.0b2,Note that inference results are always methods even if the inference is for a property
v0.12.0b2,(e.g. coef__inference() is a method but coef_ is a property)
v0.12.0b2,Therefore we must insert a lambda if getting inference for a non-callable
v0.12.0b2,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.12.0b2,"try to get interval/std first if appropriate,"
v0.12.0b2,since we don't prefer a wrapped method with this name
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,
v0.12.0b2,This code contains snippets of code from:
v0.12.0b2,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b2,published under the following license and copyright:
v0.12.0b2,BSD 3-Clause License
v0.12.0b2,
v0.12.0b2,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b2,All rights reserved.
v0.12.0b2,=============================================================================
v0.12.0b2,Types and constants
v0.12.0b2,=============================================================================
v0.12.0b2,=============================================================================
v0.12.0b2,Base GRF tree
v0.12.0b2,=============================================================================
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,=============================================================================
v0.12.0b2,A MultOutputWrapper for GRF classes
v0.12.0b2,=============================================================================
v0.12.0b2,=============================================================================
v0.12.0b2,Instantiations of Generalized Random Forest
v0.12.0b2,=============================================================================
v0.12.0b2,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.12.0b2,in front of the constant treatment is the intercept in the moment equation.
v0.12.0b2,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.12.0b2,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,
v0.12.0b2,This code contains snippets of code from
v0.12.0b2,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b2,published under the following license and copyright:
v0.12.0b2,BSD 3-Clause License
v0.12.0b2,
v0.12.0b2,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b2,All rights reserved.
v0.12.0b2,=============================================================================
v0.12.0b2,Base Generalized Random Forest
v0.12.0b2,=============================================================================
v0.12.0b2,TODO: support freq_weight and sample_var
v0.12.0b2,Remap output
v0.12.0b2,reshape is necessary to preserve the data contiguity against vs
v0.12.0b2,"[:, np.newaxis] that does not."
v0.12.0b2,reshape is necessary to preserve the data contiguity against vs
v0.12.0b2,"[:, np.newaxis] that does not."
v0.12.0b2,Get subsample sample size
v0.12.0b2,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.12.0b2,We calculate the min eigenvalue proxy that each criterion is considering
v0.12.0b2,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.12.0b2,Check parameters
v0.12.0b2,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b2,if this is the first `fit` call of the warm start mode.
v0.12.0b2,"Free allocated memory, if any"
v0.12.0b2,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b2,We draw from the random state to get the random state we
v0.12.0b2,would have got if we hadn't used a warm_start.
v0.12.0b2,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b2,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b2,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b2,Generating indices a priori before parallelism ended up being orders of magnitude
v0.12.0b2,faster than how sklearn does it. The reason is that random samplers do not release the
v0.12.0b2,gil it seems.
v0.12.0b2,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b2,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b2,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b2,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b2,for fitting the trees is internally releasing the Python GIL
v0.12.0b2,making threading more efficient than multiprocessing in
v0.12.0b2,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b2,"parallel_backend contexts set at a higher level,"
v0.12.0b2,since correctness does not rely on using threads.
v0.12.0b2,Collect newly grown trees
v0.12.0b2,Check data
v0.12.0b2,Assign chunk of trees to jobs
v0.12.0b2,avoid storing the output of every estimator by summing them here
v0.12.0b2,Parallel loop
v0.12.0b2,Check data
v0.12.0b2,Assign chunk of trees to jobs
v0.12.0b2,Parallel loop
v0.12.0b2,Check data
v0.12.0b2,Assign chunk of trees to jobs
v0.12.0b2,Parallel loop
v0.12.0b2,####################
v0.12.0b2,Variance correction
v0.12.0b2,####################
v0.12.0b2,Subtract the average within bag variance. This ends up being equal to the
v0.12.0b2,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.12.0b2,The negative part is just sq_between.
v0.12.0b2,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.12.0b2,"The off diagonals we have no objective prior, so no correction is applied."
v0.12.0b2,Finally correcting the pred_cov or pred_var
v0.12.0b2,avoid storing the output of every estimator by summing them here
v0.12.0b2,Parallel loop
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,testing importances
v0.12.0b2,testing heterogeneity importances
v0.12.0b2,Testing that all parameters do what they are supposed to
v0.12.0b2,"testing predict, apply and decision path"
v0.12.0b2,test that the subsampling scheme past to the trees is correct
v0.12.0b2,The sample size is chosen in particular to test rounding based error when subsampling
v0.12.0b2,test that the estimator calcualtes var correctly
v0.12.0b2,test api
v0.12.0b2,test accuracy
v0.12.0b2,test the projection functionality of forests
v0.12.0b2,test that the estimator calcualtes var correctly
v0.12.0b2,test api
v0.12.0b2,test that the estimator calcualtes var correctly
v0.12.0b2,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b2,test that we raise errors in mishandled situations.
v0.12.0b2,test that the subsampling scheme past to the trees is correct
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,omit the lalonde notebook
v0.12.0b2,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.12.0b2,creating notebooks that are annoying for our users to actually run themselves
v0.12.0b2,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b2,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.12.0b2,"prior to calling interpret, can't plot, render, etc."
v0.12.0b2,can interpret without uncertainty
v0.12.0b2,can't interpret with uncertainty if inference wasn't used during fit
v0.12.0b2,can interpret with uncertainty if we refit
v0.12.0b2,can interpret without uncertainty
v0.12.0b2,can't treat before interpreting
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,simple DGP only for illustration
v0.12.0b2,Define the treatment model neural network architecture
v0.12.0b2,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b2,"so the input shape is (d_z + d_x,)"
v0.12.0b2,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b2,add extra layers on top for the mixture density network
v0.12.0b2,Define the response model neural network architecture
v0.12.0b2,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b2,"so the input shape is (d_t + d_x,)"
v0.12.0b2,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b2,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b2,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b2,so that the same weights will be reused in each instantiation
v0.12.0b2,number of samples to use in second estimate of the response
v0.12.0b2,(to make loss estimate unbiased)
v0.12.0b2,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b2,do something with predictions...
v0.12.0b2,also test vector t and y
v0.12.0b2,simple DGP only for illustration
v0.12.0b2,Define the treatment model neural network architecture
v0.12.0b2,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b2,"so the input shape is (d_z + d_x,)"
v0.12.0b2,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b2,add extra layers on top for the mixture density network
v0.12.0b2,Define the response model neural network architecture
v0.12.0b2,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b2,"so the input shape is (d_t + d_x,)"
v0.12.0b2,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b2,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b2,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b2,so that the same weights will be reused in each instantiation
v0.12.0b2,number of samples to use in second estimate of the response
v0.12.0b2,(to make loss estimate unbiased)
v0.12.0b2,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b2,do something with predictions...
v0.12.0b2,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.12.0b2,test = True ensures we draw test set images
v0.12.0b2,test = True ensures we draw test set images
v0.12.0b2,re-draw to get new independent treatment and implied response
v0.12.0b2,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b2,above is necesary so that reduced form doesn't win
v0.12.0b2,covariates: time and emotion
v0.12.0b2,random instrument
v0.12.0b2,z -> price
v0.12.0b2,true observable demand function
v0.12.0b2,errors
v0.12.0b2,response
v0.12.0b2,test = True ensures we draw test set images
v0.12.0b2,test = True ensures we draw test set images
v0.12.0b2,re-draw to get new independent treatment and implied response
v0.12.0b2,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b2,above is necesary so that reduced form doesn't win
v0.12.0b2,covariates: time and emotion
v0.12.0b2,random instrument
v0.12.0b2,z -> price
v0.12.0b2,true observable demand function
v0.12.0b2,errors
v0.12.0b2,response
v0.12.0b2,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.12.0b2,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.12.0b2,For some reason this doesn't work at all when run against the CNTK backend...
v0.12.0b2,"model.compile('nadam', loss=lambda _,l:l)"
v0.12.0b2,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.12.0b2,generate a valiation set
v0.12.0b2,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.12.0b2,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,DGP constants
v0.12.0b2,Generate data
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,testing importances
v0.12.0b2,testing heterogeneity importances
v0.12.0b2,Testing that all parameters do what they are supposed to
v0.12.0b2,"testing predict, apply and decision path"
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.12.0b2,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.12.0b2,so we need to transpose the result
v0.12.0b2,1-d output
v0.12.0b2,2-d output
v0.12.0b2,Single dimensional output y
v0.12.0b2,compare with weight
v0.12.0b2,compare with weight
v0.12.0b2,compare with weight
v0.12.0b2,compare with weight
v0.12.0b2,Multi-dimensional output y
v0.12.0b2,1-d y
v0.12.0b2,compare when both sample_var and sample_weight exist
v0.12.0b2,multi-d y
v0.12.0b2,compare when both sample_var and sample_weight exist
v0.12.0b2,compare when both sample_var and sample_weight exist
v0.12.0b2,compare when both sample_var and sample_weight exist
v0.12.0b2,compare when both sample_var and sample_weight exist
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,test that we can fit with the same arguments as the base estimator
v0.12.0b2,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b2,with the same shape for the lower and upper bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,test that we can do the same thing once we provide percentile bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b2,with the same shape for the lower and upper bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,test that we can do the same thing once we provide percentile bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,test that we can fit with the same arguments as the base estimator
v0.12.0b2,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b2,with the same shape for the lower and upper bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,test that we can do the same thing once we provide percentile bounds
v0.12.0b2,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b2,with the same shape for the lower and upper bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,test that we can do the same thing once we provide percentile bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,test that we can fit with the same arguments as the base estimator
v0.12.0b2,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b2,with the same shape for the lower and upper bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,test that we can do the same thing once we provide percentile bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b2,with the same shape for the lower and upper bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,test that we can do the same thing once we provide percentile bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b2,with the same shape for the lower and upper bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,test that the estimated effect is usually within the bounds
v0.12.0b2,test that we can do the same thing once we provide alpha explicitly
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,test that the estimated effect is usually within the bounds
v0.12.0b2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b2,with the same shape for the lower and upper bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,TODO: test that the estimated effect is usually within the bounds
v0.12.0b2,and that the true effect is also usually within the bounds
v0.12.0b2,test that we can do the same thing once we provide percentile bounds
v0.12.0b2,test that the lower and upper bounds differ
v0.12.0b2,TODO: test that the estimated effect is usually within the bounds
v0.12.0b2,and that the true effect is also usually within the bounds
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,test that the subsampling scheme past to the trees is correct
v0.12.0b2,test that the estimator calcualtes var correctly
v0.12.0b2,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b2,test that we raise errors in mishandled situations.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,DGP constants
v0.12.0b2,Generate data
v0.12.0b2,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b2,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b2,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.12.0b2,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b2,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.12.0b2,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b2,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b2,pvalue for second column should be greater than zero since some points are on either side
v0.12.0b2,of the tested value
v0.12.0b2,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.12.0b2,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b2,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b2,only is not None when T1 is a constant or a list of constant
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.12.0b2,Test non keyword based calls to fit
v0.12.0b2,test non-array inputs
v0.12.0b2,Test custom splitter
v0.12.0b2,Test incomplete set of test folds
v0.12.0b2,"y scores should be positive, since W predicts Y somewhat"
v0.12.0b2,"t scores might not be, since W and T are uncorrelated"
v0.12.0b2,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,make sure cross product varies more slowly with first array
v0.12.0b2,and that vectors are okay as inputs
v0.12.0b2,number of inputs in specification must match number of inputs
v0.12.0b2,must have an output
v0.12.0b2,output indices must be unique
v0.12.0b2,output indices must be present in an input
v0.12.0b2,number of indices must match number of dimensions for each input
v0.12.0b2,repeated indices must always have consistent sizes
v0.12.0b2,transpose
v0.12.0b2,tensordot
v0.12.0b2,trace
v0.12.0b2,TODO: set up proper flag for this
v0.12.0b2,pick indices at random with replacement from the first 7 letters of the alphabet
v0.12.0b2,"of all of the distinct indices that appear in any input,"
v0.12.0b2,pick a random subset of them (of size at most 5) to appear in the output
v0.12.0b2,creating an instance should warn
v0.12.0b2,using the instance should not warn
v0.12.0b2,using the deprecated method should warn
v0.12.0b2,don't warn if b and c are passed by keyword
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Preprocess data
v0.12.0b2,Convert 'week' to a date
v0.12.0b2,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.12.0b2,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.12.0b2,Take log of price
v0.12.0b2,Make brand numeric
v0.12.0b2,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.12.0b2,which have all zero coeffs
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,test at least one estimator from each category
v0.12.0b2,test causal graph
v0.12.0b2,test refutation estimate
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.12.0b2,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.12.0b2,TODO: test something rather than just print...
v0.12.0b2,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.12.0b2,pick some arbitrary X
v0.12.0b2,pick some arbitrary T
v0.12.0b2,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b2,The average variance should be lower when using monte carlo iterations
v0.12.0b2,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b2,The average variance should be lower when using monte carlo iterations
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,ensure that we've got at least two of every row
v0.12.0b2,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b2,need to make sure we get all *joint* combinations
v0.12.0b2,IntentToTreat only supports binary treatments/instruments
v0.12.0b2,IntentToTreat only supports binary treatments/instruments
v0.12.0b2,IntentToTreat requires X
v0.12.0b2,ensure we can serialize unfit estimator
v0.12.0b2,these support only W but not X
v0.12.0b2,"these support only binary, not general discrete T and Z"
v0.12.0b2,ensure we can serialize fit estimator
v0.12.0b2,make sure we can call the marginal_effect and effect methods
v0.12.0b2,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b2,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b2,"make sure we can call effect with implied scalar treatments,"
v0.12.0b2,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b2,are multiple treatments
v0.12.0b2,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b2,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.12.0b2,"however, with custom splits the checking happens in the first stage wrapper"
v0.12.0b2,where we don't have all of the required information to do this;
v0.12.0b2,we'd probably need to add it to _crossfit instead
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.12.0b2,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.12.0b2,The __warningregistry__'s need to be in a pristine state for tests
v0.12.0b2,to work properly.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Set random seed
v0.12.0b2,Generate data
v0.12.0b2,DGP constants
v0.12.0b2,Test data
v0.12.0b2,Constant treatment effect
v0.12.0b2,Constant treatment with multi output Y
v0.12.0b2,Heterogeneous treatment
v0.12.0b2,Heterogeneous treatment with multi output Y
v0.12.0b2,TLearner test
v0.12.0b2,Instantiate TLearner
v0.12.0b2,Test inputs
v0.12.0b2,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b2,Instantiate SLearner
v0.12.0b2,Test inputs
v0.12.0b2,Test constant treatment effect
v0.12.0b2,Test constant treatment effect with multi output Y
v0.12.0b2,Test heterogeneous treatment effect
v0.12.0b2,Need interactions between T and features
v0.12.0b2,Test heterogeneous treatment effect with multi output Y
v0.12.0b2,Instantiate XLearner
v0.12.0b2,Test inputs
v0.12.0b2,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b2,Instantiate DomainAdaptationLearner
v0.12.0b2,Test inputs
v0.12.0b2,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b2,Get the true treatment effect
v0.12.0b2,Get the true treatment effect
v0.12.0b2,Fit learner and get the effect and marginal effect
v0.12.0b2,Compute treatment effect residuals (absolute)
v0.12.0b2,Check that at least 90% of predictions are within tolerance interval
v0.12.0b2,Check whether the output shape is right
v0.12.0b2,Check that one can pass in regular lists
v0.12.0b2,Check that it fails correctly if lists of different shape are passed in
v0.12.0b2,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b2,Generate covariates
v0.12.0b2,Generate treatment
v0.12.0b2,Calculate outcome
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,DGP constants
v0.12.0b2,Generate data
v0.12.0b2,Test data
v0.12.0b2,Remove warnings that might be raised by the models passed into the ORF
v0.12.0b2,Generate data with continuous treatments
v0.12.0b2,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.12.0b2,does not work well with parallelism.
v0.12.0b2,Test inputs for continuous treatments
v0.12.0b2,--> Check that one can pass in regular lists
v0.12.0b2,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b2,Check that outputs have the correct shape
v0.12.0b2,Test continuous treatments with controls
v0.12.0b2,Test continuous treatments without controls
v0.12.0b2,Generate data with binary treatments
v0.12.0b2,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.12.0b2,does not work well with parallelism.
v0.12.0b2,Test inputs for binary treatments
v0.12.0b2,--> Check that one can pass in regular lists
v0.12.0b2,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b2,"--> Check that it works when T, Y have shape (n, 1)"
v0.12.0b2,"--> Check that it fails correctly when T has shape (n, 2)"
v0.12.0b2,--> Check that it fails correctly when the treatments are not numeric
v0.12.0b2,Check that outputs have the correct shape
v0.12.0b2,Test binary treatments with controls
v0.12.0b2,Test binary treatments without controls
v0.12.0b2,Only applicable to continuous treatments
v0.12.0b2,Generate data for 2 treatments
v0.12.0b2,Test multiple treatments with controls
v0.12.0b2,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.12.0b2,The rest for controls. Just as an example.
v0.12.0b2,Generating A/B test data
v0.12.0b2,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.12.0b2,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.12.0b2,Create a wrapper around Lasso that doesn't support weights
v0.12.0b2,since Lasso does natively support them starting in sklearn 0.23
v0.12.0b2,Generate data with continuous treatments
v0.12.0b2,Instantiate model with most of the default parameters
v0.12.0b2,Compute the treatment effect on test points
v0.12.0b2,Compute treatment effect residuals
v0.12.0b2,Multiple treatments
v0.12.0b2,Allow at most 10% test points to be outside of the tolerance interval
v0.12.0b2,Compute treatment effect residuals
v0.12.0b2,Multiple treatments
v0.12.0b2,Allow at most 20% test points to be outside of the confidence interval
v0.12.0b2,Check that the intervals are not too wide
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.12.0b2,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.12.0b2,ensure that we've got at least 6 of every element
v0.12.0b2,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.12.0b2,NOTE: this number may need to change if the default number of folds in
v0.12.0b2,WeightedStratifiedKFold changes
v0.12.0b2,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b2,ensure we can serialize the unfit estimator
v0.12.0b2,ensure we can pickle the fit estimator
v0.12.0b2,make sure we can call the marginal_effect and effect methods
v0.12.0b2,test const marginal inference
v0.12.0b2,test effect inference
v0.12.0b2,test marginal effect inference
v0.12.0b2,test coef__inference and intercept__inference
v0.12.0b2,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b2,"make sure we can call effect with implied scalar treatments,"
v0.12.0b2,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b2,are multiple treatments
v0.12.0b2,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b2,ensure that we've got at least two of every element
v0.12.0b2,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b2,make sure we can call the marginal_effect and effect methods
v0.12.0b2,test const marginal inference
v0.12.0b2,test effect inference
v0.12.0b2,test marginal effect inference
v0.12.0b2,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b2,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b2,We concatenate the two copies data
v0.12.0b2,make sure we can get out post-fit stuff
v0.12.0b2,create a simple artificial setup where effect of moving from treatment
v0.12.0b2,"1 -> 2 is 2,"
v0.12.0b2,"1 -> 3 is 1, and"
v0.12.0b2,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b2,"Using an uneven number of examples from different classes,"
v0.12.0b2,"and having the treatments in non-lexicographic order,"
v0.12.0b2,Should rule out some basic issues.
v0.12.0b2,test that we can fit with a KFold instance
v0.12.0b2,test that we can fit with a train/test iterable
v0.12.0b2,predetermined splits ensure that all features are seen in each split
v0.12.0b2,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.12.0b2,(incorrectly) use a final model with an intercept
v0.12.0b2,"Because final model is fixed, actual values of T and Y don't matter"
v0.12.0b2,Ensure reproducibility
v0.12.0b2,Sparse DGP
v0.12.0b2,Treatment effect coef
v0.12.0b2,Other coefs
v0.12.0b2,Features and controls
v0.12.0b2,Test sparse estimator
v0.12.0b2,"--> test coef_, intercept_"
v0.12.0b2,--> test treatment effects
v0.12.0b2,Restrict x_test to vectors of norm < 1
v0.12.0b2,--> check inference
v0.12.0b2,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b2,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.12.0b2,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.12.0b2,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.12.0b2,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.12.0b2,sparse test case: heterogeneous effect by product
v0.12.0b2,need at least as many rows in e_y as there are distinct columns
v0.12.0b2,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.12.0b2,create a simple artificial setup where effect of moving from treatment
v0.12.0b2,"a -> b is 2,"
v0.12.0b2,"a -> c is 1, and"
v0.12.0b2,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.12.0b2,"Using an uneven number of examples from different classes,"
v0.12.0b2,"and having the treatments in non-lexicographic order,"
v0.12.0b2,should rule out some basic issues.
v0.12.0b2,Note that explicitly specifying the dtype as object is necessary until
v0.12.0b2,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.12.0b2,estimated effects should be identical when treatment is explicitly given
v0.12.0b2,but const_marginal_effect should be reordered based on the explicit cagetories
v0.12.0b2,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.12.0b2,test outer grouping
v0.12.0b2,test nested grouping
v0.12.0b2,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b2,whichever groups we saw
v0.12.0b2,test nested grouping
v0.12.0b2,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b2,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b2,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Set random seed
v0.12.0b2,Generate data
v0.12.0b2,DGP constants
v0.12.0b2,Test data
v0.12.0b2,Constant treatment effect and propensity
v0.12.0b2,Heterogeneous treatment and propensity
v0.12.0b2,ensure that we've got at least two of every element
v0.12.0b2,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b2,ensure that we can serialize unfit estimator
v0.12.0b2,ensure that we can serialize fit estimator
v0.12.0b2,make sure we can call the marginal_effect and effect methods
v0.12.0b2,test const marginal inference
v0.12.0b2,test effect inference
v0.12.0b2,test marginal effect inference
v0.12.0b2,test coef_ and intercept_ inference
v0.12.0b2,verify we can generate the summary
v0.12.0b2,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b2,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b2,create a simple artificial setup where effect of moving from treatment
v0.12.0b2,"1 -> 2 is 2,"
v0.12.0b2,"1 -> 3 is 1, and"
v0.12.0b2,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b2,"Using an uneven number of examples from different classes,"
v0.12.0b2,"and having the treatments in non-lexicographic order,"
v0.12.0b2,Should rule out some basic issues.
v0.12.0b2,test that we can fit with a KFold instance
v0.12.0b2,test that we can fit with a train/test iterable
v0.12.0b2,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b2,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b2,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b2,test coef__inference function works
v0.12.0b2,test intercept__inference function works
v0.12.0b2,test summary function works
v0.12.0b2,Test inputs
v0.12.0b2,self._test_inputs(DR_learner)
v0.12.0b2,Test constant treatment effect
v0.12.0b2,Test heterogeneous treatment effect
v0.12.0b2,Test heterogenous treatment effect for W =/= None
v0.12.0b2,Sparse DGP
v0.12.0b2,Treatment effect coef
v0.12.0b2,Other coefs
v0.12.0b2,Features and controls
v0.12.0b2,Test sparse estimator
v0.12.0b2,"--> test coef_, intercept_"
v0.12.0b2,--> test treatment effects
v0.12.0b2,Restrict x_test to vectors of norm < 1
v0.12.0b2,--> check inference
v0.12.0b2,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b2,test outer grouping
v0.12.0b2,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.12.0b2,test nested grouping
v0.12.0b2,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b2,whichever groups we saw
v0.12.0b2,test nested grouping
v0.12.0b2,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b2,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b2,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b2,helper class
v0.12.0b2,Fit learner and get the effect
v0.12.0b2,Get the true treatment effect
v0.12.0b2,Compute treatment effect residuals (absolute)
v0.12.0b2,Check that at least 90% of predictions are within tolerance interval
v0.12.0b2,Only for heterogeneous TE
v0.12.0b2,Fit learner on X and W and get the effect
v0.12.0b2,Get the true treatment effect
v0.12.0b2,Compute treatment effect residuals (absolute)
v0.12.0b2,Check that at least 90% of predictions are within tolerance interval
v0.12.0b2,Check that one can pass in regular lists
v0.12.0b2,Check that it fails correctly if lists of different shape are passed in
v0.12.0b2,Check that it fails when T contains values other than 0 and 1
v0.12.0b2,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b2,Generate covariates
v0.12.0b2,Generate treatment
v0.12.0b2,Calculate outcome
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,DGP constants
v0.12.0b2,DGP coefficients
v0.12.0b2,Generated outcomes
v0.12.0b2,################
v0.12.0b2,WeightedLasso #
v0.12.0b2,################
v0.12.0b2,Define weights
v0.12.0b2,Define extended datasets
v0.12.0b2,Range of alphas
v0.12.0b2,Compare with Lasso
v0.12.0b2,--> No intercept
v0.12.0b2,--> With intercept
v0.12.0b2,When DGP has no intercept
v0.12.0b2,When DGP has intercept
v0.12.0b2,--> Coerce coefficients to be positive
v0.12.0b2,--> Toggle max_iter & tol
v0.12.0b2,Define weights
v0.12.0b2,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b2,Mixed DGP scenario.
v0.12.0b2,Define extended datasets
v0.12.0b2,Define weights
v0.12.0b2,Define multioutput
v0.12.0b2,##################
v0.12.0b2,WeightedLassoCV #
v0.12.0b2,##################
v0.12.0b2,Define alphas to test
v0.12.0b2,Compare with LassoCV
v0.12.0b2,--> No intercept
v0.12.0b2,--> With intercept
v0.12.0b2,--> Force parameters to be positive
v0.12.0b2,Choose a smaller n to speed-up process
v0.12.0b2,Compare fold weights
v0.12.0b2,Define weights
v0.12.0b2,Define extended datasets
v0.12.0b2,Define splitters
v0.12.0b2,WeightedKFold splitter
v0.12.0b2,Map weighted splitter to an extended splitter
v0.12.0b2,Define alphas to test
v0.12.0b2,Compare with LassoCV
v0.12.0b2,--> No intercept
v0.12.0b2,--> With intercept
v0.12.0b2,--> Force parameters to be positive
v0.12.0b2,###########################
v0.12.0b2,MultiTaskWeightedLassoCV #
v0.12.0b2,###########################
v0.12.0b2,Define alphas to test
v0.12.0b2,Define splitter
v0.12.0b2,Compare with MultiTaskLassoCV
v0.12.0b2,--> No intercept
v0.12.0b2,--> With intercept
v0.12.0b2,Define weights
v0.12.0b2,Define extended datasets
v0.12.0b2,Define splitters
v0.12.0b2,WeightedKFold splitter
v0.12.0b2,Map weighted splitter to an extended splitter
v0.12.0b2,Define alphas to test
v0.12.0b2,Compare with LassoCV
v0.12.0b2,--> No intercept
v0.12.0b2,--> With intercept
v0.12.0b2,#########################
v0.12.0b2,WeightedLassoCVWrapper #
v0.12.0b2,#########################
v0.12.0b2,perform 1D fit
v0.12.0b2,perform 2D fit
v0.12.0b2,################
v0.12.0b2,DebiasedLasso #
v0.12.0b2,################
v0.12.0b2,Test DebiasedLasso without weights
v0.12.0b2,--> Check debiased coeffcients without intercept
v0.12.0b2,--> Check debiased coeffcients with intercept
v0.12.0b2,--> Check 5-95 CI coverage for unit vectors
v0.12.0b2,Test DebiasedLasso with weights for one DGP
v0.12.0b2,Define weights
v0.12.0b2,Define extended datasets
v0.12.0b2,--> Check debiased coefficients
v0.12.0b2,Define weights
v0.12.0b2,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b2,--> Check debiased coeffcients
v0.12.0b2,Test that attributes propagate correctly
v0.12.0b2,Test MultiOutputDebiasedLasso without weights
v0.12.0b2,--> Check debiased coeffcients without intercept
v0.12.0b2,--> Check debiased coeffcients with intercept
v0.12.0b2,--> Check CI coverage
v0.12.0b2,Test MultiOutputDebiasedLasso with weights
v0.12.0b2,Define weights
v0.12.0b2,Define extended datasets
v0.12.0b2,--> Check debiased coefficients
v0.12.0b2,Unit vectors
v0.12.0b2,Unit vectors
v0.12.0b2,Check coeffcients and intercept are the same within tolerance
v0.12.0b2,Check results are similar with tolerance 1e-6
v0.12.0b2,Check if multitask
v0.12.0b2,Check that same alpha is chosen
v0.12.0b2,Check that the coefficients are similar
v0.12.0b2,selective ridge has a simple implementation that we can test against
v0.12.0b2,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.12.0b2,"it should be the case that when we set fit_intercept to true,"
v0.12.0b2,it doesn't matter whether the penalized model also fits an intercept or not
v0.12.0b2,create an extra copy of rows with weight 2
v0.12.0b2,"instead of a slice, explicitly return an array of indices"
v0.12.0b2,_penalized_inds is only set during fitting
v0.12.0b2,cv exists on penalized model
v0.12.0b2,now we can access _penalized_inds
v0.12.0b2,check that we can read the cv attribute back out from the underlying model
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b2,local index should have as many times entries as global as there were rows passed in
v0.12.0b2,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b2,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b2,policy value should exceed always treating with any treatment
v0.12.0b2,"global shape is (d_y, sum(d_t))"
v0.12.0b2,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b2,features; for categoricals they should appear #cats-1 times each
v0.12.0b2,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b2,local index should have as many times entries as global as there were rows passed in
v0.12.0b2,features; for categoricals they should appear #cats-1 times each
v0.12.0b2,"global shape is (d_y, sum(d_t))"
v0.12.0b2,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b2,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b2,policy value should exceed always treating with any treatment
v0.12.0b2,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b2,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b2,local index should have as many times entries as global as there were rows passed in
v0.12.0b2,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b2,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b2,policy value should exceed always treating with any treatment
v0.12.0b2,"global shape is (d_y, sum(d_t))"
v0.12.0b2,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b2,features; for categoricals they should appear #cats-1 times each
v0.12.0b2,make sure we don't run into problems dropping every index
v0.12.0b2,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b2,local index should have as many times entries as global as there were rows passed in
v0.12.0b2,"global shape is (d_y, sum(d_t))"
v0.12.0b2,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b2,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b2,policy value should exceed always treating with any treatment
v0.12.0b2,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b2,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b2,local index should have as many times entries as global as there were rows passed in
v0.12.0b2,features; for categoricals they should appear #cats-1 times each
v0.12.0b2,"global shape is (d_y, sum(d_t))"
v0.12.0b2,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b2,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b2,policy value should exceed always treating with any treatment
v0.12.0b2,dgp
v0.12.0b2,model
v0.12.0b2,model
v0.12.0b2,"columns 'd', 'e', 'h' have too many values"
v0.12.0b2,"columns 'd', 'e' have too many values"
v0.12.0b2,lowering bound shouldn't affect already fit columns when warm starting
v0.12.0b2,"column d is now okay, too"
v0.12.0b2,verify that we can use a scalar treatment cost
v0.12.0b2,verify that we can specify per-treatment costs for each sample
v0.12.0b2,verify that using the same state returns the same results each time
v0.12.0b2,set the categories for column 'd' explicitly so that b is default
v0.12.0b2,"first column: 10 ones, this is fine"
v0.12.0b2,"second column: 6 categories, plenty of random instances of each"
v0.12.0b2,this is fine only if we increase the cateogry limit
v0.12.0b2,"third column: nine ones, lots of twos, not enough unless we disable check"
v0.12.0b2,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
v0.12.0b2,"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
v0.12.0b2,forest heterogeneity won't work
v0.12.0b2,"sixth column: just 1 one, not enough even without check"
v0.12.0b2,increase bound on cat expansion
v0.12.0b2,skip checks (reducing folds accordingly)
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,DGP constants
v0.12.0b2,Define data features
v0.12.0b2,Added `_df`to names to be different from the default cate_estimator names
v0.12.0b2,Generate data
v0.12.0b2,################################
v0.12.0b2,Single treatment and outcome #
v0.12.0b2,################################
v0.12.0b2,Test LinearDML
v0.12.0b2,|--> Test featurizers
v0.12.0b2,ColumnTransformer doesn't propagate column names
v0.12.0b2,|--> Test re-fit
v0.12.0b2,Test SparseLinearDML
v0.12.0b2,Test ForestDML
v0.12.0b2,###################################
v0.12.0b2,Mutiple treatments and outcomes #
v0.12.0b2,###################################
v0.12.0b2,Test LinearDML
v0.12.0b2,Test SparseLinearDML
v0.12.0b2,"Single outcome only, ORF does not support multiple outcomes"
v0.12.0b2,Test DMLOrthoForest
v0.12.0b2,Test DROrthoForest
v0.12.0b2,Test XLearner
v0.12.0b2,Skipping population summary names test because bootstrap inference is too slow
v0.12.0b2,Test SLearner
v0.12.0b2,Test TLearner
v0.12.0b2,Test LinearDRLearner
v0.12.0b2,Test SparseLinearDRLearner
v0.12.0b2,Test ForestDRLearner
v0.12.0b2,Test LinearIntentToTreatDRIV
v0.12.0b2,Test DeepIV
v0.12.0b2,Test categorical treatments
v0.12.0b2,Check refit
v0.12.0b2,Check refit after setting categories
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Linear models are required for parametric dml
v0.12.0b2,sample weighting models are required for nonparametric dml
v0.12.0b2,Test values
v0.12.0b2,TLearner test
v0.12.0b2,Instantiate TLearner
v0.12.0b2,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b2,Test constant treatment effect with multi output Y
v0.12.0b2,Test heterogeneous treatment effect
v0.12.0b2,Need interactions between T and features
v0.12.0b2,Test heterogeneous treatment effect with multi output Y
v0.12.0b2,Instantiate DomainAdaptationLearner
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,test base values equals to mean of constant marginal effect
v0.12.0b2,test shape of shap values output is as expected
v0.12.0b2,test shape of attribute of explanation object is as expected
v0.12.0b2,test length of feature names equals to shap values shape
v0.12.0b2,test base values equals to mean of constant marginal effect
v0.12.0b2,test shape of shap values output is as expected
v0.12.0b2,test shape of attribute of explanation object is as expected
v0.12.0b2,test length of feature names equals to shap values shape
v0.12.0b2,Treatment effect function
v0.12.0b2,Outcome support
v0.12.0b2,Treatment support
v0.12.0b2,"Generate controls, covariates, treatments and outcomes"
v0.12.0b2,Heterogeneous treatment effects
v0.12.0b2,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.12.0b2,through shap package.
v0.12.0b2,test shap could generate the plot from the shap_values
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Check inputs
v0.12.0b2,Check inputs
v0.12.0b2,Check inputs
v0.12.0b2,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.12.0b2,"Thus, we append the controls column before the one-hot-encoded T"
v0.12.0b2,"We might want to revisit, though, since it's linearly determined by the others"
v0.12.0b2,Check inputs
v0.12.0b2,Check inputs
v0.12.0b2,Estimate response function
v0.12.0b2,Check inputs
v0.12.0b2,Train model on controls. Assign higher weight to units resembling
v0.12.0b2,treated units.
v0.12.0b2,Train model on the treated. Assign higher weight to units resembling
v0.12.0b2,control units.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.12.0b2,output is
v0.12.0b2,"* a column of ones if X, W, and Z are all None"
v0.12.0b2,* just X or W or Z if both of the others are None
v0.12.0b2,* hstack([arrs]) for whatever subset are not None otherwise
v0.12.0b2,ensure Z is 2D
v0.12.0b2,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b2,We need to go back to the label representation of the one-hot so as to call
v0.12.0b2,the classifier.
v0.12.0b2,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b2,We need to go back to the label representation of the one-hot so as to call
v0.12.0b2,the classifier.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,TODO: make sure to use random seeds wherever necessary
v0.12.0b2,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.12.0b2,"unfortunately with the Theano and Tensorflow backends,"
v0.12.0b2,the straightforward use of K.stop_gradient can cause an error
v0.12.0b2,because the parameters of the intermediate layers are now disconnected from the loss;
v0.12.0b2,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.12.0b2,so that those layers remain connected but with 0 gradient
v0.12.0b2,|| t - mu_i || ^2
v0.12.0b2,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.12.0b2,Use logsumexp for numeric stability:
v0.12.0b2,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.12.0b2,TODO: does the numeric stability actually make any difference?
v0.12.0b2,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.12.0b2,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.12.0b2,generate cumulative sum via matrix multiplication
v0.12.0b2,"Generate standard uniform values in shape (batch_size,1)"
v0.12.0b2,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.12.0b2,we use uniform_like instead with an input of an appropriate shape)
v0.12.0b2,convert to floats and multiply to perform equivalent of logical AND
v0.12.0b2,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.12.0b2,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.12.0b2,we use normal_like instead with an input of an appropriate shape)
v0.12.0b2,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.12.0b2,prevent gradient from passing through sampling
v0.12.0b2,three options: biased or upper-bound loss require a single number of samples;
v0.12.0b2,unbiased can take different numbers for the network and its gradient
v0.12.0b2,"sample: (() -> Layer, int) -> Layer"
v0.12.0b2,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.12.0b2,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.12.0b2,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.12.0b2,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.12.0b2,the dimensionality of the output of the network
v0.12.0b2,TODO: is there a more robust way to do this?
v0.12.0b2,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b2,"subtle point: we need to build a new model each time,"
v0.12.0b2,because each model encapsulates its randomness
v0.12.0b2,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b2,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.12.0b2,not a general tensor (because of how backprop works in every framework)
v0.12.0b2,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.12.0b2,but this seems annoying...)
v0.12.0b2,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.12.0b2,TODO: any way to get this to work on batches of arbitrary size?
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b2,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b2,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b2,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b2,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b2,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.12.0b2,"instruments, and outcomes"
v0.12.0b2,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b2,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b2,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b2,for internal use by the library
v0.12.0b2,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b2,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b2,since it expects raw values
v0.12.0b2,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b2,since it expects raw values
v0.12.0b2,"TODO: check that Y, T, Z do not have multiple columns"
v0.12.0b2,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b2,TODO: do correct adjustment for sample_var
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.12.0b2,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.12.0b2,since we would actually be calculating a CATE rather than ATE.
v0.12.0b2,TODO: allow the final model to actually use X?
v0.12.0b2,TODO: allow the final model to actually use X?
v0.12.0b2,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b2,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.12.0b2,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.12.0b2,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b2,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b2,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b2,for internal use by the library
v0.12.0b2,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,"this will have dimension (d,) + shape(X)"
v0.12.0b2,send the first dimension to the end
v0.12.0b2,columns are featurized independently; partial derivatives are only non-zero
v0.12.0b2,when taken with respect to the same column each time
v0.12.0b2,don't fit intercept; manually add column of ones to the data instead;
v0.12.0b2,this allows us to ignore the intercept when computing marginal effects
v0.12.0b2,make T 2D if if was a vector
v0.12.0b2,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.12.0b2,two stage approximation
v0.12.0b2,"first, get basis expansions of T, X, and Z"
v0.12.0b2,TODO: is it right that the effective number of intruments is the
v0.12.0b2,"product of ft_X and ft_Z, not just ft_Z?"
v0.12.0b2,"regress T expansion on X,Z expansions concatenated with W"
v0.12.0b2,"predict ft_T from interacted ft_X, ft_Z"
v0.12.0b2,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.12.0b2,dT may be only 2-dimensional)
v0.12.0b2,promote dT to 3D if necessary (e.g. if T was a vector)
v0.12.0b2,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,TODO: this utility is documented but internal; reimplement?
v0.12.0b2,TODO: this utility is even less public...
v0.12.0b2,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.12.0b2,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.12.0b2,but also supports get_feature_names with expected signature
v0.12.0b2,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.12.0b2,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.12.0b2,Convert SingleTreeInterpreter to a python dictionary
v0.12.0b2,named tuple type for storing results inside CausalAnalysis class;
v0.12.0b2,must be lifted to module level to enable pickling
v0.12.0b2,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.12.0b2,Controls are all other columns of X
v0.12.0b2,"can't use X[:, feat_ind] when X is a DataFrame"
v0.12.0b2,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.12.0b2,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.12.0b2,so that the user can opt-in to allowing unseen treatment values
v0.12.0b2,(and return NaN or something in that case)
v0.12.0b2,array checking routines don't accept 0-width arrays
v0.12.0b2,perform model selection
v0.12.0b2,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.12.0b2,convert to NormalInferenceResults for consistency
v0.12.0b2,Set the dictionary values shared between local and global summaries
v0.12.0b2,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.12.0b2,"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
v0.12.0b2,required to fit a discrete DML model
v0.12.0b2,Validate inputs
v0.12.0b2,TODO: check compatibility of X and Y lengths
v0.12.0b2,"no previous fit, cancel warm start"
v0.12.0b2,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.12.0b2,"if heterogeneity_inds is 1D, repeat it"
v0.12.0b2,heterogeneity inds should be a 2D list of length same as train_inds
v0.12.0b2,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.12.0b2,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.12.0b2,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.12.0b2,train the Y model
v0.12.0b2,"perform model selection for the Y model using all X, not on a per-column basis"
v0.12.0b2,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.12.0b2,work with the regression wrapper
v0.12.0b2,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.12.0b2,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.12.0b2,since otherwise we'll have too many columns to be able to train a classifier
v0.12.0b2,start with empty results and default shared insights
v0.12.0b2,convert categorical indicators to numeric indices
v0.12.0b2,check for indices over the categorical expansion bound
v0.12.0b2,assume we'll be able to train former failures this time; we'll add them back if not
v0.12.0b2,"can't remove in place while iterating over new_inds, so store in separate list"
v0.12.0b2,"train the model, but warn"
v0.12.0b2,no model can be trained in this case since we need more folds
v0.12.0b2,"don't train a model, but suggest workaround since there are enough instances of least"
v0.12.0b2,populated class
v0.12.0b2,also remove from train_inds so we don't try to access the result later
v0.12.0b2,extract subset of names matching new columns
v0.12.0b2,"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
v0.12.0b2,don't want to cache this failed result
v0.12.0b2,properties to return from effect InferenceResults
v0.12.0b2,properties to return from PopulationSummaryResults
v0.12.0b2,Converts strings to property lookups or method calls as a convenience so that the
v0.12.0b2,_point_props and _summary_props above can be applied to an inference object
v0.12.0b2,Create a summary combining all results into a single output; this is used
v0.12.0b2,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.12.0b2,"or a dictionary, respectively, based on the summary function passed into this method"
v0.12.0b2,"ensure array has shape (m,y,t)"
v0.12.0b2,population summary is missing sample dimension; add it for consistency
v0.12.0b2,outcome dimension is missing; add it for consistency
v0.12.0b2,add singleton treatment dimension if missing
v0.12.0b2,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.12.0b2,"each attr has dimension (m,y) or (m,y,t)"
v0.12.0b2,concatenate along treatment dimension
v0.12.0b2,"for dictionary representation, want to remove unneeded sample dimension"
v0.12.0b2,in cohort and global results
v0.12.0b2,TODO: enrich outcome logic for multi-class classification when that is supported
v0.12.0b2,can't drop only level
v0.12.0b2,should be serialization-ready and contain no numpy arrays
v0.12.0b2,a global inference indicates the effect of that one feature on the outcome
v0.12.0b2,need to reshape the output to match the input
v0.12.0b2,we want to offset the inference object by the baseline estimate of y
v0.12.0b2,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.12.0b2,because then scaling the difference between treatment value and treatment costs is the
v0.12.0b2,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.12.0b2,
v0.12.0b2,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.12.0b2,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.12.0b2,(rather than just not treating at all)
v0.12.0b2,
v0.12.0b2,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.12.0b2,2 * policy_value - always_treat
v0.12.0b2,includes exactly their contribution because policy_value and always_treat both include it
v0.12.0b2,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.12.0b2,2 * policy_value - always-treat
v0.12.0b2,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.12.0b2,is zero and the contribution to always_treat is negative
v0.12.0b2,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b2,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b2,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b2,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b2,get dataframe with all but selected column
v0.12.0b2,apply 10% of a typical treatment for this feature
v0.12.0b2,set the effect bounds; for positive treatments these agree with
v0.12.0b2,"the estimates; for negative treatments, we need to invert the interval"
v0.12.0b2,the effect is now always positive since we decrease treatment when negative
v0.12.0b2,"for discrete treatment, stack a zero result in front for control"
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,TODO: conisder working around relying on sklearn implementation details
v0.12.0b2,"Found a good split, return."
v0.12.0b2,Record all splits in case the stratification by weight yeilds a worse partition
v0.12.0b2,Reseed random generator and try again
v0.12.0b2,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.12.0b2,"Found a good split, return."
v0.12.0b2,Did not find a good split
v0.12.0b2,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.12.0b2,Return most weight-balanced partition
v0.12.0b2,Weight stratification algorithm
v0.12.0b2,Sort weights for weight strata search
v0.12.0b2,There are some leftover indices that have yet to be assigned
v0.12.0b2,Append stratum splits to overall splits
v0.12.0b2,"If classification methods produce multiple columns of output,"
v0.12.0b2,we need to manually encode classes to ensure consistent column ordering.
v0.12.0b2,We clone the estimator to make sure that all the folds are
v0.12.0b2,"independent, and that it is pickle-able."
v0.12.0b2,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.12.0b2,`predictions` is a list of method outputs from each fold.
v0.12.0b2,"If each of those is also a list, then treat this as a"
v0.12.0b2,multioutput-multiclass task. We need to separately concatenate
v0.12.0b2,the method outputs for each label into an `n_labels` long list.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Our classes that derive from sklearn ones sometimes include
v0.12.0b2,inherited docstrings that have embedded doctests; we need the following imports
v0.12.0b2,so that they don't break.
v0.12.0b2,TODO: consider working around relying on sklearn implementation details
v0.12.0b2,"Convert X, y into numpy arrays"
v0.12.0b2,Define fit parameters
v0.12.0b2,Some algorithms don't have a check_input option
v0.12.0b2,Check weights array
v0.12.0b2,Check that weights are size-compatible
v0.12.0b2,Normalize inputs
v0.12.0b2,Weight inputs
v0.12.0b2,Fit base class without intercept
v0.12.0b2,Fit Lasso
v0.12.0b2,Reset intercept
v0.12.0b2,The intercept is not calculated properly due the sqrt(weights) factor
v0.12.0b2,so it must be recomputed
v0.12.0b2,Fit lasso without weights
v0.12.0b2,Make weighted splitter
v0.12.0b2,Fit weighted model
v0.12.0b2,Make weighted splitter
v0.12.0b2,Fit weighted model
v0.12.0b2,Call weighted lasso on reduced design matrix
v0.12.0b2,Weighted tau
v0.12.0b2,Select optimal penalty
v0.12.0b2,Warn about consistency
v0.12.0b2,"Convert X, y into numpy arrays"
v0.12.0b2,Fit weighted lasso with user input
v0.12.0b2,"Center X, y"
v0.12.0b2,Calculate quantities that will be used later on. Account for centered data
v0.12.0b2,Calculate coefficient and error variance
v0.12.0b2,Add coefficient correction
v0.12.0b2,Set coefficients and intercept standard errors
v0.12.0b2,Set intercept
v0.12.0b2,Return alpha to 'auto' state
v0.12.0b2,"Note that in the case of no intercept, X_offset is 0"
v0.12.0b2,Calculate the variance of the predictions
v0.12.0b2,Calculate prediction confidence intervals
v0.12.0b2,Assumes flattened y
v0.12.0b2,Compute weighted residuals
v0.12.0b2,To be done once per target. Assumes y can be flattened.
v0.12.0b2,Assumes that X has already been offset
v0.12.0b2,Special case: n_features=1
v0.12.0b2,Compute Lasso coefficients for the columns of the design matrix
v0.12.0b2,Compute C_hat
v0.12.0b2,Compute theta_hat
v0.12.0b2,Allow for single output as well
v0.12.0b2,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.12.0b2,Set coef_ attribute
v0.12.0b2,Set intercept_ attribute
v0.12.0b2,Set selected_alpha_ attribute
v0.12.0b2,Set coef_stderr_
v0.12.0b2,intercept_stderr_
v0.12.0b2,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.12.0b2,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.12.0b2,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.12.0b2,set intercept_ attribute
v0.12.0b2,set coef_ attribute
v0.12.0b2,set alpha_ attribute
v0.12.0b2,set alphas_ attribute
v0.12.0b2,set n_iter_ attribute
v0.12.0b2,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.12.0b2,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.12.0b2,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.12.0b2,now regress X1 on y - X2 * beta2 to learn beta1
v0.12.0b2,set coef_ and intercept_ attributes
v0.12.0b2,Note that the penalized model should *not* have an intercept
v0.12.0b2,don't proxy special methods
v0.12.0b2,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.12.0b2,regressor incorrectly
v0.12.0b2,"Note: for known attributes that have been set this method will not be called,"
v0.12.0b2,so we should just throw here because this is an attribute belonging to this class
v0.12.0b2,but which hasn't yet been set on this instance
v0.12.0b2,set default values for None
v0.12.0b2,check freq_weight should be integer and should be accompanied by sample_var
v0.12.0b2,check array shape
v0.12.0b2,weight X and y and sample_var
v0.12.0b2,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,AzureML
v0.12.0b2,helper imports
v0.12.0b2,write the details of the workspace to a configuration file to the notebook library
v0.12.0b2,if y is a multioutput model
v0.12.0b2,Make sure second dimension has 1 or more item
v0.12.0b2,switch _inner Model to a MultiOutputRegressor
v0.12.0b2,flatten array as automl only takes vectors for y
v0.12.0b2,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.12.0b2,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.12.0b2,as an sklearn estimator
v0.12.0b2,fit implementation for a single output model.
v0.12.0b2,Create experiment for specified workspace
v0.12.0b2,Configure automl_config with training set information.
v0.12.0b2,"Wait for remote run to complete, the set the model"
v0.12.0b2,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.12.0b2,create model and pass model into final.
v0.12.0b2,"If item is an automl config, get its corresponding"
v0.12.0b2,AutomatedML Model and add it to new_Args
v0.12.0b2,"If item is an automl config, get its corresponding"
v0.12.0b2,AutomatedML Model and set it for this key in
v0.12.0b2,kwargs
v0.12.0b2,takes in either automated_ml config and instantiates
v0.12.0b2,an AutomatedMLModel
v0.12.0b2,The prefix can only be 18 characters long
v0.12.0b2,"because prefixes come from kwarg_names, we must ensure they are"
v0.12.0b2,short enough.
v0.12.0b2,Get workspace from config file.
v0.12.0b2,Take the intersect of the white for sample
v0.12.0b2,weights and linear models
v0.12.0b2,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,average the outcome dimension if it exists and ensure 2d y_pred
v0.12.0b2,get index of best treatment
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,TODO: consider working around relying on sklearn implementation details
v0.12.0b2,Create splits of causal tree
v0.12.0b2,Make sure the correct exception is being rethrown
v0.12.0b2,Must make sure indices are merged correctly
v0.12.0b2,Convert rows to columns
v0.12.0b2,Require group assignment t to be one-hot-encoded
v0.12.0b2,Get predictions for the 2 splits
v0.12.0b2,Must make sure indices are merged correctly
v0.12.0b2,Crossfitting
v0.12.0b2,Compute weighted nuisance estimates
v0.12.0b2,-------------------------------------------------------------------------------
v0.12.0b2,Calculate the covariance matrix corresponding to the BLB inference
v0.12.0b2,
v0.12.0b2,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.12.0b2,2. Calculate the weighted moments for each tree slice to create a matrix
v0.12.0b2,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.12.0b2,in that slice from the overall parameter estimate.
v0.12.0b2,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.12.0b2,-------------------------------------------------------------------------------
v0.12.0b2,Calclulate covariance matrix through BLB
v0.12.0b2,Estimators
v0.12.0b2,OrthoForest parameters
v0.12.0b2,Sub-forests
v0.12.0b2,Auxiliary attributes
v0.12.0b2,Fit check
v0.12.0b2,TODO: Check performance
v0.12.0b2,Must normalize weights
v0.12.0b2,Override the CATE inference options
v0.12.0b2,Add blb inference to parent's options
v0.12.0b2,Generate subsample indices
v0.12.0b2,Build trees in parallel
v0.12.0b2,Bootstraping has repetitions in tree sample
v0.12.0b2,Similar for `a` weights
v0.12.0b2,Bootstraping has repetitions in tree sample
v0.12.0b2,Define subsample size
v0.12.0b2,Safety check
v0.12.0b2,Draw points to create little bags
v0.12.0b2,Copy and/or define models
v0.12.0b2,Define nuisance estimators
v0.12.0b2,Define parameter estimators
v0.12.0b2,Define
v0.12.0b2,Need to redefine fit here for auto inference to work due to a quirk in how
v0.12.0b2,wrap_fit is defined
v0.12.0b2,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b2,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b2,Override to flatten output if T is flat
v0.12.0b2,Check that all discrete treatments are represented
v0.12.0b2,Nuissance estimates evaluated with cross-fitting
v0.12.0b2,Define 2-fold iterator
v0.12.0b2,Check if there is only one example of some class
v0.12.0b2,Define 2-fold iterator
v0.12.0b2,need safe=False when cloning for WeightedModelWrapper
v0.12.0b2,Compute residuals
v0.12.0b2,Compute coefficient by OLS on residuals
v0.12.0b2,"Parameter returned by LinearRegression is (d_T, )"
v0.12.0b2,Compute residuals
v0.12.0b2,Compute coefficient by OLS on residuals
v0.12.0b2,ell_2 regularization
v0.12.0b2,Ridge regression estimate
v0.12.0b2,"Parameter returned is of shape (d_T, )"
v0.12.0b2,Return moments and gradients
v0.12.0b2,Compute residuals
v0.12.0b2,Compute moments
v0.12.0b2,"Moments shape is (n, d_T)"
v0.12.0b2,Compute moment gradients
v0.12.0b2,returns shape-conforming residuals
v0.12.0b2,Copy and/or define models
v0.12.0b2,Define parameter estimators
v0.12.0b2,Define moment and mean gradient estimator
v0.12.0b2,"Check that T is shape (n, )"
v0.12.0b2,Check T is numeric
v0.12.0b2,Train label encoder
v0.12.0b2,Call `fit` from parent class
v0.12.0b2,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b2,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b2,Override to flatten output if T is flat
v0.12.0b2,Expand one-hot encoding to include the zero treatment
v0.12.0b2,"Test that T contains all treatments. If not, return None"
v0.12.0b2,Nuissance estimates evaluated with cross-fitting
v0.12.0b2,Define 2-fold iterator
v0.12.0b2,Check if there is only one example of some class
v0.12.0b2,No need to crossfit for internal nodes
v0.12.0b2,Compute partial moments
v0.12.0b2,"If any of the values in the parameter estimate is nan, return None"
v0.12.0b2,Compute partial moments
v0.12.0b2,Compute coefficient by OLS on residuals
v0.12.0b2,ell_2 regularization
v0.12.0b2,Ridge regression estimate
v0.12.0b2,"Parameter returned is of shape (d_T, )"
v0.12.0b2,Return moments and gradients
v0.12.0b2,Compute partial moments
v0.12.0b2,Compute moments
v0.12.0b2,"Moments shape is (n, d_T-1)"
v0.12.0b2,Compute moment gradients
v0.12.0b2,Need to calculate this in an elegant way for when propensity is 0
v0.12.0b2,This will flatten T
v0.12.0b2,Check that T is numeric
v0.12.0b2,Test whether the input estimator is supported
v0.12.0b2,Calculate confidence intervals for the parameter (marginal effect)
v0.12.0b2,Calculate confidence intervals for the effect
v0.12.0b2,Calculate the effects
v0.12.0b2,Calculate the standard deviations for the effects
v0.12.0b2,d_t=None here since we measure the effect across all Ts
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b2,Licensed under the MIT License.
v0.12.0b2,Causal tree parameters
v0.12.0b2,Tree structure
v0.12.0b2,No need for a random split since the data is already
v0.12.0b2,a random subsample from the original input
v0.12.0b2,node list stores the nodes that are yet to be splitted
v0.12.0b2,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b2,Create local sample set
v0.12.0b2,Compute nuisance estimates for the current node
v0.12.0b2,Nuisance estimate cannot be calculated
v0.12.0b2,Estimate parameter for current node
v0.12.0b2,Node estimate cannot be calculated
v0.12.0b2,Calculate moments and gradient of moments for current data
v0.12.0b2,Calculate inverse gradient
v0.12.0b2,The gradient matrix is not invertible.
v0.12.0b2,No good split can be found
v0.12.0b2,Calculate point-wise pseudo-outcomes rho
v0.12.0b2,a split is determined by a feature and a sample pair
v0.12.0b2,the number of possible splits is at most (number of features) * (number of node samples)
v0.12.0b2,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.12.0b2,parse row and column of random pair
v0.12.0b2,the sample of the pair is the integer division of the random number with n_feats
v0.12.0b2,calculate the binary indicator of whether sample i is on the left or the right
v0.12.0b2,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.12.0b2,calculate the number of samples on the left child for each proposed split
v0.12.0b2,calculate the analogous binary indicator for the samples in the estimation set
v0.12.0b2,calculate the number of estimation samples on the left child of each proposed split
v0.12.0b2,find the upper and lower bound on the size of the left split for the split
v0.12.0b2,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.12.0b2,on each side.
v0.12.0b2,similarly for the estimation sample set
v0.12.0b2,if there is no valid split then don't create any children
v0.12.0b2,filter only the valid splits
v0.12.0b2,calculate the average influence vector of the samples in the left child
v0.12.0b2,calculate the average influence vector of the samples in the right child
v0.12.0b2,take the square of each of the entries of the influence vectors and normalize
v0.12.0b2,by size of each child
v0.12.0b2,calculate the vector score of each candidate split as the average of left and right
v0.12.0b2,influence vectors
v0.12.0b2,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.12.0b2,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.12.0b2,where there might be large discontinuities in some parameter as the conditioning set varies
v0.12.0b2,calculate the scalar score of each split by aggregating across the vector of scores
v0.12.0b2,Find split that minimizes criterion
v0.12.0b2,Create child nodes with corresponding subsamples
v0.12.0b2,add the created children to the list of not yet split nodes
v0.12.0b1,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.12.0b1,configuration is all pulled from setup.cfg
v0.12.0b1,-*- coding: utf-8 -*-
v0.12.0b1,
v0.12.0b1,Configuration file for the Sphinx documentation builder.
v0.12.0b1,
v0.12.0b1,This file does only contain a selection of the most common options. For a
v0.12.0b1,full list see the documentation:
v0.12.0b1,http://www.sphinx-doc.org/en/master/config
v0.12.0b1,-- Path setup --------------------------------------------------------------
v0.12.0b1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.12.0b1,add these directories to sys.path here. If the directory is relative to the
v0.12.0b1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.12.0b1,
v0.12.0b1,-- Project information -----------------------------------------------------
v0.12.0b1,-- General configuration ---------------------------------------------------
v0.12.0b1,"If your documentation needs a minimal Sphinx version, state it here."
v0.12.0b1,
v0.12.0b1,needs_sphinx = '1.0'
v0.12.0b1,"Add any Sphinx extension module names here, as strings. They can be"
v0.12.0b1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.12.0b1,ones.
v0.12.0b1,"Add any paths that contain templates here, relative to this directory."
v0.12.0b1,The suffix(es) of source filenames.
v0.12.0b1,You can specify multiple suffix as a list of string:
v0.12.0b1,
v0.12.0b1,"source_suffix = ['.rst', '.md']"
v0.12.0b1,The master toctree document.
v0.12.0b1,The language for content autogenerated by Sphinx. Refer to documentation
v0.12.0b1,for a list of supported languages.
v0.12.0b1,
v0.12.0b1,This is also used if you do content translation via gettext catalogs.
v0.12.0b1,"Usually you set ""language"" from the command line for these cases."
v0.12.0b1,"List of patterns, relative to source directory, that match files and"
v0.12.0b1,directories to ignore when looking for source files.
v0.12.0b1,This pattern also affects html_static_path and html_extra_path.
v0.12.0b1,The name of the Pygments (syntax highlighting) style to use.
v0.12.0b1,-- Options for HTML output -------------------------------------------------
v0.12.0b1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.12.0b1,a list of builtin themes.
v0.12.0b1,
v0.12.0b1,Theme options are theme-specific and customize the look and feel of a theme
v0.12.0b1,"further.  For a list of options available for each theme, see the"
v0.12.0b1,documentation.
v0.12.0b1,
v0.12.0b1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.12.0b1,"relative to this directory. They are copied after the builtin static files,"
v0.12.0b1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.12.0b1,html_static_path = ['_static']
v0.12.0b1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.12.0b1,to template names.
v0.12.0b1,
v0.12.0b1,The default sidebars (for documents that don't match any pattern) are
v0.12.0b1,defined by theme itself.  Builtin themes are using these templates by
v0.12.0b1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.12.0b1,'searchbox.html']``.
v0.12.0b1,
v0.12.0b1,html_sidebars = {}
v0.12.0b1,-- Options for HTMLHelp output ---------------------------------------------
v0.12.0b1,Output file base name for HTML help builder.
v0.12.0b1,-- Options for LaTeX output ------------------------------------------------
v0.12.0b1,The paper size ('letterpaper' or 'a4paper').
v0.12.0b1,
v0.12.0b1,"'papersize': 'letterpaper',"
v0.12.0b1,"The font size ('10pt', '11pt' or '12pt')."
v0.12.0b1,
v0.12.0b1,"'pointsize': '10pt',"
v0.12.0b1,Additional stuff for the LaTeX preamble.
v0.12.0b1,
v0.12.0b1,"'preamble': '',"
v0.12.0b1,Latex figure (float) alignment
v0.12.0b1,
v0.12.0b1,"'figure_align': 'htbp',"
v0.12.0b1,Grouping the document tree into LaTeX files. List of tuples
v0.12.0b1,"(source start file, target name, title,"
v0.12.0b1,"author, documentclass [howto, manual, or own class])."
v0.12.0b1,-- Options for manual page output ------------------------------------------
v0.12.0b1,One entry per manual page. List of tuples
v0.12.0b1,"(source start file, name, description, authors, manual section)."
v0.12.0b1,-- Options for Texinfo output ----------------------------------------------
v0.12.0b1,Grouping the document tree into Texinfo files. List of tuples
v0.12.0b1,"(source start file, target name, title, author,"
v0.12.0b1,"dir menu entry, description, category)"
v0.12.0b1,-- Options for Epub output -------------------------------------------------
v0.12.0b1,Bibliographic Dublin Core info.
v0.12.0b1,The unique identifier of the text. This can be a ISBN number
v0.12.0b1,or the project homepage.
v0.12.0b1,
v0.12.0b1,epub_identifier = ''
v0.12.0b1,A unique identification for the text.
v0.12.0b1,
v0.12.0b1,epub_uid = ''
v0.12.0b1,A list of files that should not be packed into the epub file.
v0.12.0b1,-- Extension configuration -------------------------------------------------
v0.12.0b1,-- Options for intersphinx extension ---------------------------------------
v0.12.0b1,Example configuration for intersphinx: refer to the Python standard library.
v0.12.0b1,-- Options for todo extension ----------------------------------------------
v0.12.0b1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.12.0b1,-- Options for doctest extension -------------------------------------------
v0.12.0b1,we can document otherwise excluded entities here by returning False
v0.12.0b1,or skip otherwise included entities by returning True
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Calculate residuals
v0.12.0b1,Estimate E[T_res | Z_res]
v0.12.0b1,TODO. Deal with multi-class instrument
v0.12.0b1,Calculate nuisances
v0.12.0b1,Estimate E[T_res | Z_res]
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.12.0b1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.12.0b1,TODO. Deal with multi-class instrument
v0.12.0b1,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b1,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b1,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b1,Estimate preliminary theta in cross fitting manner
v0.12.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.12.0b1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.12.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.12.0b1,TODO. The solution below is not really a valid cross-fitting
v0.12.0b1,as the test data are used to create the proj_t on the train
v0.12.0b1,which in the second train-test loop is used to create the nuisance
v0.12.0b1,cov on the test data. Hence the T variable of some sample
v0.12.0b1,"is implicitly correlated with its cov nuisance, through this flow"
v0.12.0b1,"of information. However, this seems a rather weak correlation."
v0.12.0b1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.12.0b1,model.
v0.12.0b1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.12.0b1,Estimate preliminary theta in cross fitting manner
v0.12.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.12.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.12.0b1,#############################################################################
v0.12.0b1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.12.0b1,A/B test
v0.12.0b1,#############################################################################
v0.12.0b1,Estimate preliminary theta in cross fitting manner
v0.12.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.12.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.12.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.12.0b1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.12.0b1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.12.0b1,model_T_XZ = lambda: model_clf()
v0.12.0b1,#'days_visited': lambda:
v0.12.0b1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.12.0b1,Turn strings into categories for numeric mapping
v0.12.0b1,### Defining some generic regressors and classifiers
v0.12.0b1,This a generic non-parametric regressor
v0.12.0b1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.12.0b1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.12.0b1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.12.0b1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.12.0b1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.12.0b1,model = lambda: LinearRegression(n_jobs=-1)
v0.12.0b1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.12.0b1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.12.0b1,underlying model whenever predict is called.
v0.12.0b1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.12.0b1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.12.0b1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.12.0b1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.12.0b1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.12.0b1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.12.0b1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.12.0b1,We need to specify models to be used for each of these residualizations
v0.12.0b1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.12.0b1,"E[T | X, Z]"
v0.12.0b1,E[TZ | X]
v0.12.0b1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.12.0b1,n_splits determines the number of splits to be used for cross-fitting.
v0.12.0b1,# Algorithm 2 - Current Method
v0.12.0b1,In[121]:
v0.12.0b1,# Algorithm 3 - DRIV ATE
v0.12.0b1,dmliv_model_effect = lambda: model()
v0.12.0b1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.12.0b1,"dmliv_model_effect(),"
v0.12.0b1,n_splits=1)
v0.12.0b1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.12.0b1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.12.0b1,"Once multiple treatments are supported, we'll need to fix this"
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.12.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,TODO. Deal with multi-class instrument/treatment
v0.12.0b1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.12.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.12.0b1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.12.0b1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.12.0b1,##################
v0.12.0b1,Global settings #
v0.12.0b1,##################
v0.12.0b1,Global plotting controls
v0.12.0b1,"Control for support size, can control for more"
v0.12.0b1,#################
v0.12.0b1,File utilities #
v0.12.0b1,#################
v0.12.0b1,#################
v0.12.0b1,Plotting utils #
v0.12.0b1,#################
v0.12.0b1,bias
v0.12.0b1,var
v0.12.0b1,rmse
v0.12.0b1,r2
v0.12.0b1,Infer feature dimension
v0.12.0b1,Metrics by support plots
v0.12.0b1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.12.0b1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.12.0b1,Steven Wu <zhiww@microsoft.com>
v0.12.0b1,Initialize causal tree parameters
v0.12.0b1,Create splits of causal tree
v0.12.0b1,Estimate treatment effects at the leafs
v0.12.0b1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.12.0b1,the corresponding split and associating the effect computed on that leaf
v0.12.0b1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.12.0b1,Safety check
v0.12.0b1,Weighted linear regression
v0.12.0b1,Calculates weights
v0.12.0b1,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b1,over all indices
v0.12.0b1,Similar for `a` weights
v0.12.0b1,Doesn't have sample weights
v0.12.0b1,Is a linear model
v0.12.0b1,Weighted linear regression
v0.12.0b1,Calculates weights
v0.12.0b1,Bootstraping has repetitions in tree sample so we need to iterate
v0.12.0b1,over all indices
v0.12.0b1,Similar for `a` weights
v0.12.0b1,normalize weights
v0.12.0b1,"Split the data in half, train and test"
v0.12.0b1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b1,"a function of W, using only the train fold"
v0.12.0b1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b1,"Split the data in half, train and test"
v0.12.0b1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.12.0b1,"a function of W, using only the train fold"
v0.12.0b1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.12.0b1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.12.0b1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.12.0b1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.12.0b1,"Split the data in half, train and test"
v0.12.0b1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b1,"a function of x, using only the train fold"
v0.12.0b1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b1,Compute coefficient by OLS on residuals
v0.12.0b1,"Split the data in half, train and test"
v0.12.0b1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.12.0b1,"a function of x, using only the train fold"
v0.12.0b1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.12.0b1,Estimate multipliers for second order orthogonal method
v0.12.0b1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.12.0b1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b1,Create local sample set
v0.12.0b1,compute the base estimate for the current node using double ml or second order double ml
v0.12.0b1,compute the influence functions here that are used for the criterion
v0.12.0b1,generate random proposals of dimensions to split
v0.12.0b1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.12.0b1,compute criterion for each proposal
v0.12.0b1,if splitting creates valid leafs in terms of mean leaf size
v0.12.0b1,Calculate criterion for split
v0.12.0b1,Else set criterion to infinity so that this split is not chosen
v0.12.0b1,If no good split was found
v0.12.0b1,Find split that minimizes criterion
v0.12.0b1,Set the split attributes at the node
v0.12.0b1,Create child nodes with corresponding subsamples
v0.12.0b1,Recursively split children
v0.12.0b1,Return parent node
v0.12.0b1,estimate the local parameter at the leaf using the estimate data
v0.12.0b1,###################
v0.12.0b1,Argument parsing #
v0.12.0b1,###################
v0.12.0b1,#########################################
v0.12.0b1,Parameters constant across experiments #
v0.12.0b1,#########################################
v0.12.0b1,Outcome support
v0.12.0b1,Treatment support
v0.12.0b1,Evaluation grid
v0.12.0b1,Treatment effects array
v0.12.0b1,Other variables
v0.12.0b1,##########################
v0.12.0b1,Data Generating Process #
v0.12.0b1,##########################
v0.12.0b1,Log iteration
v0.12.0b1,"Generate controls, features, treatment and outcome"
v0.12.0b1,T and Y residuals to be used in later scripts
v0.12.0b1,Save generated dataset
v0.12.0b1,#################
v0.12.0b1,ORF parameters #
v0.12.0b1,#################
v0.12.0b1,######################################
v0.12.0b1,Train and evaluate treatment effect #
v0.12.0b1,######################################
v0.12.0b1,########
v0.12.0b1,Plots #
v0.12.0b1,########
v0.12.0b1,###############
v0.12.0b1,Save results #
v0.12.0b1,###############
v0.12.0b1,##############
v0.12.0b1,Run Rscript #
v0.12.0b1,##############
v0.12.0b1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b1,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.12.0b1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b1,def mlasso_model(): return MultiTaskLassoCV(
v0.12.0b1,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.12.0b1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b1,heterogeneity
v0.12.0b1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b1,heterogeneity
v0.12.0b1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.12.0b1,heterogeneity
v0.12.0b1,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.12.0b1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b1,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.12.0b1,subset of features that are exogenous and create heterogeneity
v0.12.0b1,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.12.0b1,subset of features wrt we estimate heterogeneity
v0.12.0b1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.12.0b1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,introspect the constructor arguments to find the model parameters
v0.12.0b1,to represent
v0.12.0b1,"if the argument is deprecated, ignore it"
v0.12.0b1,Extract and sort argument names excluding 'self'
v0.12.0b1,column names
v0.12.0b1,transfer input to numpy arrays
v0.12.0b1,transfer input to 2d arrays
v0.12.0b1,create dataframe
v0.12.0b1,currently dowhy only support single outcome and single treatment
v0.12.0b1,call dowhy
v0.12.0b1,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.12.0b1,cate estimator but not the effect.
v0.12.0b1,don't proxy special methods
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Check if model is sparse enough for this model
v0.12.0b1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.12.0b1,TODO: any way to avoid creating a copy if the array was already dense?
v0.12.0b1,"the call is necessary if the input was something like a list, though"
v0.12.0b1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.12.0b1,so convert to pydata sparse first
v0.12.0b1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.12.0b1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.12.0b1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.12.0b1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.12.0b1,For when checking input values is disabled
v0.12.0b1,Type to column extraction function
v0.12.0b1,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.12.0b1,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.12.0b1,Get feature names using featurizer
v0.12.0b1,All attempts at retrieving transformed feature names have failed
v0.12.0b1,Delegate handling to downstream logic
v0.12.0b1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.12.0b1,same number of input definitions as arrays
v0.12.0b1,input definitions have same number of dimensions as each array
v0.12.0b1,all result indices are unique
v0.12.0b1,all result indices must match at least one input index
v0.12.0b1,"map indices to all array, axis pairs for that index"
v0.12.0b1,each index has the same cardinality wherever it appears
v0.12.0b1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.12.0b1,Algo: while list contains more than one entry
v0.12.0b1,take two entries
v0.12.0b1,sort both lists by intersection of their indices
v0.12.0b1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.12.0b1,"take the union of indices and the product of values), stepping through each list linearly"
v0.12.0b1,TODO: might be faster to break into connected components first
v0.12.0b1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.12.0b1,"so compute their content separately, then take cartesian product"
v0.12.0b1,this would save a few pointless sorts by empty tuples
v0.12.0b1,TODO: Consider investigating other performance ideas for these cases
v0.12.0b1,where the dense method beat the sparse method (usually sparse is faster)
v0.12.0b1,"e,facd,c->cfed"
v0.12.0b1,sparse: 0.0335489
v0.12.0b1,dense:  0.011465999999999997
v0.12.0b1,"gbd,da,egb->da"
v0.12.0b1,sparse: 0.0791625
v0.12.0b1,dense:  0.007319099999999995
v0.12.0b1,"dcc,d,faedb,c->abe"
v0.12.0b1,sparse: 1.2868097
v0.12.0b1,dense:  0.44605229999999985
v0.12.0b1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.12.0b1,TODO: would using einsum's paths to optimize the order of merging help?
v0.12.0b1,assume that we should perform nested cross-validation if and only if
v0.12.0b1,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.12.0b1,logic copied from check_cv
v0.12.0b1,otherwise we will assume the user already set the cv attribute to something
v0.12.0b1,compatible with splitting with a 'groups' argument
v0.12.0b1,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.12.0b1,don't use the groups when calling split internally
v0.12.0b1,Normalize weights
v0.12.0b1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.12.0b1,"if we're decorating a class, just update the __init__ method,"
v0.12.0b1,so that the result is still a class instead of a wrapper method
v0.12.0b1,"want to enforce that each bad_arg was either in kwargs,"
v0.12.0b1,or else it was in neither and is just taking its default value
v0.12.0b1,Any access should throw
v0.12.0b1,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b1,input feature name is already updated by cate_feature_names.
v0.12.0b1,define the index of d_x to filter for each given T
v0.12.0b1,filter X after broadcast with T for each given T
v0.12.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,
v0.12.0b1,This code contains some snippets of code from:
v0.12.0b1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
v0.12.0b1,published under the following license and copyright:
v0.12.0b1,BSD 3-Clause License
v0.12.0b1,
v0.12.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b1,All rights reserved.
v0.12.0b1,make any access to matplotlib or plt throw an exception
v0.12.0b1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.12.0b1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.12.0b1,Initialize saturation & value; calculate chroma & value shift
v0.12.0b1,Calculate some intermediate values
v0.12.0b1,Initialize RGB with same hue & chroma as our color
v0.12.0b1,Shift the initial RGB values to match value and store
v0.12.0b1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.12.0b1,clean way of achieving this
v0.12.0b1,make sure we don't accidentally escape anything in the substitution
v0.12.0b1,Fetch appropriate color for node
v0.12.0b1,"red for negative, green for positive"
v0.12.0b1,in multi-target use mean of targets
v0.12.0b1,Write node mean CATE
v0.12.0b1,Write node std of CATE
v0.12.0b1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.12.0b1,Fetch appropriate color for node
v0.12.0b1,Write node mean CATE
v0.12.0b1,Write node mean CATE
v0.12.0b1,Write recommended treatment and value - cost
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"since inference objects can be stateful, we must copy it before fitting;"
v0.12.0b1,otherwise this sequence wouldn't work:
v0.12.0b1,"est1.fit(..., inference=inf)"
v0.12.0b1,"est2.fit(..., inference=inf)"
v0.12.0b1,est1.effect_interval(...)
v0.12.0b1,because inf now stores state from fitting est2
v0.12.0b1,This flag is true when names are set in a child class instead
v0.12.0b1,"If names are set in a child class, add an attribute reflecting that"
v0.12.0b1,This works only if X is passed as a kwarg
v0.12.0b1,We plan to enforce X as kwarg only in future releases
v0.12.0b1,This checks if names have been set in a child class
v0.12.0b1,"If names were set in a child class, don't do it again"
v0.12.0b1,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.12.0b1,call the wrapped fit method
v0.12.0b1,NOTE: we call inference fit *after* calling the main fit method
v0.12.0b1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.12.0b1,but tensordot can't be applied to this problem because we don't sum over m
v0.12.0b1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.12.0b1,of rows of T was not taken into account
v0.12.0b1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.12.0b1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.12.0b1,"Treatment names is None, default to BaseCateEstimator"
v0.12.0b1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.12.0b1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.12.0b1,Get input names
v0.12.0b1,Summary
v0.12.0b1,add statsmodels to parent's options
v0.12.0b1,add debiasedlasso to parent's options
v0.12.0b1,add blb to parent's options
v0.12.0b1,TODO Share some logic with non-discrete version
v0.12.0b1,Get input names
v0.12.0b1,Summary
v0.12.0b1,add statsmodels to parent's options
v0.12.0b1,add statsmodels to parent's options
v0.12.0b1,add blb to parent's options
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,remove None arguments
v0.12.0b1,"scores entries should be lists of scores, so make each entry a singleton list"
v0.12.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b1,generate an instance of the final model
v0.12.0b1,generate an instance of the nuisance model
v0.12.0b1,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.12.0b1,alteration even when we only want to fit_final.
v0.12.0b1,use a binary array to get stratified split in case of discrete treatment
v0.12.0b1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.12.0b1,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.12.0b1,"however, sklearn doesn't support both stratifying and grouping (see"
v0.12.0b1,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.12.0b1,their own object that supports grouping if they want to use groups.
v0.12.0b1,for each mc iteration
v0.12.0b1,for each model under cross fit setting
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,
v0.12.0b1,This code contains snippets of code from
v0.12.0b1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b1,published under the following license and copyright:
v0.12.0b1,BSD 3-Clause License
v0.12.0b1,
v0.12.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b1,All rights reserved.
v0.12.0b1,=============================================================================
v0.12.0b1,Policy Forest
v0.12.0b1,=============================================================================
v0.12.0b1,Remap output
v0.12.0b1,reshape is necessary to preserve the data contiguity against vs
v0.12.0b1,"[:, np.newaxis] that does not."
v0.12.0b1,Get subsample sample size
v0.12.0b1,Check parameters
v0.12.0b1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b1,if this is the first `fit` call of the warm start mode.
v0.12.0b1,"Free allocated memory, if any"
v0.12.0b1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b1,We draw from the random state to get the random state we
v0.12.0b1,would have got if we hadn't used a warm_start.
v0.12.0b1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b1,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b1,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b1,for fitting the trees is internally releasing the Python GIL
v0.12.0b1,making threading more efficient than multiprocessing in
v0.12.0b1,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b1,"parallel_backend contexts set at a higher level,"
v0.12.0b1,since correctness does not rely on using threads.
v0.12.0b1,Collect newly grown trees
v0.12.0b1,Check data
v0.12.0b1,Assign chunk of trees to jobs
v0.12.0b1,avoid storing the output of every estimator by summing them here
v0.12.0b1,Parallel loop
v0.12.0b1,Check data
v0.12.0b1,Assign chunk of trees to jobs
v0.12.0b1,avoid storing the output of every estimator by summing them here
v0.12.0b1,Parallel loop
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,
v0.12.0b1,This code contains snippets of code from:
v0.12.0b1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b1,published under the following license and copyright:
v0.12.0b1,BSD 3-Clause License
v0.12.0b1,
v0.12.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b1,All rights reserved.
v0.12.0b1,=============================================================================
v0.12.0b1,Types and constants
v0.12.0b1,=============================================================================
v0.12.0b1,=============================================================================
v0.12.0b1,Base Policy tree
v0.12.0b1,=============================================================================
v0.12.0b1,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.12.0b1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.12.0b1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.12.0b1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.12.0b1,checks that X is 2D array.
v0.12.0b1,"since we only allow single dimensional y, we could flatten the prediction"
v0.12.0b1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b1,Handles the corner case when X=None but featurizer might be not None
v0.12.0b1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.12.0b1,"Replacing this method which is invalid for this class, so that we make the"
v0.12.0b1,dosctring empty and not appear in the docs.
v0.12.0b1,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.12.0b1,Replacing to remove docstring
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"if both X and W are None, just return a column of ones"
v0.12.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b1,We need to go back to the label representation of the one-hot so as to call
v0.12.0b1,the classifier.
v0.12.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b1,We need to go back to the label representation of the one-hot so as to call
v0.12.0b1,the classifier.
v0.12.0b1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b1,This works both with our without the weighting trick as the treatments T are unit vector
v0.12.0b1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.12.0b1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.12.0b1,both Parametric and Non Parametric DML.
v0.12.0b1,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.12.0b1,attribute so that the trained featurizer will be passed through
v0.12.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b1,for internal use by the library
v0.12.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b1,We need to use the rlearner's copy to retain the information from fitting
v0.12.0b1,Handles the corner case when X=None but featurizer might be not None
v0.12.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b1,since we clone it and fit separate copies
v0.12.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b1,TODO: support freq_weight and sample_var in debiased lasso
v0.12.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b1,since we clone it and fit separate copies
v0.12.0b1,add blb to parent's options
v0.12.0b1,override only so that we can update the docstring to indicate
v0.12.0b1,support for `GenericSingleTreatmentModelFinalInference`
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,note that groups are not passed to score because they are only used for fitting
v0.12.0b1,note that groups are not passed to score because they are only used for fitting
v0.12.0b1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.12.0b1,NOTE: important to get parent's wrapped copy so that
v0.12.0b1,"after training wrapped featurizer is also trained, etc."
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.12.0b1,Fit a doubly robust average effect
v0.12.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.12.0b1,since we clone it and fit separate copies
v0.12.0b1,"If custom param grid, check that only estimator parameters are being altered"
v0.12.0b1,override only so that we can update the docstring to indicate support for `blb`
v0.12.0b1,Get input names
v0.12.0b1,Summary
v0.12.0b1,Determine output settings
v0.12.0b1,"Important: This must be the first invocation of the random state at fit time, so that"
v0.12.0b1,train/test splits are re-generatable from an external object simply by knowing the
v0.12.0b1,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.12.0b1,linear predictions. Currently is also useful for testing.
v0.12.0b1,reshape is necessary to preserve the data contiguity against vs
v0.12.0b1,"[:, np.newaxis] that does not."
v0.12.0b1,Check parameters
v0.12.0b1,Set min_weight_leaf from min_weight_fraction_leaf
v0.12.0b1,Build tree
v0.12.0b1,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.12.0b1,hold. Used by criterion for memory space savings.
v0.12.0b1,Initialize the criterion object and the criterion_val object if honest.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,
v0.12.0b1,This code is a fork from:
v0.12.0b1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.12.0b1,published under the following license and copyright:
v0.12.0b1,BSD 3-Clause License
v0.12.0b1,
v0.12.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b1,All rights reserved.
v0.12.0b1,Set parameters
v0.12.0b1,Don't instantiate estimators now! Parameters of base_estimator might
v0.12.0b1,"still change. Eg., when grid-searching with the nested object syntax."
v0.12.0b1,self.estimators_ needs to be filled by the derived classes in fit.
v0.12.0b1,Compute the number of jobs
v0.12.0b1,Partition estimators between jobs
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Remove children with nonwhite mothers from the treatment group
v0.12.0b1,Remove children with nonwhite mothers from the treatment group
v0.12.0b1,Select columns
v0.12.0b1,Scale the numeric variables
v0.12.0b1,"Change the binary variable 'first' takes values in {1,2}"
v0.12.0b1,Append a column of ones as intercept
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.12.0b1,d_t=None here since we measure the effect across all Ts
v0.12.0b1,once the estimator has been fit
v0.12.0b1,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.12.0b1,an intercept even when bias is part of coef.
v0.12.0b1,We can write effect inference as a function of prediction and prediction standard error of
v0.12.0b1,the final method for linear models
v0.12.0b1,squeeze the first axis
v0.12.0b1,d_t=None here since we measure the effect across all Ts
v0.12.0b1,set the mean_pred_stderr
v0.12.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.12.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.12.0b1,"send treatment to the end, pull bounds to the front"
v0.12.0b1,d_t=None here since we measure the effect across all Ts
v0.12.0b1,set the mean_pred_stderr
v0.12.0b1,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.12.0b1,d_t=None here since we measure the effect across all Ts
v0.12.0b1,d_t=None here since we measure the effect across all Ts
v0.12.0b1,need to set the fit args before the estimator is fit
v0.12.0b1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b1,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.12.0b1,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.12.0b1,NOTE: use np.asarray(offset) because if offset is a pd.Series direct addition would make the sum
v0.12.0b1,"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
v0.12.0b1,NOTE: use np.asarray(factor) because if offset is a pd.Series direct addition would make the product
v0.12.0b1,"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
v0.12.0b1,scale preds
v0.12.0b1,scale std errs
v0.12.0b1,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.12.0b1,offset preds
v0.12.0b1,"offset the distribution, too"
v0.12.0b1,scale preds
v0.12.0b1,"scale the distribution, too"
v0.12.0b1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.12.0b1,1. Uncertainty of Mean Point Estimate
v0.12.0b1,2. Distribution of Point Estimate
v0.12.0b1,3. Total Variance of Point Estimate
v0.12.0b1,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.12.0b1,so bail out early; note that it might be possible to correct the algorithm for
v0.12.0b1,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.12.0b1,be clean
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,TODO: Add a __dir__ implementation?
v0.12.0b1,don't proxy special methods
v0.12.0b1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.12.0b1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.12.0b1,then we should be computing a confidence interval for the wrapped calls
v0.12.0b1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.12.0b1,second level bootstrap which would be prohibitive computationally?
v0.12.0b1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.12.0b1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.12.0b1,can't import from econml.inference at top level without creating cyclical dependencies
v0.12.0b1,Note that inference results are always methods even if the inference is for a property
v0.12.0b1,(e.g. coef__inference() is a method but coef_ is a property)
v0.12.0b1,Therefore we must insert a lambda if getting inference for a non-callable
v0.12.0b1,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.12.0b1,"try to get interval/std first if appropriate,"
v0.12.0b1,since we don't prefer a wrapped method with this name
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,
v0.12.0b1,This code contains snippets of code from:
v0.12.0b1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.12.0b1,published under the following license and copyright:
v0.12.0b1,BSD 3-Clause License
v0.12.0b1,
v0.12.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b1,All rights reserved.
v0.12.0b1,=============================================================================
v0.12.0b1,Types and constants
v0.12.0b1,=============================================================================
v0.12.0b1,=============================================================================
v0.12.0b1,Base GRF tree
v0.12.0b1,=============================================================================
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,=============================================================================
v0.12.0b1,A MultOutputWrapper for GRF classes
v0.12.0b1,=============================================================================
v0.12.0b1,=============================================================================
v0.12.0b1,Instantiations of Generalized Random Forest
v0.12.0b1,=============================================================================
v0.12.0b1,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.12.0b1,in front of the constant treatment is the intercept in the moment equation.
v0.12.0b1,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.12.0b1,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,
v0.12.0b1,This code contains snippets of code from
v0.12.0b1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.12.0b1,published under the following license and copyright:
v0.12.0b1,BSD 3-Clause License
v0.12.0b1,
v0.12.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.12.0b1,All rights reserved.
v0.12.0b1,=============================================================================
v0.12.0b1,Base Generalized Random Forest
v0.12.0b1,=============================================================================
v0.12.0b1,TODO: support freq_weight and sample_var
v0.12.0b1,Remap output
v0.12.0b1,reshape is necessary to preserve the data contiguity against vs
v0.12.0b1,"[:, np.newaxis] that does not."
v0.12.0b1,reshape is necessary to preserve the data contiguity against vs
v0.12.0b1,"[:, np.newaxis] that does not."
v0.12.0b1,Get subsample sample size
v0.12.0b1,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.12.0b1,We calculate the min eigenvalue proxy that each criterion is considering
v0.12.0b1,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.12.0b1,Check parameters
v0.12.0b1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.12.0b1,if this is the first `fit` call of the warm start mode.
v0.12.0b1,"Free allocated memory, if any"
v0.12.0b1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.12.0b1,We draw from the random state to get the random state we
v0.12.0b1,would have got if we hadn't used a warm_start.
v0.12.0b1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b1,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b1,Generating indices a priori before parallelism ended up being orders of magnitude
v0.12.0b1,faster than how sklearn does it. The reason is that random samplers do not release the
v0.12.0b1,gil it seems.
v0.12.0b1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.12.0b1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.12.0b1,but would still advance randomness enough so that tree subsamples will be different.
v0.12.0b1,Parallel loop: we prefer the threading backend as the Cython code
v0.12.0b1,for fitting the trees is internally releasing the Python GIL
v0.12.0b1,making threading more efficient than multiprocessing in
v0.12.0b1,"that case. However, for joblib 0.12+ we respect any"
v0.12.0b1,"parallel_backend contexts set at a higher level,"
v0.12.0b1,since correctness does not rely on using threads.
v0.12.0b1,Collect newly grown trees
v0.12.0b1,Check data
v0.12.0b1,Assign chunk of trees to jobs
v0.12.0b1,avoid storing the output of every estimator by summing them here
v0.12.0b1,Parallel loop
v0.12.0b1,Check data
v0.12.0b1,Assign chunk of trees to jobs
v0.12.0b1,Parallel loop
v0.12.0b1,Check data
v0.12.0b1,Assign chunk of trees to jobs
v0.12.0b1,Parallel loop
v0.12.0b1,####################
v0.12.0b1,Variance correction
v0.12.0b1,####################
v0.12.0b1,Subtract the average within bag variance. This ends up being equal to the
v0.12.0b1,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.12.0b1,The negative part is just sq_between.
v0.12.0b1,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.12.0b1,"The off diagonals we have no objective prior, so no correction is applied."
v0.12.0b1,Finally correcting the pred_cov or pred_var
v0.12.0b1,avoid storing the output of every estimator by summing them here
v0.12.0b1,Parallel loop
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,testing importances
v0.12.0b1,testing heterogeneity importances
v0.12.0b1,Testing that all parameters do what they are supposed to
v0.12.0b1,"testing predict, apply and decision path"
v0.12.0b1,test that the subsampling scheme past to the trees is correct
v0.12.0b1,The sample size is chosen in particular to test rounding based error when subsampling
v0.12.0b1,test that the estimator calcualtes var correctly
v0.12.0b1,test api
v0.12.0b1,test accuracy
v0.12.0b1,test the projection functionality of forests
v0.12.0b1,test that the estimator calcualtes var correctly
v0.12.0b1,test api
v0.12.0b1,test that the estimator calcualtes var correctly
v0.12.0b1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b1,test that we raise errors in mishandled situations.
v0.12.0b1,test that the subsampling scheme past to the trees is correct
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,omit the lalonde notebook
v0.12.0b1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.12.0b1,creating notebooks that are annoying for our users to actually run themselves
v0.12.0b1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.12.0b1,"prior to calling interpret, can't plot, render, etc."
v0.12.0b1,can interpret without uncertainty
v0.12.0b1,can't interpret with uncertainty if inference wasn't used during fit
v0.12.0b1,can interpret with uncertainty if we refit
v0.12.0b1,can interpret without uncertainty
v0.12.0b1,can't treat before interpreting
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,simple DGP only for illustration
v0.12.0b1,Define the treatment model neural network architecture
v0.12.0b1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b1,"so the input shape is (d_z + d_x,)"
v0.12.0b1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b1,add extra layers on top for the mixture density network
v0.12.0b1,Define the response model neural network architecture
v0.12.0b1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b1,"so the input shape is (d_t + d_x,)"
v0.12.0b1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b1,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b1,so that the same weights will be reused in each instantiation
v0.12.0b1,number of samples to use in second estimate of the response
v0.12.0b1,(to make loss estimate unbiased)
v0.12.0b1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b1,do something with predictions...
v0.12.0b1,also test vector t and y
v0.12.0b1,simple DGP only for illustration
v0.12.0b1,Define the treatment model neural network architecture
v0.12.0b1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.12.0b1,"so the input shape is (d_z + d_x,)"
v0.12.0b1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.12.0b1,add extra layers on top for the mixture density network
v0.12.0b1,Define the response model neural network architecture
v0.12.0b1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.12.0b1,"so the input shape is (d_t + d_x,)"
v0.12.0b1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.12.0b1,"NOTE: For the response model, it is important to define the model *outside*"
v0.12.0b1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.12.0b1,so that the same weights will be reused in each instantiation
v0.12.0b1,number of samples to use in second estimate of the response
v0.12.0b1,(to make loss estimate unbiased)
v0.12.0b1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.12.0b1,do something with predictions...
v0.12.0b1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.12.0b1,test = True ensures we draw test set images
v0.12.0b1,test = True ensures we draw test set images
v0.12.0b1,re-draw to get new independent treatment and implied response
v0.12.0b1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b1,above is necesary so that reduced form doesn't win
v0.12.0b1,covariates: time and emotion
v0.12.0b1,random instrument
v0.12.0b1,z -> price
v0.12.0b1,true observable demand function
v0.12.0b1,errors
v0.12.0b1,response
v0.12.0b1,test = True ensures we draw test set images
v0.12.0b1,test = True ensures we draw test set images
v0.12.0b1,re-draw to get new independent treatment and implied response
v0.12.0b1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.12.0b1,above is necesary so that reduced form doesn't win
v0.12.0b1,covariates: time and emotion
v0.12.0b1,random instrument
v0.12.0b1,z -> price
v0.12.0b1,true observable demand function
v0.12.0b1,errors
v0.12.0b1,response
v0.12.0b1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.12.0b1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.12.0b1,For some reason this doesn't work at all when run against the CNTK backend...
v0.12.0b1,"model.compile('nadam', loss=lambda _,l:l)"
v0.12.0b1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.12.0b1,generate a valiation set
v0.12.0b1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.12.0b1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,DGP constants
v0.12.0b1,Generate data
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,testing importances
v0.12.0b1,testing heterogeneity importances
v0.12.0b1,Testing that all parameters do what they are supposed to
v0.12.0b1,"testing predict, apply and decision path"
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.12.0b1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.12.0b1,so we need to transpose the result
v0.12.0b1,1-d output
v0.12.0b1,2-d output
v0.12.0b1,Single dimensional output y
v0.12.0b1,compare with weight
v0.12.0b1,compare with weight
v0.12.0b1,compare with weight
v0.12.0b1,compare with weight
v0.12.0b1,Multi-dimensional output y
v0.12.0b1,1-d y
v0.12.0b1,compare when both sample_var and sample_weight exist
v0.12.0b1,multi-d y
v0.12.0b1,compare when both sample_var and sample_weight exist
v0.12.0b1,compare when both sample_var and sample_weight exist
v0.12.0b1,compare when both sample_var and sample_weight exist
v0.12.0b1,compare when both sample_var and sample_weight exist
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,test that we can fit with the same arguments as the base estimator
v0.12.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b1,with the same shape for the lower and upper bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,test that we can do the same thing once we provide percentile bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b1,with the same shape for the lower and upper bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,test that we can do the same thing once we provide percentile bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,test that we can fit with the same arguments as the base estimator
v0.12.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b1,with the same shape for the lower and upper bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,test that we can do the same thing once we provide percentile bounds
v0.12.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b1,with the same shape for the lower and upper bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,test that we can do the same thing once we provide percentile bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,test that we can fit with the same arguments as the base estimator
v0.12.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.12.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b1,with the same shape for the lower and upper bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,test that we can do the same thing once we provide percentile bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.12.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b1,with the same shape for the lower and upper bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,test that we can do the same thing once we provide percentile bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b1,with the same shape for the lower and upper bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,test that the estimated effect is usually within the bounds
v0.12.0b1,test that we can do the same thing once we provide alpha explicitly
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,test that the estimated effect is usually within the bounds
v0.12.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.12.0b1,with the same shape for the lower and upper bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,TODO: test that the estimated effect is usually within the bounds
v0.12.0b1,and that the true effect is also usually within the bounds
v0.12.0b1,test that we can do the same thing once we provide percentile bounds
v0.12.0b1,test that the lower and upper bounds differ
v0.12.0b1,TODO: test that the estimated effect is usually within the bounds
v0.12.0b1,and that the true effect is also usually within the bounds
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,test that the subsampling scheme past to the trees is correct
v0.12.0b1,test that the estimator calcualtes var correctly
v0.12.0b1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.12.0b1,test that we raise errors in mishandled situations.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,DGP constants
v0.12.0b1,Generate data
v0.12.0b1,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b1,Test inference results when `cate_feature_names` doesn not exist
v0.12.0b1,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.12.0b1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b1,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.12.0b1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b1,pvalue for second column should be greater than zero since some points are on either side
v0.12.0b1,of the tested value
v0.12.0b1,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.12.0b1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.12.0b1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.12.0b1,only is not None when T1 is a constant or a list of constant
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.12.0b1,Test non keyword based calls to fit
v0.12.0b1,test non-array inputs
v0.12.0b1,Test custom splitter
v0.12.0b1,Test incomplete set of test folds
v0.12.0b1,"y scores should be positive, since W predicts Y somewhat"
v0.12.0b1,"t scores might not be, since W and T are uncorrelated"
v0.12.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,make sure cross product varies more slowly with first array
v0.12.0b1,and that vectors are okay as inputs
v0.12.0b1,number of inputs in specification must match number of inputs
v0.12.0b1,must have an output
v0.12.0b1,output indices must be unique
v0.12.0b1,output indices must be present in an input
v0.12.0b1,number of indices must match number of dimensions for each input
v0.12.0b1,repeated indices must always have consistent sizes
v0.12.0b1,transpose
v0.12.0b1,tensordot
v0.12.0b1,trace
v0.12.0b1,TODO: set up proper flag for this
v0.12.0b1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.12.0b1,"of all of the distinct indices that appear in any input,"
v0.12.0b1,pick a random subset of them (of size at most 5) to appear in the output
v0.12.0b1,creating an instance should warn
v0.12.0b1,using the instance should not warn
v0.12.0b1,using the deprecated method should warn
v0.12.0b1,don't warn if b and c are passed by keyword
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Preprocess data
v0.12.0b1,Convert 'week' to a date
v0.12.0b1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.12.0b1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.12.0b1,Take log of price
v0.12.0b1,Make brand numeric
v0.12.0b1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.12.0b1,which have all zero coeffs
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,test at least one estimator from each category
v0.12.0b1,test causal graph
v0.12.0b1,test refutation estimate
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.12.0b1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.12.0b1,TODO: test something rather than just print...
v0.12.0b1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.12.0b1,pick some arbitrary X
v0.12.0b1,pick some arbitrary T
v0.12.0b1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b1,The average variance should be lower when using monte carlo iterations
v0.12.0b1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.12.0b1,The average variance should be lower when using monte carlo iterations
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,ensure that we've got at least two of every row
v0.12.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b1,need to make sure we get all *joint* combinations
v0.12.0b1,IntentToTreat only supports binary treatments/instruments
v0.12.0b1,IntentToTreat only supports binary treatments/instruments
v0.12.0b1,IntentToTreat requires X
v0.12.0b1,ensure we can serialize unfit estimator
v0.12.0b1,these support only W but not X
v0.12.0b1,"these support only binary, not general discrete T and Z"
v0.12.0b1,ensure we can serialize fit estimator
v0.12.0b1,make sure we can call the marginal_effect and effect methods
v0.12.0b1,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b1,TODO: add tests for extra properties like coef_ where they exist
v0.12.0b1,"make sure we can call effect with implied scalar treatments,"
v0.12.0b1,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b1,are multiple treatments
v0.12.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.12.0b1,"however, with custom splits the checking happens in the first stage wrapper"
v0.12.0b1,where we don't have all of the required information to do this;
v0.12.0b1,we'd probably need to add it to _crossfit instead
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.12.0b1,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.12.0b1,The __warningregistry__'s need to be in a pristine state for tests
v0.12.0b1,to work properly.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Set random seed
v0.12.0b1,Generate data
v0.12.0b1,DGP constants
v0.12.0b1,Test data
v0.12.0b1,Constant treatment effect
v0.12.0b1,Constant treatment with multi output Y
v0.12.0b1,Heterogeneous treatment
v0.12.0b1,Heterogeneous treatment with multi output Y
v0.12.0b1,TLearner test
v0.12.0b1,Instantiate TLearner
v0.12.0b1,Test inputs
v0.12.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b1,Instantiate SLearner
v0.12.0b1,Test inputs
v0.12.0b1,Test constant treatment effect
v0.12.0b1,Test constant treatment effect with multi output Y
v0.12.0b1,Test heterogeneous treatment effect
v0.12.0b1,Need interactions between T and features
v0.12.0b1,Test heterogeneous treatment effect with multi output Y
v0.12.0b1,Instantiate XLearner
v0.12.0b1,Test inputs
v0.12.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b1,Instantiate DomainAdaptationLearner
v0.12.0b1,Test inputs
v0.12.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b1,Get the true treatment effect
v0.12.0b1,Get the true treatment effect
v0.12.0b1,Fit learner and get the effect and marginal effect
v0.12.0b1,Compute treatment effect residuals (absolute)
v0.12.0b1,Check that at least 90% of predictions are within tolerance interval
v0.12.0b1,Check whether the output shape is right
v0.12.0b1,Check that one can pass in regular lists
v0.12.0b1,Check that it fails correctly if lists of different shape are passed in
v0.12.0b1,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b1,Generate covariates
v0.12.0b1,Generate treatment
v0.12.0b1,Calculate outcome
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,DGP constants
v0.12.0b1,Generate data
v0.12.0b1,Test data
v0.12.0b1,Remove warnings that might be raised by the models passed into the ORF
v0.12.0b1,Generate data with continuous treatments
v0.12.0b1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.12.0b1,does not work well with parallelism.
v0.12.0b1,Test inputs for continuous treatments
v0.12.0b1,--> Check that one can pass in regular lists
v0.12.0b1,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b1,Check that outputs have the correct shape
v0.12.0b1,Test continuous treatments with controls
v0.12.0b1,Test continuous treatments without controls
v0.12.0b1,Generate data with binary treatments
v0.12.0b1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.12.0b1,does not work well with parallelism.
v0.12.0b1,Test inputs for binary treatments
v0.12.0b1,--> Check that one can pass in regular lists
v0.12.0b1,--> Check that it fails correctly if lists of different shape are passed in
v0.12.0b1,"--> Check that it works when T, Y have shape (n, 1)"
v0.12.0b1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.12.0b1,--> Check that it fails correctly when the treatments are not numeric
v0.12.0b1,Check that outputs have the correct shape
v0.12.0b1,Test binary treatments with controls
v0.12.0b1,Test binary treatments without controls
v0.12.0b1,Only applicable to continuous treatments
v0.12.0b1,Generate data for 2 treatments
v0.12.0b1,Test multiple treatments with controls
v0.12.0b1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.12.0b1,The rest for controls. Just as an example.
v0.12.0b1,Generating A/B test data
v0.12.0b1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.12.0b1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.12.0b1,Create a wrapper around Lasso that doesn't support weights
v0.12.0b1,since Lasso does natively support them starting in sklearn 0.23
v0.12.0b1,Generate data with continuous treatments
v0.12.0b1,Instantiate model with most of the default parameters
v0.12.0b1,Compute the treatment effect on test points
v0.12.0b1,Compute treatment effect residuals
v0.12.0b1,Multiple treatments
v0.12.0b1,Allow at most 10% test points to be outside of the tolerance interval
v0.12.0b1,Compute treatment effect residuals
v0.12.0b1,Multiple treatments
v0.12.0b1,Allow at most 20% test points to be outside of the confidence interval
v0.12.0b1,Check that the intervals are not too wide
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.12.0b1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.12.0b1,ensure that we've got at least 6 of every element
v0.12.0b1,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.12.0b1,NOTE: this number may need to change if the default number of folds in
v0.12.0b1,WeightedStratifiedKFold changes
v0.12.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b1,ensure we can serialize the unfit estimator
v0.12.0b1,ensure we can pickle the fit estimator
v0.12.0b1,make sure we can call the marginal_effect and effect methods
v0.12.0b1,test const marginal inference
v0.12.0b1,test effect inference
v0.12.0b1,test marginal effect inference
v0.12.0b1,test coef__inference and intercept__inference
v0.12.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b1,"make sure we can call effect with implied scalar treatments,"
v0.12.0b1,"no matter the dimensions of T, and also that we warn when there"
v0.12.0b1,are multiple treatments
v0.12.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b1,ensure that we've got at least two of every element
v0.12.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b1,make sure we can call the marginal_effect and effect methods
v0.12.0b1,test const marginal inference
v0.12.0b1,test effect inference
v0.12.0b1,test marginal effect inference
v0.12.0b1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b1,We concatenate the two copies data
v0.12.0b1,make sure we can get out post-fit stuff
v0.12.0b1,create a simple artificial setup where effect of moving from treatment
v0.12.0b1,"1 -> 2 is 2,"
v0.12.0b1,"1 -> 3 is 1, and"
v0.12.0b1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b1,"Using an uneven number of examples from different classes,"
v0.12.0b1,"and having the treatments in non-lexicographic order,"
v0.12.0b1,Should rule out some basic issues.
v0.12.0b1,test that we can fit with a KFold instance
v0.12.0b1,test that we can fit with a train/test iterable
v0.12.0b1,predetermined splits ensure that all features are seen in each split
v0.12.0b1,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.12.0b1,(incorrectly) use a final model with an intercept
v0.12.0b1,"Because final model is fixed, actual values of T and Y don't matter"
v0.12.0b1,Ensure reproducibility
v0.12.0b1,Sparse DGP
v0.12.0b1,Treatment effect coef
v0.12.0b1,Other coefs
v0.12.0b1,Features and controls
v0.12.0b1,Test sparse estimator
v0.12.0b1,"--> test coef_, intercept_"
v0.12.0b1,--> test treatment effects
v0.12.0b1,Restrict x_test to vectors of norm < 1
v0.12.0b1,--> check inference
v0.12.0b1,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.12.0b1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.12.0b1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.12.0b1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.12.0b1,sparse test case: heterogeneous effect by product
v0.12.0b1,need at least as many rows in e_y as there are distinct columns
v0.12.0b1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.12.0b1,create a simple artificial setup where effect of moving from treatment
v0.12.0b1,"a -> b is 2,"
v0.12.0b1,"a -> c is 1, and"
v0.12.0b1,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.12.0b1,"Using an uneven number of examples from different classes,"
v0.12.0b1,"and having the treatments in non-lexicographic order,"
v0.12.0b1,should rule out some basic issues.
v0.12.0b1,Note that explicitly specifying the dtype as object is necessary until
v0.12.0b1,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.12.0b1,estimated effects should be identical when treatment is explicitly given
v0.12.0b1,but const_marginal_effect should be reordered based on the explicit cagetories
v0.12.0b1,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.12.0b1,test outer grouping
v0.12.0b1,test nested grouping
v0.12.0b1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b1,whichever groups we saw
v0.12.0b1,test nested grouping
v0.12.0b1,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Set random seed
v0.12.0b1,Generate data
v0.12.0b1,DGP constants
v0.12.0b1,Test data
v0.12.0b1,Constant treatment effect and propensity
v0.12.0b1,Heterogeneous treatment and propensity
v0.12.0b1,ensure that we've got at least two of every element
v0.12.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.12.0b1,ensure that we can serialize unfit estimator
v0.12.0b1,ensure that we can serialize fit estimator
v0.12.0b1,make sure we can call the marginal_effect and effect methods
v0.12.0b1,test const marginal inference
v0.12.0b1,test effect inference
v0.12.0b1,test marginal effect inference
v0.12.0b1,test coef_ and intercept_ inference
v0.12.0b1,verify we can generate the summary
v0.12.0b1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.12.0b1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.12.0b1,create a simple artificial setup where effect of moving from treatment
v0.12.0b1,"1 -> 2 is 2,"
v0.12.0b1,"1 -> 3 is 1, and"
v0.12.0b1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.12.0b1,"Using an uneven number of examples from different classes,"
v0.12.0b1,"and having the treatments in non-lexicographic order,"
v0.12.0b1,Should rule out some basic issues.
v0.12.0b1,test that we can fit with a KFold instance
v0.12.0b1,test that we can fit with a train/test iterable
v0.12.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.12.0b1,test coef__inference function works
v0.12.0b1,test intercept__inference function works
v0.12.0b1,test summary function works
v0.12.0b1,Test inputs
v0.12.0b1,self._test_inputs(DR_learner)
v0.12.0b1,Test constant treatment effect
v0.12.0b1,Test heterogeneous treatment effect
v0.12.0b1,Test heterogenous treatment effect for W =/= None
v0.12.0b1,Sparse DGP
v0.12.0b1,Treatment effect coef
v0.12.0b1,Other coefs
v0.12.0b1,Features and controls
v0.12.0b1,Test sparse estimator
v0.12.0b1,"--> test coef_, intercept_"
v0.12.0b1,--> test treatment effects
v0.12.0b1,Restrict x_test to vectors of norm < 1
v0.12.0b1,--> check inference
v0.12.0b1,Check that a majority of true effects lie in the 5-95% CI
v0.12.0b1,test outer grouping
v0.12.0b1,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.12.0b1,test nested grouping
v0.12.0b1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.12.0b1,whichever groups we saw
v0.12.0b1,test nested grouping
v0.12.0b1,"by default, we use 5 split cross-validation for our T and Y models"
v0.12.0b1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.12.0b1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.12.0b1,helper class
v0.12.0b1,Fit learner and get the effect
v0.12.0b1,Get the true treatment effect
v0.12.0b1,Compute treatment effect residuals (absolute)
v0.12.0b1,Check that at least 90% of predictions are within tolerance interval
v0.12.0b1,Only for heterogeneous TE
v0.12.0b1,Fit learner on X and W and get the effect
v0.12.0b1,Get the true treatment effect
v0.12.0b1,Compute treatment effect residuals (absolute)
v0.12.0b1,Check that at least 90% of predictions are within tolerance interval
v0.12.0b1,Check that one can pass in regular lists
v0.12.0b1,Check that it fails correctly if lists of different shape are passed in
v0.12.0b1,Check that it fails when T contains values other than 0 and 1
v0.12.0b1,"Check that it works when T, Y have shape (n, 1)"
v0.12.0b1,Generate covariates
v0.12.0b1,Generate treatment
v0.12.0b1,Calculate outcome
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,DGP constants
v0.12.0b1,DGP coefficients
v0.12.0b1,Generated outcomes
v0.12.0b1,################
v0.12.0b1,WeightedLasso #
v0.12.0b1,################
v0.12.0b1,Define weights
v0.12.0b1,Define extended datasets
v0.12.0b1,Range of alphas
v0.12.0b1,Compare with Lasso
v0.12.0b1,--> No intercept
v0.12.0b1,--> With intercept
v0.12.0b1,When DGP has no intercept
v0.12.0b1,When DGP has intercept
v0.12.0b1,--> Coerce coefficients to be positive
v0.12.0b1,--> Toggle max_iter & tol
v0.12.0b1,Define weights
v0.12.0b1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b1,Mixed DGP scenario.
v0.12.0b1,Define extended datasets
v0.12.0b1,Define weights
v0.12.0b1,Define multioutput
v0.12.0b1,##################
v0.12.0b1,WeightedLassoCV #
v0.12.0b1,##################
v0.12.0b1,Define alphas to test
v0.12.0b1,Compare with LassoCV
v0.12.0b1,--> No intercept
v0.12.0b1,--> With intercept
v0.12.0b1,--> Force parameters to be positive
v0.12.0b1,Choose a smaller n to speed-up process
v0.12.0b1,Compare fold weights
v0.12.0b1,Define weights
v0.12.0b1,Define extended datasets
v0.12.0b1,Define splitters
v0.12.0b1,WeightedKFold splitter
v0.12.0b1,Map weighted splitter to an extended splitter
v0.12.0b1,Define alphas to test
v0.12.0b1,Compare with LassoCV
v0.12.0b1,--> No intercept
v0.12.0b1,--> With intercept
v0.12.0b1,--> Force parameters to be positive
v0.12.0b1,###########################
v0.12.0b1,MultiTaskWeightedLassoCV #
v0.12.0b1,###########################
v0.12.0b1,Define alphas to test
v0.12.0b1,Define splitter
v0.12.0b1,Compare with MultiTaskLassoCV
v0.12.0b1,--> No intercept
v0.12.0b1,--> With intercept
v0.12.0b1,Define weights
v0.12.0b1,Define extended datasets
v0.12.0b1,Define splitters
v0.12.0b1,WeightedKFold splitter
v0.12.0b1,Map weighted splitter to an extended splitter
v0.12.0b1,Define alphas to test
v0.12.0b1,Compare with LassoCV
v0.12.0b1,--> No intercept
v0.12.0b1,--> With intercept
v0.12.0b1,#########################
v0.12.0b1,WeightedLassoCVWrapper #
v0.12.0b1,#########################
v0.12.0b1,perform 1D fit
v0.12.0b1,perform 2D fit
v0.12.0b1,################
v0.12.0b1,DebiasedLasso #
v0.12.0b1,################
v0.12.0b1,Test DebiasedLasso without weights
v0.12.0b1,--> Check debiased coeffcients without intercept
v0.12.0b1,--> Check debiased coeffcients with intercept
v0.12.0b1,--> Check 5-95 CI coverage for unit vectors
v0.12.0b1,Test DebiasedLasso with weights for one DGP
v0.12.0b1,Define weights
v0.12.0b1,Define extended datasets
v0.12.0b1,--> Check debiased coefficients
v0.12.0b1,Define weights
v0.12.0b1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.12.0b1,--> Check debiased coeffcients
v0.12.0b1,Test that attributes propagate correctly
v0.12.0b1,Test MultiOutputDebiasedLasso without weights
v0.12.0b1,--> Check debiased coeffcients without intercept
v0.12.0b1,--> Check debiased coeffcients with intercept
v0.12.0b1,--> Check CI coverage
v0.12.0b1,Test MultiOutputDebiasedLasso with weights
v0.12.0b1,Define weights
v0.12.0b1,Define extended datasets
v0.12.0b1,--> Check debiased coefficients
v0.12.0b1,Unit vectors
v0.12.0b1,Unit vectors
v0.12.0b1,Check coeffcients and intercept are the same within tolerance
v0.12.0b1,Check results are similar with tolerance 1e-6
v0.12.0b1,Check if multitask
v0.12.0b1,Check that same alpha is chosen
v0.12.0b1,Check that the coefficients are similar
v0.12.0b1,selective ridge has a simple implementation that we can test against
v0.12.0b1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.12.0b1,"it should be the case that when we set fit_intercept to true,"
v0.12.0b1,it doesn't matter whether the penalized model also fits an intercept or not
v0.12.0b1,create an extra copy of rows with weight 2
v0.12.0b1,"instead of a slice, explicitly return an array of indices"
v0.12.0b1,_penalized_inds is only set during fitting
v0.12.0b1,cv exists on penalized model
v0.12.0b1,now we can access _penalized_inds
v0.12.0b1,check that we can read the cv attribute back out from the underlying model
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b1,local index should have as many times entries as global as there were rows passed in
v0.12.0b1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b1,policy value should exceed always treating with any treatment
v0.12.0b1,"global shape is (d_y, sum(d_t))"
v0.12.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b1,features; for categoricals they should appear #cats-1 times each
v0.12.0b1,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b1,local index should have as many times entries as global as there were rows passed in
v0.12.0b1,features; for categoricals they should appear #cats-1 times each
v0.12.0b1,"global shape is (d_y, sum(d_t))"
v0.12.0b1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b1,policy value should exceed always treating with any treatment
v0.12.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b1,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b1,local index should have as many times entries as global as there were rows passed in
v0.12.0b1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b1,policy value should exceed always treating with any treatment
v0.12.0b1,"global shape is (d_y, sum(d_t))"
v0.12.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b1,features; for categoricals they should appear #cats-1 times each
v0.12.0b1,make sure we don't run into problems dropping every index
v0.12.0b1,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b1,local index should have as many times entries as global as there were rows passed in
v0.12.0b1,"global shape is (d_y, sum(d_t))"
v0.12.0b1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b1,policy value should exceed always treating with any treatment
v0.12.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.12.0b1,"global and cohort data should have exactly the same structure, but different values"
v0.12.0b1,local index should have as many times entries as global as there were rows passed in
v0.12.0b1,features; for categoricals they should appear #cats-1 times each
v0.12.0b1,"global shape is (d_y, sum(d_t))"
v0.12.0b1,"Make sure we handle continuous, binary, and multi-class treatments"
v0.12.0b1,"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
v0.12.0b1,policy value should exceed always treating with any treatment
v0.12.0b1,dgp
v0.12.0b1,model
v0.12.0b1,model
v0.12.0b1,"columns 'd', 'e', 'h' have too many values"
v0.12.0b1,"columns 'd', 'e' have too many values"
v0.12.0b1,lowering bound shouldn't affect already fit columns when warm starting
v0.12.0b1,"column d is now okay, too"
v0.12.0b1,verify that we can use a scalar treatment cost
v0.12.0b1,verify that we can specify per-treatment costs for each sample
v0.12.0b1,verify that using the same state returns the same results each time
v0.12.0b1,set the categories for column 'd' explicitly so that b is default
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,DGP constants
v0.12.0b1,Define data features
v0.12.0b1,Added `_df`to names to be different from the default cate_estimator names
v0.12.0b1,Generate data
v0.12.0b1,################################
v0.12.0b1,Single treatment and outcome #
v0.12.0b1,################################
v0.12.0b1,Test LinearDML
v0.12.0b1,|--> Test featurizers
v0.12.0b1,ColumnTransformer doesn't propagate column names
v0.12.0b1,|--> Test re-fit
v0.12.0b1,Test SparseLinearDML
v0.12.0b1,Test ForestDML
v0.12.0b1,###################################
v0.12.0b1,Mutiple treatments and outcomes #
v0.12.0b1,###################################
v0.12.0b1,Test LinearDML
v0.12.0b1,Test SparseLinearDML
v0.12.0b1,"Single outcome only, ORF does not support multiple outcomes"
v0.12.0b1,Test DMLOrthoForest
v0.12.0b1,Test DROrthoForest
v0.12.0b1,Test XLearner
v0.12.0b1,Skipping population summary names test because bootstrap inference is too slow
v0.12.0b1,Test SLearner
v0.12.0b1,Test TLearner
v0.12.0b1,Test LinearDRLearner
v0.12.0b1,Test SparseLinearDRLearner
v0.12.0b1,Test ForestDRLearner
v0.12.0b1,Test LinearIntentToTreatDRIV
v0.12.0b1,Test DeepIV
v0.12.0b1,Test categorical treatments
v0.12.0b1,Check refit
v0.12.0b1,Check refit after setting categories
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Linear models are required for parametric dml
v0.12.0b1,sample weighting models are required for nonparametric dml
v0.12.0b1,Test values
v0.12.0b1,TLearner test
v0.12.0b1,Instantiate TLearner
v0.12.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.12.0b1,Test constant treatment effect with multi output Y
v0.12.0b1,Test heterogeneous treatment effect
v0.12.0b1,Need interactions between T and features
v0.12.0b1,Test heterogeneous treatment effect with multi output Y
v0.12.0b1,Instantiate DomainAdaptationLearner
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,test base values equals to mean of constant marginal effect
v0.12.0b1,test shape of shap values output is as expected
v0.12.0b1,test shape of attribute of explanation object is as expected
v0.12.0b1,test length of feature names equals to shap values shape
v0.12.0b1,test base values equals to mean of constant marginal effect
v0.12.0b1,test shape of shap values output is as expected
v0.12.0b1,test shape of attribute of explanation object is as expected
v0.12.0b1,test length of feature names equals to shap values shape
v0.12.0b1,Treatment effect function
v0.12.0b1,Outcome support
v0.12.0b1,Treatment support
v0.12.0b1,"Generate controls, covariates, treatments and outcomes"
v0.12.0b1,Heterogeneous treatment effects
v0.12.0b1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.12.0b1,through shap package.
v0.12.0b1,test shap could generate the plot from the shap_values
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Check inputs
v0.12.0b1,Check inputs
v0.12.0b1,Check inputs
v0.12.0b1,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.12.0b1,"Thus, we append the controls column before the one-hot-encoded T"
v0.12.0b1,"We might want to revisit, though, since it's linearly determined by the others"
v0.12.0b1,Check inputs
v0.12.0b1,Check inputs
v0.12.0b1,Estimate response function
v0.12.0b1,Check inputs
v0.12.0b1,Train model on controls. Assign higher weight to units resembling
v0.12.0b1,treated units.
v0.12.0b1,Train model on the treated. Assign higher weight to units resembling
v0.12.0b1,control units.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.12.0b1,output is
v0.12.0b1,"* a column of ones if X, W, and Z are all None"
v0.12.0b1,* just X or W or Z if both of the others are None
v0.12.0b1,* hstack([arrs]) for whatever subset are not None otherwise
v0.12.0b1,ensure Z is 2D
v0.12.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b1,We need to go back to the label representation of the one-hot so as to call
v0.12.0b1,the classifier.
v0.12.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.12.0b1,We need to go back to the label representation of the one-hot so as to call
v0.12.0b1,the classifier.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,TODO: make sure to use random seeds wherever necessary
v0.12.0b1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.12.0b1,"unfortunately with the Theano and Tensorflow backends,"
v0.12.0b1,the straightforward use of K.stop_gradient can cause an error
v0.12.0b1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.12.0b1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.12.0b1,so that those layers remain connected but with 0 gradient
v0.12.0b1,|| t - mu_i || ^2
v0.12.0b1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.12.0b1,Use logsumexp for numeric stability:
v0.12.0b1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.12.0b1,TODO: does the numeric stability actually make any difference?
v0.12.0b1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.12.0b1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.12.0b1,generate cumulative sum via matrix multiplication
v0.12.0b1,"Generate standard uniform values in shape (batch_size,1)"
v0.12.0b1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.12.0b1,we use uniform_like instead with an input of an appropriate shape)
v0.12.0b1,convert to floats and multiply to perform equivalent of logical AND
v0.12.0b1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.12.0b1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.12.0b1,we use normal_like instead with an input of an appropriate shape)
v0.12.0b1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.12.0b1,prevent gradient from passing through sampling
v0.12.0b1,three options: biased or upper-bound loss require a single number of samples;
v0.12.0b1,unbiased can take different numbers for the network and its gradient
v0.12.0b1,"sample: (() -> Layer, int) -> Layer"
v0.12.0b1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.12.0b1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.12.0b1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.12.0b1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.12.0b1,the dimensionality of the output of the network
v0.12.0b1,TODO: is there a more robust way to do this?
v0.12.0b1,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b1,"subtle point: we need to build a new model each time,"
v0.12.0b1,because each model encapsulates its randomness
v0.12.0b1,TODO: do we need to give the user more control over other arguments to fit?
v0.12.0b1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.12.0b1,not a general tensor (because of how backprop works in every framework)
v0.12.0b1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.12.0b1,but this seems annoying...)
v0.12.0b1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.12.0b1,TODO: any way to get this to work on batches of arbitrary size?
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Estimate final model of theta(X) by minimizing the square loss:
v0.12.0b1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.12.0b1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.12.0b1,at the expense of some small bias. For points with very small covariance we revert
v0.12.0b1,to the model-based preliminary estimate and do not add the correction term.
v0.12.0b1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.12.0b1,"instruments, and outcomes"
v0.12.0b1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b1,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.12.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b1,for internal use by the library
v0.12.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b1,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b1,since it expects raw values
v0.12.0b1,"we need to undo the one-hot encoding for calling effect,"
v0.12.0b1,since it expects raw values
v0.12.0b1,"TODO: check that Y, T, Z do not have multiple columns"
v0.12.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.12.0b1,TODO: do correct adjustment for sample_var
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.12.0b1,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.12.0b1,since we would actually be calculating a CATE rather than ATE.
v0.12.0b1,TODO: allow the final model to actually use X?
v0.12.0b1,TODO: allow the final model to actually use X?
v0.12.0b1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b1,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.12.0b1,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.12.0b1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b1,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.12.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.12.0b1,for internal use by the library
v0.12.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,"this will have dimension (d,) + shape(X)"
v0.12.0b1,send the first dimension to the end
v0.12.0b1,columns are featurized independently; partial derivatives are only non-zero
v0.12.0b1,when taken with respect to the same column each time
v0.12.0b1,don't fit intercept; manually add column of ones to the data instead;
v0.12.0b1,this allows us to ignore the intercept when computing marginal effects
v0.12.0b1,make T 2D if if was a vector
v0.12.0b1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.12.0b1,two stage approximation
v0.12.0b1,"first, get basis expansions of T, X, and Z"
v0.12.0b1,TODO: is it right that the effective number of intruments is the
v0.12.0b1,"product of ft_X and ft_Z, not just ft_Z?"
v0.12.0b1,"regress T expansion on X,Z expansions concatenated with W"
v0.12.0b1,"predict ft_T from interacted ft_X, ft_Z"
v0.12.0b1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.12.0b1,dT may be only 2-dimensional)
v0.12.0b1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.12.0b1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,TODO: this utility is documented but internal; reimplement?
v0.12.0b1,TODO: this utility is even less public...
v0.12.0b1,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.12.0b1,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.12.0b1,but also supports get_feature_names with expected signature
v0.12.0b1,NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
v0.12.0b1,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.12.0b1,Convert SingleTreeInterpreter to a python dictionary
v0.12.0b1,named tuple type for storing results inside CausalAnalysis class;
v0.12.0b1,must be lifted to module level to enable pickling
v0.12.0b1,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.12.0b1,Controls are all other columns of X
v0.12.0b1,"can't use X[:, feat_ind] when X is a DataFrame"
v0.12.0b1,TODO: we can't currently handle unseen values of the feature column when getting the effect;
v0.12.0b1,we might want to modify OrthoLearner (and other discrete treatment classes)
v0.12.0b1,so that the user can opt-in to allowing unseen treatment values
v0.12.0b1,(and return NaN or something in that case)
v0.12.0b1,array checking routines don't accept 0-width arrays
v0.12.0b1,perform model selection
v0.12.0b1,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.12.0b1,convert to NormalInferenceResults for consistency
v0.12.0b1,Set the dictionary values shared between local and global summaries
v0.12.0b1,"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
v0.12.0b1,Validate inputs
v0.12.0b1,TODO: check compatibility of X and Y lengths
v0.12.0b1,"no previous fit, cancel warm start"
v0.12.0b1,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.12.0b1,"if heterogeneity_inds is 1D, repeat it"
v0.12.0b1,heterogeneity inds should be a 2D list of length same as train_inds
v0.12.0b1,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.12.0b1,"TODO: bail out also if categorical columns, classification, random_state changed?"
v0.12.0b1,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.12.0b1,train the Y model
v0.12.0b1,"perform model selection for the Y model using all X, not on a per-column basis"
v0.12.0b1,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.12.0b1,work with the regression wrapper
v0.12.0b1,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.12.0b1,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.12.0b1,since otherwise we'll have too many columns to be able to train a classifier
v0.12.0b1,start with empty results and default shared insights
v0.12.0b1,convert categorical indicators to numeric indices
v0.12.0b1,check for indices over the categorical expansion bound
v0.12.0b1,"can't remove in place while iterating over new_inds, so store in separate list"
v0.12.0b1,also remove from train_inds so we don't try to access the result later
v0.12.0b1,extract subset of names matching new columns
v0.12.0b1,properties to return from effect InferenceResults
v0.12.0b1,properties to return from PopulationSummaryResults
v0.12.0b1,Converts strings to property lookups or method calls as a convenience so that the
v0.12.0b1,_point_props and _summary_props above can be applied to an inference object
v0.12.0b1,Create a summary combining all results into a single output; this is used
v0.12.0b1,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.12.0b1,"or a dictionary, respectively, based on the summary function passed into this method"
v0.12.0b1,"ensure array has shape (m,y,t)"
v0.12.0b1,population summary is missing sample dimension; add it for consistency
v0.12.0b1,outcome dimension is missing; add it for consistency
v0.12.0b1,add singleton treatment dimension if missing
v0.12.0b1,store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
v0.12.0b1,"each attr has dimension (m,y) or (m,y,t)"
v0.12.0b1,concatenate along treatment dimension
v0.12.0b1,"for dictionary representation, want to remove unneeded sample dimension"
v0.12.0b1,in cohort and global results
v0.12.0b1,TODO: enrich outcome logic for multi-class classification when that is supported
v0.12.0b1,can't drop only level
v0.12.0b1,should be serialization-ready and contain no numpy arrays
v0.12.0b1,a global inference indicates the effect of that one feature on the outcome
v0.12.0b1,need to reshape the output to match the input
v0.12.0b1,we want to offset the inference object by the baseline estimate of y
v0.12.0b1,"NOTE: this calculation is correct only if treatment costs are marginal costs,"
v0.12.0b1,because then scaling the difference between treatment value and treatment costs is the
v0.12.0b1,same as scaling the treatment value and subtracting the scaled treatment cost.
v0.12.0b1,
v0.12.0b1,"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
v0.12.0b1,"continuous treatments, the policy value should include the benefit of decreasing treatments"
v0.12.0b1,(rather than just not treating at all)
v0.12.0b1,
v0.12.0b1,"We can get the total by seeing that if we restrict attention to units where we would treat,"
v0.12.0b1,2 * policy_value - always_treat
v0.12.0b1,includes exactly their contribution because policy_value and always_treat both include it
v0.12.0b1,"and likewise restricting attention to the units where we want to decrease treatment,"
v0.12.0b1,2 * policy_value - always-treat
v0.12.0b1,"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
v0.12.0b1,is zero and the contribution to always_treat is negative
v0.12.0b1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b1,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.12.0b1,"however, the tree can't store the feature and treatment names we compute here..."
v0.12.0b1,get dataframe with all but selected column
v0.12.0b1,apply 10% of a typical treatment for this feature
v0.12.0b1,set the effect bounds; for positive treatments these agree with
v0.12.0b1,"the estimates; for negative treatments, we need to invert the interval"
v0.12.0b1,the effect is now always positive since we decrease treatment when negative
v0.12.0b1,"for discrete treatment, stack a zero result in front for control"
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,TODO: conisder working around relying on sklearn implementation details
v0.12.0b1,"Found a good split, return."
v0.12.0b1,Record all splits in case the stratification by weight yeilds a worse partition
v0.12.0b1,Reseed random generator and try again
v0.12.0b1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.12.0b1,"Found a good split, return."
v0.12.0b1,Did not find a good split
v0.12.0b1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.12.0b1,Return most weight-balanced partition
v0.12.0b1,Weight stratification algorithm
v0.12.0b1,Sort weights for weight strata search
v0.12.0b1,There are some leftover indices that have yet to be assigned
v0.12.0b1,Append stratum splits to overall splits
v0.12.0b1,"If classification methods produce multiple columns of output,"
v0.12.0b1,we need to manually encode classes to ensure consistent column ordering.
v0.12.0b1,We clone the estimator to make sure that all the folds are
v0.12.0b1,"independent, and that it is pickle-able."
v0.12.0b1,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.12.0b1,`predictions` is a list of method outputs from each fold.
v0.12.0b1,"If each of those is also a list, then treat this as a"
v0.12.0b1,multioutput-multiclass task. We need to separately concatenate
v0.12.0b1,the method outputs for each label into an `n_labels` long list.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Our classes that derive from sklearn ones sometimes include
v0.12.0b1,inherited docstrings that have embedded doctests; we need the following imports
v0.12.0b1,so that they don't break.
v0.12.0b1,TODO: consider working around relying on sklearn implementation details
v0.12.0b1,"Convert X, y into numpy arrays"
v0.12.0b1,Define fit parameters
v0.12.0b1,Some algorithms don't have a check_input option
v0.12.0b1,Check weights array
v0.12.0b1,Check that weights are size-compatible
v0.12.0b1,Normalize inputs
v0.12.0b1,Weight inputs
v0.12.0b1,Fit base class without intercept
v0.12.0b1,Fit Lasso
v0.12.0b1,Reset intercept
v0.12.0b1,The intercept is not calculated properly due the sqrt(weights) factor
v0.12.0b1,so it must be recomputed
v0.12.0b1,Fit lasso without weights
v0.12.0b1,Make weighted splitter
v0.12.0b1,Fit weighted model
v0.12.0b1,Make weighted splitter
v0.12.0b1,Fit weighted model
v0.12.0b1,Call weighted lasso on reduced design matrix
v0.12.0b1,Weighted tau
v0.12.0b1,Select optimal penalty
v0.12.0b1,Warn about consistency
v0.12.0b1,"Convert X, y into numpy arrays"
v0.12.0b1,Fit weighted lasso with user input
v0.12.0b1,"Center X, y"
v0.12.0b1,Calculate quantities that will be used later on. Account for centered data
v0.12.0b1,Calculate coefficient and error variance
v0.12.0b1,Add coefficient correction
v0.12.0b1,Set coefficients and intercept standard errors
v0.12.0b1,Set intercept
v0.12.0b1,Return alpha to 'auto' state
v0.12.0b1,"Note that in the case of no intercept, X_offset is 0"
v0.12.0b1,Calculate the variance of the predictions
v0.12.0b1,Calculate prediction confidence intervals
v0.12.0b1,Assumes flattened y
v0.12.0b1,Compute weighted residuals
v0.12.0b1,To be done once per target. Assumes y can be flattened.
v0.12.0b1,Assumes that X has already been offset
v0.12.0b1,Special case: n_features=1
v0.12.0b1,Compute Lasso coefficients for the columns of the design matrix
v0.12.0b1,Compute C_hat
v0.12.0b1,Compute theta_hat
v0.12.0b1,Allow for single output as well
v0.12.0b1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.12.0b1,Set coef_ attribute
v0.12.0b1,Set intercept_ attribute
v0.12.0b1,Set selected_alpha_ attribute
v0.12.0b1,Set coef_stderr_
v0.12.0b1,intercept_stderr_
v0.12.0b1,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.12.0b1,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.12.0b1,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.12.0b1,set intercept_ attribute
v0.12.0b1,set coef_ attribute
v0.12.0b1,set alpha_ attribute
v0.12.0b1,set alphas_ attribute
v0.12.0b1,set n_iter_ attribute
v0.12.0b1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.12.0b1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.12.0b1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.12.0b1,now regress X1 on y - X2 * beta2 to learn beta1
v0.12.0b1,set coef_ and intercept_ attributes
v0.12.0b1,Note that the penalized model should *not* have an intercept
v0.12.0b1,don't proxy special methods
v0.12.0b1,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.12.0b1,regressor incorrectly
v0.12.0b1,"Note: for known attributes that have been set this method will not be called,"
v0.12.0b1,so we should just throw here because this is an attribute belonging to this class
v0.12.0b1,but which hasn't yet been set on this instance
v0.12.0b1,set default values for None
v0.12.0b1,check freq_weight should be integer and should be accompanied by sample_var
v0.12.0b1,check array shape
v0.12.0b1,weight X and y and sample_var
v0.12.0b1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,AzureML
v0.12.0b1,helper imports
v0.12.0b1,write the details of the workspace to a configuration file to the notebook library
v0.12.0b1,if y is a multioutput model
v0.12.0b1,Make sure second dimension has 1 or more item
v0.12.0b1,switch _inner Model to a MultiOutputRegressor
v0.12.0b1,flatten array as automl only takes vectors for y
v0.12.0b1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.12.0b1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.12.0b1,as an sklearn estimator
v0.12.0b1,fit implementation for a single output model.
v0.12.0b1,Create experiment for specified workspace
v0.12.0b1,Configure automl_config with training set information.
v0.12.0b1,"Wait for remote run to complete, the set the model"
v0.12.0b1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.12.0b1,create model and pass model into final.
v0.12.0b1,"If item is an automl config, get its corresponding"
v0.12.0b1,AutomatedML Model and add it to new_Args
v0.12.0b1,"If item is an automl config, get its corresponding"
v0.12.0b1,AutomatedML Model and set it for this key in
v0.12.0b1,kwargs
v0.12.0b1,takes in either automated_ml config and instantiates
v0.12.0b1,an AutomatedMLModel
v0.12.0b1,The prefix can only be 18 characters long
v0.12.0b1,"because prefixes come from kwarg_names, we must ensure they are"
v0.12.0b1,short enough.
v0.12.0b1,Get workspace from config file.
v0.12.0b1,Take the intersect of the white for sample
v0.12.0b1,weights and linear models
v0.12.0b1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,average the outcome dimension if it exists and ensure 2d y_pred
v0.12.0b1,get index of best treatment
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,TODO: consider working around relying on sklearn implementation details
v0.12.0b1,Create splits of causal tree
v0.12.0b1,Make sure the correct exception is being rethrown
v0.12.0b1,Must make sure indices are merged correctly
v0.12.0b1,Convert rows to columns
v0.12.0b1,Require group assignment t to be one-hot-encoded
v0.12.0b1,Get predictions for the 2 splits
v0.12.0b1,Must make sure indices are merged correctly
v0.12.0b1,Crossfitting
v0.12.0b1,Compute weighted nuisance estimates
v0.12.0b1,-------------------------------------------------------------------------------
v0.12.0b1,Calculate the covariance matrix corresponding to the BLB inference
v0.12.0b1,
v0.12.0b1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.12.0b1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.12.0b1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.12.0b1,in that slice from the overall parameter estimate.
v0.12.0b1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.12.0b1,-------------------------------------------------------------------------------
v0.12.0b1,Calclulate covariance matrix through BLB
v0.12.0b1,Estimators
v0.12.0b1,OrthoForest parameters
v0.12.0b1,Sub-forests
v0.12.0b1,Auxiliary attributes
v0.12.0b1,Fit check
v0.12.0b1,TODO: Check performance
v0.12.0b1,Must normalize weights
v0.12.0b1,Override the CATE inference options
v0.12.0b1,Add blb inference to parent's options
v0.12.0b1,Generate subsample indices
v0.12.0b1,Build trees in parallel
v0.12.0b1,Bootstraping has repetitions in tree sample
v0.12.0b1,Similar for `a` weights
v0.12.0b1,Bootstraping has repetitions in tree sample
v0.12.0b1,Define subsample size
v0.12.0b1,Safety check
v0.12.0b1,Draw points to create little bags
v0.12.0b1,Copy and/or define models
v0.12.0b1,Define nuisance estimators
v0.12.0b1,Define parameter estimators
v0.12.0b1,Define
v0.12.0b1,Need to redefine fit here for auto inference to work due to a quirk in how
v0.12.0b1,wrap_fit is defined
v0.12.0b1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b1,Override to flatten output if T is flat
v0.12.0b1,Check that all discrete treatments are represented
v0.12.0b1,Nuissance estimates evaluated with cross-fitting
v0.12.0b1,Define 2-fold iterator
v0.12.0b1,Check if there is only one example of some class
v0.12.0b1,Define 2-fold iterator
v0.12.0b1,need safe=False when cloning for WeightedModelWrapper
v0.12.0b1,Compute residuals
v0.12.0b1,Compute coefficient by OLS on residuals
v0.12.0b1,"Parameter returned by LinearRegression is (d_T, )"
v0.12.0b1,Compute residuals
v0.12.0b1,Compute coefficient by OLS on residuals
v0.12.0b1,ell_2 regularization
v0.12.0b1,Ridge regression estimate
v0.12.0b1,"Parameter returned is of shape (d_T, )"
v0.12.0b1,Return moments and gradients
v0.12.0b1,Compute residuals
v0.12.0b1,Compute moments
v0.12.0b1,"Moments shape is (n, d_T)"
v0.12.0b1,Compute moment gradients
v0.12.0b1,returns shape-conforming residuals
v0.12.0b1,Copy and/or define models
v0.12.0b1,Define parameter estimators
v0.12.0b1,Define moment and mean gradient estimator
v0.12.0b1,"Check that T is shape (n, )"
v0.12.0b1,Check T is numeric
v0.12.0b1,Train label encoder
v0.12.0b1,Call `fit` from parent class
v0.12.0b1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.12.0b1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.12.0b1,Override to flatten output if T is flat
v0.12.0b1,Expand one-hot encoding to include the zero treatment
v0.12.0b1,"Test that T contains all treatments. If not, return None"
v0.12.0b1,Nuissance estimates evaluated with cross-fitting
v0.12.0b1,Define 2-fold iterator
v0.12.0b1,Check if there is only one example of some class
v0.12.0b1,No need to crossfit for internal nodes
v0.12.0b1,Compute partial moments
v0.12.0b1,"If any of the values in the parameter estimate is nan, return None"
v0.12.0b1,Compute partial moments
v0.12.0b1,Compute coefficient by OLS on residuals
v0.12.0b1,ell_2 regularization
v0.12.0b1,Ridge regression estimate
v0.12.0b1,"Parameter returned is of shape (d_T, )"
v0.12.0b1,Return moments and gradients
v0.12.0b1,Compute partial moments
v0.12.0b1,Compute moments
v0.12.0b1,"Moments shape is (n, d_T-1)"
v0.12.0b1,Compute moment gradients
v0.12.0b1,Need to calculate this in an elegant way for when propensity is 0
v0.12.0b1,This will flatten T
v0.12.0b1,Check that T is numeric
v0.12.0b1,Test whether the input estimator is supported
v0.12.0b1,Calculate confidence intervals for the parameter (marginal effect)
v0.12.0b1,Calculate confidence intervals for the effect
v0.12.0b1,Calculate the effects
v0.12.0b1,Calculate the standard deviations for the effects
v0.12.0b1,d_t=None here since we measure the effect across all Ts
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.12.0b1,Licensed under the MIT License.
v0.12.0b1,Causal tree parameters
v0.12.0b1,Tree structure
v0.12.0b1,No need for a random split since the data is already
v0.12.0b1,a random subsample from the original input
v0.12.0b1,node list stores the nodes that are yet to be splitted
v0.12.0b1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.12.0b1,Create local sample set
v0.12.0b1,Compute nuisance estimates for the current node
v0.12.0b1,Nuisance estimate cannot be calculated
v0.12.0b1,Estimate parameter for current node
v0.12.0b1,Node estimate cannot be calculated
v0.12.0b1,Calculate moments and gradient of moments for current data
v0.12.0b1,Calculate inverse gradient
v0.12.0b1,The gradient matrix is not invertible.
v0.12.0b1,No good split can be found
v0.12.0b1,Calculate point-wise pseudo-outcomes rho
v0.12.0b1,a split is determined by a feature and a sample pair
v0.12.0b1,the number of possible splits is at most (number of features) * (number of node samples)
v0.12.0b1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.12.0b1,parse row and column of random pair
v0.12.0b1,the sample of the pair is the integer division of the random number with n_feats
v0.12.0b1,calculate the binary indicator of whether sample i is on the left or the right
v0.12.0b1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.12.0b1,calculate the number of samples on the left child for each proposed split
v0.12.0b1,calculate the analogous binary indicator for the samples in the estimation set
v0.12.0b1,calculate the number of estimation samples on the left child of each proposed split
v0.12.0b1,find the upper and lower bound on the size of the left split for the split
v0.12.0b1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.12.0b1,on each side.
v0.12.0b1,similarly for the estimation sample set
v0.12.0b1,if there is no valid split then don't create any children
v0.12.0b1,filter only the valid splits
v0.12.0b1,calculate the average influence vector of the samples in the left child
v0.12.0b1,calculate the average influence vector of the samples in the right child
v0.12.0b1,take the square of each of the entries of the influence vectors and normalize
v0.12.0b1,by size of each child
v0.12.0b1,calculate the vector score of each candidate split as the average of left and right
v0.12.0b1,influence vectors
v0.12.0b1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.12.0b1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.12.0b1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.12.0b1,calculate the scalar score of each split by aggregating across the vector of scores
v0.12.0b1,Find split that minimizes criterion
v0.12.0b1,Create child nodes with corresponding subsamples
v0.12.0b1,add the created children to the list of not yet split nodes
v0.11.1,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.11.1,configuration is all pulled from setup.cfg
v0.11.1,-*- coding: utf-8 -*-
v0.11.1,
v0.11.1,Configuration file for the Sphinx documentation builder.
v0.11.1,
v0.11.1,This file does only contain a selection of the most common options. For a
v0.11.1,full list see the documentation:
v0.11.1,http://www.sphinx-doc.org/en/master/config
v0.11.1,-- Path setup --------------------------------------------------------------
v0.11.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.11.1,add these directories to sys.path here. If the directory is relative to the
v0.11.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.11.1,
v0.11.1,-- Project information -----------------------------------------------------
v0.11.1,-- General configuration ---------------------------------------------------
v0.11.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.11.1,
v0.11.1,needs_sphinx = '1.0'
v0.11.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.11.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.11.1,ones.
v0.11.1,"Add any paths that contain templates here, relative to this directory."
v0.11.1,The suffix(es) of source filenames.
v0.11.1,You can specify multiple suffix as a list of string:
v0.11.1,
v0.11.1,"source_suffix = ['.rst', '.md']"
v0.11.1,The master toctree document.
v0.11.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.11.1,for a list of supported languages.
v0.11.1,
v0.11.1,This is also used if you do content translation via gettext catalogs.
v0.11.1,"Usually you set ""language"" from the command line for these cases."
v0.11.1,"List of patterns, relative to source directory, that match files and"
v0.11.1,directories to ignore when looking for source files.
v0.11.1,This pattern also affects html_static_path and html_extra_path.
v0.11.1,The name of the Pygments (syntax highlighting) style to use.
v0.11.1,-- Options for HTML output -------------------------------------------------
v0.11.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.11.1,a list of builtin themes.
v0.11.1,
v0.11.1,Theme options are theme-specific and customize the look and feel of a theme
v0.11.1,"further.  For a list of options available for each theme, see the"
v0.11.1,documentation.
v0.11.1,
v0.11.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.11.1,"relative to this directory. They are copied after the builtin static files,"
v0.11.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.11.1,html_static_path = ['_static']
v0.11.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.11.1,to template names.
v0.11.1,
v0.11.1,The default sidebars (for documents that don't match any pattern) are
v0.11.1,defined by theme itself.  Builtin themes are using these templates by
v0.11.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.11.1,'searchbox.html']``.
v0.11.1,
v0.11.1,html_sidebars = {}
v0.11.1,-- Options for HTMLHelp output ---------------------------------------------
v0.11.1,Output file base name for HTML help builder.
v0.11.1,-- Options for LaTeX output ------------------------------------------------
v0.11.1,The paper size ('letterpaper' or 'a4paper').
v0.11.1,
v0.11.1,"'papersize': 'letterpaper',"
v0.11.1,"The font size ('10pt', '11pt' or '12pt')."
v0.11.1,
v0.11.1,"'pointsize': '10pt',"
v0.11.1,Additional stuff for the LaTeX preamble.
v0.11.1,
v0.11.1,"'preamble': '',"
v0.11.1,Latex figure (float) alignment
v0.11.1,
v0.11.1,"'figure_align': 'htbp',"
v0.11.1,Grouping the document tree into LaTeX files. List of tuples
v0.11.1,"(source start file, target name, title,"
v0.11.1,"author, documentclass [howto, manual, or own class])."
v0.11.1,-- Options for manual page output ------------------------------------------
v0.11.1,One entry per manual page. List of tuples
v0.11.1,"(source start file, name, description, authors, manual section)."
v0.11.1,-- Options for Texinfo output ----------------------------------------------
v0.11.1,Grouping the document tree into Texinfo files. List of tuples
v0.11.1,"(source start file, target name, title, author,"
v0.11.1,"dir menu entry, description, category)"
v0.11.1,-- Options for Epub output -------------------------------------------------
v0.11.1,Bibliographic Dublin Core info.
v0.11.1,The unique identifier of the text. This can be a ISBN number
v0.11.1,or the project homepage.
v0.11.1,
v0.11.1,epub_identifier = ''
v0.11.1,A unique identification for the text.
v0.11.1,
v0.11.1,epub_uid = ''
v0.11.1,A list of files that should not be packed into the epub file.
v0.11.1,-- Extension configuration -------------------------------------------------
v0.11.1,-- Options for intersphinx extension ---------------------------------------
v0.11.1,Example configuration for intersphinx: refer to the Python standard library.
v0.11.1,-- Options for todo extension ----------------------------------------------
v0.11.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.11.1,-- Options for doctest extension -------------------------------------------
v0.11.1,we can document otherwise excluded entities here by returning False
v0.11.1,or skip otherwise included entities by returning True
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Calculate residuals
v0.11.1,Estimate E[T_res | Z_res]
v0.11.1,TODO. Deal with multi-class instrument
v0.11.1,Calculate nuisances
v0.11.1,Estimate E[T_res | Z_res]
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.11.1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.11.1,TODO. Deal with multi-class instrument
v0.11.1,Estimate final model of theta(X) by minimizing the square loss:
v0.11.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.11.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.11.1,at the expense of some small bias. For points with very small covariance we revert
v0.11.1,to the model-based preliminary estimate and do not add the correction term.
v0.11.1,Estimate preliminary theta in cross fitting manner
v0.11.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.11.1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.11.1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.11.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.11.1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.11.1,TODO. The solution below is not really a valid cross-fitting
v0.11.1,as the test data are used to create the proj_t on the train
v0.11.1,which in the second train-test loop is used to create the nuisance
v0.11.1,cov on the test data. Hence the T variable of some sample
v0.11.1,"is implicitly correlated with its cov nuisance, through this flow"
v0.11.1,"of information. However, this seems a rather weak correlation."
v0.11.1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.11.1,model.
v0.11.1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.11.1,Estimate preliminary theta in cross fitting manner
v0.11.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.11.1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.11.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.11.1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.11.1,#############################################################################
v0.11.1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.11.1,A/B test
v0.11.1,#############################################################################
v0.11.1,Estimate preliminary theta in cross fitting manner
v0.11.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.11.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.11.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.11.1,We can use statsmodel for all hypothesis testing capabilities
v0.11.1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.11.1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.11.1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.11.1,model_T_XZ = lambda: model_clf()
v0.11.1,#'days_visited': lambda:
v0.11.1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.11.1,Turn strings into categories for numeric mapping
v0.11.1,### Defining some generic regressors and classifiers
v0.11.1,This a generic non-parametric regressor
v0.11.1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.11.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.11.1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.11.1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.11.1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.11.1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.11.1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.11.1,model = lambda: LinearRegression(n_jobs=-1)
v0.11.1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.11.1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.11.1,underlying model whenever predict is called.
v0.11.1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.11.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.11.1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.11.1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.11.1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.11.1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.11.1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.11.1,We need to specify models to be used for each of these residualizations
v0.11.1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.11.1,"E[T | X, Z]"
v0.11.1,E[TZ | X]
v0.11.1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.11.1,n_splits determines the number of splits to be used for cross-fitting.
v0.11.1,# Algorithm 2 - Current Method
v0.11.1,In[121]:
v0.11.1,# Algorithm 3 - DRIV ATE
v0.11.1,dmliv_model_effect = lambda: model()
v0.11.1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.11.1,"dmliv_model_effect(),"
v0.11.1,n_splits=1)
v0.11.1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.11.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.11.1,"Once multiple treatments are supported, we'll need to fix this"
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.11.1,We can use statsmodel for all hypothesis testing capabilities
v0.11.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.11.1,We can use statsmodel for all hypothesis testing capabilities
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,TODO. Deal with multi-class instrument/treatment
v0.11.1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.11.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.11.1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.11.1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.11.1,##################
v0.11.1,Global settings #
v0.11.1,##################
v0.11.1,Global plotting controls
v0.11.1,"Control for support size, can control for more"
v0.11.1,#################
v0.11.1,File utilities #
v0.11.1,#################
v0.11.1,#################
v0.11.1,Plotting utils #
v0.11.1,#################
v0.11.1,bias
v0.11.1,var
v0.11.1,rmse
v0.11.1,r2
v0.11.1,Infer feature dimension
v0.11.1,Metrics by support plots
v0.11.1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.11.1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.11.1,Steven Wu <zhiww@microsoft.com>
v0.11.1,Initialize causal tree parameters
v0.11.1,Create splits of causal tree
v0.11.1,Estimate treatment effects at the leafs
v0.11.1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.11.1,the corresponding split and associating the effect computed on that leaf
v0.11.1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.11.1,Safety check
v0.11.1,Weighted linear regression
v0.11.1,Calculates weights
v0.11.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.11.1,over all indices
v0.11.1,Similar for `a` weights
v0.11.1,Doesn't have sample weights
v0.11.1,Is a linear model
v0.11.1,Weighted linear regression
v0.11.1,Calculates weights
v0.11.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.11.1,over all indices
v0.11.1,Similar for `a` weights
v0.11.1,normalize weights
v0.11.1,"Split the data in half, train and test"
v0.11.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.11.1,"a function of W, using only the train fold"
v0.11.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.11.1,"Split the data in half, train and test"
v0.11.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.11.1,"a function of W, using only the train fold"
v0.11.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.11.1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.11.1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.11.1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.11.1,"Split the data in half, train and test"
v0.11.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.11.1,"a function of x, using only the train fold"
v0.11.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.11.1,Compute coefficient by OLS on residuals
v0.11.1,"Split the data in half, train and test"
v0.11.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.11.1,"a function of x, using only the train fold"
v0.11.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.11.1,Estimate multipliers for second order orthogonal method
v0.11.1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.11.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.11.1,Create local sample set
v0.11.1,compute the base estimate for the current node using double ml or second order double ml
v0.11.1,compute the influence functions here that are used for the criterion
v0.11.1,generate random proposals of dimensions to split
v0.11.1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.11.1,compute criterion for each proposal
v0.11.1,if splitting creates valid leafs in terms of mean leaf size
v0.11.1,Calculate criterion for split
v0.11.1,Else set criterion to infinity so that this split is not chosen
v0.11.1,If no good split was found
v0.11.1,Find split that minimizes criterion
v0.11.1,Set the split attributes at the node
v0.11.1,Create child nodes with corresponding subsamples
v0.11.1,Recursively split children
v0.11.1,Return parent node
v0.11.1,estimate the local parameter at the leaf using the estimate data
v0.11.1,###################
v0.11.1,Argument parsing #
v0.11.1,###################
v0.11.1,#########################################
v0.11.1,Parameters constant across experiments #
v0.11.1,#########################################
v0.11.1,Outcome support
v0.11.1,Treatment support
v0.11.1,Evaluation grid
v0.11.1,Treatment effects array
v0.11.1,Other variables
v0.11.1,##########################
v0.11.1,Data Generating Process #
v0.11.1,##########################
v0.11.1,Log iteration
v0.11.1,"Generate controls, features, treatment and outcome"
v0.11.1,T and Y residuals to be used in later scripts
v0.11.1,Save generated dataset
v0.11.1,#################
v0.11.1,ORF parameters #
v0.11.1,#################
v0.11.1,######################################
v0.11.1,Train and evaluate treatment effect #
v0.11.1,######################################
v0.11.1,########
v0.11.1,Plots #
v0.11.1,########
v0.11.1,###############
v0.11.1,Save results #
v0.11.1,###############
v0.11.1,##############
v0.11.1,Run Rscript #
v0.11.1,##############
v0.11.1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.11.1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.11.1,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.11.1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.11.1,def mlasso_model(): return MultiTaskLassoCV(
v0.11.1,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.11.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.11.1,heterogeneity
v0.11.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.11.1,heterogeneity
v0.11.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.11.1,heterogeneity
v0.11.1,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.11.1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.11.1,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.11.1,subset of features that are exogenous and create heterogeneity
v0.11.1,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.11.1,subset of features wrt we estimate heterogeneity
v0.11.1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.11.1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,introspect the constructor arguments to find the model parameters
v0.11.1,to represent
v0.11.1,"if the argument is deprecated, ignore it"
v0.11.1,Extract and sort argument names excluding 'self'
v0.11.1,column names
v0.11.1,transfer input to numpy arrays
v0.11.1,transfer input to 2d arrays
v0.11.1,create dataframe
v0.11.1,currently dowhy only support single outcome and single treatment
v0.11.1,call dowhy
v0.11.1,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.11.1,cate estimator but not the effect.
v0.11.1,don't proxy special methods
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Check if model is sparse enough for this model
v0.11.1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.11.1,TODO: any way to avoid creating a copy if the array was already dense?
v0.11.1,"the call is necessary if the input was something like a list, though"
v0.11.1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.11.1,so convert to pydata sparse first
v0.11.1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.11.1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.11.1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.11.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.11.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.11.1,For when checking input values is disabled
v0.11.1,Type to column extraction function
v0.11.1,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.11.1,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.11.1,Get feature names using featurizer
v0.11.1,All attempts at retrieving transformed feature names have failed
v0.11.1,Delegate handling to downstream logic
v0.11.1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.11.1,same number of input definitions as arrays
v0.11.1,input definitions have same number of dimensions as each array
v0.11.1,all result indices are unique
v0.11.1,all result indices must match at least one input index
v0.11.1,"map indices to all array, axis pairs for that index"
v0.11.1,each index has the same cardinality wherever it appears
v0.11.1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.11.1,Algo: while list contains more than one entry
v0.11.1,take two entries
v0.11.1,sort both lists by intersection of their indices
v0.11.1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.11.1,"take the union of indices and the product of values), stepping through each list linearly"
v0.11.1,TODO: might be faster to break into connected components first
v0.11.1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.11.1,"so compute their content separately, then take cartesian product"
v0.11.1,this would save a few pointless sorts by empty tuples
v0.11.1,TODO: Consider investigating other performance ideas for these cases
v0.11.1,where the dense method beat the sparse method (usually sparse is faster)
v0.11.1,"e,facd,c->cfed"
v0.11.1,sparse: 0.0335489
v0.11.1,dense:  0.011465999999999997
v0.11.1,"gbd,da,egb->da"
v0.11.1,sparse: 0.0791625
v0.11.1,dense:  0.007319099999999995
v0.11.1,"dcc,d,faedb,c->abe"
v0.11.1,sparse: 1.2868097
v0.11.1,dense:  0.44605229999999985
v0.11.1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.11.1,TODO: would using einsum's paths to optimize the order of merging help?
v0.11.1,assume that we should perform nested cross-validation if and only if
v0.11.1,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.11.1,logic copied from check_cv
v0.11.1,otherwise we will assume the user already set the cv attribute to something
v0.11.1,compatible with splitting with a 'groups' argument
v0.11.1,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.11.1,don't use the groups when calling split internally
v0.11.1,Normalize weights
v0.11.1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.11.1,"if we're decorating a class, just update the __init__ method,"
v0.11.1,so that the result is still a class instead of a wrapper method
v0.11.1,"want to enforce that each bad_arg was either in kwargs,"
v0.11.1,or else it was in neither and is just taking its default value
v0.11.1,Any access should throw
v0.11.1,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.11.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.11.1,input feature name is already updated by cate_feature_names.
v0.11.1,define the index of d_x to filter for each given T
v0.11.1,filter X after broadcast with T for each given T
v0.11.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.11.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,
v0.11.1,This code contains some snippets of code from:
v0.11.1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
v0.11.1,published under the following license and copyright:
v0.11.1,BSD 3-Clause License
v0.11.1,
v0.11.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.1,All rights reserved.
v0.11.1,make any access to matplotlib or plt throw an exception
v0.11.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.11.1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.11.1,Initialize saturation & value; calculate chroma & value shift
v0.11.1,Calculate some intermediate values
v0.11.1,Initialize RGB with same hue & chroma as our color
v0.11.1,Shift the initial RGB values to match value and store
v0.11.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.11.1,clean way of achieving this
v0.11.1,make sure we don't accidentally escape anything in the substitution
v0.11.1,Fetch appropriate color for node
v0.11.1,"red for negative, green for positive"
v0.11.1,in multi-target use mean of targets
v0.11.1,Write node mean CATE
v0.11.1,Write node std of CATE
v0.11.1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.11.1,Fetch appropriate color for node
v0.11.1,Write node mean CATE
v0.11.1,Write node mean CATE
v0.11.1,Write recommended treatment and value - cost
v0.11.1,Licensed under the MIT License.
v0.11.1,"since inference objects can be stateful, we must copy it before fitting;"
v0.11.1,otherwise this sequence wouldn't work:
v0.11.1,"est1.fit(..., inference=inf)"
v0.11.1,"est2.fit(..., inference=inf)"
v0.11.1,est1.effect_interval(...)
v0.11.1,because inf now stores state from fitting est2
v0.11.1,This flag is true when names are set in a child class instead
v0.11.1,"If names are set in a child class, add an attribute reflecting that"
v0.11.1,This works only if X is passed as a kwarg
v0.11.1,We plan to enforce X as kwarg only in future releases
v0.11.1,This checks if names have been set in a child class
v0.11.1,"If names were set in a child class, don't do it again"
v0.11.1,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.11.1,call the wrapped fit method
v0.11.1,NOTE: we call inference fit *after* calling the main fit method
v0.11.1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.11.1,but tensordot can't be applied to this problem because we don't sum over m
v0.11.1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.11.1,of rows of T was not taken into account
v0.11.1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.11.1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.11.1,"Treatment names is None, default to BaseCateEstimator"
v0.11.1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.11.1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.11.1,Get input names
v0.11.1,Summary
v0.11.1,add statsmodels to parent's options
v0.11.1,add debiasedlasso to parent's options
v0.11.1,add blb to parent's options
v0.11.1,TODO Share some logic with non-discrete version
v0.11.1,Get input names
v0.11.1,Summary
v0.11.1,add statsmodels to parent's options
v0.11.1,add statsmodels to parent's options
v0.11.1,add blb to parent's options
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,remove None arguments
v0.11.1,"scores entries should be lists of scores, so make each entry a singleton list"
v0.11.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.11.1,generate an instance of the final model
v0.11.1,generate an instance of the nuisance model
v0.11.1,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.11.1,alteration even when we only want to fit_final.
v0.11.1,use a binary array to get stratified split in case of discrete treatment
v0.11.1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.11.1,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.11.1,"however, sklearn doesn't support both stratifying and grouping (see"
v0.11.1,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.11.1,their own object that supports grouping if they want to use groups.
v0.11.1,for each mc iteration
v0.11.1,for each model under cross fit setting
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,
v0.11.1,This code contains snippets of code from
v0.11.1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.11.1,published under the following license and copyright:
v0.11.1,BSD 3-Clause License
v0.11.1,
v0.11.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.1,All rights reserved.
v0.11.1,=============================================================================
v0.11.1,Policy Forest
v0.11.1,=============================================================================
v0.11.1,Remap output
v0.11.1,reshape is necessary to preserve the data contiguity against vs
v0.11.1,"[:, np.newaxis] that does not."
v0.11.1,Get subsample sample size
v0.11.1,Check parameters
v0.11.1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.11.1,if this is the first `fit` call of the warm start mode.
v0.11.1,"Free allocated memory, if any"
v0.11.1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.11.1,We draw from the random state to get the random state we
v0.11.1,would have got if we hadn't used a warm_start.
v0.11.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.11.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.11.1,but would still advance randomness enough so that tree subsamples will be different.
v0.11.1,Parallel loop: we prefer the threading backend as the Cython code
v0.11.1,for fitting the trees is internally releasing the Python GIL
v0.11.1,making threading more efficient than multiprocessing in
v0.11.1,"that case. However, for joblib 0.12+ we respect any"
v0.11.1,"parallel_backend contexts set at a higher level,"
v0.11.1,since correctness does not rely on using threads.
v0.11.1,Collect newly grown trees
v0.11.1,Check data
v0.11.1,Assign chunk of trees to jobs
v0.11.1,avoid storing the output of every estimator by summing them here
v0.11.1,Parallel loop
v0.11.1,Check data
v0.11.1,Assign chunk of trees to jobs
v0.11.1,avoid storing the output of every estimator by summing them here
v0.11.1,Parallel loop
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,
v0.11.1,This code contains snippets of code from:
v0.11.1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.11.1,published under the following license and copyright:
v0.11.1,BSD 3-Clause License
v0.11.1,
v0.11.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.1,All rights reserved.
v0.11.1,=============================================================================
v0.11.1,Types and constants
v0.11.1,=============================================================================
v0.11.1,=============================================================================
v0.11.1,Base Policy tree
v0.11.1,=============================================================================
v0.11.1,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.11.1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.11.1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.11.1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.11.1,checks that X is 2D array.
v0.11.1,"since we only allow single dimensional y, we could flatten the prediction"
v0.11.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.11.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.11.1,Handles the corner case when X=None but featurizer might be not None
v0.11.1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.11.1,"Replacing this method which is invalid for this class, so that we make the"
v0.11.1,dosctring empty and not appear in the docs.
v0.11.1,TODO: support freq_weight and sample_var in debiased lasso
v0.11.1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.11.1,Replacing to remove docstring
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"if both X and W are None, just return a column of ones"
v0.11.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.11.1,We need to go back to the label representation of the one-hot so as to call
v0.11.1,the classifier.
v0.11.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.11.1,We need to go back to the label representation of the one-hot so as to call
v0.11.1,the classifier.
v0.11.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.11.1,This works both with our without the weighting trick as the treatments T are unit vector
v0.11.1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.11.1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.11.1,both Parametric and Non Parametric DML.
v0.11.1,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.11.1,attribute so that the trained featurizer will be passed through
v0.11.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.11.1,for internal use by the library
v0.11.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.11.1,We need to use the rlearner's copy to retain the information from fitting
v0.11.1,Handles the corner case when X=None but featurizer might be not None
v0.11.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.11.1,since we clone it and fit separate copies
v0.11.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.11.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.11.1,TODO: support freq_weight and sample_var in debiased lasso
v0.11.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.11.1,since we clone it and fit separate copies
v0.11.1,add blb to parent's options
v0.11.1,override only so that we can update the docstring to indicate
v0.11.1,support for `GenericSingleTreatmentModelFinalInference`
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,note that groups are not passed to score because they are only used for fitting
v0.11.1,note that groups are not passed to score because they are only used for fitting
v0.11.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.11.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.11.1,NOTE: important to get parent's wrapped copy so that
v0.11.1,"after training wrapped featurizer is also trained, etc."
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.11.1,Fit a doubly robust average effect
v0.11.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.11.1,(which needs to have been expanded if there's a discrete treatment)
v0.11.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.11.1,since we clone it and fit separate copies
v0.11.1,"If custom param grid, check that only estimator parameters are being altered"
v0.11.1,override only so that we can update the docstring to indicate support for `blb`
v0.11.1,Get input names
v0.11.1,Summary
v0.11.1,Determine output settings
v0.11.1,"Important: This must be the first invocation of the random state at fit time, so that"
v0.11.1,train/test splits are re-generatable from an external object simply by knowing the
v0.11.1,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.11.1,linear predictions. Currently is also useful for testing.
v0.11.1,reshape is necessary to preserve the data contiguity against vs
v0.11.1,"[:, np.newaxis] that does not."
v0.11.1,Check parameters
v0.11.1,Set min_weight_leaf from min_weight_fraction_leaf
v0.11.1,Build tree
v0.11.1,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.11.1,hold. Used by criterion for memory space savings.
v0.11.1,Initialize the criterion object and the criterion_val object if honest.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,
v0.11.1,This code is a fork from:
v0.11.1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.11.1,published under the following license and copyright:
v0.11.1,BSD 3-Clause License
v0.11.1,
v0.11.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.1,All rights reserved.
v0.11.1,Set parameters
v0.11.1,Don't instantiate estimators now! Parameters of base_estimator might
v0.11.1,"still change. Eg., when grid-searching with the nested object syntax."
v0.11.1,self.estimators_ needs to be filled by the derived classes in fit.
v0.11.1,Compute the number of jobs
v0.11.1,Partition estimators between jobs
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Remove children with nonwhite mothers from the treatment group
v0.11.1,Remove children with nonwhite mothers from the treatment group
v0.11.1,Select columns
v0.11.1,Scale the numeric variables
v0.11.1,"Change the binary variable 'first' takes values in {1,2}"
v0.11.1,Append a column of ones as intercept
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.11.1,(which needs to have been expanded if there's a discrete treatment)
v0.11.1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.11.1,d_t=None here since we measure the effect across all Ts
v0.11.1,once the estimator has been fit
v0.11.1,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.11.1,an intercept even when bias is part of coef.
v0.11.1,We can write effect inference as a function of prediction and prediction standard error of
v0.11.1,the final method for linear models
v0.11.1,squeeze the first axis
v0.11.1,d_t=None here since we measure the effect across all Ts
v0.11.1,set the mean_pred_stderr
v0.11.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.11.1,(which needs to have been expanded if there's a discrete treatment)
v0.11.1,"send treatment to the end, pull bounds to the front"
v0.11.1,d_t=None here since we measure the effect across all Ts
v0.11.1,set the mean_pred_stderr
v0.11.1,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.11.1,d_t=None here since we measure the effect across all Ts
v0.11.1,d_t=None here since we measure the effect across all Ts
v0.11.1,need to set the fit args before the estimator is fit
v0.11.1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.11.1,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.11.1,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.11.1,NOTE: use np.asarray(offset) becuase if offset is a pd.Series direct addition would make the sum
v0.11.1,"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
v0.11.1,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.11.1,offset preds
v0.11.1,"offset the distribution, too"
v0.11.1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.11.1,1. Uncertainty of Mean Point Estimate
v0.11.1,2. Distribution of Point Estimate
v0.11.1,3. Total Variance of Point Estimate
v0.11.1,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.11.1,so bail out early; note that it might be possible to correct the algorithm for
v0.11.1,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.11.1,be clean
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,TODO: Add a __dir__ implementation?
v0.11.1,don't proxy special methods
v0.11.1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.11.1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.11.1,then we should be computing a confidence interval for the wrapped calls
v0.11.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.11.1,second level bootstrap which would be prohibitive computationally?
v0.11.1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.11.1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.11.1,can't import from econml.inference at top level without creating cyclical dependencies
v0.11.1,Note that inference results are always methods even if the inference is for a property
v0.11.1,(e.g. coef__inference() is a method but coef_ is a property)
v0.11.1,Therefore we must insert a lambda if getting inference for a non-callable
v0.11.1,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.11.1,"try to get interval/std first if appropriate,"
v0.11.1,since we don't prefer a wrapped method with this name
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,
v0.11.1,This code contains snippets of code from:
v0.11.1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.11.1,published under the following license and copyright:
v0.11.1,BSD 3-Clause License
v0.11.1,
v0.11.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.1,All rights reserved.
v0.11.1,=============================================================================
v0.11.1,Types and constants
v0.11.1,=============================================================================
v0.11.1,=============================================================================
v0.11.1,Base GRF tree
v0.11.1,=============================================================================
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,=============================================================================
v0.11.1,A MultOutputWrapper for GRF classes
v0.11.1,=============================================================================
v0.11.1,=============================================================================
v0.11.1,Instantiations of Generalized Random Forest
v0.11.1,=============================================================================
v0.11.1,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.11.1,in front of the constant treatment is the intercept in the moment equation.
v0.11.1,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.11.1,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,
v0.11.1,This code contains snippets of code from
v0.11.1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.11.1,published under the following license and copyright:
v0.11.1,BSD 3-Clause License
v0.11.1,
v0.11.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.1,All rights reserved.
v0.11.1,=============================================================================
v0.11.1,Base Generalized Random Forest
v0.11.1,=============================================================================
v0.11.1,TODO: support freq_weight and sample_var
v0.11.1,Remap output
v0.11.1,reshape is necessary to preserve the data contiguity against vs
v0.11.1,"[:, np.newaxis] that does not."
v0.11.1,reshape is necessary to preserve the data contiguity against vs
v0.11.1,"[:, np.newaxis] that does not."
v0.11.1,Get subsample sample size
v0.11.1,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.11.1,We calculate the min eigenvalue proxy that each criterion is considering
v0.11.1,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.11.1,Check parameters
v0.11.1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.11.1,if this is the first `fit` call of the warm start mode.
v0.11.1,"Free allocated memory, if any"
v0.11.1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.11.1,We draw from the random state to get the random state we
v0.11.1,would have got if we hadn't used a warm_start.
v0.11.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.11.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.11.1,but would still advance randomness enough so that tree subsamples will be different.
v0.11.1,Generating indices a priori before parallelism ended up being orders of magnitude
v0.11.1,faster than how sklearn does it. The reason is that random samplers do not release the
v0.11.1,gil it seems.
v0.11.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.11.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.11.1,but would still advance randomness enough so that tree subsamples will be different.
v0.11.1,Parallel loop: we prefer the threading backend as the Cython code
v0.11.1,for fitting the trees is internally releasing the Python GIL
v0.11.1,making threading more efficient than multiprocessing in
v0.11.1,"that case. However, for joblib 0.12+ we respect any"
v0.11.1,"parallel_backend contexts set at a higher level,"
v0.11.1,since correctness does not rely on using threads.
v0.11.1,Collect newly grown trees
v0.11.1,Check data
v0.11.1,Assign chunk of trees to jobs
v0.11.1,avoid storing the output of every estimator by summing them here
v0.11.1,Parallel loop
v0.11.1,Check data
v0.11.1,Assign chunk of trees to jobs
v0.11.1,Parallel loop
v0.11.1,Check data
v0.11.1,Assign chunk of trees to jobs
v0.11.1,Parallel loop
v0.11.1,####################
v0.11.1,Variance correction
v0.11.1,####################
v0.11.1,Subtract the average within bag variance. This ends up being equal to the
v0.11.1,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.11.1,The negative part is just sq_between.
v0.11.1,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.11.1,"The off diagonals we have no objective prior, so no correction is applied."
v0.11.1,Finally correcting the pred_cov or pred_var
v0.11.1,avoid storing the output of every estimator by summing them here
v0.11.1,Parallel loop
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,testing importances
v0.11.1,testing heterogeneity importances
v0.11.1,Testing that all parameters do what they are supposed to
v0.11.1,"testing predict, apply and decision path"
v0.11.1,test that the subsampling scheme past to the trees is correct
v0.11.1,The sample size is chosen in particular to test rounding based error when subsampling
v0.11.1,test that the estimator calcualtes var correctly
v0.11.1,test api
v0.11.1,test accuracy
v0.11.1,test the projection functionality of forests
v0.11.1,test that the estimator calcualtes var correctly
v0.11.1,test api
v0.11.1,test that the estimator calcualtes var correctly
v0.11.1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.11.1,test that we raise errors in mishandled situations.
v0.11.1,test that the subsampling scheme past to the trees is correct
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,omit the lalonde notebook
v0.11.1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.11.1,creating notebooks that are annoying for our users to actually run themselves
v0.11.1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.11.1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.11.1,"prior to calling interpret, can't plot, render, etc."
v0.11.1,can interpret without uncertainty
v0.11.1,can't interpret with uncertainty if inference wasn't used during fit
v0.11.1,can interpret with uncertainty if we refit
v0.11.1,can interpret without uncertainty
v0.11.1,can't treat before interpreting
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,simple DGP only for illustration
v0.11.1,Define the treatment model neural network architecture
v0.11.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.11.1,"so the input shape is (d_z + d_x,)"
v0.11.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.11.1,add extra layers on top for the mixture density network
v0.11.1,Define the response model neural network architecture
v0.11.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.11.1,"so the input shape is (d_t + d_x,)"
v0.11.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.11.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.11.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.11.1,so that the same weights will be reused in each instantiation
v0.11.1,number of samples to use in second estimate of the response
v0.11.1,(to make loss estimate unbiased)
v0.11.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.11.1,do something with predictions...
v0.11.1,also test vector t and y
v0.11.1,simple DGP only for illustration
v0.11.1,Define the treatment model neural network architecture
v0.11.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.11.1,"so the input shape is (d_z + d_x,)"
v0.11.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.11.1,add extra layers on top for the mixture density network
v0.11.1,Define the response model neural network architecture
v0.11.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.11.1,"so the input shape is (d_t + d_x,)"
v0.11.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.11.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.11.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.11.1,so that the same weights will be reused in each instantiation
v0.11.1,number of samples to use in second estimate of the response
v0.11.1,(to make loss estimate unbiased)
v0.11.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.11.1,do something with predictions...
v0.11.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.11.1,test = True ensures we draw test set images
v0.11.1,test = True ensures we draw test set images
v0.11.1,re-draw to get new independent treatment and implied response
v0.11.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.11.1,above is necesary so that reduced form doesn't win
v0.11.1,covariates: time and emotion
v0.11.1,random instrument
v0.11.1,z -> price
v0.11.1,true observable demand function
v0.11.1,errors
v0.11.1,response
v0.11.1,test = True ensures we draw test set images
v0.11.1,test = True ensures we draw test set images
v0.11.1,re-draw to get new independent treatment and implied response
v0.11.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.11.1,above is necesary so that reduced form doesn't win
v0.11.1,covariates: time and emotion
v0.11.1,random instrument
v0.11.1,z -> price
v0.11.1,true observable demand function
v0.11.1,errors
v0.11.1,response
v0.11.1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.11.1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.11.1,For some reason this doesn't work at all when run against the CNTK backend...
v0.11.1,"model.compile('nadam', loss=lambda _,l:l)"
v0.11.1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.11.1,generate a valiation set
v0.11.1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.11.1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,DGP constants
v0.11.1,Generate data
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,testing importances
v0.11.1,testing heterogeneity importances
v0.11.1,Testing that all parameters do what they are supposed to
v0.11.1,"testing predict, apply and decision path"
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.11.1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.11.1,so we need to transpose the result
v0.11.1,1-d output
v0.11.1,2-d output
v0.11.1,Single dimensional output y
v0.11.1,compare with weight
v0.11.1,compare with weight
v0.11.1,compare with weight
v0.11.1,compare with weight
v0.11.1,Multi-dimensional output y
v0.11.1,1-d y
v0.11.1,compare when both sample_var and sample_weight exist
v0.11.1,multi-d y
v0.11.1,compare when both sample_var and sample_weight exist
v0.11.1,compare when both sample_var and sample_weight exist
v0.11.1,compare when both sample_var and sample_weight exist
v0.11.1,compare when both sample_var and sample_weight exist
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,test that we can fit with the same arguments as the base estimator
v0.11.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.11.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.1,with the same shape for the lower and upper bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,test that we can do the same thing once we provide percentile bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.11.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.1,with the same shape for the lower and upper bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,test that we can do the same thing once we provide percentile bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,test that we can fit with the same arguments as the base estimator
v0.11.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.11.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.1,with the same shape for the lower and upper bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,test that we can do the same thing once we provide percentile bounds
v0.11.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.11.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.1,with the same shape for the lower and upper bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,test that we can do the same thing once we provide percentile bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,test that we can fit with the same arguments as the base estimator
v0.11.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.11.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.1,with the same shape for the lower and upper bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,test that we can do the same thing once we provide percentile bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.11.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.1,with the same shape for the lower and upper bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,test that we can do the same thing once we provide percentile bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.1,with the same shape for the lower and upper bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,test that the estimated effect is usually within the bounds
v0.11.1,test that we can do the same thing once we provide alpha explicitly
v0.11.1,test that the lower and upper bounds differ
v0.11.1,test that the estimated effect is usually within the bounds
v0.11.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.1,with the same shape for the lower and upper bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,TODO: test that the estimated effect is usually within the bounds
v0.11.1,and that the true effect is also usually within the bounds
v0.11.1,test that we can do the same thing once we provide percentile bounds
v0.11.1,test that the lower and upper bounds differ
v0.11.1,TODO: test that the estimated effect is usually within the bounds
v0.11.1,and that the true effect is also usually within the bounds
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,test that the subsampling scheme past to the trees is correct
v0.11.1,test that the estimator calcualtes var correctly
v0.11.1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.11.1,test that we raise errors in mishandled situations.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,DGP constants
v0.11.1,Generate data
v0.11.1,Test inference results when `cate_feature_names` doesn not exist
v0.11.1,Test inference results when `cate_feature_names` doesn not exist
v0.11.1,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.11.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.11.1,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.11.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.11.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.11.1,pvalue for second column should be greater than zero since some points are on either side
v0.11.1,of the tested value
v0.11.1,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.11.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.11.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.11.1,only is not None when T1 is a constant or a list of constant
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.11.1,Test non keyword based calls to fit
v0.11.1,test non-array inputs
v0.11.1,Test custom splitter
v0.11.1,Test incomplete set of test folds
v0.11.1,"y scores should be positive, since W predicts Y somewhat"
v0.11.1,"t scores might not be, since W and T are uncorrelated"
v0.11.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,make sure cross product varies more slowly with first array
v0.11.1,and that vectors are okay as inputs
v0.11.1,number of inputs in specification must match number of inputs
v0.11.1,must have an output
v0.11.1,output indices must be unique
v0.11.1,output indices must be present in an input
v0.11.1,number of indices must match number of dimensions for each input
v0.11.1,repeated indices must always have consistent sizes
v0.11.1,transpose
v0.11.1,tensordot
v0.11.1,trace
v0.11.1,TODO: set up proper flag for this
v0.11.1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.11.1,"of all of the distinct indices that appear in any input,"
v0.11.1,pick a random subset of them (of size at most 5) to appear in the output
v0.11.1,creating an instance should warn
v0.11.1,using the instance should not warn
v0.11.1,using the deprecated method should warn
v0.11.1,don't warn if b and c are passed by keyword
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Preprocess data
v0.11.1,Convert 'week' to a date
v0.11.1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.11.1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.11.1,Take log of price
v0.11.1,Make brand numeric
v0.11.1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.11.1,which have all zero coeffs
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,test at least one estimator from each category
v0.11.1,test causal graph
v0.11.1,test refutation estimate
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.11.1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.11.1,TODO: test something rather than just print...
v0.11.1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.11.1,pick some arbitrary X
v0.11.1,pick some arbitrary T
v0.11.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.11.1,The average variance should be lower when using monte carlo iterations
v0.11.1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.11.1,The average variance should be lower when using monte carlo iterations
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,ensure that we've got at least two of every row
v0.11.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.11.1,need to make sure we get all *joint* combinations
v0.11.1,IntentToTreat only supports binary treatments/instruments
v0.11.1,IntentToTreat only supports binary treatments/instruments
v0.11.1,IntentToTreat requires X
v0.11.1,ensure we can serialize unfit estimator
v0.11.1,these support only W but not X
v0.11.1,"these support only binary, not general discrete T and Z"
v0.11.1,ensure we can serialize fit estimator
v0.11.1,make sure we can call the marginal_effect and effect methods
v0.11.1,TODO: add tests for extra properties like coef_ where they exist
v0.11.1,TODO: add tests for extra properties like coef_ where they exist
v0.11.1,"make sure we can call effect with implied scalar treatments,"
v0.11.1,"no matter the dimensions of T, and also that we warn when there"
v0.11.1,are multiple treatments
v0.11.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.11.1,"however, with custom splits the checking happens in the first stage wrapper"
v0.11.1,where we don't have all of the required information to do this;
v0.11.1,we'd probably need to add it to _crossfit instead
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.11.1,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.11.1,The __warningregistry__'s need to be in a pristine state for tests
v0.11.1,to work properly.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Set random seed
v0.11.1,Generate data
v0.11.1,DGP constants
v0.11.1,Test data
v0.11.1,Constant treatment effect
v0.11.1,Constant treatment with multi output Y
v0.11.1,Heterogeneous treatment
v0.11.1,Heterogeneous treatment with multi output Y
v0.11.1,TLearner test
v0.11.1,Instantiate TLearner
v0.11.1,Test inputs
v0.11.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.11.1,Instantiate SLearner
v0.11.1,Test inputs
v0.11.1,Test constant treatment effect
v0.11.1,Test constant treatment effect with multi output Y
v0.11.1,Test heterogeneous treatment effect
v0.11.1,Need interactions between T and features
v0.11.1,Test heterogeneous treatment effect with multi output Y
v0.11.1,Instantiate XLearner
v0.11.1,Test inputs
v0.11.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.11.1,Instantiate DomainAdaptationLearner
v0.11.1,Test inputs
v0.11.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.11.1,Get the true treatment effect
v0.11.1,Get the true treatment effect
v0.11.1,Fit learner and get the effect and marginal effect
v0.11.1,Compute treatment effect residuals (absolute)
v0.11.1,Check that at least 90% of predictions are within tolerance interval
v0.11.1,Check whether the output shape is right
v0.11.1,Check that one can pass in regular lists
v0.11.1,Check that it fails correctly if lists of different shape are passed in
v0.11.1,"Check that it works when T, Y have shape (n, 1)"
v0.11.1,Generate covariates
v0.11.1,Generate treatment
v0.11.1,Calculate outcome
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,DGP constants
v0.11.1,Generate data
v0.11.1,Test data
v0.11.1,Remove warnings that might be raised by the models passed into the ORF
v0.11.1,Generate data with continuous treatments
v0.11.1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.11.1,does not work well with parallelism.
v0.11.1,Test inputs for continuous treatments
v0.11.1,--> Check that one can pass in regular lists
v0.11.1,--> Check that it fails correctly if lists of different shape are passed in
v0.11.1,Check that outputs have the correct shape
v0.11.1,Test continuous treatments with controls
v0.11.1,Test continuous treatments without controls
v0.11.1,Generate data with binary treatments
v0.11.1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.11.1,does not work well with parallelism.
v0.11.1,Test inputs for binary treatments
v0.11.1,--> Check that one can pass in regular lists
v0.11.1,--> Check that it fails correctly if lists of different shape are passed in
v0.11.1,"--> Check that it works when T, Y have shape (n, 1)"
v0.11.1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.11.1,--> Check that it fails correctly when the treatments are not numeric
v0.11.1,Check that outputs have the correct shape
v0.11.1,Test binary treatments with controls
v0.11.1,Test binary treatments without controls
v0.11.1,Only applicable to continuous treatments
v0.11.1,Generate data for 2 treatments
v0.11.1,Test multiple treatments with controls
v0.11.1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.11.1,The rest for controls. Just as an example.
v0.11.1,Generating A/B test data
v0.11.1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.11.1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.11.1,Create a wrapper around Lasso that doesn't support weights
v0.11.1,since Lasso does natively support them starting in sklearn 0.23
v0.11.1,Generate data with continuous treatments
v0.11.1,Instantiate model with most of the default parameters
v0.11.1,Compute the treatment effect on test points
v0.11.1,Compute treatment effect residuals
v0.11.1,Multiple treatments
v0.11.1,Allow at most 10% test points to be outside of the tolerance interval
v0.11.1,Compute treatment effect residuals
v0.11.1,Multiple treatments
v0.11.1,Allow at most 20% test points to be outside of the confidence interval
v0.11.1,Check that the intervals are not too wide
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.11.1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.11.1,ensure that we've got at least 6 of every element
v0.11.1,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.11.1,NOTE: this number may need to change if the default number of folds in
v0.11.1,WeightedStratifiedKFold changes
v0.11.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.11.1,ensure we can serialize the unfit estimator
v0.11.1,ensure we can pickle the fit estimator
v0.11.1,make sure we can call the marginal_effect and effect methods
v0.11.1,test const marginal inference
v0.11.1,test effect inference
v0.11.1,test marginal effect inference
v0.11.1,test coef__inference and intercept__inference
v0.11.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.1,"make sure we can call effect with implied scalar treatments,"
v0.11.1,"no matter the dimensions of T, and also that we warn when there"
v0.11.1,are multiple treatments
v0.11.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.1,ensure that we've got at least two of every element
v0.11.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.11.1,make sure we can call the marginal_effect and effect methods
v0.11.1,test const marginal inference
v0.11.1,test effect inference
v0.11.1,test marginal effect inference
v0.11.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.11.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.11.1,We concatenate the two copies data
v0.11.1,make sure we can get out post-fit stuff
v0.11.1,create a simple artificial setup where effect of moving from treatment
v0.11.1,"1 -> 2 is 2,"
v0.11.1,"1 -> 3 is 1, and"
v0.11.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.11.1,"Using an uneven number of examples from different classes,"
v0.11.1,"and having the treatments in non-lexicographic order,"
v0.11.1,Should rule out some basic issues.
v0.11.1,test that we can fit with a KFold instance
v0.11.1,test that we can fit with a train/test iterable
v0.11.1,predetermined splits ensure that all features are seen in each split
v0.11.1,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.11.1,(incorrectly) use a final model with an intercept
v0.11.1,"Because final model is fixed, actual values of T and Y don't matter"
v0.11.1,Ensure reproducibility
v0.11.1,Sparse DGP
v0.11.1,Treatment effect coef
v0.11.1,Other coefs
v0.11.1,Features and controls
v0.11.1,Test sparse estimator
v0.11.1,"--> test coef_, intercept_"
v0.11.1,--> test treatment effects
v0.11.1,Restrict x_test to vectors of norm < 1
v0.11.1,--> check inference
v0.11.1,Check that a majority of true effects lie in the 5-95% CI
v0.11.1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.11.1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.11.1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.11.1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.11.1,sparse test case: heterogeneous effect by product
v0.11.1,need at least as many rows in e_y as there are distinct columns
v0.11.1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.11.1,create a simple artificial setup where effect of moving from treatment
v0.11.1,"a -> b is 2,"
v0.11.1,"a -> c is 1, and"
v0.11.1,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.11.1,"Using an uneven number of examples from different classes,"
v0.11.1,"and having the treatments in non-lexicographic order,"
v0.11.1,should rule out some basic issues.
v0.11.1,Note that explicitly specifying the dtype as object is necessary until
v0.11.1,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.11.1,estimated effects should be identical when treatment is explicitly given
v0.11.1,but const_marginal_effect should be reordered based on the explicit cagetories
v0.11.1,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.11.1,test outer grouping
v0.11.1,test nested grouping
v0.11.1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.11.1,whichever groups we saw
v0.11.1,test nested grouping
v0.11.1,"by default, we use 5 split cross-validation for our T and Y models"
v0.11.1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.11.1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Set random seed
v0.11.1,Generate data
v0.11.1,DGP constants
v0.11.1,Test data
v0.11.1,Constant treatment effect and propensity
v0.11.1,Heterogeneous treatment and propensity
v0.11.1,ensure that we've got at least two of every element
v0.11.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.11.1,ensure that we can serialize unfit estimator
v0.11.1,ensure that we can serialize fit estimator
v0.11.1,make sure we can call the marginal_effect and effect methods
v0.11.1,test const marginal inference
v0.11.1,test effect inference
v0.11.1,test marginal effect inference
v0.11.1,test coef_ and intercept_ inference
v0.11.1,verify we can generate the summary
v0.11.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.11.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.11.1,create a simple artificial setup where effect of moving from treatment
v0.11.1,"1 -> 2 is 2,"
v0.11.1,"1 -> 3 is 1, and"
v0.11.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.11.1,"Using an uneven number of examples from different classes,"
v0.11.1,"and having the treatments in non-lexicographic order,"
v0.11.1,Should rule out some basic issues.
v0.11.1,test that we can fit with a KFold instance
v0.11.1,test that we can fit with a train/test iterable
v0.11.1,"for at least some of the examples, the CI should have nonzero width"
v0.11.1,"for at least some of the examples, the CI should have nonzero width"
v0.11.1,"for at least some of the examples, the CI should have nonzero width"
v0.11.1,test coef__inference function works
v0.11.1,test intercept__inference function works
v0.11.1,test summary function works
v0.11.1,Test inputs
v0.11.1,self._test_inputs(DR_learner)
v0.11.1,Test constant treatment effect
v0.11.1,Test heterogeneous treatment effect
v0.11.1,Test heterogenous treatment effect for W =/= None
v0.11.1,Sparse DGP
v0.11.1,Treatment effect coef
v0.11.1,Other coefs
v0.11.1,Features and controls
v0.11.1,Test sparse estimator
v0.11.1,"--> test coef_, intercept_"
v0.11.1,--> test treatment effects
v0.11.1,Restrict x_test to vectors of norm < 1
v0.11.1,--> check inference
v0.11.1,Check that a majority of true effects lie in the 5-95% CI
v0.11.1,test outer grouping
v0.11.1,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.11.1,test nested grouping
v0.11.1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.11.1,whichever groups we saw
v0.11.1,test nested grouping
v0.11.1,"by default, we use 5 split cross-validation for our T and Y models"
v0.11.1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.11.1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.11.1,helper class
v0.11.1,Fit learner and get the effect
v0.11.1,Get the true treatment effect
v0.11.1,Compute treatment effect residuals (absolute)
v0.11.1,Check that at least 90% of predictions are within tolerance interval
v0.11.1,Only for heterogeneous TE
v0.11.1,Fit learner on X and W and get the effect
v0.11.1,Get the true treatment effect
v0.11.1,Compute treatment effect residuals (absolute)
v0.11.1,Check that at least 90% of predictions are within tolerance interval
v0.11.1,Check that one can pass in regular lists
v0.11.1,Check that it fails correctly if lists of different shape are passed in
v0.11.1,Check that it fails when T contains values other than 0 and 1
v0.11.1,"Check that it works when T, Y have shape (n, 1)"
v0.11.1,Generate covariates
v0.11.1,Generate treatment
v0.11.1,Calculate outcome
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,DGP constants
v0.11.1,DGP coefficients
v0.11.1,Generated outcomes
v0.11.1,################
v0.11.1,WeightedLasso #
v0.11.1,################
v0.11.1,Define weights
v0.11.1,Define extended datasets
v0.11.1,Range of alphas
v0.11.1,Compare with Lasso
v0.11.1,--> No intercept
v0.11.1,--> With intercept
v0.11.1,When DGP has no intercept
v0.11.1,When DGP has intercept
v0.11.1,--> Coerce coefficients to be positive
v0.11.1,--> Toggle max_iter & tol
v0.11.1,Define weights
v0.11.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.11.1,Mixed DGP scenario.
v0.11.1,Define extended datasets
v0.11.1,Define weights
v0.11.1,Define multioutput
v0.11.1,##################
v0.11.1,WeightedLassoCV #
v0.11.1,##################
v0.11.1,Define alphas to test
v0.11.1,Compare with LassoCV
v0.11.1,--> No intercept
v0.11.1,--> With intercept
v0.11.1,--> Force parameters to be positive
v0.11.1,Choose a smaller n to speed-up process
v0.11.1,Compare fold weights
v0.11.1,Define weights
v0.11.1,Define extended datasets
v0.11.1,Define splitters
v0.11.1,WeightedKFold splitter
v0.11.1,Map weighted splitter to an extended splitter
v0.11.1,Define alphas to test
v0.11.1,Compare with LassoCV
v0.11.1,--> No intercept
v0.11.1,--> With intercept
v0.11.1,--> Force parameters to be positive
v0.11.1,###########################
v0.11.1,MultiTaskWeightedLassoCV #
v0.11.1,###########################
v0.11.1,Define alphas to test
v0.11.1,Define splitter
v0.11.1,Compare with MultiTaskLassoCV
v0.11.1,--> No intercept
v0.11.1,--> With intercept
v0.11.1,Define weights
v0.11.1,Define extended datasets
v0.11.1,Define splitters
v0.11.1,WeightedKFold splitter
v0.11.1,Map weighted splitter to an extended splitter
v0.11.1,Define alphas to test
v0.11.1,Compare with LassoCV
v0.11.1,--> No intercept
v0.11.1,--> With intercept
v0.11.1,#########################
v0.11.1,WeightedLassoCVWrapper #
v0.11.1,#########################
v0.11.1,perform 1D fit
v0.11.1,perform 2D fit
v0.11.1,################
v0.11.1,DebiasedLasso #
v0.11.1,################
v0.11.1,Test DebiasedLasso without weights
v0.11.1,--> Check debiased coeffcients without intercept
v0.11.1,--> Check debiased coeffcients with intercept
v0.11.1,--> Check 5-95 CI coverage for unit vectors
v0.11.1,Test DebiasedLasso with weights for one DGP
v0.11.1,Define weights
v0.11.1,Define extended datasets
v0.11.1,--> Check debiased coefficients
v0.11.1,Define weights
v0.11.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.11.1,--> Check debiased coeffcients
v0.11.1,Test that attributes propagate correctly
v0.11.1,Test MultiOutputDebiasedLasso without weights
v0.11.1,--> Check debiased coeffcients without intercept
v0.11.1,--> Check debiased coeffcients with intercept
v0.11.1,--> Check CI coverage
v0.11.1,Test MultiOutputDebiasedLasso with weights
v0.11.1,Define weights
v0.11.1,Define extended datasets
v0.11.1,--> Check debiased coefficients
v0.11.1,Unit vectors
v0.11.1,Unit vectors
v0.11.1,Check coeffcients and intercept are the same within tolerance
v0.11.1,Check results are similar with tolerance 1e-6
v0.11.1,Check if multitask
v0.11.1,Check that same alpha is chosen
v0.11.1,Check that the coefficients are similar
v0.11.1,selective ridge has a simple implementation that we can test against
v0.11.1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.11.1,"it should be the case that when we set fit_intercept to true,"
v0.11.1,it doesn't matter whether the penalized model also fits an intercept or not
v0.11.1,create an extra copy of rows with weight 2
v0.11.1,"instead of a slice, explicitly return an array of indices"
v0.11.1,_penalized_inds is only set during fitting
v0.11.1,cv exists on penalized model
v0.11.1,now we can access _penalized_inds
v0.11.1,check that we can read the cv attribute back out from the underlying model
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"global and cohort data should have exactly the same structure, but different values"
v0.11.1,local index should have as many times entries as global as there were rows passed in
v0.11.1,Can't handle multi-dimensional treatments
v0.11.1,"global shape is (d_y, sum(d_t))"
v0.11.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.1,features; for categoricals they should appear #cats-1 times each
v0.11.1,"global and cohort data should have exactly the same structure, but different values"
v0.11.1,local index should have as many times entries as global as there were rows passed in
v0.11.1,features; for categoricals they should appear #cats-1 times each
v0.11.1,"global shape is (d_y, sum(d_t))"
v0.11.1,Can't handle multi-dimensional treatments
v0.11.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.1,"global and cohort data should have exactly the same structure, but different values"
v0.11.1,local index should have as many times entries as global as there were rows passed in
v0.11.1,Can't handle multi-dimensional treatments
v0.11.1,"global shape is (d_y, sum(d_t))"
v0.11.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.1,features; for categoricals they should appear #cats-1 times each
v0.11.1,make sure we don't run into problems dropping every index
v0.11.1,"global and cohort data should have exactly the same structure, but different values"
v0.11.1,local index should have as many times entries as global as there were rows passed in
v0.11.1,"global shape is (d_y, sum(d_t))"
v0.11.1,Can't handle multi-dimensional treatments
v0.11.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.1,"global and cohort data should have exactly the same structure, but different values"
v0.11.1,local index should have as many times entries as global as there were rows passed in
v0.11.1,features; for categoricals they should appear #cats-1 times each
v0.11.1,"global shape is (d_y, sum(d_t))"
v0.11.1,Can't handle multi-dimensional treatments
v0.11.1,dgp
v0.11.1,model
v0.11.1,model
v0.11.1,"columns 'd', 'e', 'h' have too many values"
v0.11.1,"columns 'd', 'e' have too many values"
v0.11.1,lowering bound shouldn't affect already fit columns when warm starting
v0.11.1,"column d is now okay, too"
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,DGP constants
v0.11.1,Define data features
v0.11.1,Added `_df`to names to be different from the default cate_estimator names
v0.11.1,Generate data
v0.11.1,################################
v0.11.1,Single treatment and outcome #
v0.11.1,################################
v0.11.1,Test LinearDML
v0.11.1,|--> Test featurizers
v0.11.1,ColumnTransformer doesn't propagate column names
v0.11.1,|--> Test re-fit
v0.11.1,Test SparseLinearDML
v0.11.1,Test ForestDML
v0.11.1,###################################
v0.11.1,Mutiple treatments and outcomes #
v0.11.1,###################################
v0.11.1,Test LinearDML
v0.11.1,Test SparseLinearDML
v0.11.1,"Single outcome only, ORF does not support multiple outcomes"
v0.11.1,Test DMLOrthoForest
v0.11.1,Test DROrthoForest
v0.11.1,Test XLearner
v0.11.1,Skipping population summary names test because bootstrap inference is too slow
v0.11.1,Test SLearner
v0.11.1,Test TLearner
v0.11.1,Test LinearDRLearner
v0.11.1,Test SparseLinearDRLearner
v0.11.1,Test ForestDRLearner
v0.11.1,Test LinearIntentToTreatDRIV
v0.11.1,Test DeepIV
v0.11.1,Test categorical treatments
v0.11.1,Check refit
v0.11.1,Check refit after setting categories
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Linear models are required for parametric dml
v0.11.1,sample weighting models are required for nonparametric dml
v0.11.1,Test values
v0.11.1,TLearner test
v0.11.1,Instantiate TLearner
v0.11.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.11.1,Test constant treatment effect with multi output Y
v0.11.1,Test heterogeneous treatment effect
v0.11.1,Need interactions between T and features
v0.11.1,Test heterogeneous treatment effect with multi output Y
v0.11.1,Instantiate DomainAdaptationLearner
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,test base values equals to mean of constant marginal effect
v0.11.1,test shape of shap values output is as expected
v0.11.1,test shape of attribute of explanation object is as expected
v0.11.1,test length of feature names equals to shap values shape
v0.11.1,test base values equals to mean of constant marginal effect
v0.11.1,test shape of shap values output is as expected
v0.11.1,test shape of attribute of explanation object is as expected
v0.11.1,test length of feature names equals to shap values shape
v0.11.1,Treatment effect function
v0.11.1,Outcome support
v0.11.1,Treatment support
v0.11.1,"Generate controls, covariates, treatments and outcomes"
v0.11.1,Heterogeneous treatment effects
v0.11.1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.11.1,through shap package.
v0.11.1,test shap could generate the plot from the shap_values
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Check inputs
v0.11.1,Check inputs
v0.11.1,Check inputs
v0.11.1,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.11.1,"Thus, we append the controls column before the one-hot-encoded T"
v0.11.1,"We might want to revisit, though, since it's linearly determined by the others"
v0.11.1,Check inputs
v0.11.1,Check inputs
v0.11.1,Estimate response function
v0.11.1,Check inputs
v0.11.1,Train model on controls. Assign higher weight to units resembling
v0.11.1,treated units.
v0.11.1,Train model on the treated. Assign higher weight to units resembling
v0.11.1,control units.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.11.1,output is
v0.11.1,"* a column of ones if X, W, and Z are all None"
v0.11.1,* just X or W or Z if both of the others are None
v0.11.1,* hstack([arrs]) for whatever subset are not None otherwise
v0.11.1,ensure Z is 2D
v0.11.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.11.1,We need to go back to the label representation of the one-hot so as to call
v0.11.1,the classifier.
v0.11.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.11.1,We need to go back to the label representation of the one-hot so as to call
v0.11.1,the classifier.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,TODO: make sure to use random seeds wherever necessary
v0.11.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.11.1,"unfortunately with the Theano and Tensorflow backends,"
v0.11.1,the straightforward use of K.stop_gradient can cause an error
v0.11.1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.11.1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.11.1,so that those layers remain connected but with 0 gradient
v0.11.1,|| t - mu_i || ^2
v0.11.1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.11.1,Use logsumexp for numeric stability:
v0.11.1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.11.1,TODO: does the numeric stability actually make any difference?
v0.11.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.11.1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.11.1,generate cumulative sum via matrix multiplication
v0.11.1,"Generate standard uniform values in shape (batch_size,1)"
v0.11.1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.11.1,we use uniform_like instead with an input of an appropriate shape)
v0.11.1,convert to floats and multiply to perform equivalent of logical AND
v0.11.1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.11.1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.11.1,we use normal_like instead with an input of an appropriate shape)
v0.11.1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.11.1,prevent gradient from passing through sampling
v0.11.1,three options: biased or upper-bound loss require a single number of samples;
v0.11.1,unbiased can take different numbers for the network and its gradient
v0.11.1,"sample: (() -> Layer, int) -> Layer"
v0.11.1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.11.1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.11.1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.11.1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.11.1,the dimensionality of the output of the network
v0.11.1,TODO: is there a more robust way to do this?
v0.11.1,TODO: do we need to give the user more control over other arguments to fit?
v0.11.1,"subtle point: we need to build a new model each time,"
v0.11.1,because each model encapsulates its randomness
v0.11.1,TODO: do we need to give the user more control over other arguments to fit?
v0.11.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.11.1,not a general tensor (because of how backprop works in every framework)
v0.11.1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.11.1,but this seems annoying...)
v0.11.1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.11.1,TODO: any way to get this to work on batches of arbitrary size?
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Estimate final model of theta(X) by minimizing the square loss:
v0.11.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.11.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.11.1,at the expense of some small bias. For points with very small covariance we revert
v0.11.1,to the model-based preliminary estimate and do not add the correction term.
v0.11.1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.11.1,"instruments, and outcomes"
v0.11.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.11.1,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.11.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.11.1,for internal use by the library
v0.11.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.11.1,"we need to undo the one-hot encoding for calling effect,"
v0.11.1,since it expects raw values
v0.11.1,"we need to undo the one-hot encoding for calling effect,"
v0.11.1,since it expects raw values
v0.11.1,"TODO: check that Y, T, Z do not have multiple columns"
v0.11.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.11.1,TODO: do correct adjustment for sample_var
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.11.1,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.11.1,since we would actually be calculating a CATE rather than ATE.
v0.11.1,TODO: allow the final model to actually use X?
v0.11.1,TODO: allow the final model to actually use X?
v0.11.1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.11.1,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.11.1,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.11.1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.11.1,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.11.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.11.1,for internal use by the library
v0.11.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,"this will have dimension (d,) + shape(X)"
v0.11.1,send the first dimension to the end
v0.11.1,columns are featurized independently; partial derivatives are only non-zero
v0.11.1,when taken with respect to the same column each time
v0.11.1,don't fit intercept; manually add column of ones to the data instead;
v0.11.1,this allows us to ignore the intercept when computing marginal effects
v0.11.1,make T 2D if if was a vector
v0.11.1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.11.1,two stage approximation
v0.11.1,"first, get basis expansions of T, X, and Z"
v0.11.1,TODO: is it right that the effective number of intruments is the
v0.11.1,"product of ft_X and ft_Z, not just ft_Z?"
v0.11.1,"regress T expansion on X,Z expansions concatenated with W"
v0.11.1,"predict ft_T from interacted ft_X, ft_Z"
v0.11.1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.11.1,dT may be only 2-dimensional)
v0.11.1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.11.1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,TODO: this utility is documented but internal; reimplement?
v0.11.1,TODO: this utility is even less public...
v0.11.1,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.11.1,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.11.1,but also supports get_feature_names with expected signature
v0.11.1,Convert python objects to (possibly nested) types that can easily be represented as literals
v0.11.1,named tuple type for storing results inside CausalAnalysis class;
v0.11.1,must be lifted to module level to enable pickling
v0.11.1,Validate inputs
v0.11.1,TODO: check compatibility of X and Y lengths
v0.11.1,"no previous fit, cancel warm start"
v0.11.1,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.11.1,"if heterogeneity_inds is 1D, repeat it"
v0.11.1,heterogeneity inds should be a 2D list of length same as train_inds
v0.11.1,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.11.1,"TODO: bail out also if categorical columns, classification changed?"
v0.11.1,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.11.1,train the Y model
v0.11.1,"perform model selection for the Y model using all X, not on a per-column basis"
v0.11.1,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.11.1,work with the regression wrapper
v0.11.1,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.11.1,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.11.1,since otherwise we'll have too many columns to be able to train a classifier
v0.11.1,start with empty results and default shared insights
v0.11.1,convert categorical indicators to numeric indices
v0.11.1,check for indices over the categorical expansion bound
v0.11.1,"can't remove in place while iterating over new_inds, so store in separate list"
v0.11.1,also remove from train_inds so we don't try to access the result later
v0.11.1,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.11.1,Controls are all other columns of X
v0.11.1,"can't use X[:, feat_ind] when X is a DataFrame"
v0.11.1,array checking routines don't accept 0-width arrays
v0.11.1,perform model selection
v0.11.1,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.11.1,convert to NormalInferenceResults for consistency
v0.11.1,Set the dictionary values shared between local and global summaries
v0.11.1,extract subset of names matching new columns
v0.11.1,properties to return from effect InferenceResults
v0.11.1,properties to return from PopulationSummaryResults
v0.11.1,Converts strings to property lookups or method calls as a convenience so that the
v0.11.1,_point_props and _summary_props above can be applied to an inference object
v0.11.1,Create a summary combining all results into a single output; this is used
v0.11.1,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.11.1,"or a dictionary, respectively, based on the summary function passed into this method"
v0.11.1,"ensure array has shape (m,y,t)"
v0.11.1,population summary is missing sample dimension; add it for consistency
v0.11.1,outcome dimension is missing; add it for consistency
v0.11.1,add singleton treatment dimension if missing
v0.11.1,"each attr has dimension (m,y) or (m,y,t)"
v0.11.1,concatenate along treatment dimension
v0.11.1,"for dictionary representation, want to remove unneeded sample dimension"
v0.11.1,in cohort and global results
v0.11.1,TODO: enrich outcome logic for multi-class classification when that is supported
v0.11.1,can't drop only level
v0.11.1,should be serialization-ready and contain no numpy arrays
v0.11.1,a global inference indicates the effect of that one feature on the outcome
v0.11.1,need to reshape the output to match the input
v0.11.1,we want to offset the inference object by the baseline estimate of y
v0.11.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.11.1,"however, the tree can't store the feature and treatment names we compute here..."
v0.11.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.11.1,"however, the tree can't store the feature and treatment names we compute here..."
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,TODO: conisder working around relying on sklearn implementation details
v0.11.1,"Found a good split, return."
v0.11.1,Record all splits in case the stratification by weight yeilds a worse partition
v0.11.1,Reseed random generator and try again
v0.11.1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.11.1,"Found a good split, return."
v0.11.1,Did not find a good split
v0.11.1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.11.1,Return most weight-balanced partition
v0.11.1,Weight stratification algorithm
v0.11.1,Sort weights for weight strata search
v0.11.1,There are some leftover indices that have yet to be assigned
v0.11.1,Append stratum splits to overall splits
v0.11.1,"If classification methods produce multiple columns of output,"
v0.11.1,we need to manually encode classes to ensure consistent column ordering.
v0.11.1,We clone the estimator to make sure that all the folds are
v0.11.1,"independent, and that it is pickle-able."
v0.11.1,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.11.1,`predictions` is a list of method outputs from each fold.
v0.11.1,"If each of those is also a list, then treat this as a"
v0.11.1,multioutput-multiclass task. We need to separately concatenate
v0.11.1,the method outputs for each label into an `n_labels` long list.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Our classes that derive from sklearn ones sometimes include
v0.11.1,inherited docstrings that have embedded doctests; we need the following imports
v0.11.1,so that they don't break.
v0.11.1,TODO: consider working around relying on sklearn implementation details
v0.11.1,"Convert X, y into numpy arrays"
v0.11.1,Define fit parameters
v0.11.1,Some algorithms don't have a check_input option
v0.11.1,Check weights array
v0.11.1,Check that weights are size-compatible
v0.11.1,Normalize inputs
v0.11.1,Weight inputs
v0.11.1,Fit base class without intercept
v0.11.1,Fit Lasso
v0.11.1,Reset intercept
v0.11.1,The intercept is not calculated properly due the sqrt(weights) factor
v0.11.1,so it must be recomputed
v0.11.1,Fit lasso without weights
v0.11.1,Make weighted splitter
v0.11.1,Fit weighted model
v0.11.1,Make weighted splitter
v0.11.1,Fit weighted model
v0.11.1,Call weighted lasso on reduced design matrix
v0.11.1,Weighted tau
v0.11.1,Select optimal penalty
v0.11.1,Warn about consistency
v0.11.1,"Convert X, y into numpy arrays"
v0.11.1,Fit weighted lasso with user input
v0.11.1,"Center X, y"
v0.11.1,Calculate quantities that will be used later on. Account for centered data
v0.11.1,Calculate coefficient and error variance
v0.11.1,Add coefficient correction
v0.11.1,Set coefficients and intercept standard errors
v0.11.1,Set intercept
v0.11.1,Return alpha to 'auto' state
v0.11.1,"Note that in the case of no intercept, X_offset is 0"
v0.11.1,Calculate the variance of the predictions
v0.11.1,Calculate prediction confidence intervals
v0.11.1,Assumes flattened y
v0.11.1,Compute weighted residuals
v0.11.1,To be done once per target. Assumes y can be flattened.
v0.11.1,Assumes that X has already been offset
v0.11.1,Special case: n_features=1
v0.11.1,Compute Lasso coefficients for the columns of the design matrix
v0.11.1,Compute C_hat
v0.11.1,Compute theta_hat
v0.11.1,Allow for single output as well
v0.11.1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.11.1,Set coef_ attribute
v0.11.1,Set intercept_ attribute
v0.11.1,Set selected_alpha_ attribute
v0.11.1,Set coef_stderr_
v0.11.1,intercept_stderr_
v0.11.1,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.11.1,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.11.1,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.11.1,set intercept_ attribute
v0.11.1,set coef_ attribute
v0.11.1,set alpha_ attribute
v0.11.1,set alphas_ attribute
v0.11.1,set n_iter_ attribute
v0.11.1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.11.1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.11.1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.11.1,now regress X1 on y - X2 * beta2 to learn beta1
v0.11.1,set coef_ and intercept_ attributes
v0.11.1,Note that the penalized model should *not* have an intercept
v0.11.1,don't proxy special methods
v0.11.1,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.11.1,regressor incorrectly
v0.11.1,"Note: for known attributes that have been set this method will not be called,"
v0.11.1,so we should just throw here because this is an attribute belonging to this class
v0.11.1,but which hasn't yet been set on this instance
v0.11.1,set default values for None
v0.11.1,check freq_weight should be integer and should be accompanied by sample_var
v0.11.1,check array shape
v0.11.1,weight X and y and sample_var
v0.11.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,AzureML
v0.11.1,helper imports
v0.11.1,write the details of the workspace to a configuration file to the notebook library
v0.11.1,if y is a multioutput model
v0.11.1,Make sure second dimension has 1 or more item
v0.11.1,switch _inner Model to a MultiOutputRegressor
v0.11.1,flatten array as automl only takes vectors for y
v0.11.1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.11.1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.11.1,as an sklearn estimator
v0.11.1,fit implementation for a single output model.
v0.11.1,Create experiment for specified workspace
v0.11.1,Configure automl_config with training set information.
v0.11.1,"Wait for remote run to complete, the set the model"
v0.11.1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.11.1,create model and pass model into final.
v0.11.1,"If item is an automl config, get its corresponding"
v0.11.1,AutomatedML Model and add it to new_Args
v0.11.1,"If item is an automl config, get its corresponding"
v0.11.1,AutomatedML Model and set it for this key in
v0.11.1,kwargs
v0.11.1,takes in either automated_ml config and instantiates
v0.11.1,an AutomatedMLModel
v0.11.1,The prefix can only be 18 characters long
v0.11.1,"because prefixes come from kwarg_names, we must ensure they are"
v0.11.1,short enough.
v0.11.1,Get workspace from config file.
v0.11.1,Take the intersect of the white for sample
v0.11.1,weights and linear models
v0.11.1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,average the outcome dimension if it exists and ensure 2d y_pred
v0.11.1,get index of best treatment
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,TODO: consider working around relying on sklearn implementation details
v0.11.1,Create splits of causal tree
v0.11.1,Make sure the correct exception is being rethrown
v0.11.1,Must make sure indices are merged correctly
v0.11.1,Convert rows to columns
v0.11.1,Require group assignment t to be one-hot-encoded
v0.11.1,Get predictions for the 2 splits
v0.11.1,Must make sure indices are merged correctly
v0.11.1,Crossfitting
v0.11.1,Compute weighted nuisance estimates
v0.11.1,-------------------------------------------------------------------------------
v0.11.1,Calculate the covariance matrix corresponding to the BLB inference
v0.11.1,
v0.11.1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.11.1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.11.1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.11.1,in that slice from the overall parameter estimate.
v0.11.1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.11.1,-------------------------------------------------------------------------------
v0.11.1,Calclulate covariance matrix through BLB
v0.11.1,Estimators
v0.11.1,OrthoForest parameters
v0.11.1,Sub-forests
v0.11.1,Auxiliary attributes
v0.11.1,Fit check
v0.11.1,TODO: Check performance
v0.11.1,Must normalize weights
v0.11.1,Override the CATE inference options
v0.11.1,Add blb inference to parent's options
v0.11.1,Generate subsample indices
v0.11.1,Build trees in parallel
v0.11.1,Bootstraping has repetitions in tree sample
v0.11.1,Similar for `a` weights
v0.11.1,Bootstraping has repetitions in tree sample
v0.11.1,Define subsample size
v0.11.1,Safety check
v0.11.1,Draw points to create little bags
v0.11.1,Copy and/or define models
v0.11.1,Define nuisance estimators
v0.11.1,Define parameter estimators
v0.11.1,Define
v0.11.1,Need to redefine fit here for auto inference to work due to a quirk in how
v0.11.1,wrap_fit is defined
v0.11.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.11.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.11.1,Override to flatten output if T is flat
v0.11.1,Check that all discrete treatments are represented
v0.11.1,Nuissance estimates evaluated with cross-fitting
v0.11.1,Define 2-fold iterator
v0.11.1,Check if there is only one example of some class
v0.11.1,Define 2-fold iterator
v0.11.1,need safe=False when cloning for WeightedModelWrapper
v0.11.1,Compute residuals
v0.11.1,Compute coefficient by OLS on residuals
v0.11.1,"Parameter returned by LinearRegression is (d_T, )"
v0.11.1,Compute residuals
v0.11.1,Compute coefficient by OLS on residuals
v0.11.1,ell_2 regularization
v0.11.1,Ridge regression estimate
v0.11.1,"Parameter returned is of shape (d_T, )"
v0.11.1,Return moments and gradients
v0.11.1,Compute residuals
v0.11.1,Compute moments
v0.11.1,"Moments shape is (n, d_T)"
v0.11.1,Compute moment gradients
v0.11.1,returns shape-conforming residuals
v0.11.1,Copy and/or define models
v0.11.1,Define parameter estimators
v0.11.1,Define moment and mean gradient estimator
v0.11.1,"Check that T is shape (n, )"
v0.11.1,Check T is numeric
v0.11.1,Train label encoder
v0.11.1,Call `fit` from parent class
v0.11.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.11.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.11.1,Override to flatten output if T is flat
v0.11.1,Expand one-hot encoding to include the zero treatment
v0.11.1,"Test that T contains all treatments. If not, return None"
v0.11.1,Nuissance estimates evaluated with cross-fitting
v0.11.1,Define 2-fold iterator
v0.11.1,Check if there is only one example of some class
v0.11.1,No need to crossfit for internal nodes
v0.11.1,Compute partial moments
v0.11.1,"If any of the values in the parameter estimate is nan, return None"
v0.11.1,Compute partial moments
v0.11.1,Compute coefficient by OLS on residuals
v0.11.1,ell_2 regularization
v0.11.1,Ridge regression estimate
v0.11.1,"Parameter returned is of shape (d_T, )"
v0.11.1,Return moments and gradients
v0.11.1,Compute partial moments
v0.11.1,Compute moments
v0.11.1,"Moments shape is (n, d_T-1)"
v0.11.1,Compute moment gradients
v0.11.1,Need to calculate this in an elegant way for when propensity is 0
v0.11.1,This will flatten T
v0.11.1,Check that T is numeric
v0.11.1,Test whether the input estimator is supported
v0.11.1,Calculate confidence intervals for the parameter (marginal effect)
v0.11.1,Calculate confidence intervals for the effect
v0.11.1,Calculate the effects
v0.11.1,Calculate the standard deviations for the effects
v0.11.1,d_t=None here since we measure the effect across all Ts
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.1,Licensed under the MIT License.
v0.11.1,Causal tree parameters
v0.11.1,Tree structure
v0.11.1,No need for a random split since the data is already
v0.11.1,a random subsample from the original input
v0.11.1,node list stores the nodes that are yet to be splitted
v0.11.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.11.1,Create local sample set
v0.11.1,Compute nuisance estimates for the current node
v0.11.1,Nuisance estimate cannot be calculated
v0.11.1,Estimate parameter for current node
v0.11.1,Node estimate cannot be calculated
v0.11.1,Calculate moments and gradient of moments for current data
v0.11.1,Calculate inverse gradient
v0.11.1,The gradient matrix is not invertible.
v0.11.1,No good split can be found
v0.11.1,Calculate point-wise pseudo-outcomes rho
v0.11.1,a split is determined by a feature and a sample pair
v0.11.1,the number of possible splits is at most (number of features) * (number of node samples)
v0.11.1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.11.1,parse row and column of random pair
v0.11.1,the sample of the pair is the integer division of the random number with n_feats
v0.11.1,calculate the binary indicator of whether sample i is on the left or the right
v0.11.1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.11.1,calculate the number of samples on the left child for each proposed split
v0.11.1,calculate the analogous binary indicator for the samples in the estimation set
v0.11.1,calculate the number of estimation samples on the left child of each proposed split
v0.11.1,find the upper and lower bound on the size of the left split for the split
v0.11.1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.11.1,on each side.
v0.11.1,similarly for the estimation sample set
v0.11.1,if there is no valid split then don't create any children
v0.11.1,filter only the valid splits
v0.11.1,calculate the average influence vector of the samples in the left child
v0.11.1,calculate the average influence vector of the samples in the right child
v0.11.1,take the square of each of the entries of the influence vectors and normalize
v0.11.1,by size of each child
v0.11.1,calculate the vector score of each candidate split as the average of left and right
v0.11.1,influence vectors
v0.11.1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.11.1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.11.1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.11.1,calculate the scalar score of each split by aggregating across the vector of scores
v0.11.1,Find split that minimizes criterion
v0.11.1,Create child nodes with corresponding subsamples
v0.11.1,add the created children to the list of not yet split nodes
v0.11.0,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.11.0,configuration is all pulled from setup.cfg
v0.11.0,-*- coding: utf-8 -*-
v0.11.0,
v0.11.0,Configuration file for the Sphinx documentation builder.
v0.11.0,
v0.11.0,This file does only contain a selection of the most common options. For a
v0.11.0,full list see the documentation:
v0.11.0,http://www.sphinx-doc.org/en/master/config
v0.11.0,-- Path setup --------------------------------------------------------------
v0.11.0,"If extensions (or modules to document with autodoc) are in another directory,"
v0.11.0,add these directories to sys.path here. If the directory is relative to the
v0.11.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.11.0,
v0.11.0,-- Project information -----------------------------------------------------
v0.11.0,-- General configuration ---------------------------------------------------
v0.11.0,"If your documentation needs a minimal Sphinx version, state it here."
v0.11.0,
v0.11.0,needs_sphinx = '1.0'
v0.11.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.11.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.11.0,ones.
v0.11.0,"Add any paths that contain templates here, relative to this directory."
v0.11.0,The suffix(es) of source filenames.
v0.11.0,You can specify multiple suffix as a list of string:
v0.11.0,
v0.11.0,"source_suffix = ['.rst', '.md']"
v0.11.0,The master toctree document.
v0.11.0,The language for content autogenerated by Sphinx. Refer to documentation
v0.11.0,for a list of supported languages.
v0.11.0,
v0.11.0,This is also used if you do content translation via gettext catalogs.
v0.11.0,"Usually you set ""language"" from the command line for these cases."
v0.11.0,"List of patterns, relative to source directory, that match files and"
v0.11.0,directories to ignore when looking for source files.
v0.11.0,This pattern also affects html_static_path and html_extra_path.
v0.11.0,The name of the Pygments (syntax highlighting) style to use.
v0.11.0,-- Options for HTML output -------------------------------------------------
v0.11.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.11.0,a list of builtin themes.
v0.11.0,
v0.11.0,Theme options are theme-specific and customize the look and feel of a theme
v0.11.0,"further.  For a list of options available for each theme, see the"
v0.11.0,documentation.
v0.11.0,
v0.11.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.11.0,"relative to this directory. They are copied after the builtin static files,"
v0.11.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.11.0,html_static_path = ['_static']
v0.11.0,"Custom sidebar templates, must be a dictionary that maps document names"
v0.11.0,to template names.
v0.11.0,
v0.11.0,The default sidebars (for documents that don't match any pattern) are
v0.11.0,defined by theme itself.  Builtin themes are using these templates by
v0.11.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.11.0,'searchbox.html']``.
v0.11.0,
v0.11.0,html_sidebars = {}
v0.11.0,-- Options for HTMLHelp output ---------------------------------------------
v0.11.0,Output file base name for HTML help builder.
v0.11.0,-- Options for LaTeX output ------------------------------------------------
v0.11.0,The paper size ('letterpaper' or 'a4paper').
v0.11.0,
v0.11.0,"'papersize': 'letterpaper',"
v0.11.0,"The font size ('10pt', '11pt' or '12pt')."
v0.11.0,
v0.11.0,"'pointsize': '10pt',"
v0.11.0,Additional stuff for the LaTeX preamble.
v0.11.0,
v0.11.0,"'preamble': '',"
v0.11.0,Latex figure (float) alignment
v0.11.0,
v0.11.0,"'figure_align': 'htbp',"
v0.11.0,Grouping the document tree into LaTeX files. List of tuples
v0.11.0,"(source start file, target name, title,"
v0.11.0,"author, documentclass [howto, manual, or own class])."
v0.11.0,-- Options for manual page output ------------------------------------------
v0.11.0,One entry per manual page. List of tuples
v0.11.0,"(source start file, name, description, authors, manual section)."
v0.11.0,-- Options for Texinfo output ----------------------------------------------
v0.11.0,Grouping the document tree into Texinfo files. List of tuples
v0.11.0,"(source start file, target name, title, author,"
v0.11.0,"dir menu entry, description, category)"
v0.11.0,-- Options for Epub output -------------------------------------------------
v0.11.0,Bibliographic Dublin Core info.
v0.11.0,The unique identifier of the text. This can be a ISBN number
v0.11.0,or the project homepage.
v0.11.0,
v0.11.0,epub_identifier = ''
v0.11.0,A unique identification for the text.
v0.11.0,
v0.11.0,epub_uid = ''
v0.11.0,A list of files that should not be packed into the epub file.
v0.11.0,-- Extension configuration -------------------------------------------------
v0.11.0,-- Options for intersphinx extension ---------------------------------------
v0.11.0,Example configuration for intersphinx: refer to the Python standard library.
v0.11.0,-- Options for todo extension ----------------------------------------------
v0.11.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.11.0,-- Options for doctest extension -------------------------------------------
v0.11.0,we can document otherwise excluded entities here by returning False
v0.11.0,or skip otherwise included entities by returning True
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Calculate residuals
v0.11.0,Estimate E[T_res | Z_res]
v0.11.0,TODO. Deal with multi-class instrument
v0.11.0,Calculate nuisances
v0.11.0,Estimate E[T_res | Z_res]
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"We do a three way split, as typically a preliminary theta estimator would require"
v0.11.0,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.11.0,TODO. Deal with multi-class instrument
v0.11.0,Estimate final model of theta(X) by minimizing the square loss:
v0.11.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.11.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.11.0,at the expense of some small bias. For points with very small covariance we revert
v0.11.0,to the model-based preliminary estimate and do not add the correction term.
v0.11.0,Estimate preliminary theta in cross fitting manner
v0.11.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.11.0,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.11.0,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.11.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.11.0,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.11.0,TODO. The solution below is not really a valid cross-fitting
v0.11.0,as the test data are used to create the proj_t on the train
v0.11.0,which in the second train-test loop is used to create the nuisance
v0.11.0,cov on the test data. Hence the T variable of some sample
v0.11.0,"is implicitly correlated with its cov nuisance, through this flow"
v0.11.0,"of information. However, this seems a rather weak correlation."
v0.11.0,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.11.0,model.
v0.11.0,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.11.0,Estimate preliminary theta in cross fitting manner
v0.11.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.11.0,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.11.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.11.0,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.11.0,#############################################################################
v0.11.0,Classes for the DRIV implementation for the special case of intent-to-treat
v0.11.0,A/B test
v0.11.0,#############################################################################
v0.11.0,Estimate preliminary theta in cross fitting manner
v0.11.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.11.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.11.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.11.0,We can use statsmodel for all hypothesis testing capabilities
v0.11.0,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.11.0,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.11.0,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.11.0,model_T_XZ = lambda: model_clf()
v0.11.0,#'days_visited': lambda:
v0.11.0,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.11.0,Turn strings into categories for numeric mapping
v0.11.0,### Defining some generic regressors and classifiers
v0.11.0,This a generic non-parametric regressor
v0.11.0,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.11.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.11.0,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.11.0,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.11.0,model = lambda: RandomForestRegressor(n_estimators=100)
v0.11.0,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.11.0,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.11.0,model = lambda: LinearRegression(n_jobs=-1)
v0.11.0,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.11.0,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.11.0,underlying model whenever predict is called.
v0.11.0,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.11.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.11.0,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.11.0,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.11.0,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.11.0,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.11.0,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.11.0,We need to specify models to be used for each of these residualizations
v0.11.0,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.11.0,"E[T | X, Z]"
v0.11.0,E[TZ | X]
v0.11.0,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.11.0,n_splits determines the number of splits to be used for cross-fitting.
v0.11.0,# Algorithm 2 - Current Method
v0.11.0,In[121]:
v0.11.0,# Algorithm 3 - DRIV ATE
v0.11.0,dmliv_model_effect = lambda: model()
v0.11.0,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.11.0,"dmliv_model_effect(),"
v0.11.0,n_splits=1)
v0.11.0,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.11.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.11.0,"Once multiple treatments are supported, we'll need to fix this"
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.11.0,We can use statsmodel for all hypothesis testing capabilities
v0.11.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.11.0,We can use statsmodel for all hypothesis testing capabilities
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,TODO. Deal with multi-class instrument/treatment
v0.11.0,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.11.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.11.0,Estimate p(X) = E[T | X] in cross-fitting manner
v0.11.0,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.11.0,##################
v0.11.0,Global settings #
v0.11.0,##################
v0.11.0,Global plotting controls
v0.11.0,"Control for support size, can control for more"
v0.11.0,#################
v0.11.0,File utilities #
v0.11.0,#################
v0.11.0,#################
v0.11.0,Plotting utils #
v0.11.0,#################
v0.11.0,bias
v0.11.0,var
v0.11.0,rmse
v0.11.0,r2
v0.11.0,Infer feature dimension
v0.11.0,Metrics by support plots
v0.11.0,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.11.0,Vasilis Syrgkanis <vasy@microsoft.com>
v0.11.0,Steven Wu <zhiww@microsoft.com>
v0.11.0,Initialize causal tree parameters
v0.11.0,Create splits of causal tree
v0.11.0,Estimate treatment effects at the leafs
v0.11.0,Compute heterogeneous treatement effect for x's in x_list by finding
v0.11.0,the corresponding split and associating the effect computed on that leaf
v0.11.0,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.11.0,Safety check
v0.11.0,Weighted linear regression
v0.11.0,Calculates weights
v0.11.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.11.0,over all indices
v0.11.0,Similar for `a` weights
v0.11.0,Doesn't have sample weights
v0.11.0,Is a linear model
v0.11.0,Weighted linear regression
v0.11.0,Calculates weights
v0.11.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.11.0,over all indices
v0.11.0,Similar for `a` weights
v0.11.0,normalize weights
v0.11.0,"Split the data in half, train and test"
v0.11.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.11.0,"a function of W, using only the train fold"
v0.11.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.11.0,"Split the data in half, train and test"
v0.11.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.11.0,"a function of W, using only the train fold"
v0.11.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.11.0,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.11.0,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.11.0,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.11.0,"Split the data in half, train and test"
v0.11.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.11.0,"a function of x, using only the train fold"
v0.11.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.11.0,Compute coefficient by OLS on residuals
v0.11.0,"Split the data in half, train and test"
v0.11.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.11.0,"a function of x, using only the train fold"
v0.11.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.11.0,Estimate multipliers for second order orthogonal method
v0.11.0,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.11.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.11.0,Create local sample set
v0.11.0,compute the base estimate for the current node using double ml or second order double ml
v0.11.0,compute the influence functions here that are used for the criterion
v0.11.0,generate random proposals of dimensions to split
v0.11.0,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.11.0,compute criterion for each proposal
v0.11.0,if splitting creates valid leafs in terms of mean leaf size
v0.11.0,Calculate criterion for split
v0.11.0,Else set criterion to infinity so that this split is not chosen
v0.11.0,If no good split was found
v0.11.0,Find split that minimizes criterion
v0.11.0,Set the split attributes at the node
v0.11.0,Create child nodes with corresponding subsamples
v0.11.0,Recursively split children
v0.11.0,Return parent node
v0.11.0,estimate the local parameter at the leaf using the estimate data
v0.11.0,###################
v0.11.0,Argument parsing #
v0.11.0,###################
v0.11.0,#########################################
v0.11.0,Parameters constant across experiments #
v0.11.0,#########################################
v0.11.0,Outcome support
v0.11.0,Treatment support
v0.11.0,Evaluation grid
v0.11.0,Treatment effects array
v0.11.0,Other variables
v0.11.0,##########################
v0.11.0,Data Generating Process #
v0.11.0,##########################
v0.11.0,Log iteration
v0.11.0,"Generate controls, features, treatment and outcome"
v0.11.0,T and Y residuals to be used in later scripts
v0.11.0,Save generated dataset
v0.11.0,#################
v0.11.0,ORF parameters #
v0.11.0,#################
v0.11.0,######################################
v0.11.0,Train and evaluate treatment effect #
v0.11.0,######################################
v0.11.0,########
v0.11.0,Plots #
v0.11.0,########
v0.11.0,###############
v0.11.0,Save results #
v0.11.0,###############
v0.11.0,##############
v0.11.0,Run Rscript #
v0.11.0,##############
v0.11.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.11.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.11.0,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.11.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.11.0,def mlasso_model(): return MultiTaskLassoCV(
v0.11.0,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.11.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.11.0,heterogeneity
v0.11.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.11.0,heterogeneity
v0.11.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.11.0,heterogeneity
v0.11.0,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.11.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.11.0,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.11.0,subset of features that are exogenous and create heterogeneity
v0.11.0,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.11.0,subset of features wrt we estimate heterogeneity
v0.11.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.11.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,introspect the constructor arguments to find the model parameters
v0.11.0,to represent
v0.11.0,"if the argument is deprecated, ignore it"
v0.11.0,Extract and sort argument names excluding 'self'
v0.11.0,column names
v0.11.0,transfer input to numpy arrays
v0.11.0,transfer input to 2d arrays
v0.11.0,create dataframe
v0.11.0,currently dowhy only support single outcome and single treatment
v0.11.0,call dowhy
v0.11.0,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.11.0,cate estimator but not the effect.
v0.11.0,don't proxy special methods
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Check if model is sparse enough for this model
v0.11.0,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.11.0,TODO: any way to avoid creating a copy if the array was already dense?
v0.11.0,"the call is necessary if the input was something like a list, though"
v0.11.0,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.11.0,so convert to pydata sparse first
v0.11.0,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.11.0,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.11.0,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.11.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.11.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.11.0,For when checking input values is disabled
v0.11.0,Type to column extraction function
v0.11.0,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.11.0,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.11.0,Get feature names using featurizer
v0.11.0,All attempts at retrieving transformed feature names have failed
v0.11.0,Delegate handling to downstream logic
v0.11.0,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.11.0,same number of input definitions as arrays
v0.11.0,input definitions have same number of dimensions as each array
v0.11.0,all result indices are unique
v0.11.0,all result indices must match at least one input index
v0.11.0,"map indices to all array, axis pairs for that index"
v0.11.0,each index has the same cardinality wherever it appears
v0.11.0,"State: list of (set of letters, list of (corresponding indices, value))"
v0.11.0,Algo: while list contains more than one entry
v0.11.0,take two entries
v0.11.0,sort both lists by intersection of their indices
v0.11.0,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.11.0,"take the union of indices and the product of values), stepping through each list linearly"
v0.11.0,TODO: might be faster to break into connected components first
v0.11.0,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.11.0,"so compute their content separately, then take cartesian product"
v0.11.0,this would save a few pointless sorts by empty tuples
v0.11.0,TODO: Consider investigating other performance ideas for these cases
v0.11.0,where the dense method beat the sparse method (usually sparse is faster)
v0.11.0,"e,facd,c->cfed"
v0.11.0,sparse: 0.0335489
v0.11.0,dense:  0.011465999999999997
v0.11.0,"gbd,da,egb->da"
v0.11.0,sparse: 0.0791625
v0.11.0,dense:  0.007319099999999995
v0.11.0,"dcc,d,faedb,c->abe"
v0.11.0,sparse: 1.2868097
v0.11.0,dense:  0.44605229999999985
v0.11.0,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.11.0,TODO: would using einsum's paths to optimize the order of merging help?
v0.11.0,assume that we should perform nested cross-validation if and only if
v0.11.0,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.11.0,logic copied from check_cv
v0.11.0,otherwise we will assume the user already set the cv attribute to something
v0.11.0,compatible with splitting with a 'groups' argument
v0.11.0,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.11.0,don't use the groups when calling split internally
v0.11.0,Normalize weights
v0.11.0,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.11.0,"if we're decorating a class, just update the __init__ method,"
v0.11.0,so that the result is still a class instead of a wrapper method
v0.11.0,"want to enforce that each bad_arg was either in kwargs,"
v0.11.0,or else it was in neither and is just taking its default value
v0.11.0,Any access should throw
v0.11.0,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.11.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.11.0,input feature name is already updated by cate_feature_names.
v0.11.0,define the index of d_x to filter for each given T
v0.11.0,filter X after broadcast with T for each given T
v0.11.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.11.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,
v0.11.0,This code contains some snippets of code from:
v0.11.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
v0.11.0,published under the following license and copyright:
v0.11.0,BSD 3-Clause License
v0.11.0,
v0.11.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.0,All rights reserved.
v0.11.0,make any access to matplotlib or plt throw an exception
v0.11.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.11.0,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.11.0,Initialize saturation & value; calculate chroma & value shift
v0.11.0,Calculate some intermediate values
v0.11.0,Initialize RGB with same hue & chroma as our color
v0.11.0,Shift the initial RGB values to match value and store
v0.11.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.11.0,clean way of achieving this
v0.11.0,make sure we don't accidentally escape anything in the substitution
v0.11.0,Fetch appropriate color for node
v0.11.0,"red for negative, green for positive"
v0.11.0,in multi-target use mean of targets
v0.11.0,Write node mean CATE
v0.11.0,Write node std of CATE
v0.11.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.11.0,Fetch appropriate color for node
v0.11.0,Write node mean CATE
v0.11.0,Write node mean CATE
v0.11.0,Write recommended treatment and value - cost
v0.11.0,Licensed under the MIT License.
v0.11.0,"since inference objects can be stateful, we must copy it before fitting;"
v0.11.0,otherwise this sequence wouldn't work:
v0.11.0,"est1.fit(..., inference=inf)"
v0.11.0,"est2.fit(..., inference=inf)"
v0.11.0,est1.effect_interval(...)
v0.11.0,because inf now stores state from fitting est2
v0.11.0,This flag is true when names are set in a child class instead
v0.11.0,"If names are set in a child class, add an attribute reflecting that"
v0.11.0,This works only if X is passed as a kwarg
v0.11.0,We plan to enforce X as kwarg only in future releases
v0.11.0,This checks if names have been set in a child class
v0.11.0,"If names were set in a child class, don't do it again"
v0.11.0,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.11.0,call the wrapped fit method
v0.11.0,NOTE: we call inference fit *after* calling the main fit method
v0.11.0,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.11.0,but tensordot can't be applied to this problem because we don't sum over m
v0.11.0,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.11.0,of rows of T was not taken into account
v0.11.0,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.11.0,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.11.0,"Treatment names is None, default to BaseCateEstimator"
v0.11.0,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.11.0,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.11.0,Get input names
v0.11.0,Summary
v0.11.0,add statsmodels to parent's options
v0.11.0,add debiasedlasso to parent's options
v0.11.0,add blb to parent's options
v0.11.0,TODO Share some logic with non-discrete version
v0.11.0,Get input names
v0.11.0,Summary
v0.11.0,add statsmodels to parent's options
v0.11.0,add statsmodels to parent's options
v0.11.0,add blb to parent's options
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,remove None arguments
v0.11.0,"scores entries should be lists of scores, so make each entry a singleton list"
v0.11.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.11.0,generate an instance of the final model
v0.11.0,generate an instance of the nuisance model
v0.11.0,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.11.0,alteration even when we only want to fit_final.
v0.11.0,use a binary array to get stratified split in case of discrete treatment
v0.11.0,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.11.0,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.11.0,"however, sklearn doesn't support both stratifying and grouping (see"
v0.11.0,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.11.0,their own object that supports grouping if they want to use groups.
v0.11.0,for each mc iteration
v0.11.0,for each model under cross fit setting
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,
v0.11.0,This code contains snippets of code from
v0.11.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.11.0,published under the following license and copyright:
v0.11.0,BSD 3-Clause License
v0.11.0,
v0.11.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.0,All rights reserved.
v0.11.0,=============================================================================
v0.11.0,Policy Forest
v0.11.0,=============================================================================
v0.11.0,Remap output
v0.11.0,reshape is necessary to preserve the data contiguity against vs
v0.11.0,"[:, np.newaxis] that does not."
v0.11.0,Get subsample sample size
v0.11.0,Check parameters
v0.11.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.11.0,if this is the first `fit` call of the warm start mode.
v0.11.0,"Free allocated memory, if any"
v0.11.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.11.0,We draw from the random state to get the random state we
v0.11.0,would have got if we hadn't used a warm_start.
v0.11.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.11.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.11.0,but would still advance randomness enough so that tree subsamples will be different.
v0.11.0,Parallel loop: we prefer the threading backend as the Cython code
v0.11.0,for fitting the trees is internally releasing the Python GIL
v0.11.0,making threading more efficient than multiprocessing in
v0.11.0,"that case. However, for joblib 0.12+ we respect any"
v0.11.0,"parallel_backend contexts set at a higher level,"
v0.11.0,since correctness does not rely on using threads.
v0.11.0,Collect newly grown trees
v0.11.0,Check data
v0.11.0,Assign chunk of trees to jobs
v0.11.0,avoid storing the output of every estimator by summing them here
v0.11.0,Parallel loop
v0.11.0,Check data
v0.11.0,Assign chunk of trees to jobs
v0.11.0,avoid storing the output of every estimator by summing them here
v0.11.0,Parallel loop
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,
v0.11.0,This code contains snippets of code from:
v0.11.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.11.0,published under the following license and copyright:
v0.11.0,BSD 3-Clause License
v0.11.0,
v0.11.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.0,All rights reserved.
v0.11.0,=============================================================================
v0.11.0,Types and constants
v0.11.0,=============================================================================
v0.11.0,=============================================================================
v0.11.0,Base Policy tree
v0.11.0,=============================================================================
v0.11.0,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.11.0,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.11.0,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.11.0,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.11.0,checks that X is 2D array.
v0.11.0,"since we only allow single dimensional y, we could flatten the prediction"
v0.11.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.11.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.11.0,Handles the corner case when X=None but featurizer might be not None
v0.11.0,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.11.0,"Replacing this method which is invalid for this class, so that we make the"
v0.11.0,dosctring empty and not appear in the docs.
v0.11.0,TODO: support freq_weight and sample_var in debiased lasso
v0.11.0,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.11.0,Replacing to remove docstring
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"if both X and W are None, just return a column of ones"
v0.11.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.11.0,We need to go back to the label representation of the one-hot so as to call
v0.11.0,the classifier.
v0.11.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.11.0,We need to go back to the label representation of the one-hot so as to call
v0.11.0,the classifier.
v0.11.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.11.0,This works both with our without the weighting trick as the treatments T are unit vector
v0.11.0,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.11.0,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.11.0,both Parametric and Non Parametric DML.
v0.11.0,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.11.0,attribute so that the trained featurizer will be passed through
v0.11.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.11.0,for internal use by the library
v0.11.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.11.0,We need to use the rlearner's copy to retain the information from fitting
v0.11.0,Handles the corner case when X=None but featurizer might be not None
v0.11.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.11.0,since we clone it and fit separate copies
v0.11.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.11.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.11.0,TODO: support freq_weight and sample_var in debiased lasso
v0.11.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.11.0,since we clone it and fit separate copies
v0.11.0,add blb to parent's options
v0.11.0,override only so that we can update the docstring to indicate
v0.11.0,support for `GenericSingleTreatmentModelFinalInference`
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,note that groups are not passed to score because they are only used for fitting
v0.11.0,note that groups are not passed to score because they are only used for fitting
v0.11.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.11.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.11.0,NOTE: important to get parent's wrapped copy so that
v0.11.0,"after training wrapped featurizer is also trained, etc."
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.11.0,Fit a doubly robust average effect
v0.11.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.11.0,(which needs to have been expanded if there's a discrete treatment)
v0.11.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.11.0,since we clone it and fit separate copies
v0.11.0,"If custom param grid, check that only estimator parameters are being altered"
v0.11.0,override only so that we can update the docstring to indicate support for `blb`
v0.11.0,Get input names
v0.11.0,Summary
v0.11.0,Determine output settings
v0.11.0,"Important: This must be the first invocation of the random state at fit time, so that"
v0.11.0,train/test splits are re-generatable from an external object simply by knowing the
v0.11.0,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.11.0,linear predictions. Currently is also useful for testing.
v0.11.0,reshape is necessary to preserve the data contiguity against vs
v0.11.0,"[:, np.newaxis] that does not."
v0.11.0,Check parameters
v0.11.0,Set min_weight_leaf from min_weight_fraction_leaf
v0.11.0,Build tree
v0.11.0,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.11.0,hold. Used by criterion for memory space savings.
v0.11.0,Initialize the criterion object and the criterion_val object if honest.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,
v0.11.0,This code is a fork from:
v0.11.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.11.0,published under the following license and copyright:
v0.11.0,BSD 3-Clause License
v0.11.0,
v0.11.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.0,All rights reserved.
v0.11.0,Set parameters
v0.11.0,Don't instantiate estimators now! Parameters of base_estimator might
v0.11.0,"still change. Eg., when grid-searching with the nested object syntax."
v0.11.0,self.estimators_ needs to be filled by the derived classes in fit.
v0.11.0,Compute the number of jobs
v0.11.0,Partition estimators between jobs
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Remove children with nonwhite mothers from the treatment group
v0.11.0,Remove children with nonwhite mothers from the treatment group
v0.11.0,Select columns
v0.11.0,Scale the numeric variables
v0.11.0,"Change the binary variable 'first' takes values in {1,2}"
v0.11.0,Append a column of ones as intercept
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.11.0,(which needs to have been expanded if there's a discrete treatment)
v0.11.0,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.11.0,d_t=None here since we measure the effect across all Ts
v0.11.0,once the estimator has been fit
v0.11.0,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.11.0,an intercept even when bias is part of coef.
v0.11.0,We can write effect inference as a function of prediction and prediction standard error of
v0.11.0,the final method for linear models
v0.11.0,squeeze the first axis
v0.11.0,d_t=None here since we measure the effect across all Ts
v0.11.0,set the mean_pred_stderr
v0.11.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.11.0,(which needs to have been expanded if there's a discrete treatment)
v0.11.0,"send treatment to the end, pull bounds to the front"
v0.11.0,d_t=None here since we measure the effect across all Ts
v0.11.0,set the mean_pred_stderr
v0.11.0,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.11.0,d_t=None here since we measure the effect across all Ts
v0.11.0,d_t=None here since we measure the effect across all Ts
v0.11.0,need to set the fit args before the estimator is fit
v0.11.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.11.0,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.11.0,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.11.0,NOTE: use np.asarray(offset) becuase if offset is a pd.Series direct addition would make the sum
v0.11.0,"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
v0.11.0,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.11.0,offset preds
v0.11.0,"offset the distribution, too"
v0.11.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.11.0,1. Uncertainty of Mean Point Estimate
v0.11.0,2. Distribution of Point Estimate
v0.11.0,3. Total Variance of Point Estimate
v0.11.0,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.11.0,so bail out early; note that it might be possible to correct the algorithm for
v0.11.0,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.11.0,be clean
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,TODO: Add a __dir__ implementation?
v0.11.0,don't proxy special methods
v0.11.0,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.11.0,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.11.0,then we should be computing a confidence interval for the wrapped calls
v0.11.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.11.0,second level bootstrap which would be prohibitive computationally?
v0.11.0,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.11.0,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.11.0,can't import from econml.inference at top level without creating cyclical dependencies
v0.11.0,Note that inference results are always methods even if the inference is for a property
v0.11.0,(e.g. coef__inference() is a method but coef_ is a property)
v0.11.0,Therefore we must insert a lambda if getting inference for a non-callable
v0.11.0,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.11.0,"try to get interval/std first if appropriate,"
v0.11.0,since we don't prefer a wrapped method with this name
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,
v0.11.0,This code contains snippets of code from:
v0.11.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.11.0,published under the following license and copyright:
v0.11.0,BSD 3-Clause License
v0.11.0,
v0.11.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.0,All rights reserved.
v0.11.0,=============================================================================
v0.11.0,Types and constants
v0.11.0,=============================================================================
v0.11.0,=============================================================================
v0.11.0,Base GRF tree
v0.11.0,=============================================================================
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,=============================================================================
v0.11.0,A MultOutputWrapper for GRF classes
v0.11.0,=============================================================================
v0.11.0,=============================================================================
v0.11.0,Instantiations of Generalized Random Forest
v0.11.0,=============================================================================
v0.11.0,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.11.0,in front of the constant treatment is the intercept in the moment equation.
v0.11.0,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.11.0,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,
v0.11.0,This code contains snippets of code from
v0.11.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.11.0,published under the following license and copyright:
v0.11.0,BSD 3-Clause License
v0.11.0,
v0.11.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.11.0,All rights reserved.
v0.11.0,=============================================================================
v0.11.0,Base Generalized Random Forest
v0.11.0,=============================================================================
v0.11.0,TODO: support freq_weight and sample_var
v0.11.0,Remap output
v0.11.0,reshape is necessary to preserve the data contiguity against vs
v0.11.0,"[:, np.newaxis] that does not."
v0.11.0,reshape is necessary to preserve the data contiguity against vs
v0.11.0,"[:, np.newaxis] that does not."
v0.11.0,Get subsample sample size
v0.11.0,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.11.0,We calculate the min eigenvalue proxy that each criterion is considering
v0.11.0,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.11.0,Check parameters
v0.11.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.11.0,if this is the first `fit` call of the warm start mode.
v0.11.0,"Free allocated memory, if any"
v0.11.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.11.0,We draw from the random state to get the random state we
v0.11.0,would have got if we hadn't used a warm_start.
v0.11.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.11.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.11.0,but would still advance randomness enough so that tree subsamples will be different.
v0.11.0,Generating indices a priori before parallelism ended up being orders of magnitude
v0.11.0,faster than how sklearn does it. The reason is that random samplers do not release the
v0.11.0,gil it seems.
v0.11.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.11.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.11.0,but would still advance randomness enough so that tree subsamples will be different.
v0.11.0,Parallel loop: we prefer the threading backend as the Cython code
v0.11.0,for fitting the trees is internally releasing the Python GIL
v0.11.0,making threading more efficient than multiprocessing in
v0.11.0,"that case. However, for joblib 0.12+ we respect any"
v0.11.0,"parallel_backend contexts set at a higher level,"
v0.11.0,since correctness does not rely on using threads.
v0.11.0,Collect newly grown trees
v0.11.0,Check data
v0.11.0,Assign chunk of trees to jobs
v0.11.0,avoid storing the output of every estimator by summing them here
v0.11.0,Parallel loop
v0.11.0,Check data
v0.11.0,Assign chunk of trees to jobs
v0.11.0,Parallel loop
v0.11.0,Check data
v0.11.0,Assign chunk of trees to jobs
v0.11.0,Parallel loop
v0.11.0,####################
v0.11.0,Variance correction
v0.11.0,####################
v0.11.0,Subtract the average within bag variance. This ends up being equal to the
v0.11.0,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.11.0,The negative part is just sq_between.
v0.11.0,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.11.0,"The off diagonals we have no objective prior, so no correction is applied."
v0.11.0,Finally correcting the pred_cov or pred_var
v0.11.0,avoid storing the output of every estimator by summing them here
v0.11.0,Parallel loop
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,testing importances
v0.11.0,testing heterogeneity importances
v0.11.0,Testing that all parameters do what they are supposed to
v0.11.0,"testing predict, apply and decision path"
v0.11.0,test that the subsampling scheme past to the trees is correct
v0.11.0,test that the estimator calcualtes var correctly
v0.11.0,test api
v0.11.0,test accuracy
v0.11.0,test the projection functionality of forests
v0.11.0,test that the estimator calcualtes var correctly
v0.11.0,test api
v0.11.0,test that the estimator calcualtes var correctly
v0.11.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.11.0,test that we raise errors in mishandled situations.
v0.11.0,test that the subsampling scheme past to the trees is correct
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,omit the lalonde notebook
v0.11.0,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.11.0,creating notebooks that are annoying for our users to actually run themselves
v0.11.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.11.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.11.0,"prior to calling interpret, can't plot, render, etc."
v0.11.0,can interpret without uncertainty
v0.11.0,can't interpret with uncertainty if inference wasn't used during fit
v0.11.0,can interpret with uncertainty if we refit
v0.11.0,can interpret without uncertainty
v0.11.0,can't treat before interpreting
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,simple DGP only for illustration
v0.11.0,Define the treatment model neural network architecture
v0.11.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.11.0,"so the input shape is (d_z + d_x,)"
v0.11.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.11.0,add extra layers on top for the mixture density network
v0.11.0,Define the response model neural network architecture
v0.11.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.11.0,"so the input shape is (d_t + d_x,)"
v0.11.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.11.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.11.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.11.0,so that the same weights will be reused in each instantiation
v0.11.0,number of samples to use in second estimate of the response
v0.11.0,(to make loss estimate unbiased)
v0.11.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.11.0,do something with predictions...
v0.11.0,also test vector t and y
v0.11.0,simple DGP only for illustration
v0.11.0,Define the treatment model neural network architecture
v0.11.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.11.0,"so the input shape is (d_z + d_x,)"
v0.11.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.11.0,add extra layers on top for the mixture density network
v0.11.0,Define the response model neural network architecture
v0.11.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.11.0,"so the input shape is (d_t + d_x,)"
v0.11.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.11.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.11.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.11.0,so that the same weights will be reused in each instantiation
v0.11.0,number of samples to use in second estimate of the response
v0.11.0,(to make loss estimate unbiased)
v0.11.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.11.0,do something with predictions...
v0.11.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.11.0,test = True ensures we draw test set images
v0.11.0,test = True ensures we draw test set images
v0.11.0,re-draw to get new independent treatment and implied response
v0.11.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.11.0,above is necesary so that reduced form doesn't win
v0.11.0,covariates: time and emotion
v0.11.0,random instrument
v0.11.0,z -> price
v0.11.0,true observable demand function
v0.11.0,errors
v0.11.0,response
v0.11.0,test = True ensures we draw test set images
v0.11.0,test = True ensures we draw test set images
v0.11.0,re-draw to get new independent treatment and implied response
v0.11.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.11.0,above is necesary so that reduced form doesn't win
v0.11.0,covariates: time and emotion
v0.11.0,random instrument
v0.11.0,z -> price
v0.11.0,true observable demand function
v0.11.0,errors
v0.11.0,response
v0.11.0,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.11.0,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.11.0,For some reason this doesn't work at all when run against the CNTK backend...
v0.11.0,"model.compile('nadam', loss=lambda _,l:l)"
v0.11.0,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.11.0,generate a valiation set
v0.11.0,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.11.0,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,DGP constants
v0.11.0,Generate data
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,testing importances
v0.11.0,testing heterogeneity importances
v0.11.0,Testing that all parameters do what they are supposed to
v0.11.0,"testing predict, apply and decision path"
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.11.0,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.11.0,so we need to transpose the result
v0.11.0,1-d output
v0.11.0,2-d output
v0.11.0,Single dimensional output y
v0.11.0,compare with weight
v0.11.0,compare with weight
v0.11.0,compare with weight
v0.11.0,compare with weight
v0.11.0,Multi-dimensional output y
v0.11.0,1-d y
v0.11.0,compare when both sample_var and sample_weight exist
v0.11.0,multi-d y
v0.11.0,compare when both sample_var and sample_weight exist
v0.11.0,compare when both sample_var and sample_weight exist
v0.11.0,compare when both sample_var and sample_weight exist
v0.11.0,compare when both sample_var and sample_weight exist
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,test that we can fit with the same arguments as the base estimator
v0.11.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.11.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.0,with the same shape for the lower and upper bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,test that we can do the same thing once we provide percentile bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.11.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.0,with the same shape for the lower and upper bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,test that we can do the same thing once we provide percentile bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,test that we can fit with the same arguments as the base estimator
v0.11.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.11.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.0,with the same shape for the lower and upper bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,test that we can do the same thing once we provide percentile bounds
v0.11.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.11.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.0,with the same shape for the lower and upper bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,test that we can do the same thing once we provide percentile bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,test that we can fit with the same arguments as the base estimator
v0.11.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.11.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.0,with the same shape for the lower and upper bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,test that we can do the same thing once we provide percentile bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.11.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.0,with the same shape for the lower and upper bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,test that we can do the same thing once we provide percentile bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.0,with the same shape for the lower and upper bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,test that the estimated effect is usually within the bounds
v0.11.0,test that we can do the same thing once we provide alpha explicitly
v0.11.0,test that the lower and upper bounds differ
v0.11.0,test that the estimated effect is usually within the bounds
v0.11.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.11.0,with the same shape for the lower and upper bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,TODO: test that the estimated effect is usually within the bounds
v0.11.0,and that the true effect is also usually within the bounds
v0.11.0,test that we can do the same thing once we provide percentile bounds
v0.11.0,test that the lower and upper bounds differ
v0.11.0,TODO: test that the estimated effect is usually within the bounds
v0.11.0,and that the true effect is also usually within the bounds
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,test that the subsampling scheme past to the trees is correct
v0.11.0,test that the estimator calcualtes var correctly
v0.11.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.11.0,test that we raise errors in mishandled situations.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,DGP constants
v0.11.0,Generate data
v0.11.0,Test inference results when `cate_feature_names` doesn not exist
v0.11.0,Test inference results when `cate_feature_names` doesn not exist
v0.11.0,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.11.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.11.0,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.11.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.11.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.11.0,pvalue for second column should be greater than zero since some points are on either side
v0.11.0,of the tested value
v0.11.0,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.11.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.11.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.11.0,only is not None when T1 is a constant or a list of constant
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.11.0,Test non keyword based calls to fit
v0.11.0,test non-array inputs
v0.11.0,Test custom splitter
v0.11.0,Test incomplete set of test folds
v0.11.0,"y scores should be positive, since W predicts Y somewhat"
v0.11.0,"t scores might not be, since W and T are uncorrelated"
v0.11.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,make sure cross product varies more slowly with first array
v0.11.0,and that vectors are okay as inputs
v0.11.0,number of inputs in specification must match number of inputs
v0.11.0,must have an output
v0.11.0,output indices must be unique
v0.11.0,output indices must be present in an input
v0.11.0,number of indices must match number of dimensions for each input
v0.11.0,repeated indices must always have consistent sizes
v0.11.0,transpose
v0.11.0,tensordot
v0.11.0,trace
v0.11.0,TODO: set up proper flag for this
v0.11.0,pick indices at random with replacement from the first 7 letters of the alphabet
v0.11.0,"of all of the distinct indices that appear in any input,"
v0.11.0,pick a random subset of them (of size at most 5) to appear in the output
v0.11.0,creating an instance should warn
v0.11.0,using the instance should not warn
v0.11.0,using the deprecated method should warn
v0.11.0,don't warn if b and c are passed by keyword
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Preprocess data
v0.11.0,Convert 'week' to a date
v0.11.0,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.11.0,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.11.0,Take log of price
v0.11.0,Make brand numeric
v0.11.0,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.11.0,which have all zero coeffs
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,test at least one estimator from each category
v0.11.0,test causal graph
v0.11.0,test refutation estimate
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.11.0,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.11.0,TODO: test something rather than just print...
v0.11.0,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.11.0,pick some arbitrary X
v0.11.0,pick some arbitrary T
v0.11.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.11.0,The average variance should be lower when using monte carlo iterations
v0.11.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.11.0,The average variance should be lower when using monte carlo iterations
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,ensure that we've got at least two of every row
v0.11.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.11.0,need to make sure we get all *joint* combinations
v0.11.0,IntentToTreat only supports binary treatments/instruments
v0.11.0,IntentToTreat only supports binary treatments/instruments
v0.11.0,IntentToTreat requires X
v0.11.0,ensure we can serialize unfit estimator
v0.11.0,these support only W but not X
v0.11.0,"these support only binary, not general discrete T and Z"
v0.11.0,ensure we can serialize fit estimator
v0.11.0,make sure we can call the marginal_effect and effect methods
v0.11.0,TODO: add tests for extra properties like coef_ where they exist
v0.11.0,TODO: add tests for extra properties like coef_ where they exist
v0.11.0,"make sure we can call effect with implied scalar treatments,"
v0.11.0,"no matter the dimensions of T, and also that we warn when there"
v0.11.0,are multiple treatments
v0.11.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.0,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.11.0,"however, with custom splits the checking happens in the first stage wrapper"
v0.11.0,where we don't have all of the required information to do this;
v0.11.0,we'd probably need to add it to _crossfit instead
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.11.0,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.11.0,The __warningregistry__'s need to be in a pristine state for tests
v0.11.0,to work properly.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Set random seed
v0.11.0,Generate data
v0.11.0,DGP constants
v0.11.0,Test data
v0.11.0,Constant treatment effect
v0.11.0,Constant treatment with multi output Y
v0.11.0,Heterogeneous treatment
v0.11.0,Heterogeneous treatment with multi output Y
v0.11.0,TLearner test
v0.11.0,Instantiate TLearner
v0.11.0,Test inputs
v0.11.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.11.0,Instantiate SLearner
v0.11.0,Test inputs
v0.11.0,Test constant treatment effect
v0.11.0,Test constant treatment effect with multi output Y
v0.11.0,Test heterogeneous treatment effect
v0.11.0,Need interactions between T and features
v0.11.0,Test heterogeneous treatment effect with multi output Y
v0.11.0,Instantiate XLearner
v0.11.0,Test inputs
v0.11.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.11.0,Instantiate DomainAdaptationLearner
v0.11.0,Test inputs
v0.11.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.11.0,Get the true treatment effect
v0.11.0,Get the true treatment effect
v0.11.0,Fit learner and get the effect and marginal effect
v0.11.0,Compute treatment effect residuals (absolute)
v0.11.0,Check that at least 90% of predictions are within tolerance interval
v0.11.0,Check whether the output shape is right
v0.11.0,Check that one can pass in regular lists
v0.11.0,Check that it fails correctly if lists of different shape are passed in
v0.11.0,"Check that it works when T, Y have shape (n, 1)"
v0.11.0,Generate covariates
v0.11.0,Generate treatment
v0.11.0,Calculate outcome
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,DGP constants
v0.11.0,Generate data
v0.11.0,Test data
v0.11.0,Remove warnings that might be raised by the models passed into the ORF
v0.11.0,Generate data with continuous treatments
v0.11.0,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.11.0,does not work well with parallelism.
v0.11.0,Test inputs for continuous treatments
v0.11.0,--> Check that one can pass in regular lists
v0.11.0,--> Check that it fails correctly if lists of different shape are passed in
v0.11.0,Check that outputs have the correct shape
v0.11.0,Test continuous treatments with controls
v0.11.0,Test continuous treatments without controls
v0.11.0,Generate data with binary treatments
v0.11.0,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.11.0,does not work well with parallelism.
v0.11.0,Test inputs for binary treatments
v0.11.0,--> Check that one can pass in regular lists
v0.11.0,--> Check that it fails correctly if lists of different shape are passed in
v0.11.0,"--> Check that it works when T, Y have shape (n, 1)"
v0.11.0,"--> Check that it fails correctly when T has shape (n, 2)"
v0.11.0,--> Check that it fails correctly when the treatments are not numeric
v0.11.0,Check that outputs have the correct shape
v0.11.0,Test binary treatments with controls
v0.11.0,Test binary treatments without controls
v0.11.0,Only applicable to continuous treatments
v0.11.0,Generate data for 2 treatments
v0.11.0,Test multiple treatments with controls
v0.11.0,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.11.0,The rest for controls. Just as an example.
v0.11.0,Generating A/B test data
v0.11.0,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.11.0,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.11.0,Create a wrapper around Lasso that doesn't support weights
v0.11.0,since Lasso does natively support them starting in sklearn 0.23
v0.11.0,Generate data with continuous treatments
v0.11.0,Instantiate model with most of the default parameters
v0.11.0,Compute the treatment effect on test points
v0.11.0,Compute treatment effect residuals
v0.11.0,Multiple treatments
v0.11.0,Allow at most 10% test points to be outside of the tolerance interval
v0.11.0,Compute treatment effect residuals
v0.11.0,Multiple treatments
v0.11.0,Allow at most 20% test points to be outside of the confidence interval
v0.11.0,Check that the intervals are not too wide
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.11.0,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.11.0,ensure that we've got at least 6 of every element
v0.11.0,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.11.0,NOTE: this number may need to change if the default number of folds in
v0.11.0,WeightedStratifiedKFold changes
v0.11.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.11.0,ensure we can serialize the unfit estimator
v0.11.0,ensure we can pickle the fit estimator
v0.11.0,make sure we can call the marginal_effect and effect methods
v0.11.0,test const marginal inference
v0.11.0,test effect inference
v0.11.0,test marginal effect inference
v0.11.0,test coef__inference and intercept__inference
v0.11.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.0,"make sure we can call effect with implied scalar treatments,"
v0.11.0,"no matter the dimensions of T, and also that we warn when there"
v0.11.0,are multiple treatments
v0.11.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.0,ensure that we've got at least two of every element
v0.11.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.11.0,make sure we can call the marginal_effect and effect methods
v0.11.0,test const marginal inference
v0.11.0,test effect inference
v0.11.0,test marginal effect inference
v0.11.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.11.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.11.0,We concatenate the two copies data
v0.11.0,make sure we can get out post-fit stuff
v0.11.0,create a simple artificial setup where effect of moving from treatment
v0.11.0,"1 -> 2 is 2,"
v0.11.0,"1 -> 3 is 1, and"
v0.11.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.11.0,"Using an uneven number of examples from different classes,"
v0.11.0,"and having the treatments in non-lexicographic order,"
v0.11.0,Should rule out some basic issues.
v0.11.0,test that we can fit with a KFold instance
v0.11.0,test that we can fit with a train/test iterable
v0.11.0,predetermined splits ensure that all features are seen in each split
v0.11.0,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.11.0,(incorrectly) use a final model with an intercept
v0.11.0,"Because final model is fixed, actual values of T and Y don't matter"
v0.11.0,Ensure reproducibility
v0.11.0,Sparse DGP
v0.11.0,Treatment effect coef
v0.11.0,Other coefs
v0.11.0,Features and controls
v0.11.0,Test sparse estimator
v0.11.0,"--> test coef_, intercept_"
v0.11.0,--> test treatment effects
v0.11.0,Restrict x_test to vectors of norm < 1
v0.11.0,--> check inference
v0.11.0,Check that a majority of true effects lie in the 5-95% CI
v0.11.0,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.11.0,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.11.0,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.11.0,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.11.0,sparse test case: heterogeneous effect by product
v0.11.0,need at least as many rows in e_y as there are distinct columns
v0.11.0,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.11.0,create a simple artificial setup where effect of moving from treatment
v0.11.0,"a -> b is 2,"
v0.11.0,"a -> c is 1, and"
v0.11.0,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.11.0,"Using an uneven number of examples from different classes,"
v0.11.0,"and having the treatments in non-lexicographic order,"
v0.11.0,should rule out some basic issues.
v0.11.0,Note that explicitly specifying the dtype as object is necessary until
v0.11.0,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.11.0,estimated effects should be identical when treatment is explicitly given
v0.11.0,but const_marginal_effect should be reordered based on the explicit cagetories
v0.11.0,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.11.0,test outer grouping
v0.11.0,test nested grouping
v0.11.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.11.0,whichever groups we saw
v0.11.0,test nested grouping
v0.11.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.11.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.11.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Set random seed
v0.11.0,Generate data
v0.11.0,DGP constants
v0.11.0,Test data
v0.11.0,Constant treatment effect and propensity
v0.11.0,Heterogeneous treatment and propensity
v0.11.0,ensure that we've got at least two of every element
v0.11.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.11.0,ensure that we can serialize unfit estimator
v0.11.0,ensure that we can serialize fit estimator
v0.11.0,make sure we can call the marginal_effect and effect methods
v0.11.0,test const marginal inference
v0.11.0,test effect inference
v0.11.0,test marginal effect inference
v0.11.0,test coef_ and intercept_ inference
v0.11.0,verify we can generate the summary
v0.11.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.11.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.11.0,create a simple artificial setup where effect of moving from treatment
v0.11.0,"1 -> 2 is 2,"
v0.11.0,"1 -> 3 is 1, and"
v0.11.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.11.0,"Using an uneven number of examples from different classes,"
v0.11.0,"and having the treatments in non-lexicographic order,"
v0.11.0,Should rule out some basic issues.
v0.11.0,test that we can fit with a KFold instance
v0.11.0,test that we can fit with a train/test iterable
v0.11.0,"for at least some of the examples, the CI should have nonzero width"
v0.11.0,"for at least some of the examples, the CI should have nonzero width"
v0.11.0,"for at least some of the examples, the CI should have nonzero width"
v0.11.0,test coef__inference function works
v0.11.0,test intercept__inference function works
v0.11.0,test summary function works
v0.11.0,Test inputs
v0.11.0,self._test_inputs(DR_learner)
v0.11.0,Test constant treatment effect
v0.11.0,Test heterogeneous treatment effect
v0.11.0,Test heterogenous treatment effect for W =/= None
v0.11.0,Sparse DGP
v0.11.0,Treatment effect coef
v0.11.0,Other coefs
v0.11.0,Features and controls
v0.11.0,Test sparse estimator
v0.11.0,"--> test coef_, intercept_"
v0.11.0,--> test treatment effects
v0.11.0,Restrict x_test to vectors of norm < 1
v0.11.0,--> check inference
v0.11.0,Check that a majority of true effects lie in the 5-95% CI
v0.11.0,test outer grouping
v0.11.0,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.11.0,test nested grouping
v0.11.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.11.0,whichever groups we saw
v0.11.0,test nested grouping
v0.11.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.11.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.11.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.11.0,helper class
v0.11.0,Fit learner and get the effect
v0.11.0,Get the true treatment effect
v0.11.0,Compute treatment effect residuals (absolute)
v0.11.0,Check that at least 90% of predictions are within tolerance interval
v0.11.0,Only for heterogeneous TE
v0.11.0,Fit learner on X and W and get the effect
v0.11.0,Get the true treatment effect
v0.11.0,Compute treatment effect residuals (absolute)
v0.11.0,Check that at least 90% of predictions are within tolerance interval
v0.11.0,Check that one can pass in regular lists
v0.11.0,Check that it fails correctly if lists of different shape are passed in
v0.11.0,Check that it fails when T contains values other than 0 and 1
v0.11.0,"Check that it works when T, Y have shape (n, 1)"
v0.11.0,Generate covariates
v0.11.0,Generate treatment
v0.11.0,Calculate outcome
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,DGP constants
v0.11.0,DGP coefficients
v0.11.0,Generated outcomes
v0.11.0,################
v0.11.0,WeightedLasso #
v0.11.0,################
v0.11.0,Define weights
v0.11.0,Define extended datasets
v0.11.0,Range of alphas
v0.11.0,Compare with Lasso
v0.11.0,--> No intercept
v0.11.0,--> With intercept
v0.11.0,When DGP has no intercept
v0.11.0,When DGP has intercept
v0.11.0,--> Coerce coefficients to be positive
v0.11.0,--> Toggle max_iter & tol
v0.11.0,Define weights
v0.11.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.11.0,Mixed DGP scenario.
v0.11.0,Define extended datasets
v0.11.0,Define weights
v0.11.0,Define multioutput
v0.11.0,##################
v0.11.0,WeightedLassoCV #
v0.11.0,##################
v0.11.0,Define alphas to test
v0.11.0,Compare with LassoCV
v0.11.0,--> No intercept
v0.11.0,--> With intercept
v0.11.0,--> Force parameters to be positive
v0.11.0,Choose a smaller n to speed-up process
v0.11.0,Compare fold weights
v0.11.0,Define weights
v0.11.0,Define extended datasets
v0.11.0,Define splitters
v0.11.0,WeightedKFold splitter
v0.11.0,Map weighted splitter to an extended splitter
v0.11.0,Define alphas to test
v0.11.0,Compare with LassoCV
v0.11.0,--> No intercept
v0.11.0,--> With intercept
v0.11.0,--> Force parameters to be positive
v0.11.0,###########################
v0.11.0,MultiTaskWeightedLassoCV #
v0.11.0,###########################
v0.11.0,Define alphas to test
v0.11.0,Define splitter
v0.11.0,Compare with MultiTaskLassoCV
v0.11.0,--> No intercept
v0.11.0,--> With intercept
v0.11.0,Define weights
v0.11.0,Define extended datasets
v0.11.0,Define splitters
v0.11.0,WeightedKFold splitter
v0.11.0,Map weighted splitter to an extended splitter
v0.11.0,Define alphas to test
v0.11.0,Compare with LassoCV
v0.11.0,--> No intercept
v0.11.0,--> With intercept
v0.11.0,#########################
v0.11.0,WeightedLassoCVWrapper #
v0.11.0,#########################
v0.11.0,perform 1D fit
v0.11.0,perform 2D fit
v0.11.0,################
v0.11.0,DebiasedLasso #
v0.11.0,################
v0.11.0,Test DebiasedLasso without weights
v0.11.0,--> Check debiased coeffcients without intercept
v0.11.0,--> Check debiased coeffcients with intercept
v0.11.0,--> Check 5-95 CI coverage for unit vectors
v0.11.0,Test DebiasedLasso with weights for one DGP
v0.11.0,Define weights
v0.11.0,Define extended datasets
v0.11.0,--> Check debiased coefficients
v0.11.0,Define weights
v0.11.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.11.0,--> Check debiased coeffcients
v0.11.0,Test that attributes propagate correctly
v0.11.0,Test MultiOutputDebiasedLasso without weights
v0.11.0,--> Check debiased coeffcients without intercept
v0.11.0,--> Check debiased coeffcients with intercept
v0.11.0,--> Check CI coverage
v0.11.0,Test MultiOutputDebiasedLasso with weights
v0.11.0,Define weights
v0.11.0,Define extended datasets
v0.11.0,--> Check debiased coefficients
v0.11.0,Unit vectors
v0.11.0,Unit vectors
v0.11.0,Check coeffcients and intercept are the same within tolerance
v0.11.0,Check results are similar with tolerance 1e-6
v0.11.0,Check if multitask
v0.11.0,Check that same alpha is chosen
v0.11.0,Check that the coefficients are similar
v0.11.0,selective ridge has a simple implementation that we can test against
v0.11.0,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.11.0,"it should be the case that when we set fit_intercept to true,"
v0.11.0,it doesn't matter whether the penalized model also fits an intercept or not
v0.11.0,create an extra copy of rows with weight 2
v0.11.0,"instead of a slice, explicitly return an array of indices"
v0.11.0,_penalized_inds is only set during fitting
v0.11.0,cv exists on penalized model
v0.11.0,now we can access _penalized_inds
v0.11.0,check that we can read the cv attribute back out from the underlying model
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"global and cohort data should have exactly the same structure, but different values"
v0.11.0,local index should have as many times entries as global as there were rows passed in
v0.11.0,Can't handle multi-dimensional treatments
v0.11.0,"global shape is (d_y, sum(d_t))"
v0.11.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.0,features; for categoricals they should appear #cats-1 times each
v0.11.0,"global and cohort data should have exactly the same structure, but different values"
v0.11.0,local index should have as many times entries as global as there were rows passed in
v0.11.0,features; for categoricals they should appear #cats-1 times each
v0.11.0,"global shape is (d_y, sum(d_t))"
v0.11.0,Can't handle multi-dimensional treatments
v0.11.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.0,"global and cohort data should have exactly the same structure, but different values"
v0.11.0,local index should have as many times entries as global as there were rows passed in
v0.11.0,Can't handle multi-dimensional treatments
v0.11.0,"global shape is (d_y, sum(d_t))"
v0.11.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.0,features; for categoricals they should appear #cats-1 times each
v0.11.0,make sure we don't run into problems dropping every index
v0.11.0,"global and cohort data should have exactly the same structure, but different values"
v0.11.0,local index should have as many times entries as global as there were rows passed in
v0.11.0,"global shape is (d_y, sum(d_t))"
v0.11.0,Can't handle multi-dimensional treatments
v0.11.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.11.0,"global and cohort data should have exactly the same structure, but different values"
v0.11.0,local index should have as many times entries as global as there were rows passed in
v0.11.0,features; for categoricals they should appear #cats-1 times each
v0.11.0,"global shape is (d_y, sum(d_t))"
v0.11.0,Can't handle multi-dimensional treatments
v0.11.0,dgp
v0.11.0,model
v0.11.0,model
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,DGP constants
v0.11.0,Define data features
v0.11.0,Added `_df`to names to be different from the default cate_estimator names
v0.11.0,Generate data
v0.11.0,################################
v0.11.0,Single treatment and outcome #
v0.11.0,################################
v0.11.0,Test LinearDML
v0.11.0,|--> Test featurizers
v0.11.0,ColumnTransformer doesn't propagate column names
v0.11.0,|--> Test re-fit
v0.11.0,Test SparseLinearDML
v0.11.0,Test ForestDML
v0.11.0,###################################
v0.11.0,Mutiple treatments and outcomes #
v0.11.0,###################################
v0.11.0,Test LinearDML
v0.11.0,Test SparseLinearDML
v0.11.0,"Single outcome only, ORF does not support multiple outcomes"
v0.11.0,Test DMLOrthoForest
v0.11.0,Test DROrthoForest
v0.11.0,Test XLearner
v0.11.0,Skipping population summary names test because bootstrap inference is too slow
v0.11.0,Test SLearner
v0.11.0,Test TLearner
v0.11.0,Test LinearDRLearner
v0.11.0,Test SparseLinearDRLearner
v0.11.0,Test ForestDRLearner
v0.11.0,Test LinearIntentToTreatDRIV
v0.11.0,Test DeepIV
v0.11.0,Test categorical treatments
v0.11.0,Check refit
v0.11.0,Check refit after setting categories
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Linear models are required for parametric dml
v0.11.0,sample weighting models are required for nonparametric dml
v0.11.0,Test values
v0.11.0,TLearner test
v0.11.0,Instantiate TLearner
v0.11.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.11.0,Test constant treatment effect with multi output Y
v0.11.0,Test heterogeneous treatment effect
v0.11.0,Need interactions between T and features
v0.11.0,Test heterogeneous treatment effect with multi output Y
v0.11.0,Instantiate DomainAdaptationLearner
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,test base values equals to mean of constant marginal effect
v0.11.0,test shape of shap values output is as expected
v0.11.0,test shape of attribute of explanation object is as expected
v0.11.0,test length of feature names equals to shap values shape
v0.11.0,test base values equals to mean of constant marginal effect
v0.11.0,test shape of shap values output is as expected
v0.11.0,test shape of attribute of explanation object is as expected
v0.11.0,test length of feature names equals to shap values shape
v0.11.0,Treatment effect function
v0.11.0,Outcome support
v0.11.0,Treatment support
v0.11.0,"Generate controls, covariates, treatments and outcomes"
v0.11.0,Heterogeneous treatment effects
v0.11.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.11.0,through shap package.
v0.11.0,test shap could generate the plot from the shap_values
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Check inputs
v0.11.0,Check inputs
v0.11.0,Check inputs
v0.11.0,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.11.0,"Thus, we append the controls column before the one-hot-encoded T"
v0.11.0,"We might want to revisit, though, since it's linearly determined by the others"
v0.11.0,Check inputs
v0.11.0,Check inputs
v0.11.0,Estimate response function
v0.11.0,Check inputs
v0.11.0,Train model on controls. Assign higher weight to units resembling
v0.11.0,treated units.
v0.11.0,Train model on the treated. Assign higher weight to units resembling
v0.11.0,control units.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.11.0,output is
v0.11.0,"* a column of ones if X, W, and Z are all None"
v0.11.0,* just X or W or Z if both of the others are None
v0.11.0,* hstack([arrs]) for whatever subset are not None otherwise
v0.11.0,ensure Z is 2D
v0.11.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.11.0,We need to go back to the label representation of the one-hot so as to call
v0.11.0,the classifier.
v0.11.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.11.0,We need to go back to the label representation of the one-hot so as to call
v0.11.0,the classifier.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,TODO: make sure to use random seeds wherever necessary
v0.11.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.11.0,"unfortunately with the Theano and Tensorflow backends,"
v0.11.0,the straightforward use of K.stop_gradient can cause an error
v0.11.0,because the parameters of the intermediate layers are now disconnected from the loss;
v0.11.0,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.11.0,so that those layers remain connected but with 0 gradient
v0.11.0,|| t - mu_i || ^2
v0.11.0,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.11.0,Use logsumexp for numeric stability:
v0.11.0,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.11.0,TODO: does the numeric stability actually make any difference?
v0.11.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.11.0,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.11.0,generate cumulative sum via matrix multiplication
v0.11.0,"Generate standard uniform values in shape (batch_size,1)"
v0.11.0,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.11.0,we use uniform_like instead with an input of an appropriate shape)
v0.11.0,convert to floats and multiply to perform equivalent of logical AND
v0.11.0,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.11.0,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.11.0,we use normal_like instead with an input of an appropriate shape)
v0.11.0,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.11.0,prevent gradient from passing through sampling
v0.11.0,three options: biased or upper-bound loss require a single number of samples;
v0.11.0,unbiased can take different numbers for the network and its gradient
v0.11.0,"sample: (() -> Layer, int) -> Layer"
v0.11.0,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.11.0,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.11.0,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.11.0,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.11.0,the dimensionality of the output of the network
v0.11.0,TODO: is there a more robust way to do this?
v0.11.0,TODO: do we need to give the user more control over other arguments to fit?
v0.11.0,"subtle point: we need to build a new model each time,"
v0.11.0,because each model encapsulates its randomness
v0.11.0,TODO: do we need to give the user more control over other arguments to fit?
v0.11.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.11.0,not a general tensor (because of how backprop works in every framework)
v0.11.0,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.11.0,but this seems annoying...)
v0.11.0,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.11.0,TODO: any way to get this to work on batches of arbitrary size?
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Estimate final model of theta(X) by minimizing the square loss:
v0.11.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.11.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.11.0,at the expense of some small bias. For points with very small covariance we revert
v0.11.0,to the model-based preliminary estimate and do not add the correction term.
v0.11.0,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.11.0,"instruments, and outcomes"
v0.11.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.11.0,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.11.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.11.0,for internal use by the library
v0.11.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.11.0,"we need to undo the one-hot encoding for calling effect,"
v0.11.0,since it expects raw values
v0.11.0,"we need to undo the one-hot encoding for calling effect,"
v0.11.0,since it expects raw values
v0.11.0,"TODO: check that Y, T, Z do not have multiple columns"
v0.11.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.11.0,TODO: do correct adjustment for sample_var
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.11.0,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.11.0,since we would actually be calculating a CATE rather than ATE.
v0.11.0,TODO: allow the final model to actually use X?
v0.11.0,TODO: allow the final model to actually use X?
v0.11.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.11.0,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.11.0,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.11.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.11.0,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.11.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.11.0,for internal use by the library
v0.11.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,"this will have dimension (d,) + shape(X)"
v0.11.0,send the first dimension to the end
v0.11.0,columns are featurized independently; partial derivatives are only non-zero
v0.11.0,when taken with respect to the same column each time
v0.11.0,don't fit intercept; manually add column of ones to the data instead;
v0.11.0,this allows us to ignore the intercept when computing marginal effects
v0.11.0,make T 2D if if was a vector
v0.11.0,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.11.0,two stage approximation
v0.11.0,"first, get basis expansions of T, X, and Z"
v0.11.0,TODO: is it right that the effective number of intruments is the
v0.11.0,"product of ft_X and ft_Z, not just ft_Z?"
v0.11.0,"regress T expansion on X,Z expansions concatenated with W"
v0.11.0,"predict ft_T from interacted ft_X, ft_Z"
v0.11.0,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.11.0,dT may be only 2-dimensional)
v0.11.0,promote dT to 3D if necessary (e.g. if T was a vector)
v0.11.0,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,TODO: this utility is documented but internal; reimplement?
v0.11.0,TODO: this utility is even less public...
v0.11.0,"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
v0.11.0,simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
v0.11.0,but also supports get_feature_names with expected signature
v0.11.0,Validate inputs
v0.11.0,TODO: check compatibility of X and Y lengths
v0.11.0,"no previous fit, cancel warm start"
v0.11.0,TODO: implement check for upper bound on categoricals
v0.11.0,"work with numeric feature indices, so that we can easily compare with categorical ones"
v0.11.0,"if heterogeneity_inds is 1D, repeat it"
v0.11.0,heterogeneity inds should be a 2D list of length same as train_inds
v0.11.0,replace None elements of heterogeneity_inds and ensure indices are numeric
v0.11.0,"TODO: bail out also if categorical columns, classification changed?"
v0.11.0,TODO: should we also train a new model_y under any circumstances when warm_start is True?
v0.11.0,train the Y model
v0.11.0,"perform model selection for the Y model using all X, not on a per-column basis"
v0.11.0,"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
v0.11.0,work with the regression wrapper
v0.11.0,we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
v0.11.0,"note that this needs to happen after wrapping to generalize to the multi-class case,"
v0.11.0,since otherwise we'll have too many columns to be able to train a classifier
v0.11.0,start with empty results and default shared insights
v0.11.0,convert categorical indicators to numeric indices
v0.11.0,Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
v0.11.0,Controls are all other columns of X
v0.11.0,"can't use X[:, feat_ind] when X is a DataFrame"
v0.11.0,array checking routines don't accept 0-width arrays
v0.11.0,perform model selection
v0.11.0,Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
v0.11.0,convert to NormalInferenceResults for consistency
v0.11.0,Set the dictionary values shared between local and global summaries
v0.11.0,extract subset of names matching new columns
v0.11.0,properties to return from effect InferenceResults
v0.11.0,properties to return from PopulationSummaryResults
v0.11.0,Converts strings to property lookups or method calls as a convenience so that the
v0.11.0,_point_props and _summary_props above can be applied to an inference object
v0.11.0,Create a summary combining all results into a single output; this is used
v0.11.0,by the various causal_effect and causal_effect_dict methods to generate either a dataframe
v0.11.0,"or a dictionary, respectively, based on the summary function passed into this method"
v0.11.0,"ensure array has shape (m,y,t)"
v0.11.0,population summary is missing sample dimension; add it for consistency
v0.11.0,outcome dimension is missing; add it for consistency
v0.11.0,add singleton treatment dimension if missing
v0.11.0,"each attr has dimension (m,y) or (m,y,t)"
v0.11.0,concatenate along treatment dimension
v0.11.0,"for dictionary representation, want to remove unneeded sample dimension"
v0.11.0,in cohort and global results
v0.11.0,TODO: enrich outcome logic for multi-class classification when that is supported
v0.11.0,can't drop only level
v0.11.0,should be serialization-ready and contain no numpy arrays
v0.11.0,a global inference indicates the effect of that one feature on the outcome
v0.11.0,need to reshape the output to match the input
v0.11.0,we want to offset the inference object by the baseline estimate of y
v0.11.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.11.0,"however, the tree can't store the feature and treatment names we compute here..."
v0.11.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;
v0.11.0,"however, the tree can't store the feature and treatment names we compute here..."
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,TODO: conisder working around relying on sklearn implementation details
v0.11.0,"Found a good split, return."
v0.11.0,Record all splits in case the stratification by weight yeilds a worse partition
v0.11.0,Reseed random generator and try again
v0.11.0,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.11.0,"Found a good split, return."
v0.11.0,Did not find a good split
v0.11.0,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.11.0,Return most weight-balanced partition
v0.11.0,Weight stratification algorithm
v0.11.0,Sort weights for weight strata search
v0.11.0,There are some leftover indices that have yet to be assigned
v0.11.0,Append stratum splits to overall splits
v0.11.0,"If classification methods produce multiple columns of output,"
v0.11.0,we need to manually encode classes to ensure consistent column ordering.
v0.11.0,We clone the estimator to make sure that all the folds are
v0.11.0,"independent, and that it is pickle-able."
v0.11.0,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.11.0,`predictions` is a list of method outputs from each fold.
v0.11.0,"If each of those is also a list, then treat this as a"
v0.11.0,multioutput-multiclass task. We need to separately concatenate
v0.11.0,the method outputs for each label into an `n_labels` long list.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Our classes that derive from sklearn ones sometimes include
v0.11.0,inherited docstrings that have embedded doctests; we need the following imports
v0.11.0,so that they don't break.
v0.11.0,TODO: consider working around relying on sklearn implementation details
v0.11.0,"Convert X, y into numpy arrays"
v0.11.0,Define fit parameters
v0.11.0,Some algorithms don't have a check_input option
v0.11.0,Check weights array
v0.11.0,Check that weights are size-compatible
v0.11.0,Normalize inputs
v0.11.0,Weight inputs
v0.11.0,Fit base class without intercept
v0.11.0,Fit Lasso
v0.11.0,Reset intercept
v0.11.0,The intercept is not calculated properly due the sqrt(weights) factor
v0.11.0,so it must be recomputed
v0.11.0,Fit lasso without weights
v0.11.0,Make weighted splitter
v0.11.0,Fit weighted model
v0.11.0,Make weighted splitter
v0.11.0,Fit weighted model
v0.11.0,Call weighted lasso on reduced design matrix
v0.11.0,Weighted tau
v0.11.0,Select optimal penalty
v0.11.0,Warn about consistency
v0.11.0,"Convert X, y into numpy arrays"
v0.11.0,Fit weighted lasso with user input
v0.11.0,"Center X, y"
v0.11.0,Calculate quantities that will be used later on. Account for centered data
v0.11.0,Calculate coefficient and error variance
v0.11.0,Add coefficient correction
v0.11.0,Set coefficients and intercept standard errors
v0.11.0,Set intercept
v0.11.0,Return alpha to 'auto' state
v0.11.0,"Note that in the case of no intercept, X_offset is 0"
v0.11.0,Calculate the variance of the predictions
v0.11.0,Calculate prediction confidence intervals
v0.11.0,Assumes flattened y
v0.11.0,Compute weighted residuals
v0.11.0,To be done once per target. Assumes y can be flattened.
v0.11.0,Assumes that X has already been offset
v0.11.0,Special case: n_features=1
v0.11.0,Compute Lasso coefficients for the columns of the design matrix
v0.11.0,Compute C_hat
v0.11.0,Compute theta_hat
v0.11.0,Allow for single output as well
v0.11.0,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.11.0,Set coef_ attribute
v0.11.0,Set intercept_ attribute
v0.11.0,Set selected_alpha_ attribute
v0.11.0,Set coef_stderr_
v0.11.0,intercept_stderr_
v0.11.0,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.11.0,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.11.0,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.11.0,set intercept_ attribute
v0.11.0,set coef_ attribute
v0.11.0,set alpha_ attribute
v0.11.0,set alphas_ attribute
v0.11.0,set n_iter_ attribute
v0.11.0,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.11.0,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.11.0,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.11.0,now regress X1 on y - X2 * beta2 to learn beta1
v0.11.0,set coef_ and intercept_ attributes
v0.11.0,Note that the penalized model should *not* have an intercept
v0.11.0,don't proxy special methods
v0.11.0,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.11.0,regressor incorrectly
v0.11.0,"Note: for known attributes that have been set this method will not be called,"
v0.11.0,so we should just throw here because this is an attribute belonging to this class
v0.11.0,but which hasn't yet been set on this instance
v0.11.0,set default values for None
v0.11.0,check freq_weight should be integer and should be accompanied by sample_var
v0.11.0,check array shape
v0.11.0,weight X and y and sample_var
v0.11.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,AzureML
v0.11.0,helper imports
v0.11.0,write the details of the workspace to a configuration file to the notebook library
v0.11.0,if y is a multioutput model
v0.11.0,Make sure second dimension has 1 or more item
v0.11.0,switch _inner Model to a MultiOutputRegressor
v0.11.0,flatten array as automl only takes vectors for y
v0.11.0,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.11.0,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.11.0,as an sklearn estimator
v0.11.0,fit implementation for a single output model.
v0.11.0,Create experiment for specified workspace
v0.11.0,Configure automl_config with training set information.
v0.11.0,"Wait for remote run to complete, the set the model"
v0.11.0,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.11.0,create model and pass model into final.
v0.11.0,"If item is an automl config, get its corresponding"
v0.11.0,AutomatedML Model and add it to new_Args
v0.11.0,"If item is an automl config, get its corresponding"
v0.11.0,AutomatedML Model and set it for this key in
v0.11.0,kwargs
v0.11.0,takes in either automated_ml config and instantiates
v0.11.0,an AutomatedMLModel
v0.11.0,The prefix can only be 18 characters long
v0.11.0,"because prefixes come from kwarg_names, we must ensure they are"
v0.11.0,short enough.
v0.11.0,Get workspace from config file.
v0.11.0,Take the intersect of the white for sample
v0.11.0,weights and linear models
v0.11.0,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,average the outcome dimension if it exists and ensure 2d y_pred
v0.11.0,get index of best treatment
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,TODO: consider working around relying on sklearn implementation details
v0.11.0,Create splits of causal tree
v0.11.0,Make sure the correct exception is being rethrown
v0.11.0,Must make sure indices are merged correctly
v0.11.0,Convert rows to columns
v0.11.0,Require group assignment t to be one-hot-encoded
v0.11.0,Get predictions for the 2 splits
v0.11.0,Must make sure indices are merged correctly
v0.11.0,Crossfitting
v0.11.0,Compute weighted nuisance estimates
v0.11.0,-------------------------------------------------------------------------------
v0.11.0,Calculate the covariance matrix corresponding to the BLB inference
v0.11.0,
v0.11.0,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.11.0,2. Calculate the weighted moments for each tree slice to create a matrix
v0.11.0,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.11.0,in that slice from the overall parameter estimate.
v0.11.0,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.11.0,-------------------------------------------------------------------------------
v0.11.0,Calclulate covariance matrix through BLB
v0.11.0,Estimators
v0.11.0,OrthoForest parameters
v0.11.0,Sub-forests
v0.11.0,Auxiliary attributes
v0.11.0,Fit check
v0.11.0,TODO: Check performance
v0.11.0,Must normalize weights
v0.11.0,Override the CATE inference options
v0.11.0,Add blb inference to parent's options
v0.11.0,Generate subsample indices
v0.11.0,Build trees in parallel
v0.11.0,Bootstraping has repetitions in tree sample
v0.11.0,Similar for `a` weights
v0.11.0,Bootstraping has repetitions in tree sample
v0.11.0,Define subsample size
v0.11.0,Safety check
v0.11.0,Draw points to create little bags
v0.11.0,Copy and/or define models
v0.11.0,Define nuisance estimators
v0.11.0,Define parameter estimators
v0.11.0,Define
v0.11.0,Need to redefine fit here for auto inference to work due to a quirk in how
v0.11.0,wrap_fit is defined
v0.11.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.11.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.11.0,Override to flatten output if T is flat
v0.11.0,Check that all discrete treatments are represented
v0.11.0,Nuissance estimates evaluated with cross-fitting
v0.11.0,Define 2-fold iterator
v0.11.0,Check if there is only one example of some class
v0.11.0,Define 2-fold iterator
v0.11.0,need safe=False when cloning for WeightedModelWrapper
v0.11.0,Compute residuals
v0.11.0,Compute coefficient by OLS on residuals
v0.11.0,"Parameter returned by LinearRegression is (d_T, )"
v0.11.0,Compute residuals
v0.11.0,Compute coefficient by OLS on residuals
v0.11.0,ell_2 regularization
v0.11.0,Ridge regression estimate
v0.11.0,"Parameter returned is of shape (d_T, )"
v0.11.0,Return moments and gradients
v0.11.0,Compute residuals
v0.11.0,Compute moments
v0.11.0,"Moments shape is (n, d_T)"
v0.11.0,Compute moment gradients
v0.11.0,returns shape-conforming residuals
v0.11.0,Copy and/or define models
v0.11.0,Define parameter estimators
v0.11.0,Define moment and mean gradient estimator
v0.11.0,"Check that T is shape (n, )"
v0.11.0,Check T is numeric
v0.11.0,Train label encoder
v0.11.0,Call `fit` from parent class
v0.11.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.11.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.11.0,Override to flatten output if T is flat
v0.11.0,Expand one-hot encoding to include the zero treatment
v0.11.0,"Test that T contains all treatments. If not, return None"
v0.11.0,Nuissance estimates evaluated with cross-fitting
v0.11.0,Define 2-fold iterator
v0.11.0,Check if there is only one example of some class
v0.11.0,No need to crossfit for internal nodes
v0.11.0,Compute partial moments
v0.11.0,"If any of the values in the parameter estimate is nan, return None"
v0.11.0,Compute partial moments
v0.11.0,Compute coefficient by OLS on residuals
v0.11.0,ell_2 regularization
v0.11.0,Ridge regression estimate
v0.11.0,"Parameter returned is of shape (d_T, )"
v0.11.0,Return moments and gradients
v0.11.0,Compute partial moments
v0.11.0,Compute moments
v0.11.0,"Moments shape is (n, d_T-1)"
v0.11.0,Compute moment gradients
v0.11.0,Need to calculate this in an elegant way for when propensity is 0
v0.11.0,This will flatten T
v0.11.0,Check that T is numeric
v0.11.0,Test whether the input estimator is supported
v0.11.0,Calculate confidence intervals for the parameter (marginal effect)
v0.11.0,Calculate confidence intervals for the effect
v0.11.0,Calculate the effects
v0.11.0,Calculate the standard deviations for the effects
v0.11.0,d_t=None here since we measure the effect across all Ts
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.11.0,Licensed under the MIT License.
v0.11.0,Causal tree parameters
v0.11.0,Tree structure
v0.11.0,No need for a random split since the data is already
v0.11.0,a random subsample from the original input
v0.11.0,node list stores the nodes that are yet to be splitted
v0.11.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.11.0,Create local sample set
v0.11.0,Compute nuisance estimates for the current node
v0.11.0,Nuisance estimate cannot be calculated
v0.11.0,Estimate parameter for current node
v0.11.0,Node estimate cannot be calculated
v0.11.0,Calculate moments and gradient of moments for current data
v0.11.0,Calculate inverse gradient
v0.11.0,The gradient matrix is not invertible.
v0.11.0,No good split can be found
v0.11.0,Calculate point-wise pseudo-outcomes rho
v0.11.0,a split is determined by a feature and a sample pair
v0.11.0,the number of possible splits is at most (number of features) * (number of node samples)
v0.11.0,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.11.0,parse row and column of random pair
v0.11.0,the sample of the pair is the integer division of the random number with n_feats
v0.11.0,calculate the binary indicator of whether sample i is on the left or the right
v0.11.0,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.11.0,calculate the number of samples on the left child for each proposed split
v0.11.0,calculate the analogous binary indicator for the samples in the estimation set
v0.11.0,calculate the number of estimation samples on the left child of each proposed split
v0.11.0,find the upper and lower bound on the size of the left split for the split
v0.11.0,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.11.0,on each side.
v0.11.0,similarly for the estimation sample set
v0.11.0,if there is no valid split then don't create any children
v0.11.0,filter only the valid splits
v0.11.0,calculate the average influence vector of the samples in the left child
v0.11.0,calculate the average influence vector of the samples in the right child
v0.11.0,take the square of each of the entries of the influence vectors and normalize
v0.11.0,by size of each child
v0.11.0,calculate the vector score of each candidate split as the average of left and right
v0.11.0,influence vectors
v0.11.0,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.11.0,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.11.0,where there might be large discontinuities in some parameter as the conditioning set varies
v0.11.0,calculate the scalar score of each split by aggregating across the vector of scores
v0.11.0,Find split that minimizes criterion
v0.11.0,Create child nodes with corresponding subsamples
v0.11.0,add the created children to the list of not yet split nodes
v0.10.0,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.10.0,configuration is all pulled from setup.cfg
v0.10.0,-*- coding: utf-8 -*-
v0.10.0,
v0.10.0,Configuration file for the Sphinx documentation builder.
v0.10.0,
v0.10.0,This file does only contain a selection of the most common options. For a
v0.10.0,full list see the documentation:
v0.10.0,http://www.sphinx-doc.org/en/master/config
v0.10.0,-- Path setup --------------------------------------------------------------
v0.10.0,"If extensions (or modules to document with autodoc) are in another directory,"
v0.10.0,add these directories to sys.path here. If the directory is relative to the
v0.10.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.10.0,
v0.10.0,-- Project information -----------------------------------------------------
v0.10.0,-- General configuration ---------------------------------------------------
v0.10.0,"If your documentation needs a minimal Sphinx version, state it here."
v0.10.0,
v0.10.0,needs_sphinx = '1.0'
v0.10.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.10.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.10.0,ones.
v0.10.0,"Add any paths that contain templates here, relative to this directory."
v0.10.0,The suffix(es) of source filenames.
v0.10.0,You can specify multiple suffix as a list of string:
v0.10.0,
v0.10.0,"source_suffix = ['.rst', '.md']"
v0.10.0,The master toctree document.
v0.10.0,The language for content autogenerated by Sphinx. Refer to documentation
v0.10.0,for a list of supported languages.
v0.10.0,
v0.10.0,This is also used if you do content translation via gettext catalogs.
v0.10.0,"Usually you set ""language"" from the command line for these cases."
v0.10.0,"List of patterns, relative to source directory, that match files and"
v0.10.0,directories to ignore when looking for source files.
v0.10.0,This pattern also affects html_static_path and html_extra_path.
v0.10.0,The name of the Pygments (syntax highlighting) style to use.
v0.10.0,-- Options for HTML output -------------------------------------------------
v0.10.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.10.0,a list of builtin themes.
v0.10.0,
v0.10.0,Theme options are theme-specific and customize the look and feel of a theme
v0.10.0,"further.  For a list of options available for each theme, see the"
v0.10.0,documentation.
v0.10.0,
v0.10.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.10.0,"relative to this directory. They are copied after the builtin static files,"
v0.10.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.10.0,html_static_path = ['_static']
v0.10.0,"Custom sidebar templates, must be a dictionary that maps document names"
v0.10.0,to template names.
v0.10.0,
v0.10.0,The default sidebars (for documents that don't match any pattern) are
v0.10.0,defined by theme itself.  Builtin themes are using these templates by
v0.10.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.10.0,'searchbox.html']``.
v0.10.0,
v0.10.0,html_sidebars = {}
v0.10.0,-- Options for HTMLHelp output ---------------------------------------------
v0.10.0,Output file base name for HTML help builder.
v0.10.0,-- Options for LaTeX output ------------------------------------------------
v0.10.0,The paper size ('letterpaper' or 'a4paper').
v0.10.0,
v0.10.0,"'papersize': 'letterpaper',"
v0.10.0,"The font size ('10pt', '11pt' or '12pt')."
v0.10.0,
v0.10.0,"'pointsize': '10pt',"
v0.10.0,Additional stuff for the LaTeX preamble.
v0.10.0,
v0.10.0,"'preamble': '',"
v0.10.0,Latex figure (float) alignment
v0.10.0,
v0.10.0,"'figure_align': 'htbp',"
v0.10.0,Grouping the document tree into LaTeX files. List of tuples
v0.10.0,"(source start file, target name, title,"
v0.10.0,"author, documentclass [howto, manual, or own class])."
v0.10.0,-- Options for manual page output ------------------------------------------
v0.10.0,One entry per manual page. List of tuples
v0.10.0,"(source start file, name, description, authors, manual section)."
v0.10.0,-- Options for Texinfo output ----------------------------------------------
v0.10.0,Grouping the document tree into Texinfo files. List of tuples
v0.10.0,"(source start file, target name, title, author,"
v0.10.0,"dir menu entry, description, category)"
v0.10.0,-- Options for Epub output -------------------------------------------------
v0.10.0,Bibliographic Dublin Core info.
v0.10.0,The unique identifier of the text. This can be a ISBN number
v0.10.0,or the project homepage.
v0.10.0,
v0.10.0,epub_identifier = ''
v0.10.0,A unique identification for the text.
v0.10.0,
v0.10.0,epub_uid = ''
v0.10.0,A list of files that should not be packed into the epub file.
v0.10.0,-- Extension configuration -------------------------------------------------
v0.10.0,-- Options for intersphinx extension ---------------------------------------
v0.10.0,Example configuration for intersphinx: refer to the Python standard library.
v0.10.0,-- Options for todo extension ----------------------------------------------
v0.10.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.10.0,-- Options for doctest extension -------------------------------------------
v0.10.0,we can document otherwise excluded entities here by returning False
v0.10.0,or skip otherwise included entities by returning True
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Calculate residuals
v0.10.0,Estimate E[T_res | Z_res]
v0.10.0,TODO. Deal with multi-class instrument
v0.10.0,Calculate nuisances
v0.10.0,Estimate E[T_res | Z_res]
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"We do a three way split, as typically a preliminary theta estimator would require"
v0.10.0,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.10.0,TODO. Deal with multi-class instrument
v0.10.0,Estimate final model of theta(X) by minimizing the square loss:
v0.10.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.10.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.10.0,at the expense of some small bias. For points with very small covariance we revert
v0.10.0,to the model-based preliminary estimate and do not add the correction term.
v0.10.0,Estimate preliminary theta in cross fitting manner
v0.10.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.10.0,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.10.0,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.10.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.10.0,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.10.0,TODO. The solution below is not really a valid cross-fitting
v0.10.0,as the test data are used to create the proj_t on the train
v0.10.0,which in the second train-test loop is used to create the nuisance
v0.10.0,cov on the test data. Hence the T variable of some sample
v0.10.0,"is implicitly correlated with its cov nuisance, through this flow"
v0.10.0,"of information. However, this seems a rather weak correlation."
v0.10.0,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.10.0,model.
v0.10.0,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.10.0,Estimate preliminary theta in cross fitting manner
v0.10.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.10.0,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.10.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.10.0,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.10.0,#############################################################################
v0.10.0,Classes for the DRIV implementation for the special case of intent-to-treat
v0.10.0,A/B test
v0.10.0,#############################################################################
v0.10.0,Estimate preliminary theta in cross fitting manner
v0.10.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.10.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.10.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.10.0,We can use statsmodel for all hypothesis testing capabilities
v0.10.0,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.10.0,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.10.0,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.10.0,model_T_XZ = lambda: model_clf()
v0.10.0,#'days_visited': lambda:
v0.10.0,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.10.0,Turn strings into categories for numeric mapping
v0.10.0,### Defining some generic regressors and classifiers
v0.10.0,This a generic non-parametric regressor
v0.10.0,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.10.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.10.0,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.10.0,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.10.0,model = lambda: RandomForestRegressor(n_estimators=100)
v0.10.0,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.10.0,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.10.0,model = lambda: LinearRegression(n_jobs=-1)
v0.10.0,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.10.0,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.10.0,underlying model whenever predict is called.
v0.10.0,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.10.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.10.0,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.10.0,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.10.0,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.10.0,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.10.0,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.10.0,We need to specify models to be used for each of these residualizations
v0.10.0,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.10.0,"E[T | X, Z]"
v0.10.0,E[TZ | X]
v0.10.0,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.10.0,n_splits determines the number of splits to be used for cross-fitting.
v0.10.0,# Algorithm 2 - Current Method
v0.10.0,In[121]:
v0.10.0,# Algorithm 3 - DRIV ATE
v0.10.0,dmliv_model_effect = lambda: model()
v0.10.0,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.10.0,"dmliv_model_effect(),"
v0.10.0,n_splits=1)
v0.10.0,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.10.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.10.0,"Once multiple treatments are supported, we'll need to fix this"
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.10.0,We can use statsmodel for all hypothesis testing capabilities
v0.10.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.10.0,We can use statsmodel for all hypothesis testing capabilities
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,TODO. Deal with multi-class instrument/treatment
v0.10.0,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.10.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.10.0,Estimate p(X) = E[T | X] in cross-fitting manner
v0.10.0,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.10.0,##################
v0.10.0,Global settings #
v0.10.0,##################
v0.10.0,Global plotting controls
v0.10.0,"Control for support size, can control for more"
v0.10.0,#################
v0.10.0,File utilities #
v0.10.0,#################
v0.10.0,#################
v0.10.0,Plotting utils #
v0.10.0,#################
v0.10.0,bias
v0.10.0,var
v0.10.0,rmse
v0.10.0,r2
v0.10.0,Infer feature dimension
v0.10.0,Metrics by support plots
v0.10.0,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.10.0,Vasilis Syrgkanis <vasy@microsoft.com>
v0.10.0,Steven Wu <zhiww@microsoft.com>
v0.10.0,Initialize causal tree parameters
v0.10.0,Create splits of causal tree
v0.10.0,Estimate treatment effects at the leafs
v0.10.0,Compute heterogeneous treatement effect for x's in x_list by finding
v0.10.0,the corresponding split and associating the effect computed on that leaf
v0.10.0,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.10.0,Safety check
v0.10.0,Weighted linear regression
v0.10.0,Calculates weights
v0.10.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.10.0,over all indices
v0.10.0,Similar for `a` weights
v0.10.0,Doesn't have sample weights
v0.10.0,Is a linear model
v0.10.0,Weighted linear regression
v0.10.0,Calculates weights
v0.10.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.10.0,over all indices
v0.10.0,Similar for `a` weights
v0.10.0,normalize weights
v0.10.0,"Split the data in half, train and test"
v0.10.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.10.0,"a function of W, using only the train fold"
v0.10.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.10.0,"Split the data in half, train and test"
v0.10.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.10.0,"a function of W, using only the train fold"
v0.10.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.10.0,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.10.0,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.10.0,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.10.0,"Split the data in half, train and test"
v0.10.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.10.0,"a function of x, using only the train fold"
v0.10.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.10.0,Compute coefficient by OLS on residuals
v0.10.0,"Split the data in half, train and test"
v0.10.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.10.0,"a function of x, using only the train fold"
v0.10.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.10.0,Estimate multipliers for second order orthogonal method
v0.10.0,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.10.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.10.0,Create local sample set
v0.10.0,compute the base estimate for the current node using double ml or second order double ml
v0.10.0,compute the influence functions here that are used for the criterion
v0.10.0,generate random proposals of dimensions to split
v0.10.0,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.10.0,compute criterion for each proposal
v0.10.0,if splitting creates valid leafs in terms of mean leaf size
v0.10.0,Calculate criterion for split
v0.10.0,Else set criterion to infinity so that this split is not chosen
v0.10.0,If no good split was found
v0.10.0,Find split that minimizes criterion
v0.10.0,Set the split attributes at the node
v0.10.0,Create child nodes with corresponding subsamples
v0.10.0,Recursively split children
v0.10.0,Return parent node
v0.10.0,estimate the local parameter at the leaf using the estimate data
v0.10.0,###################
v0.10.0,Argument parsing #
v0.10.0,###################
v0.10.0,#########################################
v0.10.0,Parameters constant across experiments #
v0.10.0,#########################################
v0.10.0,Outcome support
v0.10.0,Treatment support
v0.10.0,Evaluation grid
v0.10.0,Treatment effects array
v0.10.0,Other variables
v0.10.0,##########################
v0.10.0,Data Generating Process #
v0.10.0,##########################
v0.10.0,Log iteration
v0.10.0,"Generate controls, features, treatment and outcome"
v0.10.0,T and Y residuals to be used in later scripts
v0.10.0,Save generated dataset
v0.10.0,#################
v0.10.0,ORF parameters #
v0.10.0,#################
v0.10.0,######################################
v0.10.0,Train and evaluate treatment effect #
v0.10.0,######################################
v0.10.0,########
v0.10.0,Plots #
v0.10.0,########
v0.10.0,###############
v0.10.0,Save results #
v0.10.0,###############
v0.10.0,##############
v0.10.0,Run Rscript #
v0.10.0,##############
v0.10.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.10.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.10.0,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.10.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.10.0,def mlasso_model(): return MultiTaskLassoCV(
v0.10.0,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.10.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.10.0,heterogeneity
v0.10.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.10.0,heterogeneity
v0.10.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.10.0,heterogeneity
v0.10.0,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.10.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.10.0,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.10.0,subset of features that are exogenous and create heterogeneity
v0.10.0,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.10.0,subset of features wrt we estimate heterogeneity
v0.10.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.10.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,introspect the constructor arguments to find the model parameters
v0.10.0,to represent
v0.10.0,"if the argument is deprecated, ignore it"
v0.10.0,Extract and sort argument names excluding 'self'
v0.10.0,column names
v0.10.0,transfer input to numpy arrays
v0.10.0,transfer input to 2d arrays
v0.10.0,create dataframe
v0.10.0,currently dowhy only support single outcome and single treatment
v0.10.0,call dowhy
v0.10.0,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.10.0,cate estimator but not the effect.
v0.10.0,don't proxy special methods
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Check if model is sparse enough for this model
v0.10.0,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.10.0,TODO: any way to avoid creating a copy if the array was already dense?
v0.10.0,"the call is necessary if the input was something like a list, though"
v0.10.0,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.10.0,so convert to pydata sparse first
v0.10.0,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.10.0,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.10.0,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.10.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.10.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.10.0,For when checking input values is disabled
v0.10.0,Type to column extraction function
v0.10.0,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.10.0,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.10.0,Get feature names using featurizer
v0.10.0,All attempts at retrieving transformed feature names have failed
v0.10.0,Delegate handling to downstream logic
v0.10.0,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.10.0,same number of input definitions as arrays
v0.10.0,input definitions have same number of dimensions as each array
v0.10.0,all result indices are unique
v0.10.0,all result indices must match at least one input index
v0.10.0,"map indices to all array, axis pairs for that index"
v0.10.0,each index has the same cardinality wherever it appears
v0.10.0,"State: list of (set of letters, list of (corresponding indices, value))"
v0.10.0,Algo: while list contains more than one entry
v0.10.0,take two entries
v0.10.0,sort both lists by intersection of their indices
v0.10.0,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.10.0,"take the union of indices and the product of values), stepping through each list linearly"
v0.10.0,TODO: might be faster to break into connected components first
v0.10.0,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.10.0,"so compute their content separately, then take cartesian product"
v0.10.0,this would save a few pointless sorts by empty tuples
v0.10.0,TODO: Consider investigating other performance ideas for these cases
v0.10.0,where the dense method beat the sparse method (usually sparse is faster)
v0.10.0,"e,facd,c->cfed"
v0.10.0,sparse: 0.0335489
v0.10.0,dense:  0.011465999999999997
v0.10.0,"gbd,da,egb->da"
v0.10.0,sparse: 0.0791625
v0.10.0,dense:  0.007319099999999995
v0.10.0,"dcc,d,faedb,c->abe"
v0.10.0,sparse: 1.2868097
v0.10.0,dense:  0.44605229999999985
v0.10.0,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.10.0,TODO: would using einsum's paths to optimize the order of merging help?
v0.10.0,assume that we should perform nested cross-validation if and only if
v0.10.0,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.10.0,logic copied from check_cv
v0.10.0,otherwise we will assume the user already set the cv attribute to something
v0.10.0,compatible with splitting with a 'groups' argument
v0.10.0,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.10.0,don't use the groups when calling split internally
v0.10.0,Normalize weights
v0.10.0,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.10.0,"if we're decorating a class, just update the __init__ method,"
v0.10.0,so that the result is still a class instead of a wrapper method
v0.10.0,"want to enforce that each bad_arg was either in kwargs,"
v0.10.0,or else it was in neither and is just taking its default value
v0.10.0,Any access should throw
v0.10.0,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.10.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.10.0,input feature name is already updated by cate_feature_names.
v0.10.0,define the index of d_x to filter for each given T
v0.10.0,filter X after broadcast with T for each given T
v0.10.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.10.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,
v0.10.0,This code contains some snippets of code from:
v0.10.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
v0.10.0,published under the following license and copyright:
v0.10.0,BSD 3-Clause License
v0.10.0,
v0.10.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.10.0,All rights reserved.
v0.10.0,make any access to matplotlib or plt throw an exception
v0.10.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.10.0,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.10.0,Initialize saturation & value; calculate chroma & value shift
v0.10.0,Calculate some intermediate values
v0.10.0,Initialize RGB with same hue & chroma as our color
v0.10.0,Shift the initial RGB values to match value and store
v0.10.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.10.0,clean way of achieving this
v0.10.0,make sure we don't accidentally escape anything in the substitution
v0.10.0,Fetch appropriate color for node
v0.10.0,"red for negative, green for positive"
v0.10.0,in multi-target use mean of targets
v0.10.0,Write node mean CATE
v0.10.0,Write node std of CATE
v0.10.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
v0.10.0,Fetch appropriate color for node
v0.10.0,Write node mean CATE
v0.10.0,Write node mean CATE
v0.10.0,Write recommended treatment and value - cost
v0.10.0,Licensed under the MIT License.
v0.10.0,"since inference objects can be stateful, we must copy it before fitting;"
v0.10.0,otherwise this sequence wouldn't work:
v0.10.0,"est1.fit(..., inference=inf)"
v0.10.0,"est2.fit(..., inference=inf)"
v0.10.0,est1.effect_interval(...)
v0.10.0,because inf now stores state from fitting est2
v0.10.0,This flag is true when names are set in a child class instead
v0.10.0,"If names are set in a child class, add an attribute reflecting that"
v0.10.0,This works only if X is passed as a kwarg
v0.10.0,We plan to enforce X as kwarg only in future releases
v0.10.0,This checks if names have been set in a child class
v0.10.0,"If names were set in a child class, don't do it again"
v0.10.0,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.10.0,call the wrapped fit method
v0.10.0,NOTE: we call inference fit *after* calling the main fit method
v0.10.0,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.10.0,but tensordot can't be applied to this problem because we don't sum over m
v0.10.0,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.10.0,of rows of T was not taken into account
v0.10.0,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.10.0,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.10.0,"Treatment names is None, default to BaseCateEstimator"
v0.10.0,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.10.0,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.10.0,Get input names
v0.10.0,Summary
v0.10.0,add statsmodels to parent's options
v0.10.0,add debiasedlasso to parent's options
v0.10.0,add blb to parent's options
v0.10.0,TODO Share some logic with non-discrete version
v0.10.0,Get input names
v0.10.0,Summary
v0.10.0,add statsmodels to parent's options
v0.10.0,add statsmodels to parent's options
v0.10.0,add blb to parent's options
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,remove None arguments
v0.10.0,"scores entries should be lists of scores, so make each entry a singleton list"
v0.10.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.10.0,generate an instance of the final model
v0.10.0,generate an instance of the nuisance model
v0.10.0,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.10.0,alteration even when we only want to fit_final.
v0.10.0,use a binary array to get stratified split in case of discrete treatment
v0.10.0,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.10.0,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.10.0,"however, sklearn doesn't support both stratifying and grouping (see"
v0.10.0,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.10.0,their own object that supports grouping if they want to use groups.
v0.10.0,for each mc iteration
v0.10.0,for each model under cross fit setting
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,
v0.10.0,This code contains snippets of code from
v0.10.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.10.0,published under the following license and copyright:
v0.10.0,BSD 3-Clause License
v0.10.0,
v0.10.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.10.0,All rights reserved.
v0.10.0,=============================================================================
v0.10.0,Policy Forest
v0.10.0,=============================================================================
v0.10.0,Remap output
v0.10.0,reshape is necessary to preserve the data contiguity against vs
v0.10.0,"[:, np.newaxis] that does not."
v0.10.0,Get subsample sample size
v0.10.0,Check parameters
v0.10.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.10.0,if this is the first `fit` call of the warm start mode.
v0.10.0,"Free allocated memory, if any"
v0.10.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.10.0,We draw from the random state to get the random state we
v0.10.0,would have got if we hadn't used a warm_start.
v0.10.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.10.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.10.0,but would still advance randomness enough so that tree subsamples will be different.
v0.10.0,Parallel loop: we prefer the threading backend as the Cython code
v0.10.0,for fitting the trees is internally releasing the Python GIL
v0.10.0,making threading more efficient than multiprocessing in
v0.10.0,"that case. However, for joblib 0.12+ we respect any"
v0.10.0,"parallel_backend contexts set at a higher level,"
v0.10.0,since correctness does not rely on using threads.
v0.10.0,Collect newly grown trees
v0.10.0,Check data
v0.10.0,Assign chunk of trees to jobs
v0.10.0,avoid storing the output of every estimator by summing them here
v0.10.0,Parallel loop
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,
v0.10.0,This code contains snippets of code from:
v0.10.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.10.0,published under the following license and copyright:
v0.10.0,BSD 3-Clause License
v0.10.0,
v0.10.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.10.0,All rights reserved.
v0.10.0,=============================================================================
v0.10.0,Types and constants
v0.10.0,=============================================================================
v0.10.0,=============================================================================
v0.10.0,Base Policy tree
v0.10.0,=============================================================================
v0.10.0,The values below are required and utilitized by methods in the _SingleTreeExporterMixin
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.10.0,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.10.0,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.10.0,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.10.0,checks that X is 2D array.
v0.10.0,"since we only allow single dimensional y, we could flatten the prediction"
v0.10.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.10.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.10.0,Handles the corner case when X=None but featurizer might be not None
v0.10.0,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.10.0,"Replacing this method which is invalid for this class, so that we make the"
v0.10.0,dosctring empty and not appear in the docs.
v0.10.0,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.10.0,TODO: support sample_var
v0.10.0,Replacing to remove docstring
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"if both X and W are None, just return a column of ones"
v0.10.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.10.0,We need to go back to the label representation of the one-hot so as to call
v0.10.0,the classifier.
v0.10.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.10.0,We need to go back to the label representation of the one-hot so as to call
v0.10.0,the classifier.
v0.10.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.10.0,This works both with our without the weighting trick as the treatments T are unit vector
v0.10.0,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.10.0,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.10.0,both Parametric and Non Parametric DML.
v0.10.0,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.10.0,attribute so that the trained featurizer will be passed through
v0.10.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.10.0,for internal use by the library
v0.10.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.10.0,We need to use the rlearner's copy to retain the information from fitting
v0.10.0,Handles the corner case when X=None but featurizer might be not None
v0.10.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.10.0,since we clone it and fit separate copies
v0.10.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.10.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.10.0,TODO: support sample_var
v0.10.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.10.0,since we clone it and fit separate copies
v0.10.0,add blb to parent's options
v0.10.0,override only so that we can update the docstring to indicate
v0.10.0,support for `GenericSingleTreatmentModelFinalInference`
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,note that groups are not passed to score because they are only used for fitting
v0.10.0,note that groups are not passed to score because they are only used for fitting
v0.10.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.10.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.10.0,NOTE: important to get parent's wrapped copy so that
v0.10.0,"after training wrapped featurizer is also trained, etc."
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.10.0,Fit a doubly robust average effect
v0.10.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.10.0,(which needs to have been expanded if there's a discrete treatment)
v0.10.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.10.0,since we clone it and fit separate copies
v0.10.0,"If custom param grid, check that only estimator parameters are being altered"
v0.10.0,override only so that we can update the docstring to indicate support for `blb`
v0.10.0,Get input names
v0.10.0,Summary
v0.10.0,Determine output settings
v0.10.0,"Important: This must be the first invocation of the random state at fit time, so that"
v0.10.0,train/test splits are re-generatable from an external object simply by knowing the
v0.10.0,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.10.0,linear predictions. Currently is also useful for testing.
v0.10.0,reshape is necessary to preserve the data contiguity against vs
v0.10.0,"[:, np.newaxis] that does not."
v0.10.0,Check parameters
v0.10.0,Set min_weight_leaf from min_weight_fraction_leaf
v0.10.0,Build tree
v0.10.0,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.10.0,hold. Used by criterion for memory space savings.
v0.10.0,Initialize the criterion object and the criterion_val object if honest.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,
v0.10.0,This code is a fork from:
v0.10.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.10.0,published under the following license and copyright:
v0.10.0,BSD 3-Clause License
v0.10.0,
v0.10.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.10.0,All rights reserved.
v0.10.0,Set parameters
v0.10.0,Don't instantiate estimators now! Parameters of base_estimator might
v0.10.0,"still change. Eg., when grid-searching with the nested object syntax."
v0.10.0,self.estimators_ needs to be filled by the derived classes in fit.
v0.10.0,Compute the number of jobs
v0.10.0,Partition estimators between jobs
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Remove children with nonwhite mothers from the treatment group
v0.10.0,Remove children with nonwhite mothers from the treatment group
v0.10.0,Select columns
v0.10.0,Scale the numeric variables
v0.10.0,"Change the binary variable 'first' takes values in {1,2}"
v0.10.0,Append a column of ones as intercept
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.10.0,(which needs to have been expanded if there's a discrete treatment)
v0.10.0,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.10.0,d_t=None here since we measure the effect across all Ts
v0.10.0,once the estimator has been fit
v0.10.0,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.10.0,an intercept even when bias is part of coef.
v0.10.0,We can write effect inference as a function of prediction and prediction standard error of
v0.10.0,the final method for linear models
v0.10.0,squeeze the first axis
v0.10.0,d_t=None here since we measure the effect across all Ts
v0.10.0,set the mean_pred_stderr
v0.10.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.10.0,(which needs to have been expanded if there's a discrete treatment)
v0.10.0,"send treatment to the end, pull bounds to the front"
v0.10.0,d_t=None here since we measure the effect across all Ts
v0.10.0,set the mean_pred_stderr
v0.10.0,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.10.0,d_t=None here since we measure the effect across all Ts
v0.10.0,d_t=None here since we measure the effect across all Ts
v0.10.0,need to set the fit args before the estimator is fit
v0.10.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.10.0,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.10.0,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.10.0,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.10.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.10.0,1. Uncertainty of Mean Point Estimate
v0.10.0,2. Distribution of Point Estimate
v0.10.0,3. Total Variance of Point Estimate
v0.10.0,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.10.0,so bail out early; note that it might be possible to correct the algorithm for
v0.10.0,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.10.0,be clean
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,TODO: Add a __dir__ implementation?
v0.10.0,don't proxy special methods
v0.10.0,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.10.0,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.10.0,then we should be computing a confidence interval for the wrapped calls
v0.10.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.10.0,second level bootstrap which would be prohibitive computationally?
v0.10.0,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.10.0,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.10.0,can't import from econml.inference at top level without creating cyclical dependencies
v0.10.0,Note that inference results are always methods even if the inference is for a property
v0.10.0,(e.g. coef__inference() is a method but coef_ is a property)
v0.10.0,Therefore we must insert a lambda if getting inference for a non-callable
v0.10.0,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.10.0,"try to get interval/std first if appropriate,"
v0.10.0,since we don't prefer a wrapped method with this name
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,
v0.10.0,This code contains snippets of code from:
v0.10.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.10.0,published under the following license and copyright:
v0.10.0,BSD 3-Clause License
v0.10.0,
v0.10.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.10.0,All rights reserved.
v0.10.0,=============================================================================
v0.10.0,Types and constants
v0.10.0,=============================================================================
v0.10.0,=============================================================================
v0.10.0,Base GRF tree
v0.10.0,=============================================================================
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,=============================================================================
v0.10.0,A MultOutputWrapper for GRF classes
v0.10.0,=============================================================================
v0.10.0,=============================================================================
v0.10.0,Instantiations of Generalized Random Forest
v0.10.0,=============================================================================
v0.10.0,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.10.0,in front of the constant treatment is the intercept in the moment equation.
v0.10.0,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.10.0,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,
v0.10.0,This code contains snippets of code from
v0.10.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.10.0,published under the following license and copyright:
v0.10.0,BSD 3-Clause License
v0.10.0,
v0.10.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.10.0,All rights reserved.
v0.10.0,=============================================================================
v0.10.0,Base Generalized Random Forest
v0.10.0,=============================================================================
v0.10.0,Remap output
v0.10.0,reshape is necessary to preserve the data contiguity against vs
v0.10.0,"[:, np.newaxis] that does not."
v0.10.0,reshape is necessary to preserve the data contiguity against vs
v0.10.0,"[:, np.newaxis] that does not."
v0.10.0,Get subsample sample size
v0.10.0,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.10.0,We calculate the min eigenvalue proxy that each criterion is considering
v0.10.0,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.10.0,Check parameters
v0.10.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.10.0,if this is the first `fit` call of the warm start mode.
v0.10.0,"Free allocated memory, if any"
v0.10.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.10.0,We draw from the random state to get the random state we
v0.10.0,would have got if we hadn't used a warm_start.
v0.10.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.10.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.10.0,but would still advance randomness enough so that tree subsamples will be different.
v0.10.0,Generating indices a priori before parallelism ended up being orders of magnitude
v0.10.0,faster than how sklearn does it. The reason is that random samplers do not release the
v0.10.0,gil it seems.
v0.10.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.10.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.10.0,but would still advance randomness enough so that tree subsamples will be different.
v0.10.0,Parallel loop: we prefer the threading backend as the Cython code
v0.10.0,for fitting the trees is internally releasing the Python GIL
v0.10.0,making threading more efficient than multiprocessing in
v0.10.0,"that case. However, for joblib 0.12+ we respect any"
v0.10.0,"parallel_backend contexts set at a higher level,"
v0.10.0,since correctness does not rely on using threads.
v0.10.0,Collect newly grown trees
v0.10.0,Check data
v0.10.0,Assign chunk of trees to jobs
v0.10.0,avoid storing the output of every estimator by summing them here
v0.10.0,Parallel loop
v0.10.0,Check data
v0.10.0,Assign chunk of trees to jobs
v0.10.0,Parallel loop
v0.10.0,Check data
v0.10.0,Assign chunk of trees to jobs
v0.10.0,Parallel loop
v0.10.0,####################
v0.10.0,Variance correction
v0.10.0,####################
v0.10.0,Subtract the average within bag variance. This ends up being equal to the
v0.10.0,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.10.0,The negative part is just sq_between.
v0.10.0,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.10.0,"The off diagonals we have no objective prior, so no correction is applied."
v0.10.0,Finally correcting the pred_cov or pred_var
v0.10.0,avoid storing the output of every estimator by summing them here
v0.10.0,Parallel loop
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,testing importances
v0.10.0,testing heterogeneity importances
v0.10.0,Testing that all parameters do what they are supposed to
v0.10.0,"testing predict, apply and decision path"
v0.10.0,test that the subsampling scheme past to the trees is correct
v0.10.0,test that the estimator calcualtes var correctly
v0.10.0,test api
v0.10.0,test accuracy
v0.10.0,test the projection functionality of forests
v0.10.0,test that the estimator calcualtes var correctly
v0.10.0,test api
v0.10.0,test that the estimator calcualtes var correctly
v0.10.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.10.0,test that we raise errors in mishandled situations.
v0.10.0,test that the subsampling scheme past to the trees is correct
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.10.0,creating notebooks that are annoying for our users to actually run themselves
v0.10.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.10.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.10.0,"prior to calling interpret, can't plot, render, etc."
v0.10.0,can interpret without uncertainty
v0.10.0,can't interpret with uncertainty if inference wasn't used during fit
v0.10.0,can interpret with uncertainty if we refit
v0.10.0,can interpret without uncertainty
v0.10.0,can't treat before interpreting
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,simple DGP only for illustration
v0.10.0,Define the treatment model neural network architecture
v0.10.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.10.0,"so the input shape is (d_z + d_x,)"
v0.10.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.10.0,add extra layers on top for the mixture density network
v0.10.0,Define the response model neural network architecture
v0.10.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.10.0,"so the input shape is (d_t + d_x,)"
v0.10.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.10.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.10.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.10.0,so that the same weights will be reused in each instantiation
v0.10.0,number of samples to use in second estimate of the response
v0.10.0,(to make loss estimate unbiased)
v0.10.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.10.0,do something with predictions...
v0.10.0,also test vector t and y
v0.10.0,simple DGP only for illustration
v0.10.0,Define the treatment model neural network architecture
v0.10.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.10.0,"so the input shape is (d_z + d_x,)"
v0.10.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.10.0,add extra layers on top for the mixture density network
v0.10.0,Define the response model neural network architecture
v0.10.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.10.0,"so the input shape is (d_t + d_x,)"
v0.10.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.10.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.10.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.10.0,so that the same weights will be reused in each instantiation
v0.10.0,number of samples to use in second estimate of the response
v0.10.0,(to make loss estimate unbiased)
v0.10.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.10.0,do something with predictions...
v0.10.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.10.0,test = True ensures we draw test set images
v0.10.0,test = True ensures we draw test set images
v0.10.0,re-draw to get new independent treatment and implied response
v0.10.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.10.0,above is necesary so that reduced form doesn't win
v0.10.0,covariates: time and emotion
v0.10.0,random instrument
v0.10.0,z -> price
v0.10.0,true observable demand function
v0.10.0,errors
v0.10.0,response
v0.10.0,test = True ensures we draw test set images
v0.10.0,test = True ensures we draw test set images
v0.10.0,re-draw to get new independent treatment and implied response
v0.10.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.10.0,above is necesary so that reduced form doesn't win
v0.10.0,covariates: time and emotion
v0.10.0,random instrument
v0.10.0,z -> price
v0.10.0,true observable demand function
v0.10.0,errors
v0.10.0,response
v0.10.0,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.10.0,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.10.0,For some reason this doesn't work at all when run against the CNTK backend...
v0.10.0,"model.compile('nadam', loss=lambda _,l:l)"
v0.10.0,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.10.0,generate a valiation set
v0.10.0,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.10.0,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,DGP constants
v0.10.0,Generate data
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,testing importances
v0.10.0,testing heterogeneity importances
v0.10.0,Testing that all parameters do what they are supposed to
v0.10.0,"testing predict, apply and decision path"
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.10.0,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.10.0,so we need to transpose the result
v0.10.0,1-d output
v0.10.0,2-d output
v0.10.0,Single dimensional output y
v0.10.0,Multi-dimensional output y
v0.10.0,1-d y
v0.10.0,multi-d y
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,test that we can fit with the same arguments as the base estimator
v0.10.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.10.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.10.0,with the same shape for the lower and upper bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,test that we can do the same thing once we provide percentile bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.10.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.10.0,with the same shape for the lower and upper bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,test that we can do the same thing once we provide percentile bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,test that we can fit with the same arguments as the base estimator
v0.10.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.10.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.10.0,with the same shape for the lower and upper bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,test that we can do the same thing once we provide percentile bounds
v0.10.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.10.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.10.0,with the same shape for the lower and upper bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,test that we can do the same thing once we provide percentile bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,test that we can fit with the same arguments as the base estimator
v0.10.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.10.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.10.0,with the same shape for the lower and upper bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,test that we can do the same thing once we provide percentile bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.10.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.10.0,with the same shape for the lower and upper bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,test that we can do the same thing once we provide percentile bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.10.0,with the same shape for the lower and upper bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,test that the estimated effect is usually within the bounds
v0.10.0,test that we can do the same thing once we provide alpha explicitly
v0.10.0,test that the lower and upper bounds differ
v0.10.0,test that the estimated effect is usually within the bounds
v0.10.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.10.0,with the same shape for the lower and upper bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,TODO: test that the estimated effect is usually within the bounds
v0.10.0,and that the true effect is also usually within the bounds
v0.10.0,test that we can do the same thing once we provide percentile bounds
v0.10.0,test that the lower and upper bounds differ
v0.10.0,TODO: test that the estimated effect is usually within the bounds
v0.10.0,and that the true effect is also usually within the bounds
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,test that the subsampling scheme past to the trees is correct
v0.10.0,test that the estimator calcualtes var correctly
v0.10.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.10.0,test that we raise errors in mishandled situations.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,DGP constants
v0.10.0,Generate data
v0.10.0,Test inference results when `cate_feature_names` doesn not exist
v0.10.0,Test inference results when `cate_feature_names` doesn not exist
v0.10.0,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.10.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.10.0,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.10.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.10.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.10.0,pvalue for second column should be greater than zero since some points are on either side
v0.10.0,of the tested value
v0.10.0,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.10.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.10.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.10.0,only is not None when T1 is a constant or a list of constant
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.10.0,Test non keyword based calls to fit
v0.10.0,test non-array inputs
v0.10.0,Test custom splitter
v0.10.0,Test incomplete set of test folds
v0.10.0,"y scores should be positive, since W predicts Y somewhat"
v0.10.0,"t scores might not be, since W and T are uncorrelated"
v0.10.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,make sure cross product varies more slowly with first array
v0.10.0,and that vectors are okay as inputs
v0.10.0,number of inputs in specification must match number of inputs
v0.10.0,must have an output
v0.10.0,output indices must be unique
v0.10.0,output indices must be present in an input
v0.10.0,number of indices must match number of dimensions for each input
v0.10.0,repeated indices must always have consistent sizes
v0.10.0,transpose
v0.10.0,tensordot
v0.10.0,trace
v0.10.0,TODO: set up proper flag for this
v0.10.0,pick indices at random with replacement from the first 7 letters of the alphabet
v0.10.0,"of all of the distinct indices that appear in any input,"
v0.10.0,pick a random subset of them (of size at most 5) to appear in the output
v0.10.0,creating an instance should warn
v0.10.0,using the instance should not warn
v0.10.0,using the deprecated method should warn
v0.10.0,don't warn if b and c are passed by keyword
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Preprocess data
v0.10.0,Convert 'week' to a date
v0.10.0,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.10.0,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.10.0,Take log of price
v0.10.0,Make brand numeric
v0.10.0,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.10.0,which have all zero coeffs
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,test at least one estimator from each category
v0.10.0,test causal graph
v0.10.0,test refutation estimate
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.10.0,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.10.0,TODO: test something rather than just print...
v0.10.0,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.10.0,pick some arbitrary X
v0.10.0,pick some arbitrary T
v0.10.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.10.0,The average variance should be lower when using monte carlo iterations
v0.10.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.10.0,The average variance should be lower when using monte carlo iterations
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,ensure that we've got at least two of every row
v0.10.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.10.0,need to make sure we get all *joint* combinations
v0.10.0,IntentToTreat only supports binary treatments/instruments
v0.10.0,IntentToTreat only supports binary treatments/instruments
v0.10.0,IntentToTreat requires X
v0.10.0,ensure we can serialize unfit estimator
v0.10.0,these support only W but not X
v0.10.0,"these support only binary, not general discrete T and Z"
v0.10.0,ensure we can serialize fit estimator
v0.10.0,make sure we can call the marginal_effect and effect methods
v0.10.0,TODO: add tests for extra properties like coef_ where they exist
v0.10.0,TODO: add tests for extra properties like coef_ where they exist
v0.10.0,"make sure we can call effect with implied scalar treatments,"
v0.10.0,"no matter the dimensions of T, and also that we warn when there"
v0.10.0,are multiple treatments
v0.10.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.10.0,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.10.0,"however, with custom splits the checking happens in the first stage wrapper"
v0.10.0,where we don't have all of the required information to do this;
v0.10.0,we'd probably need to add it to _crossfit instead
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.10.0,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.10.0,The __warningregistry__'s need to be in a pristine state for tests
v0.10.0,to work properly.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Set random seed
v0.10.0,Generate data
v0.10.0,DGP constants
v0.10.0,Test data
v0.10.0,Constant treatment effect
v0.10.0,Constant treatment with multi output Y
v0.10.0,Heterogeneous treatment
v0.10.0,Heterogeneous treatment with multi output Y
v0.10.0,TLearner test
v0.10.0,Instantiate TLearner
v0.10.0,Test inputs
v0.10.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.10.0,Instantiate SLearner
v0.10.0,Test inputs
v0.10.0,Test constant treatment effect
v0.10.0,Test constant treatment effect with multi output Y
v0.10.0,Test heterogeneous treatment effect
v0.10.0,Need interactions between T and features
v0.10.0,Test heterogeneous treatment effect with multi output Y
v0.10.0,Instantiate XLearner
v0.10.0,Test inputs
v0.10.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.10.0,Instantiate DomainAdaptationLearner
v0.10.0,Test inputs
v0.10.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.10.0,Get the true treatment effect
v0.10.0,Get the true treatment effect
v0.10.0,Fit learner and get the effect and marginal effect
v0.10.0,Compute treatment effect residuals (absolute)
v0.10.0,Check that at least 90% of predictions are within tolerance interval
v0.10.0,Check whether the output shape is right
v0.10.0,Check that one can pass in regular lists
v0.10.0,Check that it fails correctly if lists of different shape are passed in
v0.10.0,"Check that it works when T, Y have shape (n, 1)"
v0.10.0,Generate covariates
v0.10.0,Generate treatment
v0.10.0,Calculate outcome
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,DGP constants
v0.10.0,Generate data
v0.10.0,Test data
v0.10.0,Remove warnings that might be raised by the models passed into the ORF
v0.10.0,Generate data with continuous treatments
v0.10.0,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.10.0,does not work well with parallelism.
v0.10.0,Test inputs for continuous treatments
v0.10.0,--> Check that one can pass in regular lists
v0.10.0,--> Check that it fails correctly if lists of different shape are passed in
v0.10.0,Check that outputs have the correct shape
v0.10.0,Test continuous treatments with controls
v0.10.0,Test continuous treatments without controls
v0.10.0,Generate data with binary treatments
v0.10.0,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.10.0,does not work well with parallelism.
v0.10.0,Test inputs for binary treatments
v0.10.0,--> Check that one can pass in regular lists
v0.10.0,--> Check that it fails correctly if lists of different shape are passed in
v0.10.0,"--> Check that it works when T, Y have shape (n, 1)"
v0.10.0,"--> Check that it fails correctly when T has shape (n, 2)"
v0.10.0,--> Check that it fails correctly when the treatments are not numeric
v0.10.0,Check that outputs have the correct shape
v0.10.0,Test binary treatments with controls
v0.10.0,Test binary treatments without controls
v0.10.0,Only applicable to continuous treatments
v0.10.0,Generate data for 2 treatments
v0.10.0,Test multiple treatments with controls
v0.10.0,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.10.0,The rest for controls. Just as an example.
v0.10.0,Generating A/B test data
v0.10.0,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.10.0,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.10.0,Create a wrapper around Lasso that doesn't support weights
v0.10.0,since Lasso does natively support them starting in sklearn 0.23
v0.10.0,Generate data with continuous treatments
v0.10.0,Instantiate model with most of the default parameters
v0.10.0,Compute the treatment effect on test points
v0.10.0,Compute treatment effect residuals
v0.10.0,Multiple treatments
v0.10.0,Allow at most 10% test points to be outside of the tolerance interval
v0.10.0,Compute treatment effect residuals
v0.10.0,Multiple treatments
v0.10.0,Allow at most 20% test points to be outside of the confidence interval
v0.10.0,Check that the intervals are not too wide
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.10.0,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.10.0,ensure that we've got at least 6 of every element
v0.10.0,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.10.0,NOTE: this number may need to change if the default number of folds in
v0.10.0,WeightedStratifiedKFold changes
v0.10.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.10.0,ensure we can serialize the unfit estimator
v0.10.0,ensure we can pickle the fit estimator
v0.10.0,make sure we can call the marginal_effect and effect methods
v0.10.0,test const marginal inference
v0.10.0,test effect inference
v0.10.0,test marginal effect inference
v0.10.0,test coef__inference and intercept__inference
v0.10.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.10.0,"make sure we can call effect with implied scalar treatments,"
v0.10.0,"no matter the dimensions of T, and also that we warn when there"
v0.10.0,are multiple treatments
v0.10.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.10.0,ensure that we've got at least two of every element
v0.10.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.10.0,make sure we can call the marginal_effect and effect methods
v0.10.0,test const marginal inference
v0.10.0,test effect inference
v0.10.0,test marginal effect inference
v0.10.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.10.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.10.0,We concatenate the two copies data
v0.10.0,create a simple artificial setup where effect of moving from treatment
v0.10.0,"1 -> 2 is 2,"
v0.10.0,"1 -> 3 is 1, and"
v0.10.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.10.0,"Using an uneven number of examples from different classes,"
v0.10.0,"and having the treatments in non-lexicographic order,"
v0.10.0,Should rule out some basic issues.
v0.10.0,test that we can fit with a KFold instance
v0.10.0,test that we can fit with a train/test iterable
v0.10.0,predetermined splits ensure that all features are seen in each split
v0.10.0,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.10.0,(incorrectly) use a final model with an intercept
v0.10.0,"Because final model is fixed, actual values of T and Y don't matter"
v0.10.0,Ensure reproducibility
v0.10.0,Sparse DGP
v0.10.0,Treatment effect coef
v0.10.0,Other coefs
v0.10.0,Features and controls
v0.10.0,Test sparse estimator
v0.10.0,"--> test coef_, intercept_"
v0.10.0,--> test treatment effects
v0.10.0,Restrict x_test to vectors of norm < 1
v0.10.0,--> check inference
v0.10.0,Check that a majority of true effects lie in the 5-95% CI
v0.10.0,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.10.0,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.10.0,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.10.0,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.10.0,sparse test case: heterogeneous effect by product
v0.10.0,need at least as many rows in e_y as there are distinct columns
v0.10.0,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.10.0,create a simple artificial setup where effect of moving from treatment
v0.10.0,"a -> b is 2,"
v0.10.0,"a -> c is 1, and"
v0.10.0,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.10.0,"Using an uneven number of examples from different classes,"
v0.10.0,"and having the treatments in non-lexicographic order,"
v0.10.0,should rule out some basic issues.
v0.10.0,Note that explicitly specifying the dtype as object is necessary until
v0.10.0,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.10.0,estimated effects should be identical when treatment is explicitly given
v0.10.0,but const_marginal_effect should be reordered based on the explicit cagetories
v0.10.0,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.10.0,test outer grouping
v0.10.0,test nested grouping
v0.10.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.10.0,whichever groups we saw
v0.10.0,test nested grouping
v0.10.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.10.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.10.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Set random seed
v0.10.0,Generate data
v0.10.0,DGP constants
v0.10.0,Test data
v0.10.0,Constant treatment effect and propensity
v0.10.0,Heterogeneous treatment and propensity
v0.10.0,ensure that we've got at least two of every element
v0.10.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.10.0,ensure that we can serialize unfit estimator
v0.10.0,ensure that we can serialize fit estimator
v0.10.0,make sure we can call the marginal_effect and effect methods
v0.10.0,test const marginal inference
v0.10.0,test effect inference
v0.10.0,test marginal effect inference
v0.10.0,test coef_ and intercept_ inference
v0.10.0,verify we can generate the summary
v0.10.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.10.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.10.0,create a simple artificial setup where effect of moving from treatment
v0.10.0,"1 -> 2 is 2,"
v0.10.0,"1 -> 3 is 1, and"
v0.10.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.10.0,"Using an uneven number of examples from different classes,"
v0.10.0,"and having the treatments in non-lexicographic order,"
v0.10.0,Should rule out some basic issues.
v0.10.0,test that we can fit with a KFold instance
v0.10.0,test that we can fit with a train/test iterable
v0.10.0,"for at least some of the examples, the CI should have nonzero width"
v0.10.0,"for at least some of the examples, the CI should have nonzero width"
v0.10.0,"for at least some of the examples, the CI should have nonzero width"
v0.10.0,test coef__inference function works
v0.10.0,test intercept__inference function works
v0.10.0,test summary function works
v0.10.0,Test inputs
v0.10.0,self._test_inputs(DR_learner)
v0.10.0,Test constant treatment effect
v0.10.0,Test heterogeneous treatment effect
v0.10.0,Test heterogenous treatment effect for W =/= None
v0.10.0,Sparse DGP
v0.10.0,Treatment effect coef
v0.10.0,Other coefs
v0.10.0,Features and controls
v0.10.0,Test sparse estimator
v0.10.0,"--> test coef_, intercept_"
v0.10.0,--> test treatment effects
v0.10.0,Restrict x_test to vectors of norm < 1
v0.10.0,--> check inference
v0.10.0,Check that a majority of true effects lie in the 5-95% CI
v0.10.0,test outer grouping
v0.10.0,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.10.0,test nested grouping
v0.10.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.10.0,whichever groups we saw
v0.10.0,test nested grouping
v0.10.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.10.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.10.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.10.0,helper class
v0.10.0,Fit learner and get the effect
v0.10.0,Get the true treatment effect
v0.10.0,Compute treatment effect residuals (absolute)
v0.10.0,Check that at least 90% of predictions are within tolerance interval
v0.10.0,Only for heterogeneous TE
v0.10.0,Fit learner on X and W and get the effect
v0.10.0,Get the true treatment effect
v0.10.0,Compute treatment effect residuals (absolute)
v0.10.0,Check that at least 90% of predictions are within tolerance interval
v0.10.0,Check that one can pass in regular lists
v0.10.0,Check that it fails correctly if lists of different shape are passed in
v0.10.0,Check that it fails when T contains values other than 0 and 1
v0.10.0,"Check that it works when T, Y have shape (n, 1)"
v0.10.0,Generate covariates
v0.10.0,Generate treatment
v0.10.0,Calculate outcome
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,DGP constants
v0.10.0,DGP coefficients
v0.10.0,Generated outcomes
v0.10.0,################
v0.10.0,WeightedLasso #
v0.10.0,################
v0.10.0,Define weights
v0.10.0,Define extended datasets
v0.10.0,Range of alphas
v0.10.0,Compare with Lasso
v0.10.0,--> No intercept
v0.10.0,--> With intercept
v0.10.0,When DGP has no intercept
v0.10.0,When DGP has intercept
v0.10.0,--> Coerce coefficients to be positive
v0.10.0,--> Toggle max_iter & tol
v0.10.0,Define weights
v0.10.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.10.0,Mixed DGP scenario.
v0.10.0,Define extended datasets
v0.10.0,Define weights
v0.10.0,Define multioutput
v0.10.0,##################
v0.10.0,WeightedLassoCV #
v0.10.0,##################
v0.10.0,Define alphas to test
v0.10.0,Compare with LassoCV
v0.10.0,--> No intercept
v0.10.0,--> With intercept
v0.10.0,--> Force parameters to be positive
v0.10.0,Choose a smaller n to speed-up process
v0.10.0,Compare fold weights
v0.10.0,Define weights
v0.10.0,Define extended datasets
v0.10.0,Define splitters
v0.10.0,WeightedKFold splitter
v0.10.0,Map weighted splitter to an extended splitter
v0.10.0,Define alphas to test
v0.10.0,Compare with LassoCV
v0.10.0,--> No intercept
v0.10.0,--> With intercept
v0.10.0,--> Force parameters to be positive
v0.10.0,###########################
v0.10.0,MultiTaskWeightedLassoCV #
v0.10.0,###########################
v0.10.0,Define alphas to test
v0.10.0,Define splitter
v0.10.0,Compare with MultiTaskLassoCV
v0.10.0,--> No intercept
v0.10.0,--> With intercept
v0.10.0,Define weights
v0.10.0,Define extended datasets
v0.10.0,Define splitters
v0.10.0,WeightedKFold splitter
v0.10.0,Map weighted splitter to an extended splitter
v0.10.0,Define alphas to test
v0.10.0,Compare with LassoCV
v0.10.0,--> No intercept
v0.10.0,--> With intercept
v0.10.0,#########################
v0.10.0,WeightedLassoCVWrapper #
v0.10.0,#########################
v0.10.0,perform 1D fit
v0.10.0,perform 2D fit
v0.10.0,################
v0.10.0,DebiasedLasso #
v0.10.0,################
v0.10.0,Test DebiasedLasso without weights
v0.10.0,--> Check debiased coeffcients without intercept
v0.10.0,--> Check debiased coeffcients with intercept
v0.10.0,--> Check 5-95 CI coverage for unit vectors
v0.10.0,Test DebiasedLasso with weights for one DGP
v0.10.0,Define weights
v0.10.0,Define extended datasets
v0.10.0,--> Check debiased coefficients
v0.10.0,Define weights
v0.10.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.10.0,--> Check debiased coeffcients
v0.10.0,Test that attributes propagate correctly
v0.10.0,Test MultiOutputDebiasedLasso without weights
v0.10.0,--> Check debiased coeffcients without intercept
v0.10.0,--> Check debiased coeffcients with intercept
v0.10.0,--> Check CI coverage
v0.10.0,Test MultiOutputDebiasedLasso with weights
v0.10.0,Define weights
v0.10.0,Define extended datasets
v0.10.0,--> Check debiased coefficients
v0.10.0,Unit vectors
v0.10.0,Unit vectors
v0.10.0,Check coeffcients and intercept are the same within tolerance
v0.10.0,Check results are similar with tolerance 1e-6
v0.10.0,Check if multitask
v0.10.0,Check that same alpha is chosen
v0.10.0,Check that the coefficients are similar
v0.10.0,selective ridge has a simple implementation that we can test against
v0.10.0,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.10.0,"it should be the case that when we set fit_intercept to true,"
v0.10.0,it doesn't matter whether the penalized model also fits an intercept or not
v0.10.0,create an extra copy of rows with weight 2
v0.10.0,"instead of a slice, explicitly return an array of indices"
v0.10.0,_penalized_inds is only set during fitting
v0.10.0,cv exists on penalized model
v0.10.0,now we can access _penalized_inds
v0.10.0,check that we can read the cv attribute back out from the underlying model
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,DGP constants
v0.10.0,Define data features
v0.10.0,Added `_df`to names to be different from the default cate_estimator names
v0.10.0,Generate data
v0.10.0,################################
v0.10.0,Single treatment and outcome #
v0.10.0,################################
v0.10.0,Test LinearDML
v0.10.0,|--> Test featurizers
v0.10.0,ColumnTransformer doesn't propagate column names
v0.10.0,|--> Test re-fit
v0.10.0,Test SparseLinearDML
v0.10.0,Test ForestDML
v0.10.0,###################################
v0.10.0,Mutiple treatments and outcomes #
v0.10.0,###################################
v0.10.0,Test LinearDML
v0.10.0,Test SparseLinearDML
v0.10.0,"Single outcome only, ORF does not support multiple outcomes"
v0.10.0,Test DMLOrthoForest
v0.10.0,Test DROrthoForest
v0.10.0,Test XLearner
v0.10.0,Skipping population summary names test because bootstrap inference is too slow
v0.10.0,Test SLearner
v0.10.0,Test TLearner
v0.10.0,Test LinearDRLearner
v0.10.0,Test SparseLinearDRLearner
v0.10.0,Test ForestDRLearner
v0.10.0,Test LinearIntentToTreatDRIV
v0.10.0,Test DeepIV
v0.10.0,Test categorical treatments
v0.10.0,Check refit
v0.10.0,Check refit after setting categories
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Linear models are required for parametric dml
v0.10.0,sample weighting models are required for nonparametric dml
v0.10.0,Test values
v0.10.0,TLearner test
v0.10.0,Instantiate TLearner
v0.10.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.10.0,Test constant treatment effect with multi output Y
v0.10.0,Test heterogeneous treatment effect
v0.10.0,Need interactions between T and features
v0.10.0,Test heterogeneous treatment effect with multi output Y
v0.10.0,Instantiate DomainAdaptationLearner
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,test base values equals to mean of constant marginal effect
v0.10.0,test shape of shap values output is as expected
v0.10.0,test shape of attribute of explanation object is as expected
v0.10.0,test length of feature names equals to shap values shape
v0.10.0,test base values equals to mean of constant marginal effect
v0.10.0,test shape of shap values output is as expected
v0.10.0,test shape of attribute of explanation object is as expected
v0.10.0,test length of feature names equals to shap values shape
v0.10.0,Treatment effect function
v0.10.0,Outcome support
v0.10.0,Treatment support
v0.10.0,"Generate controls, covariates, treatments and outcomes"
v0.10.0,Heterogeneous treatment effects
v0.10.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.10.0,through shap package.
v0.10.0,test shap could generate the plot from the shap_values
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Check inputs
v0.10.0,Check inputs
v0.10.0,Check inputs
v0.10.0,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.10.0,"Thus, we append the controls column before the one-hot-encoded T"
v0.10.0,"We might want to revisit, though, since it's linearly determined by the others"
v0.10.0,Check inputs
v0.10.0,Check inputs
v0.10.0,Estimate response function
v0.10.0,Check inputs
v0.10.0,Train model on controls. Assign higher weight to units resembling
v0.10.0,treated units.
v0.10.0,Train model on the treated. Assign higher weight to units resembling
v0.10.0,control units.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.10.0,output is
v0.10.0,"* a column of ones if X, W, and Z are all None"
v0.10.0,* just X or W or Z if both of the others are None
v0.10.0,* hstack([arrs]) for whatever subset are not None otherwise
v0.10.0,ensure Z is 2D
v0.10.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.10.0,We need to go back to the label representation of the one-hot so as to call
v0.10.0,the classifier.
v0.10.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.10.0,We need to go back to the label representation of the one-hot so as to call
v0.10.0,the classifier.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,TODO: make sure to use random seeds wherever necessary
v0.10.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.10.0,"unfortunately with the Theano and Tensorflow backends,"
v0.10.0,the straightforward use of K.stop_gradient can cause an error
v0.10.0,because the parameters of the intermediate layers are now disconnected from the loss;
v0.10.0,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.10.0,so that those layers remain connected but with 0 gradient
v0.10.0,|| t - mu_i || ^2
v0.10.0,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.10.0,Use logsumexp for numeric stability:
v0.10.0,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.10.0,TODO: does the numeric stability actually make any difference?
v0.10.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.10.0,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.10.0,generate cumulative sum via matrix multiplication
v0.10.0,"Generate standard uniform values in shape (batch_size,1)"
v0.10.0,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.10.0,we use uniform_like instead with an input of an appropriate shape)
v0.10.0,convert to floats and multiply to perform equivalent of logical AND
v0.10.0,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.10.0,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.10.0,we use normal_like instead with an input of an appropriate shape)
v0.10.0,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.10.0,prevent gradient from passing through sampling
v0.10.0,three options: biased or upper-bound loss require a single number of samples;
v0.10.0,unbiased can take different numbers for the network and its gradient
v0.10.0,"sample: (() -> Layer, int) -> Layer"
v0.10.0,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.10.0,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.10.0,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.10.0,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.10.0,the dimensionality of the output of the network
v0.10.0,TODO: is there a more robust way to do this?
v0.10.0,TODO: do we need to give the user more control over other arguments to fit?
v0.10.0,"subtle point: we need to build a new model each time,"
v0.10.0,because each model encapsulates its randomness
v0.10.0,TODO: do we need to give the user more control over other arguments to fit?
v0.10.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.10.0,not a general tensor (because of how backprop works in every framework)
v0.10.0,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.10.0,but this seems annoying...)
v0.10.0,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.10.0,TODO: any way to get this to work on batches of arbitrary size?
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Estimate final model of theta(X) by minimizing the square loss:
v0.10.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.10.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.10.0,at the expense of some small bias. For points with very small covariance we revert
v0.10.0,to the model-based preliminary estimate and do not add the correction term.
v0.10.0,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.10.0,"instruments, and outcomes"
v0.10.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.10.0,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.10.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.10.0,for internal use by the library
v0.10.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.10.0,"we need to undo the one-hot encoding for calling effect,"
v0.10.0,since it expects raw values
v0.10.0,"we need to undo the one-hot encoding for calling effect,"
v0.10.0,since it expects raw values
v0.10.0,"TODO: check that Y, T, Z do not have multiple columns"
v0.10.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.10.0,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.10.0,since we would actually be calculating a CATE rather than ATE.
v0.10.0,TODO: allow the final model to actually use X?
v0.10.0,TODO: allow the final model to actually use X?
v0.10.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.10.0,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.10.0,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.10.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.10.0,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.10.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.10.0,for internal use by the library
v0.10.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,"this will have dimension (d,) + shape(X)"
v0.10.0,send the first dimension to the end
v0.10.0,columns are featurized independently; partial derivatives are only non-zero
v0.10.0,when taken with respect to the same column each time
v0.10.0,don't fit intercept; manually add column of ones to the data instead;
v0.10.0,this allows us to ignore the intercept when computing marginal effects
v0.10.0,make T 2D if if was a vector
v0.10.0,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.10.0,two stage approximation
v0.10.0,"first, get basis expansions of T, X, and Z"
v0.10.0,TODO: is it right that the effective number of intruments is the
v0.10.0,"product of ft_X and ft_Z, not just ft_Z?"
v0.10.0,"regress T expansion on X,Z expansions concatenated with W"
v0.10.0,"predict ft_T from interacted ft_X, ft_Z"
v0.10.0,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.10.0,dT may be only 2-dimensional)
v0.10.0,promote dT to 3D if necessary (e.g. if T was a vector)
v0.10.0,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,TODO: conisder working around relying on sklearn implementation details
v0.10.0,"Found a good split, return."
v0.10.0,Record all splits in case the stratification by weight yeilds a worse partition
v0.10.0,Reseed random generator and try again
v0.10.0,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.10.0,"Found a good split, return."
v0.10.0,Did not find a good split
v0.10.0,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.10.0,Return most weight-balanced partition
v0.10.0,Weight stratification algorithm
v0.10.0,Sort weights for weight strata search
v0.10.0,There are some leftover indices that have yet to be assigned
v0.10.0,Append stratum splits to overall splits
v0.10.0,"If classification methods produce multiple columns of output,"
v0.10.0,we need to manually encode classes to ensure consistent column ordering.
v0.10.0,We clone the estimator to make sure that all the folds are
v0.10.0,"independent, and that it is pickle-able."
v0.10.0,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.10.0,`predictions` is a list of method outputs from each fold.
v0.10.0,"If each of those is also a list, then treat this as a"
v0.10.0,multioutput-multiclass task. We need to separately concatenate
v0.10.0,the method outputs for each label into an `n_labels` long list.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Our classes that derive from sklearn ones sometimes include
v0.10.0,inherited docstrings that have embedded doctests; we need the following imports
v0.10.0,so that they don't break.
v0.10.0,TODO: consider working around relying on sklearn implementation details
v0.10.0,"Convert X, y into numpy arrays"
v0.10.0,Define fit parameters
v0.10.0,Some algorithms don't have a check_input option
v0.10.0,Check weights array
v0.10.0,Check that weights are size-compatible
v0.10.0,Normalize inputs
v0.10.0,Weight inputs
v0.10.0,Fit base class without intercept
v0.10.0,Fit Lasso
v0.10.0,Reset intercept
v0.10.0,The intercept is not calculated properly due the sqrt(weights) factor
v0.10.0,so it must be recomputed
v0.10.0,Fit lasso without weights
v0.10.0,Make weighted splitter
v0.10.0,Fit weighted model
v0.10.0,Make weighted splitter
v0.10.0,Fit weighted model
v0.10.0,Call weighted lasso on reduced design matrix
v0.10.0,Weighted tau
v0.10.0,Select optimal penalty
v0.10.0,Warn about consistency
v0.10.0,"Convert X, y into numpy arrays"
v0.10.0,Fit weighted lasso with user input
v0.10.0,"Center X, y"
v0.10.0,Calculate quantities that will be used later on. Account for centered data
v0.10.0,Calculate coefficient and error variance
v0.10.0,Add coefficient correction
v0.10.0,Set coefficients and intercept standard errors
v0.10.0,Set intercept
v0.10.0,Return alpha to 'auto' state
v0.10.0,"Note that in the case of no intercept, X_offset is 0"
v0.10.0,Calculate the variance of the predictions
v0.10.0,Calculate prediction confidence intervals
v0.10.0,Assumes flattened y
v0.10.0,Compute weighted residuals
v0.10.0,To be done once per target. Assumes y can be flattened.
v0.10.0,Assumes that X has already been offset
v0.10.0,Special case: n_features=1
v0.10.0,Compute Lasso coefficients for the columns of the design matrix
v0.10.0,Compute C_hat
v0.10.0,Compute theta_hat
v0.10.0,Allow for single output as well
v0.10.0,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.10.0,Set coef_ attribute
v0.10.0,Set intercept_ attribute
v0.10.0,Set selected_alpha_ attribute
v0.10.0,Set coef_stderr_
v0.10.0,intercept_stderr_
v0.10.0,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.10.0,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.10.0,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.10.0,set intercept_ attribute
v0.10.0,set coef_ attribute
v0.10.0,set alpha_ attribute
v0.10.0,set alphas_ attribute
v0.10.0,set n_iter_ attribute
v0.10.0,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.10.0,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.10.0,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.10.0,now regress X1 on y - X2 * beta2 to learn beta1
v0.10.0,set coef_ and intercept_ attributes
v0.10.0,Note that the penalized model should *not* have an intercept
v0.10.0,don't proxy special methods
v0.10.0,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.10.0,regressor incorrectly
v0.10.0,"Note: for known attributes that have been set this method will not be called,"
v0.10.0,so we should just throw here because this is an attribute belonging to this class
v0.10.0,but which hasn't yet been set on this instance
v0.10.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,AzureML
v0.10.0,helper imports
v0.10.0,write the details of the workspace to a configuration file to the notebook library
v0.10.0,if y is a multioutput model
v0.10.0,Make sure second dimension has 1 or more item
v0.10.0,switch _inner Model to a MultiOutputRegressor
v0.10.0,flatten array as automl only takes vectors for y
v0.10.0,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.10.0,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.10.0,as an sklearn estimator
v0.10.0,fit implementation for a single output model.
v0.10.0,Create experiment for specified workspace
v0.10.0,Configure automl_config with training set information.
v0.10.0,"Wait for remote run to complete, the set the model"
v0.10.0,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.10.0,create model and pass model into final.
v0.10.0,"If item is an automl config, get its corresponding"
v0.10.0,AutomatedML Model and add it to new_Args
v0.10.0,"If item is an automl config, get its corresponding"
v0.10.0,AutomatedML Model and set it for this key in
v0.10.0,kwargs
v0.10.0,takes in either automated_ml config and instantiates
v0.10.0,an AutomatedMLModel
v0.10.0,The prefix can only be 18 characters long
v0.10.0,"because prefixes come from kwarg_names, we must ensure they are"
v0.10.0,short enough.
v0.10.0,Get workspace from config file.
v0.10.0,Take the intersect of the white for sample
v0.10.0,weights and linear models
v0.10.0,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,average the outcome dimension if it exists and ensure 2d y_pred
v0.10.0,get index of best treatment
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,TODO: consider working around relying on sklearn implementation details
v0.10.0,Create splits of causal tree
v0.10.0,Make sure the correct exception is being rethrown
v0.10.0,Must make sure indices are merged correctly
v0.10.0,Convert rows to columns
v0.10.0,Require group assignment t to be one-hot-encoded
v0.10.0,Get predictions for the 2 splits
v0.10.0,Must make sure indices are merged correctly
v0.10.0,Crossfitting
v0.10.0,Compute weighted nuisance estimates
v0.10.0,-------------------------------------------------------------------------------
v0.10.0,Calculate the covariance matrix corresponding to the BLB inference
v0.10.0,
v0.10.0,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.10.0,2. Calculate the weighted moments for each tree slice to create a matrix
v0.10.0,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.10.0,in that slice from the overall parameter estimate.
v0.10.0,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.10.0,-------------------------------------------------------------------------------
v0.10.0,Calclulate covariance matrix through BLB
v0.10.0,Estimators
v0.10.0,OrthoForest parameters
v0.10.0,Sub-forests
v0.10.0,Auxiliary attributes
v0.10.0,Fit check
v0.10.0,TODO: Check performance
v0.10.0,Must normalize weights
v0.10.0,Override the CATE inference options
v0.10.0,Add blb inference to parent's options
v0.10.0,Generate subsample indices
v0.10.0,Build trees in parallel
v0.10.0,Bootstraping has repetitions in tree sample
v0.10.0,Similar for `a` weights
v0.10.0,Bootstraping has repetitions in tree sample
v0.10.0,Define subsample size
v0.10.0,Safety check
v0.10.0,Draw points to create little bags
v0.10.0,Copy and/or define models
v0.10.0,Define nuisance estimators
v0.10.0,Define parameter estimators
v0.10.0,Define
v0.10.0,Need to redefine fit here for auto inference to work due to a quirk in how
v0.10.0,wrap_fit is defined
v0.10.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.10.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.10.0,Override to flatten output if T is flat
v0.10.0,Check that all discrete treatments are represented
v0.10.0,Nuissance estimates evaluated with cross-fitting
v0.10.0,Define 2-fold iterator
v0.10.0,Check if there is only one example of some class
v0.10.0,Define 2-fold iterator
v0.10.0,need safe=False when cloning for WeightedModelWrapper
v0.10.0,Compute residuals
v0.10.0,Compute coefficient by OLS on residuals
v0.10.0,"Parameter returned by LinearRegression is (d_T, )"
v0.10.0,Compute residuals
v0.10.0,Compute coefficient by OLS on residuals
v0.10.0,ell_2 regularization
v0.10.0,Ridge regression estimate
v0.10.0,"Parameter returned is of shape (d_T, )"
v0.10.0,Return moments and gradients
v0.10.0,Compute residuals
v0.10.0,Compute moments
v0.10.0,"Moments shape is (n, d_T)"
v0.10.0,Compute moment gradients
v0.10.0,returns shape-conforming residuals
v0.10.0,Copy and/or define models
v0.10.0,Define parameter estimators
v0.10.0,Define moment and mean gradient estimator
v0.10.0,"Check that T is shape (n, )"
v0.10.0,Check T is numeric
v0.10.0,Train label encoder
v0.10.0,Call `fit` from parent class
v0.10.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.10.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.10.0,Override to flatten output if T is flat
v0.10.0,Expand one-hot encoding to include the zero treatment
v0.10.0,"Test that T contains all treatments. If not, return None"
v0.10.0,Nuissance estimates evaluated with cross-fitting
v0.10.0,Define 2-fold iterator
v0.10.0,Check if there is only one example of some class
v0.10.0,No need to crossfit for internal nodes
v0.10.0,Compute partial moments
v0.10.0,"If any of the values in the parameter estimate is nan, return None"
v0.10.0,Compute partial moments
v0.10.0,Compute coefficient by OLS on residuals
v0.10.0,ell_2 regularization
v0.10.0,Ridge regression estimate
v0.10.0,"Parameter returned is of shape (d_T, )"
v0.10.0,Return moments and gradients
v0.10.0,Compute partial moments
v0.10.0,Compute moments
v0.10.0,"Moments shape is (n, d_T-1)"
v0.10.0,Compute moment gradients
v0.10.0,Need to calculate this in an elegant way for when propensity is 0
v0.10.0,This will flatten T
v0.10.0,Check that T is numeric
v0.10.0,Test whether the input estimator is supported
v0.10.0,Calculate confidence intervals for the parameter (marginal effect)
v0.10.0,Calculate confidence intervals for the effect
v0.10.0,Calculate the effects
v0.10.0,Calculate the standard deviations for the effects
v0.10.0,d_t=None here since we measure the effect across all Ts
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.10.0,Licensed under the MIT License.
v0.10.0,Causal tree parameters
v0.10.0,Tree structure
v0.10.0,No need for a random split since the data is already
v0.10.0,a random subsample from the original input
v0.10.0,node list stores the nodes that are yet to be splitted
v0.10.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.10.0,Create local sample set
v0.10.0,Compute nuisance estimates for the current node
v0.10.0,Nuisance estimate cannot be calculated
v0.10.0,Estimate parameter for current node
v0.10.0,Node estimate cannot be calculated
v0.10.0,Calculate moments and gradient of moments for current data
v0.10.0,Calculate inverse gradient
v0.10.0,The gradient matrix is not invertible.
v0.10.0,No good split can be found
v0.10.0,Calculate point-wise pseudo-outcomes rho
v0.10.0,a split is determined by a feature and a sample pair
v0.10.0,the number of possible splits is at most (number of features) * (number of node samples)
v0.10.0,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.10.0,parse row and column of random pair
v0.10.0,the sample of the pair is the integer division of the random number with n_feats
v0.10.0,calculate the binary indicator of whether sample i is on the left or the right
v0.10.0,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.10.0,calculate the number of samples on the left child for each proposed split
v0.10.0,calculate the analogous binary indicator for the samples in the estimation set
v0.10.0,calculate the number of estimation samples on the left child of each proposed split
v0.10.0,find the upper and lower bound on the size of the left split for the split
v0.10.0,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.10.0,on each side.
v0.10.0,similarly for the estimation sample set
v0.10.0,if there is no valid split then don't create any children
v0.10.0,filter only the valid splits
v0.10.0,calculate the average influence vector of the samples in the left child
v0.10.0,calculate the average influence vector of the samples in the right child
v0.10.0,take the square of each of the entries of the influence vectors and normalize
v0.10.0,by size of each child
v0.10.0,calculate the vector score of each candidate split as the average of left and right
v0.10.0,influence vectors
v0.10.0,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.10.0,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.10.0,where there might be large discontinuities in some parameter as the conditioning set varies
v0.10.0,calculate the scalar score of each split by aggregating across the vector of scores
v0.10.0,Find split that minimizes criterion
v0.10.0,Create child nodes with corresponding subsamples
v0.10.0,add the created children to the list of not yet split nodes
v0.9.2,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.9.2,configuration is all pulled from setup.cfg
v0.9.2,-*- coding: utf-8 -*-
v0.9.2,
v0.9.2,Configuration file for the Sphinx documentation builder.
v0.9.2,
v0.9.2,This file does only contain a selection of the most common options. For a
v0.9.2,full list see the documentation:
v0.9.2,http://www.sphinx-doc.org/en/master/config
v0.9.2,-- Path setup --------------------------------------------------------------
v0.9.2,"If extensions (or modules to document with autodoc) are in another directory,"
v0.9.2,add these directories to sys.path here. If the directory is relative to the
v0.9.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.9.2,
v0.9.2,-- Project information -----------------------------------------------------
v0.9.2,-- General configuration ---------------------------------------------------
v0.9.2,"If your documentation needs a minimal Sphinx version, state it here."
v0.9.2,
v0.9.2,needs_sphinx = '1.0'
v0.9.2,"Add any Sphinx extension module names here, as strings. They can be"
v0.9.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.9.2,ones.
v0.9.2,"Add any paths that contain templates here, relative to this directory."
v0.9.2,The suffix(es) of source filenames.
v0.9.2,You can specify multiple suffix as a list of string:
v0.9.2,
v0.9.2,"source_suffix = ['.rst', '.md']"
v0.9.2,The master toctree document.
v0.9.2,The language for content autogenerated by Sphinx. Refer to documentation
v0.9.2,for a list of supported languages.
v0.9.2,
v0.9.2,This is also used if you do content translation via gettext catalogs.
v0.9.2,"Usually you set ""language"" from the command line for these cases."
v0.9.2,"List of patterns, relative to source directory, that match files and"
v0.9.2,directories to ignore when looking for source files.
v0.9.2,This pattern also affects html_static_path and html_extra_path.
v0.9.2,The name of the Pygments (syntax highlighting) style to use.
v0.9.2,-- Options for HTML output -------------------------------------------------
v0.9.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.9.2,a list of builtin themes.
v0.9.2,
v0.9.2,Theme options are theme-specific and customize the look and feel of a theme
v0.9.2,"further.  For a list of options available for each theme, see the"
v0.9.2,documentation.
v0.9.2,
v0.9.2,"Add any paths that contain custom static files (such as style sheets) here,"
v0.9.2,"relative to this directory. They are copied after the builtin static files,"
v0.9.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.9.2,html_static_path = ['_static']
v0.9.2,"Custom sidebar templates, must be a dictionary that maps document names"
v0.9.2,to template names.
v0.9.2,
v0.9.2,The default sidebars (for documents that don't match any pattern) are
v0.9.2,defined by theme itself.  Builtin themes are using these templates by
v0.9.2,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.9.2,'searchbox.html']``.
v0.9.2,
v0.9.2,html_sidebars = {}
v0.9.2,-- Options for HTMLHelp output ---------------------------------------------
v0.9.2,Output file base name for HTML help builder.
v0.9.2,-- Options for LaTeX output ------------------------------------------------
v0.9.2,The paper size ('letterpaper' or 'a4paper').
v0.9.2,
v0.9.2,"'papersize': 'letterpaper',"
v0.9.2,"The font size ('10pt', '11pt' or '12pt')."
v0.9.2,
v0.9.2,"'pointsize': '10pt',"
v0.9.2,Additional stuff for the LaTeX preamble.
v0.9.2,
v0.9.2,"'preamble': '',"
v0.9.2,Latex figure (float) alignment
v0.9.2,
v0.9.2,"'figure_align': 'htbp',"
v0.9.2,Grouping the document tree into LaTeX files. List of tuples
v0.9.2,"(source start file, target name, title,"
v0.9.2,"author, documentclass [howto, manual, or own class])."
v0.9.2,-- Options for manual page output ------------------------------------------
v0.9.2,One entry per manual page. List of tuples
v0.9.2,"(source start file, name, description, authors, manual section)."
v0.9.2,-- Options for Texinfo output ----------------------------------------------
v0.9.2,Grouping the document tree into Texinfo files. List of tuples
v0.9.2,"(source start file, target name, title, author,"
v0.9.2,"dir menu entry, description, category)"
v0.9.2,-- Options for Epub output -------------------------------------------------
v0.9.2,Bibliographic Dublin Core info.
v0.9.2,The unique identifier of the text. This can be a ISBN number
v0.9.2,or the project homepage.
v0.9.2,
v0.9.2,epub_identifier = ''
v0.9.2,A unique identification for the text.
v0.9.2,
v0.9.2,epub_uid = ''
v0.9.2,A list of files that should not be packed into the epub file.
v0.9.2,-- Extension configuration -------------------------------------------------
v0.9.2,-- Options for intersphinx extension ---------------------------------------
v0.9.2,Example configuration for intersphinx: refer to the Python standard library.
v0.9.2,-- Options for todo extension ----------------------------------------------
v0.9.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.9.2,-- Options for doctest extension -------------------------------------------
v0.9.2,we can document otherwise excluded entities here by returning False
v0.9.2,or skip otherwise included entities by returning True
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Calculate residuals
v0.9.2,Estimate E[T_res | Z_res]
v0.9.2,TODO. Deal with multi-class instrument
v0.9.2,Calculate nuisances
v0.9.2,Estimate E[T_res | Z_res]
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"We do a three way split, as typically a preliminary theta estimator would require"
v0.9.2,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.9.2,TODO. Deal with multi-class instrument
v0.9.2,Estimate final model of theta(X) by minimizing the square loss:
v0.9.2,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.9.2,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.9.2,at the expense of some small bias. For points with very small covariance we revert
v0.9.2,to the model-based preliminary estimate and do not add the correction term.
v0.9.2,Estimate preliminary theta in cross fitting manner
v0.9.2,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.2,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.9.2,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.9.2,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.2,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.9.2,TODO. The solution below is not really a valid cross-fitting
v0.9.2,as the test data are used to create the proj_t on the train
v0.9.2,which in the second train-test loop is used to create the nuisance
v0.9.2,cov on the test data. Hence the T variable of some sample
v0.9.2,"is implicitly correlated with its cov nuisance, through this flow"
v0.9.2,"of information. However, this seems a rather weak correlation."
v0.9.2,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.9.2,model.
v0.9.2,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.9.2,Estimate preliminary theta in cross fitting manner
v0.9.2,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.2,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.9.2,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.2,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.9.2,#############################################################################
v0.9.2,Classes for the DRIV implementation for the special case of intent-to-treat
v0.9.2,A/B test
v0.9.2,#############################################################################
v0.9.2,Estimate preliminary theta in cross fitting manner
v0.9.2,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.2,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.2,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.2,We can use statsmodel for all hypothesis testing capabilities
v0.9.2,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.9.2,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.9.2,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.9.2,model_T_XZ = lambda: model_clf()
v0.9.2,#'days_visited': lambda:
v0.9.2,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.9.2,Turn strings into categories for numeric mapping
v0.9.2,### Defining some generic regressors and classifiers
v0.9.2,This a generic non-parametric regressor
v0.9.2,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.9.2,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.9.2,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.9.2,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.9.2,model = lambda: RandomForestRegressor(n_estimators=100)
v0.9.2,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.9.2,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.9.2,model = lambda: LinearRegression(n_jobs=-1)
v0.9.2,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.9.2,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.9.2,underlying model whenever predict is called.
v0.9.2,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.9.2,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.9.2,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.9.2,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.9.2,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.9.2,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.9.2,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.9.2,We need to specify models to be used for each of these residualizations
v0.9.2,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.9.2,"E[T | X, Z]"
v0.9.2,E[TZ | X]
v0.9.2,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.9.2,n_splits determines the number of splits to be used for cross-fitting.
v0.9.2,# Algorithm 2 - Current Method
v0.9.2,In[121]:
v0.9.2,# Algorithm 3 - DRIV ATE
v0.9.2,dmliv_model_effect = lambda: model()
v0.9.2,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.9.2,"dmliv_model_effect(),"
v0.9.2,n_splits=1)
v0.9.2,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.9.2,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.9.2,"Once multiple treatments are supported, we'll need to fix this"
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.2,We can use statsmodel for all hypothesis testing capabilities
v0.9.2,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.2,We can use statsmodel for all hypothesis testing capabilities
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,TODO. Deal with multi-class instrument/treatment
v0.9.2,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.9.2,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.9.2,Estimate p(X) = E[T | X] in cross-fitting manner
v0.9.2,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.9.2,##################
v0.9.2,Global settings #
v0.9.2,##################
v0.9.2,Global plotting controls
v0.9.2,"Control for support size, can control for more"
v0.9.2,#################
v0.9.2,File utilities #
v0.9.2,#################
v0.9.2,#################
v0.9.2,Plotting utils #
v0.9.2,#################
v0.9.2,bias
v0.9.2,var
v0.9.2,rmse
v0.9.2,r2
v0.9.2,Infer feature dimension
v0.9.2,Metrics by support plots
v0.9.2,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.9.2,Vasilis Syrgkanis <vasy@microsoft.com>
v0.9.2,Steven Wu <zhiww@microsoft.com>
v0.9.2,Initialize causal tree parameters
v0.9.2,Create splits of causal tree
v0.9.2,Estimate treatment effects at the leafs
v0.9.2,Compute heterogeneous treatement effect for x's in x_list by finding
v0.9.2,the corresponding split and associating the effect computed on that leaf
v0.9.2,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.9.2,Safety check
v0.9.2,Weighted linear regression
v0.9.2,Calculates weights
v0.9.2,Bootstraping has repetitions in tree sample so we need to iterate
v0.9.2,over all indices
v0.9.2,Similar for `a` weights
v0.9.2,Doesn't have sample weights
v0.9.2,Is a linear model
v0.9.2,Weighted linear regression
v0.9.2,Calculates weights
v0.9.2,Bootstraping has repetitions in tree sample so we need to iterate
v0.9.2,over all indices
v0.9.2,Similar for `a` weights
v0.9.2,normalize weights
v0.9.2,"Split the data in half, train and test"
v0.9.2,Fit with LassoCV the treatment as a function of W and the outcome as
v0.9.2,"a function of W, using only the train fold"
v0.9.2,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.9.2,"Split the data in half, train and test"
v0.9.2,Fit with LassoCV the treatment as a function of W and the outcome as
v0.9.2,"a function of W, using only the train fold"
v0.9.2,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.9.2,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.9.2,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.9.2,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.9.2,"Split the data in half, train and test"
v0.9.2,Fit with LassoCV the treatment as a function of x and the outcome as
v0.9.2,"a function of x, using only the train fold"
v0.9.2,Then compute residuals p-g(x) and q-q(x) on test fold
v0.9.2,Compute coefficient by OLS on residuals
v0.9.2,"Split the data in half, train and test"
v0.9.2,Fit with LassoCV the treatment as a function of x and the outcome as
v0.9.2,"a function of x, using only the train fold"
v0.9.2,Then compute residuals p-g(x) and q-q(x) on test fold
v0.9.2,Estimate multipliers for second order orthogonal method
v0.9.2,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.9.2,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.9.2,Create local sample set
v0.9.2,compute the base estimate for the current node using double ml or second order double ml
v0.9.2,compute the influence functions here that are used for the criterion
v0.9.2,generate random proposals of dimensions to split
v0.9.2,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.9.2,compute criterion for each proposal
v0.9.2,if splitting creates valid leafs in terms of mean leaf size
v0.9.2,Calculate criterion for split
v0.9.2,Else set criterion to infinity so that this split is not chosen
v0.9.2,If no good split was found
v0.9.2,Find split that minimizes criterion
v0.9.2,Set the split attributes at the node
v0.9.2,Create child nodes with corresponding subsamples
v0.9.2,Recursively split children
v0.9.2,Return parent node
v0.9.2,estimate the local parameter at the leaf using the estimate data
v0.9.2,###################
v0.9.2,Argument parsing #
v0.9.2,###################
v0.9.2,#########################################
v0.9.2,Parameters constant across experiments #
v0.9.2,#########################################
v0.9.2,Outcome support
v0.9.2,Treatment support
v0.9.2,Evaluation grid
v0.9.2,Treatment effects array
v0.9.2,Other variables
v0.9.2,##########################
v0.9.2,Data Generating Process #
v0.9.2,##########################
v0.9.2,Log iteration
v0.9.2,"Generate controls, features, treatment and outcome"
v0.9.2,T and Y residuals to be used in later scripts
v0.9.2,Save generated dataset
v0.9.2,#################
v0.9.2,ORF parameters #
v0.9.2,#################
v0.9.2,######################################
v0.9.2,Train and evaluate treatment effect #
v0.9.2,######################################
v0.9.2,########
v0.9.2,Plots #
v0.9.2,########
v0.9.2,###############
v0.9.2,Save results #
v0.9.2,###############
v0.9.2,##############
v0.9.2,Run Rscript #
v0.9.2,##############
v0.9.2,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.9.2,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.9.2,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.9.2,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.9.2,def mlasso_model(): return MultiTaskLassoCV(
v0.9.2,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.9.2,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.9.2,heterogeneity
v0.9.2,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.9.2,heterogeneity
v0.9.2,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.9.2,heterogeneity
v0.9.2,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.9.2,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.9.2,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.9.2,subset of features that are exogenous and create heterogeneity
v0.9.2,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.9.2,subset of features wrt we estimate heterogeneity
v0.9.2,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.9.2,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,introspect the constructor arguments to find the model parameters
v0.9.2,to represent
v0.9.2,Extract and sort argument names excluding 'self'
v0.9.2,create dataframe
v0.9.2,currently dowhy only support single outcome and single treatment
v0.9.2,column names
v0.9.2,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.9.2,cate estimator but not the effect.
v0.9.2,don't proxy special methods
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Check if model is sparse enough for this model
v0.9.2,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.9.2,TODO: any way to avoid creating a copy if the array was already dense?
v0.9.2,"the call is necessary if the input was something like a list, though"
v0.9.2,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.9.2,so convert to pydata sparse first
v0.9.2,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.9.2,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.9.2,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.9.2,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.9.2,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.9.2,For when checking input values is disabled
v0.9.2,Type to column extraction function
v0.9.2,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.9.2,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.9.2,Get feature names using featurizer
v0.9.2,All attempts at retrieving transformed feature names have failed
v0.9.2,Delegate handling to downstream logic
v0.9.2,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.9.2,same number of input definitions as arrays
v0.9.2,input definitions have same number of dimensions as each array
v0.9.2,all result indices are unique
v0.9.2,all result indices must match at least one input index
v0.9.2,"map indices to all array, axis pairs for that index"
v0.9.2,each index has the same cardinality wherever it appears
v0.9.2,"State: list of (set of letters, list of (corresponding indices, value))"
v0.9.2,Algo: while list contains more than one entry
v0.9.2,take two entries
v0.9.2,sort both lists by intersection of their indices
v0.9.2,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.9.2,"take the union of indices and the product of values), stepping through each list linearly"
v0.9.2,TODO: might be faster to break into connected components first
v0.9.2,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.9.2,"so compute their content separately, then take cartesian product"
v0.9.2,this would save a few pointless sorts by empty tuples
v0.9.2,TODO: Consider investigating other performance ideas for these cases
v0.9.2,where the dense method beat the sparse method (usually sparse is faster)
v0.9.2,"e,facd,c->cfed"
v0.9.2,sparse: 0.0335489
v0.9.2,dense:  0.011465999999999997
v0.9.2,"gbd,da,egb->da"
v0.9.2,sparse: 0.0791625
v0.9.2,dense:  0.007319099999999995
v0.9.2,"dcc,d,faedb,c->abe"
v0.9.2,sparse: 1.2868097
v0.9.2,dense:  0.44605229999999985
v0.9.2,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.9.2,TODO: would using einsum's paths to optimize the order of merging help?
v0.9.2,assume that we should perform nested cross-validation if and only if
v0.9.2,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.9.2,logic copied from check_cv
v0.9.2,otherwise we will assume the user already set the cv attribute to something
v0.9.2,compatible with splitting with a 'groups' argument
v0.9.2,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.9.2,don't use the groups when calling split internally
v0.9.2,Normalize weights
v0.9.2,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.9.2,"if we're decorating a class, just update the __init__ method,"
v0.9.2,so that the result is still a class instead of a wrapper method
v0.9.2,"want to enforce that each bad_arg was either in kwargs,"
v0.9.2,or else it was in neither and is just taking its default value
v0.9.2,Any access should throw
v0.9.2,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.2,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.2,input feature name is already updated by cate_feature_names.
v0.9.2,define the index of d_x to filter for each given T
v0.9.2,filter X after broadcast with T for each given T
v0.9.2,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.2,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.2,Licensed under the MIT License.
v0.9.2,"since inference objects can be stateful, we must copy it before fitting;"
v0.9.2,otherwise this sequence wouldn't work:
v0.9.2,"est1.fit(..., inference=inf)"
v0.9.2,"est2.fit(..., inference=inf)"
v0.9.2,est1.effect_interval(...)
v0.9.2,because inf now stores state from fitting est2
v0.9.2,This flag is true when names are set in a child class instead
v0.9.2,"If names are set in a child class, add an attribute reflecting that"
v0.9.2,This works only if X is passed as a kwarg
v0.9.2,We plan to enforce X as kwarg only in future releases
v0.9.2,This checks if names have been set in a child class
v0.9.2,"If names were set in a child class, don't do it again"
v0.9.2,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.9.2,call the wrapped fit method
v0.9.2,NOTE: we call inference fit *after* calling the main fit method
v0.9.2,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.9.2,but tensordot can't be applied to this problem because we don't sum over m
v0.9.2,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.9.2,of rows of T was not taken into account
v0.9.2,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.9.2,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.9.2,"Treatment names is None, default to BaseCateEstimator"
v0.9.2,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.9.2,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.9.2,Get input names
v0.9.2,Summary
v0.9.2,add statsmodels to parent's options
v0.9.2,add debiasedlasso to parent's options
v0.9.2,add blb to parent's options
v0.9.2,TODO Share some logic with non-discrete version
v0.9.2,Get input names
v0.9.2,Summary
v0.9.2,add statsmodels to parent's options
v0.9.2,add statsmodels to parent's options
v0.9.2,add blb to parent's options
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,remove None arguments
v0.9.2,"scores entries should be lists of scores, so make each entry a singleton list"
v0.9.2,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.9.2,generate an instance of the final model
v0.9.2,generate an instance of the nuisance model
v0.9.2,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.9.2,alteration even when we only want to fit_final.
v0.9.2,use a binary array to get stratified split in case of discrete treatment
v0.9.2,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.9.2,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.9.2,"however, sklearn doesn't support both stratifying and grouping (see"
v0.9.2,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.9.2,their own object that supports grouping if they want to use groups.
v0.9.2,######################################################
v0.9.2,These should be removed once `n_splits` is deprecated
v0.9.2,######################################################
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.9.2,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.9.2,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.9.2,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.9.2,checks that X is 2D array.
v0.9.2,"since we only allow single dimensional y, we could flatten the prediction"
v0.9.2,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.2,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.2,Handles the corner case when X=None but featurizer might be not None
v0.9.2,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.9.2,"Replacing this method which is invalid for this class, so that we make the"
v0.9.2,dosctring empty and not appear in the docs.
v0.9.2,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.9.2,TODO: support sample_var
v0.9.2,Replacing to remove docstring
v0.9.2,###################################################################
v0.9.2,Everything below should be removed once parameters are deprecated
v0.9.2,###################################################################
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"if both X and W are None, just return a column of ones"
v0.9.2,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.2,We need to go back to the label representation of the one-hot so as to call
v0.9.2,the classifier.
v0.9.2,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.2,We need to go back to the label representation of the one-hot so as to call
v0.9.2,the classifier.
v0.9.2,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.9.2,This works both with our without the weighting trick as the treatments T are unit vector
v0.9.2,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.9.2,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.9.2,both Parametric and Non Parametric DML.
v0.9.2,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.9.2,attribute so that the trained featurizer will be passed through
v0.9.2,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.2,for internal use by the library
v0.9.2,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.2,We need to use the rlearner's copy to retain the information from fitting
v0.9.2,Handles the corner case when X=None but featurizer might be not None
v0.9.2,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.2,since we clone it and fit separate copies
v0.9.2,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.2,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.2,TODO: support sample_var
v0.9.2,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.2,since we clone it and fit separate copies
v0.9.2,add blb to parent's options
v0.9.2,override only so that we can update the docstring to indicate
v0.9.2,support for `GenericSingleTreatmentModelFinalInference`
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,note that groups are not passed to score because they are only used for fitting
v0.9.2,note that groups are not passed to score because they are only used for fitting
v0.9.2,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.2,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.2,NOTE: important to get parent's wrapped copy so that
v0.9.2,"after training wrapped featurizer is also trained, etc."
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.9.2,Fit a doubly robust average effect
v0.9.2,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.2,(which needs to have been expanded if there's a discrete treatment)
v0.9.2,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.2,since we clone it and fit separate copies
v0.9.2,"If custom param grid, check that only estimator parameters are being altered"
v0.9.2,override only so that we can update the docstring to indicate support for `blb`
v0.9.2,Get input names
v0.9.2,Summary
v0.9.2,######################################################
v0.9.2,These should be removed once `n_splits` is deprecated
v0.9.2,######################################################
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Remove children with nonwhite mothers from the treatment group
v0.9.2,Remove children with nonwhite mothers from the treatment group
v0.9.2,Select columns
v0.9.2,Scale the numeric variables
v0.9.2,"Change the binary variable 'first' takes values in {1,2}"
v0.9.2,Append a column of ones as intercept
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.2,(which needs to have been expanded if there's a discrete treatment)
v0.9.2,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.9.2,d_t=None here since we measure the effect across all Ts
v0.9.2,once the estimator has been fit
v0.9.2,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.9.2,an intercept even when bias is part of coef.
v0.9.2,We can write effect inference as a function of prediction and prediction standard error of
v0.9.2,the final method for linear models
v0.9.2,squeeze the first axis
v0.9.2,d_t=None here since we measure the effect across all Ts
v0.9.2,set the mean_pred_stderr
v0.9.2,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.2,(which needs to have been expanded if there's a discrete treatment)
v0.9.2,"send treatment to the end, pull bounds to the front"
v0.9.2,d_t=None here since we measure the effect across all Ts
v0.9.2,set the mean_pred_stderr
v0.9.2,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.9.2,d_t=None here since we measure the effect across all Ts
v0.9.2,d_t=None here since we measure the effect across all Ts
v0.9.2,need to set the fit args before the estimator is fit
v0.9.2,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.9.2,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.9.2,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.9.2,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.9.2,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.9.2,1. Uncertainty of Mean Point Estimate
v0.9.2,2. Distribution of Point Estimate
v0.9.2,3. Total Variance of Point Estimate
v0.9.2,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.9.2,so bail out early; note that it might be possible to correct the algorithm for
v0.9.2,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.9.2,be clean
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,TODO: Add a __dir__ implementation?
v0.9.2,don't proxy special methods
v0.9.2,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.9.2,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.9.2,then we should be computing a confidence interval for the wrapped calls
v0.9.2,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.9.2,second level bootstrap which would be prohibitive computationally?
v0.9.2,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.9.2,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.9.2,can't import from econml.inference at top level without creating cyclical dependencies
v0.9.2,Note that inference results are always methods even if the inference is for a property
v0.9.2,(e.g. coef__inference() is a method but coef_ is a property)
v0.9.2,Therefore we must insert a lambda if getting inference for a non-callable
v0.9.2,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.9.2,"try to get interval/std first if appropriate,"
v0.9.2,since we don't prefer a wrapped method with this name
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,
v0.9.2,This code is a fork from:
v0.9.2,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.9.2,published under the following license and copyright:
v0.9.2,BSD 3-Clause License
v0.9.2,
v0.9.2,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.2,All rights reserved.
v0.9.2,Set parameters
v0.9.2,Don't instantiate estimators now! Parameters of base_estimator might
v0.9.2,"still change. Eg., when grid-searching with the nested object syntax."
v0.9.2,self.estimators_ needs to be filled by the derived classes in fit.
v0.9.2,Compute the number of jobs
v0.9.2,Partition estimators between jobs
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,
v0.9.2,This code contains snippets of code from:
v0.9.2,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.9.2,published under the following license and copyright:
v0.9.2,BSD 3-Clause License
v0.9.2,
v0.9.2,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.2,All rights reserved.
v0.9.2,=============================================================================
v0.9.2,Types and constants
v0.9.2,=============================================================================
v0.9.2,=============================================================================
v0.9.2,Base GRF tree
v0.9.2,=============================================================================
v0.9.2,Determine output settings
v0.9.2,"Important: This must be the first invocation of the random state at fit time, so that"
v0.9.2,train/test splits are re-generatable from an external object simply by knowing the
v0.9.2,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.9.2,linear predictions. Currently is also useful for testing.
v0.9.2,reshape is necessary to preserve the data contiguity against vs
v0.9.2,"[:, np.newaxis] that does not."
v0.9.2,Check parameters
v0.9.2,Set min_weight_leaf from min_weight_fraction_leaf
v0.9.2,Build tree
v0.9.2,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.9.2,hold. Used by criterion for memory space savings.
v0.9.2,Initialize the criterion object and the criterion_val object if honest.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,=============================================================================
v0.9.2,A MultOutputWrapper for GRF classes
v0.9.2,=============================================================================
v0.9.2,=============================================================================
v0.9.2,Instantiations of Generalized Random Forest
v0.9.2,=============================================================================
v0.9.2,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.9.2,in front of the constant treatment is the intercept in the moment equation.
v0.9.2,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.9.2,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,
v0.9.2,This code contains snippets of code from
v0.9.2,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.9.2,published under the following license and copyright:
v0.9.2,BSD 3-Clause License
v0.9.2,
v0.9.2,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.2,All rights reserved.
v0.9.2,=============================================================================
v0.9.2,Base Generalized Random Forest
v0.9.2,=============================================================================
v0.9.2,Remap output
v0.9.2,reshape is necessary to preserve the data contiguity against vs
v0.9.2,"[:, np.newaxis] that does not."
v0.9.2,reshape is necessary to preserve the data contiguity against vs
v0.9.2,"[:, np.newaxis] that does not."
v0.9.2,Get subsample sample size
v0.9.2,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.9.2,We calculate the min eigenvalue proxy that each criterion is considering
v0.9.2,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.9.2,Check parameters
v0.9.2,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.9.2,if this is the first `fit` call of the warm start mode.
v0.9.2,"Free allocated memory, if any"
v0.9.2,the below are needed to replicate randomness of subsampling when warm_start=True
v0.9.2,We draw from the random state to get the random state we
v0.9.2,would have got if we hadn't used a warm_start.
v0.9.2,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.9.2,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.9.2,but would still advance randomness enough so that tree subsamples will be different.
v0.9.2,Generating indices a priori before parallelism ended up being orders of magnitude
v0.9.2,faster than how sklearn does it. The reason is that random samplers do not release the
v0.9.2,gil it seems.
v0.9.2,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.9.2,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.9.2,but would still advance randomness enough so that tree subsamples will be different.
v0.9.2,Parallel loop: we prefer the threading backend as the Cython code
v0.9.2,for fitting the trees is internally releasing the Python GIL
v0.9.2,making threading more efficient than multiprocessing in
v0.9.2,"that case. However, for joblib 0.12+ we respect any"
v0.9.2,"parallel_backend contexts set at a higher level,"
v0.9.2,since correctness does not rely on using threads.
v0.9.2,Collect newly grown trees
v0.9.2,Check data
v0.9.2,Assign chunk of trees to jobs
v0.9.2,avoid storing the output of every estimator by summing them here
v0.9.2,Parallel loop
v0.9.2,Check data
v0.9.2,Assign chunk of trees to jobs
v0.9.2,Parallel loop
v0.9.2,Check data
v0.9.2,Assign chunk of trees to jobs
v0.9.2,Parallel loop
v0.9.2,####################
v0.9.2,Variance correction
v0.9.2,####################
v0.9.2,Subtract the average within bag variance. This ends up being equal to the
v0.9.2,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.9.2,The negative part is just sq_between.
v0.9.2,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.9.2,"The off diagonals we have no objective prior, so no correction is applied."
v0.9.2,Finally correcting the pred_cov or pred_var
v0.9.2,avoid storing the output of every estimator by summing them here
v0.9.2,Parallel loop
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,testing importances
v0.9.2,testing heterogeneity importances
v0.9.2,Testing that all parameters do what they are supposed to
v0.9.2,"testing predict, apply and decision path"
v0.9.2,test that the subsampling scheme past to the trees is correct
v0.9.2,test that the estimator calcualtes var correctly
v0.9.2,test api
v0.9.2,test accuracy
v0.9.2,test the projection functionality of forests
v0.9.2,test that the estimator calcualtes var correctly
v0.9.2,test api
v0.9.2,test that the estimator calcualtes var correctly
v0.9.2,"test that the estimator accepts lists, tuples and pandas data frames"
v0.9.2,test that we raise errors in mishandled situations.
v0.9.2,test that the subsampling scheme past to the trees is correct
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.9.2,creating notebooks that are annoying for our users to actually run themselves
v0.9.2,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.9.2,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.9.2,"prior to calling interpret, can't plot, render, etc."
v0.9.2,can interpret without uncertainty
v0.9.2,can't interpret with uncertainty if inference wasn't used during fit
v0.9.2,can interpret with uncertainty if we refit
v0.9.2,can interpret without uncertainty
v0.9.2,can't treat before interpreting
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,simple DGP only for illustration
v0.9.2,Define the treatment model neural network architecture
v0.9.2,"This will take the concatenation of one-dimensional values z and x as input,"
v0.9.2,"so the input shape is (d_z + d_x,)"
v0.9.2,The exact shape of the final layer is not critical because the Deep IV framework will
v0.9.2,add extra layers on top for the mixture density network
v0.9.2,Define the response model neural network architecture
v0.9.2,"This will take the concatenation of one-dimensional values t and x as input,"
v0.9.2,"so the input shape is (d_t + d_x,)"
v0.9.2,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.9.2,"NOTE: For the response model, it is important to define the model *outside*"
v0.9.2,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.9.2,so that the same weights will be reused in each instantiation
v0.9.2,number of samples to use in second estimate of the response
v0.9.2,(to make loss estimate unbiased)
v0.9.2,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.9.2,do something with predictions...
v0.9.2,also test vector t and y
v0.9.2,simple DGP only for illustration
v0.9.2,Define the treatment model neural network architecture
v0.9.2,"This will take the concatenation of one-dimensional values z and x as input,"
v0.9.2,"so the input shape is (d_z + d_x,)"
v0.9.2,The exact shape of the final layer is not critical because the Deep IV framework will
v0.9.2,add extra layers on top for the mixture density network
v0.9.2,Define the response model neural network architecture
v0.9.2,"This will take the concatenation of one-dimensional values t and x as input,"
v0.9.2,"so the input shape is (d_t + d_x,)"
v0.9.2,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.9.2,"NOTE: For the response model, it is important to define the model *outside*"
v0.9.2,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.9.2,so that the same weights will be reused in each instantiation
v0.9.2,number of samples to use in second estimate of the response
v0.9.2,(to make loss estimate unbiased)
v0.9.2,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.9.2,do something with predictions...
v0.9.2,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.9.2,test = True ensures we draw test set images
v0.9.2,test = True ensures we draw test set images
v0.9.2,re-draw to get new independent treatment and implied response
v0.9.2,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.9.2,above is necesary so that reduced form doesn't win
v0.9.2,covariates: time and emotion
v0.9.2,random instrument
v0.9.2,z -> price
v0.9.2,true observable demand function
v0.9.2,errors
v0.9.2,response
v0.9.2,test = True ensures we draw test set images
v0.9.2,test = True ensures we draw test set images
v0.9.2,re-draw to get new independent treatment and implied response
v0.9.2,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.9.2,above is necesary so that reduced form doesn't win
v0.9.2,covariates: time and emotion
v0.9.2,random instrument
v0.9.2,z -> price
v0.9.2,true observable demand function
v0.9.2,errors
v0.9.2,response
v0.9.2,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.9.2,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.9.2,For some reason this doesn't work at all when run against the CNTK backend...
v0.9.2,"model.compile('nadam', loss=lambda _,l:l)"
v0.9.2,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.9.2,generate a valiation set
v0.9.2,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.9.2,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,DGP constants
v0.9.2,Generate data
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,testing importances
v0.9.2,testing heterogeneity importances
v0.9.2,Testing that all parameters do what they are supposed to
v0.9.2,"testing predict, apply and decision path"
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.9.2,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.9.2,so we need to transpose the result
v0.9.2,1-d output
v0.9.2,2-d output
v0.9.2,Single dimensional output y
v0.9.2,Multi-dimensional output y
v0.9.2,1-d y
v0.9.2,multi-d y
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,test that we can fit with the same arguments as the base estimator
v0.9.2,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.2,with the same shape for the lower and upper bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,test that we can do the same thing once we provide percentile bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.2,with the same shape for the lower and upper bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,test that we can do the same thing once we provide percentile bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,test that we can fit with the same arguments as the base estimator
v0.9.2,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.2,with the same shape for the lower and upper bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,test that we can do the same thing once we provide percentile bounds
v0.9.2,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.2,with the same shape for the lower and upper bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,test that we can do the same thing once we provide percentile bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,test that we can fit with the same arguments as the base estimator
v0.9.2,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.2,with the same shape for the lower and upper bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,test that we can do the same thing once we provide percentile bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.2,with the same shape for the lower and upper bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,test that we can do the same thing once we provide percentile bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.2,with the same shape for the lower and upper bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,test that the estimated effect is usually within the bounds
v0.9.2,test that we can do the same thing once we provide alpha explicitly
v0.9.2,test that the lower and upper bounds differ
v0.9.2,test that the estimated effect is usually within the bounds
v0.9.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.2,with the same shape for the lower and upper bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,TODO: test that the estimated effect is usually within the bounds
v0.9.2,and that the true effect is also usually within the bounds
v0.9.2,test that we can do the same thing once we provide percentile bounds
v0.9.2,test that the lower and upper bounds differ
v0.9.2,TODO: test that the estimated effect is usually within the bounds
v0.9.2,and that the true effect is also usually within the bounds
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,DGP constants
v0.9.2,Generate data
v0.9.2,Test inference results when `cate_feature_names` doesn not exist
v0.9.2,Test inference results when `cate_feature_names` doesn not exist
v0.9.2,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.9.2,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.2,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.9.2,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.2,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.9.2,pvalue for second column should be greater than zero since some points are on either side
v0.9.2,of the tested value
v0.9.2,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.9.2,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.2,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.9.2,only is not None when T1 is a constant or a list of constant
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.9.2,Test non keyword based calls to fit
v0.9.2,test non-array inputs
v0.9.2,Test custom splitter
v0.9.2,Test incomplete set of test folds
v0.9.2,"y scores should be positive, since W predicts Y somewhat"
v0.9.2,"t scores might not be, since W and T are uncorrelated"
v0.9.2,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,make sure cross product varies more slowly with first array
v0.9.2,and that vectors are okay as inputs
v0.9.2,number of inputs in specification must match number of inputs
v0.9.2,must have an output
v0.9.2,output indices must be unique
v0.9.2,output indices must be present in an input
v0.9.2,number of indices must match number of dimensions for each input
v0.9.2,repeated indices must always have consistent sizes
v0.9.2,transpose
v0.9.2,tensordot
v0.9.2,trace
v0.9.2,TODO: set up proper flag for this
v0.9.2,pick indices at random with replacement from the first 7 letters of the alphabet
v0.9.2,"of all of the distinct indices that appear in any input,"
v0.9.2,pick a random subset of them (of size at most 5) to appear in the output
v0.9.2,creating an instance should warn
v0.9.2,using the instance should not warn
v0.9.2,using the deprecated method should warn
v0.9.2,don't warn if b and c are passed by keyword
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Preprocess data
v0.9.2,Convert 'week' to a date
v0.9.2,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.9.2,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.9.2,Take log of price
v0.9.2,Make brand numeric
v0.9.2,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.9.2,which have all zero coeffs
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,test at least one estimator from each category
v0.9.2,test causal graph
v0.9.2,test refutation estimate
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.9.2,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.9.2,TODO: test something rather than just print...
v0.9.2,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.9.2,pick some arbitrary X
v0.9.2,pick some arbitrary T
v0.9.2,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.9.2,The average variance should be lower when using monte carlo iterations
v0.9.2,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.9.2,The average variance should be lower when using monte carlo iterations
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,ensure that we've got at least two of every row
v0.9.2,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.2,need to make sure we get all *joint* combinations
v0.9.2,IntentToTreat only supports binary treatments/instruments
v0.9.2,IntentToTreat only supports binary treatments/instruments
v0.9.2,IntentToTreat requires X
v0.9.2,ensure we can serialize unfit estimator
v0.9.2,these support only W but not X
v0.9.2,"these support only binary, not general discrete T and Z"
v0.9.2,ensure we can serialize fit estimator
v0.9.2,make sure we can call the marginal_effect and effect methods
v0.9.2,TODO: add tests for extra properties like coef_ where they exist
v0.9.2,TODO: add tests for extra properties like coef_ where they exist
v0.9.2,"make sure we can call effect with implied scalar treatments,"
v0.9.2,"no matter the dimensions of T, and also that we warn when there"
v0.9.2,are multiple treatments
v0.9.2,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.2,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.9.2,"however, with custom splits the checking happens in the first stage wrapper"
v0.9.2,where we don't have all of the required information to do this;
v0.9.2,we'd probably need to add it to _crossfit instead
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.9.2,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.9.2,The __warningregistry__'s need to be in a pristine state for tests
v0.9.2,to work properly.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Set random seed
v0.9.2,Generate data
v0.9.2,DGP constants
v0.9.2,Test data
v0.9.2,Constant treatment effect
v0.9.2,Constant treatment with multi output Y
v0.9.2,Heterogeneous treatment
v0.9.2,Heterogeneous treatment with multi output Y
v0.9.2,TLearner test
v0.9.2,Instantiate TLearner
v0.9.2,Test inputs
v0.9.2,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.2,Instantiate SLearner
v0.9.2,Test inputs
v0.9.2,Test constant treatment effect
v0.9.2,Test constant treatment effect with multi output Y
v0.9.2,Test heterogeneous treatment effect
v0.9.2,Need interactions between T and features
v0.9.2,Test heterogeneous treatment effect with multi output Y
v0.9.2,Instantiate XLearner
v0.9.2,Test inputs
v0.9.2,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.2,Instantiate DomainAdaptationLearner
v0.9.2,Test inputs
v0.9.2,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.2,Get the true treatment effect
v0.9.2,Get the true treatment effect
v0.9.2,Fit learner and get the effect and marginal effect
v0.9.2,Compute treatment effect residuals (absolute)
v0.9.2,Check that at least 90% of predictions are within tolerance interval
v0.9.2,Check whether the output shape is right
v0.9.2,Check that one can pass in regular lists
v0.9.2,Check that it fails correctly if lists of different shape are passed in
v0.9.2,"Check that it works when T, Y have shape (n, 1)"
v0.9.2,Generate covariates
v0.9.2,Generate treatment
v0.9.2,Calculate outcome
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,DGP constants
v0.9.2,Generate data
v0.9.2,Test data
v0.9.2,Remove warnings that might be raised by the models passed into the ORF
v0.9.2,Generate data with continuous treatments
v0.9.2,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.9.2,does not work well with parallelism.
v0.9.2,Test inputs for continuous treatments
v0.9.2,--> Check that one can pass in regular lists
v0.9.2,--> Check that it fails correctly if lists of different shape are passed in
v0.9.2,Check that outputs have the correct shape
v0.9.2,Test continuous treatments with controls
v0.9.2,Test continuous treatments without controls
v0.9.2,Generate data with binary treatments
v0.9.2,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.9.2,does not work well with parallelism.
v0.9.2,Test inputs for binary treatments
v0.9.2,--> Check that one can pass in regular lists
v0.9.2,--> Check that it fails correctly if lists of different shape are passed in
v0.9.2,"--> Check that it works when T, Y have shape (n, 1)"
v0.9.2,"--> Check that it fails correctly when T has shape (n, 2)"
v0.9.2,--> Check that it fails correctly when the treatments are not numeric
v0.9.2,Check that outputs have the correct shape
v0.9.2,Test binary treatments with controls
v0.9.2,Test binary treatments without controls
v0.9.2,Only applicable to continuous treatments
v0.9.2,Generate data for 2 treatments
v0.9.2,Test multiple treatments with controls
v0.9.2,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.9.2,The rest for controls. Just as an example.
v0.9.2,Generating A/B test data
v0.9.2,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.9.2,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.9.2,Create a wrapper around Lasso that doesn't support weights
v0.9.2,since Lasso does natively support them starting in sklearn 0.23
v0.9.2,Generate data with continuous treatments
v0.9.2,Instantiate model with most of the default parameters
v0.9.2,Compute the treatment effect on test points
v0.9.2,Compute treatment effect residuals
v0.9.2,Multiple treatments
v0.9.2,Allow at most 10% test points to be outside of the tolerance interval
v0.9.2,Compute treatment effect residuals
v0.9.2,Multiple treatments
v0.9.2,Allow at most 20% test points to be outside of the confidence interval
v0.9.2,Check that the intervals are not too wide
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.9.2,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.9.2,ensure that we've got at least 6 of every element
v0.9.2,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.9.2,NOTE: this number may need to change if the default number of folds in
v0.9.2,WeightedStratifiedKFold changes
v0.9.2,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.2,ensure we can serialize the unfit estimator
v0.9.2,ensure we can pickle the fit estimator
v0.9.2,make sure we can call the marginal_effect and effect methods
v0.9.2,test const marginal inference
v0.9.2,test effect inference
v0.9.2,test marginal effect inference
v0.9.2,test coef__inference and intercept__inference
v0.9.2,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.2,"make sure we can call effect with implied scalar treatments,"
v0.9.2,"no matter the dimensions of T, and also that we warn when there"
v0.9.2,are multiple treatments
v0.9.2,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.2,ensure that we've got at least two of every element
v0.9.2,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.2,make sure we can call the marginal_effect and effect methods
v0.9.2,test const marginal inference
v0.9.2,test effect inference
v0.9.2,test marginal effect inference
v0.9.2,"make sure we can call effect with implied scalar treatments, no matter the"
v0.9.2,"dimensions of T, and also that we warn when there are multiple treatments"
v0.9.2,We concatenate the two copies data
v0.9.2,create a simple artificial setup where effect of moving from treatment
v0.9.2,"1 -> 2 is 2,"
v0.9.2,"1 -> 3 is 1, and"
v0.9.2,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.9.2,"Using an uneven number of examples from different classes,"
v0.9.2,"and having the treatments in non-lexicographic order,"
v0.9.2,Should rule out some basic issues.
v0.9.2,test that we can fit with a KFold instance
v0.9.2,test that we can fit with a train/test iterable
v0.9.2,predetermined splits ensure that all features are seen in each split
v0.9.2,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.9.2,(incorrectly) use a final model with an intercept
v0.9.2,"Because final model is fixed, actual values of T and Y don't matter"
v0.9.2,Ensure reproducibility
v0.9.2,Sparse DGP
v0.9.2,Treatment effect coef
v0.9.2,Other coefs
v0.9.2,Features and controls
v0.9.2,Test sparse estimator
v0.9.2,"--> test coef_, intercept_"
v0.9.2,--> test treatment effects
v0.9.2,Restrict x_test to vectors of norm < 1
v0.9.2,--> check inference
v0.9.2,Check that a majority of true effects lie in the 5-95% CI
v0.9.2,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.9.2,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.9.2,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.9.2,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.9.2,sparse test case: heterogeneous effect by product
v0.9.2,need at least as many rows in e_y as there are distinct columns
v0.9.2,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.9.2,create a simple artificial setup where effect of moving from treatment
v0.9.2,"a -> b is 2,"
v0.9.2,"a -> c is 1, and"
v0.9.2,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.9.2,"Using an uneven number of examples from different classes,"
v0.9.2,"and having the treatments in non-lexicographic order,"
v0.9.2,should rule out some basic issues.
v0.9.2,Note that explicitly specifying the dtype as object is necessary until
v0.9.2,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.9.2,estimated effects should be identical when treatment is explicitly given
v0.9.2,but const_marginal_effect should be reordered based on the explicit cagetories
v0.9.2,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.9.2,test outer grouping
v0.9.2,test nested grouping
v0.9.2,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.9.2,whichever groups we saw
v0.9.2,test nested grouping
v0.9.2,"by default, we use 5 split cross-validation for our T and Y models"
v0.9.2,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.9.2,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Set random seed
v0.9.2,Generate data
v0.9.2,DGP constants
v0.9.2,Test data
v0.9.2,Constant treatment effect and propensity
v0.9.2,Heterogeneous treatment and propensity
v0.9.2,ensure that we've got at least two of every element
v0.9.2,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.2,ensure that we can serialize unfit estimator
v0.9.2,ensure that we can serialize fit estimator
v0.9.2,make sure we can call the marginal_effect and effect methods
v0.9.2,test const marginal inference
v0.9.2,test effect inference
v0.9.2,test marginal effect inference
v0.9.2,test coef_ and intercept_ inference
v0.9.2,verify we can generate the summary
v0.9.2,"make sure we can call effect with implied scalar treatments, no matter the"
v0.9.2,"dimensions of T, and also that we warn when there are multiple treatments"
v0.9.2,create a simple artificial setup where effect of moving from treatment
v0.9.2,"1 -> 2 is 2,"
v0.9.2,"1 -> 3 is 1, and"
v0.9.2,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.9.2,"Using an uneven number of examples from different classes,"
v0.9.2,"and having the treatments in non-lexicographic order,"
v0.9.2,Should rule out some basic issues.
v0.9.2,test that we can fit with a KFold instance
v0.9.2,test that we can fit with a train/test iterable
v0.9.2,"for at least some of the examples, the CI should have nonzero width"
v0.9.2,"for at least some of the examples, the CI should have nonzero width"
v0.9.2,"for at least some of the examples, the CI should have nonzero width"
v0.9.2,test coef__inference function works
v0.9.2,test intercept__inference function works
v0.9.2,test summary function works
v0.9.2,Test inputs
v0.9.2,self._test_inputs(DR_learner)
v0.9.2,Test constant treatment effect
v0.9.2,Test heterogeneous treatment effect
v0.9.2,Test heterogenous treatment effect for W =/= None
v0.9.2,Sparse DGP
v0.9.2,Treatment effect coef
v0.9.2,Other coefs
v0.9.2,Features and controls
v0.9.2,Test sparse estimator
v0.9.2,"--> test coef_, intercept_"
v0.9.2,--> test treatment effects
v0.9.2,Restrict x_test to vectors of norm < 1
v0.9.2,--> check inference
v0.9.2,Check that a majority of true effects lie in the 5-95% CI
v0.9.2,test outer grouping
v0.9.2,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.9.2,test nested grouping
v0.9.2,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.9.2,whichever groups we saw
v0.9.2,test nested grouping
v0.9.2,"by default, we use 5 split cross-validation for our T and Y models"
v0.9.2,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.9.2,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.9.2,helper class
v0.9.2,Fit learner and get the effect
v0.9.2,Get the true treatment effect
v0.9.2,Compute treatment effect residuals (absolute)
v0.9.2,Check that at least 90% of predictions are within tolerance interval
v0.9.2,Only for heterogeneous TE
v0.9.2,Fit learner on X and W and get the effect
v0.9.2,Get the true treatment effect
v0.9.2,Compute treatment effect residuals (absolute)
v0.9.2,Check that at least 90% of predictions are within tolerance interval
v0.9.2,Check that one can pass in regular lists
v0.9.2,Check that it fails correctly if lists of different shape are passed in
v0.9.2,Check that it fails when T contains values other than 0 and 1
v0.9.2,"Check that it works when T, Y have shape (n, 1)"
v0.9.2,Generate covariates
v0.9.2,Generate treatment
v0.9.2,Calculate outcome
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,DGP constants
v0.9.2,DGP coefficients
v0.9.2,Generated outcomes
v0.9.2,################
v0.9.2,WeightedLasso #
v0.9.2,################
v0.9.2,Define weights
v0.9.2,Define extended datasets
v0.9.2,Range of alphas
v0.9.2,Compare with Lasso
v0.9.2,--> No intercept
v0.9.2,--> With intercept
v0.9.2,When DGP has no intercept
v0.9.2,When DGP has intercept
v0.9.2,--> Coerce coefficients to be positive
v0.9.2,--> Toggle max_iter & tol
v0.9.2,Define weights
v0.9.2,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.9.2,Mixed DGP scenario.
v0.9.2,Define extended datasets
v0.9.2,Define weights
v0.9.2,Define multioutput
v0.9.2,##################
v0.9.2,WeightedLassoCV #
v0.9.2,##################
v0.9.2,Define alphas to test
v0.9.2,Compare with LassoCV
v0.9.2,--> No intercept
v0.9.2,--> With intercept
v0.9.2,--> Force parameters to be positive
v0.9.2,Choose a smaller n to speed-up process
v0.9.2,Compare fold weights
v0.9.2,Define weights
v0.9.2,Define extended datasets
v0.9.2,Define splitters
v0.9.2,WeightedKFold splitter
v0.9.2,Map weighted splitter to an extended splitter
v0.9.2,Define alphas to test
v0.9.2,Compare with LassoCV
v0.9.2,--> No intercept
v0.9.2,--> With intercept
v0.9.2,--> Force parameters to be positive
v0.9.2,###########################
v0.9.2,MultiTaskWeightedLassoCV #
v0.9.2,###########################
v0.9.2,Define alphas to test
v0.9.2,Define splitter
v0.9.2,Compare with MultiTaskLassoCV
v0.9.2,--> No intercept
v0.9.2,--> With intercept
v0.9.2,Define weights
v0.9.2,Define extended datasets
v0.9.2,Define splitters
v0.9.2,WeightedKFold splitter
v0.9.2,Map weighted splitter to an extended splitter
v0.9.2,Define alphas to test
v0.9.2,Compare with LassoCV
v0.9.2,--> No intercept
v0.9.2,--> With intercept
v0.9.2,#########################
v0.9.2,WeightedLassoCVWrapper #
v0.9.2,#########################
v0.9.2,perform 1D fit
v0.9.2,perform 2D fit
v0.9.2,################
v0.9.2,DebiasedLasso #
v0.9.2,################
v0.9.2,Test DebiasedLasso without weights
v0.9.2,--> Check debiased coeffcients without intercept
v0.9.2,--> Check debiased coeffcients with intercept
v0.9.2,--> Check 5-95 CI coverage for unit vectors
v0.9.2,Test DebiasedLasso with weights for one DGP
v0.9.2,Define weights
v0.9.2,Define extended datasets
v0.9.2,--> Check debiased coefficients
v0.9.2,Define weights
v0.9.2,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.9.2,--> Check debiased coeffcients
v0.9.2,Test that attributes propagate correctly
v0.9.2,Test MultiOutputDebiasedLasso without weights
v0.9.2,--> Check debiased coeffcients without intercept
v0.9.2,--> Check debiased coeffcients with intercept
v0.9.2,--> Check CI coverage
v0.9.2,Test MultiOutputDebiasedLasso with weights
v0.9.2,Define weights
v0.9.2,Define extended datasets
v0.9.2,--> Check debiased coefficients
v0.9.2,Unit vectors
v0.9.2,Unit vectors
v0.9.2,Check coeffcients and intercept are the same within tolerance
v0.9.2,Check results are similar with tolerance 1e-6
v0.9.2,Check if multitask
v0.9.2,Check that same alpha is chosen
v0.9.2,Check that the coefficients are similar
v0.9.2,selective ridge has a simple implementation that we can test against
v0.9.2,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.9.2,"it should be the case that when we set fit_intercept to true,"
v0.9.2,it doesn't matter whether the penalized model also fits an intercept or not
v0.9.2,create an extra copy of rows with weight 2
v0.9.2,"instead of a slice, explicitly return an array of indices"
v0.9.2,_penalized_inds is only set during fitting
v0.9.2,cv exists on penalized model
v0.9.2,now we can access _penalized_inds
v0.9.2,check that we can read the cv attribute back out from the underlying model
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,DGP constants
v0.9.2,Define data features
v0.9.2,Added `_df`to names to be different from the default cate_estimator names
v0.9.2,Generate data
v0.9.2,################################
v0.9.2,Single treatment and outcome #
v0.9.2,################################
v0.9.2,Test LinearDML
v0.9.2,|--> Test featurizers
v0.9.2,ColumnTransformer doesn't propagate column names
v0.9.2,|--> Test re-fit
v0.9.2,Test SparseLinearDML
v0.9.2,Test ForestDML
v0.9.2,###################################
v0.9.2,Mutiple treatments and outcomes #
v0.9.2,###################################
v0.9.2,Test LinearDML
v0.9.2,Test SparseLinearDML
v0.9.2,"Single outcome only, ORF does not support multiple outcomes"
v0.9.2,Test DMLOrthoForest
v0.9.2,Test DROrthoForest
v0.9.2,Test XLearner
v0.9.2,Skipping population summary names test because bootstrap inference is too slow
v0.9.2,Test SLearner
v0.9.2,Test TLearner
v0.9.2,Test LinearDRLearner
v0.9.2,Test SparseLinearDRLearner
v0.9.2,Test ForestDRLearner
v0.9.2,Test LinearIntentToTreatDRIV
v0.9.2,Test DeepIV
v0.9.2,Test categorical treatments
v0.9.2,Check refit
v0.9.2,Check refit after setting categories
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Linear models are required for parametric dml
v0.9.2,sample weighting models are required for nonparametric dml
v0.9.2,Test values
v0.9.2,TLearner test
v0.9.2,Instantiate TLearner
v0.9.2,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.2,Test constant treatment effect with multi output Y
v0.9.2,Test heterogeneous treatment effect
v0.9.2,Need interactions between T and features
v0.9.2,Test heterogeneous treatment effect with multi output Y
v0.9.2,Instantiate DomainAdaptationLearner
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,test base values equals to mean of constant marginal effect
v0.9.2,test shape of shap values output is as expected
v0.9.2,test shape of attribute of explanation object is as expected
v0.9.2,test length of feature names equals to shap values shape
v0.9.2,test base values equals to mean of constant marginal effect
v0.9.2,test shape of shap values output is as expected
v0.9.2,test shape of attribute of explanation object is as expected
v0.9.2,test length of feature names equals to shap values shape
v0.9.2,Treatment effect function
v0.9.2,Outcome support
v0.9.2,Treatment support
v0.9.2,"Generate controls, covariates, treatments and outcomes"
v0.9.2,Heterogeneous treatment effects
v0.9.2,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.9.2,through shap package.
v0.9.2,test shap could generate the plot from the shap_values
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Check inputs
v0.9.2,Check inputs
v0.9.2,Check inputs
v0.9.2,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.9.2,"Thus, we append the controls column before the one-hot-encoded T"
v0.9.2,"We might want to revisit, though, since it's linearly determined by the others"
v0.9.2,Check inputs
v0.9.2,Check inputs
v0.9.2,Estimate response function
v0.9.2,Check inputs
v0.9.2,Train model on controls. Assign higher weight to units resembling
v0.9.2,treated units.
v0.9.2,Train model on the treated. Assign higher weight to units resembling
v0.9.2,control units.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.9.2,output is
v0.9.2,"* a column of ones if X, W, and Z are all None"
v0.9.2,* just X or W or Z if both of the others are None
v0.9.2,* hstack([arrs]) for whatever subset are not None otherwise
v0.9.2,ensure Z is 2D
v0.9.2,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.2,We need to go back to the label representation of the one-hot so as to call
v0.9.2,the classifier.
v0.9.2,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.2,We need to go back to the label representation of the one-hot so as to call
v0.9.2,the classifier.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,TODO: make sure to use random seeds wherever necessary
v0.9.2,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.9.2,"unfortunately with the Theano and Tensorflow backends,"
v0.9.2,the straightforward use of K.stop_gradient can cause an error
v0.9.2,because the parameters of the intermediate layers are now disconnected from the loss;
v0.9.2,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.9.2,so that those layers remain connected but with 0 gradient
v0.9.2,|| t - mu_i || ^2
v0.9.2,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.9.2,Use logsumexp for numeric stability:
v0.9.2,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.9.2,TODO: does the numeric stability actually make any difference?
v0.9.2,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.9.2,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.9.2,generate cumulative sum via matrix multiplication
v0.9.2,"Generate standard uniform values in shape (batch_size,1)"
v0.9.2,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.9.2,we use uniform_like instead with an input of an appropriate shape)
v0.9.2,convert to floats and multiply to perform equivalent of logical AND
v0.9.2,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.9.2,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.9.2,we use normal_like instead with an input of an appropriate shape)
v0.9.2,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.9.2,prevent gradient from passing through sampling
v0.9.2,three options: biased or upper-bound loss require a single number of samples;
v0.9.2,unbiased can take different numbers for the network and its gradient
v0.9.2,"sample: (() -> Layer, int) -> Layer"
v0.9.2,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.9.2,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.9.2,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.9.2,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.9.2,the dimensionality of the output of the network
v0.9.2,TODO: is there a more robust way to do this?
v0.9.2,TODO: do we need to give the user more control over other arguments to fit?
v0.9.2,"subtle point: we need to build a new model each time,"
v0.9.2,because each model encapsulates its randomness
v0.9.2,TODO: do we need to give the user more control over other arguments to fit?
v0.9.2,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.9.2,not a general tensor (because of how backprop works in every framework)
v0.9.2,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.9.2,but this seems annoying...)
v0.9.2,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.9.2,TODO: any way to get this to work on batches of arbitrary size?
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Estimate final model of theta(X) by minimizing the square loss:
v0.9.2,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.9.2,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.9.2,at the expense of some small bias. For points with very small covariance we revert
v0.9.2,to the model-based preliminary estimate and do not add the correction term.
v0.9.2,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.9.2,"instruments, and outcomes"
v0.9.2,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.9.2,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.9.2,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.2,for internal use by the library
v0.9.2,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.2,"we need to undo the one-hot encoding for calling effect,"
v0.9.2,since it expects raw values
v0.9.2,"we need to undo the one-hot encoding for calling effect,"
v0.9.2,since it expects raw values
v0.9.2,"TODO: check that Y, T, Z do not have multiple columns"
v0.9.2,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.9.2,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.9.2,since we would actually be calculating a CATE rather than ATE.
v0.9.2,TODO: allow the final model to actually use X?
v0.9.2,TODO: allow the final model to actually use X?
v0.9.2,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.2,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.9.2,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.9.2,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.2,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.2,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.2,for internal use by the library
v0.9.2,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,"this will have dimension (d,) + shape(X)"
v0.9.2,send the first dimension to the end
v0.9.2,columns are featurized independently; partial derivatives are only non-zero
v0.9.2,when taken with respect to the same column each time
v0.9.2,don't fit intercept; manually add column of ones to the data instead;
v0.9.2,this allows us to ignore the intercept when computing marginal effects
v0.9.2,make T 2D if if was a vector
v0.9.2,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.9.2,two stage approximation
v0.9.2,"first, get basis expansions of T, X, and Z"
v0.9.2,TODO: is it right that the effective number of intruments is the
v0.9.2,"product of ft_X and ft_Z, not just ft_Z?"
v0.9.2,"regress T expansion on X,Z expansions concatenated with W"
v0.9.2,"predict ft_T from interacted ft_X, ft_Z"
v0.9.2,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.9.2,dT may be only 2-dimensional)
v0.9.2,promote dT to 3D if necessary (e.g. if T was a vector)
v0.9.2,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,TODO: conisder working around relying on sklearn implementation details
v0.9.2,"Found a good split, return."
v0.9.2,Record all splits in case the stratification by weight yeilds a worse partition
v0.9.2,Reseed random generator and try again
v0.9.2,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.9.2,"Found a good split, return."
v0.9.2,Did not find a good split
v0.9.2,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.9.2,Return most weight-balanced partition
v0.9.2,Weight stratification algorithm
v0.9.2,Sort weights for weight strata search
v0.9.2,There are some leftover indices that have yet to be assigned
v0.9.2,Append stratum splits to overall splits
v0.9.2,"If classification methods produce multiple columns of output,"
v0.9.2,we need to manually encode classes to ensure consistent column ordering.
v0.9.2,We clone the estimator to make sure that all the folds are
v0.9.2,"independent, and that it is pickle-able."
v0.9.2,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.9.2,`predictions` is a list of method outputs from each fold.
v0.9.2,"If each of those is also a list, then treat this as a"
v0.9.2,multioutput-multiclass task. We need to separately concatenate
v0.9.2,the method outputs for each label into an `n_labels` long list.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Our classes that derive from sklearn ones sometimes include
v0.9.2,inherited docstrings that have embedded doctests; we need the following imports
v0.9.2,so that they don't break.
v0.9.2,TODO: consider working around relying on sklearn implementation details
v0.9.2,"Convert X, y into numpy arrays"
v0.9.2,Define fit parameters
v0.9.2,Some algorithms don't have a check_input option
v0.9.2,Check weights array
v0.9.2,Check that weights are size-compatible
v0.9.2,Normalize inputs
v0.9.2,Weight inputs
v0.9.2,Fit base class without intercept
v0.9.2,Fit Lasso
v0.9.2,Reset intercept
v0.9.2,The intercept is not calculated properly due the sqrt(weights) factor
v0.9.2,so it must be recomputed
v0.9.2,Fit lasso without weights
v0.9.2,Make weighted splitter
v0.9.2,Fit weighted model
v0.9.2,Make weighted splitter
v0.9.2,Fit weighted model
v0.9.2,Call weighted lasso on reduced design matrix
v0.9.2,Weighted tau
v0.9.2,Select optimal penalty
v0.9.2,Warn about consistency
v0.9.2,"Convert X, y into numpy arrays"
v0.9.2,Fit weighted lasso with user input
v0.9.2,"Center X, y"
v0.9.2,Calculate quantities that will be used later on. Account for centered data
v0.9.2,Calculate coefficient and error variance
v0.9.2,Add coefficient correction
v0.9.2,Set coefficients and intercept standard errors
v0.9.2,Set intercept
v0.9.2,Return alpha to 'auto' state
v0.9.2,"Note that in the case of no intercept, X_offset is 0"
v0.9.2,Calculate the variance of the predictions
v0.9.2,Calculate prediction confidence intervals
v0.9.2,Assumes flattened y
v0.9.2,Compute weighted residuals
v0.9.2,To be done once per target. Assumes y can be flattened.
v0.9.2,Assumes that X has already been offset
v0.9.2,Special case: n_features=1
v0.9.2,Compute Lasso coefficients for the columns of the design matrix
v0.9.2,Compute C_hat
v0.9.2,Compute theta_hat
v0.9.2,Allow for single output as well
v0.9.2,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.9.2,Set coef_ attribute
v0.9.2,Set intercept_ attribute
v0.9.2,Set selected_alpha_ attribute
v0.9.2,Set coef_stderr_
v0.9.2,intercept_stderr_
v0.9.2,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.9.2,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.9.2,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.9.2,set intercept_ attribute
v0.9.2,set coef_ attribute
v0.9.2,set alpha_ attribute
v0.9.2,set alphas_ attribute
v0.9.2,set n_iter_ attribute
v0.9.2,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.9.2,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.9.2,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.9.2,now regress X1 on y - X2 * beta2 to learn beta1
v0.9.2,set coef_ and intercept_ attributes
v0.9.2,Note that the penalized model should *not* have an intercept
v0.9.2,don't proxy special methods
v0.9.2,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.9.2,regressor incorrectly
v0.9.2,"Note: for known attributes that have been set this method will not be called,"
v0.9.2,so we should just throw here because this is an attribute belonging to this class
v0.9.2,but which hasn't yet been set on this instance
v0.9.2,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,AzureML
v0.9.2,helper imports
v0.9.2,write the details of the workspace to a configuration file to the notebook library
v0.9.2,if y is a multioutput model
v0.9.2,Make sure second dimension has 1 or more item
v0.9.2,switch _inner Model to a MultiOutputRegressor
v0.9.2,flatten array as automl only takes vectors for y
v0.9.2,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.9.2,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.9.2,as an sklearn estimator
v0.9.2,fit implementation for a single output model.
v0.9.2,Create experiment for specified workspace
v0.9.2,Configure automl_config with training set information.
v0.9.2,"Wait for remote run to complete, the set the model"
v0.9.2,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.9.2,create model and pass model into final.
v0.9.2,"If item is an automl config, get its corresponding"
v0.9.2,AutomatedML Model and add it to new_Args
v0.9.2,"If item is an automl config, get its corresponding"
v0.9.2,AutomatedML Model and set it for this key in
v0.9.2,kwargs
v0.9.2,takes in either automated_ml config and instantiates
v0.9.2,an AutomatedMLModel
v0.9.2,The prefix can only be 18 characters long
v0.9.2,"because prefixes come from kwarg_names, we must ensure they are"
v0.9.2,short enough.
v0.9.2,Get workspace from config file.
v0.9.2,Take the intersect of the white for sample
v0.9.2,weights and linear models
v0.9.2,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,TODO: generalize to multiple treatment case?
v0.9.2,get index of best treatment
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,make any access to matplotlib or plt throw an exception
v0.9.2,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.9.2,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.9.2,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.9.2,clean way of achieving this
v0.9.2,make sure we don't accidentally escape anything in the substitution
v0.9.2,Fetch appropriate color for node
v0.9.2,"red for negative, green for positive"
v0.9.2,in multi-target use first target
v0.9.2,Write node mean CATE
v0.9.2,Write node std of CATE
v0.9.2,Fetch appropriate color for node
v0.9.2,"red for negative, green for positive"
v0.9.2,Write node mean CATE
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,TODO: consider working around relying on sklearn implementation details
v0.9.2,Create splits of causal tree
v0.9.2,Make sure the correct exception is being rethrown
v0.9.2,Must make sure indices are merged correctly
v0.9.2,Convert rows to columns
v0.9.2,Require group assignment t to be one-hot-encoded
v0.9.2,Get predictions for the 2 splits
v0.9.2,Must make sure indices are merged correctly
v0.9.2,Crossfitting
v0.9.2,Compute weighted nuisance estimates
v0.9.2,-------------------------------------------------------------------------------
v0.9.2,Calculate the covariance matrix corresponding to the BLB inference
v0.9.2,
v0.9.2,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.9.2,2. Calculate the weighted moments for each tree slice to create a matrix
v0.9.2,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.9.2,in that slice from the overall parameter estimate.
v0.9.2,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.9.2,-------------------------------------------------------------------------------
v0.9.2,Calclulate covariance matrix through BLB
v0.9.2,Estimators
v0.9.2,OrthoForest parameters
v0.9.2,Sub-forests
v0.9.2,Auxiliary attributes
v0.9.2,Fit check
v0.9.2,TODO: Check performance
v0.9.2,Must normalize weights
v0.9.2,Override the CATE inference options
v0.9.2,Add blb inference to parent's options
v0.9.2,Generate subsample indices
v0.9.2,Build trees in parallel
v0.9.2,Bootstraping has repetitions in tree sample
v0.9.2,Similar for `a` weights
v0.9.2,Bootstraping has repetitions in tree sample
v0.9.2,Define subsample size
v0.9.2,Safety check
v0.9.2,Draw points to create little bags
v0.9.2,Copy and/or define models
v0.9.2,Define nuisance estimators
v0.9.2,Define parameter estimators
v0.9.2,Define
v0.9.2,Need to redefine fit here for auto inference to work due to a quirk in how
v0.9.2,wrap_fit is defined
v0.9.2,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.9.2,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.9.2,Override to flatten output if T is flat
v0.9.2,Check that all discrete treatments are represented
v0.9.2,Nuissance estimates evaluated with cross-fitting
v0.9.2,Define 2-fold iterator
v0.9.2,Check if there is only one example of some class
v0.9.2,Define 2-fold iterator
v0.9.2,need safe=False when cloning for WeightedModelWrapper
v0.9.2,Compute residuals
v0.9.2,Compute coefficient by OLS on residuals
v0.9.2,"Parameter returned by LinearRegression is (d_T, )"
v0.9.2,Compute residuals
v0.9.2,Compute coefficient by OLS on residuals
v0.9.2,ell_2 regularization
v0.9.2,Ridge regression estimate
v0.9.2,"Parameter returned is of shape (d_T, )"
v0.9.2,Return moments and gradients
v0.9.2,Compute residuals
v0.9.2,Compute moments
v0.9.2,"Moments shape is (n, d_T)"
v0.9.2,Compute moment gradients
v0.9.2,returns shape-conforming residuals
v0.9.2,Copy and/or define models
v0.9.2,Define parameter estimators
v0.9.2,Define moment and mean gradient estimator
v0.9.2,"Check that T is shape (n, )"
v0.9.2,Check T is numeric
v0.9.2,Train label encoder
v0.9.2,Call `fit` from parent class
v0.9.2,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.9.2,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.9.2,Override to flatten output if T is flat
v0.9.2,Expand one-hot encoding to include the zero treatment
v0.9.2,"Test that T contains all treatments. If not, return None"
v0.9.2,Nuissance estimates evaluated with cross-fitting
v0.9.2,Define 2-fold iterator
v0.9.2,Check if there is only one example of some class
v0.9.2,No need to crossfit for internal nodes
v0.9.2,Compute partial moments
v0.9.2,"If any of the values in the parameter estimate is nan, return None"
v0.9.2,Compute partial moments
v0.9.2,Compute coefficient by OLS on residuals
v0.9.2,ell_2 regularization
v0.9.2,Ridge regression estimate
v0.9.2,"Parameter returned is of shape (d_T, )"
v0.9.2,Return moments and gradients
v0.9.2,Compute partial moments
v0.9.2,Compute moments
v0.9.2,"Moments shape is (n, d_T-1)"
v0.9.2,Compute moment gradients
v0.9.2,Need to calculate this in an elegant way for when propensity is 0
v0.9.2,This will flatten T
v0.9.2,Check that T is numeric
v0.9.2,Test whether the input estimator is supported
v0.9.2,Calculate confidence intervals for the parameter (marginal effect)
v0.9.2,Calculate confidence intervals for the effect
v0.9.2,Calculate the effects
v0.9.2,Calculate the standard deviations for the effects
v0.9.2,d_t=None here since we measure the effect across all Ts
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.2,Licensed under the MIT License.
v0.9.2,Causal tree parameters
v0.9.2,Tree structure
v0.9.2,No need for a random split since the data is already
v0.9.2,a random subsample from the original input
v0.9.2,node list stores the nodes that are yet to be splitted
v0.9.2,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.9.2,Create local sample set
v0.9.2,Compute nuisance estimates for the current node
v0.9.2,Nuisance estimate cannot be calculated
v0.9.2,Estimate parameter for current node
v0.9.2,Node estimate cannot be calculated
v0.9.2,Calculate moments and gradient of moments for current data
v0.9.2,Calculate inverse gradient
v0.9.2,The gradient matrix is not invertible.
v0.9.2,No good split can be found
v0.9.2,Calculate point-wise pseudo-outcomes rho
v0.9.2,a split is determined by a feature and a sample pair
v0.9.2,the number of possible splits is at most (number of features) * (number of node samples)
v0.9.2,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.9.2,parse row and column of random pair
v0.9.2,the sample of the pair is the integer division of the random number with n_feats
v0.9.2,calculate the binary indicator of whether sample i is on the left or the right
v0.9.2,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.9.2,calculate the number of samples on the left child for each proposed split
v0.9.2,calculate the analogous binary indicator for the samples in the estimation set
v0.9.2,calculate the number of estimation samples on the left child of each proposed split
v0.9.2,find the upper and lower bound on the size of the left split for the split
v0.9.2,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.9.2,on each side.
v0.9.2,similarly for the estimation sample set
v0.9.2,if there is no valid split then don't create any children
v0.9.2,filter only the valid splits
v0.9.2,calculate the average influence vector of the samples in the left child
v0.9.2,calculate the average influence vector of the samples in the right child
v0.9.2,take the square of each of the entries of the influence vectors and normalize
v0.9.2,by size of each child
v0.9.2,calculate the vector score of each candidate split as the average of left and right
v0.9.2,influence vectors
v0.9.2,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.9.2,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.9.2,where there might be large discontinuities in some parameter as the conditioning set varies
v0.9.2,calculate the scalar score of each split by aggregating across the vector of scores
v0.9.2,Find split that minimizes criterion
v0.9.2,Create child nodes with corresponding subsamples
v0.9.2,add the created children to the list of not yet split nodes
v0.9.1,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.9.1,configuration is all pulled from setup.cfg
v0.9.1,-*- coding: utf-8 -*-
v0.9.1,
v0.9.1,Configuration file for the Sphinx documentation builder.
v0.9.1,
v0.9.1,This file does only contain a selection of the most common options. For a
v0.9.1,full list see the documentation:
v0.9.1,http://www.sphinx-doc.org/en/master/config
v0.9.1,-- Path setup --------------------------------------------------------------
v0.9.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.9.1,add these directories to sys.path here. If the directory is relative to the
v0.9.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.9.1,
v0.9.1,-- Project information -----------------------------------------------------
v0.9.1,-- General configuration ---------------------------------------------------
v0.9.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.9.1,
v0.9.1,needs_sphinx = '1.0'
v0.9.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.9.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.9.1,ones.
v0.9.1,"Add any paths that contain templates here, relative to this directory."
v0.9.1,The suffix(es) of source filenames.
v0.9.1,You can specify multiple suffix as a list of string:
v0.9.1,
v0.9.1,"source_suffix = ['.rst', '.md']"
v0.9.1,The master toctree document.
v0.9.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.9.1,for a list of supported languages.
v0.9.1,
v0.9.1,This is also used if you do content translation via gettext catalogs.
v0.9.1,"Usually you set ""language"" from the command line for these cases."
v0.9.1,"List of patterns, relative to source directory, that match files and"
v0.9.1,directories to ignore when looking for source files.
v0.9.1,This pattern also affects html_static_path and html_extra_path.
v0.9.1,The name of the Pygments (syntax highlighting) style to use.
v0.9.1,-- Options for HTML output -------------------------------------------------
v0.9.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.9.1,a list of builtin themes.
v0.9.1,
v0.9.1,Theme options are theme-specific and customize the look and feel of a theme
v0.9.1,"further.  For a list of options available for each theme, see the"
v0.9.1,documentation.
v0.9.1,
v0.9.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.9.1,"relative to this directory. They are copied after the builtin static files,"
v0.9.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.9.1,html_static_path = ['_static']
v0.9.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.9.1,to template names.
v0.9.1,
v0.9.1,The default sidebars (for documents that don't match any pattern) are
v0.9.1,defined by theme itself.  Builtin themes are using these templates by
v0.9.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.9.1,'searchbox.html']``.
v0.9.1,
v0.9.1,html_sidebars = {}
v0.9.1,-- Options for HTMLHelp output ---------------------------------------------
v0.9.1,Output file base name for HTML help builder.
v0.9.1,-- Options for LaTeX output ------------------------------------------------
v0.9.1,The paper size ('letterpaper' or 'a4paper').
v0.9.1,
v0.9.1,"'papersize': 'letterpaper',"
v0.9.1,"The font size ('10pt', '11pt' or '12pt')."
v0.9.1,
v0.9.1,"'pointsize': '10pt',"
v0.9.1,Additional stuff for the LaTeX preamble.
v0.9.1,
v0.9.1,"'preamble': '',"
v0.9.1,Latex figure (float) alignment
v0.9.1,
v0.9.1,"'figure_align': 'htbp',"
v0.9.1,Grouping the document tree into LaTeX files. List of tuples
v0.9.1,"(source start file, target name, title,"
v0.9.1,"author, documentclass [howto, manual, or own class])."
v0.9.1,-- Options for manual page output ------------------------------------------
v0.9.1,One entry per manual page. List of tuples
v0.9.1,"(source start file, name, description, authors, manual section)."
v0.9.1,-- Options for Texinfo output ----------------------------------------------
v0.9.1,Grouping the document tree into Texinfo files. List of tuples
v0.9.1,"(source start file, target name, title, author,"
v0.9.1,"dir menu entry, description, category)"
v0.9.1,-- Options for Epub output -------------------------------------------------
v0.9.1,Bibliographic Dublin Core info.
v0.9.1,The unique identifier of the text. This can be a ISBN number
v0.9.1,or the project homepage.
v0.9.1,
v0.9.1,epub_identifier = ''
v0.9.1,A unique identification for the text.
v0.9.1,
v0.9.1,epub_uid = ''
v0.9.1,A list of files that should not be packed into the epub file.
v0.9.1,-- Extension configuration -------------------------------------------------
v0.9.1,-- Options for intersphinx extension ---------------------------------------
v0.9.1,Example configuration for intersphinx: refer to the Python standard library.
v0.9.1,-- Options for todo extension ----------------------------------------------
v0.9.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.9.1,-- Options for doctest extension -------------------------------------------
v0.9.1,we can document otherwise excluded entities here by returning False
v0.9.1,or skip otherwise included entities by returning True
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Calculate residuals
v0.9.1,Estimate E[T_res | Z_res]
v0.9.1,TODO. Deal with multi-class instrument
v0.9.1,Calculate nuisances
v0.9.1,Estimate E[T_res | Z_res]
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.9.1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.9.1,TODO. Deal with multi-class instrument
v0.9.1,Estimate final model of theta(X) by minimizing the square loss:
v0.9.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.9.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.9.1,at the expense of some small bias. For points with very small covariance we revert
v0.9.1,to the model-based preliminary estimate and do not add the correction term.
v0.9.1,Estimate preliminary theta in cross fitting manner
v0.9.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.9.1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.9.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.9.1,TODO. The solution below is not really a valid cross-fitting
v0.9.1,as the test data are used to create the proj_t on the train
v0.9.1,which in the second train-test loop is used to create the nuisance
v0.9.1,cov on the test data. Hence the T variable of some sample
v0.9.1,"is implicitly correlated with its cov nuisance, through this flow"
v0.9.1,"of information. However, this seems a rather weak correlation."
v0.9.1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.9.1,model.
v0.9.1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.9.1,Estimate preliminary theta in cross fitting manner
v0.9.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.9.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.9.1,#############################################################################
v0.9.1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.9.1,A/B test
v0.9.1,#############################################################################
v0.9.1,Estimate preliminary theta in cross fitting manner
v0.9.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.1,We can use statsmodel for all hypothesis testing capabilities
v0.9.1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.9.1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.9.1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.9.1,model_T_XZ = lambda: model_clf()
v0.9.1,#'days_visited': lambda:
v0.9.1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.9.1,Turn strings into categories for numeric mapping
v0.9.1,### Defining some generic regressors and classifiers
v0.9.1,This a generic non-parametric regressor
v0.9.1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.9.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.9.1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.9.1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.9.1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.9.1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.9.1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.9.1,model = lambda: LinearRegression(n_jobs=-1)
v0.9.1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.9.1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.9.1,underlying model whenever predict is called.
v0.9.1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.9.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.9.1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.9.1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.9.1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.9.1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.9.1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.9.1,We need to specify models to be used for each of these residualizations
v0.9.1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.9.1,"E[T | X, Z]"
v0.9.1,E[TZ | X]
v0.9.1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.9.1,n_splits determines the number of splits to be used for cross-fitting.
v0.9.1,# Algorithm 2 - Current Method
v0.9.1,In[121]:
v0.9.1,# Algorithm 3 - DRIV ATE
v0.9.1,dmliv_model_effect = lambda: model()
v0.9.1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.9.1,"dmliv_model_effect(),"
v0.9.1,n_splits=1)
v0.9.1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.9.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.9.1,"Once multiple treatments are supported, we'll need to fix this"
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.1,We can use statsmodel for all hypothesis testing capabilities
v0.9.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.1,We can use statsmodel for all hypothesis testing capabilities
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,TODO. Deal with multi-class instrument/treatment
v0.9.1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.9.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.9.1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.9.1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.9.1,##################
v0.9.1,Global settings #
v0.9.1,##################
v0.9.1,Global plotting controls
v0.9.1,"Control for support size, can control for more"
v0.9.1,#################
v0.9.1,File utilities #
v0.9.1,#################
v0.9.1,#################
v0.9.1,Plotting utils #
v0.9.1,#################
v0.9.1,bias
v0.9.1,var
v0.9.1,rmse
v0.9.1,r2
v0.9.1,Infer feature dimension
v0.9.1,Metrics by support plots
v0.9.1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.9.1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.9.1,Steven Wu <zhiww@microsoft.com>
v0.9.1,Initialize causal tree parameters
v0.9.1,Create splits of causal tree
v0.9.1,Estimate treatment effects at the leafs
v0.9.1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.9.1,the corresponding split and associating the effect computed on that leaf
v0.9.1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.9.1,Safety check
v0.9.1,Weighted linear regression
v0.9.1,Calculates weights
v0.9.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.9.1,over all indices
v0.9.1,Similar for `a` weights
v0.9.1,Doesn't have sample weights
v0.9.1,Is a linear model
v0.9.1,Weighted linear regression
v0.9.1,Calculates weights
v0.9.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.9.1,over all indices
v0.9.1,Similar for `a` weights
v0.9.1,normalize weights
v0.9.1,"Split the data in half, train and test"
v0.9.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.9.1,"a function of W, using only the train fold"
v0.9.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.9.1,"Split the data in half, train and test"
v0.9.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.9.1,"a function of W, using only the train fold"
v0.9.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.9.1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.9.1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.9.1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.9.1,"Split the data in half, train and test"
v0.9.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.9.1,"a function of x, using only the train fold"
v0.9.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.9.1,Compute coefficient by OLS on residuals
v0.9.1,"Split the data in half, train and test"
v0.9.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.9.1,"a function of x, using only the train fold"
v0.9.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.9.1,Estimate multipliers for second order orthogonal method
v0.9.1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.9.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.9.1,Create local sample set
v0.9.1,compute the base estimate for the current node using double ml or second order double ml
v0.9.1,compute the influence functions here that are used for the criterion
v0.9.1,generate random proposals of dimensions to split
v0.9.1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.9.1,compute criterion for each proposal
v0.9.1,if splitting creates valid leafs in terms of mean leaf size
v0.9.1,Calculate criterion for split
v0.9.1,Else set criterion to infinity so that this split is not chosen
v0.9.1,If no good split was found
v0.9.1,Find split that minimizes criterion
v0.9.1,Set the split attributes at the node
v0.9.1,Create child nodes with corresponding subsamples
v0.9.1,Recursively split children
v0.9.1,Return parent node
v0.9.1,estimate the local parameter at the leaf using the estimate data
v0.9.1,###################
v0.9.1,Argument parsing #
v0.9.1,###################
v0.9.1,#########################################
v0.9.1,Parameters constant across experiments #
v0.9.1,#########################################
v0.9.1,Outcome support
v0.9.1,Treatment support
v0.9.1,Evaluation grid
v0.9.1,Treatment effects array
v0.9.1,Other variables
v0.9.1,##########################
v0.9.1,Data Generating Process #
v0.9.1,##########################
v0.9.1,Log iteration
v0.9.1,"Generate controls, features, treatment and outcome"
v0.9.1,T and Y residuals to be used in later scripts
v0.9.1,Save generated dataset
v0.9.1,#################
v0.9.1,ORF parameters #
v0.9.1,#################
v0.9.1,######################################
v0.9.1,Train and evaluate treatment effect #
v0.9.1,######################################
v0.9.1,########
v0.9.1,Plots #
v0.9.1,########
v0.9.1,###############
v0.9.1,Save results #
v0.9.1,###############
v0.9.1,##############
v0.9.1,Run Rscript #
v0.9.1,##############
v0.9.1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.9.1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.9.1,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.9.1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.9.1,def mlasso_model(): return MultiTaskLassoCV(
v0.9.1,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.9.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.9.1,heterogeneity
v0.9.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.9.1,heterogeneity
v0.9.1,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.9.1,heterogeneity
v0.9.1,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.9.1,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.9.1,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.9.1,subset of features that are exogenous and create heterogeneity
v0.9.1,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.9.1,subset of features wrt we estimate heterogeneity
v0.9.1,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.9.1,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,introspect the constructor arguments to find the model parameters
v0.9.1,to represent
v0.9.1,Extract and sort argument names excluding 'self'
v0.9.1,create dataframe
v0.9.1,currently dowhy only support single outcome and single treatment
v0.9.1,column names
v0.9.1,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.9.1,cate estimator but not the effect.
v0.9.1,don't proxy special methods
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Check if model is sparse enough for this model
v0.9.1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.9.1,TODO: any way to avoid creating a copy if the array was already dense?
v0.9.1,"the call is necessary if the input was something like a list, though"
v0.9.1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.9.1,so convert to pydata sparse first
v0.9.1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.9.1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.9.1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.9.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.9.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.9.1,For when checking input values is disabled
v0.9.1,Type to column extraction function
v0.9.1,"Get number of arguments, some sklearn featurizer don't accept feature_names"
v0.9.1,Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
v0.9.1,Get feature names using featurizer
v0.9.1,All attempts at retrieving transformed feature names have failed
v0.9.1,Delegate handling to downstream logic
v0.9.1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.9.1,same number of input definitions as arrays
v0.9.1,input definitions have same number of dimensions as each array
v0.9.1,all result indices are unique
v0.9.1,all result indices must match at least one input index
v0.9.1,"map indices to all array, axis pairs for that index"
v0.9.1,each index has the same cardinality wherever it appears
v0.9.1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.9.1,Algo: while list contains more than one entry
v0.9.1,take two entries
v0.9.1,sort both lists by intersection of their indices
v0.9.1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.9.1,"take the union of indices and the product of values), stepping through each list linearly"
v0.9.1,TODO: might be faster to break into connected components first
v0.9.1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.9.1,"so compute their content separately, then take cartesian product"
v0.9.1,this would save a few pointless sorts by empty tuples
v0.9.1,TODO: Consider investigating other performance ideas for these cases
v0.9.1,where the dense method beat the sparse method (usually sparse is faster)
v0.9.1,"e,facd,c->cfed"
v0.9.1,sparse: 0.0335489
v0.9.1,dense:  0.011465999999999997
v0.9.1,"gbd,da,egb->da"
v0.9.1,sparse: 0.0791625
v0.9.1,dense:  0.007319099999999995
v0.9.1,"dcc,d,faedb,c->abe"
v0.9.1,sparse: 1.2868097
v0.9.1,dense:  0.44605229999999985
v0.9.1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.9.1,TODO: would using einsum's paths to optimize the order of merging help?
v0.9.1,assume that we should perform nested cross-validation if and only if
v0.9.1,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.9.1,logic copied from check_cv
v0.9.1,otherwise we will assume the user already set the cv attribute to something
v0.9.1,compatible with splitting with a 'groups' argument
v0.9.1,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.9.1,don't use the groups when calling split internally
v0.9.1,Normalize weights
v0.9.1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.9.1,"if we're decorating a class, just update the __init__ method,"
v0.9.1,so that the result is still a class instead of a wrapper method
v0.9.1,"want to enforce that each bad_arg was either in kwargs,"
v0.9.1,or else it was in neither and is just taking its default value
v0.9.1,Any access should throw
v0.9.1,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.1,input feature name is already updated by cate_feature_names.
v0.9.1,define the index of d_x to filter for each given T
v0.9.1,filter X after broadcast with T for each given T
v0.9.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.1,Licensed under the MIT License.
v0.9.1,"since inference objects can be stateful, we must copy it before fitting;"
v0.9.1,otherwise this sequence wouldn't work:
v0.9.1,"est1.fit(..., inference=inf)"
v0.9.1,"est2.fit(..., inference=inf)"
v0.9.1,est1.effect_interval(...)
v0.9.1,because inf now stores state from fitting est2
v0.9.1,This flag is true when names are set in a child class instead
v0.9.1,"If names are set in a child class, add an attribute reflecting that"
v0.9.1,This works only if X is passed as a kwarg
v0.9.1,We plan to enforce X as kwarg only in future releases
v0.9.1,This checks if names have been set in a child class
v0.9.1,"If names were set in a child class, don't do it again"
v0.9.1,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.9.1,call the wrapped fit method
v0.9.1,NOTE: we call inference fit *after* calling the main fit method
v0.9.1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.9.1,but tensordot can't be applied to this problem because we don't sum over m
v0.9.1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.9.1,of rows of T was not taken into account
v0.9.1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.9.1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.9.1,"Treatment names is None, default to BaseCateEstimator"
v0.9.1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.9.1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.9.1,Get input names
v0.9.1,Summary
v0.9.1,add statsmodels to parent's options
v0.9.1,add debiasedlasso to parent's options
v0.9.1,add blb to parent's options
v0.9.1,TODO Share some logic with non-discrete version
v0.9.1,Get input names
v0.9.1,Summary
v0.9.1,add statsmodels to parent's options
v0.9.1,add statsmodels to parent's options
v0.9.1,add blb to parent's options
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,remove None arguments
v0.9.1,"scores entries should be lists of scores, so make each entry a singleton list"
v0.9.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.9.1,generate an instance of the final model
v0.9.1,generate an instance of the nuisance model
v0.9.1,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.9.1,alteration even when we only want to fit_final.
v0.9.1,use a binary array to get stratified split in case of discrete treatment
v0.9.1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.9.1,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.9.1,"however, sklearn doesn't support both stratifying and grouping (see"
v0.9.1,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.9.1,their own object that supports grouping if they want to use groups.
v0.9.1,######################################################
v0.9.1,These should be removed once `n_splits` is deprecated
v0.9.1,######################################################
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.9.1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.9.1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.9.1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.9.1,checks that X is 2D array.
v0.9.1,"since we only allow single dimensional y, we could flatten the prediction"
v0.9.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.1,Handles the corner case when X=None but featurizer might be not None
v0.9.1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.9.1,"Replacing this method which is invalid for this class, so that we make the"
v0.9.1,dosctring empty and not appear in the docs.
v0.9.1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.9.1,TODO: support sample_var
v0.9.1,Replacing to remove docstring
v0.9.1,###################################################################
v0.9.1,Everything below should be removed once parameters are deprecated
v0.9.1,###################################################################
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"if both X and W are None, just return a column of ones"
v0.9.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.1,We need to go back to the label representation of the one-hot so as to call
v0.9.1,the classifier.
v0.9.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.1,We need to go back to the label representation of the one-hot so as to call
v0.9.1,the classifier.
v0.9.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.9.1,This works both with our without the weighting trick as the treatments T are unit vector
v0.9.1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.9.1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.9.1,both Parametric and Non Parametric DML.
v0.9.1,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.9.1,attribute so that the trained featurizer will be passed through
v0.9.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.1,for internal use by the library
v0.9.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.1,We need to use the rlearner's copy to retain the information from fitting
v0.9.1,Handles the corner case when X=None but featurizer might be not None
v0.9.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.1,since we clone it and fit separate copies
v0.9.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.1,TODO: support sample_var
v0.9.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.1,since we clone it and fit separate copies
v0.9.1,add blb to parent's options
v0.9.1,override only so that we can update the docstring to indicate
v0.9.1,support for `GenericSingleTreatmentModelFinalInference`
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,note that groups are not passed to score because they are only used for fitting
v0.9.1,note that groups are not passed to score because they are only used for fitting
v0.9.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.1,NOTE: important to get parent's wrapped copy so that
v0.9.1,"after training wrapped featurizer is also trained, etc."
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.9.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.1,(which needs to have been expanded if there's a discrete treatment)
v0.9.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.1,since we clone it and fit separate copies
v0.9.1,override only so that we can update the docstring to indicate support for `blb`
v0.9.1,######################################################
v0.9.1,These should be removed once `n_splits` is deprecated
v0.9.1,######################################################
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Remove children with nonwhite mothers from the treatment group
v0.9.1,Remove children with nonwhite mothers from the treatment group
v0.9.1,Select columns
v0.9.1,Scale the numeric variables
v0.9.1,"Change the binary variable 'first' takes values in {1,2}"
v0.9.1,Append a column of ones as intercept
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.1,(which needs to have been expanded if there's a discrete treatment)
v0.9.1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.9.1,d_t=None here since we measure the effect across all Ts
v0.9.1,once the estimator has been fit
v0.9.1,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.9.1,an intercept even when bias is part of coef.
v0.9.1,We can write effect inference as a function of prediction and prediction standard error of
v0.9.1,the final method for linear models
v0.9.1,squeeze the first axis
v0.9.1,d_t=None here since we measure the effect across all Ts
v0.9.1,set the mean_pred_stderr
v0.9.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.1,(which needs to have been expanded if there's a discrete treatment)
v0.9.1,"send treatment to the end, pull bounds to the front"
v0.9.1,d_t=None here since we measure the effect across all Ts
v0.9.1,set the mean_pred_stderr
v0.9.1,replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
v0.9.1,d_t=None here since we measure the effect across all Ts
v0.9.1,d_t=None here since we measure the effect across all Ts
v0.9.1,need to set the fit args before the estimator is fit
v0.9.1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.9.1,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.9.1,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.9.1,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.9.1,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.9.1,1. Uncertainty of Mean Point Estimate
v0.9.1,2. Distribution of Point Estimate
v0.9.1,3. Total Variance of Point Estimate
v0.9.1,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.9.1,so bail out early; note that it might be possible to correct the algorithm for
v0.9.1,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.9.1,be clean
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,TODO: Add a __dir__ implementation?
v0.9.1,don't proxy special methods
v0.9.1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.9.1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.9.1,then we should be computing a confidence interval for the wrapped calls
v0.9.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.9.1,second level bootstrap which would be prohibitive computationally?
v0.9.1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.9.1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.9.1,can't import from econml.inference at top level without creating cyclical dependencies
v0.9.1,Note that inference results are always methods even if the inference is for a property
v0.9.1,(e.g. coef__inference() is a method but coef_ is a property)
v0.9.1,Therefore we must insert a lambda if getting inference for a non-callable
v0.9.1,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.9.1,"try to get interval/std first if appropriate,"
v0.9.1,since we don't prefer a wrapped method with this name
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,
v0.9.1,This code is a fork from:
v0.9.1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.9.1,published under the following license and copyright:
v0.9.1,BSD 3-Clause License
v0.9.1,
v0.9.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.1,All rights reserved.
v0.9.1,Set parameters
v0.9.1,Don't instantiate estimators now! Parameters of base_estimator might
v0.9.1,"still change. Eg., when grid-searching with the nested object syntax."
v0.9.1,self.estimators_ needs to be filled by the derived classes in fit.
v0.9.1,Compute the number of jobs
v0.9.1,Partition estimators between jobs
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,
v0.9.1,This code contains snippets of code from:
v0.9.1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.9.1,published under the following license and copyright:
v0.9.1,BSD 3-Clause License
v0.9.1,
v0.9.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.1,All rights reserved.
v0.9.1,=============================================================================
v0.9.1,Types and constants
v0.9.1,=============================================================================
v0.9.1,=============================================================================
v0.9.1,Base GRF tree
v0.9.1,=============================================================================
v0.9.1,Determine output settings
v0.9.1,"Important: This must be the first invocation of the random state at fit time, so that"
v0.9.1,train/test splits are re-generatable from an external object simply by knowing the
v0.9.1,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.9.1,linear predictions. Currently is also useful for testing.
v0.9.1,reshape is necessary to preserve the data contiguity against vs
v0.9.1,"[:, np.newaxis] that does not."
v0.9.1,Check parameters
v0.9.1,Set min_weight_leaf from min_weight_fraction_leaf
v0.9.1,Build tree
v0.9.1,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.9.1,hold. Used by criterion for memory space savings.
v0.9.1,Initialize the criterion object and the criterion_val object if honest.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,=============================================================================
v0.9.1,A MultOutputWrapper for GRF classes
v0.9.1,=============================================================================
v0.9.1,=============================================================================
v0.9.1,Instantiations of Generalized Random Forest
v0.9.1,=============================================================================
v0.9.1,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.9.1,in front of the constant treatment is the intercept in the moment equation.
v0.9.1,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.9.1,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,
v0.9.1,This code contains snippets of code from
v0.9.1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.9.1,published under the following license and copyright:
v0.9.1,BSD 3-Clause License
v0.9.1,
v0.9.1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.1,All rights reserved.
v0.9.1,=============================================================================
v0.9.1,Base Generalized Random Forest
v0.9.1,=============================================================================
v0.9.1,Remap output
v0.9.1,reshape is necessary to preserve the data contiguity against vs
v0.9.1,"[:, np.newaxis] that does not."
v0.9.1,reshape is necessary to preserve the data contiguity against vs
v0.9.1,"[:, np.newaxis] that does not."
v0.9.1,Get subsample sample size
v0.9.1,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.9.1,We calculate the min eigenvalue proxy that each criterion is considering
v0.9.1,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.9.1,Check parameters
v0.9.1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.9.1,if this is the first `fit` call of the warm start mode.
v0.9.1,"Free allocated memory, if any"
v0.9.1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.9.1,We draw from the random state to get the random state we
v0.9.1,would have got if we hadn't used a warm_start.
v0.9.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.9.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.9.1,but would still advance randomness enough so that tree subsamples will be different.
v0.9.1,Generating indices a priori before parallelism ended up being orders of magnitude
v0.9.1,faster than how sklearn does it. The reason is that random samplers do not release the
v0.9.1,gil it seems.
v0.9.1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.9.1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.9.1,but would still advance randomness enough so that tree subsamples will be different.
v0.9.1,Parallel loop: we prefer the threading backend as the Cython code
v0.9.1,for fitting the trees is internally releasing the Python GIL
v0.9.1,making threading more efficient than multiprocessing in
v0.9.1,"that case. However, for joblib 0.12+ we respect any"
v0.9.1,"parallel_backend contexts set at a higher level,"
v0.9.1,since correctness does not rely on using threads.
v0.9.1,Collect newly grown trees
v0.9.1,Check data
v0.9.1,Assign chunk of trees to jobs
v0.9.1,avoid storing the output of every estimator by summing them here
v0.9.1,Parallel loop
v0.9.1,Check data
v0.9.1,Assign chunk of trees to jobs
v0.9.1,Parallel loop
v0.9.1,Check data
v0.9.1,Assign chunk of trees to jobs
v0.9.1,Parallel loop
v0.9.1,####################
v0.9.1,Variance correction
v0.9.1,####################
v0.9.1,Subtract the average within bag variance. This ends up being equal to the
v0.9.1,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.9.1,The negative part is just sq_between.
v0.9.1,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.9.1,"The off diagonals we have no objective prior, so no correction is applied."
v0.9.1,Finally correcting the pred_cov or pred_var
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,testing importances
v0.9.1,testing heterogeneity importances
v0.9.1,Testing that all parameters do what they are supposed to
v0.9.1,"testing predict, apply and decision path"
v0.9.1,test that the subsampling scheme past to the trees is correct
v0.9.1,test that the estimator calcualtes var correctly
v0.9.1,test api
v0.9.1,test accuracy
v0.9.1,test the projection functionality of forests
v0.9.1,test that the estimator calcualtes var correctly
v0.9.1,test api
v0.9.1,test that the estimator calcualtes var correctly
v0.9.1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.9.1,test that we raise errors in mishandled situations.
v0.9.1,test that the subsampling scheme past to the trees is correct
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.9.1,creating notebooks that are annoying for our users to actually run themselves
v0.9.1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.9.1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.9.1,"prior to calling interpret, can't plot, render, etc."
v0.9.1,can interpret without uncertainty
v0.9.1,can't interpret with uncertainty if inference wasn't used during fit
v0.9.1,can interpret with uncertainty if we refit
v0.9.1,can interpret without uncertainty
v0.9.1,can't treat before interpreting
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,simple DGP only for illustration
v0.9.1,Define the treatment model neural network architecture
v0.9.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.9.1,"so the input shape is (d_z + d_x,)"
v0.9.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.9.1,add extra layers on top for the mixture density network
v0.9.1,Define the response model neural network architecture
v0.9.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.9.1,"so the input shape is (d_t + d_x,)"
v0.9.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.9.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.9.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.9.1,so that the same weights will be reused in each instantiation
v0.9.1,number of samples to use in second estimate of the response
v0.9.1,(to make loss estimate unbiased)
v0.9.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.9.1,do something with predictions...
v0.9.1,also test vector t and y
v0.9.1,simple DGP only for illustration
v0.9.1,Define the treatment model neural network architecture
v0.9.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.9.1,"so the input shape is (d_z + d_x,)"
v0.9.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.9.1,add extra layers on top for the mixture density network
v0.9.1,Define the response model neural network architecture
v0.9.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.9.1,"so the input shape is (d_t + d_x,)"
v0.9.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.9.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.9.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.9.1,so that the same weights will be reused in each instantiation
v0.9.1,number of samples to use in second estimate of the response
v0.9.1,(to make loss estimate unbiased)
v0.9.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.9.1,do something with predictions...
v0.9.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.9.1,test = True ensures we draw test set images
v0.9.1,test = True ensures we draw test set images
v0.9.1,re-draw to get new independent treatment and implied response
v0.9.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.9.1,above is necesary so that reduced form doesn't win
v0.9.1,covariates: time and emotion
v0.9.1,random instrument
v0.9.1,z -> price
v0.9.1,true observable demand function
v0.9.1,errors
v0.9.1,response
v0.9.1,test = True ensures we draw test set images
v0.9.1,test = True ensures we draw test set images
v0.9.1,re-draw to get new independent treatment and implied response
v0.9.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.9.1,above is necesary so that reduced form doesn't win
v0.9.1,covariates: time and emotion
v0.9.1,random instrument
v0.9.1,z -> price
v0.9.1,true observable demand function
v0.9.1,errors
v0.9.1,response
v0.9.1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.9.1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.9.1,For some reason this doesn't work at all when run against the CNTK backend...
v0.9.1,"model.compile('nadam', loss=lambda _,l:l)"
v0.9.1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.9.1,generate a valiation set
v0.9.1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.9.1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,DGP constants
v0.9.1,Generate data
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,testing importances
v0.9.1,testing heterogeneity importances
v0.9.1,Testing that all parameters do what they are supposed to
v0.9.1,"testing predict, apply and decision path"
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.9.1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.9.1,so we need to transpose the result
v0.9.1,1-d output
v0.9.1,2-d output
v0.9.1,Single dimensional output y
v0.9.1,Multi-dimensional output y
v0.9.1,1-d y
v0.9.1,multi-d y
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,test that we can fit with the same arguments as the base estimator
v0.9.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.1,with the same shape for the lower and upper bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,test that we can do the same thing once we provide percentile bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.1,with the same shape for the lower and upper bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,test that we can do the same thing once we provide percentile bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,test that we can fit with the same arguments as the base estimator
v0.9.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.1,with the same shape for the lower and upper bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,test that we can do the same thing once we provide percentile bounds
v0.9.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.1,with the same shape for the lower and upper bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,test that we can do the same thing once we provide percentile bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,test that we can fit with the same arguments as the base estimator
v0.9.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.1,with the same shape for the lower and upper bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,test that we can do the same thing once we provide percentile bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.1,with the same shape for the lower and upper bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,test that we can do the same thing once we provide percentile bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.1,with the same shape for the lower and upper bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,test that the estimated effect is usually within the bounds
v0.9.1,test that we can do the same thing once we provide alpha explicitly
v0.9.1,test that the lower and upper bounds differ
v0.9.1,test that the estimated effect is usually within the bounds
v0.9.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.1,with the same shape for the lower and upper bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,TODO: test that the estimated effect is usually within the bounds
v0.9.1,and that the true effect is also usually within the bounds
v0.9.1,test that we can do the same thing once we provide percentile bounds
v0.9.1,test that the lower and upper bounds differ
v0.9.1,TODO: test that the estimated effect is usually within the bounds
v0.9.1,and that the true effect is also usually within the bounds
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,DGP constants
v0.9.1,Generate data
v0.9.1,Test inference results when `cate_feature_names` doesn not exist
v0.9.1,Test inference results when `cate_feature_names` doesn not exist
v0.9.1,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.9.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.1,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.9.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.9.1,pvalue for second column should be greater than zero since some points are on either side
v0.9.1,of the tested value
v0.9.1,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.9.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.9.1,only is not None when T1 is a constant or a list of constant
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.9.1,Test non keyword based calls to fit
v0.9.1,test non-array inputs
v0.9.1,Test custom splitter
v0.9.1,Test incomplete set of test folds
v0.9.1,"y scores should be positive, since W predicts Y somewhat"
v0.9.1,"t scores might not be, since W and T are uncorrelated"
v0.9.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,make sure cross product varies more slowly with first array
v0.9.1,and that vectors are okay as inputs
v0.9.1,number of inputs in specification must match number of inputs
v0.9.1,must have an output
v0.9.1,output indices must be unique
v0.9.1,output indices must be present in an input
v0.9.1,number of indices must match number of dimensions for each input
v0.9.1,repeated indices must always have consistent sizes
v0.9.1,transpose
v0.9.1,tensordot
v0.9.1,trace
v0.9.1,TODO: set up proper flag for this
v0.9.1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.9.1,"of all of the distinct indices that appear in any input,"
v0.9.1,pick a random subset of them (of size at most 5) to appear in the output
v0.9.1,creating an instance should warn
v0.9.1,using the instance should not warn
v0.9.1,using the deprecated method should warn
v0.9.1,don't warn if b and c are passed by keyword
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Preprocess data
v0.9.1,Convert 'week' to a date
v0.9.1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.9.1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.9.1,Take log of price
v0.9.1,Make brand numeric
v0.9.1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.9.1,which have all zero coeffs
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,test at least one estimator from each category
v0.9.1,test causal graph
v0.9.1,test refutation estimate
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.9.1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.9.1,TODO: test something rather than just print...
v0.9.1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.9.1,pick some arbitrary X
v0.9.1,pick some arbitrary T
v0.9.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.9.1,The average variance should be lower when using monte carlo iterations
v0.9.1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.9.1,The average variance should be lower when using monte carlo iterations
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,ensure that we've got at least two of every row
v0.9.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.1,need to make sure we get all *joint* combinations
v0.9.1,IntentToTreat only supports binary treatments/instruments
v0.9.1,IntentToTreat only supports binary treatments/instruments
v0.9.1,IntentToTreat requires X
v0.9.1,ensure we can serialize unfit estimator
v0.9.1,these support only W but not X
v0.9.1,"these support only binary, not general discrete T and Z"
v0.9.1,ensure we can serialize fit estimator
v0.9.1,make sure we can call the marginal_effect and effect methods
v0.9.1,TODO: add tests for extra properties like coef_ where they exist
v0.9.1,TODO: add tests for extra properties like coef_ where they exist
v0.9.1,"make sure we can call effect with implied scalar treatments,"
v0.9.1,"no matter the dimensions of T, and also that we warn when there"
v0.9.1,are multiple treatments
v0.9.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.9.1,"however, with custom splits the checking happens in the first stage wrapper"
v0.9.1,where we don't have all of the required information to do this;
v0.9.1,we'd probably need to add it to _crossfit instead
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.9.1,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.9.1,The __warningregistry__'s need to be in a pristine state for tests
v0.9.1,to work properly.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Set random seed
v0.9.1,Generate data
v0.9.1,DGP constants
v0.9.1,Test data
v0.9.1,Constant treatment effect
v0.9.1,Constant treatment with multi output Y
v0.9.1,Heterogeneous treatment
v0.9.1,Heterogeneous treatment with multi output Y
v0.9.1,TLearner test
v0.9.1,Instantiate TLearner
v0.9.1,Test inputs
v0.9.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.1,Instantiate SLearner
v0.9.1,Test inputs
v0.9.1,Test constant treatment effect
v0.9.1,Test constant treatment effect with multi output Y
v0.9.1,Test heterogeneous treatment effect
v0.9.1,Need interactions between T and features
v0.9.1,Test heterogeneous treatment effect with multi output Y
v0.9.1,Instantiate XLearner
v0.9.1,Test inputs
v0.9.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.1,Instantiate DomainAdaptationLearner
v0.9.1,Test inputs
v0.9.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.1,Get the true treatment effect
v0.9.1,Get the true treatment effect
v0.9.1,Fit learner and get the effect and marginal effect
v0.9.1,Compute treatment effect residuals (absolute)
v0.9.1,Check that at least 90% of predictions are within tolerance interval
v0.9.1,Check whether the output shape is right
v0.9.1,Check that one can pass in regular lists
v0.9.1,Check that it fails correctly if lists of different shape are passed in
v0.9.1,"Check that it works when T, Y have shape (n, 1)"
v0.9.1,Generate covariates
v0.9.1,Generate treatment
v0.9.1,Calculate outcome
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,DGP constants
v0.9.1,Generate data
v0.9.1,Test data
v0.9.1,Remove warnings that might be raised by the models passed into the ORF
v0.9.1,Generate data with continuous treatments
v0.9.1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.9.1,does not work well with parallelism.
v0.9.1,Test inputs for continuous treatments
v0.9.1,--> Check that one can pass in regular lists
v0.9.1,--> Check that it fails correctly if lists of different shape are passed in
v0.9.1,Check that outputs have the correct shape
v0.9.1,Test continuous treatments with controls
v0.9.1,Test continuous treatments without controls
v0.9.1,Generate data with binary treatments
v0.9.1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.9.1,does not work well with parallelism.
v0.9.1,Test inputs for binary treatments
v0.9.1,--> Check that one can pass in regular lists
v0.9.1,--> Check that it fails correctly if lists of different shape are passed in
v0.9.1,"--> Check that it works when T, Y have shape (n, 1)"
v0.9.1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.9.1,--> Check that it fails correctly when the treatments are not numeric
v0.9.1,Check that outputs have the correct shape
v0.9.1,Test binary treatments with controls
v0.9.1,Test binary treatments without controls
v0.9.1,Only applicable to continuous treatments
v0.9.1,Generate data for 2 treatments
v0.9.1,Test multiple treatments with controls
v0.9.1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.9.1,The rest for controls. Just as an example.
v0.9.1,Generating A/B test data
v0.9.1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.9.1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.9.1,Create a wrapper around Lasso that doesn't support weights
v0.9.1,since Lasso does natively support them starting in sklearn 0.23
v0.9.1,Generate data with continuous treatments
v0.9.1,Instantiate model with most of the default parameters
v0.9.1,Compute the treatment effect on test points
v0.9.1,Compute treatment effect residuals
v0.9.1,Multiple treatments
v0.9.1,Allow at most 10% test points to be outside of the tolerance interval
v0.9.1,Compute treatment effect residuals
v0.9.1,Multiple treatments
v0.9.1,Allow at most 20% test points to be outside of the confidence interval
v0.9.1,Check that the intervals are not too wide
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.9.1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.9.1,ensure that we've got at least 6 of every element
v0.9.1,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.9.1,NOTE: this number may need to change if the default number of folds in
v0.9.1,WeightedStratifiedKFold changes
v0.9.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.1,ensure we can serialize the unfit estimator
v0.9.1,ensure we can pickle the fit estimator
v0.9.1,make sure we can call the marginal_effect and effect methods
v0.9.1,test const marginal inference
v0.9.1,test effect inference
v0.9.1,test marginal effect inference
v0.9.1,test coef__inference and intercept__inference
v0.9.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.1,"make sure we can call effect with implied scalar treatments,"
v0.9.1,"no matter the dimensions of T, and also that we warn when there"
v0.9.1,are multiple treatments
v0.9.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.1,ensure that we've got at least two of every element
v0.9.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.1,make sure we can call the marginal_effect and effect methods
v0.9.1,test const marginal inference
v0.9.1,test effect inference
v0.9.1,test marginal effect inference
v0.9.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.9.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.9.1,We concatenate the two copies data
v0.9.1,create a simple artificial setup where effect of moving from treatment
v0.9.1,"1 -> 2 is 2,"
v0.9.1,"1 -> 3 is 1, and"
v0.9.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.9.1,"Using an uneven number of examples from different classes,"
v0.9.1,"and having the treatments in non-lexicographic order,"
v0.9.1,Should rule out some basic issues.
v0.9.1,test that we can fit with a KFold instance
v0.9.1,test that we can fit with a train/test iterable
v0.9.1,predetermined splits ensure that all features are seen in each split
v0.9.1,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.9.1,(incorrectly) use a final model with an intercept
v0.9.1,"Because final model is fixed, actual values of T and Y don't matter"
v0.9.1,Ensure reproducibility
v0.9.1,Sparse DGP
v0.9.1,Treatment effect coef
v0.9.1,Other coefs
v0.9.1,Features and controls
v0.9.1,Test sparse estimator
v0.9.1,"--> test coef_, intercept_"
v0.9.1,--> test treatment effects
v0.9.1,Restrict x_test to vectors of norm < 1
v0.9.1,--> check inference
v0.9.1,Check that a majority of true effects lie in the 5-95% CI
v0.9.1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.9.1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.9.1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.9.1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.9.1,sparse test case: heterogeneous effect by product
v0.9.1,need at least as many rows in e_y as there are distinct columns
v0.9.1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.9.1,create a simple artificial setup where effect of moving from treatment
v0.9.1,"a -> b is 2,"
v0.9.1,"a -> c is 1, and"
v0.9.1,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.9.1,"Using an uneven number of examples from different classes,"
v0.9.1,"and having the treatments in non-lexicographic order,"
v0.9.1,should rule out some basic issues.
v0.9.1,Note that explicitly specifying the dtype as object is necessary until
v0.9.1,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.9.1,estimated effects should be identical when treatment is explicitly given
v0.9.1,but const_marginal_effect should be reordered based on the explicit cagetories
v0.9.1,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.9.1,test outer grouping
v0.9.1,test nested grouping
v0.9.1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.9.1,whichever groups we saw
v0.9.1,test nested grouping
v0.9.1,"by default, we use 5 split cross-validation for our T and Y models"
v0.9.1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.9.1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Set random seed
v0.9.1,Generate data
v0.9.1,DGP constants
v0.9.1,Test data
v0.9.1,Constant treatment effect and propensity
v0.9.1,Heterogeneous treatment and propensity
v0.9.1,ensure that we've got at least two of every element
v0.9.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.1,ensure that we can serialize unfit estimator
v0.9.1,ensure that we can serialize fit estimator
v0.9.1,make sure we can call the marginal_effect and effect methods
v0.9.1,test const marginal inference
v0.9.1,test effect inference
v0.9.1,test marginal effect inference
v0.9.1,test coef_ and intercept_ inference
v0.9.1,verify we can generate the summary
v0.9.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.9.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.9.1,create a simple artificial setup where effect of moving from treatment
v0.9.1,"1 -> 2 is 2,"
v0.9.1,"1 -> 3 is 1, and"
v0.9.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.9.1,"Using an uneven number of examples from different classes,"
v0.9.1,"and having the treatments in non-lexicographic order,"
v0.9.1,Should rule out some basic issues.
v0.9.1,test that we can fit with a KFold instance
v0.9.1,test that we can fit with a train/test iterable
v0.9.1,"for at least some of the examples, the CI should have nonzero width"
v0.9.1,"for at least some of the examples, the CI should have nonzero width"
v0.9.1,"for at least some of the examples, the CI should have nonzero width"
v0.9.1,test coef__inference function works
v0.9.1,test intercept__inference function works
v0.9.1,test summary function works
v0.9.1,Test inputs
v0.9.1,self._test_inputs(DR_learner)
v0.9.1,Test constant treatment effect
v0.9.1,Test heterogeneous treatment effect
v0.9.1,Test heterogenous treatment effect for W =/= None
v0.9.1,Sparse DGP
v0.9.1,Treatment effect coef
v0.9.1,Other coefs
v0.9.1,Features and controls
v0.9.1,Test sparse estimator
v0.9.1,"--> test coef_, intercept_"
v0.9.1,--> test treatment effects
v0.9.1,Restrict x_test to vectors of norm < 1
v0.9.1,--> check inference
v0.9.1,Check that a majority of true effects lie in the 5-95% CI
v0.9.1,test outer grouping
v0.9.1,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.9.1,test nested grouping
v0.9.1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.9.1,whichever groups we saw
v0.9.1,test nested grouping
v0.9.1,"by default, we use 5 split cross-validation for our T and Y models"
v0.9.1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.9.1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.9.1,helper class
v0.9.1,Fit learner and get the effect
v0.9.1,Get the true treatment effect
v0.9.1,Compute treatment effect residuals (absolute)
v0.9.1,Check that at least 90% of predictions are within tolerance interval
v0.9.1,Only for heterogeneous TE
v0.9.1,Fit learner on X and W and get the effect
v0.9.1,Get the true treatment effect
v0.9.1,Compute treatment effect residuals (absolute)
v0.9.1,Check that at least 90% of predictions are within tolerance interval
v0.9.1,Check that one can pass in regular lists
v0.9.1,Check that it fails correctly if lists of different shape are passed in
v0.9.1,Check that it fails when T contains values other than 0 and 1
v0.9.1,"Check that it works when T, Y have shape (n, 1)"
v0.9.1,Generate covariates
v0.9.1,Generate treatment
v0.9.1,Calculate outcome
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,DGP constants
v0.9.1,DGP coefficients
v0.9.1,Generated outcomes
v0.9.1,################
v0.9.1,WeightedLasso #
v0.9.1,################
v0.9.1,Define weights
v0.9.1,Define extended datasets
v0.9.1,Range of alphas
v0.9.1,Compare with Lasso
v0.9.1,--> No intercept
v0.9.1,--> With intercept
v0.9.1,When DGP has no intercept
v0.9.1,When DGP has intercept
v0.9.1,--> Coerce coefficients to be positive
v0.9.1,--> Toggle max_iter & tol
v0.9.1,Define weights
v0.9.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.9.1,Mixed DGP scenario.
v0.9.1,Define extended datasets
v0.9.1,Define weights
v0.9.1,Define multioutput
v0.9.1,##################
v0.9.1,WeightedLassoCV #
v0.9.1,##################
v0.9.1,Define alphas to test
v0.9.1,Compare with LassoCV
v0.9.1,--> No intercept
v0.9.1,--> With intercept
v0.9.1,--> Force parameters to be positive
v0.9.1,Choose a smaller n to speed-up process
v0.9.1,Compare fold weights
v0.9.1,Define weights
v0.9.1,Define extended datasets
v0.9.1,Define splitters
v0.9.1,WeightedKFold splitter
v0.9.1,Map weighted splitter to an extended splitter
v0.9.1,Define alphas to test
v0.9.1,Compare with LassoCV
v0.9.1,--> No intercept
v0.9.1,--> With intercept
v0.9.1,--> Force parameters to be positive
v0.9.1,###########################
v0.9.1,MultiTaskWeightedLassoCV #
v0.9.1,###########################
v0.9.1,Define alphas to test
v0.9.1,Define splitter
v0.9.1,Compare with MultiTaskLassoCV
v0.9.1,--> No intercept
v0.9.1,--> With intercept
v0.9.1,Define weights
v0.9.1,Define extended datasets
v0.9.1,Define splitters
v0.9.1,WeightedKFold splitter
v0.9.1,Map weighted splitter to an extended splitter
v0.9.1,Define alphas to test
v0.9.1,Compare with LassoCV
v0.9.1,--> No intercept
v0.9.1,--> With intercept
v0.9.1,#########################
v0.9.1,WeightedLassoCVWrapper #
v0.9.1,#########################
v0.9.1,perform 1D fit
v0.9.1,perform 2D fit
v0.9.1,################
v0.9.1,DebiasedLasso #
v0.9.1,################
v0.9.1,Test DebiasedLasso without weights
v0.9.1,--> Check debiased coeffcients without intercept
v0.9.1,--> Check debiased coeffcients with intercept
v0.9.1,--> Check 5-95 CI coverage for unit vectors
v0.9.1,Test DebiasedLasso with weights for one DGP
v0.9.1,Define weights
v0.9.1,Define extended datasets
v0.9.1,--> Check debiased coefficients
v0.9.1,Define weights
v0.9.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.9.1,--> Check debiased coeffcients
v0.9.1,Test that attributes propagate correctly
v0.9.1,Test MultiOutputDebiasedLasso without weights
v0.9.1,--> Check debiased coeffcients without intercept
v0.9.1,--> Check debiased coeffcients with intercept
v0.9.1,--> Check CI coverage
v0.9.1,Test MultiOutputDebiasedLasso with weights
v0.9.1,Define weights
v0.9.1,Define extended datasets
v0.9.1,--> Check debiased coefficients
v0.9.1,Unit vectors
v0.9.1,Unit vectors
v0.9.1,Check coeffcients and intercept are the same within tolerance
v0.9.1,Check results are similar with tolerance 1e-6
v0.9.1,Check if multitask
v0.9.1,Check that same alpha is chosen
v0.9.1,Check that the coefficients are similar
v0.9.1,selective ridge has a simple implementation that we can test against
v0.9.1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.9.1,"it should be the case that when we set fit_intercept to true,"
v0.9.1,it doesn't matter whether the penalized model also fits an intercept or not
v0.9.1,create an extra copy of rows with weight 2
v0.9.1,"instead of a slice, explicitly return an array of indices"
v0.9.1,_penalized_inds is only set during fitting
v0.9.1,cv exists on penalized model
v0.9.1,now we can access _penalized_inds
v0.9.1,check that we can read the cv attribute back out from the underlying model
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,DGP constants
v0.9.1,Define data features
v0.9.1,Added `_df`to names to be different from the default cate_estimator names
v0.9.1,Generate data
v0.9.1,################################
v0.9.1,Single treatment and outcome #
v0.9.1,################################
v0.9.1,Test LinearDML
v0.9.1,|--> Test featurizers
v0.9.1,ColumnTransformer doesn't propagate column names
v0.9.1,|--> Test re-fit
v0.9.1,Test SparseLinearDML
v0.9.1,Test ForestDML
v0.9.1,###################################
v0.9.1,Mutiple treatments and outcomes #
v0.9.1,###################################
v0.9.1,Test LinearDML
v0.9.1,Test SparseLinearDML
v0.9.1,"Single outcome only, ORF does not support multiple outcomes"
v0.9.1,Test DMLOrthoForest
v0.9.1,Test DROrthoForest
v0.9.1,Test XLearner
v0.9.1,Skipping population summary names test because bootstrap inference is too slow
v0.9.1,Test SLearner
v0.9.1,Test TLearner
v0.9.1,Test LinearDRLearner
v0.9.1,Test SparseLinearDRLearner
v0.9.1,Test ForestDRLearner
v0.9.1,Test LinearIntentToTreatDRIV
v0.9.1,Test DeepIV
v0.9.1,Test categorical treatments
v0.9.1,Check refit
v0.9.1,Check refit after setting categories
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Linear models are required for parametric dml
v0.9.1,sample weighting models are required for nonparametric dml
v0.9.1,Test values
v0.9.1,TLearner test
v0.9.1,Instantiate TLearner
v0.9.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.1,Test constant treatment effect with multi output Y
v0.9.1,Test heterogeneous treatment effect
v0.9.1,Need interactions between T and features
v0.9.1,Test heterogeneous treatment effect with multi output Y
v0.9.1,Instantiate DomainAdaptationLearner
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,test base values equals to mean of constant marginal effect
v0.9.1,test shape of shap values output is as expected
v0.9.1,test shape of attribute of explanation object is as expected
v0.9.1,test length of feature names equals to shap values shape
v0.9.1,test base values equals to mean of constant marginal effect
v0.9.1,test shape of shap values output is as expected
v0.9.1,test shape of attribute of explanation object is as expected
v0.9.1,test length of feature names equals to shap values shape
v0.9.1,Treatment effect function
v0.9.1,Outcome support
v0.9.1,Treatment support
v0.9.1,"Generate controls, covariates, treatments and outcomes"
v0.9.1,Heterogeneous treatment effects
v0.9.1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.9.1,through shap package.
v0.9.1,test shap could generate the plot from the shap_values
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Check inputs
v0.9.1,Check inputs
v0.9.1,Check inputs
v0.9.1,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.9.1,"Thus, we append the controls column before the one-hot-encoded T"
v0.9.1,"We might want to revisit, though, since it's linearly determined by the others"
v0.9.1,Check inputs
v0.9.1,Check inputs
v0.9.1,Estimate response function
v0.9.1,Check inputs
v0.9.1,Train model on controls. Assign higher weight to units resembling
v0.9.1,treated units.
v0.9.1,Train model on the treated. Assign higher weight to units resembling
v0.9.1,control units.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.9.1,output is
v0.9.1,"* a column of ones if X, W, and Z are all None"
v0.9.1,* just X or W or Z if both of the others are None
v0.9.1,* hstack([arrs]) for whatever subset are not None otherwise
v0.9.1,ensure Z is 2D
v0.9.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.1,We need to go back to the label representation of the one-hot so as to call
v0.9.1,the classifier.
v0.9.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.1,We need to go back to the label representation of the one-hot so as to call
v0.9.1,the classifier.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,TODO: make sure to use random seeds wherever necessary
v0.9.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.9.1,"unfortunately with the Theano and Tensorflow backends,"
v0.9.1,the straightforward use of K.stop_gradient can cause an error
v0.9.1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.9.1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.9.1,so that those layers remain connected but with 0 gradient
v0.9.1,|| t - mu_i || ^2
v0.9.1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.9.1,Use logsumexp for numeric stability:
v0.9.1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.9.1,TODO: does the numeric stability actually make any difference?
v0.9.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.9.1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.9.1,generate cumulative sum via matrix multiplication
v0.9.1,"Generate standard uniform values in shape (batch_size,1)"
v0.9.1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.9.1,we use uniform_like instead with an input of an appropriate shape)
v0.9.1,convert to floats and multiply to perform equivalent of logical AND
v0.9.1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.9.1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.9.1,we use normal_like instead with an input of an appropriate shape)
v0.9.1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.9.1,prevent gradient from passing through sampling
v0.9.1,three options: biased or upper-bound loss require a single number of samples;
v0.9.1,unbiased can take different numbers for the network and its gradient
v0.9.1,"sample: (() -> Layer, int) -> Layer"
v0.9.1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.9.1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.9.1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.9.1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.9.1,the dimensionality of the output of the network
v0.9.1,TODO: is there a more robust way to do this?
v0.9.1,TODO: do we need to give the user more control over other arguments to fit?
v0.9.1,"subtle point: we need to build a new model each time,"
v0.9.1,because each model encapsulates its randomness
v0.9.1,TODO: do we need to give the user more control over other arguments to fit?
v0.9.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.9.1,not a general tensor (because of how backprop works in every framework)
v0.9.1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.9.1,but this seems annoying...)
v0.9.1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.9.1,TODO: any way to get this to work on batches of arbitrary size?
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Estimate final model of theta(X) by minimizing the square loss:
v0.9.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.9.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.9.1,at the expense of some small bias. For points with very small covariance we revert
v0.9.1,to the model-based preliminary estimate and do not add the correction term.
v0.9.1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.9.1,"instruments, and outcomes"
v0.9.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.9.1,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.9.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.1,for internal use by the library
v0.9.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.1,"we need to undo the one-hot encoding for calling effect,"
v0.9.1,since it expects raw values
v0.9.1,"we need to undo the one-hot encoding for calling effect,"
v0.9.1,since it expects raw values
v0.9.1,"TODO: check that Y, T, Z do not have multiple columns"
v0.9.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.9.1,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.9.1,since we would actually be calculating a CATE rather than ATE.
v0.9.1,TODO: allow the final model to actually use X?
v0.9.1,TODO: allow the final model to actually use X?
v0.9.1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.1,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.9.1,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.9.1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.1,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.1,for internal use by the library
v0.9.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,"this will have dimension (d,) + shape(X)"
v0.9.1,send the first dimension to the end
v0.9.1,columns are featurized independently; partial derivatives are only non-zero
v0.9.1,when taken with respect to the same column each time
v0.9.1,don't fit intercept; manually add column of ones to the data instead;
v0.9.1,this allows us to ignore the intercept when computing marginal effects
v0.9.1,make T 2D if if was a vector
v0.9.1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.9.1,two stage approximation
v0.9.1,"first, get basis expansions of T, X, and Z"
v0.9.1,TODO: is it right that the effective number of intruments is the
v0.9.1,"product of ft_X and ft_Z, not just ft_Z?"
v0.9.1,"regress T expansion on X,Z expansions concatenated with W"
v0.9.1,"predict ft_T from interacted ft_X, ft_Z"
v0.9.1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.9.1,dT may be only 2-dimensional)
v0.9.1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.9.1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,TODO: conisder working around relying on sklearn implementation details
v0.9.1,"Found a good split, return."
v0.9.1,Record all splits in case the stratification by weight yeilds a worse partition
v0.9.1,Reseed random generator and try again
v0.9.1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.9.1,"Found a good split, return."
v0.9.1,Did not find a good split
v0.9.1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.9.1,Return most weight-balanced partition
v0.9.1,Weight stratification algorithm
v0.9.1,Sort weights for weight strata search
v0.9.1,There are some leftover indices that have yet to be assigned
v0.9.1,Append stratum splits to overall splits
v0.9.1,"If classification methods produce multiple columns of output,"
v0.9.1,we need to manually encode classes to ensure consistent column ordering.
v0.9.1,We clone the estimator to make sure that all the folds are
v0.9.1,"independent, and that it is pickle-able."
v0.9.1,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.9.1,`predictions` is a list of method outputs from each fold.
v0.9.1,"If each of those is also a list, then treat this as a"
v0.9.1,multioutput-multiclass task. We need to separately concatenate
v0.9.1,the method outputs for each label into an `n_labels` long list.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Our classes that derive from sklearn ones sometimes include
v0.9.1,inherited docstrings that have embedded doctests; we need the following imports
v0.9.1,so that they don't break.
v0.9.1,TODO: consider working around relying on sklearn implementation details
v0.9.1,"Convert X, y into numpy arrays"
v0.9.1,Define fit parameters
v0.9.1,Some algorithms don't have a check_input option
v0.9.1,Check weights array
v0.9.1,Check that weights are size-compatible
v0.9.1,Normalize inputs
v0.9.1,Weight inputs
v0.9.1,Fit base class without intercept
v0.9.1,Fit Lasso
v0.9.1,Reset intercept
v0.9.1,The intercept is not calculated properly due the sqrt(weights) factor
v0.9.1,so it must be recomputed
v0.9.1,Fit lasso without weights
v0.9.1,Make weighted splitter
v0.9.1,Fit weighted model
v0.9.1,Make weighted splitter
v0.9.1,Fit weighted model
v0.9.1,Call weighted lasso on reduced design matrix
v0.9.1,Weighted tau
v0.9.1,Select optimal penalty
v0.9.1,Warn about consistency
v0.9.1,"Convert X, y into numpy arrays"
v0.9.1,Fit weighted lasso with user input
v0.9.1,"Center X, y"
v0.9.1,Calculate quantities that will be used later on. Account for centered data
v0.9.1,Calculate coefficient and error variance
v0.9.1,Add coefficient correction
v0.9.1,Set coefficients and intercept standard errors
v0.9.1,Set intercept
v0.9.1,Return alpha to 'auto' state
v0.9.1,"Note that in the case of no intercept, X_offset is 0"
v0.9.1,Calculate the variance of the predictions
v0.9.1,Calculate prediction confidence intervals
v0.9.1,Assumes flattened y
v0.9.1,Compute weighted residuals
v0.9.1,To be done once per target. Assumes y can be flattened.
v0.9.1,Assumes that X has already been offset
v0.9.1,Special case: n_features=1
v0.9.1,Compute Lasso coefficients for the columns of the design matrix
v0.9.1,Compute C_hat
v0.9.1,Compute theta_hat
v0.9.1,Allow for single output as well
v0.9.1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.9.1,Set coef_ attribute
v0.9.1,Set intercept_ attribute
v0.9.1,Set selected_alpha_ attribute
v0.9.1,Set coef_stderr_
v0.9.1,intercept_stderr_
v0.9.1,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.9.1,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.9.1,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.9.1,set intercept_ attribute
v0.9.1,set coef_ attribute
v0.9.1,set alpha_ attribute
v0.9.1,set alphas_ attribute
v0.9.1,set n_iter_ attribute
v0.9.1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.9.1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.9.1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.9.1,now regress X1 on y - X2 * beta2 to learn beta1
v0.9.1,set coef_ and intercept_ attributes
v0.9.1,Note that the penalized model should *not* have an intercept
v0.9.1,don't proxy special methods
v0.9.1,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.9.1,regressor incorrectly
v0.9.1,"Note: for known attributes that have been set this method will not be called,"
v0.9.1,so we should just throw here because this is an attribute belonging to this class
v0.9.1,but which hasn't yet been set on this instance
v0.9.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,AzureML
v0.9.1,helper imports
v0.9.1,write the details of the workspace to a configuration file to the notebook library
v0.9.1,if y is a multioutput model
v0.9.1,Make sure second dimension has 1 or more item
v0.9.1,switch _inner Model to a MultiOutputRegressor
v0.9.1,flatten array as automl only takes vectors for y
v0.9.1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.9.1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.9.1,as an sklearn estimator
v0.9.1,fit implementation for a single output model.
v0.9.1,Create experiment for specified workspace
v0.9.1,Configure automl_config with training set information.
v0.9.1,"Wait for remote run to complete, the set the model"
v0.9.1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.9.1,create model and pass model into final.
v0.9.1,"If item is an automl config, get its corresponding"
v0.9.1,AutomatedML Model and add it to new_Args
v0.9.1,"If item is an automl config, get its corresponding"
v0.9.1,AutomatedML Model and set it for this key in
v0.9.1,kwargs
v0.9.1,takes in either automated_ml config and instantiates
v0.9.1,an AutomatedMLModel
v0.9.1,The prefix can only be 18 characters long
v0.9.1,"because prefixes come from kwarg_names, we must ensure they are"
v0.9.1,short enough.
v0.9.1,Get workspace from config file.
v0.9.1,Take the intersect of the white for sample
v0.9.1,weights and linear models
v0.9.1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,TODO: generalize to multiple treatment case?
v0.9.1,get index of best treatment
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,make any access to matplotlib or plt throw an exception
v0.9.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.9.1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.9.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.9.1,clean way of achieving this
v0.9.1,make sure we don't accidentally escape anything in the substitution
v0.9.1,Fetch appropriate color for node
v0.9.1,"red for negative, green for positive"
v0.9.1,in multi-target use first target
v0.9.1,Write node mean CATE
v0.9.1,Write node std of CATE
v0.9.1,Fetch appropriate color for node
v0.9.1,"red for negative, green for positive"
v0.9.1,Write node mean CATE
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,TODO: consider working around relying on sklearn implementation details
v0.9.1,Create splits of causal tree
v0.9.1,Make sure the correct exception is being rethrown
v0.9.1,Must make sure indices are merged correctly
v0.9.1,Convert rows to columns
v0.9.1,Require group assignment t to be one-hot-encoded
v0.9.1,Get predictions for the 2 splits
v0.9.1,Must make sure indices are merged correctly
v0.9.1,Crossfitting
v0.9.1,Compute weighted nuisance estimates
v0.9.1,-------------------------------------------------------------------------------
v0.9.1,Calculate the covariance matrix corresponding to the BLB inference
v0.9.1,
v0.9.1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.9.1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.9.1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.9.1,in that slice from the overall parameter estimate.
v0.9.1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.9.1,-------------------------------------------------------------------------------
v0.9.1,Calclulate covariance matrix through BLB
v0.9.1,Estimators
v0.9.1,OrthoForest parameters
v0.9.1,Sub-forests
v0.9.1,Auxiliary attributes
v0.9.1,Fit check
v0.9.1,TODO: Check performance
v0.9.1,Must normalize weights
v0.9.1,Override the CATE inference options
v0.9.1,Add blb inference to parent's options
v0.9.1,Generate subsample indices
v0.9.1,Build trees in parallel
v0.9.1,Bootstraping has repetitions in tree sample
v0.9.1,Similar for `a` weights
v0.9.1,Bootstraping has repetitions in tree sample
v0.9.1,Define subsample size
v0.9.1,Safety check
v0.9.1,Draw points to create little bags
v0.9.1,Copy and/or define models
v0.9.1,Define nuisance estimators
v0.9.1,Define parameter estimators
v0.9.1,Define
v0.9.1,Need to redefine fit here for auto inference to work due to a quirk in how
v0.9.1,wrap_fit is defined
v0.9.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.9.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.9.1,Override to flatten output if T is flat
v0.9.1,Check that all discrete treatments are represented
v0.9.1,Nuissance estimates evaluated with cross-fitting
v0.9.1,Define 2-fold iterator
v0.9.1,Check if there is only one example of some class
v0.9.1,Define 2-fold iterator
v0.9.1,need safe=False when cloning for WeightedModelWrapper
v0.9.1,Compute residuals
v0.9.1,Compute coefficient by OLS on residuals
v0.9.1,"Parameter returned by LinearRegression is (d_T, )"
v0.9.1,Compute residuals
v0.9.1,Compute coefficient by OLS on residuals
v0.9.1,ell_2 regularization
v0.9.1,Ridge regression estimate
v0.9.1,"Parameter returned is of shape (d_T, )"
v0.9.1,Return moments and gradients
v0.9.1,Compute residuals
v0.9.1,Compute moments
v0.9.1,"Moments shape is (n, d_T)"
v0.9.1,Compute moment gradients
v0.9.1,returns shape-conforming residuals
v0.9.1,Copy and/or define models
v0.9.1,Define parameter estimators
v0.9.1,Define moment and mean gradient estimator
v0.9.1,"Check that T is shape (n, )"
v0.9.1,Check T is numeric
v0.9.1,Train label encoder
v0.9.1,Call `fit` from parent class
v0.9.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.9.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.9.1,Override to flatten output if T is flat
v0.9.1,Expand one-hot encoding to include the zero treatment
v0.9.1,"Test that T contains all treatments. If not, return None"
v0.9.1,Nuissance estimates evaluated with cross-fitting
v0.9.1,Define 2-fold iterator
v0.9.1,Check if there is only one example of some class
v0.9.1,No need to crossfit for internal nodes
v0.9.1,Compute partial moments
v0.9.1,"If any of the values in the parameter estimate is nan, return None"
v0.9.1,Compute partial moments
v0.9.1,Compute coefficient by OLS on residuals
v0.9.1,ell_2 regularization
v0.9.1,Ridge regression estimate
v0.9.1,"Parameter returned is of shape (d_T, )"
v0.9.1,Return moments and gradients
v0.9.1,Compute partial moments
v0.9.1,Compute moments
v0.9.1,"Moments shape is (n, d_T-1)"
v0.9.1,Compute moment gradients
v0.9.1,Need to calculate this in an elegant way for when propensity is 0
v0.9.1,This will flatten T
v0.9.1,Check that T is numeric
v0.9.1,Test whether the input estimator is supported
v0.9.1,Calculate confidence intervals for the parameter (marginal effect)
v0.9.1,Calculate confidence intervals for the effect
v0.9.1,Calculate the effects
v0.9.1,Calculate the standard deviations for the effects
v0.9.1,d_t=None here since we measure the effect across all Ts
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.1,Licensed under the MIT License.
v0.9.1,Causal tree parameters
v0.9.1,Tree structure
v0.9.1,No need for a random split since the data is already
v0.9.1,a random subsample from the original input
v0.9.1,node list stores the nodes that are yet to be splitted
v0.9.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.9.1,Create local sample set
v0.9.1,Compute nuisance estimates for the current node
v0.9.1,Nuisance estimate cannot be calculated
v0.9.1,Estimate parameter for current node
v0.9.1,Node estimate cannot be calculated
v0.9.1,Calculate moments and gradient of moments for current data
v0.9.1,Calculate inverse gradient
v0.9.1,The gradient matrix is not invertible.
v0.9.1,No good split can be found
v0.9.1,Calculate point-wise pseudo-outcomes rho
v0.9.1,a split is determined by a feature and a sample pair
v0.9.1,the number of possible splits is at most (number of features) * (number of node samples)
v0.9.1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.9.1,parse row and column of random pair
v0.9.1,the sample of the pair is the integer division of the random number with n_feats
v0.9.1,calculate the binary indicator of whether sample i is on the left or the right
v0.9.1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.9.1,calculate the number of samples on the left child for each proposed split
v0.9.1,calculate the analogous binary indicator for the samples in the estimation set
v0.9.1,calculate the number of estimation samples on the left child of each proposed split
v0.9.1,find the upper and lower bound on the size of the left split for the split
v0.9.1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.9.1,on each side.
v0.9.1,similarly for the estimation sample set
v0.9.1,if there is no valid split then don't create any children
v0.9.1,filter only the valid splits
v0.9.1,calculate the average influence vector of the samples in the left child
v0.9.1,calculate the average influence vector of the samples in the right child
v0.9.1,take the square of each of the entries of the influence vectors and normalize
v0.9.1,by size of each child
v0.9.1,calculate the vector score of each candidate split as the average of left and right
v0.9.1,influence vectors
v0.9.1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.9.1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.9.1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.9.1,calculate the scalar score of each split by aggregating across the vector of scores
v0.9.1,Find split that minimizes criterion
v0.9.1,Create child nodes with corresponding subsamples
v0.9.1,add the created children to the list of not yet split nodes
v0.9.0,"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
v0.9.0,configuration is all pulled from setup.cfg
v0.9.0,-*- coding: utf-8 -*-
v0.9.0,
v0.9.0,Configuration file for the Sphinx documentation builder.
v0.9.0,
v0.9.0,This file does only contain a selection of the most common options. For a
v0.9.0,full list see the documentation:
v0.9.0,http://www.sphinx-doc.org/en/master/config
v0.9.0,-- Path setup --------------------------------------------------------------
v0.9.0,"If extensions (or modules to document with autodoc) are in another directory,"
v0.9.0,add these directories to sys.path here. If the directory is relative to the
v0.9.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.9.0,
v0.9.0,-- Project information -----------------------------------------------------
v0.9.0,-- General configuration ---------------------------------------------------
v0.9.0,"If your documentation needs a minimal Sphinx version, state it here."
v0.9.0,
v0.9.0,needs_sphinx = '1.0'
v0.9.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.9.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.9.0,ones.
v0.9.0,"Add any paths that contain templates here, relative to this directory."
v0.9.0,The suffix(es) of source filenames.
v0.9.0,You can specify multiple suffix as a list of string:
v0.9.0,
v0.9.0,"source_suffix = ['.rst', '.md']"
v0.9.0,The master toctree document.
v0.9.0,The language for content autogenerated by Sphinx. Refer to documentation
v0.9.0,for a list of supported languages.
v0.9.0,
v0.9.0,This is also used if you do content translation via gettext catalogs.
v0.9.0,"Usually you set ""language"" from the command line for these cases."
v0.9.0,"List of patterns, relative to source directory, that match files and"
v0.9.0,directories to ignore when looking for source files.
v0.9.0,This pattern also affects html_static_path and html_extra_path.
v0.9.0,The name of the Pygments (syntax highlighting) style to use.
v0.9.0,-- Options for HTML output -------------------------------------------------
v0.9.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.9.0,a list of builtin themes.
v0.9.0,
v0.9.0,Theme options are theme-specific and customize the look and feel of a theme
v0.9.0,"further.  For a list of options available for each theme, see the"
v0.9.0,documentation.
v0.9.0,
v0.9.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.9.0,"relative to this directory. They are copied after the builtin static files,"
v0.9.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.9.0,html_static_path = ['_static']
v0.9.0,"Custom sidebar templates, must be a dictionary that maps document names"
v0.9.0,to template names.
v0.9.0,
v0.9.0,The default sidebars (for documents that don't match any pattern) are
v0.9.0,defined by theme itself.  Builtin themes are using these templates by
v0.9.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.9.0,'searchbox.html']``.
v0.9.0,
v0.9.0,html_sidebars = {}
v0.9.0,-- Options for HTMLHelp output ---------------------------------------------
v0.9.0,Output file base name for HTML help builder.
v0.9.0,-- Options for LaTeX output ------------------------------------------------
v0.9.0,The paper size ('letterpaper' or 'a4paper').
v0.9.0,
v0.9.0,"'papersize': 'letterpaper',"
v0.9.0,"The font size ('10pt', '11pt' or '12pt')."
v0.9.0,
v0.9.0,"'pointsize': '10pt',"
v0.9.0,Additional stuff for the LaTeX preamble.
v0.9.0,
v0.9.0,"'preamble': '',"
v0.9.0,Latex figure (float) alignment
v0.9.0,
v0.9.0,"'figure_align': 'htbp',"
v0.9.0,Grouping the document tree into LaTeX files. List of tuples
v0.9.0,"(source start file, target name, title,"
v0.9.0,"author, documentclass [howto, manual, or own class])."
v0.9.0,-- Options for manual page output ------------------------------------------
v0.9.0,One entry per manual page. List of tuples
v0.9.0,"(source start file, name, description, authors, manual section)."
v0.9.0,-- Options for Texinfo output ----------------------------------------------
v0.9.0,Grouping the document tree into Texinfo files. List of tuples
v0.9.0,"(source start file, target name, title, author,"
v0.9.0,"dir menu entry, description, category)"
v0.9.0,-- Options for Epub output -------------------------------------------------
v0.9.0,Bibliographic Dublin Core info.
v0.9.0,The unique identifier of the text. This can be a ISBN number
v0.9.0,or the project homepage.
v0.9.0,
v0.9.0,epub_identifier = ''
v0.9.0,A unique identification for the text.
v0.9.0,
v0.9.0,epub_uid = ''
v0.9.0,A list of files that should not be packed into the epub file.
v0.9.0,-- Extension configuration -------------------------------------------------
v0.9.0,-- Options for intersphinx extension ---------------------------------------
v0.9.0,Example configuration for intersphinx: refer to the Python standard library.
v0.9.0,-- Options for todo extension ----------------------------------------------
v0.9.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.9.0,-- Options for doctest extension -------------------------------------------
v0.9.0,we can document otherwise excluded entities here by returning False
v0.9.0,or skip otherwise included entities by returning True
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Calculate residuals
v0.9.0,Estimate E[T_res | Z_res]
v0.9.0,TODO. Deal with multi-class instrument
v0.9.0,Calculate nuisances
v0.9.0,Estimate E[T_res | Z_res]
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"We do a three way split, as typically a preliminary theta estimator would require"
v0.9.0,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.9.0,TODO. Deal with multi-class instrument
v0.9.0,Estimate final model of theta(X) by minimizing the square loss:
v0.9.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.9.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.9.0,at the expense of some small bias. For points with very small covariance we revert
v0.9.0,to the model-based preliminary estimate and do not add the correction term.
v0.9.0,Estimate preliminary theta in cross fitting manner
v0.9.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.0,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.9.0,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.9.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.0,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.9.0,TODO. The solution below is not really a valid cross-fitting
v0.9.0,as the test data are used to create the proj_t on the train
v0.9.0,which in the second train-test loop is used to create the nuisance
v0.9.0,cov on the test data. Hence the T variable of some sample
v0.9.0,"is implicitly correlated with its cov nuisance, through this flow"
v0.9.0,"of information. However, this seems a rather weak correlation."
v0.9.0,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.9.0,model.
v0.9.0,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.9.0,Estimate preliminary theta in cross fitting manner
v0.9.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.0,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.9.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.0,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.9.0,#############################################################################
v0.9.0,Classes for the DRIV implementation for the special case of intent-to-treat
v0.9.0,A/B test
v0.9.0,#############################################################################
v0.9.0,Estimate preliminary theta in cross fitting manner
v0.9.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.0,We can use statsmodel for all hypothesis testing capabilities
v0.9.0,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.9.0,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.9.0,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.9.0,model_T_XZ = lambda: model_clf()
v0.9.0,#'days_visited': lambda:
v0.9.0,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.9.0,Turn strings into categories for numeric mapping
v0.9.0,### Defining some generic regressors and classifiers
v0.9.0,This a generic non-parametric regressor
v0.9.0,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.9.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.9.0,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.9.0,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.9.0,model = lambda: RandomForestRegressor(n_estimators=100)
v0.9.0,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.9.0,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.9.0,model = lambda: LinearRegression(n_jobs=-1)
v0.9.0,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.9.0,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.9.0,underlying model whenever predict is called.
v0.9.0,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.9.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.9.0,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.9.0,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.9.0,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.9.0,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.9.0,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.9.0,We need to specify models to be used for each of these residualizations
v0.9.0,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.9.0,"E[T | X, Z]"
v0.9.0,E[TZ | X]
v0.9.0,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.9.0,n_splits determines the number of splits to be used for cross-fitting.
v0.9.0,# Algorithm 2 - Current Method
v0.9.0,In[121]:
v0.9.0,# Algorithm 3 - DRIV ATE
v0.9.0,dmliv_model_effect = lambda: model()
v0.9.0,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.9.0,"dmliv_model_effect(),"
v0.9.0,n_splits=1)
v0.9.0,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.9.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.9.0,"Once multiple treatments are supported, we'll need to fix this"
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.0,We can use statsmodel for all hypothesis testing capabilities
v0.9.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.0,We can use statsmodel for all hypothesis testing capabilities
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,TODO. Deal with multi-class instrument/treatment
v0.9.0,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.9.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.9.0,Estimate p(X) = E[T | X] in cross-fitting manner
v0.9.0,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.9.0,##################
v0.9.0,Global settings #
v0.9.0,##################
v0.9.0,Global plotting controls
v0.9.0,"Control for support size, can control for more"
v0.9.0,#################
v0.9.0,File utilities #
v0.9.0,#################
v0.9.0,#################
v0.9.0,Plotting utils #
v0.9.0,#################
v0.9.0,bias
v0.9.0,var
v0.9.0,rmse
v0.9.0,r2
v0.9.0,Infer feature dimension
v0.9.0,Metrics by support plots
v0.9.0,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.9.0,Vasilis Syrgkanis <vasy@microsoft.com>
v0.9.0,Steven Wu <zhiww@microsoft.com>
v0.9.0,Initialize causal tree parameters
v0.9.0,Create splits of causal tree
v0.9.0,Estimate treatment effects at the leafs
v0.9.0,Compute heterogeneous treatement effect for x's in x_list by finding
v0.9.0,the corresponding split and associating the effect computed on that leaf
v0.9.0,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.9.0,Safety check
v0.9.0,Weighted linear regression
v0.9.0,Calculates weights
v0.9.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.9.0,over all indices
v0.9.0,Similar for `a` weights
v0.9.0,Doesn't have sample weights
v0.9.0,Is a linear model
v0.9.0,Weighted linear regression
v0.9.0,Calculates weights
v0.9.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.9.0,over all indices
v0.9.0,Similar for `a` weights
v0.9.0,normalize weights
v0.9.0,"Split the data in half, train and test"
v0.9.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.9.0,"a function of W, using only the train fold"
v0.9.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.9.0,"Split the data in half, train and test"
v0.9.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.9.0,"a function of W, using only the train fold"
v0.9.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.9.0,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.9.0,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.9.0,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.9.0,"Split the data in half, train and test"
v0.9.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.9.0,"a function of x, using only the train fold"
v0.9.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.9.0,Compute coefficient by OLS on residuals
v0.9.0,"Split the data in half, train and test"
v0.9.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.9.0,"a function of x, using only the train fold"
v0.9.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.9.0,Estimate multipliers for second order orthogonal method
v0.9.0,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.9.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.9.0,Create local sample set
v0.9.0,compute the base estimate for the current node using double ml or second order double ml
v0.9.0,compute the influence functions here that are used for the criterion
v0.9.0,generate random proposals of dimensions to split
v0.9.0,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.9.0,compute criterion for each proposal
v0.9.0,if splitting creates valid leafs in terms of mean leaf size
v0.9.0,Calculate criterion for split
v0.9.0,Else set criterion to infinity so that this split is not chosen
v0.9.0,If no good split was found
v0.9.0,Find split that minimizes criterion
v0.9.0,Set the split attributes at the node
v0.9.0,Create child nodes with corresponding subsamples
v0.9.0,Recursively split children
v0.9.0,Return parent node
v0.9.0,estimate the local parameter at the leaf using the estimate data
v0.9.0,###################
v0.9.0,Argument parsing #
v0.9.0,###################
v0.9.0,#########################################
v0.9.0,Parameters constant across experiments #
v0.9.0,#########################################
v0.9.0,Outcome support
v0.9.0,Treatment support
v0.9.0,Evaluation grid
v0.9.0,Treatment effects array
v0.9.0,Other variables
v0.9.0,##########################
v0.9.0,Data Generating Process #
v0.9.0,##########################
v0.9.0,Log iteration
v0.9.0,"Generate controls, features, treatment and outcome"
v0.9.0,T and Y residuals to be used in later scripts
v0.9.0,Save generated dataset
v0.9.0,#################
v0.9.0,ORF parameters #
v0.9.0,#################
v0.9.0,######################################
v0.9.0,Train and evaluate treatment effect #
v0.9.0,######################################
v0.9.0,########
v0.9.0,Plots #
v0.9.0,########
v0.9.0,###############
v0.9.0,Save results #
v0.9.0,###############
v0.9.0,##############
v0.9.0,Run Rscript #
v0.9.0,##############
v0.9.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.9.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.9.0,"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
v0.9.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
v0.9.0,def mlasso_model(): return MultiTaskLassoCV(
v0.9.0,"cv=3, alphas=alpha_regs, max_iter=200)"
v0.9.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.9.0,heterogeneity
v0.9.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.9.0,heterogeneity
v0.9.0,The first s_x state variables are confounders. The final s_x variables are exogenous and can create
v0.9.0,heterogeneity
v0.9.0,"alpha_regs = [5e-3, 1e-2, 5e-2]"
v0.9.0,"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
v0.9.0,"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
v0.9.0,subset of features that are exogenous and create heterogeneity
v0.9.0,strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
v0.9.0,subset of features wrt we estimate heterogeneity
v0.9.0,"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
v0.9.0,"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,introspect the constructor arguments to find the model parameters
v0.9.0,to represent
v0.9.0,Extract and sort argument names excluding 'self'
v0.9.0,create dataframe
v0.9.0,currently dowhy only support single outcome and single treatment
v0.9.0,column names
v0.9.0,"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
v0.9.0,cate estimator but not the effect.
v0.9.0,don't proxy special methods
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Check if model is sparse enough for this model
v0.9.0,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.9.0,TODO: any way to avoid creating a copy if the array was already dense?
v0.9.0,"the call is necessary if the input was something like a list, though"
v0.9.0,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.9.0,so convert to pydata sparse first
v0.9.0,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.9.0,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.9.0,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.9.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.9.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.9.0,Type to column extraction function
v0.9.0,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.9.0,same number of input definitions as arrays
v0.9.0,input definitions have same number of dimensions as each array
v0.9.0,all result indices are unique
v0.9.0,all result indices must match at least one input index
v0.9.0,"map indices to all array, axis pairs for that index"
v0.9.0,each index has the same cardinality wherever it appears
v0.9.0,"State: list of (set of letters, list of (corresponding indices, value))"
v0.9.0,Algo: while list contains more than one entry
v0.9.0,take two entries
v0.9.0,sort both lists by intersection of their indices
v0.9.0,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.9.0,"take the union of indices and the product of values), stepping through each list linearly"
v0.9.0,TODO: might be faster to break into connected components first
v0.9.0,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.9.0,"so compute their content separately, then take cartesian product"
v0.9.0,this would save a few pointless sorts by empty tuples
v0.9.0,TODO: Consider investigating other performance ideas for these cases
v0.9.0,where the dense method beat the sparse method (usually sparse is faster)
v0.9.0,"e,facd,c->cfed"
v0.9.0,sparse: 0.0335489
v0.9.0,dense:  0.011465999999999997
v0.9.0,"gbd,da,egb->da"
v0.9.0,sparse: 0.0791625
v0.9.0,dense:  0.007319099999999995
v0.9.0,"dcc,d,faedb,c->abe"
v0.9.0,sparse: 1.2868097
v0.9.0,dense:  0.44605229999999985
v0.9.0,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.9.0,TODO: would using einsum's paths to optimize the order of merging help?
v0.9.0,assume that we should perform nested cross-validation if and only if
v0.9.0,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.9.0,logic copied from check_cv
v0.9.0,otherwise we will assume the user already set the cv attribute to something
v0.9.0,compatible with splitting with a 'groups' argument
v0.9.0,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.9.0,don't use the groups when calling split internally
v0.9.0,Normalize weights
v0.9.0,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.9.0,"if we're decorating a class, just update the __init__ method,"
v0.9.0,so that the result is still a class instead of a wrapper method
v0.9.0,"want to enforce that each bad_arg was either in kwargs,"
v0.9.0,or else it was in neither and is just taking its default value
v0.9.0,Any access should throw
v0.9.0,"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.0,define the index of d_x to filter for each given T
v0.9.0,filter X after broadcast with T for each given T
v0.9.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.0,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.0,Licensed under the MIT License.
v0.9.0,"since inference objects can be stateful, we must copy it before fitting;"
v0.9.0,otherwise this sequence wouldn't work:
v0.9.0,"est1.fit(..., inference=inf)"
v0.9.0,"est2.fit(..., inference=inf)"
v0.9.0,est1.effect_interval(...)
v0.9.0,because inf now stores state from fitting est2
v0.9.0,This flag is true when names are set in a child class instead
v0.9.0,"If names are set in a child class, add an attribute reflecting that"
v0.9.0,This works only if X is passed as a kwarg
v0.9.0,We plan to enforce X as kwarg only in future releases
v0.9.0,This checks if names have been set in a child class
v0.9.0,"If names were set in a child class, don't do it again"
v0.9.0,"Wraps-up fit by setting attributes, cleaning up, etc."
v0.9.0,call the wrapped fit method
v0.9.0,NOTE: we call inference fit *after* calling the main fit method
v0.9.0,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.9.0,but tensordot can't be applied to this problem because we don't sum over m
v0.9.0,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.9.0,of rows of T was not taken into account
v0.9.0,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.9.0,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.9.0,"Treatment names is None, default to BaseCateEstimator"
v0.9.0,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.9.0,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.9.0,Get input names
v0.9.0,Summary
v0.9.0,add statsmodels to parent's options
v0.9.0,add debiasedlasso to parent's options
v0.9.0,add blb to parent's options
v0.9.0,TODO Share some logic with non-discrete version
v0.9.0,Get input names
v0.9.0,Summary
v0.9.0,add statsmodels to parent's options
v0.9.0,add statsmodels to parent's options
v0.9.0,add blb to parent's options
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,remove None arguments
v0.9.0,"scores entries should be lists of scores, so make each entry a singleton list"
v0.9.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.9.0,generate an instance of the final model
v0.9.0,generate an instance of the nuisance model
v0.9.0,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.9.0,alteration even when we only want to fit_final.
v0.9.0,use a binary array to get stratified split in case of discrete treatment
v0.9.0,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.9.0,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.9.0,"however, sklearn doesn't support both stratifying and grouping (see"
v0.9.0,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.9.0,their own object that supports grouping if they want to use groups.
v0.9.0,######################################################
v0.9.0,These should be removed once `n_splits` is deprecated
v0.9.0,######################################################
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.9.0,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.9.0,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.9.0,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.9.0,checks that X is 2D array.
v0.9.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.0,Handles the corner case when X=None but featurizer might be not None
v0.9.0,"This fails if X=None and featurizer is not None, but that case is handled above"
v0.9.0,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.9.0,"Replacing this method which is invalid for this class, so that we make the"
v0.9.0,dosctring empty and not appear in the docs.
v0.9.0,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.9.0,TODO: support sample_var
v0.9.0,Replacing to remove docstring
v0.9.0,###################################################################
v0.9.0,Everything below should be removed once parameters are deprecated
v0.9.0,###################################################################
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"if both X and W are None, just return a column of ones"
v0.9.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.0,We need to go back to the label representation of the one-hot so as to call
v0.9.0,the classifier.
v0.9.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.0,We need to go back to the label representation of the one-hot so as to call
v0.9.0,the classifier.
v0.9.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.9.0,This works both with our without the weighting trick as the treatments T are unit vector
v0.9.0,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.9.0,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.9.0,both Parametric and Non Parametric DML.
v0.9.0,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.9.0,attribute so that the trained featurizer will be passed through
v0.9.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.0,for internal use by the library
v0.9.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.0,We need to use the rlearner's copy to retain the information from fitting
v0.9.0,Handles the corner case when X=None but featurizer might be not None
v0.9.0,"This fails if X=None and featurizer is not None, but that case is handled above"
v0.9.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.0,since we clone it and fit separate copies
v0.9.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.0,TODO: support sample_var
v0.9.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.0,since we clone it and fit separate copies
v0.9.0,add blb to parent's options
v0.9.0,override only so that we can update the docstring to indicate
v0.9.0,support for `GenericSingleTreatmentModelFinalInference`
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,note that groups are not passed to score because they are only used for fitting
v0.9.0,note that groups are not passed to score because they are only used for fitting
v0.9.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.0,NOTE: important to get parent's wrapped copy so that
v0.9.0,"after training wrapped featurizer is also trained, etc."
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.9.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.0,(which needs to have been expanded if there's a discrete treatment)
v0.9.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.0,since we clone it and fit separate copies
v0.9.0,override only so that we can update the docstring to indicate support for `blb`
v0.9.0,######################################################
v0.9.0,These should be removed once `n_splits` is deprecated
v0.9.0,######################################################
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Remove children with nonwhite mothers from the treatment group
v0.9.0,Remove children with nonwhite mothers from the treatment group
v0.9.0,Select columns
v0.9.0,Scale the numeric variables
v0.9.0,"Change the binary variable 'first' takes values in {1,2}"
v0.9.0,Append a column of ones as intercept
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.0,(which needs to have been expanded if there's a discrete treatment)
v0.9.0,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.9.0,d_t=None here since we measure the effect across all Ts
v0.9.0,once the estimator has been fit
v0.9.0,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.9.0,an intercept even when bias is part of coef.
v0.9.0,We can write effect inference as a function of prediction and prediction standard error of
v0.9.0,the final method for linear models
v0.9.0,d_t=None here since we measure the effect across all Ts
v0.9.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.0,(which needs to have been expanded if there's a discrete treatment)
v0.9.0,"send treatment to the end, pull bounds to the front"
v0.9.0,d_t=None here since we measure the effect across all Ts
v0.9.0,d_t=None here since we measure the effect across all Ts
v0.9.0,d_t=None here since we measure the effect across all Ts
v0.9.0,need to set the fit args before the estimator is fit
v0.9.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.9.0,"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
v0.9.0,to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
v0.9.0,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.9.0,"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
v0.9.0,1. Uncertainty of Mean Point Estimate
v0.9.0,2. Distribution of Point Estimate
v0.9.0,3. Total Variance of Point Estimate
v0.9.0,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.9.0,so bail out early; note that it might be possible to correct the algorithm for
v0.9.0,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.9.0,be clean
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,TODO: Add a __dir__ implementation?
v0.9.0,don't proxy special methods
v0.9.0,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.9.0,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.9.0,then we should be computing a confidence interval for the wrapped calls
v0.9.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.9.0,second level bootstrap which would be prohibitive computationally?
v0.9.0,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.9.0,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.9.0,can't import from econml.inference at top level without creating cyclical dependencies
v0.9.0,Note that inference results are always methods even if the inference is for a property
v0.9.0,(e.g. coef__inference() is a method but coef_ is a property)
v0.9.0,Therefore we must insert a lambda if getting inference for a non-callable
v0.9.0,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.9.0,"try to get interval/std first if appropriate,"
v0.9.0,since we don't prefer a wrapped method with this name
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,
v0.9.0,This code is a fork from:
v0.9.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.9.0,published under the following license and copyright:
v0.9.0,BSD 3-Clause License
v0.9.0,
v0.9.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.0,All rights reserved.
v0.9.0,Set parameters
v0.9.0,Don't instantiate estimators now! Parameters of base_estimator might
v0.9.0,"still change. Eg., when grid-searching with the nested object syntax."
v0.9.0,self.estimators_ needs to be filled by the derived classes in fit.
v0.9.0,Compute the number of jobs
v0.9.0,Partition estimators between jobs
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,
v0.9.0,This code contains snippets of code from:
v0.9.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.9.0,published under the following license and copyright:
v0.9.0,BSD 3-Clause License
v0.9.0,
v0.9.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.0,All rights reserved.
v0.9.0,=============================================================================
v0.9.0,Types and constants
v0.9.0,=============================================================================
v0.9.0,=============================================================================
v0.9.0,Base GRF tree
v0.9.0,=============================================================================
v0.9.0,Determine output settings
v0.9.0,"Important: This must be the first invocation of the random state at fit time, so that"
v0.9.0,train/test splits are re-generatable from an external object simply by knowing the
v0.9.0,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.9.0,linear predictions. Currently is also useful for testing.
v0.9.0,reshape is necessary to preserve the data contiguity against vs
v0.9.0,"[:, np.newaxis] that does not."
v0.9.0,Check parameters
v0.9.0,Set min_weight_leaf from min_weight_fraction_leaf
v0.9.0,Build tree
v0.9.0,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.9.0,hold. Used by criterion for memory space savings.
v0.9.0,Initialize the criterion object and the criterion_val object if honest.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,=============================================================================
v0.9.0,A MultOutputWrapper for GRF classes
v0.9.0,=============================================================================
v0.9.0,=============================================================================
v0.9.0,Instantiations of Generalized Random Forest
v0.9.0,=============================================================================
v0.9.0,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.9.0,in front of the constant treatment is the intercept in the moment equation.
v0.9.0,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.9.0,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,
v0.9.0,This code contains snippets of code from
v0.9.0,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.9.0,published under the following license and copyright:
v0.9.0,BSD 3-Clause License
v0.9.0,
v0.9.0,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.0,All rights reserved.
v0.9.0,=============================================================================
v0.9.0,Base Generalized Random Forest
v0.9.0,=============================================================================
v0.9.0,Remap output
v0.9.0,reshape is necessary to preserve the data contiguity against vs
v0.9.0,"[:, np.newaxis] that does not."
v0.9.0,reshape is necessary to preserve the data contiguity against vs
v0.9.0,"[:, np.newaxis] that does not."
v0.9.0,Get subsample sample size
v0.9.0,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.9.0,We calculate the min eigenvalue proxy that each criterion is considering
v0.9.0,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.9.0,Check parameters
v0.9.0,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.9.0,if this is the first `fit` call of the warm start mode.
v0.9.0,"Free allocated memory, if any"
v0.9.0,the below are needed to replicate randomness of subsampling when warm_start=True
v0.9.0,We draw from the random state to get the random state we
v0.9.0,would have got if we hadn't used a warm_start.
v0.9.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.9.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.9.0,but would still advance randomness enough so that tree subsamples will be different.
v0.9.0,Generating indices a priori before parallelism ended up being orders of magnitude
v0.9.0,faster than how sklearn does it. The reason is that random samplers do not release the
v0.9.0,gil it seems.
v0.9.0,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.9.0,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.9.0,but would still advance randomness enough so that tree subsamples will be different.
v0.9.0,Parallel loop: we prefer the threading backend as the Cython code
v0.9.0,for fitting the trees is internally releasing the Python GIL
v0.9.0,making threading more efficient than multiprocessing in
v0.9.0,"that case. However, for joblib 0.12+ we respect any"
v0.9.0,"parallel_backend contexts set at a higher level,"
v0.9.0,since correctness does not rely on using threads.
v0.9.0,Collect newly grown trees
v0.9.0,Check data
v0.9.0,Assign chunk of trees to jobs
v0.9.0,avoid storing the output of every estimator by summing them here
v0.9.0,Parallel loop
v0.9.0,Check data
v0.9.0,Assign chunk of trees to jobs
v0.9.0,Parallel loop
v0.9.0,Check data
v0.9.0,Assign chunk of trees to jobs
v0.9.0,Parallel loop
v0.9.0,####################
v0.9.0,Variance correction
v0.9.0,####################
v0.9.0,Subtract the average within bag variance. This ends up being equal to the
v0.9.0,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.9.0,The negative part is just sq_between.
v0.9.0,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.9.0,"The off diagonals we have no objective prior, so no correction is applied."
v0.9.0,Finally correcting the pred_cov or pred_var
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,testing importances
v0.9.0,testing heterogeneity importances
v0.9.0,Testing that all parameters do what they are supposed to
v0.9.0,"testing predict, apply and decision path"
v0.9.0,test that the subsampling scheme past to the trees is correct
v0.9.0,test that the estimator calcualtes var correctly
v0.9.0,test api
v0.9.0,test accuracy
v0.9.0,test the projection functionality of forests
v0.9.0,test that the estimator calcualtes var correctly
v0.9.0,test api
v0.9.0,test that the estimator calcualtes var correctly
v0.9.0,"test that the estimator accepts lists, tuples and pandas data frames"
v0.9.0,test that we raise errors in mishandled situations.
v0.9.0,test that the subsampling scheme past to the trees is correct
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.9.0,creating notebooks that are annoying for our users to actually run themselves
v0.9.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.9.0,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.9.0,"prior to calling interpret, can't plot, render, etc."
v0.9.0,can interpret without uncertainty
v0.9.0,can't interpret with uncertainty if inference wasn't used during fit
v0.9.0,can interpret with uncertainty if we refit
v0.9.0,can interpret without uncertainty
v0.9.0,can't treat before interpreting
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,simple DGP only for illustration
v0.9.0,Define the treatment model neural network architecture
v0.9.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.9.0,"so the input shape is (d_z + d_x,)"
v0.9.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.9.0,add extra layers on top for the mixture density network
v0.9.0,Define the response model neural network architecture
v0.9.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.9.0,"so the input shape is (d_t + d_x,)"
v0.9.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.9.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.9.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.9.0,so that the same weights will be reused in each instantiation
v0.9.0,number of samples to use in second estimate of the response
v0.9.0,(to make loss estimate unbiased)
v0.9.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.9.0,do something with predictions...
v0.9.0,also test vector t and y
v0.9.0,simple DGP only for illustration
v0.9.0,Define the treatment model neural network architecture
v0.9.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.9.0,"so the input shape is (d_z + d_x,)"
v0.9.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.9.0,add extra layers on top for the mixture density network
v0.9.0,Define the response model neural network architecture
v0.9.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.9.0,"so the input shape is (d_t + d_x,)"
v0.9.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.9.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.9.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.9.0,so that the same weights will be reused in each instantiation
v0.9.0,number of samples to use in second estimate of the response
v0.9.0,(to make loss estimate unbiased)
v0.9.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.9.0,do something with predictions...
v0.9.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.9.0,test = True ensures we draw test set images
v0.9.0,test = True ensures we draw test set images
v0.9.0,re-draw to get new independent treatment and implied response
v0.9.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.9.0,above is necesary so that reduced form doesn't win
v0.9.0,covariates: time and emotion
v0.9.0,random instrument
v0.9.0,z -> price
v0.9.0,true observable demand function
v0.9.0,errors
v0.9.0,response
v0.9.0,test = True ensures we draw test set images
v0.9.0,test = True ensures we draw test set images
v0.9.0,re-draw to get new independent treatment and implied response
v0.9.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.9.0,above is necesary so that reduced form doesn't win
v0.9.0,covariates: time and emotion
v0.9.0,random instrument
v0.9.0,z -> price
v0.9.0,true observable demand function
v0.9.0,errors
v0.9.0,response
v0.9.0,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.9.0,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.9.0,For some reason this doesn't work at all when run against the CNTK backend...
v0.9.0,"model.compile('nadam', loss=lambda _,l:l)"
v0.9.0,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.9.0,generate a valiation set
v0.9.0,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.9.0,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,DGP constants
v0.9.0,Generate data
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,testing importances
v0.9.0,testing heterogeneity importances
v0.9.0,Testing that all parameters do what they are supposed to
v0.9.0,"testing predict, apply and decision path"
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.9.0,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.9.0,so we need to transpose the result
v0.9.0,1-d output
v0.9.0,2-d output
v0.9.0,Single dimensional output y
v0.9.0,Multi-dimensional output y
v0.9.0,1-d y
v0.9.0,multi-d y
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,test that we can fit with the same arguments as the base estimator
v0.9.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0,with the same shape for the lower and upper bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,test that we can do the same thing once we provide percentile bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0,with the same shape for the lower and upper bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,test that we can do the same thing once we provide percentile bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,test that we can fit with the same arguments as the base estimator
v0.9.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0,with the same shape for the lower and upper bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,test that we can do the same thing once we provide percentile bounds
v0.9.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0,with the same shape for the lower and upper bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,test that we can do the same thing once we provide percentile bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,test that we can fit with the same arguments as the base estimator
v0.9.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0,with the same shape for the lower and upper bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,test that we can do the same thing once we provide percentile bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0,with the same shape for the lower and upper bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,test that we can do the same thing once we provide percentile bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0,with the same shape for the lower and upper bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,test that the estimated effect is usually within the bounds
v0.9.0,test that we can do the same thing once we provide alpha explicitly
v0.9.0,test that the lower and upper bounds differ
v0.9.0,test that the estimated effect is usually within the bounds
v0.9.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0,with the same shape for the lower and upper bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,TODO: test that the estimated effect is usually within the bounds
v0.9.0,and that the true effect is also usually within the bounds
v0.9.0,test that we can do the same thing once we provide percentile bounds
v0.9.0,test that the lower and upper bounds differ
v0.9.0,TODO: test that the estimated effect is usually within the bounds
v0.9.0,and that the true effect is also usually within the bounds
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,DGP constants
v0.9.0,Generate data
v0.9.0,Test inference results when `cate_feature_names` doesn not exist
v0.9.0,Test inference results when `cate_feature_names` doesn not exist
v0.9.0,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.9.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.0,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.9.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.9.0,pvalue for second column should be greater than zero since some points are on either side
v0.9.0,of the tested value
v0.9.0,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.9.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.9.0,Test non keyword based calls to fit
v0.9.0,test non-array inputs
v0.9.0,Test custom splitter
v0.9.0,Test incomplete set of test folds
v0.9.0,"y scores should be positive, since W predicts Y somewhat"
v0.9.0,"t scores might not be, since W and T are uncorrelated"
v0.9.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,make sure cross product varies more slowly with first array
v0.9.0,and that vectors are okay as inputs
v0.9.0,number of inputs in specification must match number of inputs
v0.9.0,must have an output
v0.9.0,output indices must be unique
v0.9.0,output indices must be present in an input
v0.9.0,number of indices must match number of dimensions for each input
v0.9.0,repeated indices must always have consistent sizes
v0.9.0,transpose
v0.9.0,tensordot
v0.9.0,trace
v0.9.0,TODO: set up proper flag for this
v0.9.0,pick indices at random with replacement from the first 7 letters of the alphabet
v0.9.0,"of all of the distinct indices that appear in any input,"
v0.9.0,pick a random subset of them (of size at most 5) to appear in the output
v0.9.0,creating an instance should warn
v0.9.0,using the instance should not warn
v0.9.0,using the deprecated method should warn
v0.9.0,don't warn if b and c are passed by keyword
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Preprocess data
v0.9.0,Convert 'week' to a date
v0.9.0,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.9.0,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.9.0,Take log of price
v0.9.0,Make brand numeric
v0.9.0,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.9.0,which have all zero coeffs
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,test at least one estimator from each category
v0.9.0,test causal graph
v0.9.0,test refutation estimate
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.9.0,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.9.0,TODO: test something rather than just print...
v0.9.0,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.9.0,pick some arbitrary X
v0.9.0,pick some arbitrary T
v0.9.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.9.0,The average variance should be lower when using monte carlo iterations
v0.9.0,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.9.0,The average variance should be lower when using monte carlo iterations
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,ensure that we've got at least two of every row
v0.9.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.0,need to make sure we get all *joint* combinations
v0.9.0,IntentToTreat only supports binary treatments/instruments
v0.9.0,IntentToTreat only supports binary treatments/instruments
v0.9.0,IntentToTreat requires X
v0.9.0,ensure we can serialize unfit estimator
v0.9.0,these support only W but not X
v0.9.0,"these support only binary, not general discrete T and Z"
v0.9.0,ensure we can serialize fit estimator
v0.9.0,make sure we can call the marginal_effect and effect methods
v0.9.0,TODO: add tests for extra properties like coef_ where they exist
v0.9.0,TODO: add tests for extra properties like coef_ where they exist
v0.9.0,"make sure we can call effect with implied scalar treatments,"
v0.9.0,"no matter the dimensions of T, and also that we warn when there"
v0.9.0,are multiple treatments
v0.9.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.0,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.9.0,"however, with custom splits the checking happens in the first stage wrapper"
v0.9.0,where we don't have all of the required information to do this;
v0.9.0,we'd probably need to add it to _crossfit instead
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.9.0,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.9.0,The __warningregistry__'s need to be in a pristine state for tests
v0.9.0,to work properly.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Set random seed
v0.9.0,Generate data
v0.9.0,DGP constants
v0.9.0,Test data
v0.9.0,Constant treatment effect
v0.9.0,Constant treatment with multi output Y
v0.9.0,Heterogeneous treatment
v0.9.0,Heterogeneous treatment with multi output Y
v0.9.0,TLearner test
v0.9.0,Instantiate TLearner
v0.9.0,Test inputs
v0.9.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.0,Instantiate SLearner
v0.9.0,Test inputs
v0.9.0,Test constant treatment effect
v0.9.0,Test constant treatment effect with multi output Y
v0.9.0,Test heterogeneous treatment effect
v0.9.0,Need interactions between T and features
v0.9.0,Test heterogeneous treatment effect with multi output Y
v0.9.0,Instantiate XLearner
v0.9.0,Test inputs
v0.9.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.0,Instantiate DomainAdaptationLearner
v0.9.0,Test inputs
v0.9.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.0,Get the true treatment effect
v0.9.0,Get the true treatment effect
v0.9.0,Fit learner and get the effect and marginal effect
v0.9.0,Compute treatment effect residuals (absolute)
v0.9.0,Check that at least 90% of predictions are within tolerance interval
v0.9.0,Check whether the output shape is right
v0.9.0,Check that one can pass in regular lists
v0.9.0,Check that it fails correctly if lists of different shape are passed in
v0.9.0,"Check that it works when T, Y have shape (n, 1)"
v0.9.0,Generate covariates
v0.9.0,Generate treatment
v0.9.0,Calculate outcome
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,DGP constants
v0.9.0,Generate data
v0.9.0,Test data
v0.9.0,Remove warnings that might be raised by the models passed into the ORF
v0.9.0,Generate data with continuous treatments
v0.9.0,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.9.0,does not work well with parallelism.
v0.9.0,Test inputs for continuous treatments
v0.9.0,--> Check that one can pass in regular lists
v0.9.0,--> Check that it fails correctly if lists of different shape are passed in
v0.9.0,Check that outputs have the correct shape
v0.9.0,Test continuous treatments with controls
v0.9.0,Test continuous treatments without controls
v0.9.0,Generate data with binary treatments
v0.9.0,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.9.0,does not work well with parallelism.
v0.9.0,Test inputs for binary treatments
v0.9.0,--> Check that one can pass in regular lists
v0.9.0,--> Check that it fails correctly if lists of different shape are passed in
v0.9.0,"--> Check that it works when T, Y have shape (n, 1)"
v0.9.0,"--> Check that it fails correctly when T has shape (n, 2)"
v0.9.0,--> Check that it fails correctly when the treatments are not numeric
v0.9.0,Check that outputs have the correct shape
v0.9.0,Test binary treatments with controls
v0.9.0,Test binary treatments without controls
v0.9.0,Only applicable to continuous treatments
v0.9.0,Generate data for 2 treatments
v0.9.0,Test multiple treatments with controls
v0.9.0,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.9.0,The rest for controls. Just as an example.
v0.9.0,Generating A/B test data
v0.9.0,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.9.0,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.9.0,Create a wrapper around Lasso that doesn't support weights
v0.9.0,since Lasso does natively support them starting in sklearn 0.23
v0.9.0,Generate data with continuous treatments
v0.9.0,Instantiate model with most of the default parameters
v0.9.0,Compute the treatment effect on test points
v0.9.0,Compute treatment effect residuals
v0.9.0,Multiple treatments
v0.9.0,Allow at most 10% test points to be outside of the tolerance interval
v0.9.0,Compute treatment effect residuals
v0.9.0,Multiple treatments
v0.9.0,Allow at most 20% test points to be outside of the confidence interval
v0.9.0,Check that the intervals are not too wide
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.9.0,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.9.0,ensure that we've got at least 6 of every element
v0.9.0,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.9.0,NOTE: this number may need to change if the default number of folds in
v0.9.0,WeightedStratifiedKFold changes
v0.9.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.0,ensure we can serialize the unfit estimator
v0.9.0,ensure we can pickle the fit estimator
v0.9.0,make sure we can call the marginal_effect and effect methods
v0.9.0,test const marginal inference
v0.9.0,test effect inference
v0.9.0,test marginal effect inference
v0.9.0,test coef__inference and intercept__inference
v0.9.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.0,"make sure we can call effect with implied scalar treatments,"
v0.9.0,"no matter the dimensions of T, and also that we warn when there"
v0.9.0,are multiple treatments
v0.9.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.0,ensure that we've got at least two of every element
v0.9.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.0,make sure we can call the marginal_effect and effect methods
v0.9.0,test const marginal inference
v0.9.0,test effect inference
v0.9.0,test marginal effect inference
v0.9.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.9.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.9.0,We concatenate the two copies data
v0.9.0,create a simple artificial setup where effect of moving from treatment
v0.9.0,"1 -> 2 is 2,"
v0.9.0,"1 -> 3 is 1, and"
v0.9.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.9.0,"Using an uneven number of examples from different classes,"
v0.9.0,"and having the treatments in non-lexicographic order,"
v0.9.0,Should rule out some basic issues.
v0.9.0,test that we can fit with a KFold instance
v0.9.0,test that we can fit with a train/test iterable
v0.9.0,predetermined splits ensure that all features are seen in each split
v0.9.0,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.9.0,(incorrectly) use a final model with an intercept
v0.9.0,"Because final model is fixed, actual values of T and Y don't matter"
v0.9.0,Ensure reproducibility
v0.9.0,Sparse DGP
v0.9.0,Treatment effect coef
v0.9.0,Other coefs
v0.9.0,Features and controls
v0.9.0,Test sparse estimator
v0.9.0,"--> test coef_, intercept_"
v0.9.0,--> test treatment effects
v0.9.0,Restrict x_test to vectors of norm < 1
v0.9.0,--> check inference
v0.9.0,Check that a majority of true effects lie in the 5-95% CI
v0.9.0,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.9.0,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.9.0,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.9.0,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.9.0,sparse test case: heterogeneous effect by product
v0.9.0,need at least as many rows in e_y as there are distinct columns
v0.9.0,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.9.0,create a simple artificial setup where effect of moving from treatment
v0.9.0,"a -> b is 2,"
v0.9.0,"a -> c is 1, and"
v0.9.0,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.9.0,"Using an uneven number of examples from different classes,"
v0.9.0,"and having the treatments in non-lexicographic order,"
v0.9.0,should rule out some basic issues.
v0.9.0,Note that explicitly specifying the dtype as object is necessary until
v0.9.0,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.9.0,estimated effects should be identical when treatment is explicitly given
v0.9.0,but const_marginal_effect should be reordered based on the explicit cagetories
v0.9.0,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.9.0,test outer grouping
v0.9.0,test nested grouping
v0.9.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.9.0,whichever groups we saw
v0.9.0,test nested grouping
v0.9.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.9.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.9.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Set random seed
v0.9.0,Generate data
v0.9.0,DGP constants
v0.9.0,Test data
v0.9.0,Constant treatment effect and propensity
v0.9.0,Heterogeneous treatment and propensity
v0.9.0,ensure that we've got at least two of every element
v0.9.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.0,ensure that we can serialize unfit estimator
v0.9.0,ensure that we can serialize fit estimator
v0.9.0,make sure we can call the marginal_effect and effect methods
v0.9.0,test const marginal inference
v0.9.0,test effect inference
v0.9.0,test marginal effect inference
v0.9.0,test coef_ and intercept_ inference
v0.9.0,verify we can generate the summary
v0.9.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.9.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.9.0,create a simple artificial setup where effect of moving from treatment
v0.9.0,"1 -> 2 is 2,"
v0.9.0,"1 -> 3 is 1, and"
v0.9.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.9.0,"Using an uneven number of examples from different classes,"
v0.9.0,"and having the treatments in non-lexicographic order,"
v0.9.0,Should rule out some basic issues.
v0.9.0,test that we can fit with a KFold instance
v0.9.0,test that we can fit with a train/test iterable
v0.9.0,"for at least some of the examples, the CI should have nonzero width"
v0.9.0,"for at least some of the examples, the CI should have nonzero width"
v0.9.0,"for at least some of the examples, the CI should have nonzero width"
v0.9.0,test coef__inference function works
v0.9.0,test intercept__inference function works
v0.9.0,test summary function works
v0.9.0,Test inputs
v0.9.0,self._test_inputs(DR_learner)
v0.9.0,Test constant treatment effect
v0.9.0,Test heterogeneous treatment effect
v0.9.0,Test heterogenous treatment effect for W =/= None
v0.9.0,Sparse DGP
v0.9.0,Treatment effect coef
v0.9.0,Other coefs
v0.9.0,Features and controls
v0.9.0,Test sparse estimator
v0.9.0,"--> test coef_, intercept_"
v0.9.0,--> test treatment effects
v0.9.0,Restrict x_test to vectors of norm < 1
v0.9.0,--> check inference
v0.9.0,Check that a majority of true effects lie in the 5-95% CI
v0.9.0,test outer grouping
v0.9.0,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.9.0,test nested grouping
v0.9.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.9.0,whichever groups we saw
v0.9.0,test nested grouping
v0.9.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.9.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.9.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.9.0,Fit learner and get the effect
v0.9.0,Get the true treatment effect
v0.9.0,Compute treatment effect residuals (absolute)
v0.9.0,Check that at least 90% of predictions are within tolerance interval
v0.9.0,Only for heterogeneous TE
v0.9.0,Fit learner on X and W and get the effect
v0.9.0,Get the true treatment effect
v0.9.0,Compute treatment effect residuals (absolute)
v0.9.0,Check that at least 90% of predictions are within tolerance interval
v0.9.0,Check that one can pass in regular lists
v0.9.0,Check that it fails correctly if lists of different shape are passed in
v0.9.0,Check that it fails when T contains values other than 0 and 1
v0.9.0,"Check that it works when T, Y have shape (n, 1)"
v0.9.0,Generate covariates
v0.9.0,Generate treatment
v0.9.0,Calculate outcome
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,DGP constants
v0.9.0,DGP coefficients
v0.9.0,Generated outcomes
v0.9.0,################
v0.9.0,WeightedLasso #
v0.9.0,################
v0.9.0,Define weights
v0.9.0,Define extended datasets
v0.9.0,Range of alphas
v0.9.0,Compare with Lasso
v0.9.0,--> No intercept
v0.9.0,--> With intercept
v0.9.0,When DGP has no intercept
v0.9.0,When DGP has intercept
v0.9.0,--> Coerce coefficients to be positive
v0.9.0,--> Toggle max_iter & tol
v0.9.0,Define weights
v0.9.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.9.0,Mixed DGP scenario.
v0.9.0,Define extended datasets
v0.9.0,Define weights
v0.9.0,Define multioutput
v0.9.0,##################
v0.9.0,WeightedLassoCV #
v0.9.0,##################
v0.9.0,Define alphas to test
v0.9.0,Compare with LassoCV
v0.9.0,--> No intercept
v0.9.0,--> With intercept
v0.9.0,--> Force parameters to be positive
v0.9.0,Choose a smaller n to speed-up process
v0.9.0,Compare fold weights
v0.9.0,Define weights
v0.9.0,Define extended datasets
v0.9.0,Define splitters
v0.9.0,WeightedKFold splitter
v0.9.0,Map weighted splitter to an extended splitter
v0.9.0,Define alphas to test
v0.9.0,Compare with LassoCV
v0.9.0,--> No intercept
v0.9.0,--> With intercept
v0.9.0,--> Force parameters to be positive
v0.9.0,###########################
v0.9.0,MultiTaskWeightedLassoCV #
v0.9.0,###########################
v0.9.0,Define alphas to test
v0.9.0,Define splitter
v0.9.0,Compare with MultiTaskLassoCV
v0.9.0,--> No intercept
v0.9.0,--> With intercept
v0.9.0,Define weights
v0.9.0,Define extended datasets
v0.9.0,Define splitters
v0.9.0,WeightedKFold splitter
v0.9.0,Map weighted splitter to an extended splitter
v0.9.0,Define alphas to test
v0.9.0,Compare with LassoCV
v0.9.0,--> No intercept
v0.9.0,--> With intercept
v0.9.0,#########################
v0.9.0,WeightedLassoCVWrapper #
v0.9.0,#########################
v0.9.0,perform 1D fit
v0.9.0,perform 2D fit
v0.9.0,################
v0.9.0,DebiasedLasso #
v0.9.0,################
v0.9.0,Test DebiasedLasso without weights
v0.9.0,--> Check debiased coeffcients without intercept
v0.9.0,--> Check debiased coeffcients with intercept
v0.9.0,--> Check 5-95 CI coverage for unit vectors
v0.9.0,Test DebiasedLasso with weights for one DGP
v0.9.0,Define weights
v0.9.0,Define extended datasets
v0.9.0,--> Check debiased coefficients
v0.9.0,Define weights
v0.9.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.9.0,--> Check debiased coeffcients
v0.9.0,Test that attributes propagate correctly
v0.9.0,Test MultiOutputDebiasedLasso without weights
v0.9.0,--> Check debiased coeffcients without intercept
v0.9.0,--> Check debiased coeffcients with intercept
v0.9.0,--> Check CI coverage
v0.9.0,Test MultiOutputDebiasedLasso with weights
v0.9.0,Define weights
v0.9.0,Define extended datasets
v0.9.0,--> Check debiased coefficients
v0.9.0,Unit vectors
v0.9.0,Unit vectors
v0.9.0,Check coeffcients and intercept are the same within tolerance
v0.9.0,Check results are similar with tolerance 1e-6
v0.9.0,Check if multitask
v0.9.0,Check that same alpha is chosen
v0.9.0,Check that the coefficients are similar
v0.9.0,selective ridge has a simple implementation that we can test against
v0.9.0,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.9.0,"it should be the case that when we set fit_intercept to true,"
v0.9.0,it doesn't matter whether the penalized model also fits an intercept or not
v0.9.0,create an extra copy of rows with weight 2
v0.9.0,"instead of a slice, explicitly return an array of indices"
v0.9.0,_penalized_inds is only set during fitting
v0.9.0,cv exists on penalized model
v0.9.0,now we can access _penalized_inds
v0.9.0,check that we can read the cv attribute back out from the underlying model
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,DGP constants
v0.9.0,Define data features
v0.9.0,Added `_df`to names to be different from the default cate_estimator names
v0.9.0,Generate data
v0.9.0,################################
v0.9.0,Single treatment and outcome #
v0.9.0,################################
v0.9.0,Test LinearDML
v0.9.0,Test re-fit
v0.9.0,Test SparseLinearDML
v0.9.0,ForestDML
v0.9.0,###################################
v0.9.0,Mutiple treatments and outcomes #
v0.9.0,###################################
v0.9.0,Test LinearDML
v0.9.0,Test SparseLinearDML
v0.9.0,"Single outcome only, ORF does not support multiple outcomes"
v0.9.0,Test DMLOrthoForest
v0.9.0,Test DROrthoForest
v0.9.0,Test XLearner
v0.9.0,Skipping population summary names test because bootstrap inference is too slow
v0.9.0,Test SLearner
v0.9.0,Test TLearner
v0.9.0,Test LinearDRLearner
v0.9.0,Test SparseLinearDRLearner
v0.9.0,Test ForestDRLearner
v0.9.0,Test LinearIntentToTreatDRIV
v0.9.0,Test DeepIV
v0.9.0,Test categorical treatments
v0.9.0,Check refit
v0.9.0,Check refit after setting categories
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Linear models are required for parametric dml
v0.9.0,sample weighting models are required for nonparametric dml
v0.9.0,Test values
v0.9.0,TLearner test
v0.9.0,Instantiate TLearner
v0.9.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.0,Test constant treatment effect with multi output Y
v0.9.0,Test heterogeneous treatment effect
v0.9.0,Need interactions between T and features
v0.9.0,Test heterogeneous treatment effect with multi output Y
v0.9.0,Instantiate DomainAdaptationLearner
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,test base values equals to mean of constant marginal effect
v0.9.0,test shape of shap values output is as expected
v0.9.0,test shape of attribute of explanation object is as expected
v0.9.0,test base values equals to mean of constant marginal effect
v0.9.0,test shape of shap values output is as expected
v0.9.0,test shape of attribute of explanation object is as expected
v0.9.0,Treatment effect function
v0.9.0,Outcome support
v0.9.0,Treatment support
v0.9.0,"Generate controls, covariates, treatments and outcomes"
v0.9.0,Heterogeneous treatment effects
v0.9.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.9.0,through shap package.
v0.9.0,test shap could generate the plot from the shap_values
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Check inputs
v0.9.0,Check inputs
v0.9.0,Check inputs
v0.9.0,"Note: unlike other Metalearners, we need the controls' encoded column for training"
v0.9.0,"Thus, we append the controls column before the one-hot-encoded T"
v0.9.0,"We might want to revisit, though, since it's linearly determined by the others"
v0.9.0,Check inputs
v0.9.0,Check inputs
v0.9.0,Estimate response function
v0.9.0,Check inputs
v0.9.0,Train model on controls. Assign higher weight to units resembling
v0.9.0,treated units.
v0.9.0,Train model on the treated. Assign higher weight to units resembling
v0.9.0,control units.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.9.0,output is
v0.9.0,"* a column of ones if X, W, and Z are all None"
v0.9.0,* just X or W or Z if both of the others are None
v0.9.0,* hstack([arrs]) for whatever subset are not None otherwise
v0.9.0,ensure Z is 2D
v0.9.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.0,We need to go back to the label representation of the one-hot so as to call
v0.9.0,the classifier.
v0.9.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.0,We need to go back to the label representation of the one-hot so as to call
v0.9.0,the classifier.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,TODO: make sure to use random seeds wherever necessary
v0.9.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.9.0,"unfortunately with the Theano and Tensorflow backends,"
v0.9.0,the straightforward use of K.stop_gradient can cause an error
v0.9.0,because the parameters of the intermediate layers are now disconnected from the loss;
v0.9.0,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.9.0,so that those layers remain connected but with 0 gradient
v0.9.0,|| t - mu_i || ^2
v0.9.0,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.9.0,Use logsumexp for numeric stability:
v0.9.0,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.9.0,TODO: does the numeric stability actually make any difference?
v0.9.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.9.0,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.9.0,generate cumulative sum via matrix multiplication
v0.9.0,"Generate standard uniform values in shape (batch_size,1)"
v0.9.0,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.9.0,we use uniform_like instead with an input of an appropriate shape)
v0.9.0,convert to floats and multiply to perform equivalent of logical AND
v0.9.0,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.9.0,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.9.0,we use normal_like instead with an input of an appropriate shape)
v0.9.0,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.9.0,prevent gradient from passing through sampling
v0.9.0,three options: biased or upper-bound loss require a single number of samples;
v0.9.0,unbiased can take different numbers for the network and its gradient
v0.9.0,"sample: (() -> Layer, int) -> Layer"
v0.9.0,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.9.0,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.9.0,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.9.0,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.9.0,the dimensionality of the output of the network
v0.9.0,TODO: is there a more robust way to do this?
v0.9.0,TODO: do we need to give the user more control over other arguments to fit?
v0.9.0,"subtle point: we need to build a new model each time,"
v0.9.0,because each model encapsulates its randomness
v0.9.0,TODO: do we need to give the user more control over other arguments to fit?
v0.9.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.9.0,not a general tensor (because of how backprop works in every framework)
v0.9.0,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.9.0,but this seems annoying...)
v0.9.0,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.9.0,TODO: any way to get this to work on batches of arbitrary size?
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Estimate final model of theta(X) by minimizing the square loss:
v0.9.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.9.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.9.0,at the expense of some small bias. For points with very small covariance we revert
v0.9.0,to the model-based preliminary estimate and do not add the correction term.
v0.9.0,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.9.0,"instruments, and outcomes"
v0.9.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.9.0,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.9.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.0,for internal use by the library
v0.9.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.0,"we need to undo the one-hot encoding for calling effect,"
v0.9.0,since it expects raw values
v0.9.0,"we need to undo the one-hot encoding for calling effect,"
v0.9.0,since it expects raw values
v0.9.0,"TODO: check that Y, T, Z do not have multiple columns"
v0.9.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.9.0,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.9.0,since we would actually be calculating a CATE rather than ATE.
v0.9.0,TODO: allow the final model to actually use X?
v0.9.0,TODO: allow the final model to actually use X?
v0.9.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.0,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.9.0,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.9.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.0,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.0,for internal use by the library
v0.9.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,"this will have dimension (d,) + shape(X)"
v0.9.0,send the first dimension to the end
v0.9.0,columns are featurized independently; partial derivatives are only non-zero
v0.9.0,when taken with respect to the same column each time
v0.9.0,don't fit intercept; manually add column of ones to the data instead;
v0.9.0,this allows us to ignore the intercept when computing marginal effects
v0.9.0,make T 2D if if was a vector
v0.9.0,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.9.0,two stage approximation
v0.9.0,"first, get basis expansions of T, X, and Z"
v0.9.0,TODO: is it right that the effective number of intruments is the
v0.9.0,"product of ft_X and ft_Z, not just ft_Z?"
v0.9.0,"regress T expansion on X,Z expansions concatenated with W"
v0.9.0,"predict ft_T from interacted ft_X, ft_Z"
v0.9.0,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.9.0,dT may be only 2-dimensional)
v0.9.0,promote dT to 3D if necessary (e.g. if T was a vector)
v0.9.0,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,TODO: conisder working around relying on sklearn implementation details
v0.9.0,"Found a good split, return."
v0.9.0,Record all splits in case the stratification by weight yeilds a worse partition
v0.9.0,Reseed random generator and try again
v0.9.0,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.9.0,"Found a good split, return."
v0.9.0,Did not find a good split
v0.9.0,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.9.0,Return most weight-balanced partition
v0.9.0,Weight stratification algorithm
v0.9.0,Sort weights for weight strata search
v0.9.0,There are some leftover indices that have yet to be assigned
v0.9.0,Append stratum splits to overall splits
v0.9.0,"If classification methods produce multiple columns of output,"
v0.9.0,we need to manually encode classes to ensure consistent column ordering.
v0.9.0,We clone the estimator to make sure that all the folds are
v0.9.0,"independent, and that it is pickle-able."
v0.9.0,"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
v0.9.0,`predictions` is a list of method outputs from each fold.
v0.9.0,"If each of those is also a list, then treat this as a"
v0.9.0,multioutput-multiclass task. We need to separately concatenate
v0.9.0,the method outputs for each label into an `n_labels` long list.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Our classes that derive from sklearn ones sometimes include
v0.9.0,inherited docstrings that have embedded doctests; we need the following imports
v0.9.0,so that they don't break.
v0.9.0,TODO: consider working around relying on sklearn implementation details
v0.9.0,"Convert X, y into numpy arrays"
v0.9.0,Define fit parameters
v0.9.0,Some algorithms don't have a check_input option
v0.9.0,Check weights array
v0.9.0,Check that weights are size-compatible
v0.9.0,Normalize inputs
v0.9.0,Weight inputs
v0.9.0,Fit base class without intercept
v0.9.0,Fit Lasso
v0.9.0,Reset intercept
v0.9.0,The intercept is not calculated properly due the sqrt(weights) factor
v0.9.0,so it must be recomputed
v0.9.0,Fit lasso without weights
v0.9.0,Make weighted splitter
v0.9.0,Fit weighted model
v0.9.0,Make weighted splitter
v0.9.0,Fit weighted model
v0.9.0,Call weighted lasso on reduced design matrix
v0.9.0,Weighted tau
v0.9.0,Select optimal penalty
v0.9.0,Warn about consistency
v0.9.0,"Convert X, y into numpy arrays"
v0.9.0,Fit weighted lasso with user input
v0.9.0,"Center X, y"
v0.9.0,Calculate quantities that will be used later on. Account for centered data
v0.9.0,Calculate coefficient and error variance
v0.9.0,Add coefficient correction
v0.9.0,Set coefficients and intercept standard errors
v0.9.0,Set intercept
v0.9.0,Return alpha to 'auto' state
v0.9.0,"Note that in the case of no intercept, X_offset is 0"
v0.9.0,Calculate the variance of the predictions
v0.9.0,Calculate prediction confidence intervals
v0.9.0,Assumes flattened y
v0.9.0,Compute weighted residuals
v0.9.0,To be done once per target. Assumes y can be flattened.
v0.9.0,Assumes that X has already been offset
v0.9.0,Special case: n_features=1
v0.9.0,Compute Lasso coefficients for the columns of the design matrix
v0.9.0,Compute C_hat
v0.9.0,Compute theta_hat
v0.9.0,Allow for single output as well
v0.9.0,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.9.0,Set coef_ attribute
v0.9.0,Set intercept_ attribute
v0.9.0,Set selected_alpha_ attribute
v0.9.0,Set coef_stderr_
v0.9.0,intercept_stderr_
v0.9.0,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.9.0,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.9.0,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.9.0,set intercept_ attribute
v0.9.0,set coef_ attribute
v0.9.0,set alpha_ attribute
v0.9.0,set alphas_ attribute
v0.9.0,set n_iter_ attribute
v0.9.0,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.9.0,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.9.0,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.9.0,now regress X1 on y - X2 * beta2 to learn beta1
v0.9.0,set coef_ and intercept_ attributes
v0.9.0,Note that the penalized model should *not* have an intercept
v0.9.0,don't proxy special methods
v0.9.0,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.9.0,regressor incorrectly
v0.9.0,"Note: for known attributes that have been set this method will not be called,"
v0.9.0,so we should just throw here because this is an attribute belonging to this class
v0.9.0,but which hasn't yet been set on this instance
v0.9.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,AzureML
v0.9.0,helper imports
v0.9.0,write the details of the workspace to a configuration file to the notebook library
v0.9.0,if y is a multioutput model
v0.9.0,Make sure second dimension has 1 or more item
v0.9.0,switch _inner Model to a MultiOutputRegressor
v0.9.0,flatten array as automl only takes vectors for y
v0.9.0,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.9.0,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.9.0,as an sklearn estimator
v0.9.0,fit implementation for a single output model.
v0.9.0,Create experiment for specified workspace
v0.9.0,Configure automl_config with training set information.
v0.9.0,"Wait for remote run to complete, the set the model"
v0.9.0,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.9.0,create model and pass model into final.
v0.9.0,"If item is an automl config, get its corresponding"
v0.9.0,AutomatedML Model and add it to new_Args
v0.9.0,"If item is an automl config, get its corresponding"
v0.9.0,AutomatedML Model and set it for this key in
v0.9.0,kwargs
v0.9.0,takes in either automated_ml config and instantiates
v0.9.0,an AutomatedMLModel
v0.9.0,The prefix can only be 18 characters long
v0.9.0,"because prefixes come from kwarg_names, we must ensure they are"
v0.9.0,short enough.
v0.9.0,Get workspace from config file.
v0.9.0,Take the intersect of the white for sample
v0.9.0,weights and linear models
v0.9.0,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,TODO: generalize to multiple treatment case?
v0.9.0,get index of best treatment
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,make any access to matplotlib or plt throw an exception
v0.9.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.9.0,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.9.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.9.0,clean way of achieving this
v0.9.0,make sure we don't accidentally escape anything in the substitution
v0.9.0,Fetch appropriate color for node
v0.9.0,"red for negative, green for positive"
v0.9.0,in multi-target use first target
v0.9.0,Write node mean CATE
v0.9.0,Write node std of CATE
v0.9.0,Fetch appropriate color for node
v0.9.0,"red for negative, green for positive"
v0.9.0,Write node mean CATE
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,TODO: consider working around relying on sklearn implementation details
v0.9.0,Create splits of causal tree
v0.9.0,Make sure the correct exception is being rethrown
v0.9.0,Must make sure indices are merged correctly
v0.9.0,Convert rows to columns
v0.9.0,Require group assignment t to be one-hot-encoded
v0.9.0,Get predictions for the 2 splits
v0.9.0,Must make sure indices are merged correctly
v0.9.0,Crossfitting
v0.9.0,Compute weighted nuisance estimates
v0.9.0,-------------------------------------------------------------------------------
v0.9.0,Calculate the covariance matrix corresponding to the BLB inference
v0.9.0,
v0.9.0,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.9.0,2. Calculate the weighted moments for each tree slice to create a matrix
v0.9.0,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.9.0,in that slice from the overall parameter estimate.
v0.9.0,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.9.0,-------------------------------------------------------------------------------
v0.9.0,Calclulate covariance matrix through BLB
v0.9.0,Estimators
v0.9.0,OrthoForest parameters
v0.9.0,Sub-forests
v0.9.0,Auxiliary attributes
v0.9.0,Fit check
v0.9.0,TODO: Check performance
v0.9.0,Must normalize weights
v0.9.0,Override the CATE inference options
v0.9.0,Add blb inference to parent's options
v0.9.0,Generate subsample indices
v0.9.0,Build trees in parallel
v0.9.0,Bootstraping has repetitions in tree sample
v0.9.0,Similar for `a` weights
v0.9.0,Bootstraping has repetitions in tree sample
v0.9.0,Define subsample size
v0.9.0,Safety check
v0.9.0,Draw points to create little bags
v0.9.0,Copy and/or define models
v0.9.0,Define nuisance estimators
v0.9.0,Define parameter estimators
v0.9.0,Define
v0.9.0,Need to redefine fit here for auto inference to work due to a quirk in how
v0.9.0,wrap_fit is defined
v0.9.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.9.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.9.0,Override to flatten output if T is flat
v0.9.0,Check that all discrete treatments are represented
v0.9.0,Nuissance estimates evaluated with cross-fitting
v0.9.0,Define 2-fold iterator
v0.9.0,Check if there is only one example of some class
v0.9.0,Define 2-fold iterator
v0.9.0,need safe=False when cloning for WeightedModelWrapper
v0.9.0,Compute residuals
v0.9.0,Compute coefficient by OLS on residuals
v0.9.0,"Parameter returned by LinearRegression is (d_T, )"
v0.9.0,Compute residuals
v0.9.0,Compute coefficient by OLS on residuals
v0.9.0,ell_2 regularization
v0.9.0,Ridge regression estimate
v0.9.0,"Parameter returned is of shape (d_T, )"
v0.9.0,Return moments and gradients
v0.9.0,Compute residuals
v0.9.0,Compute moments
v0.9.0,"Moments shape is (n, d_T)"
v0.9.0,Compute moment gradients
v0.9.0,returns shape-conforming residuals
v0.9.0,Copy and/or define models
v0.9.0,Define parameter estimators
v0.9.0,Define moment and mean gradient estimator
v0.9.0,"Check that T is shape (n, )"
v0.9.0,Check T is numeric
v0.9.0,Train label encoder
v0.9.0,Call `fit` from parent class
v0.9.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.9.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.9.0,Override to flatten output if T is flat
v0.9.0,Expand one-hot encoding to include the zero treatment
v0.9.0,"Test that T contains all treatments. If not, return None"
v0.9.0,Nuissance estimates evaluated with cross-fitting
v0.9.0,Define 2-fold iterator
v0.9.0,Check if there is only one example of some class
v0.9.0,No need to crossfit for internal nodes
v0.9.0,Compute partial moments
v0.9.0,"If any of the values in the parameter estimate is nan, return None"
v0.9.0,Compute partial moments
v0.9.0,Compute coefficient by OLS on residuals
v0.9.0,ell_2 regularization
v0.9.0,Ridge regression estimate
v0.9.0,"Parameter returned is of shape (d_T, )"
v0.9.0,Return moments and gradients
v0.9.0,Compute partial moments
v0.9.0,Compute moments
v0.9.0,"Moments shape is (n, d_T-1)"
v0.9.0,Compute moment gradients
v0.9.0,Need to calculate this in an elegant way for when propensity is 0
v0.9.0,This will flatten T
v0.9.0,Check that T is numeric
v0.9.0,Test whether the input estimator is supported
v0.9.0,Calculate confidence intervals for the parameter (marginal effect)
v0.9.0,Calculate confidence intervals for the effect
v0.9.0,Calculate the effects
v0.9.0,Calculate the standard deviations for the effects
v0.9.0,d_t=None here since we measure the effect across all Ts
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0,Licensed under the MIT License.
v0.9.0,Causal tree parameters
v0.9.0,Tree structure
v0.9.0,No need for a random split since the data is already
v0.9.0,a random subsample from the original input
v0.9.0,node list stores the nodes that are yet to be splitted
v0.9.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.9.0,Create local sample set
v0.9.0,Compute nuisance estimates for the current node
v0.9.0,Nuisance estimate cannot be calculated
v0.9.0,Estimate parameter for current node
v0.9.0,Node estimate cannot be calculated
v0.9.0,Calculate moments and gradient of moments for current data
v0.9.0,Calculate inverse gradient
v0.9.0,The gradient matrix is not invertible.
v0.9.0,No good split can be found
v0.9.0,Calculate point-wise pseudo-outcomes rho
v0.9.0,a split is determined by a feature and a sample pair
v0.9.0,the number of possible splits is at most (number of features) * (number of node samples)
v0.9.0,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.9.0,parse row and column of random pair
v0.9.0,the sample of the pair is the integer division of the random number with n_feats
v0.9.0,calculate the binary indicator of whether sample i is on the left or the right
v0.9.0,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.9.0,calculate the number of samples on the left child for each proposed split
v0.9.0,calculate the analogous binary indicator for the samples in the estimation set
v0.9.0,calculate the number of estimation samples on the left child of each proposed split
v0.9.0,find the upper and lower bound on the size of the left split for the split
v0.9.0,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.9.0,on each side.
v0.9.0,similarly for the estimation sample set
v0.9.0,if there is no valid split then don't create any children
v0.9.0,filter only the valid splits
v0.9.0,calculate the average influence vector of the samples in the left child
v0.9.0,calculate the average influence vector of the samples in the right child
v0.9.0,take the square of each of the entries of the influence vectors and normalize
v0.9.0,by size of each child
v0.9.0,calculate the vector score of each candidate split as the average of left and right
v0.9.0,influence vectors
v0.9.0,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.9.0,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.9.0,where there might be large discontinuities in some parameter as the conditioning set varies
v0.9.0,calculate the scalar score of each split by aggregating across the vector of scores
v0.9.0,Find split that minimizes criterion
v0.9.0,Create child nodes with corresponding subsamples
v0.9.0,add the created children to the list of not yet split nodes
v0.9.0b1,configuration is all pulled from setup.cfg
v0.9.0b1,-*- coding: utf-8 -*-
v0.9.0b1,
v0.9.0b1,Configuration file for the Sphinx documentation builder.
v0.9.0b1,
v0.9.0b1,This file does only contain a selection of the most common options. For a
v0.9.0b1,full list see the documentation:
v0.9.0b1,http://www.sphinx-doc.org/en/master/config
v0.9.0b1,-- Path setup --------------------------------------------------------------
v0.9.0b1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.9.0b1,add these directories to sys.path here. If the directory is relative to the
v0.9.0b1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.9.0b1,
v0.9.0b1,-- Project information -----------------------------------------------------
v0.9.0b1,-- General configuration ---------------------------------------------------
v0.9.0b1,"If your documentation needs a minimal Sphinx version, state it here."
v0.9.0b1,
v0.9.0b1,needs_sphinx = '1.0'
v0.9.0b1,"Add any Sphinx extension module names here, as strings. They can be"
v0.9.0b1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.9.0b1,ones.
v0.9.0b1,"Add any paths that contain templates here, relative to this directory."
v0.9.0b1,The suffix(es) of source filenames.
v0.9.0b1,You can specify multiple suffix as a list of string:
v0.9.0b1,
v0.9.0b1,"source_suffix = ['.rst', '.md']"
v0.9.0b1,The master toctree document.
v0.9.0b1,The language for content autogenerated by Sphinx. Refer to documentation
v0.9.0b1,for a list of supported languages.
v0.9.0b1,
v0.9.0b1,This is also used if you do content translation via gettext catalogs.
v0.9.0b1,"Usually you set ""language"" from the command line for these cases."
v0.9.0b1,"List of patterns, relative to source directory, that match files and"
v0.9.0b1,directories to ignore when looking for source files.
v0.9.0b1,This pattern also affects html_static_path and html_extra_path.
v0.9.0b1,The name of the Pygments (syntax highlighting) style to use.
v0.9.0b1,-- Options for HTML output -------------------------------------------------
v0.9.0b1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.9.0b1,a list of builtin themes.
v0.9.0b1,
v0.9.0b1,Theme options are theme-specific and customize the look and feel of a theme
v0.9.0b1,"further.  For a list of options available for each theme, see the"
v0.9.0b1,documentation.
v0.9.0b1,
v0.9.0b1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.9.0b1,"relative to this directory. They are copied after the builtin static files,"
v0.9.0b1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.9.0b1,html_static_path = ['_static']
v0.9.0b1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.9.0b1,to template names.
v0.9.0b1,
v0.9.0b1,The default sidebars (for documents that don't match any pattern) are
v0.9.0b1,defined by theme itself.  Builtin themes are using these templates by
v0.9.0b1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.9.0b1,'searchbox.html']``.
v0.9.0b1,
v0.9.0b1,html_sidebars = {}
v0.9.0b1,-- Options for HTMLHelp output ---------------------------------------------
v0.9.0b1,Output file base name for HTML help builder.
v0.9.0b1,-- Options for LaTeX output ------------------------------------------------
v0.9.0b1,The paper size ('letterpaper' or 'a4paper').
v0.9.0b1,
v0.9.0b1,"'papersize': 'letterpaper',"
v0.9.0b1,"The font size ('10pt', '11pt' or '12pt')."
v0.9.0b1,
v0.9.0b1,"'pointsize': '10pt',"
v0.9.0b1,Additional stuff for the LaTeX preamble.
v0.9.0b1,
v0.9.0b1,"'preamble': '',"
v0.9.0b1,Latex figure (float) alignment
v0.9.0b1,
v0.9.0b1,"'figure_align': 'htbp',"
v0.9.0b1,Grouping the document tree into LaTeX files. List of tuples
v0.9.0b1,"(source start file, target name, title,"
v0.9.0b1,"author, documentclass [howto, manual, or own class])."
v0.9.0b1,-- Options for manual page output ------------------------------------------
v0.9.0b1,One entry per manual page. List of tuples
v0.9.0b1,"(source start file, name, description, authors, manual section)."
v0.9.0b1,-- Options for Texinfo output ----------------------------------------------
v0.9.0b1,Grouping the document tree into Texinfo files. List of tuples
v0.9.0b1,"(source start file, target name, title, author,"
v0.9.0b1,"dir menu entry, description, category)"
v0.9.0b1,-- Options for Epub output -------------------------------------------------
v0.9.0b1,Bibliographic Dublin Core info.
v0.9.0b1,The unique identifier of the text. This can be a ISBN number
v0.9.0b1,or the project homepage.
v0.9.0b1,
v0.9.0b1,epub_identifier = ''
v0.9.0b1,A unique identification for the text.
v0.9.0b1,
v0.9.0b1,epub_uid = ''
v0.9.0b1,A list of files that should not be packed into the epub file.
v0.9.0b1,-- Extension configuration -------------------------------------------------
v0.9.0b1,-- Options for intersphinx extension ---------------------------------------
v0.9.0b1,Example configuration for intersphinx: refer to the Python standard library.
v0.9.0b1,-- Options for todo extension ----------------------------------------------
v0.9.0b1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.9.0b1,-- Options for doctest extension -------------------------------------------
v0.9.0b1,we can document otherwise excluded entities here by returning False
v0.9.0b1,or skip otherwise included entities by returning True
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Calculate residuals
v0.9.0b1,Estimate E[T_res | Z_res]
v0.9.0b1,TODO. Deal with multi-class instrument
v0.9.0b1,Calculate nuisances
v0.9.0b1,Estimate E[T_res | Z_res]
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.9.0b1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.9.0b1,TODO. Deal with multi-class instrument
v0.9.0b1,Estimate final model of theta(X) by minimizing the square loss:
v0.9.0b1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.9.0b1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.9.0b1,at the expense of some small bias. For points with very small covariance we revert
v0.9.0b1,to the model-based preliminary estimate and do not add the correction term.
v0.9.0b1,Estimate preliminary theta in cross fitting manner
v0.9.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.0b1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.9.0b1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.9.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.0b1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.9.0b1,TODO. The solution below is not really a valid cross-fitting
v0.9.0b1,as the test data are used to create the proj_t on the train
v0.9.0b1,which in the second train-test loop is used to create the nuisance
v0.9.0b1,cov on the test data. Hence the T variable of some sample
v0.9.0b1,"is implicitly correlated with its cov nuisance, through this flow"
v0.9.0b1,"of information. However, this seems a rather weak correlation."
v0.9.0b1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.9.0b1,model.
v0.9.0b1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.9.0b1,Estimate preliminary theta in cross fitting manner
v0.9.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.0b1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.9.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.0b1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.9.0b1,#############################################################################
v0.9.0b1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.9.0b1,A/B test
v0.9.0b1,#############################################################################
v0.9.0b1,Estimate preliminary theta in cross fitting manner
v0.9.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.9.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.9.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.9.0b1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.9.0b1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.9.0b1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.9.0b1,model_T_XZ = lambda: model_clf()
v0.9.0b1,#'days_visited': lambda:
v0.9.0b1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.9.0b1,Turn strings into categories for numeric mapping
v0.9.0b1,### Defining some generic regressors and classifiers
v0.9.0b1,This a generic non-parametric regressor
v0.9.0b1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.9.0b1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.9.0b1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.9.0b1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.9.0b1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.9.0b1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.9.0b1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.9.0b1,model = lambda: LinearRegression(n_jobs=-1)
v0.9.0b1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.9.0b1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.9.0b1,underlying model whenever predict is called.
v0.9.0b1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.9.0b1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.9.0b1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.9.0b1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.9.0b1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.9.0b1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.9.0b1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.9.0b1,We need to specify models to be used for each of these residualizations
v0.9.0b1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.9.0b1,"E[T | X, Z]"
v0.9.0b1,E[TZ | X]
v0.9.0b1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.9.0b1,n_splits determines the number of splits to be used for cross-fitting.
v0.9.0b1,# Algorithm 2 - Current Method
v0.9.0b1,In[121]:
v0.9.0b1,# Algorithm 3 - DRIV ATE
v0.9.0b1,dmliv_model_effect = lambda: model()
v0.9.0b1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.9.0b1,"dmliv_model_effect(),"
v0.9.0b1,n_splits=1)
v0.9.0b1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.9.0b1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.9.0b1,"Once multiple treatments are supported, we'll need to fix this"
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.9.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.9.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,TODO. Deal with multi-class instrument/treatment
v0.9.0b1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.9.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.9.0b1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.9.0b1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.9.0b1,##################
v0.9.0b1,Global settings #
v0.9.0b1,##################
v0.9.0b1,Global plotting controls
v0.9.0b1,"Control for support size, can control for more"
v0.9.0b1,#################
v0.9.0b1,File utilities #
v0.9.0b1,#################
v0.9.0b1,#################
v0.9.0b1,Plotting utils #
v0.9.0b1,#################
v0.9.0b1,bias
v0.9.0b1,var
v0.9.0b1,rmse
v0.9.0b1,r2
v0.9.0b1,Infer feature dimension
v0.9.0b1,Metrics by support plots
v0.9.0b1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.9.0b1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.9.0b1,Steven Wu <zhiww@microsoft.com>
v0.9.0b1,Initialize causal tree parameters
v0.9.0b1,Create splits of causal tree
v0.9.0b1,Estimate treatment effects at the leafs
v0.9.0b1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.9.0b1,the corresponding split and associating the effect computed on that leaf
v0.9.0b1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.9.0b1,Safety check
v0.9.0b1,Weighted linear regression
v0.9.0b1,Calculates weights
v0.9.0b1,Bootstraping has repetitions in tree sample so we need to iterate
v0.9.0b1,over all indices
v0.9.0b1,Similar for `a` weights
v0.9.0b1,Doesn't have sample weights
v0.9.0b1,Is a linear model
v0.9.0b1,Weighted linear regression
v0.9.0b1,Calculates weights
v0.9.0b1,Bootstraping has repetitions in tree sample so we need to iterate
v0.9.0b1,over all indices
v0.9.0b1,Similar for `a` weights
v0.9.0b1,normalize weights
v0.9.0b1,"Split the data in half, train and test"
v0.9.0b1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.9.0b1,"a function of W, using only the train fold"
v0.9.0b1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.9.0b1,"Split the data in half, train and test"
v0.9.0b1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.9.0b1,"a function of W, using only the train fold"
v0.9.0b1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.9.0b1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.9.0b1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.9.0b1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.9.0b1,"Split the data in half, train and test"
v0.9.0b1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.9.0b1,"a function of x, using only the train fold"
v0.9.0b1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.9.0b1,Compute coefficient by OLS on residuals
v0.9.0b1,"Split the data in half, train and test"
v0.9.0b1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.9.0b1,"a function of x, using only the train fold"
v0.9.0b1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.9.0b1,Estimate multipliers for second order orthogonal method
v0.9.0b1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.9.0b1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.9.0b1,Create local sample set
v0.9.0b1,compute the base estimate for the current node using double ml or second order double ml
v0.9.0b1,compute the influence functions here that are used for the criterion
v0.9.0b1,generate random proposals of dimensions to split
v0.9.0b1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.9.0b1,compute criterion for each proposal
v0.9.0b1,if splitting creates valid leafs in terms of mean leaf size
v0.9.0b1,Calculate criterion for split
v0.9.0b1,Else set criterion to infinity so that this split is not chosen
v0.9.0b1,If no good split was found
v0.9.0b1,Find split that minimizes criterion
v0.9.0b1,Set the split attributes at the node
v0.9.0b1,Create child nodes with corresponding subsamples
v0.9.0b1,Recursively split children
v0.9.0b1,Return parent node
v0.9.0b1,estimate the local parameter at the leaf using the estimate data
v0.9.0b1,###################
v0.9.0b1,Argument parsing #
v0.9.0b1,###################
v0.9.0b1,#########################################
v0.9.0b1,Parameters constant across experiments #
v0.9.0b1,#########################################
v0.9.0b1,Outcome support
v0.9.0b1,Treatment support
v0.9.0b1,Evaluation grid
v0.9.0b1,Treatment effects array
v0.9.0b1,Other variables
v0.9.0b1,##########################
v0.9.0b1,Data Generating Process #
v0.9.0b1,##########################
v0.9.0b1,Log iteration
v0.9.0b1,"Generate controls, features, treatment and outcome"
v0.9.0b1,T and Y residuals to be used in later scripts
v0.9.0b1,Save generated dataset
v0.9.0b1,#################
v0.9.0b1,ORF parameters #
v0.9.0b1,#################
v0.9.0b1,######################################
v0.9.0b1,Train and evaluate treatment effect #
v0.9.0b1,######################################
v0.9.0b1,########
v0.9.0b1,Plots #
v0.9.0b1,########
v0.9.0b1,###############
v0.9.0b1,Save results #
v0.9.0b1,###############
v0.9.0b1,##############
v0.9.0b1,Run Rscript #
v0.9.0b1,##############
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Check if model is sparse enough for this model
v0.9.0b1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.9.0b1,TODO: any way to avoid creating a copy if the array was already dense?
v0.9.0b1,"the call is necessary if the input was something like a list, though"
v0.9.0b1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.9.0b1,so convert to pydata sparse first
v0.9.0b1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.9.0b1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.9.0b1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.9.0b1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.9.0b1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.9.0b1,Type to column extraction function
v0.9.0b1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.9.0b1,same number of input definitions as arrays
v0.9.0b1,input definitions have same number of dimensions as each array
v0.9.0b1,all result indices are unique
v0.9.0b1,all result indices must match at least one input index
v0.9.0b1,"map indices to all array, axis pairs for that index"
v0.9.0b1,each index has the same cardinality wherever it appears
v0.9.0b1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.9.0b1,Algo: while list contains more than one entry
v0.9.0b1,take two entries
v0.9.0b1,sort both lists by intersection of their indices
v0.9.0b1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.9.0b1,"take the union of indices and the product of values), stepping through each list linearly"
v0.9.0b1,TODO: might be faster to break into connected components first
v0.9.0b1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.9.0b1,"so compute their content separately, then take cartesian product"
v0.9.0b1,this would save a few pointless sorts by empty tuples
v0.9.0b1,TODO: Consider investigating other performance ideas for these cases
v0.9.0b1,where the dense method beat the sparse method (usually sparse is faster)
v0.9.0b1,"e,facd,c->cfed"
v0.9.0b1,sparse: 0.0335489
v0.9.0b1,dense:  0.011465999999999997
v0.9.0b1,"gbd,da,egb->da"
v0.9.0b1,sparse: 0.0791625
v0.9.0b1,dense:  0.007319099999999995
v0.9.0b1,"dcc,d,faedb,c->abe"
v0.9.0b1,sparse: 1.2868097
v0.9.0b1,dense:  0.44605229999999985
v0.9.0b1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.9.0b1,TODO: would using einsum's paths to optimize the order of merging help?
v0.9.0b1,assume that we should perform nested cross-validation if and only if
v0.9.0b1,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.9.0b1,logic copied from check_cv
v0.9.0b1,otherwise we will assume the user already set the cv attribute to something
v0.9.0b1,compatible with splitting with a 'groups' argument
v0.9.0b1,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.9.0b1,don't use the groups when calling split internally
v0.9.0b1,Normalize weights
v0.9.0b1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.9.0b1,"if we're decorating a class, just update the __init__ method,"
v0.9.0b1,so that the result is still a class instead of a wrapper method
v0.9.0b1,"want to enforce that each bad_arg was either in kwargs,"
v0.9.0b1,or else it was in neither and is just taking its default value
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.0b1,define the index of d_x to filter for each given T
v0.9.0b1,filter X after broadcast with T for each given T
v0.9.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.0b1,"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"since inference objects can be stateful, we must copy it before fitting;"
v0.9.0b1,otherwise this sequence wouldn't work:
v0.9.0b1,"est1.fit(..., inference=inf)"
v0.9.0b1,"est2.fit(..., inference=inf)"
v0.9.0b1,est1.effect_interval(...)
v0.9.0b1,because inf now stores state from fitting est2
v0.9.0b1,This flag is true when names are set in a child class instead
v0.9.0b1,"If names are set in a child class, add an attribute reflecting that"
v0.9.0b1,This works only if X is passed as a kwarg
v0.9.0b1,We plan to enforce X as kwarg only in new releases
v0.9.0b1,This checks if names have been set in a child class
v0.9.0b1,"If names were set in a child class, don't do it again"
v0.9.0b1,call the wrapped fit method
v0.9.0b1,NOTE: we call inference fit *after* calling the main fit method
v0.9.0b1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.9.0b1,but tensordot can't be applied to this problem because we don't sum over m
v0.9.0b1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.9.0b1,of rows of T was not taken into account
v0.9.0b1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.9.0b1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.9.0b1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.9.0b1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.9.0b1,Get input names
v0.9.0b1,Summary
v0.9.0b1,add statsmodels to parent's options
v0.9.0b1,add debiasedlasso to parent's options
v0.9.0b1,add blb to parent's options
v0.9.0b1,TODO Share some logic with non-discrete version
v0.9.0b1,Get input names
v0.9.0b1,Summary
v0.9.0b1,add statsmodels to parent's options
v0.9.0b1,add statsmodels to parent's options
v0.9.0b1,add blb to parent's options
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,remove None arguments
v0.9.0b1,"scores entries should be lists of scores, so make each entry a singleton list"
v0.9.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.9.0b1,generate an instance of the final model
v0.9.0b1,generate an instance of the nuisance model
v0.9.0b1,_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
v0.9.0b1,alteration even when we only want to fit_final.
v0.9.0b1,use a binary array to get stratified split in case of discrete treatment
v0.9.0b1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.9.0b1,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.9.0b1,"however, sklearn doesn't support both stratifying and grouping (see"
v0.9.0b1,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.9.0b1,their own object that supports grouping if they want to use groups.
v0.9.0b1,######################################################
v0.9.0b1,These should be removed once `n_splits` is deprecated
v0.9.0b1,######################################################
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.9.0b1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.9.0b1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.9.0b1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.9.0b1,checks that X is 2D array.
v0.9.0b1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.0b1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.0b1,Handles the corner case when X=None but featurizer might be not None
v0.9.0b1,"This fails if X=None and featurizer is not None, but that case is handled above"
v0.9.0b1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.9.0b1,"Replacing this method which is invalid for this class, so that we make the"
v0.9.0b1,dosctring empty and not appear in the docs.
v0.9.0b1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.9.0b1,TODO: support sample_var
v0.9.0b1,Replacing to remove docstring
v0.9.0b1,###################################################################
v0.9.0b1,Everything below should be removed once parameters are deprecated
v0.9.0b1,###################################################################
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"if both X and W are None, just return a column of ones"
v0.9.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.0b1,We need to go back to the label representation of the one-hot so as to call
v0.9.0b1,the classifier.
v0.9.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.0b1,We need to go back to the label representation of the one-hot so as to call
v0.9.0b1,the classifier.
v0.9.0b1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.9.0b1,This works both with our without the weighting trick as the treatments T are unit vector
v0.9.0b1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.9.0b1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.9.0b1,both Parametric and Non Parametric DML.
v0.9.0b1,NOTE: important to use the rlearner_model_final_ attribute instead of the
v0.9.0b1,attribute so that the trained featurizer will be passed through
v0.9.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.0b1,for internal use by the library
v0.9.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.0b1,We need to use the rlearner's copy to retain the information from fitting
v0.9.0b1,Handles the corner case when X=None but featurizer might be not None
v0.9.0b1,"This fails if X=None and featurizer is not None, but that case is handled above"
v0.9.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.0b1,since we clone it and fit separate copies
v0.9.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.0b1,TODO: support sample_var
v0.9.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.0b1,since we clone it and fit separate copies
v0.9.0b1,add blb to parent's options
v0.9.0b1,override only so that we can update the docstring to indicate
v0.9.0b1,support for `GenericSingleTreatmentModelFinalInference`
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,note that groups are not passed to score because they are only used for fitting
v0.9.0b1,note that groups are not passed to score because they are only used for fitting
v0.9.0b1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.0b1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.9.0b1,NOTE: important to get parent's wrapped copy so that
v0.9.0b1,"after training wrapped featurizer is also trained, etc."
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.9.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.9.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.9.0b1,since we clone it and fit separate copies
v0.9.0b1,override only so that we can update the docstring to indicate support for `blb`
v0.9.0b1,######################################################
v0.9.0b1,These should be removed once `n_splits` is deprecated
v0.9.0b1,######################################################
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Remove children with nonwhite mothers from the treatment group
v0.9.0b1,Remove children with nonwhite mothers from the treatment group
v0.9.0b1,Select columns
v0.9.0b1,Scale the numeric variables
v0.9.0b1,"Change the binary variable 'first' takes values in {1,2}"
v0.9.0b1,Append a column of ones as intercept
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.9.0b1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.9.0b1,d_t=1 here since we measure the effect across all Ts
v0.9.0b1,once the estimator has been fit
v0.9.0b1,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.9.0b1,an intercept even when bias is part of coef.
v0.9.0b1,We can write effect inference as a function of prediction and prediction standard error of
v0.9.0b1,the final method for linear models
v0.9.0b1,d_t=1 here since we measure the effect across all Ts
v0.9.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.9.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.9.0b1,"send treatment to the end, pull bounds to the front"
v0.9.0b1,d_t=1 here since we measure the effect across all Ts
v0.9.0b1,need to set the fit args before the estimator is fit
v0.9.0b1,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.9.0b1,1. Uncertainty of Mean Point Estimate
v0.9.0b1,2. Distribution of Point Estimate
v0.9.0b1,3. Total Variance of Point Estimate
v0.9.0b1,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.9.0b1,so bail out early; note that it might be possible to correct the algorithm for
v0.9.0b1,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.9.0b1,be clean
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,TODO: Add a __dir__ implementation?
v0.9.0b1,don't proxy special methods
v0.9.0b1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.9.0b1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.9.0b1,then we should be computing a confidence interval for the wrapped calls
v0.9.0b1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.9.0b1,second level bootstrap which would be prohibitive computationally?
v0.9.0b1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.9.0b1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.9.0b1,can't import from econml.inference at top level without creating cyclical dependencies
v0.9.0b1,Note that inference results are always methods even if the inference is for a property
v0.9.0b1,(e.g. coef__inference() is a method but coef_ is a property)
v0.9.0b1,Therefore we must insert a lambda if getting inference for a non-callable
v0.9.0b1,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.9.0b1,"try to get interval/std first if appropriate,"
v0.9.0b1,since we don't prefer a wrapped method with this name
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,
v0.9.0b1,This code is a fork from:
v0.9.0b1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
v0.9.0b1,published under the following license and copyright:
v0.9.0b1,BSD 3-Clause License
v0.9.0b1,
v0.9.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.0b1,All rights reserved.
v0.9.0b1,Set parameters
v0.9.0b1,Don't instantiate estimators now! Parameters of base_estimator might
v0.9.0b1,"still change. Eg., when grid-searching with the nested object syntax."
v0.9.0b1,self.estimators_ needs to be filled by the derived classes in fit.
v0.9.0b1,Compute the number of jobs
v0.9.0b1,Partition estimators between jobs
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,
v0.9.0b1,This code contains snippets of code from:
v0.9.0b1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
v0.9.0b1,published under the following license and copyright:
v0.9.0b1,BSD 3-Clause License
v0.9.0b1,
v0.9.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.0b1,All rights reserved.
v0.9.0b1,=============================================================================
v0.9.0b1,Types and constants
v0.9.0b1,=============================================================================
v0.9.0b1,=============================================================================
v0.9.0b1,Base GRF tree
v0.9.0b1,=============================================================================
v0.9.0b1,Determine output settings
v0.9.0b1,"Important: This must be the first invocation of the random state at fit time, so that"
v0.9.0b1,train/test splits are re-generatable from an external object simply by knowing the
v0.9.0b1,random_state parameter of the tree. Can be useful in the future if one wants to create local
v0.9.0b1,linear predictions. Currently is also useful for testing.
v0.9.0b1,reshape is necessary to preserve the data contiguity against vs
v0.9.0b1,"[:, np.newaxis] that does not."
v0.9.0b1,Check parameters
v0.9.0b1,Set min_weight_leaf from min_weight_fraction_leaf
v0.9.0b1,Build tree
v0.9.0b1,We calculate the maximum number of samples from each half-split that any node in the tree can
v0.9.0b1,hold. Used by criterion for memory space savings.
v0.9.0b1,Initialize the criterion object and the criterion_val object if honest.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,=============================================================================
v0.9.0b1,A MultOutputWrapper for GRF classes
v0.9.0b1,=============================================================================
v0.9.0b1,=============================================================================
v0.9.0b1,Instantiations of Generalized Random Forest
v0.9.0b1,=============================================================================
v0.9.0b1,"Append a constant treatment if `fit_intercept=True`, the coefficient"
v0.9.0b1,in front of the constant treatment is the intercept in the moment equation.
v0.9.0b1,"Append a constant treatment and constant instrument if `fit_intercept=True`,"
v0.9.0b1,the coefficient in front of the constant treatment is the intercept in the moment equation.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,
v0.9.0b1,This code contains snippets of code from
v0.9.0b1,https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
v0.9.0b1,published under the following license and copyright:
v0.9.0b1,BSD 3-Clause License
v0.9.0b1,
v0.9.0b1,Copyright (c) 2007-2020 The scikit-learn developers.
v0.9.0b1,All rights reserved.
v0.9.0b1,=============================================================================
v0.9.0b1,Base Generalized Random Forest
v0.9.0b1,=============================================================================
v0.9.0b1,Remap output
v0.9.0b1,reshape is necessary to preserve the data contiguity against vs
v0.9.0b1,"[:, np.newaxis] that does not."
v0.9.0b1,reshape is necessary to preserve the data contiguity against vs
v0.9.0b1,"[:, np.newaxis] that does not."
v0.9.0b1,Get subsample sample size
v0.9.0b1,Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
v0.9.0b1,We calculate the min eigenvalue proxy that each criterion is considering
v0.9.0b1,"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
v0.9.0b1,Check parameters
v0.9.0b1,We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
v0.9.0b1,if this is the first `fit` call of the warm start mode.
v0.9.0b1,"Free allocated memory, if any"
v0.9.0b1,the below are needed to replicate randomness of subsampling when warm_start=True
v0.9.0b1,We draw from the random state to get the random state we
v0.9.0b1,would have got if we hadn't used a warm_start.
v0.9.0b1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.9.0b1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.9.0b1,but would still advance randomness enough so that tree subsamples will be different.
v0.9.0b1,Generating indices a priori before parallelism ended up being orders of magnitude
v0.9.0b1,faster than how sklearn does it. The reason is that random samplers do not release the
v0.9.0b1,gil it seems.
v0.9.0b1,Advancing subsample_random_state. Assumes each prior fit call has the same number of
v0.9.0b1,"samples at fit time. If not then this would not exactly replicate a single batch execution,"
v0.9.0b1,but would still advance randomness enough so that tree subsamples will be different.
v0.9.0b1,Parallel loop: we prefer the threading backend as the Cython code
v0.9.0b1,for fitting the trees is internally releasing the Python GIL
v0.9.0b1,making threading more efficient than multiprocessing in
v0.9.0b1,"that case. However, for joblib 0.12+ we respect any"
v0.9.0b1,"parallel_backend contexts set at a higher level,"
v0.9.0b1,since correctness does not rely on using threads.
v0.9.0b1,Collect newly grown trees
v0.9.0b1,Check data
v0.9.0b1,Assign chunk of trees to jobs
v0.9.0b1,avoid storing the output of every estimator by summing them here
v0.9.0b1,Parallel loop
v0.9.0b1,Check data
v0.9.0b1,Assign chunk of trees to jobs
v0.9.0b1,Parallel loop
v0.9.0b1,Check data
v0.9.0b1,Assign chunk of trees to jobs
v0.9.0b1,Parallel loop
v0.9.0b1,####################
v0.9.0b1,Variance correction
v0.9.0b1,####################
v0.9.0b1,Subtract the average within bag variance. This ends up being equal to the
v0.9.0b1,overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
v0.9.0b1,The negative part is just sq_between.
v0.9.0b1,Objective bayes debiasing for the diagonals where we know a-prior they are positive
v0.9.0b1,"The off diagonals we have no objective prior, so no correction is applied."
v0.9.0b1,Finally correcting the pred_cov or pred_var
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,testing importances
v0.9.0b1,testing heterogeneity importances
v0.9.0b1,Testing that all parameters do what they are supposed to
v0.9.0b1,"testing predict, apply and decision path"
v0.9.0b1,test that the subsampling scheme past to the trees is correct
v0.9.0b1,test that the estimator calcualtes var correctly
v0.9.0b1,test api
v0.9.0b1,test accuracy
v0.9.0b1,test the projection functionality of forests
v0.9.0b1,test that the estimator calcualtes var correctly
v0.9.0b1,test api
v0.9.0b1,test that the estimator calcualtes var correctly
v0.9.0b1,"test that the estimator accepts lists, tuples and pandas data frames"
v0.9.0b1,test that we raise errors in mishandled situations.
v0.9.0b1,test that the subsampling scheme past to the trees is correct
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.9.0b1,creating notebooks that are annoying for our users to actually run themselves
v0.9.0b1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.9.0b1,"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.9.0b1,"prior to calling interpret, can't plot, render, etc."
v0.9.0b1,can interpret without uncertainty
v0.9.0b1,can't interpret with uncertainty if inference wasn't used during fit
v0.9.0b1,can interpret with uncertainty if we refit
v0.9.0b1,can interpret without uncertainty
v0.9.0b1,can't treat before interpreting
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,simple DGP only for illustration
v0.9.0b1,Define the treatment model neural network architecture
v0.9.0b1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.9.0b1,"so the input shape is (d_z + d_x,)"
v0.9.0b1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.9.0b1,add extra layers on top for the mixture density network
v0.9.0b1,Define the response model neural network architecture
v0.9.0b1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.9.0b1,"so the input shape is (d_t + d_x,)"
v0.9.0b1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.9.0b1,"NOTE: For the response model, it is important to define the model *outside*"
v0.9.0b1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.9.0b1,so that the same weights will be reused in each instantiation
v0.9.0b1,number of samples to use in second estimate of the response
v0.9.0b1,(to make loss estimate unbiased)
v0.9.0b1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.9.0b1,do something with predictions...
v0.9.0b1,also test vector t and y
v0.9.0b1,simple DGP only for illustration
v0.9.0b1,Define the treatment model neural network architecture
v0.9.0b1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.9.0b1,"so the input shape is (d_z + d_x,)"
v0.9.0b1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.9.0b1,add extra layers on top for the mixture density network
v0.9.0b1,Define the response model neural network architecture
v0.9.0b1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.9.0b1,"so the input shape is (d_t + d_x,)"
v0.9.0b1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.9.0b1,"NOTE: For the response model, it is important to define the model *outside*"
v0.9.0b1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.9.0b1,so that the same weights will be reused in each instantiation
v0.9.0b1,number of samples to use in second estimate of the response
v0.9.0b1,(to make loss estimate unbiased)
v0.9.0b1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.9.0b1,do something with predictions...
v0.9.0b1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.9.0b1,test = True ensures we draw test set images
v0.9.0b1,test = True ensures we draw test set images
v0.9.0b1,re-draw to get new independent treatment and implied response
v0.9.0b1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.9.0b1,above is necesary so that reduced form doesn't win
v0.9.0b1,covariates: time and emotion
v0.9.0b1,random instrument
v0.9.0b1,z -> price
v0.9.0b1,true observable demand function
v0.9.0b1,errors
v0.9.0b1,response
v0.9.0b1,test = True ensures we draw test set images
v0.9.0b1,test = True ensures we draw test set images
v0.9.0b1,re-draw to get new independent treatment and implied response
v0.9.0b1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.9.0b1,above is necesary so that reduced form doesn't win
v0.9.0b1,covariates: time and emotion
v0.9.0b1,random instrument
v0.9.0b1,z -> price
v0.9.0b1,true observable demand function
v0.9.0b1,errors
v0.9.0b1,response
v0.9.0b1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.9.0b1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.9.0b1,For some reason this doesn't work at all when run against the CNTK backend...
v0.9.0b1,"model.compile('nadam', loss=lambda _,l:l)"
v0.9.0b1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.9.0b1,generate a valiation set
v0.9.0b1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.9.0b1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,DGP constants
v0.9.0b1,Generate data
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,testing importances
v0.9.0b1,testing heterogeneity importances
v0.9.0b1,Testing that all parameters do what they are supposed to
v0.9.0b1,"testing predict, apply and decision path"
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.9.0b1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.9.0b1,so we need to transpose the result
v0.9.0b1,1-d output
v0.9.0b1,2-d output
v0.9.0b1,Single dimensional output y
v0.9.0b1,Multi-dimensional output y
v0.9.0b1,1-d y
v0.9.0b1,multi-d y
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,test that we can fit with the same arguments as the base estimator
v0.9.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0b1,with the same shape for the lower and upper bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,test that we can do the same thing once we provide percentile bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0b1,with the same shape for the lower and upper bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,test that we can do the same thing once we provide percentile bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,test that we can fit with the same arguments as the base estimator
v0.9.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0b1,with the same shape for the lower and upper bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,test that we can do the same thing once we provide percentile bounds
v0.9.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0b1,with the same shape for the lower and upper bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,test that we can do the same thing once we provide percentile bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,test that we can fit with the same arguments as the base estimator
v0.9.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.9.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0b1,with the same shape for the lower and upper bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,test that we can do the same thing once we provide percentile bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.9.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0b1,with the same shape for the lower and upper bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,test that we can do the same thing once we provide percentile bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0b1,with the same shape for the lower and upper bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,test that the estimated effect is usually within the bounds
v0.9.0b1,test that we can do the same thing once we provide alpha explicitly
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,test that the estimated effect is usually within the bounds
v0.9.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.9.0b1,with the same shape for the lower and upper bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,TODO: test that the estimated effect is usually within the bounds
v0.9.0b1,and that the true effect is also usually within the bounds
v0.9.0b1,test that we can do the same thing once we provide percentile bounds
v0.9.0b1,test that the lower and upper bounds differ
v0.9.0b1,TODO: test that the estimated effect is usually within the bounds
v0.9.0b1,and that the true effect is also usually within the bounds
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,DGP constants
v0.9.0b1,Generate data
v0.9.0b1,Test inference results when `cate_feature_names` doesn not exist
v0.9.0b1,Test inference results when `cate_feature_names` doesn not exist
v0.9.0b1,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.9.0b1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.0b1,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.9.0b1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.0b1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.9.0b1,pvalue for second column should be greater than zero since some points are on either side
v0.9.0b1,of the tested value
v0.9.0b1,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.9.0b1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.9.0b1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.9.0b1,Test non keyword based calls to fit
v0.9.0b1,test non-array inputs
v0.9.0b1,Test custom splitter
v0.9.0b1,Test incomplete set of test folds
v0.9.0b1,"y scores should be positive, since W predicts Y somewhat"
v0.9.0b1,"t scores might not be, since W and T are uncorrelated"
v0.9.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,make sure cross product varies more slowly with first array
v0.9.0b1,and that vectors are okay as inputs
v0.9.0b1,number of inputs in specification must match number of inputs
v0.9.0b1,must have an output
v0.9.0b1,output indices must be unique
v0.9.0b1,output indices must be present in an input
v0.9.0b1,number of indices must match number of dimensions for each input
v0.9.0b1,repeated indices must always have consistent sizes
v0.9.0b1,transpose
v0.9.0b1,tensordot
v0.9.0b1,trace
v0.9.0b1,TODO: set up proper flag for this
v0.9.0b1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.9.0b1,"of all of the distinct indices that appear in any input,"
v0.9.0b1,pick a random subset of them (of size at most 5) to appear in the output
v0.9.0b1,creating an instance should warn
v0.9.0b1,using the instance should not warn
v0.9.0b1,using the deprecated method should warn
v0.9.0b1,don't warn if b and c are passed by keyword
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Preprocess data
v0.9.0b1,Convert 'week' to a date
v0.9.0b1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.9.0b1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.9.0b1,Take log of price
v0.9.0b1,Make brand numeric
v0.9.0b1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.9.0b1,which have all zero coeffs
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.9.0b1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.9.0b1,TODO: test something rather than just print...
v0.9.0b1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.9.0b1,pick some arbitrary X
v0.9.0b1,pick some arbitrary T
v0.9.0b1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.9.0b1,The average variance should be lower when using monte carlo iterations
v0.9.0b1,"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
v0.9.0b1,The average variance should be lower when using monte carlo iterations
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,ensure that we've got at least two of every row
v0.9.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.0b1,need to make sure we get all *joint* combinations
v0.9.0b1,IntentToTreat only supports binary treatments/instruments
v0.9.0b1,IntentToTreat only supports binary treatments/instruments
v0.9.0b1,IntentToTreat requires X
v0.9.0b1,ensure we can serialize unfit estimator
v0.9.0b1,these support only W but not X
v0.9.0b1,"these support only binary, not general discrete T and Z"
v0.9.0b1,ensure we can serialize fit estimator
v0.9.0b1,make sure we can call the marginal_effect and effect methods
v0.9.0b1,TODO: add tests for extra properties like coef_ where they exist
v0.9.0b1,TODO: add tests for extra properties like coef_ where they exist
v0.9.0b1,"make sure we can call effect with implied scalar treatments,"
v0.9.0b1,"no matter the dimensions of T, and also that we warn when there"
v0.9.0b1,are multiple treatments
v0.9.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.0b1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.9.0b1,"however, with custom splits the checking happens in the first stage wrapper"
v0.9.0b1,where we don't have all of the required information to do this;
v0.9.0b1,we'd probably need to add it to _crossfit instead
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.9.0b1,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.9.0b1,The __warningregistry__'s need to be in a pristine state for tests
v0.9.0b1,to work properly.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Set random seed
v0.9.0b1,Generate data
v0.9.0b1,DGP constants
v0.9.0b1,Test data
v0.9.0b1,Constant treatment effect
v0.9.0b1,Constant treatment with multi output Y
v0.9.0b1,Heterogeneous treatment
v0.9.0b1,Heterogeneous treatment with multi output Y
v0.9.0b1,TLearner test
v0.9.0b1,Instantiate TLearner
v0.9.0b1,Test inputs
v0.9.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.0b1,Instantiate SLearner
v0.9.0b1,Test inputs
v0.9.0b1,Test constant treatment effect
v0.9.0b1,Test constant treatment effect with multi output Y
v0.9.0b1,Test heterogeneous treatment effect
v0.9.0b1,Need interactions between T and features
v0.9.0b1,Test heterogeneous treatment effect with multi output Y
v0.9.0b1,Instantiate XLearner
v0.9.0b1,Test inputs
v0.9.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.0b1,Instantiate DomainAdaptationLearner
v0.9.0b1,Test inputs
v0.9.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.0b1,Get the true treatment effect
v0.9.0b1,Get the true treatment effect
v0.9.0b1,Fit learner and get the effect and marginal effect
v0.9.0b1,Compute treatment effect residuals (absolute)
v0.9.0b1,Check that at least 90% of predictions are within tolerance interval
v0.9.0b1,Check whether the output shape is right
v0.9.0b1,Check that one can pass in regular lists
v0.9.0b1,Check that it fails correctly if lists of different shape are passed in
v0.9.0b1,"Check that it works when T, Y have shape (n, 1)"
v0.9.0b1,Generate covariates
v0.9.0b1,Generate treatment
v0.9.0b1,Calculate outcome
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,DGP constants
v0.9.0b1,Generate data
v0.9.0b1,Test data
v0.9.0b1,Remove warnings that might be raised by the models passed into the ORF
v0.9.0b1,Generate data with continuous treatments
v0.9.0b1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.9.0b1,does not work well with parallelism.
v0.9.0b1,Test inputs for continuous treatments
v0.9.0b1,--> Check that one can pass in regular lists
v0.9.0b1,--> Check that it fails correctly if lists of different shape are passed in
v0.9.0b1,Check that outputs have the correct shape
v0.9.0b1,Test continuous treatments with controls
v0.9.0b1,Test continuous treatments without controls
v0.9.0b1,Generate data with binary treatments
v0.9.0b1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.9.0b1,does not work well with parallelism.
v0.9.0b1,Test inputs for binary treatments
v0.9.0b1,--> Check that one can pass in regular lists
v0.9.0b1,--> Check that it fails correctly if lists of different shape are passed in
v0.9.0b1,"--> Check that it works when T, Y have shape (n, 1)"
v0.9.0b1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.9.0b1,--> Check that it fails correctly when the treatments are not numeric
v0.9.0b1,Check that outputs have the correct shape
v0.9.0b1,Test binary treatments with controls
v0.9.0b1,Test binary treatments without controls
v0.9.0b1,Only applicable to continuous treatments
v0.9.0b1,Generate data for 2 treatments
v0.9.0b1,Test multiple treatments with controls
v0.9.0b1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.9.0b1,The rest for controls. Just as an example.
v0.9.0b1,Generating A/B test data
v0.9.0b1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.9.0b1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.9.0b1,Create a wrapper around Lasso that doesn't support weights
v0.9.0b1,since Lasso does natively support them starting in sklearn 0.23
v0.9.0b1,Generate data with continuous treatments
v0.9.0b1,Instantiate model with most of the default parameters
v0.9.0b1,Compute the treatment effect on test points
v0.9.0b1,Compute treatment effect residuals
v0.9.0b1,Multiple treatments
v0.9.0b1,Allow at most 10% test points to be outside of the tolerance interval
v0.9.0b1,Compute treatment effect residuals
v0.9.0b1,Multiple treatments
v0.9.0b1,Allow at most 20% test points to be outside of the confidence interval
v0.9.0b1,Check that the intervals are not too wide
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.9.0b1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.9.0b1,ensure that we've got at least 6 of every element
v0.9.0b1,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.9.0b1,NOTE: this number may need to change if the default number of folds in
v0.9.0b1,WeightedStratifiedKFold changes
v0.9.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.0b1,ensure we can serialize the unfit estimator
v0.9.0b1,ensure we can pickle the fit estimator
v0.9.0b1,make sure we can call the marginal_effect and effect methods
v0.9.0b1,test const marginal inference
v0.9.0b1,test effect inference
v0.9.0b1,test marginal effect inference
v0.9.0b1,test coef__inference and intercept__inference
v0.9.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.0b1,"make sure we can call effect with implied scalar treatments,"
v0.9.0b1,"no matter the dimensions of T, and also that we warn when there"
v0.9.0b1,are multiple treatments
v0.9.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.9.0b1,ensure that we've got at least two of every element
v0.9.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.0b1,make sure we can call the marginal_effect and effect methods
v0.9.0b1,test const marginal inference
v0.9.0b1,test effect inference
v0.9.0b1,test marginal effect inference
v0.9.0b1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.9.0b1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.9.0b1,We concatenate the two copies data
v0.9.0b1,create a simple artificial setup where effect of moving from treatment
v0.9.0b1,"1 -> 2 is 2,"
v0.9.0b1,"1 -> 3 is 1, and"
v0.9.0b1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.9.0b1,"Using an uneven number of examples from different classes,"
v0.9.0b1,"and having the treatments in non-lexicographic order,"
v0.9.0b1,Should rule out some basic issues.
v0.9.0b1,test that we can fit with a KFold instance
v0.9.0b1,test that we can fit with a train/test iterable
v0.9.0b1,predetermined splits ensure that all features are seen in each split
v0.9.0b1,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.9.0b1,(incorrectly) use a final model with an intercept
v0.9.0b1,"Because final model is fixed, actual values of T and Y don't matter"
v0.9.0b1,Ensure reproducibility
v0.9.0b1,Sparse DGP
v0.9.0b1,Treatment effect coef
v0.9.0b1,Other coefs
v0.9.0b1,Features and controls
v0.9.0b1,Test sparse estimator
v0.9.0b1,"--> test coef_, intercept_"
v0.9.0b1,--> test treatment effects
v0.9.0b1,Restrict x_test to vectors of norm < 1
v0.9.0b1,--> check inference
v0.9.0b1,Check that a majority of true effects lie in the 5-95% CI
v0.9.0b1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.9.0b1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.9.0b1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.9.0b1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.9.0b1,sparse test case: heterogeneous effect by product
v0.9.0b1,need at least as many rows in e_y as there are distinct columns
v0.9.0b1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.9.0b1,create a simple artificial setup where effect of moving from treatment
v0.9.0b1,"a -> b is 2,"
v0.9.0b1,"a -> c is 1, and"
v0.9.0b1,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.9.0b1,"Using an uneven number of examples from different classes,"
v0.9.0b1,"and having the treatments in non-lexicographic order,"
v0.9.0b1,should rule out some basic issues.
v0.9.0b1,Note that explicitly specifying the dtype as object is necessary until
v0.9.0b1,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.9.0b1,estimated effects should be identical when treatment is explicitly given
v0.9.0b1,but const_marginal_effect should be reordered based on the explicit cagetories
v0.9.0b1,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.9.0b1,test outer grouping
v0.9.0b1,test nested grouping
v0.9.0b1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.9.0b1,whichever groups we saw
v0.9.0b1,test nested grouping
v0.9.0b1,"by default, we use 5 split cross-validation for our T and Y models"
v0.9.0b1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.9.0b1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Set random seed
v0.9.0b1,Generate data
v0.9.0b1,DGP constants
v0.9.0b1,Test data
v0.9.0b1,Constant treatment effect and propensity
v0.9.0b1,Heterogeneous treatment and propensity
v0.9.0b1,ensure that we've got at least two of every element
v0.9.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.9.0b1,ensure that we can serialize unfit estimator
v0.9.0b1,ensure that we can serialize fit estimator
v0.9.0b1,make sure we can call the marginal_effect and effect methods
v0.9.0b1,test const marginal inference
v0.9.0b1,test effect inference
v0.9.0b1,test marginal effect inference
v0.9.0b1,test coef_ and intercept_ inference
v0.9.0b1,verify we can generate the summary
v0.9.0b1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.9.0b1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.9.0b1,create a simple artificial setup where effect of moving from treatment
v0.9.0b1,"1 -> 2 is 2,"
v0.9.0b1,"1 -> 3 is 1, and"
v0.9.0b1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.9.0b1,"Using an uneven number of examples from different classes,"
v0.9.0b1,"and having the treatments in non-lexicographic order,"
v0.9.0b1,Should rule out some basic issues.
v0.9.0b1,test that we can fit with a KFold instance
v0.9.0b1,test that we can fit with a train/test iterable
v0.9.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.9.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.9.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.9.0b1,test coef__inference function works
v0.9.0b1,test intercept__inference function works
v0.9.0b1,test summary function works
v0.9.0b1,Test inputs
v0.9.0b1,self._test_inputs(DR_learner)
v0.9.0b1,Test constant treatment effect
v0.9.0b1,Test heterogeneous treatment effect
v0.9.0b1,Test heterogenous treatment effect for W =/= None
v0.9.0b1,Sparse DGP
v0.9.0b1,Treatment effect coef
v0.9.0b1,Other coefs
v0.9.0b1,Features and controls
v0.9.0b1,Test sparse estimator
v0.9.0b1,"--> test coef_, intercept_"
v0.9.0b1,--> test treatment effects
v0.9.0b1,Restrict x_test to vectors of norm < 1
v0.9.0b1,--> check inference
v0.9.0b1,Check that a majority of true effects lie in the 5-95% CI
v0.9.0b1,test outer grouping
v0.9.0b1,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.9.0b1,test nested grouping
v0.9.0b1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.9.0b1,whichever groups we saw
v0.9.0b1,test nested grouping
v0.9.0b1,"by default, we use 5 split cross-validation for our T and Y models"
v0.9.0b1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.9.0b1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.9.0b1,Fit learner and get the effect
v0.9.0b1,Get the true treatment effect
v0.9.0b1,Compute treatment effect residuals (absolute)
v0.9.0b1,Check that at least 90% of predictions are within tolerance interval
v0.9.0b1,Only for heterogeneous TE
v0.9.0b1,Fit learner on X and W and get the effect
v0.9.0b1,Get the true treatment effect
v0.9.0b1,Compute treatment effect residuals (absolute)
v0.9.0b1,Check that at least 90% of predictions are within tolerance interval
v0.9.0b1,Check that one can pass in regular lists
v0.9.0b1,Check that it fails correctly if lists of different shape are passed in
v0.9.0b1,Check that it fails when T contains values other than 0 and 1
v0.9.0b1,"Check that it works when T, Y have shape (n, 1)"
v0.9.0b1,Generate covariates
v0.9.0b1,Generate treatment
v0.9.0b1,Calculate outcome
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,DGP constants
v0.9.0b1,DGP coefficients
v0.9.0b1,Generated outcomes
v0.9.0b1,################
v0.9.0b1,WeightedLasso #
v0.9.0b1,################
v0.9.0b1,Define weights
v0.9.0b1,Define extended datasets
v0.9.0b1,Range of alphas
v0.9.0b1,Compare with Lasso
v0.9.0b1,--> No intercept
v0.9.0b1,--> With intercept
v0.9.0b1,When DGP has no intercept
v0.9.0b1,When DGP has intercept
v0.9.0b1,--> Coerce coefficients to be positive
v0.9.0b1,--> Toggle max_iter & tol
v0.9.0b1,Define weights
v0.9.0b1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.9.0b1,Mixed DGP scenario.
v0.9.0b1,Define extended datasets
v0.9.0b1,Define weights
v0.9.0b1,Define multioutput
v0.9.0b1,##################
v0.9.0b1,WeightedLassoCV #
v0.9.0b1,##################
v0.9.0b1,Define alphas to test
v0.9.0b1,Compare with LassoCV
v0.9.0b1,--> No intercept
v0.9.0b1,--> With intercept
v0.9.0b1,--> Force parameters to be positive
v0.9.0b1,Choose a smaller n to speed-up process
v0.9.0b1,Compare fold weights
v0.9.0b1,Define weights
v0.9.0b1,Define extended datasets
v0.9.0b1,Define splitters
v0.9.0b1,WeightedKFold splitter
v0.9.0b1,Map weighted splitter to an extended splitter
v0.9.0b1,Define alphas to test
v0.9.0b1,Compare with LassoCV
v0.9.0b1,--> No intercept
v0.9.0b1,--> With intercept
v0.9.0b1,--> Force parameters to be positive
v0.9.0b1,###########################
v0.9.0b1,MultiTaskWeightedLassoCV #
v0.9.0b1,###########################
v0.9.0b1,Define alphas to test
v0.9.0b1,Define splitter
v0.9.0b1,Compare with MultiTaskLassoCV
v0.9.0b1,--> No intercept
v0.9.0b1,--> With intercept
v0.9.0b1,Define weights
v0.9.0b1,Define extended datasets
v0.9.0b1,Define splitters
v0.9.0b1,WeightedKFold splitter
v0.9.0b1,Map weighted splitter to an extended splitter
v0.9.0b1,Define alphas to test
v0.9.0b1,Compare with LassoCV
v0.9.0b1,--> No intercept
v0.9.0b1,--> With intercept
v0.9.0b1,#########################
v0.9.0b1,WeightedLassoCVWrapper #
v0.9.0b1,#########################
v0.9.0b1,perform 1D fit
v0.9.0b1,perform 2D fit
v0.9.0b1,################
v0.9.0b1,DebiasedLasso #
v0.9.0b1,################
v0.9.0b1,Test DebiasedLasso without weights
v0.9.0b1,--> Check debiased coeffcients without intercept
v0.9.0b1,--> Check debiased coeffcients with intercept
v0.9.0b1,--> Check 5-95 CI coverage for unit vectors
v0.9.0b1,Test DebiasedLasso with weights for one DGP
v0.9.0b1,Define weights
v0.9.0b1,Define extended datasets
v0.9.0b1,--> Check debiased coefficients
v0.9.0b1,Define weights
v0.9.0b1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.9.0b1,--> Check debiased coeffcients
v0.9.0b1,Test that attributes propagate correctly
v0.9.0b1,Test MultiOutputDebiasedLasso without weights
v0.9.0b1,--> Check debiased coeffcients without intercept
v0.9.0b1,--> Check debiased coeffcients with intercept
v0.9.0b1,--> Check CI coverage
v0.9.0b1,Test MultiOutputDebiasedLasso with weights
v0.9.0b1,Define weights
v0.9.0b1,Define extended datasets
v0.9.0b1,--> Check debiased coefficients
v0.9.0b1,Unit vectors
v0.9.0b1,Unit vectors
v0.9.0b1,Check coeffcients and intercept are the same within tolerance
v0.9.0b1,Check results are similar with tolerance 1e-6
v0.9.0b1,Check if multitask
v0.9.0b1,Check that same alpha is chosen
v0.9.0b1,Check that the coefficients are similar
v0.9.0b1,selective ridge has a simple implementation that we can test against
v0.9.0b1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.9.0b1,"it should be the case that when we set fit_intercept to true,"
v0.9.0b1,it doesn't matter whether the penalized model also fits an intercept or not
v0.9.0b1,create an extra copy of rows with weight 2
v0.9.0b1,"instead of a slice, explicitly return an array of indices"
v0.9.0b1,_penalized_inds is only set during fitting
v0.9.0b1,cv exists on penalized model
v0.9.0b1,now we can access _penalized_inds
v0.9.0b1,check that we can read the cv attribute back out from the underlying model
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Linear models are required for parametric dml
v0.9.0b1,sample weighting models are required for nonparametric dml
v0.9.0b1,Test values
v0.9.0b1,TLearner test
v0.9.0b1,Instantiate TLearner
v0.9.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.9.0b1,Test constant treatment effect with multi output Y
v0.9.0b1,Test heterogeneous treatment effect
v0.9.0b1,Need interactions between T and features
v0.9.0b1,Test heterogeneous treatment effect with multi output Y
v0.9.0b1,Instantiate DomainAdaptationLearner
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,test base values equals to mean of constant marginal effect
v0.9.0b1,test shape of shap values output is as expected
v0.9.0b1,test shape of attribute of explanation object is as expected
v0.9.0b1,test base values equals to mean of constant marginal effect
v0.9.0b1,test shape of shap values output is as expected
v0.9.0b1,test shape of attribute of explanation object is as expected
v0.9.0b1,Treatment effect function
v0.9.0b1,Outcome support
v0.9.0b1,Treatment support
v0.9.0b1,"Generate controls, covariates, treatments and outcomes"
v0.9.0b1,Heterogeneous treatment effects
v0.9.0b1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
v0.9.0b1,through shap package.
v0.9.0b1,test shap could generate the plot from the shap_values
v0.9.0b1,Re-raise with more informative error message instead:
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Check inputs
v0.9.0b1,Check inputs
v0.9.0b1,"Note: unlike other Metalearners, we don't drop the first column because"
v0.9.0b1,we concatenate all treatments to the other features;
v0.9.0b1,"We might want to revisit, though, since it's linearly determined by the others"
v0.9.0b1,Check inputs
v0.9.0b1,Check inputs
v0.9.0b1,Check inputs
v0.9.0b1,Estimate response function
v0.9.0b1,Check inputs
v0.9.0b1,Train model on controls. Assign higher weight to units resembling
v0.9.0b1,treated units.
v0.9.0b1,Train model on the treated. Assign higher weight to units resembling
v0.9.0b1,control units.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.9.0b1,output is
v0.9.0b1,"* a column of ones if X, W, and Z are all None"
v0.9.0b1,* just X or W or Z if both of the others are None
v0.9.0b1,* hstack([arrs]) for whatever subset are not None otherwise
v0.9.0b1,ensure Z is 2D
v0.9.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.0b1,We need to go back to the label representation of the one-hot so as to call
v0.9.0b1,the classifier.
v0.9.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.9.0b1,We need to go back to the label representation of the one-hot so as to call
v0.9.0b1,the classifier.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,TODO: make sure to use random seeds wherever necessary
v0.9.0b1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.9.0b1,"unfortunately with the Theano and Tensorflow backends,"
v0.9.0b1,the straightforward use of K.stop_gradient can cause an error
v0.9.0b1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.9.0b1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.9.0b1,so that those layers remain connected but with 0 gradient
v0.9.0b1,|| t - mu_i || ^2
v0.9.0b1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.9.0b1,Use logsumexp for numeric stability:
v0.9.0b1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.9.0b1,TODO: does the numeric stability actually make any difference?
v0.9.0b1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.9.0b1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.9.0b1,generate cumulative sum via matrix multiplication
v0.9.0b1,"Generate standard uniform values in shape (batch_size,1)"
v0.9.0b1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.9.0b1,we use uniform_like instead with an input of an appropriate shape)
v0.9.0b1,convert to floats and multiply to perform equivalent of logical AND
v0.9.0b1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.9.0b1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.9.0b1,we use normal_like instead with an input of an appropriate shape)
v0.9.0b1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.9.0b1,prevent gradient from passing through sampling
v0.9.0b1,three options: biased or upper-bound loss require a single number of samples;
v0.9.0b1,unbiased can take different numbers for the network and its gradient
v0.9.0b1,"sample: (() -> Layer, int) -> Layer"
v0.9.0b1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.9.0b1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.9.0b1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.9.0b1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.9.0b1,the dimensionality of the output of the network
v0.9.0b1,TODO: is there a more robust way to do this?
v0.9.0b1,TODO: do we need to give the user more control over other arguments to fit?
v0.9.0b1,"subtle point: we need to build a new model each time,"
v0.9.0b1,because each model encapsulates its randomness
v0.9.0b1,TODO: do we need to give the user more control over other arguments to fit?
v0.9.0b1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.9.0b1,not a general tensor (because of how backprop works in every framework)
v0.9.0b1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.9.0b1,but this seems annoying...)
v0.9.0b1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.9.0b1,TODO: any way to get this to work on batches of arbitrary size?
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Estimate final model of theta(X) by minimizing the square loss:
v0.9.0b1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.9.0b1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.9.0b1,at the expense of some small bias. For points with very small covariance we revert
v0.9.0b1,to the model-based preliminary estimate and do not add the correction term.
v0.9.0b1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.9.0b1,"instruments, and outcomes"
v0.9.0b1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.9.0b1,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.9.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.0b1,for internal use by the library
v0.9.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.0b1,"we need to undo the one-hot encoding for calling effect,"
v0.9.0b1,since it expects raw values
v0.9.0b1,"we need to undo the one-hot encoding for calling effect,"
v0.9.0b1,since it expects raw values
v0.9.0b1,"TODO: check that Y, T, Z do not have multiple columns"
v0.9.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.9.0b1,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.9.0b1,since we would actually be calculating a CATE rather than ATE.
v0.9.0b1,TODO: allow the final model to actually use X?
v0.9.0b1,TODO: allow the final model to actually use X?
v0.9.0b1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.0b1,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.9.0b1,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.9.0b1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.0b1,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.9.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.9.0b1,for internal use by the library
v0.9.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,"this will have dimension (d,) + shape(X)"
v0.9.0b1,send the first dimension to the end
v0.9.0b1,columns are featurized independently; partial derivatives are only non-zero
v0.9.0b1,when taken with respect to the same column each time
v0.9.0b1,don't fit intercept; manually add column of ones to the data instead;
v0.9.0b1,this allows us to ignore the intercept when computing marginal effects
v0.9.0b1,make T 2D if if was a vector
v0.9.0b1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.9.0b1,two stage approximation
v0.9.0b1,"first, get basis expansions of T, X, and Z"
v0.9.0b1,TODO: is it right that the effective number of intruments is the
v0.9.0b1,"product of ft_X and ft_Z, not just ft_Z?"
v0.9.0b1,"regress T expansion on X,Z expansions concatenated with W"
v0.9.0b1,"predict ft_T from interacted ft_X, ft_Z"
v0.9.0b1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.9.0b1,dT may be only 2-dimensional)
v0.9.0b1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.9.0b1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,TODO: conisder working around relying on sklearn implementation details
v0.9.0b1,"Found a good split, return."
v0.9.0b1,Record all splits in case the stratification by weight yeilds a worse partition
v0.9.0b1,Reseed random generator and try again
v0.9.0b1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.9.0b1,"Found a good split, return."
v0.9.0b1,Did not find a good split
v0.9.0b1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.9.0b1,Return most weight-balanced partition
v0.9.0b1,Weight stratification algorithm
v0.9.0b1,Sort weights for weight strata search
v0.9.0b1,There are some leftover indices that have yet to be assigned
v0.9.0b1,Append stratum splits to overall splits
v0.9.0b1,"If classification methods produce multiple columns of output,"
v0.9.0b1,we need to manually encode classes to ensure consistent column ordering.
v0.9.0b1,We clone the estimator to make sure that all the folds are
v0.9.0b1,"independent, and that it is pickle-able."
v0.9.0b1,TODO. The API of the private scikit-learn `_fit_and_predict` has changed
v0.9.0b1,"between 0.23.2 and 0.24. For this to work with <0.24, we need to add a"
v0.9.0b1,case analysis based on sklearn version.
v0.9.0b1,`predictions` is a list of method outputs from each fold.
v0.9.0b1,"If each of those is also a list, then treat this as a"
v0.9.0b1,multioutput-multiclass task. We need to separately concatenate
v0.9.0b1,the method outputs for each label into an `n_labels` long list.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Our classes that derive from sklearn ones sometimes include
v0.9.0b1,inherited docstrings that have embedded doctests; we need the following imports
v0.9.0b1,so that they don't break.
v0.9.0b1,TODO: consider working around relying on sklearn implementation details
v0.9.0b1,"Convert X, y into numpy arrays"
v0.9.0b1,Define fit parameters
v0.9.0b1,Some algorithms don't have a check_input option
v0.9.0b1,Check weights array
v0.9.0b1,Check that weights are size-compatible
v0.9.0b1,Normalize inputs
v0.9.0b1,Weight inputs
v0.9.0b1,Fit base class without intercept
v0.9.0b1,Fit Lasso
v0.9.0b1,Reset intercept
v0.9.0b1,The intercept is not calculated properly due the sqrt(weights) factor
v0.9.0b1,so it must be recomputed
v0.9.0b1,Fit lasso without weights
v0.9.0b1,Make weighted splitter
v0.9.0b1,Fit weighted model
v0.9.0b1,Make weighted splitter
v0.9.0b1,Fit weighted model
v0.9.0b1,Call weighted lasso on reduced design matrix
v0.9.0b1,Weighted tau
v0.9.0b1,Select optimal penalty
v0.9.0b1,Warn about consistency
v0.9.0b1,"Convert X, y into numpy arrays"
v0.9.0b1,Fit weighted lasso with user input
v0.9.0b1,"Center X, y"
v0.9.0b1,Calculate quantities that will be used later on. Account for centered data
v0.9.0b1,Calculate coefficient and error variance
v0.9.0b1,Add coefficient correction
v0.9.0b1,Set coefficients and intercept standard errors
v0.9.0b1,Set intercept
v0.9.0b1,Return alpha to 'auto' state
v0.9.0b1,"Note that in the case of no intercept, X_offset is 0"
v0.9.0b1,Calculate the variance of the predictions
v0.9.0b1,Calculate prediction confidence intervals
v0.9.0b1,Assumes flattened y
v0.9.0b1,Compute weighted residuals
v0.9.0b1,To be done once per target. Assumes y can be flattened.
v0.9.0b1,Assumes that X has already been offset
v0.9.0b1,Special case: n_features=1
v0.9.0b1,Compute Lasso coefficients for the columns of the design matrix
v0.9.0b1,Compute C_hat
v0.9.0b1,Compute theta_hat
v0.9.0b1,Allow for single output as well
v0.9.0b1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.9.0b1,Set coef_ attribute
v0.9.0b1,Set intercept_ attribute
v0.9.0b1,Set selected_alpha_ attribute
v0.9.0b1,Set coef_stderr_
v0.9.0b1,intercept_stderr_
v0.9.0b1,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.9.0b1,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.9.0b1,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.9.0b1,set intercept_ attribute
v0.9.0b1,set coef_ attribute
v0.9.0b1,set alpha_ attribute
v0.9.0b1,set alphas_ attribute
v0.9.0b1,set n_iter_ attribute
v0.9.0b1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.9.0b1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.9.0b1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.9.0b1,now regress X1 on y - X2 * beta2 to learn beta1
v0.9.0b1,set coef_ and intercept_ attributes
v0.9.0b1,Note that the penalized model should *not* have an intercept
v0.9.0b1,don't proxy special methods
v0.9.0b1,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.9.0b1,regressor incorrectly
v0.9.0b1,"Note: for known attributes that have been set this method will not be called,"
v0.9.0b1,so we should just throw here because this is an attribute belonging to this class
v0.9.0b1,but which hasn't yet been set on this instance
v0.9.0b1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,AzureML
v0.9.0b1,helper imports
v0.9.0b1,write the details of the workspace to a configuration file to the notebook library
v0.9.0b1,if y is a multioutput model
v0.9.0b1,Make sure second dimension has 1 or more item
v0.9.0b1,switch _inner Model to a MultiOutputRegressor
v0.9.0b1,flatten array as automl only takes vectors for y
v0.9.0b1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.9.0b1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.9.0b1,as an sklearn estimator
v0.9.0b1,fit implementation for a single output model.
v0.9.0b1,Create experiment for specified workspace
v0.9.0b1,Configure automl_config with training set information.
v0.9.0b1,"Wait for remote run to complete, the set the model"
v0.9.0b1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.9.0b1,create model and pass model into final.
v0.9.0b1,"If item is an automl config, get its corresponding"
v0.9.0b1,AutomatedML Model and add it to new_Args
v0.9.0b1,"If item is an automl config, get its corresponding"
v0.9.0b1,AutomatedML Model and set it for this key in
v0.9.0b1,kwargs
v0.9.0b1,takes in either automated_ml config and instantiates
v0.9.0b1,an AutomatedMLModel
v0.9.0b1,The prefix can only be 18 characters long
v0.9.0b1,"because prefixes come from kwarg_names, we must ensure they are"
v0.9.0b1,short enough.
v0.9.0b1,Get workspace from config file.
v0.9.0b1,Take the intersect of the white for sample
v0.9.0b1,weights and linear models
v0.9.0b1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,TODO: generalize to multiple treatment case?
v0.9.0b1,get index of best treatment
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.9.0b1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.9.0b1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.9.0b1,clean way of achieving this
v0.9.0b1,make sure we don't accidentally escape anything in the substitution
v0.9.0b1,Fetch appropriate color for node
v0.9.0b1,"red for negative, green for positive"
v0.9.0b1,in multi-target use first target
v0.9.0b1,Write node mean CATE
v0.9.0b1,Write node std of CATE
v0.9.0b1,Fetch appropriate color for node
v0.9.0b1,"red for negative, green for positive"
v0.9.0b1,Write node mean CATE
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,TODO: consider working around relying on sklearn implementation details
v0.9.0b1,Create splits of causal tree
v0.9.0b1,Make sure the correct exception is being rethrown
v0.9.0b1,Must make sure indices are merged correctly
v0.9.0b1,Convert rows to columns
v0.9.0b1,Require group assignment t to be one-hot-encoded
v0.9.0b1,Get predictions for the 2 splits
v0.9.0b1,Must make sure indices are merged correctly
v0.9.0b1,Crossfitting
v0.9.0b1,Compute weighted nuisance estimates
v0.9.0b1,-------------------------------------------------------------------------------
v0.9.0b1,Calculate the covariance matrix corresponding to the BLB inference
v0.9.0b1,
v0.9.0b1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.9.0b1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.9.0b1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.9.0b1,in that slice from the overall parameter estimate.
v0.9.0b1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.9.0b1,-------------------------------------------------------------------------------
v0.9.0b1,Calclulate covariance matrix through BLB
v0.9.0b1,Estimators
v0.9.0b1,OrthoForest parameters
v0.9.0b1,Sub-forests
v0.9.0b1,Auxiliary attributes
v0.9.0b1,Fit check
v0.9.0b1,TODO: Check performance
v0.9.0b1,Must normalize weights
v0.9.0b1,Override the CATE inference options
v0.9.0b1,Add blb inference to parent's options
v0.9.0b1,Generate subsample indices
v0.9.0b1,Build trees in parallel
v0.9.0b1,Bootstraping has repetitions in tree sample
v0.9.0b1,Similar for `a` weights
v0.9.0b1,Bootstraping has repetitions in tree sample
v0.9.0b1,Define subsample size
v0.9.0b1,Safety check
v0.9.0b1,Draw points to create little bags
v0.9.0b1,Copy and/or define models
v0.9.0b1,Define nuisance estimators
v0.9.0b1,Define parameter estimators
v0.9.0b1,Define
v0.9.0b1,Need to redefine fit here for auto inference to work due to a quirk in how
v0.9.0b1,wrap_fit is defined
v0.9.0b1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.9.0b1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.9.0b1,Override to flatten output if T is flat
v0.9.0b1,Check that all discrete treatments are represented
v0.9.0b1,Nuissance estimates evaluated with cross-fitting
v0.9.0b1,Define 2-fold iterator
v0.9.0b1,Check if there is only one example of some class
v0.9.0b1,Define 2-fold iterator
v0.9.0b1,need safe=False when cloning for WeightedModelWrapper
v0.9.0b1,Compute residuals
v0.9.0b1,Compute coefficient by OLS on residuals
v0.9.0b1,"Parameter returned by LinearRegression is (d_T, )"
v0.9.0b1,Compute residuals
v0.9.0b1,Compute coefficient by OLS on residuals
v0.9.0b1,ell_2 regularization
v0.9.0b1,Ridge regression estimate
v0.9.0b1,"Parameter returned is of shape (d_T, )"
v0.9.0b1,Return moments and gradients
v0.9.0b1,Compute residuals
v0.9.0b1,Compute moments
v0.9.0b1,"Moments shape is (n, d_T)"
v0.9.0b1,Compute moment gradients
v0.9.0b1,returns shape-conforming residuals
v0.9.0b1,Copy and/or define models
v0.9.0b1,Define parameter estimators
v0.9.0b1,Define moment and mean gradient estimator
v0.9.0b1,"Check that T is shape (n, )"
v0.9.0b1,Check T is numeric
v0.9.0b1,Train label encoder
v0.9.0b1,Call `fit` from parent class
v0.9.0b1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.9.0b1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.9.0b1,Override to flatten output if T is flat
v0.9.0b1,Expand one-hot encoding to include the zero treatment
v0.9.0b1,"Test that T contains all treatments. If not, return None"
v0.9.0b1,Nuissance estimates evaluated with cross-fitting
v0.9.0b1,Define 2-fold iterator
v0.9.0b1,Check if there is only one example of some class
v0.9.0b1,No need to crossfit for internal nodes
v0.9.0b1,Compute partial moments
v0.9.0b1,"If any of the values in the parameter estimate is nan, return None"
v0.9.0b1,Compute partial moments
v0.9.0b1,Compute coefficient by OLS on residuals
v0.9.0b1,ell_2 regularization
v0.9.0b1,Ridge regression estimate
v0.9.0b1,"Parameter returned is of shape (d_T, )"
v0.9.0b1,Return moments and gradients
v0.9.0b1,Compute partial moments
v0.9.0b1,Compute moments
v0.9.0b1,"Moments shape is (n, d_T-1)"
v0.9.0b1,Compute moment gradients
v0.9.0b1,Need to calculate this in an elegant way for when propensity is 0
v0.9.0b1,This will flatten T
v0.9.0b1,Check that T is numeric
v0.9.0b1,Test whether the input estimator is supported
v0.9.0b1,Calculate confidence intervals for the parameter (marginal effect)
v0.9.0b1,Calculate confidence intervals for the effect
v0.9.0b1,Calculate the effects
v0.9.0b1,Calculate the standard deviations for the effects
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.9.0b1,Licensed under the MIT License.
v0.9.0b1,Causal tree parameters
v0.9.0b1,Tree structure
v0.9.0b1,No need for a random split since the data is already
v0.9.0b1,a random subsample from the original input
v0.9.0b1,node list stores the nodes that are yet to be splitted
v0.9.0b1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.9.0b1,Create local sample set
v0.9.0b1,Compute nuisance estimates for the current node
v0.9.0b1,Nuisance estimate cannot be calculated
v0.9.0b1,Estimate parameter for current node
v0.9.0b1,Node estimate cannot be calculated
v0.9.0b1,Calculate moments and gradient of moments for current data
v0.9.0b1,Calculate inverse gradient
v0.9.0b1,The gradient matrix is not invertible.
v0.9.0b1,No good split can be found
v0.9.0b1,Calculate point-wise pseudo-outcomes rho
v0.9.0b1,a split is determined by a feature and a sample pair
v0.9.0b1,the number of possible splits is at most (number of features) * (number of node samples)
v0.9.0b1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.9.0b1,parse row and column of random pair
v0.9.0b1,the sample of the pair is the integer division of the random number with n_feats
v0.9.0b1,calculate the binary indicator of whether sample i is on the left or the right
v0.9.0b1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.9.0b1,calculate the number of samples on the left child for each proposed split
v0.9.0b1,calculate the analogous binary indicator for the samples in the estimation set
v0.9.0b1,calculate the number of estimation samples on the left child of each proposed split
v0.9.0b1,find the upper and lower bound on the size of the left split for the split
v0.9.0b1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.9.0b1,on each side.
v0.9.0b1,similarly for the estimation sample set
v0.9.0b1,if there is no valid split then don't create any children
v0.9.0b1,filter only the valid splits
v0.9.0b1,calculate the average influence vector of the samples in the left child
v0.9.0b1,calculate the average influence vector of the samples in the right child
v0.9.0b1,take the square of each of the entries of the influence vectors and normalize
v0.9.0b1,by size of each child
v0.9.0b1,calculate the vector score of each candidate split as the average of left and right
v0.9.0b1,influence vectors
v0.9.0b1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.9.0b1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.9.0b1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.9.0b1,calculate the scalar score of each split by aggregating across the vector of scores
v0.9.0b1,Find split that minimizes criterion
v0.9.0b1,Create child nodes with corresponding subsamples
v0.9.0b1,add the created children to the list of not yet split nodes
v0.8.1,configuration is all pulled from setup.cfg
v0.8.1,-*- coding: utf-8 -*-
v0.8.1,
v0.8.1,Configuration file for the Sphinx documentation builder.
v0.8.1,
v0.8.1,This file does only contain a selection of the most common options. For a
v0.8.1,full list see the documentation:
v0.8.1,http://www.sphinx-doc.org/en/master/config
v0.8.1,-- Path setup --------------------------------------------------------------
v0.8.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.8.1,add these directories to sys.path here. If the directory is relative to the
v0.8.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.8.1,
v0.8.1,-- Project information -----------------------------------------------------
v0.8.1,-- General configuration ---------------------------------------------------
v0.8.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.8.1,
v0.8.1,needs_sphinx = '1.0'
v0.8.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.8.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.8.1,ones.
v0.8.1,"Add any paths that contain templates here, relative to this directory."
v0.8.1,The suffix(es) of source filenames.
v0.8.1,You can specify multiple suffix as a list of string:
v0.8.1,
v0.8.1,"source_suffix = ['.rst', '.md']"
v0.8.1,The master toctree document.
v0.8.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.8.1,for a list of supported languages.
v0.8.1,
v0.8.1,This is also used if you do content translation via gettext catalogs.
v0.8.1,"Usually you set ""language"" from the command line for these cases."
v0.8.1,"List of patterns, relative to source directory, that match files and"
v0.8.1,directories to ignore when looking for source files.
v0.8.1,This pattern also affects html_static_path and html_extra_path.
v0.8.1,The name of the Pygments (syntax highlighting) style to use.
v0.8.1,-- Options for HTML output -------------------------------------------------
v0.8.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.8.1,a list of builtin themes.
v0.8.1,
v0.8.1,Theme options are theme-specific and customize the look and feel of a theme
v0.8.1,"further.  For a list of options available for each theme, see the"
v0.8.1,documentation.
v0.8.1,
v0.8.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.8.1,"relative to this directory. They are copied after the builtin static files,"
v0.8.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.8.1,html_static_path = ['_static']
v0.8.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.8.1,to template names.
v0.8.1,
v0.8.1,The default sidebars (for documents that don't match any pattern) are
v0.8.1,defined by theme itself.  Builtin themes are using these templates by
v0.8.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.8.1,'searchbox.html']``.
v0.8.1,
v0.8.1,html_sidebars = {}
v0.8.1,-- Options for HTMLHelp output ---------------------------------------------
v0.8.1,Output file base name for HTML help builder.
v0.8.1,-- Options for LaTeX output ------------------------------------------------
v0.8.1,The paper size ('letterpaper' or 'a4paper').
v0.8.1,
v0.8.1,"'papersize': 'letterpaper',"
v0.8.1,"The font size ('10pt', '11pt' or '12pt')."
v0.8.1,
v0.8.1,"'pointsize': '10pt',"
v0.8.1,Additional stuff for the LaTeX preamble.
v0.8.1,
v0.8.1,"'preamble': '',"
v0.8.1,Latex figure (float) alignment
v0.8.1,
v0.8.1,"'figure_align': 'htbp',"
v0.8.1,Grouping the document tree into LaTeX files. List of tuples
v0.8.1,"(source start file, target name, title,"
v0.8.1,"author, documentclass [howto, manual, or own class])."
v0.8.1,-- Options for manual page output ------------------------------------------
v0.8.1,One entry per manual page. List of tuples
v0.8.1,"(source start file, name, description, authors, manual section)."
v0.8.1,-- Options for Texinfo output ----------------------------------------------
v0.8.1,Grouping the document tree into Texinfo files. List of tuples
v0.8.1,"(source start file, target name, title, author,"
v0.8.1,"dir menu entry, description, category)"
v0.8.1,-- Options for Epub output -------------------------------------------------
v0.8.1,Bibliographic Dublin Core info.
v0.8.1,The unique identifier of the text. This can be a ISBN number
v0.8.1,or the project homepage.
v0.8.1,
v0.8.1,epub_identifier = ''
v0.8.1,A unique identification for the text.
v0.8.1,
v0.8.1,epub_uid = ''
v0.8.1,A list of files that should not be packed into the epub file.
v0.8.1,-- Extension configuration -------------------------------------------------
v0.8.1,-- Options for intersphinx extension ---------------------------------------
v0.8.1,Example configuration for intersphinx: refer to the Python standard library.
v0.8.1,-- Options for todo extension ----------------------------------------------
v0.8.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.8.1,-- Options for doctest extension -------------------------------------------
v0.8.1,we can document otherwise excluded entities here by returning False
v0.8.1,or skip otherwise included entities by returning True
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Calculate residuals
v0.8.1,Estimate E[T_res | Z_res]
v0.8.1,TODO. Deal with multi-class instrument
v0.8.1,Calculate nuisances
v0.8.1,Estimate E[T_res | Z_res]
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.8.1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.8.1,TODO. Deal with multi-class instrument
v0.8.1,Estimate final model of theta(X) by minimizing the square loss:
v0.8.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.8.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.8.1,at the expense of some small bias. For points with very small covariance we revert
v0.8.1,to the model-based preliminary estimate and do not add the correction term.
v0.8.1,Estimate preliminary theta in cross fitting manner
v0.8.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.8.1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.8.1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.8.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.8.1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.8.1,TODO. The solution below is not really a valid cross-fitting
v0.8.1,as the test data are used to create the proj_t on the train
v0.8.1,which in the second train-test loop is used to create the nuisance
v0.8.1,cov on the test data. Hence the T variable of some sample
v0.8.1,"is implicitly correlated with its cov nuisance, through this flow"
v0.8.1,"of information. However, this seems a rather weak correlation."
v0.8.1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.8.1,model.
v0.8.1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.8.1,Estimate preliminary theta in cross fitting manner
v0.8.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.8.1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.8.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.8.1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.8.1,#############################################################################
v0.8.1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.8.1,A/B test
v0.8.1,#############################################################################
v0.8.1,Estimate preliminary theta in cross fitting manner
v0.8.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.8.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.8.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.8.1,We can use statsmodel for all hypothesis testing capabilities
v0.8.1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.8.1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.8.1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.8.1,model_T_XZ = lambda: model_clf()
v0.8.1,#'days_visited': lambda:
v0.8.1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.8.1,Turn strings into categories for numeric mapping
v0.8.1,### Defining some generic regressors and classifiers
v0.8.1,This a generic non-parametric regressor
v0.8.1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.8.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.8.1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.8.1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.8.1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.8.1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.8.1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.8.1,model = lambda: LinearRegression(n_jobs=-1)
v0.8.1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.8.1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.8.1,underlying model whenever predict is called.
v0.8.1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.8.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.8.1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.8.1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.8.1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.8.1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.8.1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.8.1,We need to specify models to be used for each of these residualizations
v0.8.1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.8.1,"E[T | X, Z]"
v0.8.1,E[TZ | X]
v0.8.1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.8.1,n_splits determines the number of splits to be used for cross-fitting.
v0.8.1,# Algorithm 2 - Current Method
v0.8.1,In[121]:
v0.8.1,# Algorithm 3 - DRIV ATE
v0.8.1,dmliv_model_effect = lambda: model()
v0.8.1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.8.1,"dmliv_model_effect(),"
v0.8.1,n_splits=1)
v0.8.1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.8.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.8.1,"Once multiple treatments are supported, we'll need to fix this"
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.8.1,We can use statsmodel for all hypothesis testing capabilities
v0.8.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.8.1,We can use statsmodel for all hypothesis testing capabilities
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,TODO. Deal with multi-class instrument/treatment
v0.8.1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.8.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.8.1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.8.1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.8.1,##################
v0.8.1,Global settings #
v0.8.1,##################
v0.8.1,Global plotting controls
v0.8.1,"Control for support size, can control for more"
v0.8.1,#################
v0.8.1,File utilities #
v0.8.1,#################
v0.8.1,#################
v0.8.1,Plotting utils #
v0.8.1,#################
v0.8.1,bias
v0.8.1,var
v0.8.1,rmse
v0.8.1,r2
v0.8.1,Infer feature dimension
v0.8.1,Metrics by support plots
v0.8.1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.8.1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.8.1,Steven Wu <zhiww@microsoft.com>
v0.8.1,Initialize causal tree parameters
v0.8.1,Create splits of causal tree
v0.8.1,Estimate treatment effects at the leafs
v0.8.1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.8.1,the corresponding split and associating the effect computed on that leaf
v0.8.1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.8.1,Safety check
v0.8.1,Weighted linear regression
v0.8.1,Calculates weights
v0.8.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.8.1,over all indices
v0.8.1,Similar for `a` weights
v0.8.1,Doesn't have sample weights
v0.8.1,Is a linear model
v0.8.1,Weighted linear regression
v0.8.1,Calculates weights
v0.8.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.8.1,over all indices
v0.8.1,Similar for `a` weights
v0.8.1,normalize weights
v0.8.1,"Split the data in half, train and test"
v0.8.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.8.1,"a function of W, using only the train fold"
v0.8.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.8.1,"Split the data in half, train and test"
v0.8.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.8.1,"a function of W, using only the train fold"
v0.8.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.8.1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.8.1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.8.1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.8.1,"Split the data in half, train and test"
v0.8.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.8.1,"a function of x, using only the train fold"
v0.8.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.8.1,Compute coefficient by OLS on residuals
v0.8.1,"Split the data in half, train and test"
v0.8.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.8.1,"a function of x, using only the train fold"
v0.8.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.8.1,Estimate multipliers for second order orthogonal method
v0.8.1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.8.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.8.1,Create local sample set
v0.8.1,compute the base estimate for the current node using double ml or second order double ml
v0.8.1,compute the influence functions here that are used for the criterion
v0.8.1,generate random proposals of dimensions to split
v0.8.1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.8.1,compute criterion for each proposal
v0.8.1,if splitting creates valid leafs in terms of mean leaf size
v0.8.1,Calculate criterion for split
v0.8.1,Else set criterion to infinity so that this split is not chosen
v0.8.1,If no good split was found
v0.8.1,Find split that minimizes criterion
v0.8.1,Set the split attributes at the node
v0.8.1,Create child nodes with corresponding subsamples
v0.8.1,Recursively split children
v0.8.1,Return parent node
v0.8.1,estimate the local parameter at the leaf using the estimate data
v0.8.1,###################
v0.8.1,Argument parsing #
v0.8.1,###################
v0.8.1,#########################################
v0.8.1,Parameters constant across experiments #
v0.8.1,#########################################
v0.8.1,Outcome support
v0.8.1,Treatment support
v0.8.1,Evaluation grid
v0.8.1,Treatment effects array
v0.8.1,Other variables
v0.8.1,##########################
v0.8.1,Data Generating Process #
v0.8.1,##########################
v0.8.1,Log iteration
v0.8.1,"Generate controls, features, treatment and outcome"
v0.8.1,T and Y residuals to be used in later scripts
v0.8.1,Save generated dataset
v0.8.1,#################
v0.8.1,ORF parameters #
v0.8.1,#################
v0.8.1,######################################
v0.8.1,Train and evaluate treatment effect #
v0.8.1,######################################
v0.8.1,########
v0.8.1,Plots #
v0.8.1,########
v0.8.1,###############
v0.8.1,Save results #
v0.8.1,###############
v0.8.1,##############
v0.8.1,Run Rscript #
v0.8.1,##############
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Check inputs
v0.8.1,Check inputs
v0.8.1,"Note: unlike other Metalearners, we don't drop the first column because"
v0.8.1,we concatenate all treatments to the other features;
v0.8.1,"We might want to revisit, though, since it's linearly determined by the others"
v0.8.1,Check inputs
v0.8.1,Check inputs
v0.8.1,Check inputs
v0.8.1,Estimate response function
v0.8.1,Check inputs
v0.8.1,Train model on controls. Assign higher weight to units resembling
v0.8.1,treated units.
v0.8.1,Train model on the treated. Assign higher weight to units resembling
v0.8.1,control units.
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,TODO: consider working around relying on sklearn implementation details
v0.8.1,Create splits of causal tree
v0.8.1,Make sure the correct exception is being rethrown
v0.8.1,Must make sure indices are merged correctly
v0.8.1,Convert rows to columns
v0.8.1,Require group assignment t to be one-hot-encoded
v0.8.1,Get predictions for the 2 splits
v0.8.1,Must make sure indices are merged correctly
v0.8.1,Crossfitting
v0.8.1,Compute weighted nuisance estimates
v0.8.1,-------------------------------------------------------------------------------
v0.8.1,Calculate the covariance matrix corresponding to the BLB inference
v0.8.1,
v0.8.1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.8.1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.8.1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.8.1,in that slice from the overall parameter estimate.
v0.8.1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.8.1,-------------------------------------------------------------------------------
v0.8.1,Calclulate covariance matrix through BLB
v0.8.1,Estimators
v0.8.1,OrthoForest parameters
v0.8.1,Sub-forests
v0.8.1,Auxiliary attributes
v0.8.1,Fit check
v0.8.1,TODO: Check performance
v0.8.1,Must normalize weights
v0.8.1,Override the CATE inference options
v0.8.1,Add blb inference to parent's options
v0.8.1,Generate subsample indices
v0.8.1,Build trees in parallel
v0.8.1,Bootstraping has repetitions in tree sample
v0.8.1,Similar for `a` weights
v0.8.1,Bootstraping has repetitions in tree sample
v0.8.1,Define subsample size
v0.8.1,Safety check
v0.8.1,Draw points to create little bags
v0.8.1,Copy and/or define models
v0.8.1,Define nuisance estimators
v0.8.1,Define parameter estimators
v0.8.1,Define
v0.8.1,Need to redefine fit here for auto inference to work due to a quirk in how
v0.8.1,wrap_fit is defined
v0.8.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.8.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.8.1,Override to flatten output if T is flat
v0.8.1,Check that all discrete treatments are represented
v0.8.1,Nuissance estimates evaluated with cross-fitting
v0.8.1,Define 2-fold iterator
v0.8.1,Check if there is only one example of some class
v0.8.1,Define 2-fold iterator
v0.8.1,need safe=False when cloning for WeightedModelWrapper
v0.8.1,Compute residuals
v0.8.1,Compute coefficient by OLS on residuals
v0.8.1,"Parameter returned by LinearRegression is (d_T, )"
v0.8.1,Compute residuals
v0.8.1,Compute coefficient by OLS on residuals
v0.8.1,ell_2 regularization
v0.8.1,Ridge regression estimate
v0.8.1,"Parameter returned is of shape (d_T, )"
v0.8.1,Return moments and gradients
v0.8.1,Compute residuals
v0.8.1,Compute moments
v0.8.1,"Moments shape is (n, d_T)"
v0.8.1,Compute moment gradients
v0.8.1,returns shape-conforming residuals
v0.8.1,Copy and/or define models
v0.8.1,Define parameter estimators
v0.8.1,Define moment and mean gradient estimator
v0.8.1,"Check that T is shape (n, )"
v0.8.1,Check T is numeric
v0.8.1,Train label encoder
v0.8.1,Call `fit` from parent class
v0.8.1,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.8.1,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.8.1,Override to flatten output if T is flat
v0.8.1,Expand one-hot encoding to include the zero treatment
v0.8.1,"Test that T contains all treatments. If not, return None"
v0.8.1,Nuissance estimates evaluated with cross-fitting
v0.8.1,Define 2-fold iterator
v0.8.1,Check if there is only one example of some class
v0.8.1,No need to crossfit for internal nodes
v0.8.1,Compute partial moments
v0.8.1,"If any of the values in the parameter estimate is nan, return None"
v0.8.1,Compute partial moments
v0.8.1,Compute coefficient by OLS on residuals
v0.8.1,ell_2 regularization
v0.8.1,Ridge regression estimate
v0.8.1,"Parameter returned is of shape (d_T, )"
v0.8.1,Return moments and gradients
v0.8.1,Compute partial moments
v0.8.1,Compute moments
v0.8.1,"Moments shape is (n, d_T-1)"
v0.8.1,Compute moment gradients
v0.8.1,Need to calculate this in an elegant way for when propensity is 0
v0.8.1,This will flatten T
v0.8.1,Check that T is numeric
v0.8.1,Test whether the input estimator is supported
v0.8.1,Calculate confidence intervals for the parameter (marginal effect)
v0.8.1,Calculate confidence intervals for the effect
v0.8.1,Calculate the effects
v0.8.1,Calculate the standard deviations for the effects
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"if both X and W are None, just return a column of ones"
v0.8.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.1,We need to go back to the label representation of the one-hot so as to call
v0.8.1,the classifier.
v0.8.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.1,We need to go back to the label representation of the one-hot so as to call
v0.8.1,the classifier.
v0.8.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.8.1,This works both with our without the weighting trick as the treatments T are unit vector
v0.8.1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.8.1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.8.1,both Parametric and Non Parametric DML.
v0.8.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.8.1,for internal use by the library
v0.8.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.8.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.8.1,since we clone it and fit separate copies
v0.8.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.8.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.8.1,TODO: support sample_var
v0.8.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.8.1,since we clone it and fit separate copies
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,#######################################
v0.8.1,Core DML Tests
v0.8.1,#######################################
v0.8.1,How many samples
v0.8.1,How many control features
v0.8.1,How many treatment variables
v0.8.1,Coefficients of how controls affect treatments
v0.8.1,Coefficients of how controls affect outcome
v0.8.1,Treatment effects that we want to estimate
v0.8.1,Run dml estimation
v0.8.1,How many samples
v0.8.1,How many control features
v0.8.1,How many treatment variables
v0.8.1,Coefficients of how controls affect treatments
v0.8.1,Coefficients of how controls affect outcome
v0.8.1,Treatment effects that we want to estimate
v0.8.1,Run dml estimation
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,note that groups are not passed to score because they are only used for fitting
v0.8.1,note that groups are not passed to score because they are only used for fitting
v0.8.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"this will have dimension (d,) + shape(X)"
v0.8.1,send the first dimension to the end
v0.8.1,columns are featurized independently; partial derivatives are only non-zero
v0.8.1,when taken with respect to the same column each time
v0.8.1,don't fit intercept; manually add column of ones to the data instead;
v0.8.1,this allows us to ignore the intercept when computing marginal effects
v0.8.1,make T 2D if if was a vector
v0.8.1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.8.1,two stage approximation
v0.8.1,"first, get basis expansions of T, X, and Z"
v0.8.1,TODO: is it right that the effective number of intruments is the
v0.8.1,"product of ft_X and ft_Z, not just ft_Z?"
v0.8.1,"regress T expansion on X,Z expansions concatenated with W"
v0.8.1,"predict ft_T from interacted ft_X, ft_Z"
v0.8.1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.8.1,dT may be only 2-dimensional)
v0.8.1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.8.1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.8.1,(which needs to have been expanded if there's a discrete treatment)
v0.8.1,We can write effect interval as a function of const_marginal_effect_interval for a single treatment
v0.8.1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.8.1,d_t=1 here since we measure the effect across all Ts
v0.8.1,once the estimator has been fit
v0.8.1,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.8.1,an intercept even when bias is part of coef.
v0.8.1,We can write effect inference as a function of prediction and prediction standard error of
v0.8.1,the final method for linear models
v0.8.1,d_t=1 here since we measure the effect across all Ts
v0.8.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.8.1,(which needs to have been expanded if there's a discrete treatment)
v0.8.1,"send treatment to the end, pull bounds to the front"
v0.8.1,d_t=1 here since we measure the effect across all Ts
v0.8.1,need to set the fit args before the estimator is fit
v0.8.1,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.8.1,1. Uncertainty of Mean Point Estimate
v0.8.1,2. Distribution of Point Estimate
v0.8.1,3. Total Variance of Point Estimate
v0.8.1,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.8.1,so bail out early; note that it might be possible to correct the algorithm for
v0.8.1,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.8.1,be clean
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.8.1,output is
v0.8.1,"* a column of ones if X, W, and Z are all None"
v0.8.1,* just X or W or Z if both of the others are None
v0.8.1,* hstack([arrs]) for whatever subset are not None otherwise
v0.8.1,ensure Z is 2D
v0.8.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.1,We need to go back to the label representation of the one-hot so as to call
v0.8.1,the classifier.
v0.8.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.1,We need to go back to the label representation of the one-hot so as to call
v0.8.1,the classifier.
v0.8.1,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.8.1,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.8.1,since we would actually be calculating a CATE rather than ATE.
v0.8.1,TODO: allow the final model to actually use X?
v0.8.1,TODO: allow the final model to actually use X?
v0.8.1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.8.1,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.8.1,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.8.1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.8.1,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.8.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.8.1,for internal use by the library
v0.8.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.8.1,Estimate final model of theta(X) by minimizing the square loss:
v0.8.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.8.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.8.1,at the expense of some small bias. For points with very small covariance we revert
v0.8.1,to the model-based preliminary estimate and do not add the correction term.
v0.8.1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.8.1,"instruments, and outcomes"
v0.8.1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.8.1,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.8.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.8.1,for internal use by the library
v0.8.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.8.1,"we need to undo the one-hot encoding for calling effect,"
v0.8.1,since it expects raw values
v0.8.1,"we need to undo the one-hot encoding for calling effect,"
v0.8.1,since it expects raw values
v0.8.1,"TODO: check that Y, T, Z do not have multiple columns"
v0.8.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.8.1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.8.1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.8.1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.8.1,checks that X is 2D array.
v0.8.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.8.1,"Replacing this method which is invalid for this class, so that we make the"
v0.8.1,dosctring empty and not appear in the docs.
v0.8.1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.8.1,TODO: support sample_var
v0.8.1,"Replacing this method which is invalid for this class, so that we make the"
v0.8.1,dosctring empty and not appear in the docs.
v0.8.1,Replacing to remove docstring
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,TODO: make sure to use random seeds wherever necessary
v0.8.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.8.1,"unfortunately with the Theano and Tensorflow backends,"
v0.8.1,the straightforward use of K.stop_gradient can cause an error
v0.8.1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.8.1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.8.1,so that those layers remain connected but with 0 gradient
v0.8.1,|| t - mu_i || ^2
v0.8.1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.8.1,Use logsumexp for numeric stability:
v0.8.1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.8.1,TODO: does the numeric stability actually make any difference?
v0.8.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.8.1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.8.1,generate cumulative sum via matrix multiplication
v0.8.1,"Generate standard uniform values in shape (batch_size,1)"
v0.8.1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.8.1,we use uniform_like instead with an input of an appropriate shape)
v0.8.1,convert to floats and multiply to perform equivalent of logical AND
v0.8.1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.8.1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.8.1,we use normal_like instead with an input of an appropriate shape)
v0.8.1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.8.1,prevent gradient from passing through sampling
v0.8.1,three options: biased or upper-bound loss require a single number of samples;
v0.8.1,unbiased can take different numbers for the network and its gradient
v0.8.1,"sample: (() -> Layer, int) -> Layer"
v0.8.1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.8.1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.8.1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.8.1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.8.1,the dimensionality of the output of the network
v0.8.1,TODO: is there a more robust way to do this?
v0.8.1,TODO: do we need to give the user more control over other arguments to fit?
v0.8.1,"subtle point: we need to build a new model each time,"
v0.8.1,because each model encapsulates its randomness
v0.8.1,TODO: do we need to give the user more control over other arguments to fit?
v0.8.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.8.1,not a general tensor (because of how backprop works in every framework)
v0.8.1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.8.1,but this seems annoying...)
v0.8.1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.8.1,TODO: any way to get this to work on batches of arbitrary size?
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,TODO: generalize to multiple treatment case?
v0.8.1,get index of best treatment
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,#######################################################
v0.8.1,Perfect Data DGPs for Testing Correctness of Code
v0.8.1,#######################################################
v0.8.1,Generate random control co-variates
v0.8.1,Create epsilon residual treatments that deterministically sum up to
v0.8.1,zero
v0.8.1,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.8.1,conditional on each co-variate vector is equal to zero
v0.8.1,We simply subtract the conditional mean from the epsilons
v0.8.1,Construct treatments as T = X*A + epsilon
v0.8.1,Construct outcomes as y = X*beta + T*effect
v0.8.1,Generate random control co-variates
v0.8.1,Create epsilon residual treatments that deterministically sum up to
v0.8.1,zero
v0.8.1,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.8.1,conditional on each co-variate vector is equal to zero
v0.8.1,We simply subtract the conditional mean from the epsilons
v0.8.1,Construct treatments as T = X*A + epsilon
v0.8.1,Construct outcomes as y = X*beta + T*effect
v0.8.1,Generate random control co-variates
v0.8.1,Construct treatments as T = X*A + epsilon
v0.8.1,Construct outcomes as y = X*beta + T*effect
v0.8.1,Generate random control co-variates
v0.8.1,Create epsilon residual treatments
v0.8.1,Construct treatments as T = X*A + epsilon
v0.8.1,Construct outcomes as y = X*beta + T*effect + eta
v0.8.1,Generate random control co-variates
v0.8.1,Use the same treatment vector for each row
v0.8.1,Construct outcomes as y = X*beta + T*effect
v0.8.1,Licensed under the MIT License.
v0.8.1,"since inference objects can be stateful, we must copy it before fitting;"
v0.8.1,otherwise this sequence wouldn't work:
v0.8.1,"est1.fit(..., inference=inf)"
v0.8.1,"est2.fit(..., inference=inf)"
v0.8.1,est1.effect_interval(...)
v0.8.1,because inf now stores state from fitting est2
v0.8.1,call the wrapped fit method
v0.8.1,NOTE: we call inference fit *after* calling the main fit method
v0.8.1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.8.1,but tensordot can't be applied to this problem because we don't sum over m
v0.8.1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.8.1,of rows of T was not taken into account
v0.8.1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.8.1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.8.1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.8.1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.8.1,add statsmodels to parent's options
v0.8.1,add debiasedlasso to parent's options
v0.8.1,add blb to parent's options
v0.8.1,TODO Share some logic with non-discrete version
v0.8.1,add statsmodels to parent's options
v0.8.1,add statsmodels to parent's options
v0.8.1,add blb to parent's options
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Check if model is sparse enough for this model
v0.8.1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.8.1,TODO: any way to avoid creating a copy if the array was already dense?
v0.8.1,"the call is necessary if the input was something like a list, though"
v0.8.1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.8.1,so convert to pydata sparse first
v0.8.1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.8.1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.8.1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.8.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.8.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.8.1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.8.1,same number of input definitions as arrays
v0.8.1,input definitions have same number of dimensions as each array
v0.8.1,all result indices are unique
v0.8.1,all result indices must match at least one input index
v0.8.1,"map indices to all array, axis pairs for that index"
v0.8.1,each index has the same cardinality wherever it appears
v0.8.1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.8.1,Algo: while list contains more than one entry
v0.8.1,take two entries
v0.8.1,sort both lists by intersection of their indices
v0.8.1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.8.1,"take the union of indices and the product of values), stepping through each list linearly"
v0.8.1,TODO: might be faster to break into connected components first
v0.8.1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.8.1,"so compute their content separately, then take cartesian product"
v0.8.1,this would save a few pointless sorts by empty tuples
v0.8.1,TODO: Consider investigating other performance ideas for these cases
v0.8.1,where the dense method beat the sparse method (usually sparse is faster)
v0.8.1,"e,facd,c->cfed"
v0.8.1,sparse: 0.0335489
v0.8.1,dense:  0.011465999999999997
v0.8.1,"gbd,da,egb->da"
v0.8.1,sparse: 0.0791625
v0.8.1,dense:  0.007319099999999995
v0.8.1,"dcc,d,faedb,c->abe"
v0.8.1,sparse: 1.2868097
v0.8.1,dense:  0.44605229999999985
v0.8.1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.8.1,TODO: would using einsum's paths to optimize the order of merging help?
v0.8.1,assume that we should perform nested cross-validation if and only if
v0.8.1,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.8.1,logic copied from check_cv
v0.8.1,otherwise we will assume the user already set the cv attribute to something
v0.8.1,compatible with splitting with a 'groups' argument
v0.8.1,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.8.1,don't use the groups when calling split internally
v0.8.1,Normalize weights
v0.8.1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.8.1,"if we're decorating a class, just update the __init__ method,"
v0.8.1,so that the result is still a class instead of a wrapper method
v0.8.1,"want to enforce that each bad_arg was either in kwargs,"
v0.8.1,or else it was in neither and is just taking its default value
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.8.1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.8.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.8.1,clean way of achieving this
v0.8.1,make sure we don't accidentally escape anything in the substitution
v0.8.1,Fetch appropriate color for node
v0.8.1,"red for negative, green for positive"
v0.8.1,in multi-target use first target
v0.8.1,Write node mean CATE
v0.8.1,Write node std of CATE
v0.8.1,Write confidence interval information if at leaf node
v0.8.1,Fetch appropriate color for node
v0.8.1,"red for negative, green for positive"
v0.8.1,Write node mean CATE
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,remove None arguments
v0.8.1,"scores entries should be lists of scores, so make each entry a singleton list"
v0.8.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.8.1,use a binary array to get stratified split in case of discrete treatment
v0.8.1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.8.1,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.8.1,"however, sklearn doesn't support both stratifying and grouping (see"
v0.8.1,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.8.1,their own object that supports grouping if they want to use groups.
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Estimators
v0.8.1,Causal tree parameters
v0.8.1,Tree structure
v0.8.1,No need for a random split since the data is already
v0.8.1,a random subsample from the original input
v0.8.1,node list stores the nodes that are yet to be splitted
v0.8.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.8.1,Create local sample set
v0.8.1,Compute nuisance estimates for the current node
v0.8.1,Nuisance estimate cannot be calculated
v0.8.1,Estimate parameter for current node
v0.8.1,Node estimate cannot be calculated
v0.8.1,Calculate moments and gradient of moments for current data
v0.8.1,Calculate inverse gradient
v0.8.1,The gradient matrix is not invertible.
v0.8.1,No good split can be found
v0.8.1,Calculate point-wise pseudo-outcomes rho
v0.8.1,a split is determined by a feature and a sample pair
v0.8.1,the number of possible splits is at most (number of features) * (number of node samples)
v0.8.1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.8.1,parse row and column of random pair
v0.8.1,the sample of the pair is the integer division of the random number with n_feats
v0.8.1,calculate the binary indicator of whether sample i is on the left or the right
v0.8.1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.8.1,calculate the number of samples on the left child for each proposed split
v0.8.1,calculate the analogous binary indicator for the samples in the estimation set
v0.8.1,calculate the number of estimation samples on the left child of each proposed split
v0.8.1,find the upper and lower bound on the size of the left split for the split
v0.8.1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.8.1,on each side.
v0.8.1,similarly for the estimation sample set
v0.8.1,if there is no valid split then don't create any children
v0.8.1,filter only the valid splits
v0.8.1,calculate the average influence vector of the samples in the left child
v0.8.1,calculate the average influence vector of the samples in the right child
v0.8.1,take the square of each of the entries of the influence vectors and normalize
v0.8.1,by size of each child
v0.8.1,calculate the vector score of each candidate split as the average of left and right
v0.8.1,influence vectors
v0.8.1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.8.1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.8.1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.8.1,calculate the scalar score of each split by aggregating across the vector of scores
v0.8.1,Find split that minimizes criterion
v0.8.1,Create child nodes with corresponding subsamples
v0.8.1,add the created children to the list of not yet split nodes
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,TODO: Add a __dir__ implementation?
v0.8.1,don't proxy special methods
v0.8.1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.8.1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.8.1,then we should be computing a confidence interval for the wrapped calls
v0.8.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.8.1,second level bootstrap which would be prohibitive computationally?
v0.8.1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.8.1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.8.1,can't import from econml.inference at top level without creating cyclical dependencies
v0.8.1,Note that inference results are always methods even if the inference is for a property
v0.8.1,(e.g. coef__inference() is a method but coef_ is a property)
v0.8.1,Therefore we must insert a lambda if getting inference for a non-callable
v0.8.1,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.8.1,"try to get interval/std first if appropriate,"
v0.8.1,since we don't prefer a wrapped method with this name
v0.8.1,AzureML
v0.8.1,helper imports
v0.8.1,write the details of the workspace to a configuration file to the notebook library
v0.8.1,if y is a multioutput model
v0.8.1,Make sure second dimension has 1 or more item
v0.8.1,switch _inner Model to a MultiOutputRegressor
v0.8.1,flatten array as automl only takes vectors for y
v0.8.1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.8.1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.8.1,as an sklearn estimator
v0.8.1,fit implementation for a single output model.
v0.8.1,Create experiment for specified workspace
v0.8.1,Configure automl_config with training set information.
v0.8.1,"Wait for remote run to complete, the set the model"
v0.8.1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.8.1,create model and pass model into final.
v0.8.1,"If item is an automl config, get its corresponding"
v0.8.1,AutomatedML Model and add it to new_Args
v0.8.1,"If item is an automl config, get its corresponding"
v0.8.1,AutomatedML Model and set it for this key in
v0.8.1,kwargs
v0.8.1,takes in either automated_ml config and instantiates
v0.8.1,an AutomatedMLModel
v0.8.1,The prefix can only be 18 characters long
v0.8.1,"because prefixes come from kwarg_names, we must ensure they are"
v0.8.1,short enough.
v0.8.1,Get workspace from config file.
v0.8.1,Take the intersect of the white for sample
v0.8.1,weights and linear models
v0.8.1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.8.1,Remove children with nonwhite mothers from the treatment group
v0.8.1,Remove children with nonwhite mothers from the treatment group
v0.8.1,Select columns
v0.8.1,Scale the numeric variables
v0.8.1,"Change the binary variable 'first' takes values in {1,2}"
v0.8.1,Append a column of ones as intercept
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.8.1,creating notebooks that are annoying for our users to actually run themselves
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.8.1,"prior to calling interpret, can't plot, render, etc."
v0.8.1,can interpret without uncertainty
v0.8.1,can't interpret with uncertainty if inference wasn't used during fit
v0.8.1,can interpret with uncertainty if we refit
v0.8.1,can interpret without uncertainty
v0.8.1,can't treat before interpreting
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,simple DGP only for illustration
v0.8.1,Define the treatment model neural network architecture
v0.8.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.8.1,"so the input shape is (d_z + d_x,)"
v0.8.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.8.1,add extra layers on top for the mixture density network
v0.8.1,Define the response model neural network architecture
v0.8.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.8.1,"so the input shape is (d_t + d_x,)"
v0.8.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.8.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.8.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.8.1,so that the same weights will be reused in each instantiation
v0.8.1,number of samples to use in second estimate of the response
v0.8.1,(to make loss estimate unbiased)
v0.8.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.8.1,do something with predictions...
v0.8.1,also test vector t and y
v0.8.1,simple DGP only for illustration
v0.8.1,Define the treatment model neural network architecture
v0.8.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.8.1,"so the input shape is (d_z + d_x,)"
v0.8.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.8.1,add extra layers on top for the mixture density network
v0.8.1,Define the response model neural network architecture
v0.8.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.8.1,"so the input shape is (d_t + d_x,)"
v0.8.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.8.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.8.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.8.1,so that the same weights will be reused in each instantiation
v0.8.1,number of samples to use in second estimate of the response
v0.8.1,(to make loss estimate unbiased)
v0.8.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.8.1,do something with predictions...
v0.8.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.8.1,test = True ensures we draw test set images
v0.8.1,test = True ensures we draw test set images
v0.8.1,re-draw to get new independent treatment and implied response
v0.8.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.8.1,above is necesary so that reduced form doesn't win
v0.8.1,covariates: time and emotion
v0.8.1,random instrument
v0.8.1,z -> price
v0.8.1,true observable demand function
v0.8.1,errors
v0.8.1,response
v0.8.1,test = True ensures we draw test set images
v0.8.1,test = True ensures we draw test set images
v0.8.1,re-draw to get new independent treatment and implied response
v0.8.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.8.1,above is necesary so that reduced form doesn't win
v0.8.1,covariates: time and emotion
v0.8.1,random instrument
v0.8.1,z -> price
v0.8.1,true observable demand function
v0.8.1,errors
v0.8.1,response
v0.8.1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.8.1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.8.1,For some reason this doesn't work at all when run against the CNTK backend...
v0.8.1,"model.compile('nadam', loss=lambda _,l:l)"
v0.8.1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.8.1,generate a valiation set
v0.8.1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.8.1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.8.1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.8.1,so we need to transpose the result
v0.8.1,1-d output
v0.8.1,2-d output
v0.8.1,Single dimensional output y
v0.8.1,Multi-dimensional output y
v0.8.1,1-d y
v0.8.1,multi-d y
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,test that we can fit with the same arguments as the base estimator
v0.8.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.8.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.1,with the same shape for the lower and upper bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,test that we can do the same thing once we provide percentile bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.8.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.1,with the same shape for the lower and upper bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,test that we can do the same thing once we provide percentile bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,test that we can fit with the same arguments as the base estimator
v0.8.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.8.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.1,with the same shape for the lower and upper bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,test that we can do the same thing once we provide percentile bounds
v0.8.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.8.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.1,with the same shape for the lower and upper bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,test that we can do the same thing once we provide percentile bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,test that we can fit with the same arguments as the base estimator
v0.8.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.8.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.1,with the same shape for the lower and upper bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,test that we can do the same thing once we provide percentile bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.8.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.1,with the same shape for the lower and upper bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,test that we can do the same thing once we provide percentile bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.1,with the same shape for the lower and upper bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,test that the estimated effect is usually within the bounds
v0.8.1,test that we can do the same thing once we provide alpha explicitly
v0.8.1,test that the lower and upper bounds differ
v0.8.1,test that the estimated effect is usually within the bounds
v0.8.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.1,with the same shape for the lower and upper bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,TODO: test that the estimated effect is usually within the bounds
v0.8.1,and that the true effect is also usually within the bounds
v0.8.1,test that we can do the same thing once we provide percentile bounds
v0.8.1,test that the lower and upper bounds differ
v0.8.1,TODO: test that the estimated effect is usually within the bounds
v0.8.1,and that the true effect is also usually within the bounds
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,DGP constants
v0.8.1,Generate data
v0.8.1,Test inference results when `cate_feature_names` doesn not exist
v0.8.1,Test inference results when `cate_feature_names` doesn not exist
v0.8.1,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.8.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.8.1,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.8.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.8.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.8.1,pvalue for second column should be greater than zero since some points are on either side
v0.8.1,of the tested value
v0.8.1,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.8.1,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.8.1,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.8.1,Test non keyword based calls to fit
v0.8.1,test non-array inputs
v0.8.1,Test custom splitter
v0.8.1,Test incomplete set of test folds
v0.8.1,"y scores should be positive, since W predicts Y somewhat"
v0.8.1,"t scores might not be, since W and T are uncorrelated"
v0.8.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,make sure cross product varies more slowly with first array
v0.8.1,and that vectors are okay as inputs
v0.8.1,number of inputs in specification must match number of inputs
v0.8.1,must have an output
v0.8.1,output indices must be unique
v0.8.1,output indices must be present in an input
v0.8.1,number of indices must match number of dimensions for each input
v0.8.1,repeated indices must always have consistent sizes
v0.8.1,transpose
v0.8.1,tensordot
v0.8.1,trace
v0.8.1,TODO: set up proper flag for this
v0.8.1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.8.1,"of all of the distinct indices that appear in any input,"
v0.8.1,pick a random subset of them (of size at most 5) to appear in the output
v0.8.1,creating an instance should warn
v0.8.1,using the instance should not warn
v0.8.1,using the deprecated method should warn
v0.8.1,don't warn if b and c are passed by keyword
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Preprocess data
v0.8.1,Convert 'week' to a date
v0.8.1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.8.1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.8.1,Take log of price
v0.8.1,Make brand numeric
v0.8.1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.8.1,which have all zero coeffs
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.8.1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.8.1,TODO: test something rather than just print...
v0.8.1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.8.1,pick some arbitrary X
v0.8.1,pick some arbitrary T
v0.8.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,ensure that we've got at least two of every row
v0.8.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.1,need to make sure we get all *joint* combinations
v0.8.1,IntentToTreat only supports binary treatments/instruments
v0.8.1,IntentToTreat only supports binary treatments/instruments
v0.8.1,IntentToTreat requires X
v0.8.1,ensure we can serialize unfit estimator
v0.8.1,these support only W but not X
v0.8.1,"these support only binary, not general discrete T and Z"
v0.8.1,ensure we can serialize fit estimator
v0.8.1,make sure we can call the marginal_effect and effect methods
v0.8.1,TODO: add tests for extra properties like coef_ where they exist
v0.8.1,TODO: add tests for extra properties like coef_ where they exist
v0.8.1,"make sure we can call effect with implied scalar treatments,"
v0.8.1,"no matter the dimensions of T, and also that we warn when there"
v0.8.1,are multiple treatments
v0.8.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.8.1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.8.1,"however, with custom splits the checking happens in the first stage wrapper"
v0.8.1,where we don't have all of the required information to do this;
v0.8.1,we'd probably need to add it to _crossfit instead
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.8.1,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.8.1,The __warningregistry__'s need to be in a pristine state for tests
v0.8.1,to work properly.
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Set random seed
v0.8.1,Generate data
v0.8.1,DGP constants
v0.8.1,Test data
v0.8.1,Constant treatment effect
v0.8.1,Constant treatment with multi output Y
v0.8.1,Heterogeneous treatment
v0.8.1,Heterogeneous treatment with multi output Y
v0.8.1,TLearner test
v0.8.1,Instantiate TLearner
v0.8.1,Test inputs
v0.8.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.1,Instantiate SLearner
v0.8.1,Test inputs
v0.8.1,Test constant treatment effect
v0.8.1,Test constant treatment effect with multi output Y
v0.8.1,Test heterogeneous treatment effect
v0.8.1,Need interactions between T and features
v0.8.1,Test heterogeneous treatment effect with multi output Y
v0.8.1,Instantiate XLearner
v0.8.1,Test inputs
v0.8.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.1,Instantiate DomainAdaptationLearner
v0.8.1,Test inputs
v0.8.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.1,Get the true treatment effect
v0.8.1,Get the true treatment effect
v0.8.1,Fit learner and get the effect and marginal effect
v0.8.1,Compute treatment effect residuals (absolute)
v0.8.1,Check that at least 90% of predictions are within tolerance interval
v0.8.1,Check whether the output shape is right
v0.8.1,Check that one can pass in regular lists
v0.8.1,Check that it fails correctly if lists of different shape are passed in
v0.8.1,"Check that it works when T, Y have shape (n, 1)"
v0.8.1,Generate covariates
v0.8.1,Generate treatment
v0.8.1,Calculate outcome
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,DGP constants
v0.8.1,Generate data
v0.8.1,Test data
v0.8.1,Remove warnings that might be raised by the models passed into the ORF
v0.8.1,Generate data with continuous treatments
v0.8.1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.8.1,does not work well with parallelism.
v0.8.1,Test inputs for continuous treatments
v0.8.1,--> Check that one can pass in regular lists
v0.8.1,--> Check that it fails correctly if lists of different shape are passed in
v0.8.1,Check that outputs have the correct shape
v0.8.1,Test continuous treatments with controls
v0.8.1,Test continuous treatments without controls
v0.8.1,Test Causal Forest API
v0.8.1,Generate data with continuous treatments
v0.8.1,Instantiate model with most of the default parameters.
v0.8.1,Test inputs for continuous treatments
v0.8.1,--> Check that one can pass in regular lists
v0.8.1,--> Check that it fails correctly if lists of different shape are passed in
v0.8.1,Check that outputs have the correct shape
v0.8.1,Test continuous treatments with controls
v0.8.1,Test continuous treatments without controls
v0.8.1,Generate data with binary treatments
v0.8.1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.8.1,does not work well with parallelism.
v0.8.1,Test inputs for binary treatments
v0.8.1,--> Check that one can pass in regular lists
v0.8.1,--> Check that it fails correctly if lists of different shape are passed in
v0.8.1,"--> Check that it works when T, Y have shape (n, 1)"
v0.8.1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.8.1,--> Check that it fails correctly when the treatments are not numeric
v0.8.1,Check that outputs have the correct shape
v0.8.1,Test binary treatments with controls
v0.8.1,Test binary treatments without controls
v0.8.1,Test CausalForest API
v0.8.1,Generate data with binary treatments
v0.8.1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.8.1,does not work well with parallelism.
v0.8.1,Test inputs for binary treatments
v0.8.1,--> Check that one can pass in regular lists
v0.8.1,--> Check that it fails correctly if lists of different shape are passed in
v0.8.1,"--> Check that it works when T, Y have shape (n, 1)"
v0.8.1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.8.1,--> Check that it fails correctly when the treatments are not numeric
v0.8.1,Check that outputs have the correct shape
v0.8.1,Test binary treatments with controls
v0.8.1,Test binary treatments without controls
v0.8.1,Only applicable to continuous treatments
v0.8.1,Generate data for 2 treatments
v0.8.1,Test multiple treatments with controls
v0.8.1,Test CausalForest API
v0.8.1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.8.1,The rest for controls. Just as an example.
v0.8.1,Generating A/B test data
v0.8.1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.8.1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.8.1,Test causal foret API
v0.8.1,Test Causal Forest API
v0.8.1,Create a wrapper around Lasso that doesn't support weights
v0.8.1,since Lasso does natively support them starting in sklearn 0.23
v0.8.1,Generate data with continuous treatments
v0.8.1,Instantiate model with most of the default parameters
v0.8.1,Compute the treatment effect on test points
v0.8.1,Compute treatment effect residuals
v0.8.1,Multiple treatments
v0.8.1,Allow at most 10% test points to be outside of the tolerance interval
v0.8.1,Compute treatment effect residuals
v0.8.1,Multiple treatments
v0.8.1,Allow at most 20% test points to be outside of the confidence interval
v0.8.1,Check that the intervals are not too wide
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.8.1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.8.1,ensure that we've got at least 6 of every element
v0.8.1,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.8.1,NOTE: this number may need to change if the default number of folds in
v0.8.1,WeightedStratifiedKFold changes
v0.8.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.1,ensure we can serialize the unfit estimator
v0.8.1,ensure we can pickle the fit estimator
v0.8.1,make sure we can call the marginal_effect and effect methods
v0.8.1,test const marginal inference
v0.8.1,test effect inference
v0.8.1,test marginal effect inference
v0.8.1,test coef__inference and intercept__inference
v0.8.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.8.1,"make sure we can call effect with implied scalar treatments,"
v0.8.1,"no matter the dimensions of T, and also that we warn when there"
v0.8.1,are multiple treatments
v0.8.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.8.1,ensure that we've got at least two of every element
v0.8.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.1,make sure we can call the marginal_effect and effect methods
v0.8.1,test const marginal inference
v0.8.1,test effect inference
v0.8.1,test marginal effect inference
v0.8.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.8.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.8.1,We concatenate the two copies data
v0.8.1,create a simple artificial setup where effect of moving from treatment
v0.8.1,"1 -> 2 is 2,"
v0.8.1,"1 -> 3 is 1, and"
v0.8.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.8.1,"Using an uneven number of examples from different classes,"
v0.8.1,"and having the treatments in non-lexicographic order,"
v0.8.1,Should rule out some basic issues.
v0.8.1,test that we can fit with a KFold instance
v0.8.1,test that we can fit with a train/test iterable
v0.8.1,predetermined splits ensure that all features are seen in each split
v0.8.1,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.8.1,(incorrectly) use a final model with an intercept
v0.8.1,"Because final model is fixed, actual values of T and Y don't matter"
v0.8.1,Ensure reproducibility
v0.8.1,Sparse DGP
v0.8.1,Treatment effect coef
v0.8.1,Other coefs
v0.8.1,Features and controls
v0.8.1,Test sparse estimator
v0.8.1,"--> test coef_, intercept_"
v0.8.1,--> test treatment effects
v0.8.1,Restrict x_test to vectors of norm < 1
v0.8.1,--> check inference
v0.8.1,Check that a majority of true effects lie in the 5-95% CI
v0.8.1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.8.1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.8.1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.8.1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.8.1,sparse test case: heterogeneous effect by product
v0.8.1,need at least as many rows in e_y as there are distinct columns
v0.8.1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.8.1,create a simple artificial setup where effect of moving from treatment
v0.8.1,"a -> b is 2,"
v0.8.1,"a -> c is 1, and"
v0.8.1,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.8.1,"Using an uneven number of examples from different classes,"
v0.8.1,"and having the treatments in non-lexicographic order,"
v0.8.1,should rule out some basic issues.
v0.8.1,Note that explicitly specifying the dtype as object is necessary until
v0.8.1,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.8.1,estimated effects should be identical when treatment is explicitly given
v0.8.1,but const_marginal_effect should be reordered based on the explicit cagetories
v0.8.1,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.8.1,test outer grouping
v0.8.1,test nested grouping
v0.8.1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.8.1,whichever groups we saw
v0.8.1,test nested grouping
v0.8.1,"by default, we use 5 split cross-validation for our T and Y models"
v0.8.1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.8.1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.8.1,make sure we warn when using old aliases
v0.8.1,make sure we can use the old alias as a type
v0.8.1,make sure that we can still pickle the old aliases
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Set random seed
v0.8.1,Generate data
v0.8.1,DGP constants
v0.8.1,Test data
v0.8.1,Constant treatment effect and propensity
v0.8.1,Heterogeneous treatment and propensity
v0.8.1,ensure that we've got at least two of every element
v0.8.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.1,ensure that we can serialize unfit estimator
v0.8.1,ensure that we can serialize fit estimator
v0.8.1,make sure we can call the marginal_effect and effect methods
v0.8.1,test const marginal inference
v0.8.1,test effect inference
v0.8.1,test marginal effect inference
v0.8.1,test coef_ and intercept_ inference
v0.8.1,verify we can generate the summary
v0.8.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.8.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.8.1,create a simple artificial setup where effect of moving from treatment
v0.8.1,"1 -> 2 is 2,"
v0.8.1,"1 -> 3 is 1, and"
v0.8.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.8.1,"Using an uneven number of examples from different classes,"
v0.8.1,"and having the treatments in non-lexicographic order,"
v0.8.1,Should rule out some basic issues.
v0.8.1,test that we can fit with a KFold instance
v0.8.1,test that we can fit with a train/test iterable
v0.8.1,"for at least some of the examples, the CI should have nonzero width"
v0.8.1,"for at least some of the examples, the CI should have nonzero width"
v0.8.1,"for at least some of the examples, the CI should have nonzero width"
v0.8.1,test coef__inference function works
v0.8.1,test intercept__inference function works
v0.8.1,test summary function works
v0.8.1,Test inputs
v0.8.1,self._test_inputs(DR_learner)
v0.8.1,Test constant treatment effect
v0.8.1,Test heterogeneous treatment effect
v0.8.1,Test heterogenous treatment effect for W =/= None
v0.8.1,Sparse DGP
v0.8.1,Treatment effect coef
v0.8.1,Other coefs
v0.8.1,Features and controls
v0.8.1,Test sparse estimator
v0.8.1,"--> test coef_, intercept_"
v0.8.1,--> test treatment effects
v0.8.1,Restrict x_test to vectors of norm < 1
v0.8.1,--> check inference
v0.8.1,Check that a majority of true effects lie in the 5-95% CI
v0.8.1,test outer grouping
v0.8.1,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.8.1,test nested grouping
v0.8.1,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.8.1,whichever groups we saw
v0.8.1,test nested grouping
v0.8.1,"by default, we use 5 split cross-validation for our T and Y models"
v0.8.1,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.8.1,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.8.1,Fit learner and get the effect
v0.8.1,Get the true treatment effect
v0.8.1,Compute treatment effect residuals (absolute)
v0.8.1,Check that at least 90% of predictions are within tolerance interval
v0.8.1,Only for heterogeneous TE
v0.8.1,Fit learner on X and W and get the effect
v0.8.1,Get the true treatment effect
v0.8.1,Compute treatment effect residuals (absolute)
v0.8.1,Check that at least 90% of predictions are within tolerance interval
v0.8.1,Check that one can pass in regular lists
v0.8.1,Check that it fails correctly if lists of different shape are passed in
v0.8.1,Check that it fails when T contains values other than 0 and 1
v0.8.1,"Check that it works when T, Y have shape (n, 1)"
v0.8.1,Generate covariates
v0.8.1,Generate treatment
v0.8.1,Calculate outcome
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,DGP constants
v0.8.1,DGP coefficients
v0.8.1,Generated outcomes
v0.8.1,################
v0.8.1,WeightedLasso #
v0.8.1,################
v0.8.1,Define weights
v0.8.1,Define extended datasets
v0.8.1,Range of alphas
v0.8.1,Compare with Lasso
v0.8.1,--> No intercept
v0.8.1,--> With intercept
v0.8.1,When DGP has no intercept
v0.8.1,When DGP has intercept
v0.8.1,--> Coerce coefficients to be positive
v0.8.1,--> Toggle max_iter & tol
v0.8.1,Define weights
v0.8.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.8.1,Mixed DGP scenario.
v0.8.1,Define extended datasets
v0.8.1,Define weights
v0.8.1,Define multioutput
v0.8.1,##################
v0.8.1,WeightedLassoCV #
v0.8.1,##################
v0.8.1,Define alphas to test
v0.8.1,Compare with LassoCV
v0.8.1,--> No intercept
v0.8.1,--> With intercept
v0.8.1,--> Force parameters to be positive
v0.8.1,Choose a smaller n to speed-up process
v0.8.1,Compare fold weights
v0.8.1,Define weights
v0.8.1,Define extended datasets
v0.8.1,Define splitters
v0.8.1,WeightedKFold splitter
v0.8.1,Map weighted splitter to an extended splitter
v0.8.1,Define alphas to test
v0.8.1,Compare with LassoCV
v0.8.1,--> No intercept
v0.8.1,--> With intercept
v0.8.1,--> Force parameters to be positive
v0.8.1,###########################
v0.8.1,MultiTaskWeightedLassoCV #
v0.8.1,###########################
v0.8.1,Define alphas to test
v0.8.1,Define splitter
v0.8.1,Compare with MultiTaskLassoCV
v0.8.1,--> No intercept
v0.8.1,--> With intercept
v0.8.1,Define weights
v0.8.1,Define extended datasets
v0.8.1,Define splitters
v0.8.1,WeightedKFold splitter
v0.8.1,Map weighted splitter to an extended splitter
v0.8.1,Define alphas to test
v0.8.1,Compare with LassoCV
v0.8.1,--> No intercept
v0.8.1,--> With intercept
v0.8.1,#########################
v0.8.1,WeightedLassoCVWrapper #
v0.8.1,#########################
v0.8.1,perform 1D fit
v0.8.1,perform 2D fit
v0.8.1,################
v0.8.1,DebiasedLasso #
v0.8.1,################
v0.8.1,Test DebiasedLasso without weights
v0.8.1,--> Check debiased coeffcients without intercept
v0.8.1,--> Check debiased coeffcients with intercept
v0.8.1,--> Check 5-95 CI coverage for unit vectors
v0.8.1,Test DebiasedLasso with weights for one DGP
v0.8.1,Define weights
v0.8.1,Define extended datasets
v0.8.1,--> Check debiased coefficients
v0.8.1,Define weights
v0.8.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.8.1,--> Check debiased coeffcients
v0.8.1,Test that attributes propagate correctly
v0.8.1,Test MultiOutputDebiasedLasso without weights
v0.8.1,--> Check debiased coeffcients without intercept
v0.8.1,--> Check debiased coeffcients with intercept
v0.8.1,--> Check CI coverage
v0.8.1,Test MultiOutputDebiasedLasso with weights
v0.8.1,Define weights
v0.8.1,Define extended datasets
v0.8.1,--> Check debiased coefficients
v0.8.1,Unit vectors
v0.8.1,Unit vectors
v0.8.1,Check coeffcients and intercept are the same within tolerance
v0.8.1,Check results are similar with tolerance 1e-6
v0.8.1,Check if multitask
v0.8.1,Check that same alpha is chosen
v0.8.1,Check that the coefficients are similar
v0.8.1,selective ridge has a simple implementation that we can test against
v0.8.1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.8.1,"it should be the case that when we set fit_intercept to true,"
v0.8.1,it doesn't matter whether the penalized model also fits an intercept or not
v0.8.1,create an extra copy of rows with weight 2
v0.8.1,"instead of a slice, explicitly return an array of indices"
v0.8.1,_penalized_inds is only set during fitting
v0.8.1,cv exists on penalized model
v0.8.1,now we can access _penalized_inds
v0.8.1,check that we can read the cv attribute back out from the underlying model
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Check that the point estimates are the same
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Linear models are required for parametric dml
v0.8.1,sample weighting models are required for nonparametric dml
v0.8.1,Test values
v0.8.1,TLearner test
v0.8.1,Instantiate TLearner
v0.8.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.1,Test constant treatment effect with multi output Y
v0.8.1,Test heterogeneous treatment effect
v0.8.1,Need interactions between T and features
v0.8.1,Test heterogeneous treatment effect with multi output Y
v0.8.1,Instantiate DomainAdaptationLearner
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,TODO: conisder working around relying on sklearn implementation details
v0.8.1,"Found a good split, return."
v0.8.1,Record all splits in case the stratification by weight yeilds a worse partition
v0.8.1,Reseed random generator and try again
v0.8.1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.8.1,"Found a good split, return."
v0.8.1,Did not find a good split
v0.8.1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.8.1,Return most weight-balanced partition
v0.8.1,Weight stratification algorithm
v0.8.1,Sort weights for weight strata search
v0.8.1,There are some leftover indices that have yet to be assigned
v0.8.1,Append stratum splits to overall splits
v0.8.1,"If classification methods produce multiple columns of output,"
v0.8.1,we need to manually encode classes to ensure consistent column ordering.
v0.8.1,We clone the estimator to make sure that all the folds are
v0.8.1,"independent, and that it is pickle-able."
v0.8.1,Concatenate the predictions
v0.8.1,`predictions` is a list of method outputs from each fold.
v0.8.1,"If each of those is also a list, then treat this as a"
v0.8.1,multioutput-multiclass task. We need to separately concatenate
v0.8.1,the method outputs for each label into an `n_labels` long list.
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Our classes that derive from sklearn ones sometimes include
v0.8.1,inherited docstrings that have embedded doctests; we need the following imports
v0.8.1,so that they don't break.
v0.8.1,TODO: consider working around relying on sklearn implementation details
v0.8.1,"Convert X, y into numpy arrays"
v0.8.1,Define fit parameters
v0.8.1,Some algorithms don't have a check_input option
v0.8.1,Check weights array
v0.8.1,Check that weights are size-compatible
v0.8.1,Normalize inputs
v0.8.1,Weight inputs
v0.8.1,Fit base class without intercept
v0.8.1,Fit Lasso
v0.8.1,Reset intercept
v0.8.1,The intercept is not calculated properly due the sqrt(weights) factor
v0.8.1,so it must be recomputed
v0.8.1,Fit lasso without weights
v0.8.1,Make weighted splitter
v0.8.1,Fit weighted model
v0.8.1,Make weighted splitter
v0.8.1,Fit weighted model
v0.8.1,Select optimal penalty
v0.8.1,Warn about consistency
v0.8.1,"Convert X, y into numpy arrays"
v0.8.1,Fit weighted lasso with user input
v0.8.1,"Center X, y"
v0.8.1,Calculate quantities that will be used later on. Account for centered data
v0.8.1,Calculate coefficient and error variance
v0.8.1,Add coefficient correction
v0.8.1,Set coefficients and intercept standard errors
v0.8.1,Set intercept
v0.8.1,Return alpha to 'auto' state
v0.8.1,"Note that in the case of no intercept, X_offset is 0"
v0.8.1,Calculate the variance of the predictions
v0.8.1,"Note that in the case of no intercept, X_offset is 0"
v0.8.1,Calculate the variance of the predictions
v0.8.1,Calculate prediction confidence intervals
v0.8.1,Assumes flattened y
v0.8.1,Compute weighted residuals
v0.8.1,To be done once per target. Assumes y can be flattened.
v0.8.1,Assumes that X has already been offset
v0.8.1,Special case: n_features=1
v0.8.1,Compute Lasso coefficients for the columns of the design matrix
v0.8.1,Call weighted lasso on reduced design matrix
v0.8.1,Inherit some parameters from the parent
v0.8.1,Weighted tau
v0.8.1,Compute C_hat
v0.8.1,Compute theta_hat
v0.8.1,Allow for single output as well
v0.8.1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.8.1,Set coef_ attribute
v0.8.1,Set intercept_ attribute
v0.8.1,Set selected_alpha_ attribute
v0.8.1,Set coef_stderr_
v0.8.1,intercept_stderr_
v0.8.1,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.8.1,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.8.1,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.8.1,set intercept_ attribute
v0.8.1,set coef_ attribute
v0.8.1,set alpha_ attribute
v0.8.1,set alphas_ attribute
v0.8.1,set n_iter_ attribute
v0.8.1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.8.1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.8.1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.8.1,now regress X1 on y - X2 * beta2 to learn beta1
v0.8.1,set coef_ and intercept_ attributes
v0.8.1,Note that the penalized model should *not* have an intercept
v0.8.1,don't proxy special methods
v0.8.1,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.8.1,regressor incorrectly
v0.8.1,"Note: for known attributes that have been set this method will not be called,"
v0.8.1,so we should just throw here because this is an attribute belonging to this class
v0.8.1,but which hasn't yet been set on this instance
v0.8.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.8.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.1,Licensed under the MIT License.
v0.8.1,Construct the subsample of data
v0.8.1,Split into estimation and splitting sample set
v0.8.1,Fit the tree on the splitting sample
v0.8.1,Set the estimation values based on the estimation split
v0.8.1,Apply the trained tree on the estimation sample to get the path for every estimation sample
v0.8.1,Calculate the total weight of estimation samples on each tree node:
v0.8.1,\sum_i sample_weight[i] * 1{i \\in node}
v0.8.1,Calculate the total number of estimation samples on each tree node:
v0.8.1,|node| = \sum_{i} 1{i \\in node}
v0.8.1,Calculate the weighted sum of responses on the estimation sample on each node:
v0.8.1,\sum_{i} sample_weight[i] 1{i \\in node} Y_i
v0.8.1,Calculate the predicted value on each node based on the estimation sample:
v0.8.1,weighted sum of responses / total weight
v0.8.1,"Calculate the criterion on each node based on the estimation sample and for each output dimension,"
v0.8.1,summing the impurity across dimensions.
v0.8.1,First we calculate the difference of observed label y of each node and predicted value for each
v0.8.1,node that the sample falls in: y[i] - value_est[node]
v0.8.1,If criterion is mse then calculate weighted sum of squared differences for each node
v0.8.1,If criterion is mae then calculate weighted sum of absolute differences for each node
v0.8.1,Normalize each weighted sum of criterion for each node by the total weight of each node
v0.8.1,Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
v0.8.1,and for each un-pruned tree set the value and the weight appropriately.
v0.8.1,If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
v0.8.1,"sample, then prune the whole sub-tree"
v0.8.1,Set the numerator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
v0.8.1,Set the value of the node to:
v0.8.1,\sum_{i} sample_weight[i] 1{i \\in node} Y_i / \sum_{i} sample_weight[i] 1{i \\in node}
v0.8.1,Set the denominator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
v0.8.1,Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node}
v0.8.1,Set the count to the estimation split count
v0.8.1,Set the node impurity to the estimation split impurity
v0.8.1,Validate or convert input data
v0.8.1,Pre-sort indices to avoid that each individual tree of the
v0.8.1,ensemble sorts the indices.
v0.8.1,Remap output
v0.8.1,reshape is necessary to preserve the data contiguity against vs
v0.8.1,"[:, np.newaxis] that does not."
v0.8.1,Check parameters
v0.8.1,"Free allocated memory, if any"
v0.8.1,We draw from the random state to get the random state we
v0.8.1,would have got if we hadn't used a warm_start.
v0.8.1,Parallel loop: we prefer the threading backend as the Cython code
v0.8.1,for fitting the trees is internally releasing the Python GIL
v0.8.1,making threading more efficient than multiprocessing in
v0.8.1,"that case. However, for joblib 0.12+ we respect any"
v0.8.1,"parallel_backend contexts set at a higher level,"
v0.8.1,since correctness does not rely on using threads.
v0.8.1,TODO. This slicing should ultimately be done inside the parallel function
v0.8.1,so that we don't need to create a matrix of size roughly n_samples * n_estimators
v0.8.1,Collect newly grown trees
v0.8.1,Helper class that accumulates an arbitrary function in parallel on the accumulator acc
v0.8.1,and calls the function fn on each tree e and returns the mean output. The function fn
v0.8.1,should take as input a tree e and associated numerator n and denominator d structures and
v0.8.1,"return another function g_e, which takes as input X, check_input"
v0.8.1,"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
v0.8.1,index start to index end will be used. The returned result is essentially:
v0.8.1,(mean over e in slice)(g_e(X)).
v0.8.1,Check data
v0.8.1,Assign chunk of trees to jobs
v0.8.1,Check data
v0.8.1,Check data
v0.8.1,avoid storing the output of every estimator by summing them here
v0.8.1,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
v0.8.1,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
v0.8.1,"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
v0.8.1,where \theta(X) is the point estimate using the whole forest
v0.8.1,Calculate the variance of the latter as E[Q(S)^2]
v0.8.0,configuration is all pulled from setup.cfg
v0.8.0,-*- coding: utf-8 -*-
v0.8.0,
v0.8.0,Configuration file for the Sphinx documentation builder.
v0.8.0,
v0.8.0,This file does only contain a selection of the most common options. For a
v0.8.0,full list see the documentation:
v0.8.0,http://www.sphinx-doc.org/en/master/config
v0.8.0,-- Path setup --------------------------------------------------------------
v0.8.0,"If extensions (or modules to document with autodoc) are in another directory,"
v0.8.0,add these directories to sys.path here. If the directory is relative to the
v0.8.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.8.0,
v0.8.0,-- Project information -----------------------------------------------------
v0.8.0,-- General configuration ---------------------------------------------------
v0.8.0,"If your documentation needs a minimal Sphinx version, state it here."
v0.8.0,
v0.8.0,needs_sphinx = '1.0'
v0.8.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.8.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.8.0,ones.
v0.8.0,"Add any paths that contain templates here, relative to this directory."
v0.8.0,The suffix(es) of source filenames.
v0.8.0,You can specify multiple suffix as a list of string:
v0.8.0,
v0.8.0,"source_suffix = ['.rst', '.md']"
v0.8.0,The master toctree document.
v0.8.0,The language for content autogenerated by Sphinx. Refer to documentation
v0.8.0,for a list of supported languages.
v0.8.0,
v0.8.0,This is also used if you do content translation via gettext catalogs.
v0.8.0,"Usually you set ""language"" from the command line for these cases."
v0.8.0,"List of patterns, relative to source directory, that match files and"
v0.8.0,directories to ignore when looking for source files.
v0.8.0,This pattern also affects html_static_path and html_extra_path.
v0.8.0,The name of the Pygments (syntax highlighting) style to use.
v0.8.0,-- Options for HTML output -------------------------------------------------
v0.8.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.8.0,a list of builtin themes.
v0.8.0,
v0.8.0,Theme options are theme-specific and customize the look and feel of a theme
v0.8.0,"further.  For a list of options available for each theme, see the"
v0.8.0,documentation.
v0.8.0,
v0.8.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.8.0,"relative to this directory. They are copied after the builtin static files,"
v0.8.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.8.0,html_static_path = ['_static']
v0.8.0,"Custom sidebar templates, must be a dictionary that maps document names"
v0.8.0,to template names.
v0.8.0,
v0.8.0,The default sidebars (for documents that don't match any pattern) are
v0.8.0,defined by theme itself.  Builtin themes are using these templates by
v0.8.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.8.0,'searchbox.html']``.
v0.8.0,
v0.8.0,html_sidebars = {}
v0.8.0,-- Options for HTMLHelp output ---------------------------------------------
v0.8.0,Output file base name for HTML help builder.
v0.8.0,-- Options for LaTeX output ------------------------------------------------
v0.8.0,The paper size ('letterpaper' or 'a4paper').
v0.8.0,
v0.8.0,"'papersize': 'letterpaper',"
v0.8.0,"The font size ('10pt', '11pt' or '12pt')."
v0.8.0,
v0.8.0,"'pointsize': '10pt',"
v0.8.0,Additional stuff for the LaTeX preamble.
v0.8.0,
v0.8.0,"'preamble': '',"
v0.8.0,Latex figure (float) alignment
v0.8.0,
v0.8.0,"'figure_align': 'htbp',"
v0.8.0,Grouping the document tree into LaTeX files. List of tuples
v0.8.0,"(source start file, target name, title,"
v0.8.0,"author, documentclass [howto, manual, or own class])."
v0.8.0,-- Options for manual page output ------------------------------------------
v0.8.0,One entry per manual page. List of tuples
v0.8.0,"(source start file, name, description, authors, manual section)."
v0.8.0,-- Options for Texinfo output ----------------------------------------------
v0.8.0,Grouping the document tree into Texinfo files. List of tuples
v0.8.0,"(source start file, target name, title, author,"
v0.8.0,"dir menu entry, description, category)"
v0.8.0,-- Options for Epub output -------------------------------------------------
v0.8.0,Bibliographic Dublin Core info.
v0.8.0,The unique identifier of the text. This can be a ISBN number
v0.8.0,or the project homepage.
v0.8.0,
v0.8.0,epub_identifier = ''
v0.8.0,A unique identification for the text.
v0.8.0,
v0.8.0,epub_uid = ''
v0.8.0,A list of files that should not be packed into the epub file.
v0.8.0,-- Extension configuration -------------------------------------------------
v0.8.0,-- Options for intersphinx extension ---------------------------------------
v0.8.0,Example configuration for intersphinx: refer to the Python standard library.
v0.8.0,-- Options for todo extension ----------------------------------------------
v0.8.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.8.0,-- Options for doctest extension -------------------------------------------
v0.8.0,we can document otherwise excluded entities here by returning False
v0.8.0,or skip otherwise included entities by returning True
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Calculate residuals
v0.8.0,Estimate E[T_res | Z_res]
v0.8.0,TODO. Deal with multi-class instrument
v0.8.0,Calculate nuisances
v0.8.0,Estimate E[T_res | Z_res]
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"We do a three way split, as typically a preliminary theta estimator would require"
v0.8.0,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.8.0,TODO. Deal with multi-class instrument
v0.8.0,Estimate final model of theta(X) by minimizing the square loss:
v0.8.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.8.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.8.0,at the expense of some small bias. For points with very small covariance we revert
v0.8.0,to the model-based preliminary estimate and do not add the correction term.
v0.8.0,Estimate preliminary theta in cross fitting manner
v0.8.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.8.0,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.8.0,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.8.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.8.0,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.8.0,TODO. The solution below is not really a valid cross-fitting
v0.8.0,as the test data are used to create the proj_t on the train
v0.8.0,which in the second train-test loop is used to create the nuisance
v0.8.0,cov on the test data. Hence the T variable of some sample
v0.8.0,"is implicitly correlated with its cov nuisance, through this flow"
v0.8.0,"of information. However, this seems a rather weak correlation."
v0.8.0,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.8.0,model.
v0.8.0,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.8.0,Estimate preliminary theta in cross fitting manner
v0.8.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.8.0,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.8.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.8.0,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.8.0,#############################################################################
v0.8.0,Classes for the DRIV implementation for the special case of intent-to-treat
v0.8.0,A/B test
v0.8.0,#############################################################################
v0.8.0,Estimate preliminary theta in cross fitting manner
v0.8.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.8.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.8.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.8.0,We can use statsmodel for all hypothesis testing capabilities
v0.8.0,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.8.0,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.8.0,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.8.0,model_T_XZ = lambda: model_clf()
v0.8.0,#'days_visited': lambda:
v0.8.0,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.8.0,Turn strings into categories for numeric mapping
v0.8.0,### Defining some generic regressors and classifiers
v0.8.0,This a generic non-parametric regressor
v0.8.0,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.8.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.8.0,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.8.0,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.8.0,model = lambda: RandomForestRegressor(n_estimators=100)
v0.8.0,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.8.0,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.8.0,model = lambda: LinearRegression(n_jobs=-1)
v0.8.0,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.8.0,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.8.0,underlying model whenever predict is called.
v0.8.0,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.8.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.8.0,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.8.0,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.8.0,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.8.0,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.8.0,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.8.0,We need to specify models to be used for each of these residualizations
v0.8.0,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.8.0,"E[T | X, Z]"
v0.8.0,E[TZ | X]
v0.8.0,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.8.0,n_splits determines the number of splits to be used for cross-fitting.
v0.8.0,# Algorithm 2 - Current Method
v0.8.0,In[121]:
v0.8.0,# Algorithm 3 - DRIV ATE
v0.8.0,dmliv_model_effect = lambda: model()
v0.8.0,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.8.0,"dmliv_model_effect(),"
v0.8.0,n_splits=1)
v0.8.0,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.8.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.8.0,"Once multiple treatments are supported, we'll need to fix this"
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.8.0,We can use statsmodel for all hypothesis testing capabilities
v0.8.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.8.0,We can use statsmodel for all hypothesis testing capabilities
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,TODO. Deal with multi-class instrument/treatment
v0.8.0,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.8.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.8.0,Estimate p(X) = E[T | X] in cross-fitting manner
v0.8.0,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.8.0,##################
v0.8.0,Global settings #
v0.8.0,##################
v0.8.0,Global plotting controls
v0.8.0,"Control for support size, can control for more"
v0.8.0,#################
v0.8.0,File utilities #
v0.8.0,#################
v0.8.0,#################
v0.8.0,Plotting utils #
v0.8.0,#################
v0.8.0,bias
v0.8.0,var
v0.8.0,rmse
v0.8.0,r2
v0.8.0,Infer feature dimension
v0.8.0,Metrics by support plots
v0.8.0,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.8.0,Vasilis Syrgkanis <vasy@microsoft.com>
v0.8.0,Steven Wu <zhiww@microsoft.com>
v0.8.0,Initialize causal tree parameters
v0.8.0,Create splits of causal tree
v0.8.0,Estimate treatment effects at the leafs
v0.8.0,Compute heterogeneous treatement effect for x's in x_list by finding
v0.8.0,the corresponding split and associating the effect computed on that leaf
v0.8.0,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.8.0,Safety check
v0.8.0,Weighted linear regression
v0.8.0,Calculates weights
v0.8.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.8.0,over all indices
v0.8.0,Similar for `a` weights
v0.8.0,Doesn't have sample weights
v0.8.0,Is a linear model
v0.8.0,Weighted linear regression
v0.8.0,Calculates weights
v0.8.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.8.0,over all indices
v0.8.0,Similar for `a` weights
v0.8.0,normalize weights
v0.8.0,"Split the data in half, train and test"
v0.8.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.8.0,"a function of W, using only the train fold"
v0.8.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.8.0,"Split the data in half, train and test"
v0.8.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.8.0,"a function of W, using only the train fold"
v0.8.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.8.0,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.8.0,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.8.0,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.8.0,"Split the data in half, train and test"
v0.8.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.8.0,"a function of x, using only the train fold"
v0.8.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.8.0,Compute coefficient by OLS on residuals
v0.8.0,"Split the data in half, train and test"
v0.8.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.8.0,"a function of x, using only the train fold"
v0.8.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.8.0,Estimate multipliers for second order orthogonal method
v0.8.0,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.8.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.8.0,Create local sample set
v0.8.0,compute the base estimate for the current node using double ml or second order double ml
v0.8.0,compute the influence functions here that are used for the criterion
v0.8.0,generate random proposals of dimensions to split
v0.8.0,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.8.0,compute criterion for each proposal
v0.8.0,if splitting creates valid leafs in terms of mean leaf size
v0.8.0,Calculate criterion for split
v0.8.0,Else set criterion to infinity so that this split is not chosen
v0.8.0,If no good split was found
v0.8.0,Find split that minimizes criterion
v0.8.0,Set the split attributes at the node
v0.8.0,Create child nodes with corresponding subsamples
v0.8.0,Recursively split children
v0.8.0,Return parent node
v0.8.0,estimate the local parameter at the leaf using the estimate data
v0.8.0,###################
v0.8.0,Argument parsing #
v0.8.0,###################
v0.8.0,#########################################
v0.8.0,Parameters constant across experiments #
v0.8.0,#########################################
v0.8.0,Outcome support
v0.8.0,Treatment support
v0.8.0,Evaluation grid
v0.8.0,Treatment effects array
v0.8.0,Other variables
v0.8.0,##########################
v0.8.0,Data Generating Process #
v0.8.0,##########################
v0.8.0,Log iteration
v0.8.0,"Generate controls, features, treatment and outcome"
v0.8.0,T and Y residuals to be used in later scripts
v0.8.0,Save generated dataset
v0.8.0,#################
v0.8.0,ORF parameters #
v0.8.0,#################
v0.8.0,######################################
v0.8.0,Train and evaluate treatment effect #
v0.8.0,######################################
v0.8.0,########
v0.8.0,Plots #
v0.8.0,########
v0.8.0,###############
v0.8.0,Save results #
v0.8.0,###############
v0.8.0,##############
v0.8.0,Run Rscript #
v0.8.0,##############
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Check inputs
v0.8.0,Check inputs
v0.8.0,"Note: unlike other Metalearners, we don't drop the first column because"
v0.8.0,we concatenate all treatments to the other features;
v0.8.0,"We might want to revisit, though, since it's linearly determined by the others"
v0.8.0,Check inputs
v0.8.0,Check inputs
v0.8.0,Check inputs
v0.8.0,Estimate response function
v0.8.0,Check inputs
v0.8.0,Train model on controls. Assign higher weight to units resembling
v0.8.0,treated units.
v0.8.0,Train model on the treated. Assign higher weight to units resembling
v0.8.0,control units.
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Create splits of causal tree
v0.8.0,Make sure the correct exception is being rethrown
v0.8.0,Must make sure indices are merged correctly
v0.8.0,Convert rows to columns
v0.8.0,Require group assignment t to be one-hot-encoded
v0.8.0,Get predictions for the 2 splits
v0.8.0,Must make sure indices are merged correctly
v0.8.0,Crossfitting
v0.8.0,Compute weighted nuisance estimates
v0.8.0,-------------------------------------------------------------------------------
v0.8.0,Calculate the covariance matrix corresponding to the BLB inference
v0.8.0,
v0.8.0,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.8.0,2. Calculate the weighted moments for each tree slice to create a matrix
v0.8.0,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.8.0,in that slice from the overall parameter estimate.
v0.8.0,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.8.0,-------------------------------------------------------------------------------
v0.8.0,Calclulate covariance matrix through BLB
v0.8.0,Estimators
v0.8.0,OrthoForest parameters
v0.8.0,Sub-forests
v0.8.0,Auxiliary attributes
v0.8.0,Fit check
v0.8.0,TODO: Check performance
v0.8.0,Must normalize weights
v0.8.0,Override the CATE inference options
v0.8.0,Add blb inference to parent's options
v0.8.0,Generate subsample indices
v0.8.0,Build trees in parallel
v0.8.0,Bootstraping has repetitions in tree sample
v0.8.0,Similar for `a` weights
v0.8.0,Bootstraping has repetitions in tree sample
v0.8.0,Define subsample size
v0.8.0,Safety check
v0.8.0,Draw points to create little bags
v0.8.0,Copy and/or define models
v0.8.0,Define nuisance estimators
v0.8.0,Define parameter estimators
v0.8.0,Define
v0.8.0,Need to redefine fit here for auto inference to work due to a quirk in how
v0.8.0,wrap_fit is defined
v0.8.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.8.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.8.0,Override to flatten output if T is flat
v0.8.0,Check that all discrete treatments are represented
v0.8.0,Nuissance estimates evaluated with cross-fitting
v0.8.0,Define 2-fold iterator
v0.8.0,Check if there is only one example of some class
v0.8.0,Define 2-fold iterator
v0.8.0,need safe=False when cloning for WeightedModelWrapper
v0.8.0,Compute residuals
v0.8.0,Compute coefficient by OLS on residuals
v0.8.0,"Parameter returned by LinearRegression is (d_T, )"
v0.8.0,Compute residuals
v0.8.0,Compute coefficient by OLS on residuals
v0.8.0,ell_2 regularization
v0.8.0,Ridge regression estimate
v0.8.0,"Parameter returned is of shape (d_T, )"
v0.8.0,Return moments and gradients
v0.8.0,Compute residuals
v0.8.0,Compute moments
v0.8.0,"Moments shape is (n, d_T)"
v0.8.0,Compute moment gradients
v0.8.0,returns shape-conforming residuals
v0.8.0,Copy and/or define models
v0.8.0,Define parameter estimators
v0.8.0,Define moment and mean gradient estimator
v0.8.0,"Check that T is shape (n, )"
v0.8.0,Check T is numeric
v0.8.0,Train label encoder
v0.8.0,Call `fit` from parent class
v0.8.0,weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
v0.8.0,"fit, we need to set explicitly d_t_in here after super fit is called."
v0.8.0,Override to flatten output if T is flat
v0.8.0,Expand one-hot encoding to include the zero treatment
v0.8.0,"Test that T contains all treatments. If not, return None"
v0.8.0,Nuissance estimates evaluated with cross-fitting
v0.8.0,Define 2-fold iterator
v0.8.0,Check if there is only one example of some class
v0.8.0,No need to crossfit for internal nodes
v0.8.0,Compute partial moments
v0.8.0,"If any of the values in the parameter estimate is nan, return None"
v0.8.0,Compute partial moments
v0.8.0,Compute coefficient by OLS on residuals
v0.8.0,ell_2 regularization
v0.8.0,Ridge regression estimate
v0.8.0,"Parameter returned is of shape (d_T, )"
v0.8.0,Return moments and gradients
v0.8.0,Compute partial moments
v0.8.0,Compute moments
v0.8.0,"Moments shape is (n, d_T-1)"
v0.8.0,Compute moment gradients
v0.8.0,Need to calculate this in an elegant way for when propensity is 0
v0.8.0,This will flatten T
v0.8.0,Check that T is numeric
v0.8.0,Test whether the input estimator is supported
v0.8.0,Calculate confidence intervals for the parameter (marginal effect)
v0.8.0,Calculate confidence intervals for the effect
v0.8.0,Calculate the effects
v0.8.0,Calculate the standard deviations for the effects
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"if both X and W are None, just return a column of ones"
v0.8.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.0,We need to go back to the label representation of the one-hot so as to call
v0.8.0,the classifier.
v0.8.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.0,We need to go back to the label representation of the one-hot so as to call
v0.8.0,the classifier.
v0.8.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.8.0,This works both with our without the weighting trick as the treatments T are unit vector
v0.8.0,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.8.0,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.8.0,both Parametric and Non Parametric DML.
v0.8.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.8.0,for internal use by the library
v0.8.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.8.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.8.0,since we clone it and fit separate copies
v0.8.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.8.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.8.0,TODO: support sample_var
v0.8.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.8.0,since we clone it and fit separate copies
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,#######################################
v0.8.0,Core DML Tests
v0.8.0,#######################################
v0.8.0,How many samples
v0.8.0,How many control features
v0.8.0,How many treatment variables
v0.8.0,Coefficients of how controls affect treatments
v0.8.0,Coefficients of how controls affect outcome
v0.8.0,Treatment effects that we want to estimate
v0.8.0,Run dml estimation
v0.8.0,How many samples
v0.8.0,How many control features
v0.8.0,How many treatment variables
v0.8.0,Coefficients of how controls affect treatments
v0.8.0,Coefficients of how controls affect outcome
v0.8.0,Treatment effects that we want to estimate
v0.8.0,Run dml estimation
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,note that groups are not passed to score because they are only used for fitting
v0.8.0,note that groups are not passed to score because they are only used for fitting
v0.8.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"this will have dimension (d,) + shape(X)"
v0.8.0,send the first dimension to the end
v0.8.0,columns are featurized independently; partial derivatives are only non-zero
v0.8.0,when taken with respect to the same column each time
v0.8.0,don't fit intercept; manually add column of ones to the data instead;
v0.8.0,this allows us to ignore the intercept when computing marginal effects
v0.8.0,make T 2D if if was a vector
v0.8.0,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.8.0,two stage approximation
v0.8.0,"first, get basis expansions of T, X, and Z"
v0.8.0,TODO: is it right that the effective number of intruments is the
v0.8.0,"product of ft_X and ft_Z, not just ft_Z?"
v0.8.0,"regress T expansion on X,Z expansions concatenated with W"
v0.8.0,"predict ft_T from interacted ft_X, ft_Z"
v0.8.0,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.8.0,dT may be only 2-dimensional)
v0.8.0,promote dT to 3D if necessary (e.g. if T was a vector)
v0.8.0,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.8.0,(which needs to have been expanded if there's a discrete treatment)
v0.8.0,We can write effect interval as a function of const_marginal_effect_interval for a single treatment
v0.8.0,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.8.0,d_t=1 here since we measure the effect across all Ts
v0.8.0,once the estimator has been fit
v0.8.0,"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
v0.8.0,an intercept even when bias is part of coef.
v0.8.0,We can write effect inference as a function of prediction and prediction standard error of
v0.8.0,the final method for linear models
v0.8.0,d_t=1 here since we measure the effect across all Ts
v0.8.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.8.0,(which needs to have been expanded if there's a discrete treatment)
v0.8.0,"send treatment to the end, pull bounds to the front"
v0.8.0,d_t=1 here since we measure the effect across all Ts
v0.8.0,need to set the fit args before the estimator is fit
v0.8.0,"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
v0.8.0,1. Uncertainty of Mean Point Estimate
v0.8.0,2. Distribution of Point Estimate
v0.8.0,3. Total Variance of Point Estimate
v0.8.0,"if stderr is zero, ppf will return nans and the loop below would never terminate"
v0.8.0,so bail out early; note that it might be possible to correct the algorithm for
v0.8.0,"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
v0.8.0,be clean
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.8.0,output is
v0.8.0,"* a column of ones if X, W, and Z are all None"
v0.8.0,* just X or W or Z if both of the others are None
v0.8.0,* hstack([arrs]) for whatever subset are not None otherwise
v0.8.0,ensure Z is 2D
v0.8.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.0,We need to go back to the label representation of the one-hot so as to call
v0.8.0,the classifier.
v0.8.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.0,We need to go back to the label representation of the one-hot so as to call
v0.8.0,the classifier.
v0.8.0,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.8.0,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.8.0,since we would actually be calculating a CATE rather than ATE.
v0.8.0,TODO: allow the final model to actually use X?
v0.8.0,TODO: allow the final model to actually use X?
v0.8.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.8.0,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.8.0,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.8.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.8.0,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.8.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.8.0,for internal use by the library
v0.8.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.8.0,Estimate final model of theta(X) by minimizing the square loss:
v0.8.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.8.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.8.0,at the expense of some small bias. For points with very small covariance we revert
v0.8.0,to the model-based preliminary estimate and do not add the correction term.
v0.8.0,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.8.0,"instruments, and outcomes"
v0.8.0,TODO: how do we incorporate the sample_weight and sample_var passed into this method
v0.8.0,as arguments?
v0.8.0,TODO: is there a good way to incorporate the other nuisance terms in the score?
v0.8.0,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.8.0,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.8.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.8.0,for internal use by the library
v0.8.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.8.0,"we need to undo the one-hot encoding for calling effect,"
v0.8.0,since it expects raw values
v0.8.0,"we need to undo the one-hot encoding for calling effect,"
v0.8.0,since it expects raw values
v0.8.0,"TODO: check that Y, T, Z do not have multiple columns"
v0.8.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.8.0,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.8.0,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.8.0,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.8.0,checks that X is 2D array.
v0.8.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.0,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.8.0,"Replacing this method which is invalid for this class, so that we make the"
v0.8.0,dosctring empty and not appear in the docs.
v0.8.0,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.8.0,TODO: support sample_var
v0.8.0,"Replacing this method which is invalid for this class, so that we make the"
v0.8.0,dosctring empty and not appear in the docs.
v0.8.0,Replacing to remove docstring
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,TODO: make sure to use random seeds wherever necessary
v0.8.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.8.0,"unfortunately with the Theano and Tensorflow backends,"
v0.8.0,the straightforward use of K.stop_gradient can cause an error
v0.8.0,because the parameters of the intermediate layers are now disconnected from the loss;
v0.8.0,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.8.0,so that those layers remain connected but with 0 gradient
v0.8.0,|| t - mu_i || ^2
v0.8.0,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.8.0,Use logsumexp for numeric stability:
v0.8.0,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.8.0,TODO: does the numeric stability actually make any difference?
v0.8.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.8.0,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.8.0,generate cumulative sum via matrix multiplication
v0.8.0,"Generate standard uniform values in shape (batch_size,1)"
v0.8.0,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.8.0,we use uniform_like instead with an input of an appropriate shape)
v0.8.0,convert to floats and multiply to perform equivalent of logical AND
v0.8.0,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.8.0,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.8.0,we use normal_like instead with an input of an appropriate shape)
v0.8.0,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.8.0,prevent gradient from passing through sampling
v0.8.0,three options: biased or upper-bound loss require a single number of samples;
v0.8.0,unbiased can take different numbers for the network and its gradient
v0.8.0,"sample: (() -> Layer, int) -> Layer"
v0.8.0,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.8.0,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.8.0,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.8.0,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.8.0,the dimensionality of the output of the network
v0.8.0,TODO: is there a more robust way to do this?
v0.8.0,TODO: do we need to give the user more control over other arguments to fit?
v0.8.0,"subtle point: we need to build a new model each time,"
v0.8.0,because each model encapsulates its randomness
v0.8.0,TODO: do we need to give the user more control over other arguments to fit?
v0.8.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.8.0,not a general tensor (because of how backprop works in every framework)
v0.8.0,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.8.0,but this seems annoying...)
v0.8.0,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.8.0,TODO: any way to get this to work on batches of arbitrary size?
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,TODO: generalize to multiple treatment case?
v0.8.0,get index of best treatment
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,#######################################################
v0.8.0,Perfect Data DGPs for Testing Correctness of Code
v0.8.0,#######################################################
v0.8.0,Generate random control co-variates
v0.8.0,Create epsilon residual treatments that deterministically sum up to
v0.8.0,zero
v0.8.0,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.8.0,conditional on each co-variate vector is equal to zero
v0.8.0,We simply subtract the conditional mean from the epsilons
v0.8.0,Construct treatments as T = X*A + epsilon
v0.8.0,Construct outcomes as y = X*beta + T*effect
v0.8.0,Generate random control co-variates
v0.8.0,Create epsilon residual treatments that deterministically sum up to
v0.8.0,zero
v0.8.0,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.8.0,conditional on each co-variate vector is equal to zero
v0.8.0,We simply subtract the conditional mean from the epsilons
v0.8.0,Construct treatments as T = X*A + epsilon
v0.8.0,Construct outcomes as y = X*beta + T*effect
v0.8.0,Generate random control co-variates
v0.8.0,Construct treatments as T = X*A + epsilon
v0.8.0,Construct outcomes as y = X*beta + T*effect
v0.8.0,Generate random control co-variates
v0.8.0,Create epsilon residual treatments
v0.8.0,Construct treatments as T = X*A + epsilon
v0.8.0,Construct outcomes as y = X*beta + T*effect + eta
v0.8.0,Generate random control co-variates
v0.8.0,Use the same treatment vector for each row
v0.8.0,Construct outcomes as y = X*beta + T*effect
v0.8.0,Licensed under the MIT License.
v0.8.0,"since inference objects can be stateful, we must copy it before fitting;"
v0.8.0,otherwise this sequence wouldn't work:
v0.8.0,"est1.fit(..., inference=inf)"
v0.8.0,"est2.fit(..., inference=inf)"
v0.8.0,est1.effect_interval(...)
v0.8.0,because inf now stores state from fitting est2
v0.8.0,call the wrapped fit method
v0.8.0,NOTE: we call inference fit *after* calling the main fit method
v0.8.0,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.8.0,but tensordot can't be applied to this problem because we don't sum over m
v0.8.0,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.8.0,of rows of T was not taken into account
v0.8.0,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.8.0,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.8.0,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.8.0,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.8.0,add statsmodels to parent's options
v0.8.0,add debiasedlasso to parent's options
v0.8.0,add blb to parent's options
v0.8.0,TODO Share some logic with non-discrete version
v0.8.0,add statsmodels to parent's options
v0.8.0,add statsmodels to parent's options
v0.8.0,add blb to parent's options
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Check if model is sparse enough for this model
v0.8.0,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.8.0,TODO: any way to avoid creating a copy if the array was already dense?
v0.8.0,"the call is necessary if the input was something like a list, though"
v0.8.0,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.8.0,so convert to pydata sparse first
v0.8.0,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.8.0,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.8.0,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.8.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.8.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.8.0,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.8.0,same number of input definitions as arrays
v0.8.0,input definitions have same number of dimensions as each array
v0.8.0,all result indices are unique
v0.8.0,all result indices must match at least one input index
v0.8.0,"map indices to all array, axis pairs for that index"
v0.8.0,each index has the same cardinality wherever it appears
v0.8.0,"State: list of (set of letters, list of (corresponding indices, value))"
v0.8.0,Algo: while list contains more than one entry
v0.8.0,take two entries
v0.8.0,sort both lists by intersection of their indices
v0.8.0,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.8.0,"take the union of indices and the product of values), stepping through each list linearly"
v0.8.0,TODO: might be faster to break into connected components first
v0.8.0,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.8.0,"so compute their content separately, then take cartesian product"
v0.8.0,this would save a few pointless sorts by empty tuples
v0.8.0,TODO: Consider investigating other performance ideas for these cases
v0.8.0,where the dense method beat the sparse method (usually sparse is faster)
v0.8.0,"e,facd,c->cfed"
v0.8.0,sparse: 0.0335489
v0.8.0,dense:  0.011465999999999997
v0.8.0,"gbd,da,egb->da"
v0.8.0,sparse: 0.0791625
v0.8.0,dense:  0.007319099999999995
v0.8.0,"dcc,d,faedb,c->abe"
v0.8.0,sparse: 1.2868097
v0.8.0,dense:  0.44605229999999985
v0.8.0,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.8.0,TODO: would using einsum's paths to optimize the order of merging help?
v0.8.0,assume that we should perform nested cross-validation if and only if
v0.8.0,the model has a 'cv' attribute; this is a somewhat brittle assumption...
v0.8.0,logic copied from check_cv
v0.8.0,otherwise we will assume the user already set the cv attribute to something
v0.8.0,compatible with splitting with a 'groups' argument
v0.8.0,now we have to compute the folds explicitly because some classifiers (like LassoCV)
v0.8.0,don't use the groups when calling split internally
v0.8.0,Normalize weights
v0.8.0,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.8.0,"if we're decorating a class, just update the __init__ method,"
v0.8.0,so that the result is still a class instead of a wrapper method
v0.8.0,"want to enforce that each bad_arg was either in kwargs,"
v0.8.0,or else it was in neither and is just taking its default value
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.8.0,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.8.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.8.0,clean way of achieving this
v0.8.0,make sure we don't accidentally escape anything in the substitution
v0.8.0,Fetch appropriate color for node
v0.8.0,"red for negative, green for positive"
v0.8.0,in multi-target use first target
v0.8.0,Write node mean CATE
v0.8.0,Write node std of CATE
v0.8.0,Write confidence interval information if at leaf node
v0.8.0,Fetch appropriate color for node
v0.8.0,"red for negative, green for positive"
v0.8.0,Write node mean CATE
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,remove None arguments
v0.8.0,"scores entries should be lists of scores, so make each entry a singleton list"
v0.8.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.8.0,use a binary array to get stratified split in case of discrete treatment
v0.8.0,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.8.0,"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
v0.8.0,"however, sklearn doesn't support both stratifying and grouping (see"
v0.8.0,"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
v0.8.0,their own object that supports grouping if they want to use groups.
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Estimators
v0.8.0,Causal tree parameters
v0.8.0,Tree structure
v0.8.0,No need for a random split since the data is already
v0.8.0,a random subsample from the original input
v0.8.0,node list stores the nodes that are yet to be splitted
v0.8.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.8.0,Create local sample set
v0.8.0,Compute nuisance estimates for the current node
v0.8.0,Nuisance estimate cannot be calculated
v0.8.0,Estimate parameter for current node
v0.8.0,Node estimate cannot be calculated
v0.8.0,Calculate moments and gradient of moments for current data
v0.8.0,Calculate inverse gradient
v0.8.0,The gradient matrix is not invertible.
v0.8.0,No good split can be found
v0.8.0,Calculate point-wise pseudo-outcomes rho
v0.8.0,a split is determined by a feature and a sample pair
v0.8.0,the number of possible splits is at most (number of features) * (number of node samples)
v0.8.0,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.8.0,parse row and column of random pair
v0.8.0,the sample of the pair is the integer division of the random number with n_feats
v0.8.0,calculate the binary indicator of whether sample i is on the left or the right
v0.8.0,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.8.0,calculate the number of samples on the left child for each proposed split
v0.8.0,calculate the analogous binary indicator for the samples in the estimation set
v0.8.0,calculate the number of estimation samples on the left child of each proposed split
v0.8.0,find the upper and lower bound on the size of the left split for the split
v0.8.0,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.8.0,on each side.
v0.8.0,similarly for the estimation sample set
v0.8.0,if there is no valid split then don't create any children
v0.8.0,filter only the valid splits
v0.8.0,calculate the average influence vector of the samples in the left child
v0.8.0,calculate the average influence vector of the samples in the right child
v0.8.0,take the square of each of the entries of the influence vectors and normalize
v0.8.0,by size of each child
v0.8.0,calculate the vector score of each candidate split as the average of left and right
v0.8.0,influence vectors
v0.8.0,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.8.0,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.8.0,where there might be large discontinuities in some parameter as the conditioning set varies
v0.8.0,calculate the scalar score of each split by aggregating across the vector of scores
v0.8.0,Find split that minimizes criterion
v0.8.0,Create child nodes with corresponding subsamples
v0.8.0,add the created children to the list of not yet split nodes
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,TODO: Add a __dir__ implementation?
v0.8.0,don't proxy special methods
v0.8.0,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.8.0,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.8.0,then we should be computing a confidence interval for the wrapped calls
v0.8.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
v0.8.0,second level bootstrap which would be prohibitive computationally?
v0.8.0,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.8.0,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.8.0,can't import from econml.inference at top level without creating cyclical dependencies
v0.8.0,Note that inference results are always methods even if the inference is for a property
v0.8.0,(e.g. coef__inference() is a method but coef_ is a property)
v0.8.0,Therefore we must insert a lambda if getting inference for a non-callable
v0.8.0,"If inference is for a property, create a fresh lambda to avoid passing args through"
v0.8.0,"try to get interval/std first if appropriate,"
v0.8.0,since we don't prefer a wrapped method with this name
v0.8.0,AzureML
v0.8.0,helper imports
v0.8.0,write the details of the workspace to a configuration file to the notebook library
v0.8.0,if y is a multioutput model
v0.8.0,Make sure second dimension has 1 or more item
v0.8.0,switch _inner Model to a MultiOutputRegressor
v0.8.0,flatten array as automl only takes vectors for y
v0.8.0,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.8.0,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.8.0,as an sklearn estimator
v0.8.0,fit implementation for a single output model.
v0.8.0,Create experiment for specified workspace
v0.8.0,Configure automl_config with training set information.
v0.8.0,"Wait for remote run to complete, the set the model"
v0.8.0,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.8.0,create model and pass model into final.
v0.8.0,"If item is an automl config, get its corresponding"
v0.8.0,AutomatedML Model and add it to new_Args
v0.8.0,"If item is an automl config, get its corresponding"
v0.8.0,AutomatedML Model and set it for this key in
v0.8.0,kwargs
v0.8.0,takes in either automated_ml config and instantiates
v0.8.0,an AutomatedMLModel
v0.8.0,The prefix can only be 18 characters long
v0.8.0,"because prefixes come from kwarg_names, we must ensure they are"
v0.8.0,short enough.
v0.8.0,Get workspace from config file.
v0.8.0,Take the intersect of the white for sample
v0.8.0,weights and linear models
v0.8.0,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.8.0,Remove children with nonwhite mothers from the treatment group
v0.8.0,Remove children with nonwhite mothers from the treatment group
v0.8.0,Select columns
v0.8.0,Scale the numeric variables
v0.8.0,"Change the binary variable 'first' takes values in {1,2}"
v0.8.0,Append a column of ones as intercept
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.8.0,creating notebooks that are annoying for our users to actually run themselves
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.8.0,"prior to calling interpret, can't plot, render, etc."
v0.8.0,can interpret without uncertainty
v0.8.0,can't interpret with uncertainty if inference wasn't used during fit
v0.8.0,can interpret with uncertainty if we refit
v0.8.0,can interpret without uncertainty
v0.8.0,can't treat before interpreting
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,simple DGP only for illustration
v0.8.0,Define the treatment model neural network architecture
v0.8.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.8.0,"so the input shape is (d_z + d_x,)"
v0.8.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.8.0,add extra layers on top for the mixture density network
v0.8.0,Define the response model neural network architecture
v0.8.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.8.0,"so the input shape is (d_t + d_x,)"
v0.8.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.8.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.8.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.8.0,so that the same weights will be reused in each instantiation
v0.8.0,number of samples to use in second estimate of the response
v0.8.0,(to make loss estimate unbiased)
v0.8.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.8.0,do something with predictions...
v0.8.0,also test vector t and y
v0.8.0,simple DGP only for illustration
v0.8.0,Define the treatment model neural network architecture
v0.8.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.8.0,"so the input shape is (d_z + d_x,)"
v0.8.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.8.0,add extra layers on top for the mixture density network
v0.8.0,Define the response model neural network architecture
v0.8.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.8.0,"so the input shape is (d_t + d_x,)"
v0.8.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.8.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.8.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.8.0,so that the same weights will be reused in each instantiation
v0.8.0,number of samples to use in second estimate of the response
v0.8.0,(to make loss estimate unbiased)
v0.8.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.8.0,do something with predictions...
v0.8.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.8.0,test = True ensures we draw test set images
v0.8.0,test = True ensures we draw test set images
v0.8.0,re-draw to get new independent treatment and implied response
v0.8.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.8.0,above is necesary so that reduced form doesn't win
v0.8.0,covariates: time and emotion
v0.8.0,random instrument
v0.8.0,z -> price
v0.8.0,true observable demand function
v0.8.0,errors
v0.8.0,response
v0.8.0,test = True ensures we draw test set images
v0.8.0,test = True ensures we draw test set images
v0.8.0,re-draw to get new independent treatment and implied response
v0.8.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.8.0,above is necesary so that reduced form doesn't win
v0.8.0,covariates: time and emotion
v0.8.0,random instrument
v0.8.0,z -> price
v0.8.0,true observable demand function
v0.8.0,errors
v0.8.0,response
v0.8.0,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.8.0,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.8.0,For some reason this doesn't work at all when run against the CNTK backend...
v0.8.0,"model.compile('nadam', loss=lambda _,l:l)"
v0.8.0,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.8.0,generate a valiation set
v0.8.0,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.8.0,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.8.0,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.8.0,so we need to transpose the result
v0.8.0,1-d output
v0.8.0,2-d output
v0.8.0,Single dimensional output y
v0.8.0,Multi-dimensional output y
v0.8.0,1-d y
v0.8.0,multi-d y
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,test that we can fit with the same arguments as the base estimator
v0.8.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.8.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0,with the same shape for the lower and upper bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,test that we can do the same thing once we provide percentile bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.8.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0,with the same shape for the lower and upper bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,test that we can do the same thing once we provide percentile bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,test that we can fit with the same arguments as the base estimator
v0.8.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.8.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0,with the same shape for the lower and upper bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,test that we can do the same thing once we provide percentile bounds
v0.8.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.8.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0,with the same shape for the lower and upper bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,test that we can do the same thing once we provide percentile bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,test that we can fit with the same arguments as the base estimator
v0.8.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.8.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0,with the same shape for the lower and upper bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,test that we can do the same thing once we provide percentile bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.8.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0,with the same shape for the lower and upper bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,test that we can do the same thing once we provide percentile bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0,with the same shape for the lower and upper bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,test that the estimated effect is usually within the bounds
v0.8.0,test that we can do the same thing once we provide alpha explicitly
v0.8.0,test that the lower and upper bounds differ
v0.8.0,test that the estimated effect is usually within the bounds
v0.8.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0,with the same shape for the lower and upper bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,TODO: test that the estimated effect is usually within the bounds
v0.8.0,and that the true effect is also usually within the bounds
v0.8.0,test that we can do the same thing once we provide percentile bounds
v0.8.0,test that the lower and upper bounds differ
v0.8.0,TODO: test that the estimated effect is usually within the bounds
v0.8.0,and that the true effect is also usually within the bounds
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,DGP constants
v0.8.0,Generate data
v0.8.0,Test inference results when `cate_feature_names` doesn not exist
v0.8.0,Test inference results when `cate_feature_names` doesn not exist
v0.8.0,"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
v0.8.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.8.0,"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
v0.8.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.8.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.8.0,pvalue for second column should be greater than zero since some points are on either side
v0.8.0,of the tested value
v0.8.0,"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
v0.8.0,"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
v0.8.0,pvalue is also nan when variance is 0 and the point tested is equal to the point tested
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.8.0,Test non keyword based calls to fit
v0.8.0,test non-array inputs
v0.8.0,Test custom splitter
v0.8.0,Test incomplete set of test folds
v0.8.0,"y scores should be positive, since W predicts Y somewhat"
v0.8.0,"t scores might not be, since W and T are uncorrelated"
v0.8.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,make sure cross product varies more slowly with first array
v0.8.0,and that vectors are okay as inputs
v0.8.0,number of inputs in specification must match number of inputs
v0.8.0,must have an output
v0.8.0,output indices must be unique
v0.8.0,output indices must be present in an input
v0.8.0,number of indices must match number of dimensions for each input
v0.8.0,repeated indices must always have consistent sizes
v0.8.0,transpose
v0.8.0,tensordot
v0.8.0,trace
v0.8.0,TODO: set up proper flag for this
v0.8.0,pick indices at random with replacement from the first 7 letters of the alphabet
v0.8.0,"of all of the distinct indices that appear in any input,"
v0.8.0,pick a random subset of them (of size at most 5) to appear in the output
v0.8.0,creating an instance should warn
v0.8.0,using the instance should not warn
v0.8.0,using the deprecated method should warn
v0.8.0,don't warn if b and c are passed by keyword
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Preprocess data
v0.8.0,Convert 'week' to a date
v0.8.0,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.8.0,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.8.0,Take log of price
v0.8.0,Make brand numeric
v0.8.0,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.8.0,which have all zero coeffs
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.8.0,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.8.0,TODO: test something rather than just print...
v0.8.0,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.8.0,pick some arbitrary X
v0.8.0,pick some arbitrary T
v0.8.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,ensure that we've got at least two of every row
v0.8.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.0,need to make sure we get all *joint* combinations
v0.8.0,IntentToTreat only supports binary treatments/instruments
v0.8.0,IntentToTreat only supports binary treatments/instruments
v0.8.0,IntentToTreat requires X
v0.8.0,ensure we can serialize unfit estimator
v0.8.0,these support only W but not X
v0.8.0,"these support only binary, not general discrete T and Z"
v0.8.0,ensure we can serialize fit estimator
v0.8.0,make sure we can call the marginal_effect and effect methods
v0.8.0,TODO: add tests for extra properties like coef_ where they exist
v0.8.0,TODO: add tests for extra properties like coef_ where they exist
v0.8.0,"make sure we can call effect with implied scalar treatments,"
v0.8.0,"no matter the dimensions of T, and also that we warn when there"
v0.8.0,are multiple treatments
v0.8.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.8.0,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.8.0,"however, with custom splits the checking happens in the first stage wrapper"
v0.8.0,where we don't have all of the required information to do this;
v0.8.0,we'd probably need to add it to _crossfit instead
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.8.0,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.8.0,The __warningregistry__'s need to be in a pristine state for tests
v0.8.0,to work properly.
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Set random seed
v0.8.0,Generate data
v0.8.0,DGP constants
v0.8.0,Test data
v0.8.0,Constant treatment effect
v0.8.0,Constant treatment with multi output Y
v0.8.0,Heterogeneous treatment
v0.8.0,Heterogeneous treatment with multi output Y
v0.8.0,TLearner test
v0.8.0,Instantiate TLearner
v0.8.0,Test inputs
v0.8.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.0,Instantiate SLearner
v0.8.0,Test inputs
v0.8.0,Test constant treatment effect
v0.8.0,Test constant treatment effect with multi output Y
v0.8.0,Test heterogeneous treatment effect
v0.8.0,Need interactions between T and features
v0.8.0,Test heterogeneous treatment effect with multi output Y
v0.8.0,Instantiate XLearner
v0.8.0,Test inputs
v0.8.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.0,Instantiate DomainAdaptationLearner
v0.8.0,Test inputs
v0.8.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.0,Get the true treatment effect
v0.8.0,Get the true treatment effect
v0.8.0,Fit learner and get the effect and marginal effect
v0.8.0,Compute treatment effect residuals (absolute)
v0.8.0,Check that at least 90% of predictions are within tolerance interval
v0.8.0,Check whether the output shape is right
v0.8.0,Check that one can pass in regular lists
v0.8.0,Check that it fails correctly if lists of different shape are passed in
v0.8.0,"Check that it works when T, Y have shape (n, 1)"
v0.8.0,Generate covariates
v0.8.0,Generate treatment
v0.8.0,Calculate outcome
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,DGP constants
v0.8.0,Generate data
v0.8.0,Test data
v0.8.0,Remove warnings that might be raised by the models passed into the ORF
v0.8.0,Generate data with continuous treatments
v0.8.0,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.8.0,does not work well with parallelism.
v0.8.0,Test inputs for continuous treatments
v0.8.0,--> Check that one can pass in regular lists
v0.8.0,--> Check that it fails correctly if lists of different shape are passed in
v0.8.0,Check that outputs have the correct shape
v0.8.0,Test continuous treatments with controls
v0.8.0,Test continuous treatments without controls
v0.8.0,Test Causal Forest API
v0.8.0,Generate data with continuous treatments
v0.8.0,Instantiate model with most of the default parameters.
v0.8.0,Test inputs for continuous treatments
v0.8.0,--> Check that one can pass in regular lists
v0.8.0,--> Check that it fails correctly if lists of different shape are passed in
v0.8.0,Check that outputs have the correct shape
v0.8.0,Test continuous treatments with controls
v0.8.0,Test continuous treatments without controls
v0.8.0,Generate data with binary treatments
v0.8.0,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.8.0,does not work well with parallelism.
v0.8.0,Test inputs for binary treatments
v0.8.0,--> Check that one can pass in regular lists
v0.8.0,--> Check that it fails correctly if lists of different shape are passed in
v0.8.0,"--> Check that it works when T, Y have shape (n, 1)"
v0.8.0,"--> Check that it fails correctly when T has shape (n, 2)"
v0.8.0,--> Check that it fails correctly when the treatments are not numeric
v0.8.0,Check that outputs have the correct shape
v0.8.0,Test binary treatments with controls
v0.8.0,Test binary treatments without controls
v0.8.0,Test CausalForest API
v0.8.0,Generate data with binary treatments
v0.8.0,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.8.0,does not work well with parallelism.
v0.8.0,Test inputs for binary treatments
v0.8.0,--> Check that one can pass in regular lists
v0.8.0,--> Check that it fails correctly if lists of different shape are passed in
v0.8.0,"--> Check that it works when T, Y have shape (n, 1)"
v0.8.0,"--> Check that it fails correctly when T has shape (n, 2)"
v0.8.0,--> Check that it fails correctly when the treatments are not numeric
v0.8.0,Check that outputs have the correct shape
v0.8.0,Test binary treatments with controls
v0.8.0,Test binary treatments without controls
v0.8.0,Only applicable to continuous treatments
v0.8.0,Generate data for 2 treatments
v0.8.0,Test multiple treatments with controls
v0.8.0,Test CausalForest API
v0.8.0,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.8.0,The rest for controls. Just as an example.
v0.8.0,Generating A/B test data
v0.8.0,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.8.0,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.8.0,Test causal foret API
v0.8.0,Test Causal Forest API
v0.8.0,Create a wrapper around Lasso that doesn't support weights
v0.8.0,since Lasso does natively support them starting in sklearn 0.23
v0.8.0,Generate data with continuous treatments
v0.8.0,Instantiate model with most of the default parameters
v0.8.0,Compute the treatment effect on test points
v0.8.0,Compute treatment effect residuals
v0.8.0,Multiple treatments
v0.8.0,Allow at most 10% test points to be outside of the tolerance interval
v0.8.0,Compute treatment effect residuals
v0.8.0,Multiple treatments
v0.8.0,Allow at most 20% test points to be outside of the confidence interval
v0.8.0,Check that the intervals are not too wide
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.8.0,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.8.0,ensure that we've got at least 6 of every element
v0.8.0,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.8.0,NOTE: this number may need to change if the default number of folds in
v0.8.0,WeightedStratifiedKFold changes
v0.8.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.0,ensure we can serialize the unfit estimator
v0.8.0,ensure we can pickle the fit estimator
v0.8.0,make sure we can call the marginal_effect and effect methods
v0.8.0,test const marginal inference
v0.8.0,test effect inference
v0.8.0,test marginal effect inference
v0.8.0,test coef__inference and intercept__inference
v0.8.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.8.0,"make sure we can call effect with implied scalar treatments,"
v0.8.0,"no matter the dimensions of T, and also that we warn when there"
v0.8.0,are multiple treatments
v0.8.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.8.0,ensure that we've got at least two of every element
v0.8.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.0,make sure we can call the marginal_effect and effect methods
v0.8.0,test const marginal inference
v0.8.0,test effect inference
v0.8.0,test marginal effect inference
v0.8.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.8.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.8.0,We concatenate the two copies data
v0.8.0,create a simple artificial setup where effect of moving from treatment
v0.8.0,"1 -> 2 is 2,"
v0.8.0,"1 -> 3 is 1, and"
v0.8.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.8.0,"Using an uneven number of examples from different classes,"
v0.8.0,"and having the treatments in non-lexicographic order,"
v0.8.0,Should rule out some basic issues.
v0.8.0,test that we can fit with a KFold instance
v0.8.0,test that we can fit with a train/test iterable
v0.8.0,predetermined splits ensure that all features are seen in each split
v0.8.0,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.8.0,(incorrectly) use a final model with an intercept
v0.8.0,"Because final model is fixed, actual values of T and Y don't matter"
v0.8.0,Ensure reproducibility
v0.8.0,Sparse DGP
v0.8.0,Treatment effect coef
v0.8.0,Other coefs
v0.8.0,Features and controls
v0.8.0,Test sparse estimator
v0.8.0,"--> test coef_, intercept_"
v0.8.0,--> test treatment effects
v0.8.0,Restrict x_test to vectors of norm < 1
v0.8.0,--> check inference
v0.8.0,Check that a majority of true effects lie in the 5-95% CI
v0.8.0,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.8.0,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.8.0,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.8.0,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.8.0,sparse test case: heterogeneous effect by product
v0.8.0,need at least as many rows in e_y as there are distinct columns
v0.8.0,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.8.0,create a simple artificial setup where effect of moving from treatment
v0.8.0,"a -> b is 2,"
v0.8.0,"a -> c is 1, and"
v0.8.0,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.8.0,"Using an uneven number of examples from different classes,"
v0.8.0,"and having the treatments in non-lexicographic order,"
v0.8.0,should rule out some basic issues.
v0.8.0,Note that explicitly specifying the dtype as object is necessary until
v0.8.0,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.8.0,estimated effects should be identical when treatment is explicitly given
v0.8.0,but const_marginal_effect should be reordered based on the explicit cagetories
v0.8.0,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.8.0,test outer grouping
v0.8.0,test nested grouping
v0.8.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.8.0,whichever groups we saw
v0.8.0,test nested grouping
v0.8.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.8.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.8.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.8.0,make sure we warn when using old aliases
v0.8.0,make sure we can use the old alias as a type
v0.8.0,make sure that we can still pickle the old aliases
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Set random seed
v0.8.0,Generate data
v0.8.0,DGP constants
v0.8.0,Test data
v0.8.0,Constant treatment effect and propensity
v0.8.0,Heterogeneous treatment and propensity
v0.8.0,ensure that we've got at least two of every element
v0.8.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.0,ensure that we can serialize unfit estimator
v0.8.0,ensure that we can serialize fit estimator
v0.8.0,make sure we can call the marginal_effect and effect methods
v0.8.0,test const marginal inference
v0.8.0,test effect inference
v0.8.0,test marginal effect inference
v0.8.0,test coef_ and intercept_ inference
v0.8.0,verify we can generate the summary
v0.8.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.8.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.8.0,create a simple artificial setup where effect of moving from treatment
v0.8.0,"1 -> 2 is 2,"
v0.8.0,"1 -> 3 is 1, and"
v0.8.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.8.0,"Using an uneven number of examples from different classes,"
v0.8.0,"and having the treatments in non-lexicographic order,"
v0.8.0,Should rule out some basic issues.
v0.8.0,test that we can fit with a KFold instance
v0.8.0,test that we can fit with a train/test iterable
v0.8.0,"for at least some of the examples, the CI should have nonzero width"
v0.8.0,"for at least some of the examples, the CI should have nonzero width"
v0.8.0,"for at least some of the examples, the CI should have nonzero width"
v0.8.0,test coef__inference function works
v0.8.0,test intercept__inference function works
v0.8.0,test summary function works
v0.8.0,Test inputs
v0.8.0,self._test_inputs(DR_learner)
v0.8.0,Test constant treatment effect
v0.8.0,Test heterogeneous treatment effect
v0.8.0,Test heterogenous treatment effect for W =/= None
v0.8.0,Sparse DGP
v0.8.0,Treatment effect coef
v0.8.0,Other coefs
v0.8.0,Features and controls
v0.8.0,Test sparse estimator
v0.8.0,"--> test coef_, intercept_"
v0.8.0,--> test treatment effects
v0.8.0,Restrict x_test to vectors of norm < 1
v0.8.0,--> check inference
v0.8.0,Check that a majority of true effects lie in the 5-95% CI
v0.8.0,test outer grouping
v0.8.0,"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
v0.8.0,test nested grouping
v0.8.0,ensure that the grouping has worked correctly and we get all 10 copies of the items in
v0.8.0,whichever groups we saw
v0.8.0,test nested grouping
v0.8.0,"by default, we use 5 split cross-validation for our T and Y models"
v0.8.0,but we don't have enough groups here to split both the outer and inner samples with grouping
v0.8.0,TODO: does this imply we should change some defaults to make this more likely to succeed?
v0.8.0,Fit learner and get the effect
v0.8.0,Get the true treatment effect
v0.8.0,Compute treatment effect residuals (absolute)
v0.8.0,Check that at least 90% of predictions are within tolerance interval
v0.8.0,Only for heterogeneous TE
v0.8.0,Fit learner on X and W and get the effect
v0.8.0,Get the true treatment effect
v0.8.0,Compute treatment effect residuals (absolute)
v0.8.0,Check that at least 90% of predictions are within tolerance interval
v0.8.0,Check that one can pass in regular lists
v0.8.0,Check that it fails correctly if lists of different shape are passed in
v0.8.0,Check that it fails when T contains values other than 0 and 1
v0.8.0,"Check that it works when T, Y have shape (n, 1)"
v0.8.0,Generate covariates
v0.8.0,Generate treatment
v0.8.0,Calculate outcome
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,DGP constants
v0.8.0,DGP coefficients
v0.8.0,Generated outcomes
v0.8.0,################
v0.8.0,WeightedLasso #
v0.8.0,################
v0.8.0,Define weights
v0.8.0,Define extended datasets
v0.8.0,Range of alphas
v0.8.0,Compare with Lasso
v0.8.0,--> No intercept
v0.8.0,--> With intercept
v0.8.0,When DGP has no intercept
v0.8.0,When DGP has intercept
v0.8.0,--> Coerce coefficients to be positive
v0.8.0,--> Toggle max_iter & tol
v0.8.0,Define weights
v0.8.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.8.0,Mixed DGP scenario.
v0.8.0,Define extended datasets
v0.8.0,Define weights
v0.8.0,Define multioutput
v0.8.0,##################
v0.8.0,WeightedLassoCV #
v0.8.0,##################
v0.8.0,Define alphas to test
v0.8.0,Compare with LassoCV
v0.8.0,--> No intercept
v0.8.0,--> With intercept
v0.8.0,--> Force parameters to be positive
v0.8.0,Choose a smaller n to speed-up process
v0.8.0,Compare fold weights
v0.8.0,Define weights
v0.8.0,Define extended datasets
v0.8.0,Define splitters
v0.8.0,WeightedKFold splitter
v0.8.0,Map weighted splitter to an extended splitter
v0.8.0,Define alphas to test
v0.8.0,Compare with LassoCV
v0.8.0,--> No intercept
v0.8.0,--> With intercept
v0.8.0,--> Force parameters to be positive
v0.8.0,###########################
v0.8.0,MultiTaskWeightedLassoCV #
v0.8.0,###########################
v0.8.0,Define alphas to test
v0.8.0,Define splitter
v0.8.0,Compare with MultiTaskLassoCV
v0.8.0,--> No intercept
v0.8.0,--> With intercept
v0.8.0,Define weights
v0.8.0,Define extended datasets
v0.8.0,Define splitters
v0.8.0,WeightedKFold splitter
v0.8.0,Map weighted splitter to an extended splitter
v0.8.0,Define alphas to test
v0.8.0,Compare with LassoCV
v0.8.0,--> No intercept
v0.8.0,--> With intercept
v0.8.0,#########################
v0.8.0,WeightedLassoCVWrapper #
v0.8.0,#########################
v0.8.0,perform 1D fit
v0.8.0,perform 2D fit
v0.8.0,################
v0.8.0,DebiasedLasso #
v0.8.0,################
v0.8.0,Test DebiasedLasso without weights
v0.8.0,--> Check debiased coeffcients without intercept
v0.8.0,--> Check debiased coeffcients with intercept
v0.8.0,--> Check 5-95 CI coverage for unit vectors
v0.8.0,Test DebiasedLasso with weights for one DGP
v0.8.0,Define weights
v0.8.0,Define extended datasets
v0.8.0,--> Check debiased coefficients
v0.8.0,Define weights
v0.8.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.8.0,--> Check debiased coeffcients
v0.8.0,Test that attributes propagate correctly
v0.8.0,Test MultiOutputDebiasedLasso without weights
v0.8.0,--> Check debiased coeffcients without intercept
v0.8.0,--> Check debiased coeffcients with intercept
v0.8.0,--> Check CI coverage
v0.8.0,Test MultiOutputDebiasedLasso with weights
v0.8.0,Define weights
v0.8.0,Define extended datasets
v0.8.0,--> Check debiased coefficients
v0.8.0,Unit vectors
v0.8.0,Unit vectors
v0.8.0,Check coeffcients and intercept are the same within tolerance
v0.8.0,Check results are similar with tolerance 1e-6
v0.8.0,Check if multitask
v0.8.0,Check that same alpha is chosen
v0.8.0,Check that the coefficients are similar
v0.8.0,selective ridge has a simple implementation that we can test against
v0.8.0,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.8.0,"it should be the case that when we set fit_intercept to true,"
v0.8.0,it doesn't matter whether the penalized model also fits an intercept or not
v0.8.0,create an extra copy of rows with weight 2
v0.8.0,"instead of a slice, explicitly return an array of indices"
v0.8.0,_penalized_inds is only set during fitting
v0.8.0,cv exists on penalized model
v0.8.0,now we can access _penalized_inds
v0.8.0,check that we can read the cv attribute back out from the underlying model
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Check that the point estimates are the same
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Linear models are required for parametric dml
v0.8.0,sample weighting models are required for nonparametric dml
v0.8.0,Test values
v0.8.0,TLearner test
v0.8.0,Instantiate TLearner
v0.8.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.0,Test constant treatment effect with multi output Y
v0.8.0,Test heterogeneous treatment effect
v0.8.0,Need interactions between T and features
v0.8.0,Test heterogeneous treatment effect with multi output Y
v0.8.0,Instantiate DomainAdaptationLearner
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,"Found a good split, return."
v0.8.0,Record all splits in case the stratification by weight yeilds a worse partition
v0.8.0,Reseed random generator and try again
v0.8.0,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.8.0,"Found a good split, return."
v0.8.0,Did not find a good split
v0.8.0,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.8.0,Return most weight-balanced partition
v0.8.0,Weight stratification algorithm
v0.8.0,Sort weights for weight strata search
v0.8.0,There are some leftover indices that have yet to be assigned
v0.8.0,Append stratum splits to overall splits
v0.8.0,"If classification methods produce multiple columns of output,"
v0.8.0,we need to manually encode classes to ensure consistent column ordering.
v0.8.0,We clone the estimator to make sure that all the folds are
v0.8.0,"independent, and that it is pickle-able."
v0.8.0,Concatenate the predictions
v0.8.0,`predictions` is a list of method outputs from each fold.
v0.8.0,"If each of those is also a list, then treat this as a"
v0.8.0,multioutput-multiclass task. We need to separately concatenate
v0.8.0,the method outputs for each label into an `n_labels` long list.
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Our classes that derive from sklearn ones sometimes include
v0.8.0,inherited docstrings that have embedded doctests; we need the following imports
v0.8.0,so that they don't break.
v0.8.0,"Convert X, y into numpy arrays"
v0.8.0,Define fit parameters
v0.8.0,Some algorithms don't have a check_input option
v0.8.0,Check weights array
v0.8.0,Check that weights are size-compatible
v0.8.0,Normalize inputs
v0.8.0,Weight inputs
v0.8.0,Fit base class without intercept
v0.8.0,Fit Lasso
v0.8.0,Reset intercept
v0.8.0,The intercept is not calculated properly due the sqrt(weights) factor
v0.8.0,so it must be recomputed
v0.8.0,Fit lasso without weights
v0.8.0,Make weighted splitter
v0.8.0,Fit weighted model
v0.8.0,Make weighted splitter
v0.8.0,Fit weighted model
v0.8.0,Select optimal penalty
v0.8.0,Warn about consistency
v0.8.0,"Convert X, y into numpy arrays"
v0.8.0,Fit weighted lasso with user input
v0.8.0,"Center X, y"
v0.8.0,Calculate quantities that will be used later on. Account for centered data
v0.8.0,Calculate coefficient and error variance
v0.8.0,Add coefficient correction
v0.8.0,Set coefficients and intercept standard errors
v0.8.0,Set intercept
v0.8.0,Return alpha to 'auto' state
v0.8.0,"Note that in the case of no intercept, X_offset is 0"
v0.8.0,Calculate the variance of the predictions
v0.8.0,"Note that in the case of no intercept, X_offset is 0"
v0.8.0,Calculate the variance of the predictions
v0.8.0,Calculate prediction confidence intervals
v0.8.0,Assumes flattened y
v0.8.0,Compute weighted residuals
v0.8.0,To be done once per target. Assumes y can be flattened.
v0.8.0,Assumes that X has already been offset
v0.8.0,Special case: n_features=1
v0.8.0,Compute Lasso coefficients for the columns of the design matrix
v0.8.0,Call weighted lasso on reduced design matrix
v0.8.0,Inherit some parameters from the parent
v0.8.0,Weighted tau
v0.8.0,Compute C_hat
v0.8.0,Compute theta_hat
v0.8.0,Allow for single output as well
v0.8.0,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.8.0,Set coef_ attribute
v0.8.0,Set intercept_ attribute
v0.8.0,Set selected_alpha_ attribute
v0.8.0,Set coef_stderr_
v0.8.0,intercept_stderr_
v0.8.0,set model to WeightedLassoCV by default so there's always a model to get and set attributes on
v0.8.0,whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
v0.8.0,(e.g. former has 'positive' and 'precompute' while latter does not)
v0.8.0,set intercept_ attribute
v0.8.0,set coef_ attribute
v0.8.0,set alpha_ attribute
v0.8.0,set alphas_ attribute
v0.8.0,set n_iter_ attribute
v0.8.0,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.8.0,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.8.0,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.8.0,now regress X1 on y - X2 * beta2 to learn beta1
v0.8.0,set coef_ and intercept_ attributes
v0.8.0,Note that the penalized model should *not* have an intercept
v0.8.0,don't proxy special methods
v0.8.0,"don't pass get_params through to model, because that will cause sklearn to clone this"
v0.8.0,regressor incorrectly
v0.8.0,"Note: for known attributes that have been set this method will not be called,"
v0.8.0,so we should just throw here because this is an attribute belonging to this class
v0.8.0,but which hasn't yet been set on this instance
v0.8.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.8.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0,Licensed under the MIT License.
v0.8.0,Construct the subsample of data
v0.8.0,Split into estimation and splitting sample set
v0.8.0,Fit the tree on the splitting sample
v0.8.0,Set the estimation values based on the estimation split
v0.8.0,Apply the trained tree on the estimation sample to get the path for every estimation sample
v0.8.0,Calculate the total weight of estimation samples on each tree node:
v0.8.0,\sum_i sample_weight[i] * 1{i \\in node}
v0.8.0,Calculate the total number of estimation samples on each tree node:
v0.8.0,|node| = \sum_{i} 1{i \\in node}
v0.8.0,Calculate the weighted sum of responses on the estimation sample on each node:
v0.8.0,\sum_{i} sample_weight[i] 1{i \\in node} Y_i
v0.8.0,Calculate the predicted value on each node based on the estimation sample:
v0.8.0,weighted sum of responses / total weight
v0.8.0,"Calculate the criterion on each node based on the estimation sample and for each output dimension,"
v0.8.0,summing the impurity across dimensions.
v0.8.0,First we calculate the difference of observed label y of each node and predicted value for each
v0.8.0,node that the sample falls in: y[i] - value_est[node]
v0.8.0,If criterion is mse then calculate weighted sum of squared differences for each node
v0.8.0,If criterion is mae then calculate weighted sum of absolute differences for each node
v0.8.0,Normalize each weighted sum of criterion for each node by the total weight of each node
v0.8.0,Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
v0.8.0,and for each un-pruned tree set the value and the weight appropriately.
v0.8.0,If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
v0.8.0,"sample, then prune the whole sub-tree"
v0.8.0,Set the numerator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
v0.8.0,Set the value of the node to:
v0.8.0,\sum_{i} sample_weight[i] 1{i \\in node} Y_i / \sum_{i} sample_weight[i] 1{i \\in node}
v0.8.0,Set the denominator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
v0.8.0,Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node}
v0.8.0,Set the count to the estimation split count
v0.8.0,Set the node impurity to the estimation split impurity
v0.8.0,Validate or convert input data
v0.8.0,Pre-sort indices to avoid that each individual tree of the
v0.8.0,ensemble sorts the indices.
v0.8.0,Remap output
v0.8.0,reshape is necessary to preserve the data contiguity against vs
v0.8.0,"[:, np.newaxis] that does not."
v0.8.0,Check parameters
v0.8.0,"Free allocated memory, if any"
v0.8.0,We draw from the random state to get the random state we
v0.8.0,would have got if we hadn't used a warm_start.
v0.8.0,Parallel loop: we prefer the threading backend as the Cython code
v0.8.0,for fitting the trees is internally releasing the Python GIL
v0.8.0,making threading more efficient than multiprocessing in
v0.8.0,"that case. However, for joblib 0.12+ we respect any"
v0.8.0,"parallel_backend contexts set at a higher level,"
v0.8.0,since correctness does not rely on using threads.
v0.8.0,TODO. This slicing should ultimately be done inside the parallel function
v0.8.0,so that we don't need to create a matrix of size roughly n_samples * n_estimators
v0.8.0,Collect newly grown trees
v0.8.0,Helper class that accumulates an arbitrary function in parallel on the accumulator acc
v0.8.0,and calls the function fn on each tree e and returns the mean output. The function fn
v0.8.0,should take as input a tree e and associated numerator n and denominator d structures and
v0.8.0,"return another function g_e, which takes as input X, check_input"
v0.8.0,"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
v0.8.0,index start to index end will be used. The returned result is essentially:
v0.8.0,(mean over e in slice)(g_e(X)).
v0.8.0,Check data
v0.8.0,Assign chunk of trees to jobs
v0.8.0,Check data
v0.8.0,Check data
v0.8.0,avoid storing the output of every estimator by summing them here
v0.8.0,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
v0.8.0,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
v0.8.0,"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
v0.8.0,where \theta(X) is the point estimate using the whole forest
v0.8.0,Calculate the variance of the latter as E[Q(S)^2]
v0.8.0b1,configuration is all pulled from setup.cfg
v0.8.0b1,-*- coding: utf-8 -*-
v0.8.0b1,
v0.8.0b1,Configuration file for the Sphinx documentation builder.
v0.8.0b1,
v0.8.0b1,This file does only contain a selection of the most common options. For a
v0.8.0b1,full list see the documentation:
v0.8.0b1,http://www.sphinx-doc.org/en/master/config
v0.8.0b1,-- Path setup --------------------------------------------------------------
v0.8.0b1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.8.0b1,add these directories to sys.path here. If the directory is relative to the
v0.8.0b1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.8.0b1,
v0.8.0b1,-- Project information -----------------------------------------------------
v0.8.0b1,-- General configuration ---------------------------------------------------
v0.8.0b1,"If your documentation needs a minimal Sphinx version, state it here."
v0.8.0b1,
v0.8.0b1,needs_sphinx = '1.0'
v0.8.0b1,"Add any Sphinx extension module names here, as strings. They can be"
v0.8.0b1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.8.0b1,ones.
v0.8.0b1,"Add any paths that contain templates here, relative to this directory."
v0.8.0b1,The suffix(es) of source filenames.
v0.8.0b1,You can specify multiple suffix as a list of string:
v0.8.0b1,
v0.8.0b1,"source_suffix = ['.rst', '.md']"
v0.8.0b1,The master toctree document.
v0.8.0b1,The language for content autogenerated by Sphinx. Refer to documentation
v0.8.0b1,for a list of supported languages.
v0.8.0b1,
v0.8.0b1,This is also used if you do content translation via gettext catalogs.
v0.8.0b1,"Usually you set ""language"" from the command line for these cases."
v0.8.0b1,"List of patterns, relative to source directory, that match files and"
v0.8.0b1,directories to ignore when looking for source files.
v0.8.0b1,This pattern also affects html_static_path and html_extra_path.
v0.8.0b1,The name of the Pygments (syntax highlighting) style to use.
v0.8.0b1,-- Options for HTML output -------------------------------------------------
v0.8.0b1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.8.0b1,a list of builtin themes.
v0.8.0b1,
v0.8.0b1,Theme options are theme-specific and customize the look and feel of a theme
v0.8.0b1,"further.  For a list of options available for each theme, see the"
v0.8.0b1,documentation.
v0.8.0b1,
v0.8.0b1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.8.0b1,"relative to this directory. They are copied after the builtin static files,"
v0.8.0b1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.8.0b1,html_static_path = ['_static']
v0.8.0b1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.8.0b1,to template names.
v0.8.0b1,
v0.8.0b1,The default sidebars (for documents that don't match any pattern) are
v0.8.0b1,defined by theme itself.  Builtin themes are using these templates by
v0.8.0b1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.8.0b1,'searchbox.html']``.
v0.8.0b1,
v0.8.0b1,html_sidebars = {}
v0.8.0b1,-- Options for HTMLHelp output ---------------------------------------------
v0.8.0b1,Output file base name for HTML help builder.
v0.8.0b1,-- Options for LaTeX output ------------------------------------------------
v0.8.0b1,The paper size ('letterpaper' or 'a4paper').
v0.8.0b1,
v0.8.0b1,"'papersize': 'letterpaper',"
v0.8.0b1,"The font size ('10pt', '11pt' or '12pt')."
v0.8.0b1,
v0.8.0b1,"'pointsize': '10pt',"
v0.8.0b1,Additional stuff for the LaTeX preamble.
v0.8.0b1,
v0.8.0b1,"'preamble': '',"
v0.8.0b1,Latex figure (float) alignment
v0.8.0b1,
v0.8.0b1,"'figure_align': 'htbp',"
v0.8.0b1,Grouping the document tree into LaTeX files. List of tuples
v0.8.0b1,"(source start file, target name, title,"
v0.8.0b1,"author, documentclass [howto, manual, or own class])."
v0.8.0b1,-- Options for manual page output ------------------------------------------
v0.8.0b1,One entry per manual page. List of tuples
v0.8.0b1,"(source start file, name, description, authors, manual section)."
v0.8.0b1,-- Options for Texinfo output ----------------------------------------------
v0.8.0b1,Grouping the document tree into Texinfo files. List of tuples
v0.8.0b1,"(source start file, target name, title, author,"
v0.8.0b1,"dir menu entry, description, category)"
v0.8.0b1,-- Options for Epub output -------------------------------------------------
v0.8.0b1,Bibliographic Dublin Core info.
v0.8.0b1,The unique identifier of the text. This can be a ISBN number
v0.8.0b1,or the project homepage.
v0.8.0b1,
v0.8.0b1,epub_identifier = ''
v0.8.0b1,A unique identification for the text.
v0.8.0b1,
v0.8.0b1,epub_uid = ''
v0.8.0b1,A list of files that should not be packed into the epub file.
v0.8.0b1,-- Extension configuration -------------------------------------------------
v0.8.0b1,-- Options for intersphinx extension ---------------------------------------
v0.8.0b1,Example configuration for intersphinx: refer to the Python standard library.
v0.8.0b1,-- Options for todo extension ----------------------------------------------
v0.8.0b1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.8.0b1,-- Options for doctest extension -------------------------------------------
v0.8.0b1,we can document otherwise excluded entities here by returning False
v0.8.0b1,or skip otherwise included entities by returning True
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Calculate residuals
v0.8.0b1,Estimate E[T_res | Z_res]
v0.8.0b1,TODO. Deal with multi-class instrument
v0.8.0b1,Calculate nuisances
v0.8.0b1,Estimate E[T_res | Z_res]
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.8.0b1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.8.0b1,TODO. Deal with multi-class instrument
v0.8.0b1,Estimate final model of theta(X) by minimizing the square loss:
v0.8.0b1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.8.0b1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.8.0b1,at the expense of some small bias. For points with very small covariance we revert
v0.8.0b1,to the model-based preliminary estimate and do not add the correction term.
v0.8.0b1,Estimate preliminary theta in cross fitting manner
v0.8.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.8.0b1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.8.0b1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.8.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.8.0b1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.8.0b1,TODO. The solution below is not really a valid cross-fitting
v0.8.0b1,as the test data are used to create the proj_t on the train
v0.8.0b1,which in the second train-test loop is used to create the nuisance
v0.8.0b1,cov on the test data. Hence the T variable of some sample
v0.8.0b1,"is implicitly correlated with its cov nuisance, through this flow"
v0.8.0b1,"of information. However, this seems a rather weak correlation."
v0.8.0b1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.8.0b1,model.
v0.8.0b1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.8.0b1,Estimate preliminary theta in cross fitting manner
v0.8.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.8.0b1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.8.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.8.0b1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.8.0b1,#############################################################################
v0.8.0b1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.8.0b1,A/B test
v0.8.0b1,#############################################################################
v0.8.0b1,Estimate preliminary theta in cross fitting manner
v0.8.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.8.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.8.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.8.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.8.0b1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.8.0b1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.8.0b1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.8.0b1,model_T_XZ = lambda: model_clf()
v0.8.0b1,#'days_visited': lambda:
v0.8.0b1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.8.0b1,Turn strings into categories for numeric mapping
v0.8.0b1,### Defining some generic regressors and classifiers
v0.8.0b1,This a generic non-parametric regressor
v0.8.0b1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.8.0b1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.8.0b1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.8.0b1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.8.0b1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.8.0b1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.8.0b1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.8.0b1,model = lambda: LinearRegression(n_jobs=-1)
v0.8.0b1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.8.0b1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.8.0b1,underlying model whenever predict is called.
v0.8.0b1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.8.0b1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.8.0b1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.8.0b1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.8.0b1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.8.0b1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.8.0b1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.8.0b1,We need to specify models to be used for each of these residualizations
v0.8.0b1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.8.0b1,"E[T | X, Z]"
v0.8.0b1,E[TZ | X]
v0.8.0b1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.8.0b1,n_splits determines the number of splits to be used for cross-fitting.
v0.8.0b1,# Algorithm 2 - Current Method
v0.8.0b1,In[121]:
v0.8.0b1,# Algorithm 3 - DRIV ATE
v0.8.0b1,dmliv_model_effect = lambda: model()
v0.8.0b1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.8.0b1,"dmliv_model_effect(),"
v0.8.0b1,n_splits=1)
v0.8.0b1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.8.0b1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.8.0b1,"Once multiple treatments are supported, we'll need to fix this"
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.8.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.8.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.8.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,TODO. Deal with multi-class instrument/treatment
v0.8.0b1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.8.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.8.0b1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.8.0b1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.8.0b1,##################
v0.8.0b1,Global settings #
v0.8.0b1,##################
v0.8.0b1,Global plotting controls
v0.8.0b1,"Control for support size, can control for more"
v0.8.0b1,#################
v0.8.0b1,File utilities #
v0.8.0b1,#################
v0.8.0b1,#################
v0.8.0b1,Plotting utils #
v0.8.0b1,#################
v0.8.0b1,bias
v0.8.0b1,var
v0.8.0b1,rmse
v0.8.0b1,r2
v0.8.0b1,Infer feature dimension
v0.8.0b1,Metrics by support plots
v0.8.0b1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.8.0b1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.8.0b1,Steven Wu <zhiww@microsoft.com>
v0.8.0b1,Initialize causal tree parameters
v0.8.0b1,Create splits of causal tree
v0.8.0b1,Estimate treatment effects at the leafs
v0.8.0b1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.8.0b1,the corresponding split and associating the effect computed on that leaf
v0.8.0b1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.8.0b1,Safety check
v0.8.0b1,Weighted linear regression
v0.8.0b1,Calculates weights
v0.8.0b1,Bootstraping has repetitions in tree sample so we need to iterate
v0.8.0b1,over all indices
v0.8.0b1,Similar for `a` weights
v0.8.0b1,Doesn't have sample weights
v0.8.0b1,Is a linear model
v0.8.0b1,Weighted linear regression
v0.8.0b1,Calculates weights
v0.8.0b1,Bootstraping has repetitions in tree sample so we need to iterate
v0.8.0b1,over all indices
v0.8.0b1,Similar for `a` weights
v0.8.0b1,normalize weights
v0.8.0b1,"Split the data in half, train and test"
v0.8.0b1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.8.0b1,"a function of W, using only the train fold"
v0.8.0b1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.8.0b1,"Split the data in half, train and test"
v0.8.0b1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.8.0b1,"a function of W, using only the train fold"
v0.8.0b1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.8.0b1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.8.0b1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.8.0b1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.8.0b1,"Split the data in half, train and test"
v0.8.0b1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.8.0b1,"a function of x, using only the train fold"
v0.8.0b1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.8.0b1,Compute coefficient by OLS on residuals
v0.8.0b1,"Split the data in half, train and test"
v0.8.0b1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.8.0b1,"a function of x, using only the train fold"
v0.8.0b1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.8.0b1,Estimate multipliers for second order orthogonal method
v0.8.0b1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.8.0b1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.8.0b1,Create local sample set
v0.8.0b1,compute the base estimate for the current node using double ml or second order double ml
v0.8.0b1,compute the influence functions here that are used for the criterion
v0.8.0b1,generate random proposals of dimensions to split
v0.8.0b1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.8.0b1,compute criterion for each proposal
v0.8.0b1,if splitting creates valid leafs in terms of mean leaf size
v0.8.0b1,Calculate criterion for split
v0.8.0b1,Else set criterion to infinity so that this split is not chosen
v0.8.0b1,If no good split was found
v0.8.0b1,Find split that minimizes criterion
v0.8.0b1,Set the split attributes at the node
v0.8.0b1,Create child nodes with corresponding subsamples
v0.8.0b1,Recursively split children
v0.8.0b1,Return parent node
v0.8.0b1,estimate the local parameter at the leaf using the estimate data
v0.8.0b1,###################
v0.8.0b1,Argument parsing #
v0.8.0b1,###################
v0.8.0b1,#########################################
v0.8.0b1,Parameters constant across experiments #
v0.8.0b1,#########################################
v0.8.0b1,Outcome support
v0.8.0b1,Treatment support
v0.8.0b1,Evaluation grid
v0.8.0b1,Treatment effects array
v0.8.0b1,Other variables
v0.8.0b1,##########################
v0.8.0b1,Data Generating Process #
v0.8.0b1,##########################
v0.8.0b1,Log iteration
v0.8.0b1,"Generate controls, features, treatment and outcome"
v0.8.0b1,T and Y residuals to be used in later scripts
v0.8.0b1,Save generated dataset
v0.8.0b1,#################
v0.8.0b1,ORF parameters #
v0.8.0b1,#################
v0.8.0b1,######################################
v0.8.0b1,Train and evaluate treatment effect #
v0.8.0b1,######################################
v0.8.0b1,########
v0.8.0b1,Plots #
v0.8.0b1,########
v0.8.0b1,###############
v0.8.0b1,Save results #
v0.8.0b1,###############
v0.8.0b1,##############
v0.8.0b1,Run Rscript #
v0.8.0b1,##############
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Check inputs
v0.8.0b1,Check inputs
v0.8.0b1,"Note: unlike other Metalearners, we don't drop the first column because"
v0.8.0b1,we concatenate all treatments to the other features;
v0.8.0b1,"We might want to revisit, though, since it's linearly determined by the others"
v0.8.0b1,Check inputs
v0.8.0b1,Check inputs
v0.8.0b1,Check inputs
v0.8.0b1,Estimate response function
v0.8.0b1,Check inputs
v0.8.0b1,Train model on controls. Assign higher weight to units resembling
v0.8.0b1,treated units.
v0.8.0b1,Train model on the treated. Assign higher weight to units resembling
v0.8.0b1,control units.
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Create splits of causal tree
v0.8.0b1,Make sure the correct exception is being rethrown
v0.8.0b1,Must make sure indices are merged correctly
v0.8.0b1,Require group assignment t to be one-hot-encoded
v0.8.0b1,Define an inner function that iterates over group predictions
v0.8.0b1,Convert rows to columns
v0.8.0b1,Get predictions for the 2 splits
v0.8.0b1,Must make sure indices are merged correctly
v0.8.0b1,Estimators
v0.8.0b1,OrthoForest parameters
v0.8.0b1,Sub-forests
v0.8.0b1,Auxiliary attributes
v0.8.0b1,Fit check
v0.8.0b1,TODO: Check performance
v0.8.0b1,Override the CATE inference options
v0.8.0b1,Add blb inference to parent's options
v0.8.0b1,Must normalize weights
v0.8.0b1,Crossfitting
v0.8.0b1,Compute weighted nuisance estimates
v0.8.0b1,-------------------------------------------------------------------------------
v0.8.0b1,Calculate the covariance matrix corresponding to the BLB inference
v0.8.0b1,
v0.8.0b1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.8.0b1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.8.0b1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.8.0b1,in that slice from the overall parameter estimate.
v0.8.0b1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.8.0b1,-------------------------------------------------------------------------------
v0.8.0b1,Calclulate covariance matrix through BLB
v0.8.0b1,Generate subsample indices
v0.8.0b1,Build trees in parallel
v0.8.0b1,Bootstraping has repetitions in tree sample
v0.8.0b1,Similar for `a` weights
v0.8.0b1,Bootstraping has repetitions in tree sample
v0.8.0b1,Define subsample size
v0.8.0b1,Safety check
v0.8.0b1,Draw points to create little bags
v0.8.0b1,Copy and/or define models
v0.8.0b1,Define nuisance estimators
v0.8.0b1,Define parameter estimators
v0.8.0b1,Define
v0.8.0b1,Override to flatten output if T is flat
v0.8.0b1,T is flat
v0.8.0b1,Nuissance estimates evaluated with cross-fitting
v0.8.0b1,Define 2-fold iterator
v0.8.0b1,need safe=False when cloning for WeightedModelWrapper
v0.8.0b1,Compute residuals
v0.8.0b1,Compute coefficient by OLS on residuals
v0.8.0b1,"Parameter returned by LinearRegression is (d_T, )"
v0.8.0b1,Compute residuals
v0.8.0b1,Compute coefficient by OLS on residuals
v0.8.0b1,ell_2 regularization
v0.8.0b1,Ridge regression estimate
v0.8.0b1,"Parameter returned is of shape (d_T, )"
v0.8.0b1,Return moments and gradients
v0.8.0b1,Compute residuals
v0.8.0b1,Compute moments
v0.8.0b1,"Moments shape is (n, d_T)"
v0.8.0b1,Compute moment gradients
v0.8.0b1,Copy and/or define models
v0.8.0b1,Nuisance estimators shall be defined during fitting because they need to know the number of distinct
v0.8.0b1,treatments
v0.8.0b1,Define parameter estimators
v0.8.0b1,Define moment and mean gradient estimator
v0.8.0b1,"Check that T is shape (n, )"
v0.8.0b1,Check T is numeric
v0.8.0b1,Train label encoder
v0.8.0b1,Define number of classes
v0.8.0b1,Call `fit` from parent class
v0.8.0b1,"Test that T contains all treatments. If not, return None"
v0.8.0b1,Nuissance estimates evaluated with cross-fitting
v0.8.0b1,Define 2-fold iterator
v0.8.0b1,Check if there is only one example of some class
v0.8.0b1,No need to crossfit for internal nodes
v0.8.0b1,Compute partial moments
v0.8.0b1,"If any of the values in the parameter estimate is nan, return None"
v0.8.0b1,Compute partial moments
v0.8.0b1,Compute coefficient by OLS on residuals
v0.8.0b1,ell_2 regularization
v0.8.0b1,Ridge regression estimate
v0.8.0b1,"Parameter returned is of shape (d_T, )"
v0.8.0b1,Return moments and gradients
v0.8.0b1,Compute partial moments
v0.8.0b1,Compute moments
v0.8.0b1,"Moments shape is (n, d_T-1)"
v0.8.0b1,Compute moment gradients
v0.8.0b1,Need to calculate this in an elegant way for when propensity is 0
v0.8.0b1,This will flatten T
v0.8.0b1,Check that T is numeric
v0.8.0b1,Test whether the input estimator is supported
v0.8.0b1,Test expansion of treatment
v0.8.0b1,"If expanded treatments are a vector, flatten const_marginal_effect_interval"
v0.8.0b1,Calculate confidence intervals for the parameter (marginal effect)
v0.8.0b1,"If T is a vector, preserve shape of the effect interval"
v0.8.0b1,Calculate confidence intervals for the effect
v0.8.0b1,Calculate the effects
v0.8.0b1,Calculate the standard deviations for the effects
v0.8.0b1,"Get a list of (parameter, covariance matrix) pairs"
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"if both X and W are None, just return a column of ones"
v0.8.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.0b1,We need to go back to the label representation of the one-hot so as to call
v0.8.0b1,the classifier.
v0.8.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.0b1,We need to go back to the label representation of the one-hot so as to call
v0.8.0b1,the classifier.
v0.8.0b1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.8.0b1,This works both with our without the weighting trick as the treatments T are unit vector
v0.8.0b1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.8.0b1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.8.0b1,both Parametric and Non Parametric DML.
v0.8.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.8.0b1,for internal use by the library
v0.8.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.8.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.8.0b1,since we clone it and fit separate copies
v0.8.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.8.0b1,TODO: support sample_var
v0.8.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.8.0b1,since we clone it and fit separate copies
v0.8.0b1,add statsmodels to parent's options
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,#######################################
v0.8.0b1,Core DML Tests
v0.8.0b1,#######################################
v0.8.0b1,How many samples
v0.8.0b1,How many control features
v0.8.0b1,How many treatment variables
v0.8.0b1,Coefficients of how controls affect treatments
v0.8.0b1,Coefficients of how controls affect outcome
v0.8.0b1,Treatment effects that we want to estimate
v0.8.0b1,Run dml estimation
v0.8.0b1,How many samples
v0.8.0b1,How many control features
v0.8.0b1,How many treatment variables
v0.8.0b1,Coefficients of how controls affect treatments
v0.8.0b1,Coefficients of how controls affect outcome
v0.8.0b1,Treatment effects that we want to estimate
v0.8.0b1,Run dml estimation
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.0b1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"this will have dimension (d,) + shape(X)"
v0.8.0b1,send the first dimension to the end
v0.8.0b1,columns are featurized independently; partial derivatives are only non-zero
v0.8.0b1,when taken with respect to the same column each time
v0.8.0b1,don't fit intercept; manually add column of ones to the data instead;
v0.8.0b1,this allows us to ignore the intercept when computing marginal effects
v0.8.0b1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.8.0b1,store number of columns of T so that we can pass scalars to effect
v0.8.0b1,TODO: support vector T and Y
v0.8.0b1,two stage approximation
v0.8.0b1,"first, get basis expansions of T, X, and Z"
v0.8.0b1,TODO: is it right that the effective number of intruments is the
v0.8.0b1,"product of ft_X and ft_Z, not just ft_Z?"
v0.8.0b1,"regress T expansion on X,Z expansions concatenated with W"
v0.8.0b1,"predict ft_T from interacted ft_X, ft_Z"
v0.8.0b1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.8.0b1,dT may be only 2-dimensional)
v0.8.0b1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.8.0b1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.8.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.8.0b1,We can write effect interval as a function of const_marginal_effect_interval for a single treatment
v0.8.0b1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.8.0b1,d_t=1 here since we measure the effect across all Ts
v0.8.0b1,once the estimator has been fit
v0.8.0b1,We can write effect interval as a function of predict_interval of the final method for linear models
v0.8.0b1,We can write effect inference as a function of prediction and prediction standard error of
v0.8.0b1,the final method for linear models
v0.8.0b1,d_t=1 here since we measure the effect across all Ts
v0.8.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.8.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.8.0b1,"send treatment to the end, pull bounds to the front"
v0.8.0b1,d_t=1 here since we measure the effect across all Ts
v0.8.0b1,need to set the fit args before the estimator is fit
v0.8.0b1,1. Uncertainty of Mean Point Estimate
v0.8.0b1,2. Distribution of Point Estimate
v0.8.0b1,3. Total Variance of Point Estimate
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
v0.8.0b1,output is
v0.8.0b1,"* a column of ones if X, W, and Z are all None"
v0.8.0b1,* just X or W or Z if both of the others are None
v0.8.0b1,* hstack([arrs]) for whatever subset are not None otherwise
v0.8.0b1,ensure Z is 2D
v0.8.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.0b1,We need to go back to the label representation of the one-hot so as to call
v0.8.0b1,the classifier.
v0.8.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.8.0b1,We need to go back to the label representation of the one-hot so as to call
v0.8.0b1,the classifier.
v0.8.0b1,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.8.0b1,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.8.0b1,since we would actually be calculating a CATE rather than ATE.
v0.8.0b1,TODO: allow the final model to actually use X?
v0.8.0b1,TODO: allow the final model to actually use X?
v0.8.0b1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.8.0b1,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.8.0b1,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.8.0b1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.8.0b1,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.8.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.8.0b1,for internal use by the library
v0.8.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.8.0b1,Estimate final model of theta(X) by minimizing the square loss:
v0.8.0b1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.8.0b1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.8.0b1,at the expense of some small bias. For points with very small covariance we revert
v0.8.0b1,to the model-based preliminary estimate and do not add the correction term.
v0.8.0b1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.8.0b1,"instruments, and outcomes"
v0.8.0b1,TODO: how do we incorporate the sample_weight and sample_var passed into this method
v0.8.0b1,as arguments?
v0.8.0b1,TODO: is there a good way to incorporate the other nuisance terms in the score?
v0.8.0b1,"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
v0.8.0b1,"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
v0.8.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.8.0b1,for internal use by the library
v0.8.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.8.0b1,"we need to undo the one-hot encoding for calling effect,"
v0.8.0b1,since it expects raw values
v0.8.0b1,"we need to undo the one-hot encoding for calling effect,"
v0.8.0b1,since it expects raw values
v0.8.0b1,"TODO: check that Y, T, Z do not have multiple columns"
v0.8.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.8.0b1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.8.0b1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.8.0b1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.8.0b1,checks that X is 2D array.
v0.8.0b1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.0b1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.8.0b1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.8.0b1,"Replacing this method which is invalid for this class, so that we make the"
v0.8.0b1,dosctring empty and not appear in the docs.
v0.8.0b1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.8.0b1,TODO: support sample_var
v0.8.0b1,"Replacing this method which is invalid for this class, so that we make the"
v0.8.0b1,dosctring empty and not appear in the docs.
v0.8.0b1,add statsmodels to parent's options
v0.8.0b1,Replacing to remove docstring
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,TODO: make sure to use random seeds wherever necessary
v0.8.0b1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.8.0b1,"unfortunately with the Theano and Tensorflow backends,"
v0.8.0b1,the straightforward use of K.stop_gradient can cause an error
v0.8.0b1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.8.0b1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.8.0b1,so that those layers remain connected but with 0 gradient
v0.8.0b1,|| t - mu_i || ^2
v0.8.0b1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.8.0b1,Use logsumexp for numeric stability:
v0.8.0b1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.8.0b1,TODO: does the numeric stability actually make any difference?
v0.8.0b1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.8.0b1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.8.0b1,generate cumulative sum via matrix multiplication
v0.8.0b1,"Generate standard uniform values in shape (batch_size,1)"
v0.8.0b1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.8.0b1,we use uniform_like instead with an input of an appropriate shape)
v0.8.0b1,convert to floats and multiply to perform equivalent of logical AND
v0.8.0b1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.8.0b1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.8.0b1,we use normal_like instead with an input of an appropriate shape)
v0.8.0b1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.8.0b1,prevent gradient from passing through sampling
v0.8.0b1,three options: biased or upper-bound loss require a single number of samples;
v0.8.0b1,unbiased can take different numbers for the network and its gradient
v0.8.0b1,"sample: (() -> Layer, int) -> Layer"
v0.8.0b1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.8.0b1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.8.0b1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.8.0b1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.8.0b1,the dimensionality of the output of the network
v0.8.0b1,TODO: is there a more robust way to do this?
v0.8.0b1,TODO: do we need to give the user more control over other arguments to fit?
v0.8.0b1,"subtle point: we need to build a new model each time,"
v0.8.0b1,because each model encapsulates its randomness
v0.8.0b1,TODO: do we need to give the user more control over other arguments to fit?
v0.8.0b1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.8.0b1,not a general tensor (because of how backprop works in every framework)
v0.8.0b1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.8.0b1,but this seems annoying...)
v0.8.0b1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.8.0b1,TODO: any way to get this to work on batches of arbitrary size?
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,TODO: generalize to multiple treatment case?
v0.8.0b1,get index of best treatment
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,#######################################################
v0.8.0b1,Perfect Data DGPs for Testing Correctness of Code
v0.8.0b1,#######################################################
v0.8.0b1,Generate random control co-variates
v0.8.0b1,Create epsilon residual treatments that deterministically sum up to
v0.8.0b1,zero
v0.8.0b1,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.8.0b1,conditional on each co-variate vector is equal to zero
v0.8.0b1,We simply subtract the conditional mean from the epsilons
v0.8.0b1,Construct treatments as T = X*A + epsilon
v0.8.0b1,Construct outcomes as y = X*beta + T*effect
v0.8.0b1,Generate random control co-variates
v0.8.0b1,Create epsilon residual treatments that deterministically sum up to
v0.8.0b1,zero
v0.8.0b1,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.8.0b1,conditional on each co-variate vector is equal to zero
v0.8.0b1,We simply subtract the conditional mean from the epsilons
v0.8.0b1,Construct treatments as T = X*A + epsilon
v0.8.0b1,Construct outcomes as y = X*beta + T*effect
v0.8.0b1,Generate random control co-variates
v0.8.0b1,Construct treatments as T = X*A + epsilon
v0.8.0b1,Construct outcomes as y = X*beta + T*effect
v0.8.0b1,Generate random control co-variates
v0.8.0b1,Create epsilon residual treatments
v0.8.0b1,Construct treatments as T = X*A + epsilon
v0.8.0b1,Construct outcomes as y = X*beta + T*effect + eta
v0.8.0b1,Generate random control co-variates
v0.8.0b1,Use the same treatment vector for each row
v0.8.0b1,Construct outcomes as y = X*beta + T*effect
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"since inference objects can be stateful, we must copy it before fitting;"
v0.8.0b1,otherwise this sequence wouldn't work:
v0.8.0b1,"est1.fit(..., inference=inf)"
v0.8.0b1,"est2.fit(..., inference=inf)"
v0.8.0b1,est1.effect_interval(...)
v0.8.0b1,because inf now stores state from fitting est2
v0.8.0b1,call the wrapped fit method
v0.8.0b1,NOTE: we call inference fit *after* calling the main fit method
v0.8.0b1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.8.0b1,but tensordot can't be applied to this problem because we don't sum over m
v0.8.0b1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.8.0b1,of rows of T was not taken into account
v0.8.0b1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.8.0b1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.8.0b1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.8.0b1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.8.0b1,add statsmodels to parent's options
v0.8.0b1,add debiasedlasso to parent's options
v0.8.0b1,TODO Share some logic with non-discrete version
v0.8.0b1,add statsmodels to parent's options
v0.8.0b1,add statsmodels to parent's options
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Check if model is sparse enough for this model
v0.8.0b1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.8.0b1,TODO: any way to avoid creating a copy if the array was already dense?
v0.8.0b1,"the call is necessary if the input was something like a list, though"
v0.8.0b1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.8.0b1,so convert to pydata sparse first
v0.8.0b1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.8.0b1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.8.0b1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.8.0b1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.8.0b1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.8.0b1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.8.0b1,same number of input definitions as arrays
v0.8.0b1,input definitions have same number of dimensions as each array
v0.8.0b1,all result indices are unique
v0.8.0b1,all result indices must match at least one input index
v0.8.0b1,"map indices to all array, axis pairs for that index"
v0.8.0b1,each index has the same cardinality wherever it appears
v0.8.0b1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.8.0b1,Algo: while list contains more than one entry
v0.8.0b1,take two entries
v0.8.0b1,sort both lists by intersection of their indices
v0.8.0b1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.8.0b1,"take the union of indices and the product of values), stepping through each list linearly"
v0.8.0b1,TODO: might be faster to break into connected components first
v0.8.0b1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.8.0b1,"so compute their content separately, then take cartesian product"
v0.8.0b1,this would save a few pointless sorts by empty tuples
v0.8.0b1,TODO: Consider investigating other performance ideas for these cases
v0.8.0b1,where the dense method beat the sparse method (usually sparse is faster)
v0.8.0b1,"e,facd,c->cfed"
v0.8.0b1,sparse: 0.0335489
v0.8.0b1,dense:  0.011465999999999997
v0.8.0b1,"gbd,da,egb->da"
v0.8.0b1,sparse: 0.0791625
v0.8.0b1,dense:  0.007319099999999995
v0.8.0b1,"dcc,d,faedb,c->abe"
v0.8.0b1,sparse: 1.2868097
v0.8.0b1,dense:  0.44605229999999985
v0.8.0b1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.8.0b1,TODO: would using einsum's paths to optimize the order of merging help?
v0.8.0b1,Normalize weights
v0.8.0b1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.8.0b1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.8.0b1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.8.0b1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.8.0b1,clean way of achieving this
v0.8.0b1,make sure we don't accidentally escape anything in the substitution
v0.8.0b1,Fetch appropriate color for node
v0.8.0b1,"red for negative, green for positive"
v0.8.0b1,in multi-target use first target
v0.8.0b1,Write node mean CATE
v0.8.0b1,Write node std of CATE
v0.8.0b1,Write confidence interval information if at leaf node
v0.8.0b1,Fetch appropriate color for node
v0.8.0b1,"red for negative, green for positive"
v0.8.0b1,Write node mean CATE
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"scores entries should be lists of scores, so make each entry a singleton list"
v0.8.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.8.0b1,use a binary array to get stratified split in case of discrete treatment
v0.8.0b1,"stratify on T if discrete, and fine to pass T as second arg to KFold.split even when not"
v0.8.0b1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Estimators
v0.8.0b1,Causal tree parameters
v0.8.0b1,Tree structure
v0.8.0b1,No need for a random split since the data is already
v0.8.0b1,a random subsample from the original input
v0.8.0b1,node list stores the nodes that are yet to be splitted
v0.8.0b1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.8.0b1,Create local sample set
v0.8.0b1,Compute nuisance estimates for the current node
v0.8.0b1,Nuisance estimate cannot be calculated
v0.8.0b1,Estimate parameter for current node
v0.8.0b1,Node estimate cannot be calculated
v0.8.0b1,Calculate moments and gradient of moments for current data
v0.8.0b1,Calculate inverse gradient
v0.8.0b1,The gradient matrix is not invertible.
v0.8.0b1,No good split can be found
v0.8.0b1,Calculate point-wise pseudo-outcomes rho
v0.8.0b1,a split is determined by a feature and a sample pair
v0.8.0b1,the number of possible splits is at most (number of features) * (number of node samples)
v0.8.0b1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.8.0b1,parse row and column of random pair
v0.8.0b1,the sample of the pair is the integer division of the random number with n_feats
v0.8.0b1,calculate the binary indicator of whether sample i is on the left or the right
v0.8.0b1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.8.0b1,calculate the number of samples on the left child for each proposed split
v0.8.0b1,calculate the analogous binary indicator for the samples in the estimation set
v0.8.0b1,calculate the number of estimation samples on the left child of each proposed split
v0.8.0b1,find the upper and lower bound on the size of the left split for the split
v0.8.0b1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.8.0b1,on each side.
v0.8.0b1,similarly for the estimation sample set
v0.8.0b1,if there is no valid split then don't create any children
v0.8.0b1,filter only the valid splits
v0.8.0b1,calculate the average influence vector of the samples in the left child
v0.8.0b1,calculate the average influence vector of the samples in the right child
v0.8.0b1,take the square of each of the entries of the influence vectors and normalize
v0.8.0b1,by size of each child
v0.8.0b1,calculate the vector score of each candidate split as the average of left and right
v0.8.0b1,influence vectors
v0.8.0b1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.8.0b1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.8.0b1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.8.0b1,calculate the scalar score of each split by aggregating across the vector of scores
v0.8.0b1,Find split that minimizes criterion
v0.8.0b1,Create child nodes with corresponding subsamples
v0.8.0b1,add the created children to the list of not yet split nodes
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,TODO: Add a __dir__ implementation?
v0.8.0b1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.8.0b1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.8.0b1,then we should be computing a confidence interval for the wrapped calls
v0.8.0b1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.8.0b1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.8.0b1,"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
v0.8.0b1,AzureML
v0.8.0b1,helper imports
v0.8.0b1,write the details of the workspace to a configuration file to the notebook library
v0.8.0b1,if y is a multioutput model
v0.8.0b1,Make sure second dimension has 1 or more item
v0.8.0b1,switch _inner Model to a MultiOutputRegressor
v0.8.0b1,flatten array as automl only takes vectors for y
v0.8.0b1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.8.0b1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.8.0b1,as an sklearn estimator
v0.8.0b1,fit implementation for a single output model.
v0.8.0b1,Create experiment for specified workspace
v0.8.0b1,Configure automl_config with training set information.
v0.8.0b1,"Wait for remote run to complete, the set the model"
v0.8.0b1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.8.0b1,create model and pass model into final.
v0.8.0b1,"If item is an automl config, get its corresponding"
v0.8.0b1,AutomatedML Model and add it to new_Args
v0.8.0b1,"If item is an automl config, get its corresponding"
v0.8.0b1,AutomatedML Model and set it for this key in
v0.8.0b1,kwargs
v0.8.0b1,takes in either automated_ml config and instantiates
v0.8.0b1,an AutomatedMLModel
v0.8.0b1,The prefix can only be 18 characters long
v0.8.0b1,"because prefixes come from kwarg_names, we must ensure they are"
v0.8.0b1,short enough.
v0.8.0b1,Get workspace from config file.
v0.8.0b1,Take the intersect of the white for sample
v0.8.0b1,weights and linear models
v0.8.0b1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.8.0b1,Remove children with nonwhite mothers from the treatment group
v0.8.0b1,Remove children with nonwhite mothers from the treatment group
v0.8.0b1,Select columns
v0.8.0b1,Scale the numeric variables
v0.8.0b1,"Change the binary variable 'first' takes values in {1,2}"
v0.8.0b1,Append a column of ones as intercept
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.8.0b1,creating notebooks that are annoying for our users to actually run themselves
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.8.0b1,"prior to calling interpret, can't plot, render, etc."
v0.8.0b1,can interpret without uncertainty
v0.8.0b1,can't interpret with uncertainty if inference wasn't used during fit
v0.8.0b1,can interpret with uncertainty if we refit
v0.8.0b1,can interpret without uncertainty
v0.8.0b1,can't treat before interpreting
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,simple DGP only for illustration
v0.8.0b1,Define the treatment model neural network architecture
v0.8.0b1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.8.0b1,"so the input shape is (d_z + d_x,)"
v0.8.0b1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.8.0b1,add extra layers on top for the mixture density network
v0.8.0b1,Define the response model neural network architecture
v0.8.0b1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.8.0b1,"so the input shape is (d_t + d_x,)"
v0.8.0b1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.8.0b1,"NOTE: For the response model, it is important to define the model *outside*"
v0.8.0b1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.8.0b1,so that the same weights will be reused in each instantiation
v0.8.0b1,number of samples to use in second estimate of the response
v0.8.0b1,(to make loss estimate unbiased)
v0.8.0b1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.8.0b1,do something with predictions...
v0.8.0b1,also test vector t and y
v0.8.0b1,simple DGP only for illustration
v0.8.0b1,Define the treatment model neural network architecture
v0.8.0b1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.8.0b1,"so the input shape is (d_z + d_x,)"
v0.8.0b1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.8.0b1,add extra layers on top for the mixture density network
v0.8.0b1,Define the response model neural network architecture
v0.8.0b1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.8.0b1,"so the input shape is (d_t + d_x,)"
v0.8.0b1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.8.0b1,"NOTE: For the response model, it is important to define the model *outside*"
v0.8.0b1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.8.0b1,so that the same weights will be reused in each instantiation
v0.8.0b1,number of samples to use in second estimate of the response
v0.8.0b1,(to make loss estimate unbiased)
v0.8.0b1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.8.0b1,do something with predictions...
v0.8.0b1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.8.0b1,test = True ensures we draw test set images
v0.8.0b1,test = True ensures we draw test set images
v0.8.0b1,re-draw to get new independent treatment and implied response
v0.8.0b1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.8.0b1,above is necesary so that reduced form doesn't win
v0.8.0b1,covariates: time and emotion
v0.8.0b1,random instrument
v0.8.0b1,z -> price
v0.8.0b1,true observable demand function
v0.8.0b1,errors
v0.8.0b1,response
v0.8.0b1,test = True ensures we draw test set images
v0.8.0b1,test = True ensures we draw test set images
v0.8.0b1,re-draw to get new independent treatment and implied response
v0.8.0b1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.8.0b1,above is necesary so that reduced form doesn't win
v0.8.0b1,covariates: time and emotion
v0.8.0b1,random instrument
v0.8.0b1,z -> price
v0.8.0b1,true observable demand function
v0.8.0b1,errors
v0.8.0b1,response
v0.8.0b1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.8.0b1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.8.0b1,For some reason this doesn't work at all when run against the CNTK backend...
v0.8.0b1,"model.compile('nadam', loss=lambda _,l:l)"
v0.8.0b1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.8.0b1,generate a valiation set
v0.8.0b1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.8.0b1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.8.0b1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.8.0b1,so we need to transpose the result
v0.8.0b1,1-d output
v0.8.0b1,2-d output
v0.8.0b1,Single dimensional output y
v0.8.0b1,Multi-dimensional output y
v0.8.0b1,1-d y
v0.8.0b1,multi-d y
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,test that we can fit with the same arguments as the base estimator
v0.8.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.8.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0b1,with the same shape for the lower and upper bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,test that we can do the same thing once we provide percentile bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.8.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0b1,with the same shape for the lower and upper bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,test that we can do the same thing once we provide percentile bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,test that we can fit with the same arguments as the base estimator
v0.8.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.8.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0b1,with the same shape for the lower and upper bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,test that we can do the same thing once we provide percentile bounds
v0.8.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.8.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0b1,with the same shape for the lower and upper bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,test that we can do the same thing once we provide percentile bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,test that we can fit with the same arguments as the base estimator
v0.8.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.8.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0b1,with the same shape for the lower and upper bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,test that we can do the same thing once we provide percentile bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.8.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0b1,with the same shape for the lower and upper bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,test that we can do the same thing once we provide percentile bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0b1,with the same shape for the lower and upper bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,test that the estimated effect is usually within the bounds
v0.8.0b1,test that we can do the same thing once we provide alpha explicitly
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,test that the estimated effect is usually within the bounds
v0.8.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.8.0b1,with the same shape for the lower and upper bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,TODO: test that the estimated effect is usually within the bounds
v0.8.0b1,and that the true effect is also usually within the bounds
v0.8.0b1,test that we can do the same thing once we provide percentile bounds
v0.8.0b1,test that the lower and upper bounds differ
v0.8.0b1,TODO: test that the estimated effect is usually within the bounds
v0.8.0b1,and that the true effect is also usually within the bounds
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,DGP constants
v0.8.0b1,Generate data
v0.8.0b1,Test inference results when `cate_feature_names` doesn not exist
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"Nuisance model has no score method, so nuisance_scores_ should be none"
v0.8.0b1,Test non keyword based calls to fit
v0.8.0b1,test non-array inputs
v0.8.0b1,Test custom splitter
v0.8.0b1,Test incomplete set of test folds
v0.8.0b1,"y scores should be positive, since W predicts Y somewhat"
v0.8.0b1,"t scores might not be, since W and T are uncorrelated"
v0.8.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,make sure cross product varies more slowly with first array
v0.8.0b1,and that vectors are okay as inputs
v0.8.0b1,number of inputs in specification must match number of inputs
v0.8.0b1,must have an output
v0.8.0b1,output indices must be unique
v0.8.0b1,output indices must be present in an input
v0.8.0b1,number of indices must match number of dimensions for each input
v0.8.0b1,repeated indices must always have consistent sizes
v0.8.0b1,transpose
v0.8.0b1,tensordot
v0.8.0b1,trace
v0.8.0b1,TODO: set up proper flag for this
v0.8.0b1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.8.0b1,"of all of the distinct indices that appear in any input,"
v0.8.0b1,pick a random subset of them (of size at most 5) to appear in the output
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Preprocess data
v0.8.0b1,Convert 'week' to a date
v0.8.0b1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.8.0b1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.8.0b1,Take log of price
v0.8.0b1,Make brand numeric
v0.8.0b1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.8.0b1,which have all zero coeffs
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.8.0b1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.8.0b1,TODO: test something rather than just print...
v0.8.0b1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.8.0b1,pick some arbitrary X
v0.8.0b1,pick some arbitrary T
v0.8.0b1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,ensure that we've got at least two of every row
v0.8.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.0b1,need to make sure we get all *joint* combinations
v0.8.0b1,IntentToTreat only supports binary treatments/instruments
v0.8.0b1,IntentToTreat only supports binary treatments/instruments
v0.8.0b1,TODO: add stratification to bootstrap so that we can use it
v0.8.0b1,even with discrete treatments
v0.8.0b1,IntentToTreat requires X
v0.8.0b1,ensure we can serialize unfit estimator
v0.8.0b1,these support only W but not X
v0.8.0b1,"these support only binary, not general discrete T and Z"
v0.8.0b1,ensure we can serialize fit estimator
v0.8.0b1,make sure we can call the marginal_effect and effect methods
v0.8.0b1,TODO: add tests for extra properties like coef_ where they exist
v0.8.0b1,TODO: add tests for extra properties like coef_ where they exist
v0.8.0b1,"make sure we can call effect with implied scalar treatments,"
v0.8.0b1,"no matter the dimensions of T, and also that we warn when there"
v0.8.0b1,are multiple treatments
v0.8.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.8.0b1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.8.0b1,"however, with custom splits the checking happens in the first stage wrapper"
v0.8.0b1,where we don't have all of the required information to do this;
v0.8.0b1,we'd probably need to add it to _crossfit instead
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.8.0b1,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.8.0b1,The __warningregistry__'s need to be in a pristine state for tests
v0.8.0b1,to work properly.
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Set random seed
v0.8.0b1,Generate data
v0.8.0b1,DGP constants
v0.8.0b1,Test data
v0.8.0b1,Constant treatment effect
v0.8.0b1,Constant treatment with multi output Y
v0.8.0b1,Heterogeneous treatment
v0.8.0b1,Heterogeneous treatment with multi output Y
v0.8.0b1,TLearner test
v0.8.0b1,Instantiate TLearner
v0.8.0b1,Test inputs
v0.8.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.0b1,Instantiate SLearner
v0.8.0b1,Test inputs
v0.8.0b1,Test constant treatment effect
v0.8.0b1,Test constant treatment effect with multi output Y
v0.8.0b1,Test heterogeneous treatment effect
v0.8.0b1,Need interactions between T and features
v0.8.0b1,Test heterogeneous treatment effect with multi output Y
v0.8.0b1,Instantiate XLearner
v0.8.0b1,Test inputs
v0.8.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.0b1,Instantiate DomainAdaptationLearner
v0.8.0b1,Test inputs
v0.8.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.0b1,Get the true treatment effect
v0.8.0b1,Get the true treatment effect
v0.8.0b1,Fit learner and get the effect and marginal effect
v0.8.0b1,Compute treatment effect residuals (absolute)
v0.8.0b1,Check that at least 90% of predictions are within tolerance interval
v0.8.0b1,Check whether the output shape is right
v0.8.0b1,Check that one can pass in regular lists
v0.8.0b1,Check that it fails correctly if lists of different shape are passed in
v0.8.0b1,"Check that it works when T, Y have shape (n, 1)"
v0.8.0b1,Generate covariates
v0.8.0b1,Generate treatment
v0.8.0b1,Calculate outcome
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,DGP constants
v0.8.0b1,Generate data
v0.8.0b1,Test data
v0.8.0b1,Remove warnings that might be raised by the models passed into the ORF
v0.8.0b1,Generate data with continuous treatments
v0.8.0b1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.8.0b1,does not work well with parallelism.
v0.8.0b1,Test inputs for continuous treatments
v0.8.0b1,--> Check that one can pass in regular lists
v0.8.0b1,--> Check that it fails correctly if lists of different shape are passed in
v0.8.0b1,Check that outputs have the correct shape
v0.8.0b1,Test continuous treatments with controls
v0.8.0b1,Test continuous treatments without controls
v0.8.0b1,Generate data with binary treatments
v0.8.0b1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.8.0b1,does not work well with parallelism.
v0.8.0b1,Test inputs for binary treatments
v0.8.0b1,--> Check that one can pass in regular lists
v0.8.0b1,--> Check that it fails correctly if lists of different shape are passed in
v0.8.0b1,"--> Check that it works when T, Y have shape (n, 1)"
v0.8.0b1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.8.0b1,--> Check that it fails correctly when the treatments are not numeric
v0.8.0b1,Check that outputs have the correct shape
v0.8.0b1,Test binary treatments with controls
v0.8.0b1,Test binary treatments without controls
v0.8.0b1,Only applicable to continuous treatments
v0.8.0b1,Generate data for 2 treatments
v0.8.0b1,Test multiple treatments with controls
v0.8.0b1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.8.0b1,The rest for controls. Just as an example.
v0.8.0b1,Generating A/B test data
v0.8.0b1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.8.0b1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.8.0b1,Create a wrapper around Lasso that doesn't support weights
v0.8.0b1,since Lasso does natively support them starting in sklearn 0.23
v0.8.0b1,Generate data with continuous treatments
v0.8.0b1,Instantiate model with most of the default parameters
v0.8.0b1,Compute the treatment effect on test points
v0.8.0b1,Compute treatment effect residuals
v0.8.0b1,Multiple treatments
v0.8.0b1,Allow at most 10% test points to be outside of the tolerance interval
v0.8.0b1,Compute the treatment effect on test points
v0.8.0b1,Compute treatment effect residuals
v0.8.0b1,Multiple treatments
v0.8.0b1,Allow at most 20% test points to be outside of the confidence interval
v0.8.0b1,Check that the intervals are not too wide
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.8.0b1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.8.0b1,ensure that we've got at least 6 of every element
v0.8.0b1,"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
v0.8.0b1,NOTE: this number may need to change if the default number of folds in
v0.8.0b1,WeightedStratifiedKFold changes
v0.8.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.0b1,TODO: add stratification to bootstrap so that we can use it
v0.8.0b1,even with discrete treatments
v0.8.0b1,ensure we can serialize the unfit estimator
v0.8.0b1,ensure we can pickle the fit estimator
v0.8.0b1,make sure we can call the marginal_effect and effect methods
v0.8.0b1,test const marginal inference
v0.8.0b1,test effect inference
v0.8.0b1,test marginal effect inference
v0.8.0b1,test coef__inference and intercept__inference
v0.8.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.8.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.8.0b1,"make sure we can call effect with implied scalar treatments,"
v0.8.0b1,"no matter the dimensions of T, and also that we warn when there"
v0.8.0b1,are multiple treatments
v0.8.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.8.0b1,ensure that we've got at least two of every element
v0.8.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.0b1,"TODO Add bootstrap inference, once discrete treatment issue is fixed"
v0.8.0b1,make sure we can call the marginal_effect and effect methods
v0.8.0b1,test const marginal inference
v0.8.0b1,test effect inference
v0.8.0b1,test marginal effect inference
v0.8.0b1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.8.0b1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.8.0b1,We concatenate the two copies data
v0.8.0b1,create a simple artificial setup where effect of moving from treatment
v0.8.0b1,"1 -> 2 is 2,"
v0.8.0b1,"1 -> 3 is 1, and"
v0.8.0b1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.8.0b1,"Using an uneven number of examples from different classes,"
v0.8.0b1,"and having the treatments in non-lexicographic order,"
v0.8.0b1,Should rule out some basic issues.
v0.8.0b1,test that we can fit with a KFold instance
v0.8.0b1,test that we can fit with a train/test iterable
v0.8.0b1,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.8.0b1,(incorrectly) use a final model with an intercept
v0.8.0b1,"Because final model is fixed, actual values of T and Y don't matter"
v0.8.0b1,Ensure reproducibility
v0.8.0b1,Sparse DGP
v0.8.0b1,Treatment effect coef
v0.8.0b1,Other coefs
v0.8.0b1,Features and controls
v0.8.0b1,Test sparse estimator
v0.8.0b1,"--> test coef_, intercept_"
v0.8.0b1,--> test treatment effects
v0.8.0b1,Restrict x_test to vectors of norm < 1
v0.8.0b1,--> check inference
v0.8.0b1,Check that a majority of true effects lie in the 5-95% CI
v0.8.0b1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.8.0b1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.8.0b1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.8.0b1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.8.0b1,sparse test case: heterogeneous effect by product
v0.8.0b1,need at least as many rows in e_y as there are distinct columns
v0.8.0b1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.8.0b1,create a simple artificial setup where effect of moving from treatment
v0.8.0b1,"a -> b is 2,"
v0.8.0b1,"a -> c is 1, and"
v0.8.0b1,"b -> c is -1 (necessarily, by composing the previous two effects)"
v0.8.0b1,"Using an uneven number of examples from different classes,"
v0.8.0b1,"and having the treatments in non-lexicographic order,"
v0.8.0b1,should rule out some basic issues.
v0.8.0b1,Note that explicitly specifying the dtype as object is necessary until
v0.8.0b1,there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
v0.8.0b1,estimated effects should be identical when treatment is explicitly given
v0.8.0b1,but const_marginal_effect should be reordered based on the explicit cagetories
v0.8.0b1,1-> 2 in original ordering; combination of 3->1 and 3->2
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Set random seed
v0.8.0b1,Generate data
v0.8.0b1,DGP constants
v0.8.0b1,Test data
v0.8.0b1,Constant treatment effect and propensity
v0.8.0b1,Heterogeneous treatment and propensity
v0.8.0b1,ensure that we've got at least two of every element
v0.8.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.8.0b1,ensure that we can serialize unfit estimator
v0.8.0b1,TODO: add stratification to bootstrap so that we can use it even with discrete treatments
v0.8.0b1,ensure that we can serialize fit estimator
v0.8.0b1,make sure we can call the marginal_effect and effect methods
v0.8.0b1,test const marginal inference
v0.8.0b1,test effect inference
v0.8.0b1,test marginal effect inference
v0.8.0b1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.8.0b1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.8.0b1,create a simple artificial setup where effect of moving from treatment
v0.8.0b1,"1 -> 2 is 2,"
v0.8.0b1,"1 -> 3 is 1, and"
v0.8.0b1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.8.0b1,"Using an uneven number of examples from different classes,"
v0.8.0b1,"and having the treatments in non-lexicographic order,"
v0.8.0b1,Should rule out some basic issues.
v0.8.0b1,test that we can fit with a KFold instance
v0.8.0b1,test that we can fit with a train/test iterable
v0.8.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.8.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.8.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.8.0b1,test coef__inference function works
v0.8.0b1,test intercept__inference function works
v0.8.0b1,test summary function works
v0.8.0b1,Test inputs
v0.8.0b1,self._test_inputs(DR_learner)
v0.8.0b1,Test constant treatment effect
v0.8.0b1,Test heterogeneous treatment effect
v0.8.0b1,Test heterogenous treatment effect for W =/= None
v0.8.0b1,Sparse DGP
v0.8.0b1,Treatment effect coef
v0.8.0b1,Other coefs
v0.8.0b1,Features and controls
v0.8.0b1,Test sparse estimator
v0.8.0b1,"--> test coef_, intercept_"
v0.8.0b1,--> test treatment effects
v0.8.0b1,Restrict x_test to vectors of norm < 1
v0.8.0b1,--> check inference
v0.8.0b1,Check that a majority of true effects lie in the 5-95% CI
v0.8.0b1,Fit learner and get the effect
v0.8.0b1,Get the true treatment effect
v0.8.0b1,Compute treatment effect residuals (absolute)
v0.8.0b1,Check that at least 90% of predictions are within tolerance interval
v0.8.0b1,Only for heterogeneous TE
v0.8.0b1,Fit learner on X and W and get the effect
v0.8.0b1,Get the true treatment effect
v0.8.0b1,Compute treatment effect residuals (absolute)
v0.8.0b1,Check that at least 90% of predictions are within tolerance interval
v0.8.0b1,Check that one can pass in regular lists
v0.8.0b1,Check that it fails correctly if lists of different shape are passed in
v0.8.0b1,Check that it fails when T contains values other than 0 and 1
v0.8.0b1,"Check that it works when T, Y have shape (n, 1)"
v0.8.0b1,Generate covariates
v0.8.0b1,Generate treatment
v0.8.0b1,Calculate outcome
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,DGP constants
v0.8.0b1,DGP coefficients
v0.8.0b1,Generated outcomes
v0.8.0b1,################
v0.8.0b1,WeightedLasso #
v0.8.0b1,################
v0.8.0b1,Define weights
v0.8.0b1,Define extended datasets
v0.8.0b1,Range of alphas
v0.8.0b1,Compare with Lasso
v0.8.0b1,--> No intercept
v0.8.0b1,--> With intercept
v0.8.0b1,When DGP has no intercept
v0.8.0b1,When DGP has intercept
v0.8.0b1,--> Coerce coefficients to be positive
v0.8.0b1,--> Toggle max_iter & tol
v0.8.0b1,Define weights
v0.8.0b1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.8.0b1,Mixed DGP scenario.
v0.8.0b1,Define extended datasets
v0.8.0b1,Define weights
v0.8.0b1,Define multioutput
v0.8.0b1,##################
v0.8.0b1,WeightedLassoCV #
v0.8.0b1,##################
v0.8.0b1,Define alphas to test
v0.8.0b1,Compare with LassoCV
v0.8.0b1,--> No intercept
v0.8.0b1,--> With intercept
v0.8.0b1,--> Force parameters to be positive
v0.8.0b1,Choose a smaller n to speed-up process
v0.8.0b1,Compare fold weights
v0.8.0b1,Define weights
v0.8.0b1,Define extended datasets
v0.8.0b1,Define splitters
v0.8.0b1,WeightedKFold splitter
v0.8.0b1,Map weighted splitter to an extended splitter
v0.8.0b1,Define alphas to test
v0.8.0b1,Compare with LassoCV
v0.8.0b1,--> No intercept
v0.8.0b1,--> With intercept
v0.8.0b1,--> Force parameters to be positive
v0.8.0b1,###########################
v0.8.0b1,MultiTaskWeightedLassoCV #
v0.8.0b1,###########################
v0.8.0b1,Define alphas to test
v0.8.0b1,Define splitter
v0.8.0b1,Compare with MultiTaskLassoCV
v0.8.0b1,--> No intercept
v0.8.0b1,--> With intercept
v0.8.0b1,Define weights
v0.8.0b1,Define extended datasets
v0.8.0b1,Define splitters
v0.8.0b1,WeightedKFold splitter
v0.8.0b1,Map weighted splitter to an extended splitter
v0.8.0b1,Define alphas to test
v0.8.0b1,Compare with LassoCV
v0.8.0b1,--> No intercept
v0.8.0b1,--> With intercept
v0.8.0b1,################
v0.8.0b1,DebiasedLasso #
v0.8.0b1,################
v0.8.0b1,Test DebiasedLasso without weights
v0.8.0b1,--> Check debiased coeffcients without intercept
v0.8.0b1,--> Check debiased coeffcients with intercept
v0.8.0b1,--> Check 5-95 CI coverage for unit vectors
v0.8.0b1,Test DebiasedLasso with weights for one DGP
v0.8.0b1,Define weights
v0.8.0b1,Define extended datasets
v0.8.0b1,--> Check debiased coefficients
v0.8.0b1,Define weights
v0.8.0b1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.8.0b1,--> Check debiased coeffcients
v0.8.0b1,Test that attributes propagate correctly
v0.8.0b1,Test MultiOutputDebiasedLasso without weights
v0.8.0b1,--> Check debiased coeffcients without intercept
v0.8.0b1,--> Check debiased coeffcients with intercept
v0.8.0b1,--> Check CI coverage
v0.8.0b1,Test MultiOutputDebiasedLasso with weights
v0.8.0b1,Define weights
v0.8.0b1,Define extended datasets
v0.8.0b1,--> Check debiased coefficients
v0.8.0b1,Unit vectors
v0.8.0b1,Unit vectors
v0.8.0b1,Check coeffcients and intercept are the same within tolerance
v0.8.0b1,Check results are similar with tolerance 1e-6
v0.8.0b1,Check if multitask
v0.8.0b1,Check that same alpha is chosen
v0.8.0b1,Check that the coefficients are similar
v0.8.0b1,selective ridge has a simple implementation that we can test against
v0.8.0b1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.8.0b1,"it should be the case that when we set fit_intercept to true,"
v0.8.0b1,it doesn't matter whether the penalized model also fits an intercept or not
v0.8.0b1,create an extra copy of rows with weight 2
v0.8.0b1,"instead of a slice, explicitly return an array of indices"
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Check that the point estimates are the same
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Linear models are required for parametric dml
v0.8.0b1,sample weighting models are required for nonparametric dml
v0.8.0b1,Test values
v0.8.0b1,TLearner test
v0.8.0b1,Instantiate TLearner
v0.8.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.8.0b1,Test constant treatment effect with multi output Y
v0.8.0b1,Test heterogeneous treatment effect
v0.8.0b1,Need interactions between T and features
v0.8.0b1,Test heterogeneous treatment effect with multi output Y
v0.8.0b1,Instantiate DomainAdaptationLearner
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,"Found a good split, return."
v0.8.0b1,Record all splits in case the stratification by weight yeilds a worse partition
v0.8.0b1,Reseed random generator and try again
v0.8.0b1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.8.0b1,"Found a good split, return."
v0.8.0b1,Did not find a good split
v0.8.0b1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.8.0b1,Return most weight-balanced partition
v0.8.0b1,Weight stratification algorithm
v0.8.0b1,Sort weights for weight strata search
v0.8.0b1,There are some leftover indices that have yet to be assigned
v0.8.0b1,Append stratum splits to overall splits
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Our classes that derive from sklearn ones sometimes include
v0.8.0b1,inherited docstrings that have embedded doctests; we need the following imports
v0.8.0b1,so that they don't break.
v0.8.0b1,"Convert X, y into numpy arrays"
v0.8.0b1,Define fit parameters
v0.8.0b1,Some algorithms don't have a check_input option
v0.8.0b1,Check weights array
v0.8.0b1,Check that weights are size-compatible
v0.8.0b1,Normalize inputs
v0.8.0b1,Weight inputs
v0.8.0b1,Fit base class without intercept
v0.8.0b1,Fit Lasso
v0.8.0b1,Reset intercept
v0.8.0b1,The intercept is not calculated properly due the sqrt(weights) factor
v0.8.0b1,so it must be recomputed
v0.8.0b1,Fit lasso without weights
v0.8.0b1,Make weighted splitter
v0.8.0b1,Fit weighted model
v0.8.0b1,Make weighted splitter
v0.8.0b1,Fit weighted model
v0.8.0b1,Select optimal penalty
v0.8.0b1,Warn about consistency
v0.8.0b1,"Convert X, y into numpy arrays"
v0.8.0b1,Fit weighted lasso with user input
v0.8.0b1,"Center X, y"
v0.8.0b1,Calculate quantities that will be used later on. Account for centered data
v0.8.0b1,Calculate coefficient and error variance
v0.8.0b1,Add coefficient correction
v0.8.0b1,Set coefficients and intercept standard errors
v0.8.0b1,Set intercept
v0.8.0b1,Return alpha to 'auto' state
v0.8.0b1,"Note that in the case of no intercept, X_offset is 0"
v0.8.0b1,Calculate the variance of the predictions
v0.8.0b1,"Note that in the case of no intercept, X_offset is 0"
v0.8.0b1,Calculate the variance of the predictions
v0.8.0b1,Calculate prediction confidence intervals
v0.8.0b1,Assumes flattened y
v0.8.0b1,Compute weighted residuals
v0.8.0b1,To be done once per target. Assumes y can be flattened.
v0.8.0b1,Assumes that X has already been offset
v0.8.0b1,Special case: n_features=1
v0.8.0b1,Compute Lasso coefficients for the columns of the design matrix
v0.8.0b1,Call weighted lasso on reduced design matrix
v0.8.0b1,Inherit some parameters from the parent
v0.8.0b1,Weighted tau
v0.8.0b1,Compute C_hat
v0.8.0b1,Compute theta_hat
v0.8.0b1,Allow for single output as well
v0.8.0b1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.8.0b1,Set coef_ attribute
v0.8.0b1,Set intercept_ attribute
v0.8.0b1,Set selected_alpha_ attribute
v0.8.0b1,Set coef_stderr_
v0.8.0b1,intercept_stderr_
v0.8.0b1,set intercept_ attribute
v0.8.0b1,set coef_ attribute
v0.8.0b1,set alpha_ attribute
v0.8.0b1,set alphas_ attribute
v0.8.0b1,set n_iter_ attribute
v0.8.0b1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.8.0b1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.8.0b1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.8.0b1,now regress X1 on y - X2 * beta2 to learn beta1
v0.8.0b1,set coef_ and intercept_ attributes
v0.8.0b1,Note that the penalized model should *not* have an intercept
v0.8.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.8.0b1,Licensed under the MIT License.
v0.8.0b1,Construct the subsample of data
v0.8.0b1,Split into estimation and splitting sample set
v0.8.0b1,Fit the tree on the splitting sample
v0.8.0b1,Set the estimation values based on the estimation split
v0.8.0b1,Apply the trained tree on the estimation sample to get the path for every estimation sample
v0.8.0b1,Calculate the total weight of estimation samples on each tree node:
v0.8.0b1,\sum_i sample_weight[i] * 1{i \\in node}
v0.8.0b1,Calculate the total number of estimation samples on each tree node:
v0.8.0b1,|node| = \sum_{i} 1{i \\in node}
v0.8.0b1,Calculate the weighted sum of responses on the estimation sample on each node:
v0.8.0b1,\sum_{i} sample_weight[i] 1{i \\in node} Y_i
v0.8.0b1,Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
v0.8.0b1,and for each un-pruned tree set the value and the weight appropriately.
v0.8.0b1,If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
v0.8.0b1,"sample, then prune the whole sub-tree"
v0.8.0b1,Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
v0.8.0b1,Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
v0.8.0b1,Set the count to the estimation split count
v0.8.0b1,Validate or convert input data
v0.8.0b1,Pre-sort indices to avoid that each individual tree of the
v0.8.0b1,ensemble sorts the indices.
v0.8.0b1,Remap output
v0.8.0b1,reshape is necessary to preserve the data contiguity against vs
v0.8.0b1,"[:, np.newaxis] that does not."
v0.8.0b1,Check parameters
v0.8.0b1,"Free allocated memory, if any"
v0.8.0b1,We draw from the random state to get the random state we
v0.8.0b1,would have got if we hadn't used a warm_start.
v0.8.0b1,Parallel loop: we prefer the threading backend as the Cython code
v0.8.0b1,for fitting the trees is internally releasing the Python GIL
v0.8.0b1,making threading more efficient than multiprocessing in
v0.8.0b1,"that case. However, for joblib 0.12+ we respect any"
v0.8.0b1,"parallel_backend contexts set at a higher level,"
v0.8.0b1,since correctness does not rely on using threads.
v0.8.0b1,TODO. This slicing should ultimately be done inside the parallel function
v0.8.0b1,so that we don't need to create a matrix of size roughly n_samples * n_estimators
v0.8.0b1,Collect newly grown trees
v0.8.0b1,Helper class that accumulates an arbitrary function in parallel on the accumulator acc
v0.8.0b1,and calls the function fn on each tree e and returns the mean output. The function fn
v0.8.0b1,"should take as input a tree e, and return another function g_e, which takes as input X, check_input"
v0.8.0b1,"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
v0.8.0b1,index start to index end will be used. The returned result is essentially:
v0.8.0b1,(mean over e in slice)(g_e(X)).
v0.8.0b1,Check data
v0.8.0b1,Assign chunk of trees to jobs
v0.8.0b1,Check data
v0.8.0b1,Check data
v0.8.0b1,avoid storing the output of every estimator by summing them here
v0.8.0b1,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
v0.8.0b1,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
v0.8.0b1,"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
v0.8.0b1,where \theta(X) is the point estimate using the whole forest
v0.8.0b1,Calculate the variance of the latter as E[Q(S)^2]
v0.7.0,configuration is all pulled from setup.cfg
v0.7.0,-*- coding: utf-8 -*-
v0.7.0,
v0.7.0,Configuration file for the Sphinx documentation builder.
v0.7.0,
v0.7.0,This file does only contain a selection of the most common options. For a
v0.7.0,full list see the documentation:
v0.7.0,http://www.sphinx-doc.org/en/master/config
v0.7.0,-- Path setup --------------------------------------------------------------
v0.7.0,"If extensions (or modules to document with autodoc) are in another directory,"
v0.7.0,add these directories to sys.path here. If the directory is relative to the
v0.7.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.7.0,
v0.7.0,-- Project information -----------------------------------------------------
v0.7.0,-- General configuration ---------------------------------------------------
v0.7.0,"If your documentation needs a minimal Sphinx version, state it here."
v0.7.0,
v0.7.0,needs_sphinx = '1.0'
v0.7.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.7.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.7.0,ones.
v0.7.0,"Add any paths that contain templates here, relative to this directory."
v0.7.0,The suffix(es) of source filenames.
v0.7.0,You can specify multiple suffix as a list of string:
v0.7.0,
v0.7.0,"source_suffix = ['.rst', '.md']"
v0.7.0,The master toctree document.
v0.7.0,The language for content autogenerated by Sphinx. Refer to documentation
v0.7.0,for a list of supported languages.
v0.7.0,
v0.7.0,This is also used if you do content translation via gettext catalogs.
v0.7.0,"Usually you set ""language"" from the command line for these cases."
v0.7.0,"List of patterns, relative to source directory, that match files and"
v0.7.0,directories to ignore when looking for source files.
v0.7.0,This pattern also affects html_static_path and html_extra_path.
v0.7.0,The name of the Pygments (syntax highlighting) style to use.
v0.7.0,-- Options for HTML output -------------------------------------------------
v0.7.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.7.0,a list of builtin themes.
v0.7.0,
v0.7.0,Theme options are theme-specific and customize the look and feel of a theme
v0.7.0,"further.  For a list of options available for each theme, see the"
v0.7.0,documentation.
v0.7.0,
v0.7.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.7.0,"relative to this directory. They are copied after the builtin static files,"
v0.7.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.7.0,html_static_path = ['_static']
v0.7.0,"Custom sidebar templates, must be a dictionary that maps document names"
v0.7.0,to template names.
v0.7.0,
v0.7.0,The default sidebars (for documents that don't match any pattern) are
v0.7.0,defined by theme itself.  Builtin themes are using these templates by
v0.7.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.7.0,'searchbox.html']``.
v0.7.0,
v0.7.0,html_sidebars = {}
v0.7.0,-- Options for HTMLHelp output ---------------------------------------------
v0.7.0,Output file base name for HTML help builder.
v0.7.0,-- Options for LaTeX output ------------------------------------------------
v0.7.0,The paper size ('letterpaper' or 'a4paper').
v0.7.0,
v0.7.0,"'papersize': 'letterpaper',"
v0.7.0,"The font size ('10pt', '11pt' or '12pt')."
v0.7.0,
v0.7.0,"'pointsize': '10pt',"
v0.7.0,Additional stuff for the LaTeX preamble.
v0.7.0,
v0.7.0,"'preamble': '',"
v0.7.0,Latex figure (float) alignment
v0.7.0,
v0.7.0,"'figure_align': 'htbp',"
v0.7.0,Grouping the document tree into LaTeX files. List of tuples
v0.7.0,"(source start file, target name, title,"
v0.7.0,"author, documentclass [howto, manual, or own class])."
v0.7.0,-- Options for manual page output ------------------------------------------
v0.7.0,One entry per manual page. List of tuples
v0.7.0,"(source start file, name, description, authors, manual section)."
v0.7.0,-- Options for Texinfo output ----------------------------------------------
v0.7.0,Grouping the document tree into Texinfo files. List of tuples
v0.7.0,"(source start file, target name, title, author,"
v0.7.0,"dir menu entry, description, category)"
v0.7.0,-- Options for Epub output -------------------------------------------------
v0.7.0,Bibliographic Dublin Core info.
v0.7.0,The unique identifier of the text. This can be a ISBN number
v0.7.0,or the project homepage.
v0.7.0,
v0.7.0,epub_identifier = ''
v0.7.0,A unique identification for the text.
v0.7.0,
v0.7.0,epub_uid = ''
v0.7.0,A list of files that should not be packed into the epub file.
v0.7.0,-- Extension configuration -------------------------------------------------
v0.7.0,-- Options for intersphinx extension ---------------------------------------
v0.7.0,Example configuration for intersphinx: refer to the Python standard library.
v0.7.0,-- Options for todo extension ----------------------------------------------
v0.7.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.7.0,-- Options for doctest extension -------------------------------------------
v0.7.0,we can document otherwise excluded entities here by returning False
v0.7.0,or skip otherwise included entities by returning True
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Calculate residuals
v0.7.0,Estimate E[T_res | Z_res]
v0.7.0,TODO. Deal with multi-class instrument
v0.7.0,Calculate nuisances
v0.7.0,Estimate E[T_res | Z_res]
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"We do a three way split, as typically a preliminary theta estimator would require"
v0.7.0,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.7.0,TODO. Deal with multi-class instrument
v0.7.0,Estimate final model of theta(X) by minimizing the square loss:
v0.7.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.7.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.7.0,at the expense of some small bias. For points with very small covariance we revert
v0.7.0,to the model-based preliminary estimate and do not add the correction term.
v0.7.0,Estimate preliminary theta in cross fitting manner
v0.7.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.7.0,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.7.0,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.7.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.7.0,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.7.0,TODO. The solution below is not really a valid cross-fitting
v0.7.0,as the test data are used to create the proj_t on the train
v0.7.0,which in the second train-test loop is used to create the nuisance
v0.7.0,cov on the test data. Hence the T variable of some sample
v0.7.0,"is implicitly correlated with its cov nuisance, through this flow"
v0.7.0,"of information. However, this seems a rather weak correlation."
v0.7.0,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.7.0,model.
v0.7.0,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.7.0,Estimate preliminary theta in cross fitting manner
v0.7.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.7.0,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.7.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.7.0,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.7.0,#############################################################################
v0.7.0,Classes for the DRIV implementation for the special case of intent-to-treat
v0.7.0,A/B test
v0.7.0,#############################################################################
v0.7.0,Estimate preliminary theta in cross fitting manner
v0.7.0,Estimate p(X) = E[T | X] in cross fitting manner
v0.7.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.7.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.7.0,We can use statsmodel for all hypothesis testing capabilities
v0.7.0,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.7.0,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.7.0,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.7.0,model_T_XZ = lambda: model_clf()
v0.7.0,#'days_visited': lambda:
v0.7.0,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.7.0,Turn strings into categories for numeric mapping
v0.7.0,### Defining some generic regressors and classifiers
v0.7.0,This a generic non-parametric regressor
v0.7.0,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.7.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.7.0,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.7.0,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.7.0,model = lambda: RandomForestRegressor(n_estimators=100)
v0.7.0,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.7.0,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.7.0,model = lambda: LinearRegression(n_jobs=-1)
v0.7.0,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.7.0,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.7.0,underlying model whenever predict is called.
v0.7.0,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.7.0,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.7.0,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.7.0,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.7.0,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.7.0,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.7.0,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.7.0,We need to specify models to be used for each of these residualizations
v0.7.0,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.7.0,"E[T | X, Z]"
v0.7.0,E[TZ | X]
v0.7.0,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.7.0,n_splits determines the number of splits to be used for cross-fitting.
v0.7.0,# Algorithm 2 - Current Method
v0.7.0,In[121]:
v0.7.0,# Algorithm 3 - DRIV ATE
v0.7.0,dmliv_model_effect = lambda: model()
v0.7.0,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.7.0,"dmliv_model_effect(),"
v0.7.0,n_splits=1)
v0.7.0,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.7.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.7.0,"Once multiple treatments are supported, we'll need to fix this"
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.7.0,We can use statsmodel for all hypothesis testing capabilities
v0.7.0,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.7.0,We can use statsmodel for all hypothesis testing capabilities
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,TODO. Deal with multi-class instrument/treatment
v0.7.0,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.7.0,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.7.0,Estimate p(X) = E[T | X] in cross-fitting manner
v0.7.0,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.7.0,##################
v0.7.0,Global settings #
v0.7.0,##################
v0.7.0,Global plotting controls
v0.7.0,"Control for support size, can control for more"
v0.7.0,#################
v0.7.0,File utilities #
v0.7.0,#################
v0.7.0,#################
v0.7.0,Plotting utils #
v0.7.0,#################
v0.7.0,bias
v0.7.0,var
v0.7.0,rmse
v0.7.0,r2
v0.7.0,Infer feature dimension
v0.7.0,Metrics by support plots
v0.7.0,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.7.0,Vasilis Syrgkanis <vasy@microsoft.com>
v0.7.0,Steven Wu <zhiww@microsoft.com>
v0.7.0,Initialize causal tree parameters
v0.7.0,Create splits of causal tree
v0.7.0,Estimate treatment effects at the leafs
v0.7.0,Compute heterogeneous treatement effect for x's in x_list by finding
v0.7.0,the corresponding split and associating the effect computed on that leaf
v0.7.0,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.7.0,Safety check
v0.7.0,Weighted linear regression
v0.7.0,Calculates weights
v0.7.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.7.0,over all indices
v0.7.0,Similar for `a` weights
v0.7.0,Doesn't have sample weights
v0.7.0,Is a linear model
v0.7.0,Weighted linear regression
v0.7.0,Calculates weights
v0.7.0,Bootstraping has repetitions in tree sample so we need to iterate
v0.7.0,over all indices
v0.7.0,Similar for `a` weights
v0.7.0,normalize weights
v0.7.0,"Split the data in half, train and test"
v0.7.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.7.0,"a function of W, using only the train fold"
v0.7.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.7.0,"Split the data in half, train and test"
v0.7.0,Fit with LassoCV the treatment as a function of W and the outcome as
v0.7.0,"a function of W, using only the train fold"
v0.7.0,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.7.0,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.7.0,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.7.0,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.7.0,"Split the data in half, train and test"
v0.7.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.7.0,"a function of x, using only the train fold"
v0.7.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.7.0,Compute coefficient by OLS on residuals
v0.7.0,"Split the data in half, train and test"
v0.7.0,Fit with LassoCV the treatment as a function of x and the outcome as
v0.7.0,"a function of x, using only the train fold"
v0.7.0,Then compute residuals p-g(x) and q-q(x) on test fold
v0.7.0,Estimate multipliers for second order orthogonal method
v0.7.0,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.7.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.7.0,Create local sample set
v0.7.0,compute the base estimate for the current node using double ml or second order double ml
v0.7.0,compute the influence functions here that are used for the criterion
v0.7.0,generate random proposals of dimensions to split
v0.7.0,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.7.0,compute criterion for each proposal
v0.7.0,if splitting creates valid leafs in terms of mean leaf size
v0.7.0,Calculate criterion for split
v0.7.0,Else set criterion to infinity so that this split is not chosen
v0.7.0,If no good split was found
v0.7.0,Find split that minimizes criterion
v0.7.0,Set the split attributes at the node
v0.7.0,Create child nodes with corresponding subsamples
v0.7.0,Recursively split children
v0.7.0,Return parent node
v0.7.0,estimate the local parameter at the leaf using the estimate data
v0.7.0,###################
v0.7.0,Argument parsing #
v0.7.0,###################
v0.7.0,#########################################
v0.7.0,Parameters constant across experiments #
v0.7.0,#########################################
v0.7.0,Outcome support
v0.7.0,Treatment support
v0.7.0,Evaluation grid
v0.7.0,Treatment effects array
v0.7.0,Other variables
v0.7.0,##########################
v0.7.0,Data Generating Process #
v0.7.0,##########################
v0.7.0,Log iteration
v0.7.0,"Generate controls, features, treatment and outcome"
v0.7.0,T and Y residuals to be used in later scripts
v0.7.0,Save generated dataset
v0.7.0,#################
v0.7.0,ORF parameters #
v0.7.0,#################
v0.7.0,######################################
v0.7.0,Train and evaluate treatment effect #
v0.7.0,######################################
v0.7.0,########
v0.7.0,Plots #
v0.7.0,########
v0.7.0,###############
v0.7.0,Save results #
v0.7.0,###############
v0.7.0,##############
v0.7.0,Run Rscript #
v0.7.0,##############
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Check inputs
v0.7.0,Check inputs
v0.7.0,Check inputs
v0.7.0,Check inputs
v0.7.0,Check inputs
v0.7.0,Estimate response function
v0.7.0,Check inputs
v0.7.0,Train model on controls. Assign higher weight to units resembling
v0.7.0,treated units.
v0.7.0,Train model on the treated. Assign higher weight to units resembling
v0.7.0,control units.
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Create splits of causal tree
v0.7.0,Make sure the correct exception is being rethrown
v0.7.0,Must make sure indices are merged correctly
v0.7.0,Require group assignment t to be one-hot-encoded
v0.7.0,Define an inner function that iterates over group predictions
v0.7.0,Convert rows to columns
v0.7.0,Get predictions for the 2 splits
v0.7.0,Must make sure indices are merged correctly
v0.7.0,Estimators
v0.7.0,OrthoForest parameters
v0.7.0,Sub-forests
v0.7.0,Auxiliary attributes
v0.7.0,Fit check
v0.7.0,TODO: Check performance
v0.7.0,Override the CATE inference options
v0.7.0,Add blb inference to parent's options
v0.7.0,Must normalize weights
v0.7.0,Crossfitting
v0.7.0,Compute weighted nuisance estimates
v0.7.0,-------------------------------------------------------------------------------
v0.7.0,Calculate the covariance matrix corresponding to the BLB inference
v0.7.0,
v0.7.0,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.7.0,2. Calculate the weighted moments for each tree slice to create a matrix
v0.7.0,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.7.0,in that slice from the overall parameter estimate.
v0.7.0,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.7.0,-------------------------------------------------------------------------------
v0.7.0,Calclulate covariance matrix through BLB
v0.7.0,Generate subsample indices
v0.7.0,Build trees in parallel
v0.7.0,Bootstraping has repetitions in tree sample
v0.7.0,Similar for `a` weights
v0.7.0,Bootstraping has repetitions in tree sample
v0.7.0,Define subsample size
v0.7.0,Safety check
v0.7.0,Draw points to create little bags
v0.7.0,Copy and/or define models
v0.7.0,Define nuisance estimators
v0.7.0,Define parameter estimators
v0.7.0,Define
v0.7.0,Override to flatten output if T is flat
v0.7.0,T is flat
v0.7.0,Nuissance estimates evaluated with cross-fitting
v0.7.0,Define 2-fold iterator
v0.7.0,need safe=False when cloning for WeightedModelWrapper
v0.7.0,Compute residuals
v0.7.0,Compute coefficient by OLS on residuals
v0.7.0,"Parameter returned by LinearRegression is (d_T, )"
v0.7.0,Compute residuals
v0.7.0,Compute coefficient by OLS on residuals
v0.7.0,ell_2 regularization
v0.7.0,Ridge regression estimate
v0.7.0,"Parameter returned is of shape (d_T, )"
v0.7.0,Return moments and gradients
v0.7.0,Compute residuals
v0.7.0,Compute moments
v0.7.0,"Moments shape is (n, d_T)"
v0.7.0,Compute moment gradients
v0.7.0,Copy and/or define models
v0.7.0,Nuisance estimators shall be defined during fitting because they need to know the number of distinct
v0.7.0,treatments
v0.7.0,Define parameter estimators
v0.7.0,Define moment and mean gradient estimator
v0.7.0,Define autoencoder
v0.7.0,"Check that T is shape (n, )"
v0.7.0,Check T is numeric
v0.7.0,Train label encoder
v0.7.0,Define number of classes
v0.7.0,Call `fit` from parent class
v0.7.0,"Test that T contains all treatments. If not, return None"
v0.7.0,Nuissance estimates evaluated with cross-fitting
v0.7.0,Define 2-fold iterator
v0.7.0,Check if there is only one example of some class
v0.7.0,No need to crossfit for internal nodes
v0.7.0,Compute partial moments
v0.7.0,"If any of the values in the parameter estimate is nan, return None"
v0.7.0,Compute partial moments
v0.7.0,Compute coefficient by OLS on residuals
v0.7.0,ell_2 regularization
v0.7.0,Ridge regression estimate
v0.7.0,"Parameter returned is of shape (d_T, )"
v0.7.0,Return moments and gradients
v0.7.0,Compute partial moments
v0.7.0,Compute moments
v0.7.0,"Moments shape is (n, d_T-1)"
v0.7.0,Compute moment gradients
v0.7.0,Need to calculate this in an elegant way for when propensity is 0
v0.7.0,This will flatten T
v0.7.0,Check that T is numeric
v0.7.0,Test whether the input estimator is supported
v0.7.0,Test expansion of treatment
v0.7.0,"If expanded treatments are a vector, flatten const_marginal_effect_interval"
v0.7.0,Calculate confidence intervals for the parameter (marginal effect)
v0.7.0,"If T is a vector, preserve shape of the effect interval"
v0.7.0,Calculate confidence intervals for the effect
v0.7.0,Calculate the effects
v0.7.0,Calculate the standard deviations for the effects
v0.7.0,"Get a list of (parameter, covariance matrix) pairs"
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"if both X and W are None, just return a column of ones"
v0.7.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.7.0,We need to go back to the label representation of the one-hot so as to call
v0.7.0,the classifier.
v0.7.0,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.7.0,This works both with our without the weighting trick as the treatments T are unit vector
v0.7.0,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.7.0,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.7.0,both Parametric and Non Parametric DML.
v0.7.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.7.0,for internal use by the library
v0.7.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.7.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.7.0,since we clone it and fit separate copies
v0.7.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.7.0,TODO: support sample_var
v0.7.0,"TODO: consider whether we need more care around stateful featurizers,"
v0.7.0,since we clone it and fit separate copies
v0.7.0,add statsmodels to parent's options
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,#######################################
v0.7.0,Core DML Tests
v0.7.0,#######################################
v0.7.0,How many samples
v0.7.0,How many control features
v0.7.0,How many treatment variables
v0.7.0,Coefficients of how controls affect treatments
v0.7.0,Coefficients of how controls affect outcome
v0.7.0,Treatment effects that we want to estimate
v0.7.0,Run dml estimation
v0.7.0,How many samples
v0.7.0,How many control features
v0.7.0,How many treatment variables
v0.7.0,Coefficients of how controls affect treatments
v0.7.0,Coefficients of how controls affect outcome
v0.7.0,Treatment effects that we want to estimate
v0.7.0,Run dml estimation
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.7.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"this will have dimension (d,) + shape(X)"
v0.7.0,send the first dimension to the end
v0.7.0,columns are featurized independently; partial derivatives are only non-zero
v0.7.0,when taken with respect to the same column each time
v0.7.0,don't fit intercept; manually add column of ones to the data instead;
v0.7.0,this allows us to ignore the intercept when computing marginal effects
v0.7.0,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.7.0,store number of columns of T so that we can pass scalars to effect
v0.7.0,TODO: support vector T and Y
v0.7.0,two stage approximation
v0.7.0,"first, get basis expansions of T, X, and Z"
v0.7.0,TODO: is it right that the effective number of intruments is the
v0.7.0,"product of ft_X and ft_Z, not just ft_Z?"
v0.7.0,"regress T expansion on X,Z expansions concatenated with W"
v0.7.0,"predict ft_T from interacted ft_X, ft_Z"
v0.7.0,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.7.0,dT may be only 2-dimensional)
v0.7.0,promote dT to 3D if necessary (e.g. if T was a vector)
v0.7.0,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.7.0,(which needs to have been expanded if there's a discrete treatment)
v0.7.0,We can write effect interval as a function of const_marginal_effect_interval for a single treatment
v0.7.0,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.7.0,d_t=1 here since we measure the effect across all Ts
v0.7.0,once the estimator has been fit
v0.7.0,We can write effect interval as a function of predict_interval of the final method for linear models
v0.7.0,We can write effect inference as a function of prediction and prediction standard error of
v0.7.0,the final method for linear models
v0.7.0,d_t=1 here since we measure the effect across all Ts
v0.7.0,"once the estimator has been fit, it's kosher to store d_t here"
v0.7.0,(which needs to have been expanded if there's a discrete treatment)
v0.7.0,"send treatment to the end, pull bounds to the front"
v0.7.0,d_t=1 here since we measure the effect across all Ts
v0.7.0,need to set the fit args before the estimator is fit
v0.7.0,1. Uncertainty of Mean Point Estimate
v0.7.0,2. Distribution of Point Estimate
v0.7.0,3. Total Variance of Point Estimate
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"A cut-down version of the DML first stage wrapper, since we don't need to support W or linear first stages"
v0.7.0,"if both X and Z are None, just return a column of ones"
v0.7.0,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.7.0,We need to go back to the label representation of the one-hot so as to call
v0.7.0,the classifier.
v0.7.0,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.7.0,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.7.0,since we would actually be calculating a CATE rather than ATE.
v0.7.0,TODO: allow the final model to actually use X?
v0.7.0,TODO: allow the final model to actually use X?
v0.7.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.7.0,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.7.0,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.7.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.7.0,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.7.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.7.0,for internal use by the library
v0.7.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.7.0,Estimate final model of theta(X) by minimizing the square loss:
v0.7.0,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.7.0,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.7.0,at the expense of some small bias. For points with very small covariance we revert
v0.7.0,to the model-based preliminary estimate and do not add the correction term.
v0.7.0,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.7.0,"instruments, and outcomes"
v0.7.0,TODO: how do we incorporate the sample_weight and sample_var passed into this method
v0.7.0,as arguments?
v0.7.0,TODO: is there a good way to incorporate the other nuisance terms in the score?
v0.7.0,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.7.0,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.7.0,for internal use by the library
v0.7.0,NOTE This is used by the inference methods and is more for internal use to the library
v0.7.0,"we need to undo the one-hot encoding for calling effect,"
v0.7.0,since it expects raw values
v0.7.0,"TODO: check that Y, T, Z do not have multiple columns"
v0.7.0,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.7.0,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.7.0,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.7.0,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.7.0,checks that X is 2D array.
v0.7.0,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.7.0,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.7.0,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.7.0,"Replacing this method which is invalid for this class, so that we make the"
v0.7.0,dosctring empty and not appear in the docs.
v0.7.0,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.7.0,TODO: support sample_var
v0.7.0,"Replacing this method which is invalid for this class, so that we make the"
v0.7.0,dosctring empty and not appear in the docs.
v0.7.0,add statsmodels to parent's options
v0.7.0,Replacing to remove docstring
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,TODO: make sure to use random seeds wherever necessary
v0.7.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.7.0,"unfortunately with the Theano and Tensorflow backends,"
v0.7.0,the straightforward use of K.stop_gradient can cause an error
v0.7.0,because the parameters of the intermediate layers are now disconnected from the loss;
v0.7.0,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.7.0,so that those layers remain connected but with 0 gradient
v0.7.0,|| t - mu_i || ^2
v0.7.0,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.7.0,Use logsumexp for numeric stability:
v0.7.0,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.7.0,TODO: does the numeric stability actually make any difference?
v0.7.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.7.0,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.7.0,generate cumulative sum via matrix multiplication
v0.7.0,"Generate standard uniform values in shape (batch_size,1)"
v0.7.0,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.7.0,we use uniform_like instead with an input of an appropriate shape)
v0.7.0,convert to floats and multiply to perform equivalent of logical AND
v0.7.0,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.7.0,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.7.0,we use normal_like instead with an input of an appropriate shape)
v0.7.0,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.7.0,prevent gradient from passing through sampling
v0.7.0,three options: biased or upper-bound loss require a single number of samples;
v0.7.0,unbiased can take different numbers for the network and its gradient
v0.7.0,"sample: (() -> Layer, int) -> Layer"
v0.7.0,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.7.0,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.7.0,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.7.0,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.7.0,the dimensionality of the output of the network
v0.7.0,TODO: is there a more robust way to do this?
v0.7.0,TODO: do we need to give the user more control over other arguments to fit?
v0.7.0,"subtle point: we need to build a new model each time,"
v0.7.0,because each model encapsulates its randomness
v0.7.0,TODO: do we need to give the user more control over other arguments to fit?
v0.7.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.7.0,not a general tensor (because of how backprop works in every framework)
v0.7.0,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.7.0,but this seems annoying...)
v0.7.0,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.7.0,TODO: any way to get this to work on batches of arbitrary size?
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,TODO: generalize to multiple treatment case?
v0.7.0,get index of best treatment
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,#######################################################
v0.7.0,Perfect Data DGPs for Testing Correctness of Code
v0.7.0,#######################################################
v0.7.0,Generate random control co-variates
v0.7.0,Create epsilon residual treatments that deterministically sum up to
v0.7.0,zero
v0.7.0,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.7.0,conditional on each co-variate vector is equal to zero
v0.7.0,We simply subtract the conditional mean from the epsilons
v0.7.0,Construct treatments as T = X*A + epsilon
v0.7.0,Construct outcomes as y = X*beta + T*effect
v0.7.0,Generate random control co-variates
v0.7.0,Create epsilon residual treatments that deterministically sum up to
v0.7.0,zero
v0.7.0,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.7.0,conditional on each co-variate vector is equal to zero
v0.7.0,We simply subtract the conditional mean from the epsilons
v0.7.0,Construct treatments as T = X*A + epsilon
v0.7.0,Construct outcomes as y = X*beta + T*effect
v0.7.0,Generate random control co-variates
v0.7.0,Construct treatments as T = X*A + epsilon
v0.7.0,Construct outcomes as y = X*beta + T*effect
v0.7.0,Generate random control co-variates
v0.7.0,Create epsilon residual treatments
v0.7.0,Construct treatments as T = X*A + epsilon
v0.7.0,Construct outcomes as y = X*beta + T*effect + eta
v0.7.0,Generate random control co-variates
v0.7.0,Use the same treatment vector for each row
v0.7.0,Construct outcomes as y = X*beta + T*effect
v0.7.0,Licensed under the MIT License.
v0.7.0,"since inference objects can be stateful, we must copy it before fitting;"
v0.7.0,otherwise this sequence wouldn't work:
v0.7.0,"est1.fit(..., inference=inf)"
v0.7.0,"est2.fit(..., inference=inf)"
v0.7.0,est1.effect_interval(...)
v0.7.0,because inf now stores state from fitting est2
v0.7.0,call the wrapped fit method
v0.7.0,NOTE: we call inference fit *after* calling the main fit method
v0.7.0,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.7.0,but tensordot can't be applied to this problem because we don't sum over m
v0.7.0,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.7.0,of rows of T was not taken into account
v0.7.0,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.7.0,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.7.0,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.7.0,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.7.0,add statsmodels to parent's options
v0.7.0,add debiasedlasso to parent's options
v0.7.0,TODO Share some logic with non-discrete version
v0.7.0,add statsmodels to parent's options
v0.7.0,add statsmodels to parent's options
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Check if model is sparse enough for this model
v0.7.0,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.7.0,TODO: any way to avoid creating a copy if the array was already dense?
v0.7.0,"the call is necessary if the input was something like a list, though"
v0.7.0,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.7.0,so convert to pydata sparse first
v0.7.0,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.7.0,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.7.0,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.7.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.7.0,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.7.0,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.7.0,same number of input definitions as arrays
v0.7.0,input definitions have same number of dimensions as each array
v0.7.0,all result indices are unique
v0.7.0,all result indices must match at least one input index
v0.7.0,"map indices to all array, axis pairs for that index"
v0.7.0,each index has the same cardinality wherever it appears
v0.7.0,"State: list of (set of letters, list of (corresponding indices, value))"
v0.7.0,Algo: while list contains more than one entry
v0.7.0,take two entries
v0.7.0,sort both lists by intersection of their indices
v0.7.0,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.7.0,"take the union of indices and the product of values), stepping through each list linearly"
v0.7.0,TODO: might be faster to break into connected components first
v0.7.0,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.7.0,"so compute their content separately, then take cartesian product"
v0.7.0,this would save a few pointless sorts by empty tuples
v0.7.0,TODO: Consider investigating other performance ideas for these cases
v0.7.0,where the dense method beat the sparse method (usually sparse is faster)
v0.7.0,"e,facd,c->cfed"
v0.7.0,sparse: 0.0335489
v0.7.0,dense:  0.011465999999999997
v0.7.0,"gbd,da,egb->da"
v0.7.0,sparse: 0.0791625
v0.7.0,dense:  0.007319099999999995
v0.7.0,"dcc,d,faedb,c->abe"
v0.7.0,sparse: 1.2868097
v0.7.0,dense:  0.44605229999999985
v0.7.0,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.7.0,TODO: would using einsum's paths to optimize the order of merging help?
v0.7.0,Normalize weights
v0.7.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.7.0,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.7.0,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.7.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.7.0,clean way of achieving this
v0.7.0,make sure we don't accidentally escape anything in the substitution
v0.7.0,Fetch appropriate color for node
v0.7.0,"red for negative, green for positive"
v0.7.0,in multi-target use first target
v0.7.0,Write node mean CATE
v0.7.0,Write node std of CATE
v0.7.0,Write confidence interval information if at leaf node
v0.7.0,Fetch appropriate color for node
v0.7.0,"red for negative, green for positive"
v0.7.0,Write node mean CATE
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.7.0,use a binary array to get stratified split in case of discrete treatment
v0.7.0,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.7.0,drop first column since all columns sum to one
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Estimators
v0.7.0,Causal tree parameters
v0.7.0,Tree structure
v0.7.0,No need for a random split since the data is already
v0.7.0,a random subsample from the original input
v0.7.0,node list stores the nodes that are yet to be splitted
v0.7.0,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.7.0,Create local sample set
v0.7.0,Compute nuisance estimates for the current node
v0.7.0,Nuisance estimate cannot be calculated
v0.7.0,Estimate parameter for current node
v0.7.0,Node estimate cannot be calculated
v0.7.0,Calculate moments and gradient of moments for current data
v0.7.0,Calculate inverse gradient
v0.7.0,The gradient matrix is not invertible.
v0.7.0,No good split can be found
v0.7.0,Calculate point-wise pseudo-outcomes rho
v0.7.0,a split is determined by a feature and a sample pair
v0.7.0,the number of possible splits is at most (number of features) * (number of node samples)
v0.7.0,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.7.0,parse row and column of random pair
v0.7.0,the sample of the pair is the integer division of the random number with n_feats
v0.7.0,calculate the binary indicator of whether sample i is on the left or the right
v0.7.0,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.7.0,calculate the number of samples on the left child for each proposed split
v0.7.0,calculate the analogous binary indicator for the samples in the estimation set
v0.7.0,calculate the number of estimation samples on the left child of each proposed split
v0.7.0,find the upper and lower bound on the size of the left split for the split
v0.7.0,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.7.0,on each side.
v0.7.0,similarly for the estimation sample set
v0.7.0,if there is no valid split then don't create any children
v0.7.0,filter only the valid splits
v0.7.0,calculate the average influence vector of the samples in the left child
v0.7.0,calculate the average influence vector of the samples in the right child
v0.7.0,take the square of each of the entries of the influence vectors and normalize
v0.7.0,by size of each child
v0.7.0,calculate the vector score of each candidate split as the average of left and right
v0.7.0,influence vectors
v0.7.0,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.7.0,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.7.0,where there might be large discontinuities in some parameter as the conditioning set varies
v0.7.0,calculate the scalar score of each split by aggregating across the vector of scores
v0.7.0,Find split that minimizes criterion
v0.7.0,Create child nodes with corresponding subsamples
v0.7.0,add the created children to the list of not yet split nodes
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,TODO: Add a __dir__ implementation?
v0.7.0,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.7.0,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.7.0,then we should be computing a confidence interval for the wrapped calls
v0.7.0,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.7.0,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.7.0,"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
v0.7.0,AzureML
v0.7.0,helper imports
v0.7.0,write the details of the workspace to a configuration file to the notebook library
v0.7.0,if y is a multioutput model
v0.7.0,Make sure second dimension has 1 or more item
v0.7.0,switch _inner Model to a MultiOutputRegressor
v0.7.0,flatten array as automl only takes vectors for y
v0.7.0,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.7.0,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.7.0,as an sklearn estimator
v0.7.0,fit implementation for a single output model.
v0.7.0,Create experiment for specified workspace
v0.7.0,Configure automl_config with training set information.
v0.7.0,"Wait for remote run to complete, the set the model"
v0.7.0,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.7.0,create model and pass model into final.
v0.7.0,"If item is an automl config, get its corresponding"
v0.7.0,AutomatedML Model and add it to new_Args
v0.7.0,"If item is an automl config, get its corresponding"
v0.7.0,AutomatedML Model and set it for this key in
v0.7.0,kwargs
v0.7.0,takes in either automated_ml config and instantiates
v0.7.0,an AutomatedMLModel
v0.7.0,The prefix can only be 18 characters long
v0.7.0,"because prefixes come from kwarg_names, we must ensure they are"
v0.7.0,short enough.
v0.7.0,Get workspace from config file.
v0.7.0,Take the intersect of the white for sample
v0.7.0,weights and linear models
v0.7.0,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.7.0,Remove children with nonwhite mothers from the treatment group
v0.7.0,Remove children with nonwhite mothers from the treatment group
v0.7.0,Select columns
v0.7.0,Scale the numeric variables
v0.7.0,"Change the binary variable 'first' takes values in {1,2}"
v0.7.0,Append a column of ones as intercept
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.7.0,creating notebooks that are annoying for our users to actually run themselves
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.7.0,"prior to calling interpret, can't plot, render, etc."
v0.7.0,can interpret without uncertainty
v0.7.0,can't interpret with uncertainty if inference wasn't used during fit
v0.7.0,can interpret with uncertainty if we refit
v0.7.0,can interpret without uncertainty
v0.7.0,can't treat before interpreting
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,simple DGP only for illustration
v0.7.0,Define the treatment model neural network architecture
v0.7.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.7.0,"so the input shape is (d_z + d_x,)"
v0.7.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.7.0,add extra layers on top for the mixture density network
v0.7.0,Define the response model neural network architecture
v0.7.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.7.0,"so the input shape is (d_t + d_x,)"
v0.7.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.7.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.7.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.7.0,so that the same weights will be reused in each instantiation
v0.7.0,number of samples to use in second estimate of the response
v0.7.0,(to make loss estimate unbiased)
v0.7.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.7.0,do something with predictions...
v0.7.0,also test vector t and y
v0.7.0,simple DGP only for illustration
v0.7.0,Define the treatment model neural network architecture
v0.7.0,"This will take the concatenation of one-dimensional values z and x as input,"
v0.7.0,"so the input shape is (d_z + d_x,)"
v0.7.0,The exact shape of the final layer is not critical because the Deep IV framework will
v0.7.0,add extra layers on top for the mixture density network
v0.7.0,Define the response model neural network architecture
v0.7.0,"This will take the concatenation of one-dimensional values t and x as input,"
v0.7.0,"so the input shape is (d_t + d_x,)"
v0.7.0,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.7.0,"NOTE: For the response model, it is important to define the model *outside*"
v0.7.0,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.7.0,so that the same weights will be reused in each instantiation
v0.7.0,number of samples to use in second estimate of the response
v0.7.0,(to make loss estimate unbiased)
v0.7.0,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.7.0,do something with predictions...
v0.7.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.7.0,test = True ensures we draw test set images
v0.7.0,test = True ensures we draw test set images
v0.7.0,re-draw to get new independent treatment and implied response
v0.7.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.7.0,above is necesary so that reduced form doesn't win
v0.7.0,covariates: time and emotion
v0.7.0,random instrument
v0.7.0,z -> price
v0.7.0,true observable demand function
v0.7.0,errors
v0.7.0,response
v0.7.0,test = True ensures we draw test set images
v0.7.0,test = True ensures we draw test set images
v0.7.0,re-draw to get new independent treatment and implied response
v0.7.0,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.7.0,above is necesary so that reduced form doesn't win
v0.7.0,covariates: time and emotion
v0.7.0,random instrument
v0.7.0,z -> price
v0.7.0,true observable demand function
v0.7.0,errors
v0.7.0,response
v0.7.0,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.7.0,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.7.0,For some reason this doesn't work at all when run against the CNTK backend...
v0.7.0,"model.compile('nadam', loss=lambda _,l:l)"
v0.7.0,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.7.0,generate a valiation set
v0.7.0,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.7.0,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.7.0,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.7.0,so we need to transpose the result
v0.7.0,1-d output
v0.7.0,2-d output
v0.7.0,Single dimensional output y
v0.7.0,Multi-dimensional output y
v0.7.0,1-d y
v0.7.0,multi-d y
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,test that we can fit with the same arguments as the base estimator
v0.7.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.7.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0,with the same shape for the lower and upper bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,test that we can do the same thing once we provide percentile bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.7.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0,with the same shape for the lower and upper bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,test that we can do the same thing once we provide percentile bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,test that we can fit with the same arguments as the base estimator
v0.7.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.7.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0,with the same shape for the lower and upper bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,test that we can do the same thing once we provide percentile bounds
v0.7.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.7.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0,with the same shape for the lower and upper bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,test that we can do the same thing once we provide percentile bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,test that we can fit with the same arguments as the base estimator
v0.7.0,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.7.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0,with the same shape for the lower and upper bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,test that we can do the same thing once we provide percentile bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.7.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0,with the same shape for the lower and upper bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,test that we can do the same thing once we provide percentile bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0,with the same shape for the lower and upper bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,test that the estimated effect is usually within the bounds
v0.7.0,test that we can do the same thing once we provide alpha explicitly
v0.7.0,test that the lower and upper bounds differ
v0.7.0,test that the estimated effect is usually within the bounds
v0.7.0,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0,with the same shape for the lower and upper bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,TODO: test that the estimated effect is usually within the bounds
v0.7.0,and that the true effect is also usually within the bounds
v0.7.0,test that we can do the same thing once we provide percentile bounds
v0.7.0,test that the lower and upper bounds differ
v0.7.0,TODO: test that the estimated effect is usually within the bounds
v0.7.0,and that the true effect is also usually within the bounds
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,DGP constants
v0.7.0,Generate data
v0.7.0,Test inference results when `cate_feature_names` doesn not exist
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Test non keyword based calls to fit
v0.7.0,test non-array inputs
v0.7.0,Test custom splitter
v0.7.0,Test incomplete set of test folds
v0.7.0,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,make sure cross product varies more slowly with first array
v0.7.0,and that vectors are okay as inputs
v0.7.0,number of inputs in specification must match number of inputs
v0.7.0,must have an output
v0.7.0,output indices must be unique
v0.7.0,output indices must be present in an input
v0.7.0,number of indices must match number of dimensions for each input
v0.7.0,repeated indices must always have consistent sizes
v0.7.0,transpose
v0.7.0,tensordot
v0.7.0,trace
v0.7.0,TODO: set up proper flag for this
v0.7.0,pick indices at random with replacement from the first 7 letters of the alphabet
v0.7.0,"of all of the distinct indices that appear in any input,"
v0.7.0,pick a random subset of them (of size at most 5) to appear in the output
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Preprocess data
v0.7.0,Convert 'week' to a date
v0.7.0,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.7.0,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.7.0,Take log of price
v0.7.0,Make brand numeric
v0.7.0,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.7.0,which have all zero coeffs
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.7.0,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.7.0,TODO: test something rather than just print...
v0.7.0,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.7.0,pick some arbitrary X
v0.7.0,pick some arbitrary T
v0.7.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,ensure that we've got at least two of every row
v0.7.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.7.0,need to make sure we get all *joint* combinations
v0.7.0,IntentToTreat only supports binary treatments/instruments
v0.7.0,IntentToTreat only supports binary treatments/instruments
v0.7.0,TODO: add stratification to bootstrap so that we can use it
v0.7.0,even with discrete treatments
v0.7.0,IntentToTreat requires X
v0.7.0,these support only W but not X
v0.7.0,"these support only binary, not general discrete T and Z"
v0.7.0,make sure we can call the marginal_effect and effect methods
v0.7.0,TODO: add tests for extra properties like coef_ where they exist
v0.7.0,TODO: add tests for extra properties like coef_ where they exist
v0.7.0,"make sure we can call effect with implied scalar treatments,"
v0.7.0,"no matter the dimensions of T, and also that we warn when there"
v0.7.0,are multiple treatments
v0.7.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.7.0,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.7.0,"however, with custom splits the checking happens in the first stage wrapper"
v0.7.0,where we don't have all of the required information to do this;
v0.7.0,we'd probably need to add it to _crossfit instead
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.7.0,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.7.0,The __warningregistry__'s need to be in a pristine state for tests
v0.7.0,to work properly.
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Set random seed
v0.7.0,Generate data
v0.7.0,DGP constants
v0.7.0,Test data
v0.7.0,Constant treatment effect
v0.7.0,Constant treatment with multi output Y
v0.7.0,Heterogeneous treatment
v0.7.0,Heterogeneous treatment with multi output Y
v0.7.0,TLearner test
v0.7.0,Instantiate TLearner
v0.7.0,Test inputs
v0.7.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.7.0,Instantiate SLearner
v0.7.0,Test inputs
v0.7.0,Test constant treatment effect
v0.7.0,Test constant treatment effect with multi output Y
v0.7.0,Test heterogeneous treatment effect
v0.7.0,Need interactions between T and features
v0.7.0,Test heterogeneous treatment effect with multi output Y
v0.7.0,Instantiate XLearner
v0.7.0,Test inputs
v0.7.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.7.0,Instantiate DomainAdaptationLearner
v0.7.0,Test inputs
v0.7.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.7.0,Get the true treatment effect
v0.7.0,Get the true treatment effect
v0.7.0,Fit learner and get the effect and marginal effect
v0.7.0,Compute treatment effect residuals (absolute)
v0.7.0,Check that at least 90% of predictions are within tolerance interval
v0.7.0,Check whether the output shape is right
v0.7.0,Check that one can pass in regular lists
v0.7.0,Check that it fails correctly if lists of different shape are passed in
v0.7.0,"Check that it works when T, Y have shape (n, 1)"
v0.7.0,Generate covariates
v0.7.0,Generate treatment
v0.7.0,Calculate outcome
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,DGP constants
v0.7.0,Generate data
v0.7.0,Test data
v0.7.0,Remove warnings that might be raised by the models passed into the ORF
v0.7.0,Generate data with continuous treatments
v0.7.0,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.7.0,does not work well with parallelism.
v0.7.0,Test inputs for continuous treatments
v0.7.0,--> Check that one can pass in regular lists
v0.7.0,--> Check that it fails correctly if lists of different shape are passed in
v0.7.0,Check that outputs have the correct shape
v0.7.0,Test continuous treatments with controls
v0.7.0,Test continuous treatments without controls
v0.7.0,Generate data with binary treatments
v0.7.0,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.7.0,does not work well with parallelism.
v0.7.0,Test inputs for binary treatments
v0.7.0,--> Check that one can pass in regular lists
v0.7.0,--> Check that it fails correctly if lists of different shape are passed in
v0.7.0,"--> Check that it works when T, Y have shape (n, 1)"
v0.7.0,"--> Check that it fails correctly when T has shape (n, 2)"
v0.7.0,--> Check that it fails correctly when the treatments are not numeric
v0.7.0,Check that outputs have the correct shape
v0.7.0,Test binary treatments with controls
v0.7.0,Test binary treatments without controls
v0.7.0,Only applicable to continuous treatments
v0.7.0,Generate data for 2 treatments
v0.7.0,Test multiple treatments with controls
v0.7.0,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.7.0,The rest for controls. Just as an example.
v0.7.0,Generating A/B test data
v0.7.0,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.7.0,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.7.0,Generate data with continuous treatments
v0.7.0,Instantiate model with most of the default parameters
v0.7.0,Compute the treatment effect on test points
v0.7.0,Compute treatment effect residuals
v0.7.0,Multiple treatments
v0.7.0,Allow at most 10% test points to be outside of the tolerance interval
v0.7.0,Compute the treatment effect on test points
v0.7.0,Compute treatment effect residuals
v0.7.0,Multiple treatments
v0.7.0,Allow at most 20% test points to be outside of the confidence interval
v0.7.0,Check that the intervals are not too wide
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.7.0,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.7.0,ensure that we've got at least two of every element
v0.7.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.7.0,TODO: add stratification to bootstrap so that we can use it
v0.7.0,even with discrete treatments
v0.7.0,make sure we can call the marginal_effect and effect methods
v0.7.0,test const marginal inference
v0.7.0,test effect inference
v0.7.0,test marginal effect inference
v0.7.0,test coef__inference and intercept__inference
v0.7.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.7.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.7.0,"make sure we can call effect with implied scalar treatments,"
v0.7.0,"no matter the dimensions of T, and also that we warn when there"
v0.7.0,are multiple treatments
v0.7.0,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.7.0,ensure that we've got at least two of every element
v0.7.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.7.0,"TODO Add bootstrap inference, once discrete treatment issue is fixed"
v0.7.0,make sure we can call the marginal_effect and effect methods
v0.7.0,test const marginal inference
v0.7.0,test effect inference
v0.7.0,test marginal effect inference
v0.7.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.7.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.7.0,We concatenate the two copies data
v0.7.0,create a simple artificial setup where effect of moving from treatment
v0.7.0,"1 -> 2 is 2,"
v0.7.0,"1 -> 3 is 1, and"
v0.7.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.7.0,"Using an uneven number of examples from different classes,"
v0.7.0,"and having the treatments in non-lexicographic order,"
v0.7.0,Should rule out some basic issues.
v0.7.0,test that we can fit with a KFold instance
v0.7.0,test that we can fit with a train/test iterable
v0.7.0,"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
v0.7.0,(incorrectly) use a final model with an intercept
v0.7.0,"Because final model is fixed, actual values of T and Y don't matter"
v0.7.0,Ensure reproducibility
v0.7.0,Sparse DGP
v0.7.0,Treatment effect coef
v0.7.0,Other coefs
v0.7.0,Features and controls
v0.7.0,Test sparse estimator
v0.7.0,"--> test coef_, intercept_"
v0.7.0,--> test treatment effects
v0.7.0,Restrict x_test to vectors of norm < 1
v0.7.0,--> check inference
v0.7.0,Check that a majority of true effects lie in the 5-95% CI
v0.7.0,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.7.0,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.7.0,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.7.0,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.7.0,sparse test case: heterogeneous effect by product
v0.7.0,need at least as many rows in e_y as there are distinct columns
v0.7.0,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Set random seed
v0.7.0,Generate data
v0.7.0,DGP constants
v0.7.0,Test data
v0.7.0,Constant treatment effect and propensity
v0.7.0,Heterogeneous treatment and propensity
v0.7.0,ensure that we've got at least two of every element
v0.7.0,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.7.0,TODO: add stratification to bootstrap so that we can use it even with discrete treatments
v0.7.0,make sure we can call the marginal_effect and effect methods
v0.7.0,test const marginal inference
v0.7.0,test effect inference
v0.7.0,test marginal effect inference
v0.7.0,"make sure we can call effect with implied scalar treatments, no matter the"
v0.7.0,"dimensions of T, and also that we warn when there are multiple treatments"
v0.7.0,create a simple artificial setup where effect of moving from treatment
v0.7.0,"1 -> 2 is 2,"
v0.7.0,"1 -> 3 is 1, and"
v0.7.0,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.7.0,"Using an uneven number of examples from different classes,"
v0.7.0,"and having the treatments in non-lexicographic order,"
v0.7.0,Should rule out some basic issues.
v0.7.0,test that we can fit with a KFold instance
v0.7.0,test that we can fit with a train/test iterable
v0.7.0,"for at least some of the examples, the CI should have nonzero width"
v0.7.0,"for at least some of the examples, the CI should have nonzero width"
v0.7.0,"for at least some of the examples, the CI should have nonzero width"
v0.7.0,test coef__inference function works
v0.7.0,test intercept__inference function works
v0.7.0,test summary function works
v0.7.0,Test inputs
v0.7.0,self._test_inputs(DR_learner)
v0.7.0,Test constant treatment effect
v0.7.0,Test heterogeneous treatment effect
v0.7.0,Test heterogenous treatment effect for W =/= None
v0.7.0,Sparse DGP
v0.7.0,Treatment effect coef
v0.7.0,Other coefs
v0.7.0,Features and controls
v0.7.0,Test sparse estimator
v0.7.0,"--> test coef_, intercept_"
v0.7.0,--> test treatment effects
v0.7.0,Restrict x_test to vectors of norm < 1
v0.7.0,--> check inference
v0.7.0,Check that a majority of true effects lie in the 5-95% CI
v0.7.0,Fit learner and get the effect
v0.7.0,Get the true treatment effect
v0.7.0,Compute treatment effect residuals (absolute)
v0.7.0,Check that at least 90% of predictions are within tolerance interval
v0.7.0,Only for heterogeneous TE
v0.7.0,Fit learner on X and W and get the effect
v0.7.0,Get the true treatment effect
v0.7.0,Compute treatment effect residuals (absolute)
v0.7.0,Check that at least 90% of predictions are within tolerance interval
v0.7.0,Check that one can pass in regular lists
v0.7.0,Check that it fails correctly if lists of different shape are passed in
v0.7.0,Check that it fails when T contains values other than 0 and 1
v0.7.0,"Check that it works when T, Y have shape (n, 1)"
v0.7.0,Generate covariates
v0.7.0,Generate treatment
v0.7.0,Calculate outcome
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,DGP constants
v0.7.0,DGP coefficients
v0.7.0,Generated outcomes
v0.7.0,################
v0.7.0,WeightedLasso #
v0.7.0,################
v0.7.0,Define weights
v0.7.0,Define extended datasets
v0.7.0,Range of alphas
v0.7.0,Compare with Lasso
v0.7.0,--> No intercept
v0.7.0,--> With intercept
v0.7.0,When DGP has no intercept
v0.7.0,When DGP has intercept
v0.7.0,--> Coerce coefficients to be positive
v0.7.0,--> Toggle max_iter & tol
v0.7.0,Define weights
v0.7.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.7.0,Mixed DGP scenario.
v0.7.0,Define extended datasets
v0.7.0,Define weights
v0.7.0,Define multioutput
v0.7.0,##################
v0.7.0,WeightedLassoCV #
v0.7.0,##################
v0.7.0,Define alphas to test
v0.7.0,Compare with LassoCV
v0.7.0,--> No intercept
v0.7.0,--> With intercept
v0.7.0,--> Force parameters to be positive
v0.7.0,Choose a smaller n to speed-up process
v0.7.0,Compare fold weights
v0.7.0,Define weights
v0.7.0,Define extended datasets
v0.7.0,Define splitters
v0.7.0,WeightedKFold splitter
v0.7.0,Map weighted splitter to an extended splitter
v0.7.0,Define alphas to test
v0.7.0,Compare with LassoCV
v0.7.0,--> No intercept
v0.7.0,--> With intercept
v0.7.0,--> Force parameters to be positive
v0.7.0,###########################
v0.7.0,MultiTaskWeightedLassoCV #
v0.7.0,###########################
v0.7.0,Define alphas to test
v0.7.0,Define splitter
v0.7.0,Compare with MultiTaskLassoCV
v0.7.0,--> No intercept
v0.7.0,--> With intercept
v0.7.0,Define weights
v0.7.0,Define extended datasets
v0.7.0,Define splitters
v0.7.0,WeightedKFold splitter
v0.7.0,Map weighted splitter to an extended splitter
v0.7.0,Define alphas to test
v0.7.0,Compare with LassoCV
v0.7.0,--> No intercept
v0.7.0,--> With intercept
v0.7.0,################
v0.7.0,DebiasedLasso #
v0.7.0,################
v0.7.0,Test DebiasedLasso without weights
v0.7.0,--> Check debiased coeffcients without intercept
v0.7.0,--> Check debiased coeffcients with intercept
v0.7.0,--> Check 5-95 CI coverage for unit vectors
v0.7.0,Test DebiasedLasso with weights for one DGP
v0.7.0,Define weights
v0.7.0,Define extended datasets
v0.7.0,--> Check debiased coefficients
v0.7.0,Define weights
v0.7.0,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.7.0,--> Check debiased coeffcients
v0.7.0,Test that attributes propagate correctly
v0.7.0,Test MultiOutputDebiasedLasso without weights
v0.7.0,--> Check debiased coeffcients without intercept
v0.7.0,--> Check debiased coeffcients with intercept
v0.7.0,--> Check CI coverage
v0.7.0,Test MultiOutputDebiasedLasso with weights
v0.7.0,Define weights
v0.7.0,Define extended datasets
v0.7.0,--> Check debiased coefficients
v0.7.0,Unit vectors
v0.7.0,Unit vectors
v0.7.0,Check coeffcients and intercept are the same within tolerance
v0.7.0,Check results are similar with tolerance 1e-6
v0.7.0,Check if multitask
v0.7.0,Check that same alpha is chosen
v0.7.0,Check that the coefficients are similar
v0.7.0,selective ridge has a simple implementation that we can test against
v0.7.0,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.7.0,"it should be the case that when we set fit_intercept to true,"
v0.7.0,it doesn't matter whether the penalized model also fits an intercept or not
v0.7.0,create an extra copy of rows with weight 2
v0.7.0,"instead of a slice, explicitly return an array of indices"
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Linear models are required for parametric dml
v0.7.0,sample weighting models are required for nonparametric dml
v0.7.0,Test values
v0.7.0,TLearner test
v0.7.0,Instantiate TLearner
v0.7.0,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.7.0,Test constant treatment effect with multi output Y
v0.7.0,Test heterogeneous treatment effect
v0.7.0,Need interactions between T and features
v0.7.0,Test heterogeneous treatment effect with multi output Y
v0.7.0,Instantiate DomainAdaptationLearner
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,"Found a good split, return."
v0.7.0,Record all splits in case the stratification by weight yeilds a worse partition
v0.7.0,Reseed random generator and try again
v0.7.0,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.7.0,"Found a good split, return."
v0.7.0,Did not find a good split
v0.7.0,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.7.0,Return most weight-balanced partition
v0.7.0,Weight stratification algorithm
v0.7.0,Sort weights for weight strata search
v0.7.0,There are some leftover indices that have yet to be assigned
v0.7.0,Append stratum splits to overall splits
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Our classes that derive from sklearn ones sometimes include
v0.7.0,inherited docstrings that have embedded doctests; we need the following imports
v0.7.0,so that they don't break.
v0.7.0,"Convert X, y into numpy arrays"
v0.7.0,Define fit parameters
v0.7.0,Some algorithms don't have a check_input option
v0.7.0,Check weights array
v0.7.0,Check that weights are size-compatible
v0.7.0,Normalize inputs
v0.7.0,Weight inputs
v0.7.0,Fit base class without intercept
v0.7.0,Fit Lasso
v0.7.0,Reset intercept
v0.7.0,The intercept is not calculated properly due the sqrt(weights) factor
v0.7.0,so it must be recomputed
v0.7.0,Fit lasso without weights
v0.7.0,Make weighted splitter
v0.7.0,Fit weighted model
v0.7.0,Make weighted splitter
v0.7.0,Fit weighted model
v0.7.0,Select optimal penalty
v0.7.0,Warn about consistency
v0.7.0,"Convert X, y into numpy arrays"
v0.7.0,Fit weighted lasso with user input
v0.7.0,"Center X, y"
v0.7.0,Calculate quantities that will be used later on. Account for centered data
v0.7.0,Calculate coefficient and error variance
v0.7.0,Add coefficient correction
v0.7.0,Set coefficients and intercept standard errors
v0.7.0,Set intercept
v0.7.0,Return alpha to 'auto' state
v0.7.0,"Note that in the case of no intercept, X_offset is 0"
v0.7.0,Calculate the variance of the predictions
v0.7.0,"Note that in the case of no intercept, X_offset is 0"
v0.7.0,Calculate the variance of the predictions
v0.7.0,Calculate prediction confidence intervals
v0.7.0,Assumes flattened y
v0.7.0,Compute weighted residuals
v0.7.0,To be done once per target. Assumes y can be flattened.
v0.7.0,Assumes that X has already been offset
v0.7.0,Special case: n_features=1
v0.7.0,Compute Lasso coefficients for the columns of the design matrix
v0.7.0,Call weighted lasso on reduced design matrix
v0.7.0,Inherit some parameters from the parent
v0.7.0,Weighted tau
v0.7.0,Compute C_hat
v0.7.0,Compute theta_hat
v0.7.0,Allow for single output as well
v0.7.0,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.7.0,Set coef_ attribute
v0.7.0,Set intercept_ attribute
v0.7.0,Set selected_alpha_ attribute
v0.7.0,Set coef_stderr_
v0.7.0,intercept_stderr_
v0.7.0,set intercept_ attribute
v0.7.0,set coef_ attribute
v0.7.0,set alpha_ attribute
v0.7.0,set alphas_ attribute
v0.7.0,set n_iter_ attribute
v0.7.0,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.7.0,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.7.0,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.7.0,now regress X1 on y - X2 * beta2 to learn beta1
v0.7.0,set coef_ and intercept_ attributes
v0.7.0,Note that the penalized model should *not* have an intercept
v0.7.0,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0,Licensed under the MIT License.
v0.7.0,Construct the subsample of data
v0.7.0,Split into estimation and splitting sample set
v0.7.0,Fit the tree on the splitting sample
v0.7.0,Set the estimation values based on the estimation split
v0.7.0,Apply the trained tree on the estimation sample to get the path for every estimation sample
v0.7.0,Calculate the total weight of estimation samples on each tree node:
v0.7.0,\sum_i sample_weight[i] * 1{i \\in node}
v0.7.0,Calculate the total number of estimation samples on each tree node:
v0.7.0,|node| = \sum_{i} 1{i \\in node}
v0.7.0,Calculate the weighted sum of responses on the estimation sample on each node:
v0.7.0,\sum_{i} sample_weight[i] 1{i \\in node} Y_i
v0.7.0,Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
v0.7.0,and for each un-pruned tree set the value and the weight appropriately.
v0.7.0,If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
v0.7.0,"sample, then prune the whole sub-tree"
v0.7.0,Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
v0.7.0,Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
v0.7.0,Set the count to the estimation split count
v0.7.0,Validate or convert input data
v0.7.0,Pre-sort indices to avoid that each individual tree of the
v0.7.0,ensemble sorts the indices.
v0.7.0,Remap output
v0.7.0,reshape is necessary to preserve the data contiguity against vs
v0.7.0,"[:, np.newaxis] that does not."
v0.7.0,Check parameters
v0.7.0,"Free allocated memory, if any"
v0.7.0,We draw from the random state to get the random state we
v0.7.0,would have got if we hadn't used a warm_start.
v0.7.0,Parallel loop: we prefer the threading backend as the Cython code
v0.7.0,for fitting the trees is internally releasing the Python GIL
v0.7.0,making threading more efficient than multiprocessing in
v0.7.0,"that case. However, for joblib 0.12+ we respect any"
v0.7.0,"parallel_backend contexts set at a higher level,"
v0.7.0,since correctness does not rely on using threads.
v0.7.0,TODO. This slicing should ultimately be done inside the parallel function
v0.7.0,so that we don't need to create a matrix of size roughly n_samples * n_estimators
v0.7.0,Collect newly grown trees
v0.7.0,Helper class that accumulates an arbitrary function in parallel on the accumulator acc
v0.7.0,and calls the function fn on each tree e and returns the mean output. The function fn
v0.7.0,"should take as input a tree e, and return another function g_e, which takes as input X, check_input"
v0.7.0,"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
v0.7.0,index start to index end will be used. The returned result is essentially:
v0.7.0,(mean over e in slice)(g_e(X)).
v0.7.0,Check data
v0.7.0,Assign chunk of trees to jobs
v0.7.0,Check data
v0.7.0,Check data
v0.7.0,avoid storing the output of every estimator by summing them here
v0.7.0,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
v0.7.0,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
v0.7.0,"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
v0.7.0,where \theta(X) is the point estimate using the whole forest
v0.7.0,Calculate the variance of the latter as E[Q(S)^2]
v0.7.0b1,configuration is all pulled from setup.cfg
v0.7.0b1,-*- coding: utf-8 -*-
v0.7.0b1,
v0.7.0b1,Configuration file for the Sphinx documentation builder.
v0.7.0b1,
v0.7.0b1,This file does only contain a selection of the most common options. For a
v0.7.0b1,full list see the documentation:
v0.7.0b1,http://www.sphinx-doc.org/en/master/config
v0.7.0b1,-- Path setup --------------------------------------------------------------
v0.7.0b1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.7.0b1,add these directories to sys.path here. If the directory is relative to the
v0.7.0b1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.7.0b1,
v0.7.0b1,-- Project information -----------------------------------------------------
v0.7.0b1,-- General configuration ---------------------------------------------------
v0.7.0b1,"If your documentation needs a minimal Sphinx version, state it here."
v0.7.0b1,
v0.7.0b1,needs_sphinx = '1.0'
v0.7.0b1,"Add any Sphinx extension module names here, as strings. They can be"
v0.7.0b1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.7.0b1,ones.
v0.7.0b1,"Add any paths that contain templates here, relative to this directory."
v0.7.0b1,The suffix(es) of source filenames.
v0.7.0b1,You can specify multiple suffix as a list of string:
v0.7.0b1,
v0.7.0b1,"source_suffix = ['.rst', '.md']"
v0.7.0b1,The master toctree document.
v0.7.0b1,The language for content autogenerated by Sphinx. Refer to documentation
v0.7.0b1,for a list of supported languages.
v0.7.0b1,
v0.7.0b1,This is also used if you do content translation via gettext catalogs.
v0.7.0b1,"Usually you set ""language"" from the command line for these cases."
v0.7.0b1,"List of patterns, relative to source directory, that match files and"
v0.7.0b1,directories to ignore when looking for source files.
v0.7.0b1,This pattern also affects html_static_path and html_extra_path.
v0.7.0b1,The name of the Pygments (syntax highlighting) style to use.
v0.7.0b1,-- Options for HTML output -------------------------------------------------
v0.7.0b1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.7.0b1,a list of builtin themes.
v0.7.0b1,
v0.7.0b1,Theme options are theme-specific and customize the look and feel of a theme
v0.7.0b1,"further.  For a list of options available for each theme, see the"
v0.7.0b1,documentation.
v0.7.0b1,
v0.7.0b1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.7.0b1,"relative to this directory. They are copied after the builtin static files,"
v0.7.0b1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.7.0b1,html_static_path = ['_static']
v0.7.0b1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.7.0b1,to template names.
v0.7.0b1,
v0.7.0b1,The default sidebars (for documents that don't match any pattern) are
v0.7.0b1,defined by theme itself.  Builtin themes are using these templates by
v0.7.0b1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.7.0b1,'searchbox.html']``.
v0.7.0b1,
v0.7.0b1,html_sidebars = {}
v0.7.0b1,-- Options for HTMLHelp output ---------------------------------------------
v0.7.0b1,Output file base name for HTML help builder.
v0.7.0b1,-- Options for LaTeX output ------------------------------------------------
v0.7.0b1,The paper size ('letterpaper' or 'a4paper').
v0.7.0b1,
v0.7.0b1,"'papersize': 'letterpaper',"
v0.7.0b1,"The font size ('10pt', '11pt' or '12pt')."
v0.7.0b1,
v0.7.0b1,"'pointsize': '10pt',"
v0.7.0b1,Additional stuff for the LaTeX preamble.
v0.7.0b1,
v0.7.0b1,"'preamble': '',"
v0.7.0b1,Latex figure (float) alignment
v0.7.0b1,
v0.7.0b1,"'figure_align': 'htbp',"
v0.7.0b1,Grouping the document tree into LaTeX files. List of tuples
v0.7.0b1,"(source start file, target name, title,"
v0.7.0b1,"author, documentclass [howto, manual, or own class])."
v0.7.0b1,-- Options for manual page output ------------------------------------------
v0.7.0b1,One entry per manual page. List of tuples
v0.7.0b1,"(source start file, name, description, authors, manual section)."
v0.7.0b1,-- Options for Texinfo output ----------------------------------------------
v0.7.0b1,Grouping the document tree into Texinfo files. List of tuples
v0.7.0b1,"(source start file, target name, title, author,"
v0.7.0b1,"dir menu entry, description, category)"
v0.7.0b1,-- Options for Epub output -------------------------------------------------
v0.7.0b1,Bibliographic Dublin Core info.
v0.7.0b1,The unique identifier of the text. This can be a ISBN number
v0.7.0b1,or the project homepage.
v0.7.0b1,
v0.7.0b1,epub_identifier = ''
v0.7.0b1,A unique identification for the text.
v0.7.0b1,
v0.7.0b1,epub_uid = ''
v0.7.0b1,A list of files that should not be packed into the epub file.
v0.7.0b1,-- Extension configuration -------------------------------------------------
v0.7.0b1,-- Options for intersphinx extension ---------------------------------------
v0.7.0b1,Example configuration for intersphinx: refer to the Python standard library.
v0.7.0b1,-- Options for todo extension ----------------------------------------------
v0.7.0b1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.7.0b1,-- Options for doctest extension -------------------------------------------
v0.7.0b1,we can document otherwise excluded entities here by returning False
v0.7.0b1,or skip otherwise included entities by returning True
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Calculate residuals
v0.7.0b1,Estimate E[T_res | Z_res]
v0.7.0b1,TODO. Deal with multi-class instrument
v0.7.0b1,Calculate nuisances
v0.7.0b1,Estimate E[T_res | Z_res]
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.7.0b1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.7.0b1,TODO. Deal with multi-class instrument
v0.7.0b1,Estimate final model of theta(X) by minimizing the square loss:
v0.7.0b1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.7.0b1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.7.0b1,at the expense of some small bias. For points with very small covariance we revert
v0.7.0b1,to the model-based preliminary estimate and do not add the correction term.
v0.7.0b1,Estimate preliminary theta in cross fitting manner
v0.7.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.7.0b1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.7.0b1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.7.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.7.0b1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.7.0b1,TODO. The solution below is not really a valid cross-fitting
v0.7.0b1,as the test data are used to create the proj_t on the train
v0.7.0b1,which in the second train-test loop is used to create the nuisance
v0.7.0b1,cov on the test data. Hence the T variable of some sample
v0.7.0b1,"is implicitly correlated with its cov nuisance, through this flow"
v0.7.0b1,"of information. However, this seems a rather weak correlation."
v0.7.0b1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.7.0b1,model.
v0.7.0b1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.7.0b1,Estimate preliminary theta in cross fitting manner
v0.7.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.7.0b1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.7.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.7.0b1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.7.0b1,#############################################################################
v0.7.0b1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.7.0b1,A/B test
v0.7.0b1,#############################################################################
v0.7.0b1,Estimate preliminary theta in cross fitting manner
v0.7.0b1,Estimate p(X) = E[T | X] in cross fitting manner
v0.7.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.7.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.7.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.7.0b1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.7.0b1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.7.0b1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.7.0b1,model_T_XZ = lambda: model_clf()
v0.7.0b1,#'days_visited': lambda:
v0.7.0b1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.7.0b1,Turn strings into categories for numeric mapping
v0.7.0b1,### Defining some generic regressors and classifiers
v0.7.0b1,This a generic non-parametric regressor
v0.7.0b1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.7.0b1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.7.0b1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.7.0b1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.7.0b1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.7.0b1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.7.0b1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.7.0b1,model = lambda: LinearRegression(n_jobs=-1)
v0.7.0b1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.7.0b1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.7.0b1,underlying model whenever predict is called.
v0.7.0b1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.7.0b1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.7.0b1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.7.0b1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.7.0b1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.7.0b1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.7.0b1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.7.0b1,We need to specify models to be used for each of these residualizations
v0.7.0b1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.7.0b1,"E[T | X, Z]"
v0.7.0b1,E[TZ | X]
v0.7.0b1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.7.0b1,n_splits determines the number of splits to be used for cross-fitting.
v0.7.0b1,# Algorithm 2 - Current Method
v0.7.0b1,In[121]:
v0.7.0b1,# Algorithm 3 - DRIV ATE
v0.7.0b1,dmliv_model_effect = lambda: model()
v0.7.0b1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.7.0b1,"dmliv_model_effect(),"
v0.7.0b1,n_splits=1)
v0.7.0b1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.7.0b1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.7.0b1,"Once multiple treatments are supported, we'll need to fix this"
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.7.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.7.0b1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.7.0b1,We can use statsmodel for all hypothesis testing capabilities
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,TODO. Deal with multi-class instrument/treatment
v0.7.0b1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.7.0b1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.7.0b1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.7.0b1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.7.0b1,##################
v0.7.0b1,Global settings #
v0.7.0b1,##################
v0.7.0b1,Global plotting controls
v0.7.0b1,"Control for support size, can control for more"
v0.7.0b1,#################
v0.7.0b1,File utilities #
v0.7.0b1,#################
v0.7.0b1,#################
v0.7.0b1,Plotting utils #
v0.7.0b1,#################
v0.7.0b1,bias
v0.7.0b1,var
v0.7.0b1,rmse
v0.7.0b1,r2
v0.7.0b1,Infer feature dimension
v0.7.0b1,Metrics by support plots
v0.7.0b1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.7.0b1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.7.0b1,Steven Wu <zhiww@microsoft.com>
v0.7.0b1,Initialize causal tree parameters
v0.7.0b1,Create splits of causal tree
v0.7.0b1,Estimate treatment effects at the leafs
v0.7.0b1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.7.0b1,the corresponding split and associating the effect computed on that leaf
v0.7.0b1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.7.0b1,Safety check
v0.7.0b1,Weighted linear regression
v0.7.0b1,Calculates weights
v0.7.0b1,Bootstraping has repetitions in tree sample so we need to iterate
v0.7.0b1,over all indices
v0.7.0b1,Similar for `a` weights
v0.7.0b1,Doesn't have sample weights
v0.7.0b1,Is a linear model
v0.7.0b1,Weighted linear regression
v0.7.0b1,Calculates weights
v0.7.0b1,Bootstraping has repetitions in tree sample so we need to iterate
v0.7.0b1,over all indices
v0.7.0b1,Similar for `a` weights
v0.7.0b1,normalize weights
v0.7.0b1,"Split the data in half, train and test"
v0.7.0b1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.7.0b1,"a function of W, using only the train fold"
v0.7.0b1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.7.0b1,"Split the data in half, train and test"
v0.7.0b1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.7.0b1,"a function of W, using only the train fold"
v0.7.0b1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.7.0b1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.7.0b1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.7.0b1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.7.0b1,"Split the data in half, train and test"
v0.7.0b1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.7.0b1,"a function of x, using only the train fold"
v0.7.0b1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.7.0b1,Compute coefficient by OLS on residuals
v0.7.0b1,"Split the data in half, train and test"
v0.7.0b1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.7.0b1,"a function of x, using only the train fold"
v0.7.0b1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.7.0b1,Estimate multipliers for second order orthogonal method
v0.7.0b1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.7.0b1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.7.0b1,Create local sample set
v0.7.0b1,compute the base estimate for the current node using double ml or second order double ml
v0.7.0b1,compute the influence functions here that are used for the criterion
v0.7.0b1,generate random proposals of dimensions to split
v0.7.0b1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.7.0b1,compute criterion for each proposal
v0.7.0b1,if splitting creates valid leafs in terms of mean leaf size
v0.7.0b1,Calculate criterion for split
v0.7.0b1,Else set criterion to infinity so that this split is not chosen
v0.7.0b1,If no good split was found
v0.7.0b1,Find split that minimizes criterion
v0.7.0b1,Set the split attributes at the node
v0.7.0b1,Create child nodes with corresponding subsamples
v0.7.0b1,Recursively split children
v0.7.0b1,Return parent node
v0.7.0b1,estimate the local parameter at the leaf using the estimate data
v0.7.0b1,###################
v0.7.0b1,Argument parsing #
v0.7.0b1,###################
v0.7.0b1,#########################################
v0.7.0b1,Parameters constant across experiments #
v0.7.0b1,#########################################
v0.7.0b1,Outcome support
v0.7.0b1,Treatment support
v0.7.0b1,Evaluation grid
v0.7.0b1,Treatment effects array
v0.7.0b1,Other variables
v0.7.0b1,##########################
v0.7.0b1,Data Generating Process #
v0.7.0b1,##########################
v0.7.0b1,Log iteration
v0.7.0b1,"Generate controls, features, treatment and outcome"
v0.7.0b1,T and Y residuals to be used in later scripts
v0.7.0b1,Save generated dataset
v0.7.0b1,#################
v0.7.0b1,ORF parameters #
v0.7.0b1,#################
v0.7.0b1,######################################
v0.7.0b1,Train and evaluate treatment effect #
v0.7.0b1,######################################
v0.7.0b1,########
v0.7.0b1,Plots #
v0.7.0b1,########
v0.7.0b1,###############
v0.7.0b1,Save results #
v0.7.0b1,###############
v0.7.0b1,##############
v0.7.0b1,Run Rscript #
v0.7.0b1,##############
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Check inputs
v0.7.0b1,Check inputs
v0.7.0b1,Check inputs
v0.7.0b1,Check inputs
v0.7.0b1,Check inputs
v0.7.0b1,Estimate response function
v0.7.0b1,Check inputs
v0.7.0b1,Train model on controls. Assign higher weight to units resembling
v0.7.0b1,treated units.
v0.7.0b1,Train model on the treated. Assign higher weight to units resembling
v0.7.0b1,control units.
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Create splits of causal tree
v0.7.0b1,Make sure the correct exception is being rethrown
v0.7.0b1,Must make sure indices are merged correctly
v0.7.0b1,Require group assignment t to be one-hot-encoded
v0.7.0b1,Define an inner function that iterates over group predictions
v0.7.0b1,Convert rows to columns
v0.7.0b1,Get predictions for the 2 splits
v0.7.0b1,Must make sure indices are merged correctly
v0.7.0b1,Estimators
v0.7.0b1,OrthoForest parameters
v0.7.0b1,Sub-forests
v0.7.0b1,Auxiliary attributes
v0.7.0b1,Fit check
v0.7.0b1,TODO: Check performance
v0.7.0b1,Override the CATE inference options
v0.7.0b1,Add blb inference to parent's options
v0.7.0b1,Must normalize weights
v0.7.0b1,Crossfitting
v0.7.0b1,Compute weighted nuisance estimates
v0.7.0b1,-------------------------------------------------------------------------------
v0.7.0b1,Calculate the covariance matrix corresponding to the BLB inference
v0.7.0b1,
v0.7.0b1,1. Calculate the moments and gradient of the training data w.r.t the test point
v0.7.0b1,2. Calculate the weighted moments for each tree slice to create a matrix
v0.7.0b1,"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
v0.7.0b1,in that slice from the overall parameter estimate.
v0.7.0b1,3. Calculate the covariance matrix (V.T x V) / n_slices
v0.7.0b1,-------------------------------------------------------------------------------
v0.7.0b1,Calclulate covariance matrix through BLB
v0.7.0b1,Generate subsample indices
v0.7.0b1,Build trees in parallel
v0.7.0b1,Bootstraping has repetitions in tree sample
v0.7.0b1,Similar for `a` weights
v0.7.0b1,Bootstraping has repetitions in tree sample
v0.7.0b1,Define subsample size
v0.7.0b1,Safety check
v0.7.0b1,Draw points to create little bags
v0.7.0b1,Copy and/or define models
v0.7.0b1,Define nuisance estimators
v0.7.0b1,Define parameter estimators
v0.7.0b1,Define
v0.7.0b1,Override to flatten output if T is flat
v0.7.0b1,T is flat
v0.7.0b1,Nuissance estimates evaluated with cross-fitting
v0.7.0b1,Define 2-fold iterator
v0.7.0b1,need safe=False when cloning for WeightedModelWrapper
v0.7.0b1,Compute residuals
v0.7.0b1,Compute coefficient by OLS on residuals
v0.7.0b1,"Parameter returned by LinearRegression is (d_T, )"
v0.7.0b1,Compute residuals
v0.7.0b1,Compute coefficient by OLS on residuals
v0.7.0b1,ell_2 regularization
v0.7.0b1,Ridge regression estimate
v0.7.0b1,"Parameter returned is of shape (d_T, )"
v0.7.0b1,Return moments and gradients
v0.7.0b1,Compute residuals
v0.7.0b1,Compute moments
v0.7.0b1,"Moments shape is (n, d_T)"
v0.7.0b1,Compute moment gradients
v0.7.0b1,Copy and/or define models
v0.7.0b1,Nuisance estimators shall be defined during fitting because they need to know the number of distinct
v0.7.0b1,treatments
v0.7.0b1,Define parameter estimators
v0.7.0b1,Define moment and mean gradient estimator
v0.7.0b1,Define autoencoder
v0.7.0b1,"Check that T is shape (n, )"
v0.7.0b1,Check T is numeric
v0.7.0b1,Train label encoder
v0.7.0b1,Define number of classes
v0.7.0b1,Call `fit` from parent class
v0.7.0b1,"Test that T contains all treatments. If not, return None"
v0.7.0b1,Nuissance estimates evaluated with cross-fitting
v0.7.0b1,Define 2-fold iterator
v0.7.0b1,Check if there is only one example of some class
v0.7.0b1,No need to crossfit for internal nodes
v0.7.0b1,Compute partial moments
v0.7.0b1,"If any of the values in the parameter estimate is nan, return None"
v0.7.0b1,Compute partial moments
v0.7.0b1,Compute coefficient by OLS on residuals
v0.7.0b1,ell_2 regularization
v0.7.0b1,Ridge regression estimate
v0.7.0b1,"Parameter returned is of shape (d_T, )"
v0.7.0b1,Return moments and gradients
v0.7.0b1,Compute partial moments
v0.7.0b1,Compute moments
v0.7.0b1,"Moments shape is (n, d_T-1)"
v0.7.0b1,Compute moment gradients
v0.7.0b1,Need to calculate this in an elegant way for when propensity is 0
v0.7.0b1,This will flatten T
v0.7.0b1,Check that T is numeric
v0.7.0b1,Test whether the input estimator is supported
v0.7.0b1,Test expansion of treatment
v0.7.0b1,"If expanded treatments are a vector, flatten const_marginal_effect_interval"
v0.7.0b1,Calculate confidence intervals for the parameter (marginal effect)
v0.7.0b1,"If T is a vector, preserve shape of the effect interval"
v0.7.0b1,Calculate confidence intervals for the effect
v0.7.0b1,Calculate the effects
v0.7.0b1,Calculate the standard deviations for the effects
v0.7.0b1,"Get a list of (parameter, covariance matrix) pairs"
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"if both X and W are None, just return a column of ones"
v0.7.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.7.0b1,We need to go back to the label representation of the one-hot so as to call
v0.7.0b1,the classifier.
v0.7.0b1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.7.0b1,This works both with our without the weighting trick as the treatments T are unit vector
v0.7.0b1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.7.0b1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.7.0b1,both Parametric and Non Parametric DML.
v0.7.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.7.0b1,for internal use by the library
v0.7.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.7.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.7.0b1,since we clone it and fit separate copies
v0.7.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.7.0b1,TODO: support sample_var
v0.7.0b1,"TODO: consider whether we need more care around stateful featurizers,"
v0.7.0b1,since we clone it and fit separate copies
v0.7.0b1,add statsmodels to parent's options
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,#######################################
v0.7.0b1,Core DML Tests
v0.7.0b1,#######################################
v0.7.0b1,How many samples
v0.7.0b1,How many control features
v0.7.0b1,How many treatment variables
v0.7.0b1,Coefficients of how controls affect treatments
v0.7.0b1,Coefficients of how controls affect outcome
v0.7.0b1,Treatment effects that we want to estimate
v0.7.0b1,Run dml estimation
v0.7.0b1,How many samples
v0.7.0b1,How many control features
v0.7.0b1,How many treatment variables
v0.7.0b1,Coefficients of how controls affect treatments
v0.7.0b1,Coefficients of how controls affect outcome
v0.7.0b1,Treatment effects that we want to estimate
v0.7.0b1,Run dml estimation
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.7.0b1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"this will have dimension (d,) + shape(X)"
v0.7.0b1,send the first dimension to the end
v0.7.0b1,columns are featurized independently; partial derivatives are only non-zero
v0.7.0b1,when taken with respect to the same column each time
v0.7.0b1,don't fit intercept; manually add column of ones to the data instead;
v0.7.0b1,this allows us to ignore the intercept when computing marginal effects
v0.7.0b1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.7.0b1,store number of columns of T so that we can pass scalars to effect
v0.7.0b1,TODO: support vector T and Y
v0.7.0b1,two stage approximation
v0.7.0b1,"first, get basis expansions of T, X, and Z"
v0.7.0b1,TODO: is it right that the effective number of intruments is the
v0.7.0b1,"product of ft_X and ft_Z, not just ft_Z?"
v0.7.0b1,"regress T expansion on X,Z expansions concatenated with W"
v0.7.0b1,"predict ft_T from interacted ft_X, ft_Z"
v0.7.0b1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.7.0b1,dT may be only 2-dimensional)
v0.7.0b1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.7.0b1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.7.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.7.0b1,We can write effect interval as a function of const_marginal_effect_interval for a single treatment
v0.7.0b1,We can write effect inference as a function of const_marginal_effect_inference for a single treatment
v0.7.0b1,d_t=1 here since we measure the effect across all Ts
v0.7.0b1,once the estimator has been fit
v0.7.0b1,We can write effect interval as a function of predict_interval of the final method for linear models
v0.7.0b1,We can write effect inference as a function of prediction and prediction standard error of
v0.7.0b1,the final method for linear models
v0.7.0b1,d_t=1 here since we measure the effect across all Ts
v0.7.0b1,"once the estimator has been fit, it's kosher to store d_t here"
v0.7.0b1,(which needs to have been expanded if there's a discrete treatment)
v0.7.0b1,"send treatment to the end, pull bounds to the front"
v0.7.0b1,d_t=1 here since we measure the effect across all Ts
v0.7.0b1,need to set the fit args before the estimator is fit
v0.7.0b1,1. Uncertainty of Mean Point Estimate
v0.7.0b1,2. Distribution of Point Estimate
v0.7.0b1,3. Total Variance of Point Estimate
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"A cut-down version of the DML first stage wrapper, since we don't need to support W or linear first stages"
v0.7.0b1,"if both X and Z are None, just return a column of ones"
v0.7.0b1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.7.0b1,We need to go back to the label representation of the one-hot so as to call
v0.7.0b1,the classifier.
v0.7.0b1,"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
v0.7.0b1,TODO: allow the final model to actually use X? Then we'd need to rename the class
v0.7.0b1,since we would actually be calculating a CATE rather than ATE.
v0.7.0b1,TODO: allow the final model to actually use X?
v0.7.0b1,TODO: allow the final model to actually use X?
v0.7.0b1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.7.0b1,"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
v0.7.0b1,TODO: would it be useful to extend to handle controls ala vanilla DML?
v0.7.0b1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.7.0b1,"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
v0.7.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.7.0b1,for internal use by the library
v0.7.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.7.0b1,Estimate final model of theta(X) by minimizing the square loss:
v0.7.0b1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.7.0b1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.7.0b1,at the expense of some small bias. For points with very small covariance we revert
v0.7.0b1,to the model-based preliminary estimate and do not add the correction term.
v0.7.0b1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
v0.7.0b1,"instruments, and outcomes"
v0.7.0b1,TODO: how do we incorporate the sample_weight and sample_var passed into this method
v0.7.0b1,as arguments?
v0.7.0b1,TODO: is there a good way to incorporate the other nuisance terms in the score?
v0.7.0b1,"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
v0.7.0b1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.7.0b1,for internal use by the library
v0.7.0b1,NOTE This is used by the inference methods and is more for internal use to the library
v0.7.0b1,"we need to undo the one-hot encoding for calling effect,"
v0.7.0b1,since it expects raw values
v0.7.0b1,"TODO: check that Y, T, Z do not have multiple columns"
v0.7.0b1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.7.0b1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.7.0b1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.7.0b1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.7.0b1,checks that X is 2D array.
v0.7.0b1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.7.0b1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.7.0b1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.7.0b1,"Replacing this method which is invalid for this class, so that we make the"
v0.7.0b1,dosctring empty and not appear in the docs.
v0.7.0b1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.7.0b1,TODO: support sample_var
v0.7.0b1,"Replacing this method which is invalid for this class, so that we make the"
v0.7.0b1,dosctring empty and not appear in the docs.
v0.7.0b1,add statsmodels to parent's options
v0.7.0b1,Replacing to remove docstring
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,TODO: make sure to use random seeds wherever necessary
v0.7.0b1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.7.0b1,"unfortunately with the Theano and Tensorflow backends,"
v0.7.0b1,the straightforward use of K.stop_gradient can cause an error
v0.7.0b1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.7.0b1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.7.0b1,so that those layers remain connected but with 0 gradient
v0.7.0b1,|| t - mu_i || ^2
v0.7.0b1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.7.0b1,Use logsumexp for numeric stability:
v0.7.0b1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.7.0b1,TODO: does the numeric stability actually make any difference?
v0.7.0b1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.7.0b1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.7.0b1,generate cumulative sum via matrix multiplication
v0.7.0b1,"Generate standard uniform values in shape (batch_size,1)"
v0.7.0b1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.7.0b1,we use uniform_like instead with an input of an appropriate shape)
v0.7.0b1,convert to floats and multiply to perform equivalent of logical AND
v0.7.0b1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.7.0b1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.7.0b1,we use normal_like instead with an input of an appropriate shape)
v0.7.0b1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.7.0b1,prevent gradient from passing through sampling
v0.7.0b1,three options: biased or upper-bound loss require a single number of samples;
v0.7.0b1,unbiased can take different numbers for the network and its gradient
v0.7.0b1,"sample: (() -> Layer, int) -> Layer"
v0.7.0b1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.7.0b1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.7.0b1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.7.0b1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.7.0b1,the dimensionality of the output of the network
v0.7.0b1,TODO: is there a more robust way to do this?
v0.7.0b1,TODO: do we need to give the user more control over other arguments to fit?
v0.7.0b1,"subtle point: we need to build a new model each time,"
v0.7.0b1,because each model encapsulates its randomness
v0.7.0b1,TODO: do we need to give the user more control over other arguments to fit?
v0.7.0b1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.7.0b1,not a general tensor (because of how backprop works in every framework)
v0.7.0b1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.7.0b1,but this seems annoying...)
v0.7.0b1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.7.0b1,TODO: any way to get this to work on batches of arbitrary size?
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,#######################################################
v0.7.0b1,Perfect Data DGPs for Testing Correctness of Code
v0.7.0b1,#######################################################
v0.7.0b1,Generate random control co-variates
v0.7.0b1,Create epsilon residual treatments that deterministically sum up to
v0.7.0b1,zero
v0.7.0b1,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.7.0b1,conditional on each co-variate vector is equal to zero
v0.7.0b1,We simply subtract the conditional mean from the epsilons
v0.7.0b1,Construct treatments as T = X*A + epsilon
v0.7.0b1,Construct outcomes as y = X*beta + T*effect
v0.7.0b1,Generate random control co-variates
v0.7.0b1,Create epsilon residual treatments that deterministically sum up to
v0.7.0b1,zero
v0.7.0b1,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.7.0b1,conditional on each co-variate vector is equal to zero
v0.7.0b1,We simply subtract the conditional mean from the epsilons
v0.7.0b1,Construct treatments as T = X*A + epsilon
v0.7.0b1,Construct outcomes as y = X*beta + T*effect
v0.7.0b1,Generate random control co-variates
v0.7.0b1,Construct treatments as T = X*A + epsilon
v0.7.0b1,Construct outcomes as y = X*beta + T*effect
v0.7.0b1,Generate random control co-variates
v0.7.0b1,Create epsilon residual treatments
v0.7.0b1,Construct treatments as T = X*A + epsilon
v0.7.0b1,Construct outcomes as y = X*beta + T*effect + eta
v0.7.0b1,Generate random control co-variates
v0.7.0b1,Use the same treatment vector for each row
v0.7.0b1,Construct outcomes as y = X*beta + T*effect
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"since inference objects can be stateful, we must copy it before fitting;"
v0.7.0b1,otherwise this sequence wouldn't work:
v0.7.0b1,"est1.fit(..., inference=inf)"
v0.7.0b1,"est2.fit(..., inference=inf)"
v0.7.0b1,est1.effect_interval(...)
v0.7.0b1,because inf now stores state from fitting est2
v0.7.0b1,call the wrapped fit method
v0.7.0b1,NOTE: we call inference fit *after* calling the main fit method
v0.7.0b1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.7.0b1,but tensordot can't be applied to this problem because we don't sum over m
v0.7.0b1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.7.0b1,of rows of T was not taken into account
v0.7.0b1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.7.0b1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.7.0b1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.7.0b1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.7.0b1,add statsmodels to parent's options
v0.7.0b1,add debiasedlasso to parent's options
v0.7.0b1,TODO Share some logic with non-discrete version
v0.7.0b1,add statsmodels to parent's options
v0.7.0b1,add statsmodels to parent's options
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Check if model is sparse enough for this model
v0.7.0b1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.7.0b1,TODO: any way to avoid creating a copy if the array was already dense?
v0.7.0b1,"the call is necessary if the input was something like a list, though"
v0.7.0b1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.7.0b1,so convert to pydata sparse first
v0.7.0b1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.7.0b1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.7.0b1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.7.0b1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.7.0b1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.7.0b1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.7.0b1,same number of input definitions as arrays
v0.7.0b1,input definitions have same number of dimensions as each array
v0.7.0b1,all result indices are unique
v0.7.0b1,all result indices must match at least one input index
v0.7.0b1,"map indices to all array, axis pairs for that index"
v0.7.0b1,each index has the same cardinality wherever it appears
v0.7.0b1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.7.0b1,Algo: while list contains more than one entry
v0.7.0b1,take two entries
v0.7.0b1,sort both lists by intersection of their indices
v0.7.0b1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.7.0b1,"take the union of indices and the product of values), stepping through each list linearly"
v0.7.0b1,TODO: might be faster to break into connected components first
v0.7.0b1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.7.0b1,"so compute their content separately, then take cartesian product"
v0.7.0b1,this would save a few pointless sorts by empty tuples
v0.7.0b1,TODO: Consider investigating other performance ideas for these cases
v0.7.0b1,where the dense method beat the sparse method (usually sparse is faster)
v0.7.0b1,"e,facd,c->cfed"
v0.7.0b1,sparse: 0.0335489
v0.7.0b1,dense:  0.011465999999999997
v0.7.0b1,"gbd,da,egb->da"
v0.7.0b1,sparse: 0.0791625
v0.7.0b1,dense:  0.007319099999999995
v0.7.0b1,"dcc,d,faedb,c->abe"
v0.7.0b1,sparse: 1.2868097
v0.7.0b1,dense:  0.44605229999999985
v0.7.0b1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.7.0b1,TODO: would using einsum's paths to optimize the order of merging help?
v0.7.0b1,Normalize weights
v0.7.0b1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.7.0b1,This class is mainly derived from statsmodels.iolib.summary.Summary
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.7.0b1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.7.0b1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.7.0b1,clean way of achieving this
v0.7.0b1,make sure we don't accidentally escape anything in the substitution
v0.7.0b1,Fetch appropriate color for node
v0.7.0b1,"red for negative, green for positive"
v0.7.0b1,in multi-target use first target
v0.7.0b1,Write node mean CATE
v0.7.0b1,Write node std of CATE
v0.7.0b1,Write confidence interval information if at leaf node
v0.7.0b1,Fetch appropriate color for node
v0.7.0b1,"red for negative, green for positive"
v0.7.0b1,Write node mean CATE
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.7.0b1,use a binary array to get stratified split in case of discrete treatment
v0.7.0b1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.7.0b1,drop first column since all columns sum to one
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Estimators
v0.7.0b1,Causal tree parameters
v0.7.0b1,Tree structure
v0.7.0b1,No need for a random split since the data is already
v0.7.0b1,a random subsample from the original input
v0.7.0b1,node list stores the nodes that are yet to be splitted
v0.7.0b1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.7.0b1,Create local sample set
v0.7.0b1,Compute nuisance estimates for the current node
v0.7.0b1,Nuisance estimate cannot be calculated
v0.7.0b1,Estimate parameter for current node
v0.7.0b1,Node estimate cannot be calculated
v0.7.0b1,Calculate moments and gradient of moments for current data
v0.7.0b1,Calculate inverse gradient
v0.7.0b1,The gradient matrix is not invertible.
v0.7.0b1,No good split can be found
v0.7.0b1,Calculate point-wise pseudo-outcomes rho
v0.7.0b1,a split is determined by a feature and a sample pair
v0.7.0b1,the number of possible splits is at most (number of features) * (number of node samples)
v0.7.0b1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.7.0b1,parse row and column of random pair
v0.7.0b1,the sample of the pair is the integer division of the random number with n_feats
v0.7.0b1,calculate the binary indicator of whether sample i is on the left or the right
v0.7.0b1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.7.0b1,calculate the number of samples on the left child for each proposed split
v0.7.0b1,calculate the analogous binary indicator for the samples in the estimation set
v0.7.0b1,calculate the number of estimation samples on the left child of each proposed split
v0.7.0b1,find the upper and lower bound on the size of the left split for the split
v0.7.0b1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.7.0b1,on each side.
v0.7.0b1,similarly for the estimation sample set
v0.7.0b1,if there is no valid split then don't create any children
v0.7.0b1,filter only the valid splits
v0.7.0b1,calculate the average influence vector of the samples in the left child
v0.7.0b1,calculate the average influence vector of the samples in the right child
v0.7.0b1,take the square of each of the entries of the influence vectors and normalize
v0.7.0b1,by size of each child
v0.7.0b1,calculate the vector score of each candidate split as the average of left and right
v0.7.0b1,influence vectors
v0.7.0b1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.7.0b1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.7.0b1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.7.0b1,calculate the scalar score of each split by aggregating across the vector of scores
v0.7.0b1,Find split that minimizes criterion
v0.7.0b1,Create child nodes with corresponding subsamples
v0.7.0b1,add the created children to the list of not yet split nodes
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,TODO: Add a __dir__ implementation?
v0.7.0b1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.7.0b1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.7.0b1,then we should be computing a confidence interval for the wrapped calls
v0.7.0b1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.7.0b1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.7.0b1,"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
v0.7.0b1,AzureML
v0.7.0b1,helper imports
v0.7.0b1,write the details of the workspace to a configuration file to the notebook library
v0.7.0b1,if y is a multioutput model
v0.7.0b1,Make sure second dimension has 1 or more item
v0.7.0b1,switch _inner Model to a MultiOutputRegressor
v0.7.0b1,flatten array as automl only takes vectors for y
v0.7.0b1,Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
v0.7.0b1,Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
v0.7.0b1,as an sklearn estimator
v0.7.0b1,fit implementation for a single output model.
v0.7.0b1,Create experiment for specified workspace
v0.7.0b1,Configure automl_config with training set information.
v0.7.0b1,"Wait for remote run to complete, the set the model"
v0.7.0b1,"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
v0.7.0b1,create model and pass model into final.
v0.7.0b1,"If item is an automl config, get its corresponding"
v0.7.0b1,AutomatedML Model and add it to new_Args
v0.7.0b1,"If item is an automl config, get its corresponding"
v0.7.0b1,AutomatedML Model and set it for this key in
v0.7.0b1,kwargs
v0.7.0b1,takes in either automated_ml config and instantiates
v0.7.0b1,an AutomatedMLModel
v0.7.0b1,The prefix can only be 18 characters long
v0.7.0b1,"because prefixes come from kwarg_names, we must ensure they are"
v0.7.0b1,short enough.
v0.7.0b1,Get workspace from config file.
v0.7.0b1,Take the intersect of the white for sample
v0.7.0b1,weights and linear models
v0.7.0b1,"show output is not stored in the config in AutomatedML, so we need to make it a field."
v0.7.0b1,Remove children with nonwhite mothers from the treatment group
v0.7.0b1,Remove children with nonwhite mothers from the treatment group
v0.7.0b1,Select columns
v0.7.0b1,Scale the numeric variables
v0.7.0b1,"Change the binary variable 'first' takes values in {1,2}"
v0.7.0b1,Append a column of ones as intercept
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.7.0b1,creating notebooks that are annoying for our users to actually run themselves
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.7.0b1,"prior to calling interpret, can't plot, render, etc."
v0.7.0b1,can interpret without uncertainty
v0.7.0b1,can't interpret with uncertainty if inference wasn't used during fit
v0.7.0b1,can interpret with uncertainty if we refit
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,simple DGP only for illustration
v0.7.0b1,Define the treatment model neural network architecture
v0.7.0b1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.7.0b1,"so the input shape is (d_z + d_x,)"
v0.7.0b1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.7.0b1,add extra layers on top for the mixture density network
v0.7.0b1,Define the response model neural network architecture
v0.7.0b1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.7.0b1,"so the input shape is (d_t + d_x,)"
v0.7.0b1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.7.0b1,"NOTE: For the response model, it is important to define the model *outside*"
v0.7.0b1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.7.0b1,so that the same weights will be reused in each instantiation
v0.7.0b1,number of samples to use in second estimate of the response
v0.7.0b1,(to make loss estimate unbiased)
v0.7.0b1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.7.0b1,do something with predictions...
v0.7.0b1,also test vector t and y
v0.7.0b1,simple DGP only for illustration
v0.7.0b1,Define the treatment model neural network architecture
v0.7.0b1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.7.0b1,"so the input shape is (d_z + d_x,)"
v0.7.0b1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.7.0b1,add extra layers on top for the mixture density network
v0.7.0b1,Define the response model neural network architecture
v0.7.0b1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.7.0b1,"so the input shape is (d_t + d_x,)"
v0.7.0b1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.7.0b1,"NOTE: For the response model, it is important to define the model *outside*"
v0.7.0b1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.7.0b1,so that the same weights will be reused in each instantiation
v0.7.0b1,number of samples to use in second estimate of the response
v0.7.0b1,(to make loss estimate unbiased)
v0.7.0b1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.7.0b1,do something with predictions...
v0.7.0b1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.7.0b1,test = True ensures we draw test set images
v0.7.0b1,test = True ensures we draw test set images
v0.7.0b1,re-draw to get new independent treatment and implied response
v0.7.0b1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.7.0b1,above is necesary so that reduced form doesn't win
v0.7.0b1,covariates: time and emotion
v0.7.0b1,random instrument
v0.7.0b1,z -> price
v0.7.0b1,true observable demand function
v0.7.0b1,errors
v0.7.0b1,response
v0.7.0b1,test = True ensures we draw test set images
v0.7.0b1,test = True ensures we draw test set images
v0.7.0b1,re-draw to get new independent treatment and implied response
v0.7.0b1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.7.0b1,above is necesary so that reduced form doesn't win
v0.7.0b1,covariates: time and emotion
v0.7.0b1,random instrument
v0.7.0b1,z -> price
v0.7.0b1,true observable demand function
v0.7.0b1,errors
v0.7.0b1,response
v0.7.0b1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.7.0b1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.7.0b1,For some reason this doesn't work at all when run against the CNTK backend...
v0.7.0b1,"model.compile('nadam', loss=lambda _,l:l)"
v0.7.0b1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.7.0b1,generate a valiation set
v0.7.0b1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.7.0b1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.7.0b1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.7.0b1,so we need to transpose the result
v0.7.0b1,1-d output
v0.7.0b1,2-d output
v0.7.0b1,Single dimensional output y
v0.7.0b1,Multi-dimensional output y
v0.7.0b1,1-d y
v0.7.0b1,multi-d y
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,test that we can fit with the same arguments as the base estimator
v0.7.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.7.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0b1,with the same shape for the lower and upper bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,test that we can do the same thing once we provide percentile bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.7.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0b1,with the same shape for the lower and upper bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,test that we can do the same thing once we provide percentile bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,test that we can fit with the same arguments as the base estimator
v0.7.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.7.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0b1,with the same shape for the lower and upper bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,test that we can do the same thing once we provide percentile bounds
v0.7.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.7.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0b1,with the same shape for the lower and upper bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,test that we can do the same thing once we provide percentile bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,test that we can fit with the same arguments as the base estimator
v0.7.0b1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.7.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0b1,with the same shape for the lower and upper bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,test that we can do the same thing once we provide percentile bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.7.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0b1,with the same shape for the lower and upper bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,test that we can do the same thing once we provide percentile bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0b1,with the same shape for the lower and upper bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,test that the estimated effect is usually within the bounds
v0.7.0b1,test that we can do the same thing once we provide alpha explicitly
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,test that the estimated effect is usually within the bounds
v0.7.0b1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.7.0b1,with the same shape for the lower and upper bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,TODO: test that the estimated effect is usually within the bounds
v0.7.0b1,and that the true effect is also usually within the bounds
v0.7.0b1,test that we can do the same thing once we provide percentile bounds
v0.7.0b1,test that the lower and upper bounds differ
v0.7.0b1,TODO: test that the estimated effect is usually within the bounds
v0.7.0b1,and that the true effect is also usually within the bounds
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Test non keyword based calls to fit
v0.7.0b1,Test custom splitter
v0.7.0b1,Test incomplete set of test folds
v0.7.0b1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,make sure cross product varies more slowly with first array
v0.7.0b1,and that vectors are okay as inputs
v0.7.0b1,number of inputs in specification must match number of inputs
v0.7.0b1,must have an output
v0.7.0b1,output indices must be unique
v0.7.0b1,output indices must be present in an input
v0.7.0b1,number of indices must match number of dimensions for each input
v0.7.0b1,repeated indices must always have consistent sizes
v0.7.0b1,transpose
v0.7.0b1,tensordot
v0.7.0b1,trace
v0.7.0b1,TODO: set up proper flag for this
v0.7.0b1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.7.0b1,"of all of the distinct indices that appear in any input,"
v0.7.0b1,pick a random subset of them (of size at most 5) to appear in the output
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Preprocess data
v0.7.0b1,Convert 'week' to a date
v0.7.0b1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.7.0b1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.7.0b1,Take log of price
v0.7.0b1,Make brand numeric
v0.7.0b1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.7.0b1,which have all zero coeffs
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.7.0b1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.7.0b1,TODO: test something rather than just print...
v0.7.0b1,"Note: no noise, just testing that we can exactly recover when we ought to be able to"
v0.7.0b1,pick some arbitrary X
v0.7.0b1,pick some arbitrary T
v0.7.0b1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,ensure that we've got at least two of every row
v0.7.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.7.0b1,need to make sure we get all *joint* combinations
v0.7.0b1,IntentToTreat only supports binary treatments/instruments
v0.7.0b1,IntentToTreat only supports binary treatments/instruments
v0.7.0b1,TODO: add stratification to bootstrap so that we can use it
v0.7.0b1,even with discrete treatments
v0.7.0b1,IntentToTreat requires X
v0.7.0b1,these support only W but not X
v0.7.0b1,"these support only binary, not general discrete T and Z"
v0.7.0b1,make sure we can call the marginal_effect and effect methods
v0.7.0b1,TODO: add tests for extra properties like coef_ where they exist
v0.7.0b1,TODO: add tests for extra properties like coef_ where they exist
v0.7.0b1,"make sure we can call effect with implied scalar treatments,"
v0.7.0b1,"no matter the dimensions of T, and also that we warn when there"
v0.7.0b1,are multiple treatments
v0.7.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.7.0b1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete
v0.7.0b1,"however, with custom splits the checking happens in the first stage wrapper"
v0.7.0b1,where we don't have all of the required information to do this;
v0.7.0b1,we'd probably need to add it to _crossfit instead
v0.7.0b1,TODO: make IV related
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.7.0b1,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.7.0b1,The __warningregistry__'s need to be in a pristine state for tests
v0.7.0b1,to work properly.
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Set random seed
v0.7.0b1,Generate data
v0.7.0b1,DGP constants
v0.7.0b1,Test data
v0.7.0b1,Constant treatment effect
v0.7.0b1,Constant treatment with multi output Y
v0.7.0b1,Heterogeneous treatment
v0.7.0b1,Heterogeneous treatment with multi output Y
v0.7.0b1,TLearner test
v0.7.0b1,Instantiate TLearner
v0.7.0b1,Test inputs
v0.7.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.7.0b1,Instantiate SLearner
v0.7.0b1,Test inputs
v0.7.0b1,Test constant treatment effect
v0.7.0b1,Test constant treatment effect with multi output Y
v0.7.0b1,Test heterogeneous treatment effect
v0.7.0b1,Need interactions between T and features
v0.7.0b1,Test heterogeneous treatment effect with multi output Y
v0.7.0b1,Instantiate XLearner
v0.7.0b1,Test inputs
v0.7.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.7.0b1,Instantiate DomainAdaptationLearner
v0.7.0b1,Test inputs
v0.7.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.7.0b1,Get the true treatment effect
v0.7.0b1,Get the true treatment effect
v0.7.0b1,Fit learner and get the effect and marginal effect
v0.7.0b1,Compute treatment effect residuals (absolute)
v0.7.0b1,Check that at least 90% of predictions are within tolerance interval
v0.7.0b1,Check whether the output shape is right
v0.7.0b1,Check that one can pass in regular lists
v0.7.0b1,Check that it fails correctly if lists of different shape are passed in
v0.7.0b1,"Check that it works when T, Y have shape (n, 1)"
v0.7.0b1,Generate covariates
v0.7.0b1,Generate treatment
v0.7.0b1,Calculate outcome
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,DGP constants
v0.7.0b1,Generate data
v0.7.0b1,Test data
v0.7.0b1,Remove warnings that might be raised by the models passed into the ORF
v0.7.0b1,Generate data with continuous treatments
v0.7.0b1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.7.0b1,does not work well with parallelism.
v0.7.0b1,Test inputs for continuous treatments
v0.7.0b1,--> Check that one can pass in regular lists
v0.7.0b1,--> Check that it fails correctly if lists of different shape are passed in
v0.7.0b1,Check that outputs have the correct shape
v0.7.0b1,Test continuous treatments with controls
v0.7.0b1,Test continuous treatments without controls
v0.7.0b1,Generate data with binary treatments
v0.7.0b1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.7.0b1,does not work well with parallelism.
v0.7.0b1,Test inputs for binary treatments
v0.7.0b1,--> Check that one can pass in regular lists
v0.7.0b1,--> Check that it fails correctly if lists of different shape are passed in
v0.7.0b1,"--> Check that it works when T, Y have shape (n, 1)"
v0.7.0b1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.7.0b1,--> Check that it fails correctly when the treatments are not numeric
v0.7.0b1,Check that outputs have the correct shape
v0.7.0b1,Test binary treatments with controls
v0.7.0b1,Test binary treatments without controls
v0.7.0b1,Only applicable to continuous treatments
v0.7.0b1,Generate data for 2 treatments
v0.7.0b1,Test multiple treatments with controls
v0.7.0b1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.7.0b1,The rest for controls. Just as an example.
v0.7.0b1,Generating A/B test data
v0.7.0b1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.7.0b1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.7.0b1,Generate data with continuous treatments
v0.7.0b1,Instantiate model with most of the default parameters
v0.7.0b1,Compute the treatment effect on test points
v0.7.0b1,Compute treatment effect residuals
v0.7.0b1,Multiple treatments
v0.7.0b1,Allow at most 10% test points to be outside of the tolerance interval
v0.7.0b1,Compute the treatment effect on test points
v0.7.0b1,Compute treatment effect residuals
v0.7.0b1,Multiple treatments
v0.7.0b1,Allow at most 20% test points to be outside of the confidence interval
v0.7.0b1,Check that the intervals are not too wide
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.7.0b1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.7.0b1,ensure that we've got at least two of every element
v0.7.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.7.0b1,TODO: add stratification to bootstrap so that we can use it
v0.7.0b1,even with discrete treatments
v0.7.0b1,make sure we can call the marginal_effect and effect methods
v0.7.0b1,test const marginal inference
v0.7.0b1,test effect inference
v0.7.0b1,test marginal effect inference
v0.7.0b1,test coef__inference and intercept__inference
v0.7.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.7.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.7.0b1,"make sure we can call effect with implied scalar treatments,"
v0.7.0b1,"no matter the dimensions of T, and also that we warn when there"
v0.7.0b1,are multiple treatments
v0.7.0b1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.7.0b1,ensure that we've got at least two of every element
v0.7.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.7.0b1,"TODO Add bootstrap inference, once discrete treatment issue is fixed"
v0.7.0b1,make sure we can call the marginal_effect and effect methods
v0.7.0b1,test const marginal inference
v0.7.0b1,test effect inference
v0.7.0b1,test marginal effect inference
v0.7.0b1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.7.0b1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.7.0b1,We concatenate the two copies data
v0.7.0b1,create a simple artificial setup where effect of moving from treatment
v0.7.0b1,"1 -> 2 is 2,"
v0.7.0b1,"1 -> 3 is 1, and"
v0.7.0b1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.7.0b1,"Using an uneven number of examples from different classes,"
v0.7.0b1,"and having the treatments in non-lexicographic order,"
v0.7.0b1,Should rule out some basic issues.
v0.7.0b1,test that we can fit with a KFold instance
v0.7.0b1,test that we can fit with a train/test iterable
v0.7.0b1,(incorrectly) use a final model with an intercept
v0.7.0b1,"Because final model is fixed, actual values of T and Y don't matter"
v0.7.0b1,Ensure reproducibility
v0.7.0b1,Sparse DGP
v0.7.0b1,Treatment effect coef
v0.7.0b1,Other coefs
v0.7.0b1,Features and controls
v0.7.0b1,Test sparse estimator
v0.7.0b1,"--> test coef_, intercept_"
v0.7.0b1,--> test treatment effects
v0.7.0b1,Restrict x_test to vectors of norm < 1
v0.7.0b1,--> check inference
v0.7.0b1,Check that a majority of true effects lie in the 5-95% CI
v0.7.0b1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.7.0b1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.7.0b1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.7.0b1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.7.0b1,sparse test case: heterogeneous effect by product
v0.7.0b1,need at least as many rows in e_y as there are distinct columns
v0.7.0b1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Set random seed
v0.7.0b1,Generate data
v0.7.0b1,DGP constants
v0.7.0b1,Test data
v0.7.0b1,Constant treatment effect and propensity
v0.7.0b1,Heterogeneous treatment and propensity
v0.7.0b1,ensure that we've got at least two of every element
v0.7.0b1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.7.0b1,TODO: add stratification to bootstrap so that we can use it even with discrete treatments
v0.7.0b1,make sure we can call the marginal_effect and effect methods
v0.7.0b1,test const marginal inference
v0.7.0b1,test effect inference
v0.7.0b1,test marginal effect inference
v0.7.0b1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.7.0b1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.7.0b1,create a simple artificial setup where effect of moving from treatment
v0.7.0b1,"1 -> 2 is 2,"
v0.7.0b1,"1 -> 3 is 1, and"
v0.7.0b1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.7.0b1,"Using an uneven number of examples from different classes,"
v0.7.0b1,"and having the treatments in non-lexicographic order,"
v0.7.0b1,Should rule out some basic issues.
v0.7.0b1,test that we can fit with a KFold instance
v0.7.0b1,test that we can fit with a train/test iterable
v0.7.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.7.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.7.0b1,"for at least some of the examples, the CI should have nonzero width"
v0.7.0b1,test coef__inference function works
v0.7.0b1,test intercept__inference function works
v0.7.0b1,test summary function works
v0.7.0b1,Test inputs
v0.7.0b1,self._test_inputs(DR_learner)
v0.7.0b1,Test constant treatment effect
v0.7.0b1,Test heterogeneous treatment effect
v0.7.0b1,Test heterogenous treatment effect for W =/= None
v0.7.0b1,Sparse DGP
v0.7.0b1,Treatment effect coef
v0.7.0b1,Other coefs
v0.7.0b1,Features and controls
v0.7.0b1,Test sparse estimator
v0.7.0b1,"--> test coef_, intercept_"
v0.7.0b1,--> test treatment effects
v0.7.0b1,Restrict x_test to vectors of norm < 1
v0.7.0b1,--> check inference
v0.7.0b1,Check that a majority of true effects lie in the 5-95% CI
v0.7.0b1,Fit learner and get the effect
v0.7.0b1,Get the true treatment effect
v0.7.0b1,Compute treatment effect residuals (absolute)
v0.7.0b1,Check that at least 90% of predictions are within tolerance interval
v0.7.0b1,Only for heterogeneous TE
v0.7.0b1,Fit learner on X and W and get the effect
v0.7.0b1,Get the true treatment effect
v0.7.0b1,Compute treatment effect residuals (absolute)
v0.7.0b1,Check that at least 90% of predictions are within tolerance interval
v0.7.0b1,Check that one can pass in regular lists
v0.7.0b1,Check that it fails correctly if lists of different shape are passed in
v0.7.0b1,Check that it fails when T contains values other than 0 and 1
v0.7.0b1,"Check that it works when T, Y have shape (n, 1)"
v0.7.0b1,Generate covariates
v0.7.0b1,Generate treatment
v0.7.0b1,Calculate outcome
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,DGP constants
v0.7.0b1,DGP coefficients
v0.7.0b1,Generated outcomes
v0.7.0b1,################
v0.7.0b1,WeightedLasso #
v0.7.0b1,################
v0.7.0b1,Define weights
v0.7.0b1,Define extended datasets
v0.7.0b1,Range of alphas
v0.7.0b1,Compare with Lasso
v0.7.0b1,--> No intercept
v0.7.0b1,--> With intercept
v0.7.0b1,When DGP has no intercept
v0.7.0b1,When DGP has intercept
v0.7.0b1,--> Coerce coefficients to be positive
v0.7.0b1,--> Toggle max_iter & tol
v0.7.0b1,Define weights
v0.7.0b1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.7.0b1,Mixed DGP scenario.
v0.7.0b1,Define extended datasets
v0.7.0b1,Define weights
v0.7.0b1,Define multioutput
v0.7.0b1,##################
v0.7.0b1,WeightedLassoCV #
v0.7.0b1,##################
v0.7.0b1,Define alphas to test
v0.7.0b1,Compare with LassoCV
v0.7.0b1,--> No intercept
v0.7.0b1,--> With intercept
v0.7.0b1,--> Force parameters to be positive
v0.7.0b1,Choose a smaller n to speed-up process
v0.7.0b1,Compare fold weights
v0.7.0b1,Define weights
v0.7.0b1,Define extended datasets
v0.7.0b1,Define splitters
v0.7.0b1,WeightedKFold splitter
v0.7.0b1,Map weighted splitter to an extended splitter
v0.7.0b1,Define alphas to test
v0.7.0b1,Compare with LassoCV
v0.7.0b1,--> No intercept
v0.7.0b1,--> With intercept
v0.7.0b1,--> Force parameters to be positive
v0.7.0b1,###########################
v0.7.0b1,MultiTaskWeightedLassoCV #
v0.7.0b1,###########################
v0.7.0b1,Define alphas to test
v0.7.0b1,Define splitter
v0.7.0b1,Compare with MultiTaskLassoCV
v0.7.0b1,--> No intercept
v0.7.0b1,--> With intercept
v0.7.0b1,Define weights
v0.7.0b1,Define extended datasets
v0.7.0b1,Define splitters
v0.7.0b1,WeightedKFold splitter
v0.7.0b1,Map weighted splitter to an extended splitter
v0.7.0b1,Define alphas to test
v0.7.0b1,Compare with LassoCV
v0.7.0b1,--> No intercept
v0.7.0b1,--> With intercept
v0.7.0b1,################
v0.7.0b1,DebiasedLasso #
v0.7.0b1,################
v0.7.0b1,Test DebiasedLasso without weights
v0.7.0b1,--> Check debiased coeffcients without intercept
v0.7.0b1,--> Check debiased coeffcients with intercept
v0.7.0b1,--> Check 5-95 CI coverage for unit vectors
v0.7.0b1,Test DebiasedLasso with weights for one DGP
v0.7.0b1,Define weights
v0.7.0b1,Define extended datasets
v0.7.0b1,--> Check debiased coefficients
v0.7.0b1,Define weights
v0.7.0b1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.7.0b1,--> Check debiased coeffcients
v0.7.0b1,Test that attributes propagate correctly
v0.7.0b1,Test MultiOutputDebiasedLasso without weights
v0.7.0b1,--> Check debiased coeffcients without intercept
v0.7.0b1,--> Check debiased coeffcients with intercept
v0.7.0b1,--> Check CI coverage
v0.7.0b1,Test MultiOutputDebiasedLasso with weights
v0.7.0b1,Define weights
v0.7.0b1,Define extended datasets
v0.7.0b1,--> Check debiased coefficients
v0.7.0b1,Unit vectors
v0.7.0b1,Unit vectors
v0.7.0b1,Check coeffcients and intercept are the same within tolerance
v0.7.0b1,Check results are similar with tolerance 1e-6
v0.7.0b1,Check if multitask
v0.7.0b1,Check that same alpha is chosen
v0.7.0b1,Check that the coefficients are similar
v0.7.0b1,selective ridge has a simple implementation that we can test against
v0.7.0b1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.7.0b1,"it should be the case that when we set fit_intercept to true,"
v0.7.0b1,it doesn't matter whether the penalized model also fits an intercept or not
v0.7.0b1,create an extra copy of rows with weight 2
v0.7.0b1,"instead of a slice, explicitly return an array of indices"
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Linear models are required for parametric dml
v0.7.0b1,sample weighting models are required for nonparametric dml
v0.7.0b1,Test values
v0.7.0b1,TLearner test
v0.7.0b1,Instantiate TLearner
v0.7.0b1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.7.0b1,Test constant treatment effect with multi output Y
v0.7.0b1,Test heterogeneous treatment effect
v0.7.0b1,Need interactions between T and features
v0.7.0b1,Test heterogeneous treatment effect with multi output Y
v0.7.0b1,Instantiate DomainAdaptationLearner
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,"Found a good split, return."
v0.7.0b1,Record all splits in case the stratification by weight yeilds a worse partition
v0.7.0b1,Reseed random generator and try again
v0.7.0b1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.7.0b1,"Found a good split, return."
v0.7.0b1,Did not find a good split
v0.7.0b1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.7.0b1,Return most weight-balanced partition
v0.7.0b1,Weight stratification algorithm
v0.7.0b1,Sort weights for weight strata search
v0.7.0b1,There are some leftover indices that have yet to be assigned
v0.7.0b1,Append stratum splits to overall splits
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Our classes that derive from sklearn ones sometimes include
v0.7.0b1,inherited docstrings that have embedded doctests; we need the following imports
v0.7.0b1,so that they don't break.
v0.7.0b1,"Convert X, y into numpy arrays"
v0.7.0b1,Define fit parameters
v0.7.0b1,Some algorithms don't have a check_input option
v0.7.0b1,Check weights array
v0.7.0b1,Check that weights are size-compatible
v0.7.0b1,Normalize inputs
v0.7.0b1,Weight inputs
v0.7.0b1,Fit base class without intercept
v0.7.0b1,Fit Lasso
v0.7.0b1,Reset intercept
v0.7.0b1,The intercept is not calculated properly due the sqrt(weights) factor
v0.7.0b1,so it must be recomputed
v0.7.0b1,Fit lasso without weights
v0.7.0b1,Make weighted splitter
v0.7.0b1,Fit weighted model
v0.7.0b1,Make weighted splitter
v0.7.0b1,Fit weighted model
v0.7.0b1,Select optimal penalty
v0.7.0b1,Warn about consistency
v0.7.0b1,"Convert X, y into numpy arrays"
v0.7.0b1,Fit weighted lasso with user input
v0.7.0b1,"Center X, y"
v0.7.0b1,Calculate quantities that will be used later on. Account for centered data
v0.7.0b1,Calculate coefficient and error variance
v0.7.0b1,Add coefficient correction
v0.7.0b1,Set coefficients and intercept standard errors
v0.7.0b1,Set intercept
v0.7.0b1,Return alpha to 'auto' state
v0.7.0b1,"Note that in the case of no intercept, X_offset is 0"
v0.7.0b1,Calculate the variance of the predictions
v0.7.0b1,"Note that in the case of no intercept, X_offset is 0"
v0.7.0b1,Calculate the variance of the predictions
v0.7.0b1,Calculate prediction confidence intervals
v0.7.0b1,Assumes flattened y
v0.7.0b1,Compute weighted residuals
v0.7.0b1,To be done once per target. Assumes y can be flattened.
v0.7.0b1,Assumes that X has already been offset
v0.7.0b1,Special case: n_features=1
v0.7.0b1,Compute Lasso coefficients for the columns of the design matrix
v0.7.0b1,Call weighted lasso on reduced design matrix
v0.7.0b1,Inherit some parameters from the parent
v0.7.0b1,Weighted tau
v0.7.0b1,Compute C_hat
v0.7.0b1,Compute theta_hat
v0.7.0b1,Allow for single output as well
v0.7.0b1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.7.0b1,Set coef_ attribute
v0.7.0b1,Set intercept_ attribute
v0.7.0b1,Set selected_alpha_ attribute
v0.7.0b1,Set coef_stderr_
v0.7.0b1,intercept_stderr_
v0.7.0b1,set intercept_ attribute
v0.7.0b1,set coef_ attribute
v0.7.0b1,set alpha_ attribute
v0.7.0b1,set alphas_ attribute
v0.7.0b1,set n_iter_ attribute
v0.7.0b1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.7.0b1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.7.0b1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.7.0b1,now regress X1 on y - X2 * beta2 to learn beta1
v0.7.0b1,set coef_ and intercept_ attributes
v0.7.0b1,Note that the penalized model should *not* have an intercept
v0.7.0b1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.7.0b1,Licensed under the MIT License.
v0.7.0b1,Construct the subsample of data
v0.7.0b1,Split into estimation and splitting sample set
v0.7.0b1,Fit the tree on the splitting sample
v0.7.0b1,Set the estimation values based on the estimation split
v0.7.0b1,Apply the trained tree on the estimation sample to get the path for every estimation sample
v0.7.0b1,Calculate the total weight of estimation samples on each tree node:
v0.7.0b1,\sum_i sample_weight[i] * 1{i \\in node}
v0.7.0b1,Calculate the total number of estimation samples on each tree node:
v0.7.0b1,|node| = \sum_{i} 1{i \\in node}
v0.7.0b1,Calculate the weighted sum of responses on the estimation sample on each node:
v0.7.0b1,\sum_{i} sample_weight[i] 1{i \\in node} Y_i
v0.7.0b1,Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
v0.7.0b1,and for each un-pruned tree set the value and the weight appropriately.
v0.7.0b1,If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
v0.7.0b1,"sample, then prune the whole sub-tree"
v0.7.0b1,Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
v0.7.0b1,Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
v0.7.0b1,Set the count to the estimation split count
v0.7.0b1,Validate or convert input data
v0.7.0b1,Pre-sort indices to avoid that each individual tree of the
v0.7.0b1,ensemble sorts the indices.
v0.7.0b1,Remap output
v0.7.0b1,reshape is necessary to preserve the data contiguity against vs
v0.7.0b1,"[:, np.newaxis] that does not."
v0.7.0b1,Check parameters
v0.7.0b1,"Free allocated memory, if any"
v0.7.0b1,We draw from the random state to get the random state we
v0.7.0b1,would have got if we hadn't used a warm_start.
v0.7.0b1,Parallel loop: we prefer the threading backend as the Cython code
v0.7.0b1,for fitting the trees is internally releasing the Python GIL
v0.7.0b1,making threading more efficient than multiprocessing in
v0.7.0b1,"that case. However, for joblib 0.12+ we respect any"
v0.7.0b1,"parallel_backend contexts set at a higher level,"
v0.7.0b1,since correctness does not rely on using threads.
v0.7.0b1,TODO. This slicing should ultimately be done inside the parallel function
v0.7.0b1,so that we don't need to create a matrix of size roughly n_samples * n_estimators
v0.7.0b1,Collect newly grown trees
v0.7.0b1,Helper class that accumulates an arbitrary function in parallel on the accumulator acc
v0.7.0b1,and calls the function fn on each tree e and returns the mean output. The function fn
v0.7.0b1,"should take as input a tree e, and return another function g_e, which takes as input X, check_input"
v0.7.0b1,"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
v0.7.0b1,index start to index end will be used. The returned result is essentially:
v0.7.0b1,(mean over e in slice)(g_e(X)).
v0.7.0b1,Check data
v0.7.0b1,Assign chunk of trees to jobs
v0.7.0b1,Check data
v0.7.0b1,Check data
v0.7.0b1,avoid storing the output of every estimator by summing them here
v0.7.0b1,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
v0.7.0b1,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
v0.7.0b1,"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
v0.7.0b1,where \theta(X) is the point estimate using the whole forest
v0.7.0b1,Calculate the variance of the latter as E[Q(S)^2]
v0.6.1,configuration is all pulled from setup.cfg
v0.6.1,-*- coding: utf-8 -*-
v0.6.1,
v0.6.1,Configuration file for the Sphinx documentation builder.
v0.6.1,
v0.6.1,This file does only contain a selection of the most common options. For a
v0.6.1,full list see the documentation:
v0.6.1,http://www.sphinx-doc.org/en/master/config
v0.6.1,-- Path setup --------------------------------------------------------------
v0.6.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.6.1,add these directories to sys.path here. If the directory is relative to the
v0.6.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.6.1,
v0.6.1,-- Project information -----------------------------------------------------
v0.6.1,-- General configuration ---------------------------------------------------
v0.6.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.6.1,
v0.6.1,needs_sphinx = '1.0'
v0.6.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.6.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.6.1,ones.
v0.6.1,"Add any paths that contain templates here, relative to this directory."
v0.6.1,The suffix(es) of source filenames.
v0.6.1,You can specify multiple suffix as a list of string:
v0.6.1,
v0.6.1,"source_suffix = ['.rst', '.md']"
v0.6.1,The master toctree document.
v0.6.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.6.1,for a list of supported languages.
v0.6.1,
v0.6.1,This is also used if you do content translation via gettext catalogs.
v0.6.1,"Usually you set ""language"" from the command line for these cases."
v0.6.1,"List of patterns, relative to source directory, that match files and"
v0.6.1,directories to ignore when looking for source files.
v0.6.1,This pattern also affects html_static_path and html_extra_path.
v0.6.1,The name of the Pygments (syntax highlighting) style to use.
v0.6.1,-- Options for HTML output -------------------------------------------------
v0.6.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.6.1,a list of builtin themes.
v0.6.1,
v0.6.1,Theme options are theme-specific and customize the look and feel of a theme
v0.6.1,"further.  For a list of options available for each theme, see the"
v0.6.1,documentation.
v0.6.1,
v0.6.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.6.1,"relative to this directory. They are copied after the builtin static files,"
v0.6.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.6.1,html_static_path = ['_static']
v0.6.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.6.1,to template names.
v0.6.1,
v0.6.1,The default sidebars (for documents that don't match any pattern) are
v0.6.1,defined by theme itself.  Builtin themes are using these templates by
v0.6.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.6.1,'searchbox.html']``.
v0.6.1,
v0.6.1,html_sidebars = {}
v0.6.1,-- Options for HTMLHelp output ---------------------------------------------
v0.6.1,Output file base name for HTML help builder.
v0.6.1,-- Options for LaTeX output ------------------------------------------------
v0.6.1,The paper size ('letterpaper' or 'a4paper').
v0.6.1,
v0.6.1,"'papersize': 'letterpaper',"
v0.6.1,"The font size ('10pt', '11pt' or '12pt')."
v0.6.1,
v0.6.1,"'pointsize': '10pt',"
v0.6.1,Additional stuff for the LaTeX preamble.
v0.6.1,
v0.6.1,"'preamble': '',"
v0.6.1,Latex figure (float) alignment
v0.6.1,
v0.6.1,"'figure_align': 'htbp',"
v0.6.1,Grouping the document tree into LaTeX files. List of tuples
v0.6.1,"(source start file, target name, title,"
v0.6.1,"author, documentclass [howto, manual, or own class])."
v0.6.1,-- Options for manual page output ------------------------------------------
v0.6.1,One entry per manual page. List of tuples
v0.6.1,"(source start file, name, description, authors, manual section)."
v0.6.1,-- Options for Texinfo output ----------------------------------------------
v0.6.1,Grouping the document tree into Texinfo files. List of tuples
v0.6.1,"(source start file, target name, title, author,"
v0.6.1,"dir menu entry, description, category)"
v0.6.1,-- Options for Epub output -------------------------------------------------
v0.6.1,Bibliographic Dublin Core info.
v0.6.1,The unique identifier of the text. This can be a ISBN number
v0.6.1,or the project homepage.
v0.6.1,
v0.6.1,epub_identifier = ''
v0.6.1,A unique identification for the text.
v0.6.1,
v0.6.1,epub_uid = ''
v0.6.1,A list of files that should not be packed into the epub file.
v0.6.1,-- Extension configuration -------------------------------------------------
v0.6.1,-- Options for intersphinx extension ---------------------------------------
v0.6.1,Example configuration for intersphinx: refer to the Python standard library.
v0.6.1,-- Options for todo extension ----------------------------------------------
v0.6.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.6.1,-- Options for doctest extension -------------------------------------------
v0.6.1,we can document otherwise excluded entities here by returning False
v0.6.1,or skip otherwise included entities by returning True
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Calculate residuals
v0.6.1,Estimate E[T_res | Z_res]
v0.6.1,TODO. Deal with multi-class instrument
v0.6.1,Calculate nuisances
v0.6.1,Estimate E[T_res | Z_res]
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"We do a three way split, as typically a preliminary theta estimator would require"
v0.6.1,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.6.1,TODO. Deal with multi-class instrument
v0.6.1,Estimate final model of theta(X) by minimizing the square loss:
v0.6.1,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.6.1,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.6.1,at the expense of some small bias. For points with very small covariance we revert
v0.6.1,to the model-based preliminary estimate and do not add the correction term.
v0.6.1,Estimate preliminary theta in cross fitting manner
v0.6.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.6.1,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.6.1,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.6.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.6.1,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.6.1,TODO. The solution below is not really a valid cross-fitting
v0.6.1,as the test data are used to create the proj_t on the train
v0.6.1,which in the second train-test loop is used to create the nuisance
v0.6.1,cov on the test data. Hence the T variable of some sample
v0.6.1,"is implicitly correlated with its cov nuisance, through this flow"
v0.6.1,"of information. However, this seems a rather weak correlation."
v0.6.1,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.6.1,model.
v0.6.1,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.6.1,Estimate preliminary theta in cross fitting manner
v0.6.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.6.1,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.6.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.6.1,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.6.1,#############################################################################
v0.6.1,Classes for the DRIV implementation for the special case of intent-to-treat
v0.6.1,A/B test
v0.6.1,#############################################################################
v0.6.1,Estimate preliminary theta in cross fitting manner
v0.6.1,Estimate p(X) = E[T | X] in cross fitting manner
v0.6.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.6.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.6.1,We can use statsmodel for all hypothesis testing capabilities
v0.6.1,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.6.1,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.6.1,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.6.1,model_T_XZ = lambda: model_clf()
v0.6.1,#'days_visited': lambda:
v0.6.1,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.6.1,Turn strings into categories for numeric mapping
v0.6.1,### Defining some generic regressors and classifiers
v0.6.1,This a generic non-parametric regressor
v0.6.1,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.6.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.6.1,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.6.1,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.6.1,model = lambda: RandomForestRegressor(n_estimators=100)
v0.6.1,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.6.1,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.6.1,model = lambda: LinearRegression(n_jobs=-1)
v0.6.1,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.6.1,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.6.1,underlying model whenever predict is called.
v0.6.1,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.6.1,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.6.1,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.6.1,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.6.1,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.6.1,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.6.1,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.6.1,We need to specify models to be used for each of these residualizations
v0.6.1,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.6.1,"E[T | X, Z]"
v0.6.1,E[TZ | X]
v0.6.1,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.6.1,n_splits determines the number of splits to be used for cross-fitting.
v0.6.1,# Algorithm 2 - Current Method
v0.6.1,In[121]:
v0.6.1,# Algorithm 3 - DRIV ATE
v0.6.1,dmliv_model_effect = lambda: model()
v0.6.1,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.6.1,"dmliv_model_effect(),"
v0.6.1,n_splits=1)
v0.6.1,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.6.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.6.1,"Once multiple treatments are supported, we'll need to fix this"
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.6.1,We can use statsmodel for all hypothesis testing capabilities
v0.6.1,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.6.1,We can use statsmodel for all hypothesis testing capabilities
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,TODO. Deal with multi-class instrument/treatment
v0.6.1,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.6.1,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.6.1,Estimate p(X) = E[T | X] in cross-fitting manner
v0.6.1,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.6.1,##################
v0.6.1,Global settings #
v0.6.1,##################
v0.6.1,Global plotting controls
v0.6.1,"Control for support size, can control for more"
v0.6.1,#################
v0.6.1,File utilities #
v0.6.1,#################
v0.6.1,#################
v0.6.1,Plotting utils #
v0.6.1,#################
v0.6.1,bias
v0.6.1,var
v0.6.1,rmse
v0.6.1,r2
v0.6.1,Infer feature dimension
v0.6.1,Metrics by support plots
v0.6.1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.6.1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.6.1,Steven Wu <zhiww@microsoft.com>
v0.6.1,Initialize causal tree parameters
v0.6.1,Create splits of causal tree
v0.6.1,Estimate treatment effects at the leafs
v0.6.1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.6.1,the corresponding split and associating the effect computed on that leaf
v0.6.1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.6.1,Safety check
v0.6.1,Weighted linear regression
v0.6.1,Calculates weights
v0.6.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.6.1,over all indices
v0.6.1,Similar for `a` weights
v0.6.1,Doesn't have sample weights
v0.6.1,Is a linear model
v0.6.1,Weighted linear regression
v0.6.1,Calculates weights
v0.6.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.6.1,over all indices
v0.6.1,Similar for `a` weights
v0.6.1,normalize weights
v0.6.1,"Split the data in half, train and test"
v0.6.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.6.1,"a function of W, using only the train fold"
v0.6.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.6.1,"Split the data in half, train and test"
v0.6.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.6.1,"a function of W, using only the train fold"
v0.6.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.6.1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.6.1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.6.1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.6.1,"Split the data in half, train and test"
v0.6.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.6.1,"a function of x, using only the train fold"
v0.6.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.6.1,Compute coefficient by OLS on residuals
v0.6.1,"Split the data in half, train and test"
v0.6.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.6.1,"a function of x, using only the train fold"
v0.6.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.6.1,Estimate multipliers for second order orthogonal method
v0.6.1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.6.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.6.1,Create local sample set
v0.6.1,compute the base estimate for the current node using double ml or second order double ml
v0.6.1,compute the influence functions here that are used for the criterion
v0.6.1,generate random proposals of dimensions to split
v0.6.1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.6.1,compute criterion for each proposal
v0.6.1,if splitting creates valid leafs in terms of mean leaf size
v0.6.1,Calculate criterion for split
v0.6.1,Else set criterion to infinity so that this split is not chosen
v0.6.1,If no good split was found
v0.6.1,Find split that minimizes criterion
v0.6.1,Set the split attributes at the node
v0.6.1,Create child nodes with corresponding subsamples
v0.6.1,Recursively split children
v0.6.1,Return parent node
v0.6.1,estimate the local parameter at the leaf using the estimate data
v0.6.1,###################
v0.6.1,Argument parsing #
v0.6.1,###################
v0.6.1,#########################################
v0.6.1,Parameters constant across experiments #
v0.6.1,#########################################
v0.6.1,Outcome support
v0.6.1,Treatment support
v0.6.1,Evaluation grid
v0.6.1,Treatment effects array
v0.6.1,Other variables
v0.6.1,##########################
v0.6.1,Data Generating Process #
v0.6.1,##########################
v0.6.1,Log iteration
v0.6.1,"Generate controls, features, treatment and outcome"
v0.6.1,T and Y residuals to be used in later scripts
v0.6.1,Save generated dataset
v0.6.1,#################
v0.6.1,ORF parameters #
v0.6.1,#################
v0.6.1,######################################
v0.6.1,Train and evaluate treatment effect #
v0.6.1,######################################
v0.6.1,########
v0.6.1,Plots #
v0.6.1,########
v0.6.1,###############
v0.6.1,Save results #
v0.6.1,###############
v0.6.1,##############
v0.6.1,Run Rscript #
v0.6.1,##############
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Check inputs
v0.6.1,Check inputs
v0.6.1,Check inputs
v0.6.1,Check inputs
v0.6.1,Check inputs
v0.6.1,Estimate response function
v0.6.1,Check inputs
v0.6.1,Train model on controls. Assign higher weight to units resembling
v0.6.1,treated units.
v0.6.1,Train model on the treated. Assign higher weight to units resembling
v0.6.1,control units.
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Create splits of causal tree
v0.6.1,Make sure the correct exception is being rethrown
v0.6.1,Must make sure indices are merged correctly
v0.6.1,Require group assignment t to be one-hot-encoded
v0.6.1,Define an inner function that iterates over group predictions
v0.6.1,Convert rows to columns
v0.6.1,Get predictions for the 2 splits
v0.6.1,Must make sure indices are merged correctly
v0.6.1,Estimators
v0.6.1,OrthoForest parameters
v0.6.1,Sub-forests
v0.6.1,Fit check
v0.6.1,TODO: Check performance
v0.6.1,Must normalize weights
v0.6.1,Crossfitting
v0.6.1,Compute weighted nuisance estimates
v0.6.1,Generate subsample indices
v0.6.1,Safety check
v0.6.1,Build trees in parallel
v0.6.1,Calculates weights
v0.6.1,Bootstraping has repetitions in tree sample
v0.6.1,Similar for `a` weights
v0.6.1,Bootstraping has repetitions in tree sample
v0.6.1,Copy and/or define models
v0.6.1,Define nuisance estimators
v0.6.1,Define parameter estimators
v0.6.1,Define
v0.6.1,Nuissance estimates evaluated with cross-fitting
v0.6.1,Define 2-fold iterator
v0.6.1,need safe=False when cloning for WeightedModelWrapper
v0.6.1,Compute residuals
v0.6.1,Compute coefficient by OLS on residuals
v0.6.1,"Parameter returned by LinearRegression is (d_T, )"
v0.6.1,Compute residuals
v0.6.1,Compute coefficient by OLS on residuals
v0.6.1,ell_2 regularization
v0.6.1,Ridge regression estimate
v0.6.1,"Parameter returned by LinearRegression is (d_T, )"
v0.6.1,Return moments and gradients
v0.6.1,Compute residuals
v0.6.1,Compute moments
v0.6.1,"Moments shape is (n, d_T)"
v0.6.1,Compute moment gradients
v0.6.1,Copy and/or define models
v0.6.1,Nuisance estimators shall be defined during fitting because they need to know the number of distinct
v0.6.1,treatments
v0.6.1,Define parameter estimators
v0.6.1,Define moment and mean gradient estimator
v0.6.1,Define autoencoder
v0.6.1,"Check that T is shape (n, )"
v0.6.1,Check T is numeric
v0.6.1,Train label encoder
v0.6.1,Define number of classes
v0.6.1,Call `fit` from parent class
v0.6.1,"Test that T contains all treatments. If not, return None"
v0.6.1,Nuissance estimates evaluated with cross-fitting
v0.6.1,Define 2-fold iterator
v0.6.1,Check if there is only one example of some class
v0.6.1,No need to crossfit for internal nodes
v0.6.1,Compute partial moments
v0.6.1,"If any of the values in the parameter estimate is nan, return None"
v0.6.1,Compute partial moments
v0.6.1,Compute coefficient by OLS on residuals
v0.6.1,ell_2 regularization
v0.6.1,Ridge regression estimate
v0.6.1,"Parameter returned by LinearRegression is (d_T, )"
v0.6.1,Return moments and gradients
v0.6.1,Compute partial moments
v0.6.1,Compute moments
v0.6.1,"Moments shape is (n, d_T-1)"
v0.6.1,Compute moment gradients
v0.6.1,Need to calculate this in an elegant way for when propensity is 0
v0.6.1,This will flatten T
v0.6.1,Check that T is numeric
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"if both X and W are None, just return a column of ones"
v0.6.1,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.6.1,We need to go back to the label representation of the one-hot so as to call
v0.6.1,the classifier.
v0.6.1,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.6.1,This works both with our without the weighting trick as the treatments T are unit vector
v0.6.1,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.6.1,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.6.1,both Parametric and Non Parametric DML.
v0.6.1,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.6.1,for internal use by the library
v0.6.1,NOTE This is used by the inference methods and is more for internal use to the library
v0.6.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.6.1,since we clone it and fit separate copies
v0.6.1,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.6.1,TODO: support sample_var
v0.6.1,"TODO: consider whether we need more care around stateful featurizers,"
v0.6.1,since we clone it and fit separate copies
v0.6.1,add statsmodels to parent's options
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,#######################################
v0.6.1,Core DML Tests
v0.6.1,#######################################
v0.6.1,How many samples
v0.6.1,How many control features
v0.6.1,How many treatment variables
v0.6.1,Coefficients of how controls affect treatments
v0.6.1,Coefficients of how controls affect outcome
v0.6.1,Treatment effects that we want to estimate
v0.6.1,Run dml estimation
v0.6.1,How many samples
v0.6.1,How many control features
v0.6.1,How many treatment variables
v0.6.1,Coefficients of how controls affect treatments
v0.6.1,Coefficients of how controls affect outcome
v0.6.1,Treatment effects that we want to estimate
v0.6.1,Run dml estimation
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.6.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"this will have dimension (d,) + shape(X)"
v0.6.1,send the first dimension to the end
v0.6.1,columns are featurized independently; partial derivatives are only non-zero
v0.6.1,when taken with respect to the same column each time
v0.6.1,don't fit intercept; manually add column of ones to the data instead;
v0.6.1,this allows us to ignore the intercept when computing marginal effects
v0.6.1,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.6.1,store number of columns of T so that we can pass scalars to effect
v0.6.1,two stage approximation
v0.6.1,"first, get basis expansions of T, X, and Z"
v0.6.1,"regress T expansion on X,Z expansions concatenated with W"
v0.6.1,"predict ft_T from interacted ft_X, ft_Z"
v0.6.1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.6.1,dT may be only 2-dimensional)
v0.6.1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.6.1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.6.1,(which needs to have been expanded if there's a discrete treatment)
v0.6.1,We can write effect interval as a function of const_marginal_effect_interval for a single treatment
v0.6.1,once the estimator has been fit
v0.6.1,We can write effect interval as a function of predict_interval of the final method for linear models
v0.6.1,"once the estimator has been fit, it's kosher to store d_t here"
v0.6.1,(which needs to have been expanded if there's a discrete treatment)
v0.6.1,need to set the fit args before the estimator is fit
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.6.1,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.6.1,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.6.1,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.6.1,checks that X is 2D array.
v0.6.1,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.6.1,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.6.1,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.6.1,"Replacing this method which is invalid for this class, so that we make the"
v0.6.1,dosctring empty and not appear in the docs.
v0.6.1,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.6.1,TODO: support sample_var
v0.6.1,"Replacing this method which is invalid for this class, so that we make the"
v0.6.1,dosctring empty and not appear in the docs.
v0.6.1,add statsmodels to parent's options
v0.6.1,Replacing to remove docstring
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,TODO: make sure to use random seeds wherever necessary
v0.6.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.6.1,"unfortunately with the Theano and Tensorflow backends,"
v0.6.1,the straightforward use of K.stop_gradient can cause an error
v0.6.1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.6.1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.6.1,so that those layers remain connected but with 0 gradient
v0.6.1,|| t - mu_i || ^2
v0.6.1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.6.1,Use logsumexp for numeric stability:
v0.6.1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.6.1,TODO: does the numeric stability actually make any difference?
v0.6.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.6.1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.6.1,generate cumulative sum via matrix multiplication
v0.6.1,"Generate standard uniform values in shape (batch_size,1)"
v0.6.1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.6.1,we use uniform_like instead with an input of an appropriate shape)
v0.6.1,convert to floats and multiply to perform equivalent of logical AND
v0.6.1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.6.1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.6.1,we use normal_like instead with an input of an appropriate shape)
v0.6.1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.6.1,prevent gradient from passing through sampling
v0.6.1,three options: biased or upper-bound loss require a single number of samples;
v0.6.1,unbiased can take different numbers for the network and its gradient
v0.6.1,"sample: (() -> Layer, int) -> Layer"
v0.6.1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.6.1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.6.1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.6.1,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.6.1,the dimensionality of the output of the network
v0.6.1,TODO: is there a more robust way to do this?
v0.6.1,TODO: do we need to give the user more control over other arguments to fit?
v0.6.1,"subtle point: we need to build a new model each time,"
v0.6.1,because each model encapsulates its randomness
v0.6.1,TODO: do we need to give the user more control over other arguments to fit?
v0.6.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.6.1,not a general tensor (because of how backprop works in every framework)
v0.6.1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.6.1,but this seems annoying...)
v0.6.1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.6.1,TODO: any way to get this to work on batches of arbitrary size?
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,#######################################################
v0.6.1,Perfect Data DGPs for Testing Correctness of Code
v0.6.1,#######################################################
v0.6.1,Generate random control co-variates
v0.6.1,Create epsilon residual treatments that deterministically sum up to
v0.6.1,zero
v0.6.1,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.6.1,conditional on each co-variate vector is equal to zero
v0.6.1,We simply subtract the conditional mean from the epsilons
v0.6.1,Construct treatments as T = X*A + epsilon
v0.6.1,Construct outcomes as y = X*beta + T*effect
v0.6.1,Generate random control co-variates
v0.6.1,Create epsilon residual treatments that deterministically sum up to
v0.6.1,zero
v0.6.1,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.6.1,conditional on each co-variate vector is equal to zero
v0.6.1,We simply subtract the conditional mean from the epsilons
v0.6.1,Construct treatments as T = X*A + epsilon
v0.6.1,Construct outcomes as y = X*beta + T*effect
v0.6.1,Generate random control co-variates
v0.6.1,Construct treatments as T = X*A + epsilon
v0.6.1,Construct outcomes as y = X*beta + T*effect
v0.6.1,Generate random control co-variates
v0.6.1,Create epsilon residual treatments
v0.6.1,Construct treatments as T = X*A + epsilon
v0.6.1,Construct outcomes as y = X*beta + T*effect + eta
v0.6.1,Generate random control co-variates
v0.6.1,Use the same treatment vector for each row
v0.6.1,Construct outcomes as y = X*beta + T*effect
v0.6.1,Licensed under the MIT License.
v0.6.1,"since inference objects can be stateful, we must copy it before fitting;"
v0.6.1,otherwise this sequence wouldn't work:
v0.6.1,"est1.fit(..., inference=inf)"
v0.6.1,"est2.fit(..., inference=inf)"
v0.6.1,est1.effect_interval(...)
v0.6.1,because inf now stores state from fitting est2
v0.6.1,call the wrapped fit method
v0.6.1,NOTE: we call inference fit *after* calling the main fit method
v0.6.1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.6.1,but tensordot can't be applied to this problem because we don't sum over m
v0.6.1,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.6.1,of rows of T was not taken into account
v0.6.1,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.6.1,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.6.1,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.6.1,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.6.1,add statsmodels to parent's options
v0.6.1,add debiasedlasso to parent's options
v0.6.1,TODO Share some logic with non-discrete version
v0.6.1,add statsmodels to parent's options
v0.6.1,add statsmodels to parent's options
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Check if model is sparse enough for this model
v0.6.1,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.6.1,TODO: any way to avoid creating a copy if the array was already dense?
v0.6.1,"the call is necessary if the input was something like a list, though"
v0.6.1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.6.1,so convert to pydata sparse first
v0.6.1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.6.1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.6.1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.6.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.6.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.6.1,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.6.1,same number of input definitions as arrays
v0.6.1,input definitions have same number of dimensions as each array
v0.6.1,all result indices are unique
v0.6.1,all result indices must match at least one input index
v0.6.1,"map indices to all array, axis pairs for that index"
v0.6.1,each index has the same cardinality wherever it appears
v0.6.1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.6.1,Algo: while list contains more than one entry
v0.6.1,take two entries
v0.6.1,sort both lists by intersection of their indices
v0.6.1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.6.1,"take the union of indices and the product of values), stepping through each list linearly"
v0.6.1,TODO: might be faster to break into connected components first
v0.6.1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.6.1,"so compute their content separately, then take cartesian product"
v0.6.1,this would save a few pointless sorts by empty tuples
v0.6.1,TODO: Consider investigating other performance ideas for these cases
v0.6.1,where the dense method beat the sparse method (usually sparse is faster)
v0.6.1,"e,facd,c->cfed"
v0.6.1,sparse: 0.0335489
v0.6.1,dense:  0.011465999999999997
v0.6.1,"gbd,da,egb->da"
v0.6.1,sparse: 0.0791625
v0.6.1,dense:  0.007319099999999995
v0.6.1,"dcc,d,faedb,c->abe"
v0.6.1,sparse: 1.2868097
v0.6.1,dense:  0.44605229999999985
v0.6.1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.6.1,TODO: would using einsum's paths to optimize the order of merging help?
v0.6.1,Normalize weights
v0.6.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.6.1,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.6.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.6.1,clean way of achieving this
v0.6.1,make sure we don't accidentally escape anything in the substitution
v0.6.1,Fetch appropriate color for node
v0.6.1,"red for negative, green for positive"
v0.6.1,in multi-target use first target
v0.6.1,Write node mean CATE
v0.6.1,Write node std of CATE
v0.6.1,Write confidence interval information if at leaf node
v0.6.1,Fetch appropriate color for node
v0.6.1,"red for negative, green for positive"
v0.6.1,Write node mean CATE
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.6.1,use a binary array to get stratified split in case of discrete treatment
v0.6.1,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.6.1,drop first column since all columns sum to one
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Estimators
v0.6.1,Causal tree parameters
v0.6.1,Tree structure
v0.6.1,No need for a random split since the data is already
v0.6.1,a random subsample from the original input
v0.6.1,node list stores the nodes that are yet to be splitted
v0.6.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.6.1,Create local sample set
v0.6.1,Compute nuisance estimates for the current node
v0.6.1,Nuisance estimate cannot be calculated
v0.6.1,Estimate parameter for current node
v0.6.1,Node estimate cannot be calculated
v0.6.1,Calculate moments and gradient of moments for current data
v0.6.1,Calculate inverse gradient
v0.6.1,The gradient matrix is not invertible.
v0.6.1,No good split can be found
v0.6.1,Calculate point-wise pseudo-outcomes rho
v0.6.1,a split is determined by a feature and a sample pair
v0.6.1,the number of possible splits is at most (number of features) * (number of node samples)
v0.6.1,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.6.1,parse row and column of random pair
v0.6.1,the sample of the pair is the integer division of the random number with n_feats
v0.6.1,calculate the binary indicator of whether sample i is on the left or the right
v0.6.1,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.6.1,calculate the number of samples on the left child for each proposed split
v0.6.1,calculate the analogous binary indicator for the samples in the estimation set
v0.6.1,calculate the number of estimation samples on the left child of each proposed split
v0.6.1,find the upper and lower bound on the size of the left split for the split
v0.6.1,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.6.1,on each side.
v0.6.1,similarly for the estimation sample set
v0.6.1,if there is no valid split then don't create any children
v0.6.1,filter only the valid splits
v0.6.1,calculate the average influence vector of the samples in the left child
v0.6.1,calculate the average influence vector of the samples in the right child
v0.6.1,take the square of each of the entries of the influence vectors and normalize
v0.6.1,by size of each child
v0.6.1,calculate the vector score of each candidate split as the average of left and right
v0.6.1,influence vectors
v0.6.1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.6.1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.6.1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.6.1,calculate the scalar score of each split by aggregating across the vector of scores
v0.6.1,Find split that minimizes criterion
v0.6.1,Create child nodes with corresponding subsamples
v0.6.1,add the created children to the list of not yet split nodes
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,TODO: Add a __dir__ implementation?
v0.6.1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.6.1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.6.1,then we should be computing a confidence interval for the wrapped calls
v0.6.1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.6.1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.6.1,"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
v0.6.1,Remove children with nonwhite mothers from the treatment group
v0.6.1,Remove children with nonwhite mothers from the treatment group
v0.6.1,Select columns
v0.6.1,Scale the numeric variables
v0.6.1,"Change the binary variable 'first' takes values in {1,2}"
v0.6.1,Append a column of ones as intercept
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.6.1,creating notebooks that are annoying for our users to actually run themselves
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.6.1,"prior to calling interpret, can't plot, render, etc."
v0.6.1,can interpret without uncertainty
v0.6.1,can't interpret with uncertainty if inference wasn't used during fit
v0.6.1,can interpret with uncertainty if we refit
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,simple DGP only for illustration
v0.6.1,Define the treatment model neural network architecture
v0.6.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.6.1,"so the input shape is (d_z + d_x,)"
v0.6.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.6.1,add extra layers on top for the mixture density network
v0.6.1,Define the response model neural network architecture
v0.6.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.6.1,"so the input shape is (d_t + d_x,)"
v0.6.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.6.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.6.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.6.1,so that the same weights will be reused in each instantiation
v0.6.1,number of samples to use in second estimate of the response
v0.6.1,(to make loss estimate unbiased)
v0.6.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.6.1,do something with predictions...
v0.6.1,also test vector t and y
v0.6.1,simple DGP only for illustration
v0.6.1,Define the treatment model neural network architecture
v0.6.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.6.1,"so the input shape is (d_z + d_x,)"
v0.6.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.6.1,add extra layers on top for the mixture density network
v0.6.1,Define the response model neural network architecture
v0.6.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.6.1,"so the input shape is (d_t + d_x,)"
v0.6.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.6.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.6.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.6.1,so that the same weights will be reused in each instantiation
v0.6.1,number of samples to use in second estimate of the response
v0.6.1,(to make loss estimate unbiased)
v0.6.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.6.1,do something with predictions...
v0.6.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.6.1,test = True ensures we draw test set images
v0.6.1,test = True ensures we draw test set images
v0.6.1,re-draw to get new independent treatment and implied response
v0.6.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.6.1,above is necesary so that reduced form doesn't win
v0.6.1,covariates: time and emotion
v0.6.1,random instrument
v0.6.1,z -> price
v0.6.1,true observable demand function
v0.6.1,errors
v0.6.1,response
v0.6.1,test = True ensures we draw test set images
v0.6.1,test = True ensures we draw test set images
v0.6.1,re-draw to get new independent treatment and implied response
v0.6.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.6.1,above is necesary so that reduced form doesn't win
v0.6.1,covariates: time and emotion
v0.6.1,random instrument
v0.6.1,z -> price
v0.6.1,true observable demand function
v0.6.1,errors
v0.6.1,response
v0.6.1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.6.1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.6.1,For some reason this doesn't work at all when run against the CNTK backend...
v0.6.1,"model.compile('nadam', loss=lambda _,l:l)"
v0.6.1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.6.1,generate a valiation set
v0.6.1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.6.1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.6.1,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.6.1,so we need to transpose the result
v0.6.1,1-d output
v0.6.1,2-d output
v0.6.1,Single dimensional output y
v0.6.1,Multi-dimensional output y
v0.6.1,1-d y
v0.6.1,multi-d y
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,test that we can fit with the same arguments as the base estimator
v0.6.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.6.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6.1,with the same shape for the lower and upper bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,test that we can do the same thing once we provide percentile bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.6.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6.1,with the same shape for the lower and upper bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,test that we can do the same thing once we provide percentile bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,test that we can fit with the same arguments as the base estimator
v0.6.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.6.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6.1,with the same shape for the lower and upper bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,test that we can do the same thing once we provide percentile bounds
v0.6.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.6.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6.1,with the same shape for the lower and upper bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,test that we can do the same thing once we provide percentile bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,test that we can fit with the same arguments as the base estimator
v0.6.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.6.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6.1,with the same shape for the lower and upper bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,test that we can do the same thing once we provide percentile bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.6.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6.1,with the same shape for the lower and upper bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,test that we can do the same thing once we provide percentile bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6.1,with the same shape for the lower and upper bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,test that the estimated effect is usually within the bounds
v0.6.1,test that we can do the same thing once we provide alpha explicitly
v0.6.1,test that the lower and upper bounds differ
v0.6.1,test that the estimated effect is usually within the bounds
v0.6.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6.1,with the same shape for the lower and upper bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,TODO: test that the estimated effect is usually within the bounds
v0.6.1,and that the true effect is also usually within the bounds
v0.6.1,test that we can do the same thing once we provide percentile bounds
v0.6.1,test that the lower and upper bounds differ
v0.6.1,TODO: test that the estimated effect is usually within the bounds
v0.6.1,and that the true effect is also usually within the bounds
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Test non keyword based calls to fit
v0.6.1,Test custom splitter
v0.6.1,Test incomplete set of test folds
v0.6.1,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,make sure cross product varies more slowly with first array
v0.6.1,and that vectors are okay as inputs
v0.6.1,number of inputs in specification must match number of inputs
v0.6.1,must have an output
v0.6.1,output indices must be unique
v0.6.1,output indices must be present in an input
v0.6.1,number of indices must match number of dimensions for each input
v0.6.1,repeated indices must always have consistent sizes
v0.6.1,transpose
v0.6.1,tensordot
v0.6.1,trace
v0.6.1,TODO: set up proper flag for this
v0.6.1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.6.1,"of all of the distinct indices that appear in any input,"
v0.6.1,pick a random subset of them (of size at most 5) to appear in the output
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Preprocess data
v0.6.1,Convert 'week' to a date
v0.6.1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.6.1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.6.1,Take log of price
v0.6.1,Make brand numeric
v0.6.1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.6.1,which have all zero coeffs
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.6.1,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.6.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.6.1,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.6.1,The __warningregistry__'s need to be in a pristine state for tests
v0.6.1,to work properly.
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Set random seed
v0.6.1,Generate data
v0.6.1,DGP constants
v0.6.1,Test data
v0.6.1,Constant treatment effect
v0.6.1,Constant treatment with multi output Y
v0.6.1,Heterogeneous treatment
v0.6.1,Heterogeneous treatment with multi output Y
v0.6.1,TLearner test
v0.6.1,Instantiate TLearner
v0.6.1,Test inputs
v0.6.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.6.1,Instantiate SLearner
v0.6.1,Test inputs
v0.6.1,Test constant treatment effect
v0.6.1,Test constant treatment effect with multi output Y
v0.6.1,Test heterogeneous treatment effect
v0.6.1,Need interactions between T and features
v0.6.1,Test heterogeneous treatment effect with multi output Y
v0.6.1,Instantiate XLearner
v0.6.1,Test inputs
v0.6.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.6.1,Instantiate DomainAdaptationLearner
v0.6.1,Test inputs
v0.6.1,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.6.1,Get the true treatment effect
v0.6.1,Get the true treatment effect
v0.6.1,Fit learner and get the effect and marginal effect
v0.6.1,Compute treatment effect residuals (absolute)
v0.6.1,Check that at least 90% of predictions are within tolerance interval
v0.6.1,Check whether the output shape is right
v0.6.1,Check that one can pass in regular lists
v0.6.1,Check that it fails correctly if lists of different shape are passed in
v0.6.1,"Check that it works when T, Y have shape (n, 1)"
v0.6.1,Generate covariates
v0.6.1,Generate treatment
v0.6.1,Calculate outcome
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,DGP constants
v0.6.1,Generate data
v0.6.1,Test data
v0.6.1,Remove warnings that might be raised by the models passed into the ORF
v0.6.1,Generate data with continuous treatments
v0.6.1,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.6.1,does not work well with parallelism.
v0.6.1,Test inputs for continuous treatments
v0.6.1,--> Check that one can pass in regular lists
v0.6.1,--> Check that it fails correctly if lists of different shape are passed in
v0.6.1,Check that outputs have the correct shape
v0.6.1,Test continuous treatments with controls
v0.6.1,Test continuous treatments without controls
v0.6.1,Generate data with binary treatments
v0.6.1,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.6.1,does not work well with parallelism.
v0.6.1,Test inputs for binary treatments
v0.6.1,--> Check that one can pass in regular lists
v0.6.1,--> Check that it fails correctly if lists of different shape are passed in
v0.6.1,"--> Check that it works when T, Y have shape (n, 1)"
v0.6.1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.6.1,--> Check that it fails correctly when the treatments are not numeric
v0.6.1,Check that outputs have the correct shape
v0.6.1,Test binary treatments with controls
v0.6.1,Test binary treatments without controls
v0.6.1,Only applicable to continuous treatments
v0.6.1,Generate data for 2 treatments
v0.6.1,Test multiple treatments with controls
v0.6.1,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.6.1,The rest for controls. Just as an example.
v0.6.1,Generating A/B test data
v0.6.1,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.6.1,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.6.1,Generate data with continuous treatments
v0.6.1,Instantiate model with most of the default parameters
v0.6.1,Compute the treatment effect on test points
v0.6.1,Compute treatment effect residuals
v0.6.1,Multiple treatments
v0.6.1,Allow at most 10% test points to be outside of the tolerance interval
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.6.1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.6.1,ensure that we've got at least two of every element
v0.6.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.6.1,TODO: add stratification to bootstrap so that we can use it
v0.6.1,even with discrete treatments
v0.6.1,make sure we can call the marginal_effect and effect methods
v0.6.1,"make sure we can call effect with implied scalar treatments,"
v0.6.1,"no matter the dimensions of T, and also that we warn when there"
v0.6.1,are multiple treatments
v0.6.1,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.6.1,ensure that we've got at least two of every element
v0.6.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.6.1,"TODO Add bootstrap inference, once discrete treatment issue is fixed"
v0.6.1,make sure we can call the marginal_effect and effect methods
v0.6.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.6.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.6.1,We concatenate the two copies data
v0.6.1,create a simple artificial setup where effect of moving from treatment
v0.6.1,"1 -> 2 is 2,"
v0.6.1,"1 -> 3 is 1, and"
v0.6.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.6.1,"Using an uneven number of examples from different classes,"
v0.6.1,"and having the treatments in non-lexicographic order,"
v0.6.1,Should rule out some basic issues.
v0.6.1,test that we can fit with a KFold instance
v0.6.1,test that we can fit with a train/test iterable
v0.6.1,(incorrectly) use a final model with an intercept
v0.6.1,"Because final model is fixed, actual values of T and Y don't matter"
v0.6.1,Ensure reproducibility
v0.6.1,Sparse DGP
v0.6.1,Treatment effect coef
v0.6.1,Other coefs
v0.6.1,Features and controls
v0.6.1,Test sparse estimator
v0.6.1,"--> test coef_, intercept_"
v0.6.1,--> test treatment effects
v0.6.1,Restrict x_test to vectors of norm < 1
v0.6.1,--> check inference
v0.6.1,Check that a majority of true effects lie in the 5-95% CI
v0.6.1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.6.1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.6.1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.6.1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.6.1,sparse test case: heterogeneous effect by product
v0.6.1,need at least as many rows in e_y as there are distinct columns
v0.6.1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Set random seed
v0.6.1,Generate data
v0.6.1,DGP constants
v0.6.1,Test data
v0.6.1,Constant treatment effect and propensity
v0.6.1,Heterogeneous treatment and propensity
v0.6.1,ensure that we've got at least two of every element
v0.6.1,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.6.1,TODO: add stratification to bootstrap so that we can use it even with discrete treatments
v0.6.1,make sure we can call the marginal_effect and effect methods
v0.6.1,"make sure we can call effect with implied scalar treatments, no matter the"
v0.6.1,"dimensions of T, and also that we warn when there are multiple treatments"
v0.6.1,create a simple artificial setup where effect of moving from treatment
v0.6.1,"1 -> 2 is 2,"
v0.6.1,"1 -> 3 is 1, and"
v0.6.1,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.6.1,"Using an uneven number of examples from different classes,"
v0.6.1,"and having the treatments in non-lexicographic order,"
v0.6.1,Should rule out some basic issues.
v0.6.1,test that we can fit with a KFold instance
v0.6.1,test that we can fit with a train/test iterable
v0.6.1,Test inputs
v0.6.1,self._test_inputs(DR_learner)
v0.6.1,Test constant treatment effect
v0.6.1,Test heterogeneous treatment effect
v0.6.1,Test heterogenous treatment effect for W =/= None
v0.6.1,Sparse DGP
v0.6.1,Treatment effect coef
v0.6.1,Other coefs
v0.6.1,Features and controls
v0.6.1,Test sparse estimator
v0.6.1,"--> test coef_, intercept_"
v0.6.1,--> test treatment effects
v0.6.1,Restrict x_test to vectors of norm < 1
v0.6.1,--> check inference
v0.6.1,Check that a majority of true effects lie in the 5-95% CI
v0.6.1,Fit learner and get the effect
v0.6.1,Get the true treatment effect
v0.6.1,Compute treatment effect residuals (absolute)
v0.6.1,Check that at least 90% of predictions are within tolerance interval
v0.6.1,Only for heterogeneous TE
v0.6.1,Fit learner on X and W and get the effect
v0.6.1,Get the true treatment effect
v0.6.1,Compute treatment effect residuals (absolute)
v0.6.1,Check that at least 90% of predictions are within tolerance interval
v0.6.1,Check that one can pass in regular lists
v0.6.1,Check that it fails correctly if lists of different shape are passed in
v0.6.1,Check that it fails when T contains values other than 0 and 1
v0.6.1,"Check that it works when T, Y have shape (n, 1)"
v0.6.1,Generate covariates
v0.6.1,Generate treatment
v0.6.1,Calculate outcome
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,DGP constants
v0.6.1,DGP coefficients
v0.6.1,Generated outcomes
v0.6.1,################
v0.6.1,WeightedLasso #
v0.6.1,################
v0.6.1,Define weights
v0.6.1,Define extended datasets
v0.6.1,Range of alphas
v0.6.1,Compare with Lasso
v0.6.1,--> No intercept
v0.6.1,--> With intercept
v0.6.1,When DGP has no intercept
v0.6.1,When DGP has intercept
v0.6.1,--> Coerce coefficients to be positive
v0.6.1,--> Toggle max_iter & tol
v0.6.1,Define weights
v0.6.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.6.1,Mixed DGP scenario.
v0.6.1,Define extended datasets
v0.6.1,Define weights
v0.6.1,Define multioutput
v0.6.1,##################
v0.6.1,WeightedLassoCV #
v0.6.1,##################
v0.6.1,Define alphas to test
v0.6.1,Compare with LassoCV
v0.6.1,--> No intercept
v0.6.1,--> With intercept
v0.6.1,--> Force parameters to be positive
v0.6.1,Choose a smaller n to speed-up process
v0.6.1,Compare fold weights
v0.6.1,Define weights
v0.6.1,Define extended datasets
v0.6.1,Define splitters
v0.6.1,WeightedKFold splitter
v0.6.1,Map weighted splitter to an extended splitter
v0.6.1,Define alphas to test
v0.6.1,Compare with LassoCV
v0.6.1,--> No intercept
v0.6.1,--> With intercept
v0.6.1,--> Force parameters to be positive
v0.6.1,###########################
v0.6.1,MultiTaskWeightedLassoCV #
v0.6.1,###########################
v0.6.1,Define alphas to test
v0.6.1,Define splitter
v0.6.1,Compare with MultiTaskLassoCV
v0.6.1,--> No intercept
v0.6.1,--> With intercept
v0.6.1,Define weights
v0.6.1,Define extended datasets
v0.6.1,Define splitters
v0.6.1,WeightedKFold splitter
v0.6.1,Map weighted splitter to an extended splitter
v0.6.1,Define alphas to test
v0.6.1,Compare with LassoCV
v0.6.1,--> No intercept
v0.6.1,--> With intercept
v0.6.1,################
v0.6.1,DebiasedLasso #
v0.6.1,################
v0.6.1,Test DebiasedLasso without weights
v0.6.1,--> Check debiased coeffcients without intercept
v0.6.1,--> Check debiased coeffcients with intercept
v0.6.1,--> Check 5-95 CI coverage for unit vectors
v0.6.1,Test DebiasedLasso with weights for one DGP
v0.6.1,Define weights
v0.6.1,Define extended datasets
v0.6.1,--> Check debiased coefficients
v0.6.1,Define weights
v0.6.1,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.6.1,--> Check debiased coeffcients
v0.6.1,Test that attributes propagate correctly
v0.6.1,Test MultiOutputDebiasedLasso without weights
v0.6.1,--> Check debiased coeffcients without intercept
v0.6.1,--> Check debiased coeffcients with intercept
v0.6.1,--> Check CI coverage
v0.6.1,Test MultiOutputDebiasedLasso with weights
v0.6.1,Define weights
v0.6.1,Define extended datasets
v0.6.1,--> Check debiased coefficients
v0.6.1,Unit vectors
v0.6.1,Unit vectors
v0.6.1,Check coeffcients and intercept are the same within tolerance
v0.6.1,Check results are similar with tolerance 1e-6
v0.6.1,Check if multitask
v0.6.1,Check that same alpha is chosen
v0.6.1,Check that the coefficients are similar
v0.6.1,selective ridge has a simple implementation that we can test against
v0.6.1,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.6.1,"it should be the case that when we set fit_intercept to true,"
v0.6.1,it doesn't matter whether the penalized model also fits an intercept or not
v0.6.1,create an extra copy of rows with weight 2
v0.6.1,"instead of a slice, explicitly return an array of indices"
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,"Found a good split, return."
v0.6.1,Record all splits in case the stratification by weight yeilds a worse partition
v0.6.1,Reseed random generator and try again
v0.6.1,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.6.1,"Found a good split, return."
v0.6.1,Did not find a good split
v0.6.1,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.6.1,Return most weight-balanced partition
v0.6.1,Weight stratification algorithm
v0.6.1,Sort weights for weight strata search
v0.6.1,There are some leftover indices that have yet to be assigned
v0.6.1,Append stratum splits to overall splits
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Our classes that derive from sklearn ones sometimes include
v0.6.1,inherited docstrings that have embedded doctests; we need the following imports
v0.6.1,so that they don't break.
v0.6.1,"Convert X, y into numpy arrays"
v0.6.1,Define fit parameters
v0.6.1,Some algorithms don't have a check_input option
v0.6.1,Check weights array
v0.6.1,Check that weights are size-compatible
v0.6.1,Normalize inputs
v0.6.1,Weight inputs
v0.6.1,Fit base class without intercept
v0.6.1,Fit Lasso
v0.6.1,Reset intercept
v0.6.1,The intercept is not calculated properly due the sqrt(weights) factor
v0.6.1,so it must be recomputed
v0.6.1,Fit lasso without weights
v0.6.1,Make weighted splitter
v0.6.1,Fit weighted model
v0.6.1,Make weighted splitter
v0.6.1,Fit weighted model
v0.6.1,Select optimal penalty
v0.6.1,Warn about consistency
v0.6.1,"Convert X, y into numpy arrays"
v0.6.1,Fit weighted lasso with user input
v0.6.1,"Center X, y"
v0.6.1,Calculate quantities that will be used later on. Account for centered data
v0.6.1,Calculate coefficient and error variance
v0.6.1,Add coefficient correction
v0.6.1,Set coefficients and intercept standard errors
v0.6.1,Set intercept
v0.6.1,Return alpha to 'auto' state
v0.6.1,"Note that in the case of no intercept, X_offset is 0"
v0.6.1,Calculate the variance of the predictions
v0.6.1,Calculate prediction confidence intervals
v0.6.1,Assumes flattened y
v0.6.1,Compute weighted residuals
v0.6.1,To be done once per target. Assumes y can be flattened.
v0.6.1,Assumes that X has already been offset
v0.6.1,Special case: n_features=1
v0.6.1,Compute Lasso coefficients for the columns of the design matrix
v0.6.1,Call weighted lasso on reduced design matrix
v0.6.1,Inherit some parameters from the parent
v0.6.1,Weighted tau
v0.6.1,Compute C_hat
v0.6.1,Compute theta_hat
v0.6.1,Allow for single output as well
v0.6.1,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.6.1,Set coef_ attribute
v0.6.1,Set intercept_ attribute
v0.6.1,Set selected_alpha_ attribute
v0.6.1,Set coef_std_err_
v0.6.1,intercept_std_err_
v0.6.1,set intercept_ attribute
v0.6.1,set coef_ attribute
v0.6.1,set alpha_ attribute
v0.6.1,set alphas_ attribute
v0.6.1,set n_iter_ attribute
v0.6.1,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.6.1,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.6.1,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.6.1,now regress X1 on y - X2 * beta2 to learn beta1
v0.6.1,set coef_ and intercept_ attributes
v0.6.1,Note that the penalized model should *not* have an intercept
v0.6.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6.1,Licensed under the MIT License.
v0.6.1,Construct the subsample of data
v0.6.1,Split into estimation and splitting sample set
v0.6.1,Fit the tree on the splitting sample
v0.6.1,Set the estimation values based on the estimation split
v0.6.1,Apply the trained tree on the estimation sample to get the path for every estimation sample
v0.6.1,Calculate the total weight of estimation samples on each tree node:
v0.6.1,\sum_i sample_weight[i] * 1{i \\in node}
v0.6.1,Calculate the total number of estimation samples on each tree node:
v0.6.1,|node| = \sum_{i} 1{i \\in node}
v0.6.1,Calculate the weighted sum of responses on the estimation sample on each node:
v0.6.1,\sum_{i} sample_weight[i] 1{i \\in node} Y_i
v0.6.1,Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
v0.6.1,and for each un-pruned tree set the value and the weight appropriately.
v0.6.1,If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
v0.6.1,"sample, then prune the whole sub-tree"
v0.6.1,Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
v0.6.1,Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
v0.6.1,Set the count to the estimation split count
v0.6.1,Validate or convert input data
v0.6.1,Pre-sort indices to avoid that each individual tree of the
v0.6.1,ensemble sorts the indices.
v0.6.1,Remap output
v0.6.1,reshape is necessary to preserve the data contiguity against vs
v0.6.1,"[:, np.newaxis] that does not."
v0.6.1,Check parameters
v0.6.1,"Free allocated memory, if any"
v0.6.1,We draw from the random state to get the random state we
v0.6.1,would have got if we hadn't used a warm_start.
v0.6.1,Parallel loop: we prefer the threading backend as the Cython code
v0.6.1,for fitting the trees is internally releasing the Python GIL
v0.6.1,making threading more efficient than multiprocessing in
v0.6.1,"that case. However, for joblib 0.12+ we respect any"
v0.6.1,"parallel_backend contexts set at a higher level,"
v0.6.1,since correctness does not rely on using threads.
v0.6.1,TODO. This slicing should ultimately be done inside the parallel function
v0.6.1,so that we don't need to create a matrix of size roughly n_samples * n_estimators
v0.6.1,Collect newly grown trees
v0.6.1,Helper class that accumulates an arbitrary function in parallel on the accumulator acc
v0.6.1,and calls the function fn on each tree e and returns the mean output. The function fn
v0.6.1,"should take as input a tree e, and return another function g_e, which takes as input X, check_input"
v0.6.1,"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
v0.6.1,index start to index end will be used. The returned result is essentially:
v0.6.1,(mean over e in slice)(g_e(X)).
v0.6.1,Check data
v0.6.1,Assign chunk of trees to jobs
v0.6.1,Check data
v0.6.1,Check data
v0.6.1,avoid storing the output of every estimator by summing them here
v0.6.1,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
v0.6.1,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
v0.6.1,"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
v0.6.1,where \theta(X) is the point estimate using the whole forest
v0.6.1,Calculate the variance of the latter as E[Q(S)^2]
v0.6,configuration is all pulled from setup.cfg
v0.6,-*- coding: utf-8 -*-
v0.6,
v0.6,Configuration file for the Sphinx documentation builder.
v0.6,
v0.6,This file does only contain a selection of the most common options. For a
v0.6,full list see the documentation:
v0.6,http://www.sphinx-doc.org/en/master/config
v0.6,-- Path setup --------------------------------------------------------------
v0.6,"If extensions (or modules to document with autodoc) are in another directory,"
v0.6,add these directories to sys.path here. If the directory is relative to the
v0.6,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.6,
v0.6,-- Project information -----------------------------------------------------
v0.6,-- General configuration ---------------------------------------------------
v0.6,"If your documentation needs a minimal Sphinx version, state it here."
v0.6,
v0.6,needs_sphinx = '1.0'
v0.6,"Add any Sphinx extension module names here, as strings. They can be"
v0.6,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.6,ones.
v0.6,"Add any paths that contain templates here, relative to this directory."
v0.6,The suffix(es) of source filenames.
v0.6,You can specify multiple suffix as a list of string:
v0.6,
v0.6,"source_suffix = ['.rst', '.md']"
v0.6,The master toctree document.
v0.6,The language for content autogenerated by Sphinx. Refer to documentation
v0.6,for a list of supported languages.
v0.6,
v0.6,This is also used if you do content translation via gettext catalogs.
v0.6,"Usually you set ""language"" from the command line for these cases."
v0.6,"List of patterns, relative to source directory, that match files and"
v0.6,directories to ignore when looking for source files.
v0.6,This pattern also affects html_static_path and html_extra_path.
v0.6,The name of the Pygments (syntax highlighting) style to use.
v0.6,-- Options for HTML output -------------------------------------------------
v0.6,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.6,a list of builtin themes.
v0.6,
v0.6,Theme options are theme-specific and customize the look and feel of a theme
v0.6,"further.  For a list of options available for each theme, see the"
v0.6,documentation.
v0.6,
v0.6,"Add any paths that contain custom static files (such as style sheets) here,"
v0.6,"relative to this directory. They are copied after the builtin static files,"
v0.6,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.6,html_static_path = ['_static']
v0.6,"Custom sidebar templates, must be a dictionary that maps document names"
v0.6,to template names.
v0.6,
v0.6,The default sidebars (for documents that don't match any pattern) are
v0.6,defined by theme itself.  Builtin themes are using these templates by
v0.6,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.6,'searchbox.html']``.
v0.6,
v0.6,html_sidebars = {}
v0.6,-- Options for HTMLHelp output ---------------------------------------------
v0.6,Output file base name for HTML help builder.
v0.6,-- Options for LaTeX output ------------------------------------------------
v0.6,The paper size ('letterpaper' or 'a4paper').
v0.6,
v0.6,"'papersize': 'letterpaper',"
v0.6,"The font size ('10pt', '11pt' or '12pt')."
v0.6,
v0.6,"'pointsize': '10pt',"
v0.6,Additional stuff for the LaTeX preamble.
v0.6,
v0.6,"'preamble': '',"
v0.6,Latex figure (float) alignment
v0.6,
v0.6,"'figure_align': 'htbp',"
v0.6,Grouping the document tree into LaTeX files. List of tuples
v0.6,"(source start file, target name, title,"
v0.6,"author, documentclass [howto, manual, or own class])."
v0.6,-- Options for manual page output ------------------------------------------
v0.6,One entry per manual page. List of tuples
v0.6,"(source start file, name, description, authors, manual section)."
v0.6,-- Options for Texinfo output ----------------------------------------------
v0.6,Grouping the document tree into Texinfo files. List of tuples
v0.6,"(source start file, target name, title, author,"
v0.6,"dir menu entry, description, category)"
v0.6,-- Options for Epub output -------------------------------------------------
v0.6,Bibliographic Dublin Core info.
v0.6,The unique identifier of the text. This can be a ISBN number
v0.6,or the project homepage.
v0.6,
v0.6,epub_identifier = ''
v0.6,A unique identification for the text.
v0.6,
v0.6,epub_uid = ''
v0.6,A list of files that should not be packed into the epub file.
v0.6,-- Extension configuration -------------------------------------------------
v0.6,-- Options for intersphinx extension ---------------------------------------
v0.6,Example configuration for intersphinx: refer to the Python standard library.
v0.6,-- Options for todo extension ----------------------------------------------
v0.6,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.6,-- Options for doctest extension -------------------------------------------
v0.6,we can document otherwise excluded entities here by returning False
v0.6,or skip otherwise included entities by returning True
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Calculate residuals
v0.6,Estimate E[T_res | Z_res]
v0.6,TODO. Deal with multi-class instrument
v0.6,Calculate nuisances
v0.6,Estimate E[T_res | Z_res]
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"We do a three way split, as typically a preliminary theta estimator would require"
v0.6,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.6,TODO. Deal with multi-class instrument
v0.6,Estimate final model of theta(X) by minimizing the square loss:
v0.6,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.6,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.6,at the expense of some small bias. For points with very small covariance we revert
v0.6,to the model-based preliminary estimate and do not add the correction term.
v0.6,Estimate preliminary theta in cross fitting manner
v0.6,Estimate p(X) = E[T | X] in cross fitting manner
v0.6,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.6,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.6,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.6,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.6,TODO. The solution below is not really a valid cross-fitting
v0.6,as the test data are used to create the proj_t on the train
v0.6,which in the second train-test loop is used to create the nuisance
v0.6,cov on the test data. Hence the T variable of some sample
v0.6,"is implicitly correlated with its cov nuisance, through this flow"
v0.6,"of information. However, this seems a rather weak correlation."
v0.6,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.6,model.
v0.6,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.6,Estimate preliminary theta in cross fitting manner
v0.6,Estimate p(X) = E[T | X] in cross fitting manner
v0.6,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.6,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.6,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.6,#############################################################################
v0.6,Classes for the DRIV implementation for the special case of intent-to-treat
v0.6,A/B test
v0.6,#############################################################################
v0.6,Estimate preliminary theta in cross fitting manner
v0.6,Estimate p(X) = E[T | X] in cross fitting manner
v0.6,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.6,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.6,We can use statsmodel for all hypothesis testing capabilities
v0.6,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.6,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.6,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.6,model_T_XZ = lambda: model_clf()
v0.6,#'days_visited': lambda:
v0.6,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.6,Turn strings into categories for numeric mapping
v0.6,### Defining some generic regressors and classifiers
v0.6,This a generic non-parametric regressor
v0.6,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.6,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.6,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.6,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.6,model = lambda: RandomForestRegressor(n_estimators=100)
v0.6,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.6,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.6,model = lambda: LinearRegression(n_jobs=-1)
v0.6,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.6,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.6,underlying model whenever predict is called.
v0.6,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.6,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.6,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.6,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.6,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.6,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.6,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.6,We need to specify models to be used for each of these residualizations
v0.6,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.6,"E[T | X, Z]"
v0.6,E[TZ | X]
v0.6,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.6,n_splits determines the number of splits to be used for cross-fitting.
v0.6,# Algorithm 2 - Current Method
v0.6,In[121]:
v0.6,# Algorithm 3 - DRIV ATE
v0.6,dmliv_model_effect = lambda: model()
v0.6,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.6,"dmliv_model_effect(),"
v0.6,n_splits=1)
v0.6,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.6,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.6,"Once multiple treatments are supported, we'll need to fix this"
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.6,We can use statsmodel for all hypothesis testing capabilities
v0.6,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.6,We can use statsmodel for all hypothesis testing capabilities
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,TODO. Deal with multi-class instrument/treatment
v0.6,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.6,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.6,Estimate p(X) = E[T | X] in cross-fitting manner
v0.6,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.6,##################
v0.6,Global settings #
v0.6,##################
v0.6,Global plotting controls
v0.6,"Control for support size, can control for more"
v0.6,#################
v0.6,File utilities #
v0.6,#################
v0.6,#################
v0.6,Plotting utils #
v0.6,#################
v0.6,bias
v0.6,var
v0.6,rmse
v0.6,r2
v0.6,Infer feature dimension
v0.6,Metrics by support plots
v0.6,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.6,Vasilis Syrgkanis <vasy@microsoft.com>
v0.6,Steven Wu <zhiww@microsoft.com>
v0.6,Initialize causal tree parameters
v0.6,Create splits of causal tree
v0.6,Estimate treatment effects at the leafs
v0.6,Compute heterogeneous treatement effect for x's in x_list by finding
v0.6,the corresponding split and associating the effect computed on that leaf
v0.6,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.6,Safety check
v0.6,Weighted linear regression
v0.6,Calculates weights
v0.6,Bootstraping has repetitions in tree sample so we need to iterate
v0.6,over all indices
v0.6,Similar for `a` weights
v0.6,Doesn't have sample weights
v0.6,Is a linear model
v0.6,Weighted linear regression
v0.6,Calculates weights
v0.6,Bootstraping has repetitions in tree sample so we need to iterate
v0.6,over all indices
v0.6,Similar for `a` weights
v0.6,normalize weights
v0.6,"Split the data in half, train and test"
v0.6,Fit with LassoCV the treatment as a function of W and the outcome as
v0.6,"a function of W, using only the train fold"
v0.6,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.6,"Split the data in half, train and test"
v0.6,Fit with LassoCV the treatment as a function of W and the outcome as
v0.6,"a function of W, using only the train fold"
v0.6,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.6,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.6,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.6,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.6,"Split the data in half, train and test"
v0.6,Fit with LassoCV the treatment as a function of x and the outcome as
v0.6,"a function of x, using only the train fold"
v0.6,Then compute residuals p-g(x) and q-q(x) on test fold
v0.6,Compute coefficient by OLS on residuals
v0.6,"Split the data in half, train and test"
v0.6,Fit with LassoCV the treatment as a function of x and the outcome as
v0.6,"a function of x, using only the train fold"
v0.6,Then compute residuals p-g(x) and q-q(x) on test fold
v0.6,Estimate multipliers for second order orthogonal method
v0.6,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.6,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.6,Create local sample set
v0.6,compute the base estimate for the current node using double ml or second order double ml
v0.6,compute the influence functions here that are used for the criterion
v0.6,generate random proposals of dimensions to split
v0.6,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.6,compute criterion for each proposal
v0.6,if splitting creates valid leafs in terms of mean leaf size
v0.6,Calculate criterion for split
v0.6,Else set criterion to infinity so that this split is not chosen
v0.6,If no good split was found
v0.6,Find split that minimizes criterion
v0.6,Set the split attributes at the node
v0.6,Create child nodes with corresponding subsamples
v0.6,Recursively split children
v0.6,Return parent node
v0.6,estimate the local parameter at the leaf using the estimate data
v0.6,###################
v0.6,Argument parsing #
v0.6,###################
v0.6,#########################################
v0.6,Parameters constant across experiments #
v0.6,#########################################
v0.6,Outcome support
v0.6,Treatment support
v0.6,Evaluation grid
v0.6,Treatment effects array
v0.6,Other variables
v0.6,##########################
v0.6,Data Generating Process #
v0.6,##########################
v0.6,Log iteration
v0.6,"Generate controls, features, treatment and outcome"
v0.6,T and Y residuals to be used in later scripts
v0.6,Save generated dataset
v0.6,#################
v0.6,ORF parameters #
v0.6,#################
v0.6,######################################
v0.6,Train and evaluate treatment effect #
v0.6,######################################
v0.6,########
v0.6,Plots #
v0.6,########
v0.6,###############
v0.6,Save results #
v0.6,###############
v0.6,##############
v0.6,Run Rscript #
v0.6,##############
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Check inputs
v0.6,Check inputs
v0.6,Check inputs
v0.6,Check inputs
v0.6,Check inputs
v0.6,Estimate response function
v0.6,Check inputs
v0.6,Train model on controls. Assign higher weight to units resembling
v0.6,treated units.
v0.6,Train model on the treated. Assign higher weight to units resembling
v0.6,control units.
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Create splits of causal tree
v0.6,Make sure the correct exception is being rethrown
v0.6,Must make sure indices are merged correctly
v0.6,Require group assignment t to be one-hot-encoded
v0.6,Define an inner function that iterates over group predictions
v0.6,Convert rows to columns
v0.6,Get predictions for the 2 splits
v0.6,Must make sure indices are merged correctly
v0.6,Estimators
v0.6,OrthoForest parameters
v0.6,Sub-forests
v0.6,Fit check
v0.6,TODO: Check performance
v0.6,Must normalize weights
v0.6,Crossfitting
v0.6,Compute weighted nuisance estimates
v0.6,Generate subsample indices
v0.6,Safety check
v0.6,Build trees in parallel
v0.6,Calculates weights
v0.6,Bootstraping has repetitions in tree sample
v0.6,Similar for `a` weights
v0.6,Bootstraping has repetitions in tree sample
v0.6,Copy and/or define models
v0.6,Define nuisance estimators
v0.6,Define parameter estimators
v0.6,Define
v0.6,Nuissance estimates evaluated with cross-fitting
v0.6,Define 2-fold iterator
v0.6,need safe=False when cloning for WeightedModelWrapper
v0.6,Compute residuals
v0.6,Compute coefficient by OLS on residuals
v0.6,"Parameter returned by LinearRegression is (d_T, )"
v0.6,Compute residuals
v0.6,Compute coefficient by OLS on residuals
v0.6,ell_2 regularization
v0.6,Ridge regression estimate
v0.6,"Parameter returned by LinearRegression is (d_T, )"
v0.6,Return moments and gradients
v0.6,Compute residuals
v0.6,Compute moments
v0.6,"Moments shape is (n, d_T)"
v0.6,Compute moment gradients
v0.6,Copy and/or define models
v0.6,Nuisance estimators shall be defined during fitting because they need to know the number of distinct
v0.6,treatments
v0.6,Define parameter estimators
v0.6,Define moment and mean gradient estimator
v0.6,Define autoencoder
v0.6,"Check that T is shape (n, )"
v0.6,Check T is numeric
v0.6,Train label encoder
v0.6,Define number of classes
v0.6,Call `fit` from parent class
v0.6,"Test that T contains all treatments. If not, return None"
v0.6,Nuissance estimates evaluated with cross-fitting
v0.6,Define 2-fold iterator
v0.6,Check if there is only one example of some class
v0.6,No need to crossfit for internal nodes
v0.6,Compute partial moments
v0.6,"If any of the values in the parameter estimate is nan, return None"
v0.6,Compute partial moments
v0.6,Compute coefficient by OLS on residuals
v0.6,ell_2 regularization
v0.6,Ridge regression estimate
v0.6,"Parameter returned by LinearRegression is (d_T, )"
v0.6,Return moments and gradients
v0.6,Compute partial moments
v0.6,Compute moments
v0.6,"Moments shape is (n, d_T-1)"
v0.6,Compute moment gradients
v0.6,Need to calculate this in an elegant way for when propensity is 0
v0.6,This will flatten T
v0.6,Check that T is numeric
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"if both X and W are None, just return a column of ones"
v0.6,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.6,We need to go back to the label representation of the one-hot so as to call
v0.6,the classifier.
v0.6,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.6,This works both with our without the weighting trick as the treatments T are unit vector
v0.6,treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
v0.6,A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
v0.6,both Parametric and Non Parametric DML.
v0.6,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.6,for internal use by the library
v0.6,NOTE This is used by the inference methods and is more for internal use to the library
v0.6,"TODO: consider whether we need more care around stateful featurizers,"
v0.6,since we clone it and fit separate copies
v0.6,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.6,TODO: support sample_var
v0.6,"TODO: consider whether we need more care around stateful featurizers,"
v0.6,since we clone it and fit separate copies
v0.6,add statsmodels to parent's options
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,#######################################
v0.6,Core DML Tests
v0.6,#######################################
v0.6,How many samples
v0.6,How many control features
v0.6,How many treatment variables
v0.6,Coefficients of how controls affect treatments
v0.6,Coefficients of how controls affect outcome
v0.6,Treatment effects that we want to estimate
v0.6,Run dml estimation
v0.6,How many samples
v0.6,How many control features
v0.6,How many treatment variables
v0.6,Coefficients of how controls affect treatments
v0.6,Coefficients of how controls affect outcome
v0.6,Treatment effects that we want to estimate
v0.6,Run dml estimation
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.6,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"this will have dimension (d,) + shape(X)"
v0.6,send the first dimension to the end
v0.6,columns are featurized independently; partial derivatives are only non-zero
v0.6,when taken with respect to the same column each time
v0.6,don't fit intercept; manually add column of ones to the data instead;
v0.6,this allows us to ignore the intercept when computing marginal effects
v0.6,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.6,store number of columns of T so that we can pass scalars to effect
v0.6,two stage approximation
v0.6,"first, get basis expansions of T, X, and Z"
v0.6,"regress T expansion on X,Z expansions concatenated with W"
v0.6,"predict ft_T from interacted ft_X, ft_Z"
v0.6,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.6,dT may be only 2-dimensional)
v0.6,promote dT to 3D if necessary (e.g. if T was a vector)
v0.6,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"once the estimator has been fit, it's kosher to store d_t here"
v0.6,(which needs to have been expanded if there's a discrete treatment)
v0.6,We can write effect interval as a function of const_marginal_effect_interval for a single treatment
v0.6,once the estimator has been fit
v0.6,We can write effect interval as a function of predict_interval of the final method for linear models
v0.6,"once the estimator has been fit, it's kosher to store d_t here"
v0.6,(which needs to have been expanded if there's a discrete treatment)
v0.6,need to set the fit args before the estimator is fit
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"TODO Allow for non-vector y, i.e. of shape (n, 1)"
v0.6,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.6,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.6,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.6,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.6,checks that X is 2D array.
v0.6,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.6,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.6,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.6,"Replacing this method which is invalid for this class, so that we make the"
v0.6,dosctring empty and not appear in the docs.
v0.6,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.6,TODO: support sample_var
v0.6,"Replacing this method which is invalid for this class, so that we make the"
v0.6,dosctring empty and not appear in the docs.
v0.6,add statsmodels to parent's options
v0.6,Replacing to remove docstring
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,TODO: make sure to use random seeds wherever necessary
v0.6,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.6,"unfortunately with the Theano and Tensorflow backends,"
v0.6,the straightforward use of K.stop_gradient can cause an error
v0.6,because the parameters of the intermediate layers are now disconnected from the loss;
v0.6,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.6,so that those layers remain connected but with 0 gradient
v0.6,|| t - mu_i || ^2
v0.6,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.6,Use logsumexp for numeric stability:
v0.6,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.6,TODO: does the numeric stability actually make any difference?
v0.6,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.6,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.6,generate cumulative sum via matrix multiplication
v0.6,"Generate standard uniform values in shape (batch_size,1)"
v0.6,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.6,we use uniform_like instead with an input of an appropriate shape)
v0.6,convert to floats and multiply to perform equivalent of logical AND
v0.6,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.6,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.6,we use normal_like instead with an input of an appropriate shape)
v0.6,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.6,prevent gradient from passing through sampling
v0.6,three options: biased or upper-bound loss require a single number of samples;
v0.6,unbiased can take different numbers for the network and its gradient
v0.6,"sample: (() -> Layer, int) -> Layer"
v0.6,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.6,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.6,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.6,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.6,the dimensionality of the output of the network
v0.6,TODO: is there a more robust way to do this?
v0.6,TODO: do we need to give the user more control over other arguments to fit?
v0.6,"subtle point: we need to build a new model each time,"
v0.6,because each model encapsulates its randomness
v0.6,TODO: do we need to give the user more control over other arguments to fit?
v0.6,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.6,not a general tensor (because of how backprop works in every framework)
v0.6,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.6,but this seems annoying...)
v0.6,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.6,TODO: any way to get this to work on batches of arbitrary size?
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,#######################################################
v0.6,Perfect Data DGPs for Testing Correctness of Code
v0.6,#######################################################
v0.6,Generate random control co-variates
v0.6,Create epsilon residual treatments that deterministically sum up to
v0.6,zero
v0.6,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.6,conditional on each co-variate vector is equal to zero
v0.6,We simply subtract the conditional mean from the epsilons
v0.6,Construct treatments as T = X*A + epsilon
v0.6,Construct outcomes as y = X*beta + T*effect
v0.6,Generate random control co-variates
v0.6,Create epsilon residual treatments that deterministically sum up to
v0.6,zero
v0.6,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.6,conditional on each co-variate vector is equal to zero
v0.6,We simply subtract the conditional mean from the epsilons
v0.6,Construct treatments as T = X*A + epsilon
v0.6,Construct outcomes as y = X*beta + T*effect
v0.6,Generate random control co-variates
v0.6,Construct treatments as T = X*A + epsilon
v0.6,Construct outcomes as y = X*beta + T*effect
v0.6,Generate random control co-variates
v0.6,Create epsilon residual treatments
v0.6,Construct treatments as T = X*A + epsilon
v0.6,Construct outcomes as y = X*beta + T*effect + eta
v0.6,Generate random control co-variates
v0.6,Use the same treatment vector for each row
v0.6,Construct outcomes as y = X*beta + T*effect
v0.6,Licensed under the MIT License.
v0.6,"since inference objects can be stateful, we must copy it before fitting;"
v0.6,otherwise this sequence wouldn't work:
v0.6,"est1.fit(..., inference=inf)"
v0.6,"est2.fit(..., inference=inf)"
v0.6,est1.effect_interval(...)
v0.6,because inf now stores state from fitting est2
v0.6,call the wrapped fit method
v0.6,NOTE: we call inference fit *after* calling the main fit method
v0.6,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.6,but tensordot can't be applied to this problem because we don't sum over m
v0.6,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.6,of rows of T was not taken into account
v0.6,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.6,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.6,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.6,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.6,add statsmodels to parent's options
v0.6,add debiasedlasso to parent's options
v0.6,TODO Share some logic with non-discrete version
v0.6,add statsmodels to parent's options
v0.6,add statsmodels to parent's options
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Check if model is sparse enough for this model
v0.6,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.6,TODO: any way to avoid creating a copy if the array was already dense?
v0.6,"the call is necessary if the input was something like a list, though"
v0.6,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.6,so convert to pydata sparse first
v0.6,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.6,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.6,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.6,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.6,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.6,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.6,same number of input definitions as arrays
v0.6,input definitions have same number of dimensions as each array
v0.6,all result indices are unique
v0.6,all result indices must match at least one input index
v0.6,"map indices to all array, axis pairs for that index"
v0.6,each index has the same cardinality wherever it appears
v0.6,"State: list of (set of letters, list of (corresponding indices, value))"
v0.6,Algo: while list contains more than one entry
v0.6,take two entries
v0.6,sort both lists by intersection of their indices
v0.6,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.6,"take the union of indices and the product of values), stepping through each list linearly"
v0.6,TODO: might be faster to break into connected components first
v0.6,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.6,"so compute their content separately, then take cartesian product"
v0.6,this would save a few pointless sorts by empty tuples
v0.6,TODO: Consider investigating other performance ideas for these cases
v0.6,where the dense method beat the sparse method (usually sparse is faster)
v0.6,"e,facd,c->cfed"
v0.6,sparse: 0.0335489
v0.6,dense:  0.011465999999999997
v0.6,"gbd,da,egb->da"
v0.6,sparse: 0.0791625
v0.6,dense:  0.007319099999999995
v0.6,"dcc,d,faedb,c->abe"
v0.6,sparse: 1.2868097
v0.6,dense:  0.44605229999999985
v0.6,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.6,TODO: would using einsum's paths to optimize the order of merging help?
v0.6,Normalize weights
v0.6,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
v0.6,"However, the alternative is reimplementing a bunch of intricate stuff by hand"
v0.6,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
v0.6,clean way of achieving this
v0.6,make sure we don't accidentally escape anything in the substitution
v0.6,Fetch appropriate color for node
v0.6,"red for negative, green for positive"
v0.6,in multi-target use first target
v0.6,Write node mean CATE
v0.6,Write node std of CATE
v0.6,Write confidence interval information if at leaf node
v0.6,Fetch appropriate color for node
v0.6,"red for negative, green for positive"
v0.6,Write node mean CATE
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.6,use a binary array to get stratified split in case of discrete treatment
v0.6,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.6,drop first column since all columns sum to one
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Estimators
v0.6,Causal tree parameters
v0.6,Tree structure
v0.6,No need for a random split since the data is already
v0.6,a random subsample from the original input
v0.6,node list stores the nodes that are yet to be splitted
v0.6,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.6,Create local sample set
v0.6,Compute nuisance estimates for the current node
v0.6,Nuisance estimate cannot be calculated
v0.6,Estimate parameter for current node
v0.6,Node estimate cannot be calculated
v0.6,Calculate moments and gradient of moments for current data
v0.6,Calculate inverse gradient
v0.6,The gradient matrix is not invertible.
v0.6,No good split can be found
v0.6,Calculate point-wise pseudo-outcomes rho
v0.6,a split is determined by a feature and a sample pair
v0.6,the number of possible splits is at most (number of features) * (number of node samples)
v0.6,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.6,parse row and column of random pair
v0.6,the sample of the pair is the integer division of the random number with n_feats
v0.6,calculate the binary indicator of whether sample i is on the left or the right
v0.6,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.6,calculate the number of samples on the left child for each proposed split
v0.6,calculate the analogous binary indicator for the samples in the estimation set
v0.6,calculate the number of estimation samples on the left child of each proposed split
v0.6,find the upper and lower bound on the size of the left split for the split
v0.6,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.6,on each side.
v0.6,similarly for the estimation sample set
v0.6,if there is no valid split then don't create any children
v0.6,filter only the valid splits
v0.6,calculate the average influence vector of the samples in the left child
v0.6,calculate the average influence vector of the samples in the right child
v0.6,take the square of each of the entries of the influence vectors and normalize
v0.6,by size of each child
v0.6,calculate the vector score of each candidate split as the average of left and right
v0.6,influence vectors
v0.6,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.6,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.6,where there might be large discontinuities in some parameter as the conditioning set varies
v0.6,calculate the scalar score of each split by aggregating across the vector of scores
v0.6,Find split that minimizes criterion
v0.6,Create child nodes with corresponding subsamples
v0.6,add the created children to the list of not yet split nodes
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,TODO: Add a __dir__ implementation?
v0.6,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.6,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.6,then we should be computing a confidence interval for the wrapped calls
v0.6,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.6,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.6,"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
v0.6,Remove children with nonwhite mothers from the treatment group
v0.6,Remove children with nonwhite mothers from the treatment group
v0.6,Select columns
v0.6,Scale the numeric variables
v0.6,"Change the binary variable 'first' takes values in {1,2}"
v0.6,Append a column of ones as intercept
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.6,creating notebooks that are annoying for our users to actually run themselves
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
v0.6,"prior to calling interpret, can't plot, render, etc."
v0.6,can interpret without uncertainty
v0.6,can't interpret with uncertainty if inference wasn't used during fit
v0.6,can interpret with uncertainty if we refit
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,simple DGP only for illustration
v0.6,Define the treatment model neural network architecture
v0.6,"This will take the concatenation of one-dimensional values z and x as input,"
v0.6,"so the input shape is (d_z + d_x,)"
v0.6,The exact shape of the final layer is not critical because the Deep IV framework will
v0.6,add extra layers on top for the mixture density network
v0.6,Define the response model neural network architecture
v0.6,"This will take the concatenation of one-dimensional values t and x as input,"
v0.6,"so the input shape is (d_t + d_x,)"
v0.6,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.6,"NOTE: For the response model, it is important to define the model *outside*"
v0.6,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.6,so that the same weights will be reused in each instantiation
v0.6,number of samples to use in second estimate of the response
v0.6,(to make loss estimate unbiased)
v0.6,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.6,do something with predictions...
v0.6,also test vector t and y
v0.6,simple DGP only for illustration
v0.6,Define the treatment model neural network architecture
v0.6,"This will take the concatenation of one-dimensional values z and x as input,"
v0.6,"so the input shape is (d_z + d_x,)"
v0.6,The exact shape of the final layer is not critical because the Deep IV framework will
v0.6,add extra layers on top for the mixture density network
v0.6,Define the response model neural network architecture
v0.6,"This will take the concatenation of one-dimensional values t and x as input,"
v0.6,"so the input shape is (d_t + d_x,)"
v0.6,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.6,"NOTE: For the response model, it is important to define the model *outside*"
v0.6,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.6,so that the same weights will be reused in each instantiation
v0.6,number of samples to use in second estimate of the response
v0.6,(to make loss estimate unbiased)
v0.6,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.6,do something with predictions...
v0.6,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.6,test = True ensures we draw test set images
v0.6,test = True ensures we draw test set images
v0.6,re-draw to get new independent treatment and implied response
v0.6,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.6,above is necesary so that reduced form doesn't win
v0.6,covariates: time and emotion
v0.6,random instrument
v0.6,z -> price
v0.6,true observable demand function
v0.6,errors
v0.6,response
v0.6,test = True ensures we draw test set images
v0.6,test = True ensures we draw test set images
v0.6,re-draw to get new independent treatment and implied response
v0.6,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.6,above is necesary so that reduced form doesn't win
v0.6,covariates: time and emotion
v0.6,random instrument
v0.6,z -> price
v0.6,true observable demand function
v0.6,errors
v0.6,response
v0.6,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.6,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.6,For some reason this doesn't work at all when run against the CNTK backend...
v0.6,"model.compile('nadam', loss=lambda _,l:l)"
v0.6,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.6,generate a valiation set
v0.6,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.6,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.6,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.6,so we need to transpose the result
v0.6,1-d output
v0.6,2-d output
v0.6,Single dimensional output y
v0.6,Multi-dimensional output y
v0.6,1-d y
v0.6,multi-d y
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,test that we can fit with the same arguments as the base estimator
v0.6,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6,with the same shape for the lower and upper bounds
v0.6,test that the lower and upper bounds differ
v0.6,test that we can do the same thing once we provide percentile bounds
v0.6,test that the lower and upper bounds differ
v0.6,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6,with the same shape for the lower and upper bounds
v0.6,test that the lower and upper bounds differ
v0.6,test that we can do the same thing once we provide percentile bounds
v0.6,test that the lower and upper bounds differ
v0.6,test that we can fit with the same arguments as the base estimator
v0.6,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6,with the same shape for the lower and upper bounds
v0.6,test that the lower and upper bounds differ
v0.6,test that we can do the same thing once we provide percentile bounds
v0.6,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6,with the same shape for the lower and upper bounds
v0.6,test that the lower and upper bounds differ
v0.6,test that we can do the same thing once we provide percentile bounds
v0.6,test that the lower and upper bounds differ
v0.6,test that we can fit with the same arguments as the base estimator
v0.6,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6,with the same shape for the lower and upper bounds
v0.6,test that the lower and upper bounds differ
v0.6,test that we can do the same thing once we provide percentile bounds
v0.6,test that the lower and upper bounds differ
v0.6,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6,with the same shape for the lower and upper bounds
v0.6,test that the lower and upper bounds differ
v0.6,test that we can do the same thing once we provide percentile bounds
v0.6,test that the lower and upper bounds differ
v0.6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6,with the same shape for the lower and upper bounds
v0.6,test that the lower and upper bounds differ
v0.6,test that the estimated effect is usually within the bounds
v0.6,test that we can do the same thing once we provide alpha explicitly
v0.6,test that the lower and upper bounds differ
v0.6,test that the estimated effect is usually within the bounds
v0.6,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.6,with the same shape for the lower and upper bounds
v0.6,test that the lower and upper bounds differ
v0.6,TODO: test that the estimated effect is usually within the bounds
v0.6,and that the true effect is also usually within the bounds
v0.6,test that we can do the same thing once we provide percentile bounds
v0.6,test that the lower and upper bounds differ
v0.6,TODO: test that the estimated effect is usually within the bounds
v0.6,and that the true effect is also usually within the bounds
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Test non keyword based calls to fit
v0.6,Test custom splitter
v0.6,Test incomplete set of test folds
v0.6,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,make sure cross product varies more slowly with first array
v0.6,and that vectors are okay as inputs
v0.6,number of inputs in specification must match number of inputs
v0.6,must have an output
v0.6,output indices must be unique
v0.6,output indices must be present in an input
v0.6,number of indices must match number of dimensions for each input
v0.6,repeated indices must always have consistent sizes
v0.6,transpose
v0.6,tensordot
v0.6,trace
v0.6,TODO: set up proper flag for this
v0.6,pick indices at random with replacement from the first 7 letters of the alphabet
v0.6,"of all of the distinct indices that appear in any input,"
v0.6,pick a random subset of them (of size at most 5) to appear in the output
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Preprocess data
v0.6,Convert 'week' to a date
v0.6,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.6,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.6,Take log of price
v0.6,Make brand numeric
v0.6,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.6,which have all zero coeffs
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.6,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.6,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
v0.6,this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
v0.6,The __warningregistry__'s need to be in a pristine state for tests
v0.6,to work properly.
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Set random seed
v0.6,Generate data
v0.6,DGP constants
v0.6,Test data
v0.6,Constant treatment effect
v0.6,Constant treatment with multi output Y
v0.6,Heterogeneous treatment
v0.6,Heterogeneous treatment with multi output Y
v0.6,TLearner test
v0.6,Instantiate TLearner
v0.6,Test inputs
v0.6,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.6,Instantiate SLearner
v0.6,Test inputs
v0.6,Test constant treatment effect
v0.6,Test constant treatment effect with multi output Y
v0.6,Test heterogeneous treatment effect
v0.6,Need interactions between T and features
v0.6,Test heterogeneous treatment effect with multi output Y
v0.6,Instantiate XLearner
v0.6,Test inputs
v0.6,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.6,Instantiate DomainAdaptationLearner
v0.6,Test inputs
v0.6,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.6,Get the true treatment effect
v0.6,Get the true treatment effect
v0.6,Fit learner and get the effect and marginal effect
v0.6,Compute treatment effect residuals (absolute)
v0.6,Check that at least 90% of predictions are within tolerance interval
v0.6,Check whether the output shape is right
v0.6,Check that one can pass in regular lists
v0.6,Check that it fails correctly if lists of different shape are passed in
v0.6,"Check that it works when T, Y have shape (n, 1)"
v0.6,Generate covariates
v0.6,Generate treatment
v0.6,Calculate outcome
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,DGP constants
v0.6,Generate data
v0.6,Test data
v0.6,Remove warnings that might be raised by the models passed into the ORF
v0.6,Generate data with continuous treatments
v0.6,Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
v0.6,does not work well with parallelism.
v0.6,Test inputs for continuous treatments
v0.6,--> Check that one can pass in regular lists
v0.6,--> Check that it fails correctly if lists of different shape are passed in
v0.6,Check that outputs have the correct shape
v0.6,Test continuous treatments with controls
v0.6,Test continuous treatments without controls
v0.6,Generate data with binary treatments
v0.6,Instantiate model with default params. Using n_jobs=1 since code coverage
v0.6,does not work well with parallelism.
v0.6,Test inputs for binary treatments
v0.6,--> Check that one can pass in regular lists
v0.6,--> Check that it fails correctly if lists of different shape are passed in
v0.6,"--> Check that it works when T, Y have shape (n, 1)"
v0.6,"--> Check that it fails correctly when T has shape (n, 2)"
v0.6,--> Check that it fails correctly when the treatments are not numeric
v0.6,Check that outputs have the correct shape
v0.6,Test binary treatments with controls
v0.6,Test binary treatments without controls
v0.6,Only applicable to continuous treatments
v0.6,Generate data for 2 treatments
v0.6,Test multiple treatments with controls
v0.6,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.6,The rest for controls. Just as an example.
v0.6,Generating A/B test data
v0.6,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.6,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.6,Generate data with continuous treatments
v0.6,Instantiate model with most of the default parameters
v0.6,Compute the treatment effect on test points
v0.6,Compute treatment effect residuals
v0.6,Multiple treatments
v0.6,Allow at most 10% test points to be outside of the tolerance interval
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.6,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.6,ensure that we've got at least two of every element
v0.6,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.6,TODO: add stratification to bootstrap so that we can use it
v0.6,even with discrete treatments
v0.6,make sure we can call the marginal_effect and effect methods
v0.6,"make sure we can call effect with implied scalar treatments,"
v0.6,"no matter the dimensions of T, and also that we warn when there"
v0.6,are multiple treatments
v0.6,"ExitStack can be used as a ""do nothing"" ContextManager"
v0.6,ensure that we've got at least two of every element
v0.6,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.6,"TODO Add bootstrap inference, once discrete treatment issue is fixed"
v0.6,make sure we can call the marginal_effect and effect methods
v0.6,"make sure we can call effect with implied scalar treatments, no matter the"
v0.6,"dimensions of T, and also that we warn when there are multiple treatments"
v0.6,We concatenate the two copies data
v0.6,create a simple artificial setup where effect of moving from treatment
v0.6,"1 -> 2 is 2,"
v0.6,"1 -> 3 is 1, and"
v0.6,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.6,"Using an uneven number of examples from different classes,"
v0.6,"and having the treatments in non-lexicographic order,"
v0.6,Should rule out some basic issues.
v0.6,test that we can fit with a KFold instance
v0.6,test that we can fit with a train/test iterable
v0.6,(incorrectly) use a final model with an intercept
v0.6,"Because final model is fixed, actual values of T and Y don't matter"
v0.6,Ensure reproducibility
v0.6,Sparse DGP
v0.6,Treatment effect coef
v0.6,Other coefs
v0.6,Features and controls
v0.6,Test sparse estimator
v0.6,"--> test coef_, intercept_"
v0.6,--> test treatment effects
v0.6,Restrict x_test to vectors of norm < 1
v0.6,--> check inference
v0.6,Check that a majority of true effects lie in the 5-95% CI
v0.6,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.6,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.6,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.6,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.6,sparse test case: heterogeneous effect by product
v0.6,need at least as many rows in e_y as there are distinct columns
v0.6,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Set random seed
v0.6,Generate data
v0.6,DGP constants
v0.6,Test data
v0.6,Constant treatment effect and propensity
v0.6,Heterogeneous treatment and propensity
v0.6,ensure that we've got at least two of every element
v0.6,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.6,TODO: add stratification to bootstrap so that we can use it even with discrete treatments
v0.6,make sure we can call the marginal_effect and effect methods
v0.6,"make sure we can call effect with implied scalar treatments, no matter the"
v0.6,"dimensions of T, and also that we warn when there are multiple treatments"
v0.6,create a simple artificial setup where effect of moving from treatment
v0.6,"1 -> 2 is 2,"
v0.6,"1 -> 3 is 1, and"
v0.6,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.6,"Using an uneven number of examples from different classes,"
v0.6,"and having the treatments in non-lexicographic order,"
v0.6,Should rule out some basic issues.
v0.6,test that we can fit with a KFold instance
v0.6,test that we can fit with a train/test iterable
v0.6,Test inputs
v0.6,self._test_inputs(DR_learner)
v0.6,Test constant treatment effect
v0.6,Test heterogeneous treatment effect
v0.6,Test heterogenous treatment effect for W =/= None
v0.6,Sparse DGP
v0.6,Treatment effect coef
v0.6,Other coefs
v0.6,Features and controls
v0.6,Test sparse estimator
v0.6,"--> test coef_, intercept_"
v0.6,--> test treatment effects
v0.6,Restrict x_test to vectors of norm < 1
v0.6,--> check inference
v0.6,Check that a majority of true effects lie in the 5-95% CI
v0.6,Fit learner and get the effect
v0.6,Get the true treatment effect
v0.6,Compute treatment effect residuals (absolute)
v0.6,Check that at least 90% of predictions are within tolerance interval
v0.6,Only for heterogeneous TE
v0.6,Fit learner on X and W and get the effect
v0.6,Get the true treatment effect
v0.6,Compute treatment effect residuals (absolute)
v0.6,Check that at least 90% of predictions are within tolerance interval
v0.6,Check that one can pass in regular lists
v0.6,Check that it fails correctly if lists of different shape are passed in
v0.6,Check that it fails when T contains values other than 0 and 1
v0.6,"Check that it works when T, Y have shape (n, 1)"
v0.6,Generate covariates
v0.6,Generate treatment
v0.6,Calculate outcome
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,DGP constants
v0.6,DGP coefficients
v0.6,Generated outcomes
v0.6,################
v0.6,WeightedLasso #
v0.6,################
v0.6,Define weights
v0.6,Define extended datasets
v0.6,Range of alphas
v0.6,Compare with Lasso
v0.6,--> No intercept
v0.6,--> With intercept
v0.6,When DGP has no intercept
v0.6,When DGP has intercept
v0.6,--> Coerce coefficients to be positive
v0.6,--> Toggle max_iter & tol
v0.6,Define weights
v0.6,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.6,Mixed DGP scenario.
v0.6,Define extended datasets
v0.6,Define weights
v0.6,Define multioutput
v0.6,##################
v0.6,WeightedLassoCV #
v0.6,##################
v0.6,Define alphas to test
v0.6,Compare with LassoCV
v0.6,--> No intercept
v0.6,--> With intercept
v0.6,--> Force parameters to be positive
v0.6,Choose a smaller n to speed-up process
v0.6,Compare fold weights
v0.6,Define weights
v0.6,Define extended datasets
v0.6,Define splitters
v0.6,WeightedKFold splitter
v0.6,Map weighted splitter to an extended splitter
v0.6,Define alphas to test
v0.6,Compare with LassoCV
v0.6,--> No intercept
v0.6,--> With intercept
v0.6,--> Force parameters to be positive
v0.6,###########################
v0.6,MultiTaskWeightedLassoCV #
v0.6,###########################
v0.6,Define alphas to test
v0.6,Define splitter
v0.6,Compare with MultiTaskLassoCV
v0.6,--> No intercept
v0.6,--> With intercept
v0.6,Define weights
v0.6,Define extended datasets
v0.6,Define splitters
v0.6,WeightedKFold splitter
v0.6,Map weighted splitter to an extended splitter
v0.6,Define alphas to test
v0.6,Compare with LassoCV
v0.6,--> No intercept
v0.6,--> With intercept
v0.6,################
v0.6,DebiasedLasso #
v0.6,################
v0.6,Test DebiasedLasso without weights
v0.6,--> Check debiased coeffcients without intercept
v0.6,--> Check debiased coeffcients with intercept
v0.6,--> Check 5-95 CI coverage for unit vectors
v0.6,Test DebiasedLasso with weights for one DGP
v0.6,Define weights
v0.6,Define extended datasets
v0.6,--> Check debiased coefficients
v0.6,Define weights
v0.6,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.6,--> Check debiased coeffcients
v0.6,Test that attributes propagate correctly
v0.6,Test MultiOutputDebiasedLasso without weights
v0.6,--> Check debiased coeffcients without intercept
v0.6,--> Check debiased coeffcients with intercept
v0.6,--> Check CI coverage
v0.6,Test MultiOutputDebiasedLasso with weights
v0.6,Define weights
v0.6,Define extended datasets
v0.6,--> Check debiased coefficients
v0.6,Unit vectors
v0.6,Unit vectors
v0.6,Check coeffcients and intercept are the same within tolerance
v0.6,Check results are similar with tolerance 1e-6
v0.6,Check if multitask
v0.6,Check that same alpha is chosen
v0.6,Check that the coefficients are similar
v0.6,selective ridge has a simple implementation that we can test against
v0.6,see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
v0.6,"it should be the case that when we set fit_intercept to true,"
v0.6,it doesn't matter whether the penalized model also fits an intercept or not
v0.6,create an extra copy of rows with weight 2
v0.6,"instead of a slice, explicitly return an array of indices"
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,"Found a good split, return."
v0.6,Record all splits in case the stratification by weight yeilds a worse partition
v0.6,Reseed random generator and try again
v0.6,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.6,"Found a good split, return."
v0.6,Did not find a good split
v0.6,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.6,Return most weight-balanced partition
v0.6,Weight stratification algorithm
v0.6,Sort weights for weight strata search
v0.6,There are some leftover indices that have yet to be assigned
v0.6,Append stratum splits to overall splits
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Our classes that derive from sklearn ones sometimes include
v0.6,inherited docstrings that have embedded doctests; we need the following imports
v0.6,so that they don't break.
v0.6,"Convert X, y into numpy arrays"
v0.6,Define fit parameters
v0.6,Some algorithms don't have a check_input option
v0.6,Check weights array
v0.6,Check that weights are size-compatible
v0.6,Normalize inputs
v0.6,Weight inputs
v0.6,Fit base class without intercept
v0.6,Fit Lasso
v0.6,Reset intercept
v0.6,The intercept is not calculated properly due the sqrt(weights) factor
v0.6,so it must be recomputed
v0.6,Fit lasso without weights
v0.6,Make weighted splitter
v0.6,Fit weighted model
v0.6,Make weighted splitter
v0.6,Fit weighted model
v0.6,Select optimal penalty
v0.6,Warn about consistency
v0.6,"Convert X, y into numpy arrays"
v0.6,Fit weighted lasso with user input
v0.6,"Center X, y"
v0.6,Calculate quantities that will be used later on. Account for centered data
v0.6,Calculate coefficient and error variance
v0.6,Add coefficient correction
v0.6,Set coefficients and intercept standard errors
v0.6,Set intercept
v0.6,Return alpha to 'auto' state
v0.6,"Note that in the case of no intercept, X_offset is 0"
v0.6,Calculate the variance of the predictions
v0.6,Calculate prediction confidence intervals
v0.6,Assumes flattened y
v0.6,Compute weighted residuals
v0.6,To be done once per target. Assumes y can be flattened.
v0.6,Assumes that X has already been offset
v0.6,Special case: n_features=1
v0.6,Compute Lasso coefficients for the columns of the design matrix
v0.6,Call weighted lasso on reduced design matrix
v0.6,Inherit some parameters from the parent
v0.6,Weighted tau
v0.6,Compute C_hat
v0.6,Compute theta_hat
v0.6,Allow for single output as well
v0.6,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.6,Set coef_ attribute
v0.6,Set intercept_ attribute
v0.6,Set selected_alpha_ attribute
v0.6,Set coef_std_err_
v0.6,intercept_std_err_
v0.6,set intercept_ attribute
v0.6,set coef_ attribute
v0.6,set alpha_ attribute
v0.6,set alphas_ attribute
v0.6,set n_iter_ attribute
v0.6,"The unpenalized model can't contain an intercept, because in the analysis above"
v0.6,"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
v0.6,"as (M X) beta + c, so the learned coef and intercept will be wrong"
v0.6,now regress X1 on y - X2 * beta2 to learn beta1
v0.6,set coef_ and intercept_ attributes
v0.6,Note that the penalized model should *not* have an intercept
v0.6,Copyright (c) Microsoft Corporation. All rights reserved.
v0.6,Licensed under the MIT License.
v0.6,Construct the subsample of data
v0.6,Split into estimation and splitting sample set
v0.6,Fit the tree on the splitting sample
v0.6,Set the estimation values based on the estimation split
v0.6,Apply the trained tree on the estimation sample to get the path for every estimation sample
v0.6,Calculate the total weight of estimation samples on each tree node:
v0.6,\sum_i sample_weight[i] * 1{i \\in node}
v0.6,Calculate the total number of estimation samples on each tree node:
v0.6,|node| = \sum_{i} 1{i \\in node}
v0.6,Calculate the weighted sum of responses on the estimation sample on each node:
v0.6,\sum_{i} sample_weight[i] 1{i \\in node} Y_i
v0.6,Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
v0.6,and for each un-pruned tree set the value and the weight appropriately.
v0.6,If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
v0.6,"sample, then prune the whole sub-tree"
v0.6,Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
v0.6,Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
v0.6,Set the count to the estimation split count
v0.6,Validate or convert input data
v0.6,Pre-sort indices to avoid that each individual tree of the
v0.6,ensemble sorts the indices.
v0.6,Remap output
v0.6,reshape is necessary to preserve the data contiguity against vs
v0.6,"[:, np.newaxis] that does not."
v0.6,Check parameters
v0.6,"Free allocated memory, if any"
v0.6,We draw from the random state to get the random state we
v0.6,would have got if we hadn't used a warm_start.
v0.6,Parallel loop: we prefer the threading backend as the Cython code
v0.6,for fitting the trees is internally releasing the Python GIL
v0.6,making threading more efficient than multiprocessing in
v0.6,"that case. However, for joblib 0.12+ we respect any"
v0.6,"parallel_backend contexts set at a higher level,"
v0.6,since correctness does not rely on using threads.
v0.6,TODO. This slicing should ultimately be done inside the parallel function
v0.6,so that we don't need to create a matrix of size roughly n_samples * n_estimators
v0.6,Collect newly grown trees
v0.6,Helper class that accumulates an arbitrary function in parallel on the accumulator acc
v0.6,and calls the function fn on each tree e and returns the mean output. The function fn
v0.6,"should take as input a tree e, and return another function g_e, which takes as input X, check_input"
v0.6,"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
v0.6,index start to index end will be used. The returned result is essentially:
v0.6,(mean over e in slice)(g_e(X)).
v0.6,Check data
v0.6,Assign chunk of trees to jobs
v0.6,Check data
v0.6,Check data
v0.6,avoid storing the output of every estimator by summing them here
v0.6,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
v0.6,"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
v0.6,"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
v0.6,where \theta(X) is the point estimate using the whole forest
v0.6,Calculate the variance of the latter as E[Q(S)^2]
v0.5,configuration is all pulled from setup.cfg
v0.5,-*- coding: utf-8 -*-
v0.5,
v0.5,Configuration file for the Sphinx documentation builder.
v0.5,
v0.5,This file does only contain a selection of the most common options. For a
v0.5,full list see the documentation:
v0.5,http://www.sphinx-doc.org/en/master/config
v0.5,-- Path setup --------------------------------------------------------------
v0.5,"If extensions (or modules to document with autodoc) are in another directory,"
v0.5,add these directories to sys.path here. If the directory is relative to the
v0.5,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.5,
v0.5,-- Project information -----------------------------------------------------
v0.5,-- General configuration ---------------------------------------------------
v0.5,"If your documentation needs a minimal Sphinx version, state it here."
v0.5,
v0.5,needs_sphinx = '1.0'
v0.5,"Add any Sphinx extension module names here, as strings. They can be"
v0.5,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.5,ones.
v0.5,"Add any paths that contain templates here, relative to this directory."
v0.5,The suffix(es) of source filenames.
v0.5,You can specify multiple suffix as a list of string:
v0.5,
v0.5,"source_suffix = ['.rst', '.md']"
v0.5,The master toctree document.
v0.5,The language for content autogenerated by Sphinx. Refer to documentation
v0.5,for a list of supported languages.
v0.5,
v0.5,This is also used if you do content translation via gettext catalogs.
v0.5,"Usually you set ""language"" from the command line for these cases."
v0.5,"List of patterns, relative to source directory, that match files and"
v0.5,directories to ignore when looking for source files.
v0.5,This pattern also affects html_static_path and html_extra_path.
v0.5,The name of the Pygments (syntax highlighting) style to use.
v0.5,-- Options for HTML output -------------------------------------------------
v0.5,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.5,a list of builtin themes.
v0.5,
v0.5,Theme options are theme-specific and customize the look and feel of a theme
v0.5,"further.  For a list of options available for each theme, see the"
v0.5,documentation.
v0.5,
v0.5,"Add any paths that contain custom static files (such as style sheets) here,"
v0.5,"relative to this directory. They are copied after the builtin static files,"
v0.5,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.5,html_static_path = ['_static']
v0.5,"Custom sidebar templates, must be a dictionary that maps document names"
v0.5,to template names.
v0.5,
v0.5,The default sidebars (for documents that don't match any pattern) are
v0.5,defined by theme itself.  Builtin themes are using these templates by
v0.5,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.5,'searchbox.html']``.
v0.5,
v0.5,html_sidebars = {}
v0.5,-- Options for HTMLHelp output ---------------------------------------------
v0.5,Output file base name for HTML help builder.
v0.5,-- Options for LaTeX output ------------------------------------------------
v0.5,The paper size ('letterpaper' or 'a4paper').
v0.5,
v0.5,"'papersize': 'letterpaper',"
v0.5,"The font size ('10pt', '11pt' or '12pt')."
v0.5,
v0.5,"'pointsize': '10pt',"
v0.5,Additional stuff for the LaTeX preamble.
v0.5,
v0.5,"'preamble': '',"
v0.5,Latex figure (float) alignment
v0.5,
v0.5,"'figure_align': 'htbp',"
v0.5,Grouping the document tree into LaTeX files. List of tuples
v0.5,"(source start file, target name, title,"
v0.5,"author, documentclass [howto, manual, or own class])."
v0.5,-- Options for manual page output ------------------------------------------
v0.5,One entry per manual page. List of tuples
v0.5,"(source start file, name, description, authors, manual section)."
v0.5,-- Options for Texinfo output ----------------------------------------------
v0.5,Grouping the document tree into Texinfo files. List of tuples
v0.5,"(source start file, target name, title, author,"
v0.5,"dir menu entry, description, category)"
v0.5,-- Options for Epub output -------------------------------------------------
v0.5,Bibliographic Dublin Core info.
v0.5,The unique identifier of the text. This can be a ISBN number
v0.5,or the project homepage.
v0.5,
v0.5,epub_identifier = ''
v0.5,A unique identification for the text.
v0.5,
v0.5,epub_uid = ''
v0.5,A list of files that should not be packed into the epub file.
v0.5,-- Extension configuration -------------------------------------------------
v0.5,-- Options for intersphinx extension ---------------------------------------
v0.5,Example configuration for intersphinx: refer to the Python standard library.
v0.5,-- Options for todo extension ----------------------------------------------
v0.5,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.5,-- Options for doctest extension -------------------------------------------
v0.5,we can document otherwise excluded entities here by returning False
v0.5,or skip otherwise included entities by returning True
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,Calculate residuals
v0.5,Estimate E[T_res | Z_res]
v0.5,TODO. Deal with multi-class instrument
v0.5,Calculate nuisances
v0.5,Estimate E[T_res | Z_res]
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"We do a three way split, as typically a preliminary theta estimator would require"
v0.5,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.5,TODO. Deal with multi-class instrument
v0.5,Estimate final model of theta(X) by minimizing the square loss:
v0.5,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.5,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.5,at the expense of some small bias. For points with very small covariance we revert
v0.5,to the model-based preliminary estimate and do not add the correction term.
v0.5,Estimate preliminary theta in cross fitting manner
v0.5,Estimate p(X) = E[T | X] in cross fitting manner
v0.5,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.5,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.5,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.5,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.5,TODO. The solution below is not really a valid cross-fitting
v0.5,as the test data are used to create the proj_t on the train
v0.5,which in the second train-test loop is used to create the nuisance
v0.5,cov on the test data. Hence the T variable of some sample
v0.5,"is implicitly correlated with its cov nuisance, through this flow"
v0.5,"of information. However, this seems a rather weak correlation."
v0.5,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.5,model.
v0.5,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.5,Estimate preliminary theta in cross fitting manner
v0.5,Estimate p(X) = E[T | X] in cross fitting manner
v0.5,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.5,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.5,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.5,#############################################################################
v0.5,Classes for the DRIV implementation for the special case of intent-to-treat
v0.5,A/B test
v0.5,#############################################################################
v0.5,Estimate preliminary theta in cross fitting manner
v0.5,Estimate p(X) = E[T | X] in cross fitting manner
v0.5,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.5,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.5,We can use statsmodel for all hypothesis testing capabilities
v0.5,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.5,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.5,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.5,model_T_XZ = lambda: model_clf()
v0.5,#'days_visited': lambda:
v0.5,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.5,Turn strings into categories for numeric mapping
v0.5,### Defining some generic regressors and classifiers
v0.5,This a generic non-parametric regressor
v0.5,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.5,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.5,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.5,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.5,model = lambda: RandomForestRegressor(n_estimators=100)
v0.5,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.5,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.5,model = lambda: LinearRegression(n_jobs=-1)
v0.5,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.5,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.5,underlying model whenever predict is called.
v0.5,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.5,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.5,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.5,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.5,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.5,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.5,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.5,We need to specify models to be used for each of these residualizations
v0.5,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.5,"E[T | X, Z]"
v0.5,E[TZ | X]
v0.5,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.5,n_splits determines the number of splits to be used for cross-fitting.
v0.5,# Algorithm 2 - Current Method
v0.5,In[121]:
v0.5,# Algorithm 3 - DRIV ATE
v0.5,dmliv_model_effect = lambda: model()
v0.5,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.5,"dmliv_model_effect(),"
v0.5,n_splits=1)
v0.5,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.5,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.5,"Once multiple treatments are supported, we'll need to fix this"
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.5,We can use statsmodel for all hypothesis testing capabilities
v0.5,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.5,We can use statsmodel for all hypothesis testing capabilities
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,TODO. Deal with multi-class instrument/treatment
v0.5,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.5,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.5,Estimate p(X) = E[T | X] in cross-fitting manner
v0.5,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.5,##################
v0.5,Global settings #
v0.5,##################
v0.5,Global plotting controls
v0.5,"Control for support size, can control for more"
v0.5,#################
v0.5,File utilities #
v0.5,#################
v0.5,#################
v0.5,Plotting utils #
v0.5,#################
v0.5,bias
v0.5,var
v0.5,rmse
v0.5,r2
v0.5,Infer feature dimension
v0.5,Metrics by support plots
v0.5,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.5,Vasilis Syrgkanis <vasy@microsoft.com>
v0.5,Steven Wu <zhiww@microsoft.com>
v0.5,Initialize causal tree parameters
v0.5,Create splits of causal tree
v0.5,Estimate treatment effects at the leafs
v0.5,Compute heterogeneous treatement effect for x's in x_list by finding
v0.5,the corresponding split and associating the effect computed on that leaf
v0.5,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.5,Safety check
v0.5,Weighted linear regression
v0.5,Calculates weights
v0.5,Bootstraping has repetitions in tree sample so we need to iterate
v0.5,over all indices
v0.5,Similar for `a` weights
v0.5,Doesn't have sample weights
v0.5,Is a linear model
v0.5,Weighted linear regression
v0.5,Calculates weights
v0.5,Bootstraping has repetitions in tree sample so we need to iterate
v0.5,over all indices
v0.5,Similar for `a` weights
v0.5,normalize weights
v0.5,"Split the data in half, train and test"
v0.5,Fit with LassoCV the treatment as a function of W and the outcome as
v0.5,"a function of W, using only the train fold"
v0.5,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.5,"Split the data in half, train and test"
v0.5,Fit with LassoCV the treatment as a function of W and the outcome as
v0.5,"a function of W, using only the train fold"
v0.5,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.5,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.5,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.5,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.5,"Split the data in half, train and test"
v0.5,Fit with LassoCV the treatment as a function of x and the outcome as
v0.5,"a function of x, using only the train fold"
v0.5,Then compute residuals p-g(x) and q-q(x) on test fold
v0.5,Compute coefficient by OLS on residuals
v0.5,"Split the data in half, train and test"
v0.5,Fit with LassoCV the treatment as a function of x and the outcome as
v0.5,"a function of x, using only the train fold"
v0.5,Then compute residuals p-g(x) and q-q(x) on test fold
v0.5,Estimate multipliers for second order orthogonal method
v0.5,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.5,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.5,Create local sample set
v0.5,compute the base estimate for the current node using double ml or second order double ml
v0.5,compute the influence functions here that are used for the criterion
v0.5,generate random proposals of dimensions to split
v0.5,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.5,compute criterion for each proposal
v0.5,if splitting creates valid leafs in terms of mean leaf size
v0.5,Calculate criterion for split
v0.5,Else set criterion to infinity so that this split is not chosen
v0.5,If no good split was found
v0.5,Find split that minimizes criterion
v0.5,Set the split attributes at the node
v0.5,Create child nodes with corresponding subsamples
v0.5,Recursively split children
v0.5,Return parent node
v0.5,estimate the local parameter at the leaf using the estimate data
v0.5,###################
v0.5,Argument parsing #
v0.5,###################
v0.5,#########################################
v0.5,Parameters constant across experiments #
v0.5,#########################################
v0.5,Outcome support
v0.5,Treatment support
v0.5,Evaluation grid
v0.5,Treatment effects array
v0.5,Other variables
v0.5,##########################
v0.5,Data Generating Process #
v0.5,##########################
v0.5,Log iteration
v0.5,"Generate controls, features, treatment and outcome"
v0.5,T and Y residuals to be used in later scripts
v0.5,Save generated dataset
v0.5,#################
v0.5,ORF parameters #
v0.5,#################
v0.5,######################################
v0.5,Train and evaluate treatment effect #
v0.5,######################################
v0.5,########
v0.5,Plots #
v0.5,########
v0.5,###############
v0.5,Save results #
v0.5,###############
v0.5,##############
v0.5,Run Rscript #
v0.5,##############
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,Check inputs
v0.5,Check inputs
v0.5,Check inputs
v0.5,Check inputs
v0.5,Check inputs
v0.5,Estimate response function
v0.5,Check inputs
v0.5,Train model on controls. Assign higher weight to units resembling
v0.5,treated units.
v0.5,Train model on the treated. Assign higher weight to units resembling
v0.5,control units.
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,Create splits of causal tree
v0.5,Make sure the correct exception is being rethrown
v0.5,Must make sure indices are merged correctly
v0.5,Require group assignment t to be one-hot-encoded
v0.5,Define an inner function that iterates over group predictions
v0.5,Convert rows to columns
v0.5,Get predictions for the 2 splits
v0.5,Must make sure indices are merged correctly
v0.5,Estimators
v0.5,OrthoForest parameters
v0.5,Sub-forests
v0.5,Fit check
v0.5,TODO: Check performance
v0.5,Must normalize weights
v0.5,Crossfitting
v0.5,Compute weighted nuisance estimates
v0.5,Generate subsample indices
v0.5,Safety check
v0.5,Build trees in parallel
v0.5,Calculates weights
v0.5,Bootstraping has repetitions in tree sample
v0.5,Similar for `a` weights
v0.5,Bootstraping has repetitions in tree sample
v0.5,Copy and/or define models
v0.5,Define nuisance estimators
v0.5,Define parameter estimators
v0.5,Define
v0.5,Nuissance estimates evaluated with cross-fitting
v0.5,Define 2-fold iterator
v0.5,need safe=False when cloning for WeightedModelWrapper
v0.5,Compute residuals
v0.5,Compute coefficient by OLS on residuals
v0.5,"Parameter returned by LinearRegression is (d_T, )"
v0.5,Compute residuals
v0.5,Compute coefficient by OLS on residuals
v0.5,ell_2 regularization
v0.5,Ridge regression estimate
v0.5,"Parameter returned by LinearRegression is (d_T, )"
v0.5,Return moments and gradients
v0.5,Compute residuals
v0.5,Compute moments
v0.5,"Moments shape is (n, d_T)"
v0.5,Compute moment gradients
v0.5,Copy and/or define models
v0.5,Nuisance estimators shall be defined during fitting because they need to know the number of distinct
v0.5,treatments
v0.5,Define parameter estimators
v0.5,Define moment and mean gradient estimator
v0.5,Define autoencoder
v0.5,"Check that T is shape (n, )"
v0.5,Check T is numeric
v0.5,Train label encoder
v0.5,Define number of classes
v0.5,Call `fit` from parent class
v0.5,"Test that T contains all treatments. If not, return None"
v0.5,Nuissance estimates evaluated with cross-fitting
v0.5,Define 2-fold iterator
v0.5,Check if there is only one example of some class
v0.5,No need to crossfit for internal nodes
v0.5,Compute partial moments
v0.5,"If any of the values in the parameter estimate is nan, return None"
v0.5,Compute partial moments
v0.5,Compute coefficient by OLS on residuals
v0.5,ell_2 regularization
v0.5,Ridge regression estimate
v0.5,"Parameter returned by LinearRegression is (d_T, )"
v0.5,Return moments and gradients
v0.5,Compute partial moments
v0.5,Compute moments
v0.5,"Moments shape is (n, d_T-1)"
v0.5,Compute moment gradients
v0.5,Need to calculate this in an elegant way for when propensity is 0
v0.5,This will flatten T
v0.5,Check that T is numeric
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"TODO: consider whether we need more care around stateful featurizers,"
v0.5,since we clone it and fit separate copies
v0.5,"if both X and W are None, just return a column of ones"
v0.5,"In this case, the Target is the one-hot-encoding of the treatment variable"
v0.5,We need to go back to the label representation of the one-hot so as to call
v0.5,the classifier.
v0.5,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.5,NOTE This is used by the inference methods and has to be the overall featurizer. intended
v0.5,for internal use by the library
v0.5,NOTE This is used by the inference methods and is more for internal use to the library
v0.5,override only so that we can update the docstring to indicate support for `StatsModelsInference`
v0.5,TODO: support sample_var
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,#######################################
v0.5,Core DML Tests
v0.5,#######################################
v0.5,How many samples
v0.5,How many control features
v0.5,How many treatment variables
v0.5,Coefficients of how controls affect treatments
v0.5,Coefficients of how controls affect outcome
v0.5,Treatment effects that we want to estimate
v0.5,Run dml estimation
v0.5,How many samples
v0.5,How many control features
v0.5,How many treatment variables
v0.5,Coefficients of how controls affect treatments
v0.5,Coefficients of how controls affect outcome
v0.5,Treatment effects that we want to estimate
v0.5,Run dml estimation
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.5,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"this will have dimension (d,) + shape(X)"
v0.5,send the first dimension to the end
v0.5,columns are featurized independently; partial derivatives are only non-zero
v0.5,when taken with respect to the same column each time
v0.5,don't fit intercept; manually add column of ones to the data instead;
v0.5,this allows us to ignore the intercept when computing marginal effects
v0.5,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.5,store number of columns of T so that we can pass scalars to effect
v0.5,two stage approximation
v0.5,"first, get basis expansions of T, X, and Z"
v0.5,"regress T expansion on X,Z expansions concatenated with W"
v0.5,"predict ft_T from interacted ft_X, ft_Z"
v0.5,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.5,dT may be only 2-dimensional)
v0.5,promote dT to 3D if necessary (e.g. if T was a vector)
v0.5,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"once the estimator has been fit, it's kosher to access its effect_op and store it here"
v0.5,"(which needs to have seen the expanded d_t if there's a discrete treatment, etc.)"
v0.5,"once the estimator has been fit, it's kosher to access its effect_op and store it here"
v0.5,"(which needs to have seen the expanded d_t if there's a discrete treatment, etc.)"
v0.5,need to set the fit args before the estimator is fit
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"TODO Allow for non-vector y, i.e. of shape (n, 1)"
v0.5,Coding Remark: The reasoning around the multitask_model_final could have been simplified if
v0.5,"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
v0.5,"to allow even for model_final objects whose fit(X, y) can accept X=None"
v0.5,"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
v0.5,checks that X is 2D array.
v0.5,"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.5,"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
v0.5,"Replacing fit from DRLearner, to add statsmodels inference in docstring"
v0.5,"Replacing this method which is invalid for this class, so that we make the"
v0.5,dosctring empty and not appear in the docs.
v0.5,"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
v0.5,TODO: support sample_var
v0.5,"Replacing this method which is invalid for this class, so that we make the"
v0.5,dosctring empty and not appear in the docs.
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,TODO: make sure to use random seeds wherever necessary
v0.5,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.5,"unfortunately with the Theano and Tensorflow backends,"
v0.5,the straightforward use of K.stop_gradient can cause an error
v0.5,because the parameters of the intermediate layers are now disconnected from the loss;
v0.5,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.5,so that those layers remain connected but with 0 gradient
v0.5,|| t - mu_i || ^2
v0.5,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.5,Use logsumexp for numeric stability:
v0.5,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.5,TODO: does the numeric stability actually make any difference?
v0.5,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.5,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.5,generate cumulative sum via matrix multiplication
v0.5,"Generate standard uniform values in shape (batch_size,1)"
v0.5,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.5,we use uniform_like instead with an input of an appropriate shape)
v0.5,convert to floats and multiply to perform equivalent of logical AND
v0.5,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.5,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.5,we use normal_like instead with an input of an appropriate shape)
v0.5,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.5,prevent gradient from passing through sampling
v0.5,three options: biased or upper-bound loss require a single number of samples;
v0.5,unbiased can take different numbers for the network and its gradient
v0.5,"sample: (() -> Layer, int) -> Layer"
v0.5,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.5,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.5,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.5,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.5,the dimensionality of the output of the network
v0.5,TODO: is there a more robust way to do this?
v0.5,TODO: do we need to give the user more control over other arguments to fit?
v0.5,"subtle point: we need to build a new model each time,"
v0.5,because each model encapsulates its randomness
v0.5,TODO: do we need to give the user more control over other arguments to fit?
v0.5,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.5,not a general tensor (because of how backprop works in every framework)
v0.5,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.5,but this seems annoying...)
v0.5,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.5,TODO: any way to get this to work on batches of arbitrary size?
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,#######################################################
v0.5,Perfect Data DGPs for Testing Correctness of Code
v0.5,#######################################################
v0.5,Generate random control co-variates
v0.5,Create epsilon residual treatments that deterministically sum up to
v0.5,zero
v0.5,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.5,conditional on each co-variate vector is equal to zero
v0.5,We simply subtract the conditional mean from the epsilons
v0.5,Construct treatments as T = X*A + epsilon
v0.5,Construct outcomes as y = X*beta + T*effect
v0.5,Generate random control co-variates
v0.5,Create epsilon residual treatments that deterministically sum up to
v0.5,zero
v0.5,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.5,conditional on each co-variate vector is equal to zero
v0.5,We simply subtract the conditional mean from the epsilons
v0.5,Construct treatments as T = X*A + epsilon
v0.5,Construct outcomes as y = X*beta + T*effect
v0.5,Generate random control co-variates
v0.5,Construct treatments as T = X*A + epsilon
v0.5,Construct outcomes as y = X*beta + T*effect
v0.5,Generate random control co-variates
v0.5,Create epsilon residual treatments
v0.5,Construct treatments as T = X*A + epsilon
v0.5,Construct outcomes as y = X*beta + T*effect + eta
v0.5,Generate random control co-variates
v0.5,Use the same treatment vector for each row
v0.5,Construct outcomes as y = X*beta + T*effect
v0.5,Licensed under the MIT License.
v0.5,"since inference objects can be stateful, we must copy it before fitting;"
v0.5,otherwise this sequence wouldn't work:
v0.5,"est1.fit(..., inference=inf)"
v0.5,"est2.fit(..., inference=inf)"
v0.5,est1.effect_interval(...)
v0.5,because inf now stores state from fitting est2
v0.5,call the wrapped fit method
v0.5,NOTE: we call inference fit *after* calling the main fit method
v0.5,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.5,but tensordot can't be applied to this problem because we don't sum over m
v0.5,if X is None then the shape of const_marginal_effect will be wrong because the number
v0.5,of rows of T was not taken into account
v0.5,need to store the *original* dimensions of T so that we can expand scalar inputs to match;
v0.5,subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
v0.5,"override effect to set defaults, which works with the new definition of _expand_treatments"
v0.5,"NOTE: don't explicitly expand treatments here, because it's done in the super call"
v0.5,add statsmodels to parent's options
v0.5,add debiasedlasso to parent's options
v0.5,TODO Share some logic with non-discrete version
v0.5,add statsmodels to parent's options
v0.5,add statsmodels to parent's options
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,Our classes that derive from sklearn ones sometimes include
v0.5,inherited docstrings that have embedded doctests; we need the following imports
v0.5,so that they don't break.
v0.5,Check if model is sparse enough for this model
v0.5,"note that by default OneHotEncoder returns float64s, so need to convert to int"
v0.5,TODO: any way to avoid creating a copy if the array was already dense?
v0.5,"the call is necessary if the input was something like a list, though"
v0.5,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.5,so convert to pydata sparse first
v0.5,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.5,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.5,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.5,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.5,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.5,tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
v0.5,same number of input definitions as arrays
v0.5,input definitions have same number of dimensions as each array
v0.5,all result indices are unique
v0.5,all result indices must match at least one input index
v0.5,"map indices to all array, axis pairs for that index"
v0.5,each index has the same cardinality wherever it appears
v0.5,"State: list of (set of letters, list of (corresponding indices, value))"
v0.5,Algo: while list contains more than one entry
v0.5,take two entries
v0.5,sort both lists by intersection of their indices
v0.5,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.5,"take the union of indices and the product of values), stepping through each list linearly"
v0.5,TODO: might be faster to break into connected components first
v0.5,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.5,"so compute their content separately, then take cartesian product"
v0.5,this would save a few pointless sorts by empty tuples
v0.5,TODO: Consider investigating other performance ideas for these cases
v0.5,where the dense method beat the sparse method (usually sparse is faster)
v0.5,"e,facd,c->cfed"
v0.5,sparse: 0.0335489
v0.5,dense:  0.011465999999999997
v0.5,"gbd,da,egb->da"
v0.5,sparse: 0.0791625
v0.5,dense:  0.007319099999999995
v0.5,"dcc,d,faedb,c->abe"
v0.5,sparse: 1.2868097
v0.5,dense:  0.44605229999999985
v0.5,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.5,TODO: would using einsum's paths to optimize the order of merging help?
v0.5,Normalize weights
v0.5,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,TODO: allow different subsets for L1 and L2 regularization?
v0.5,TODO: any better way to deal with sparsity?
v0.5,TODO: any better way to deal with sparsity?
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.5,use a binary array to get stratified split in case of discrete treatment
v0.5,"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
v0.5,drop first column since all columns sum to one
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,Estimators
v0.5,Causal tree parameters
v0.5,Tree structure
v0.5,No need for a random split since the data is already
v0.5,a random subsample from the original input
v0.5,node list stores the nodes that are yet to be splitted
v0.5,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.5,Create local sample set
v0.5,Compute nuisance estimates for the current node
v0.5,Nuisance estimate cannot be calculated
v0.5,Estimate parameter for current node
v0.5,Node estimate cannot be calculated
v0.5,Calculate moments and gradient of moments for current data
v0.5,Calculate inverse gradient
v0.5,The gradient matrix is not invertible.
v0.5,No good split can be found
v0.5,Calculate point-wise pseudo-outcomes rho
v0.5,a split is determined by a feature and a sample pair
v0.5,the number of possible splits is at most (number of features) * (number of node samples)
v0.5,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.5,parse row and column of random pair
v0.5,the sample of the pair is the integer division of the random number with n_feats
v0.5,calculate the binary indicator of whether sample i is on the left or the right
v0.5,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.5,calculate the number of samples on the left child for each proposed split
v0.5,calculate the analogous binary indicator for the samples in the estimation set
v0.5,calculate the number of estimation samples on the left child of each proposed split
v0.5,find the upper and lower bound on the size of the left split for the split
v0.5,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.5,on each side.
v0.5,similarly for the estimation sample set
v0.5,if there is no valid split then don't create any children
v0.5,filter only the valid splits
v0.5,calculate the average influence vector of the samples in the left child
v0.5,calculate the average influence vector of the samples in the right child
v0.5,take the square of each of the entries of the influence vectors and normalize
v0.5,by size of each child
v0.5,calculate the vector score of each candidate split as the average of left and right
v0.5,influence vectors
v0.5,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.5,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.5,where there might be large discontinuities in some parameter as the conditioning set varies
v0.5,calculate the scalar score of each split by aggregating across the vector of scores
v0.5,Find split that minimizes criterion
v0.5,Create child nodes with corresponding subsamples
v0.5,add the created children to the list of not yet split nodes
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,group by product; sum and subtract original; divide by (n_p-1)
v0.5,group by product; sum and subtract original; divide by (n_p-1)
v0.5,"for now, require one feature per store/product combination"
v0.5,TODO: would be nice to relax this somehow
v0.5,"alphas vary by product, not by store"
v0.5,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.5,"one cross-price term per product, which is based on the average price"
v0.5,of all other goods sold at the same store in the same week
v0.5,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.5,store-specific and product-specific gammas and betas (which are positively correlated)
v0.5,"features: product dummies, store dummies"
v0.5,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.5,"observe n_products * n_stores prices, same number of quantities"
v0.5,"for direct regression comparisons, we need a pivoted version"
v0.5,"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,"
v0.5,"plus the same for ""group treatments"" (average treatment of other products in the same store/week)"
v0.5,"for direct regression, we also need to append the features"
v0.5,"(both the ""constant features"" as well as the normal ones)"
v0.5,NOTE: need to set cv because default generic algorithm is super slow for sparse matrices
v0.5,"alphas vary by product, not by store"
v0.5,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.5,store-specific and product-specific gammas
v0.5,store-specific and product-specific betas
v0.5,"features: product dummies, store dummies"
v0.5,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.5,we need only the prices for the compound model; all dummies are created internally
v0.5,"observe n_products * n_stores prices, same number of quantities"
v0.5,"simple results include treatments, plus treatments interacted with product dummies,"
v0.5,for use with the direct methods
v0.5,X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor
v0.5,underspecified model
v0.5,Y = alpha T + \sum_i alpha_i T_i + X beta + eta
v0.5,T = X gamma + eps
v0.5,how to score? distance from line of all solutions?
v0.5,"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error"
v0.5,"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)"
v0.5,"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)"
v0.5,"features: one product dummy, one store dummy (each missing one level), constant"
v0.5,"alphas vary by product, not by store"
v0.5,store-specific and product-specific gammas
v0.5,store-specific and product-specific betas
v0.5,"features: product dummies, store dummies"
v0.5,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.5,"columns: prices interacted with products (and constant), features"
v0.5,"observe n_products * n_stores prices, same number of quantities"
v0.5,use features starting at index 1+n_products to skip all prices
v0.5,"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')"
v0.5,"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)"
v0.5,pickleFile.close()
v0.5,#############################################
v0.5,Defining the parameters of Monte Carlo
v0.5,#############################################
v0.5,Estimation parameters
v0.5,###################################################################
v0.5,Estimating the parameters of the DGP with DML. Running multiple
v0.5,Monte Carlo experiments.
v0.5,###################################################################
v0.5,Sparse coefficients of treatment as a function of co-variates
v0.5,Coefficients of outcomes as a function of co-variates
v0.5,"DGP. Create samples of data (y, T, X) from known truth"
v0.5,DML Estimation.
v0.5,Estimation with other methods for comparison
v0.5,#########################################################
v0.5,Plotting the results and saving
v0.5,#########################################################
v0.5,"plt.figure(figsize=(20, 10))"
v0.5,"plt.subplot(1, 4, 1)"
v0.5,"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))"
v0.5,plt.hist(dml_r2score)
v0.5,"plt.subplot(1, 4, 2)"
v0.5,"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),"
v0.5,np.mean(direct_r2score)))
v0.5,plt.hist(direct_r2score)
v0.5,"plt.subplot(1, 4, 3)"
v0.5,"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),"
v0.5,np.std(dml_te)))
v0.5,plt.hist(np.array(dml_te).flatten())
v0.5,"plt.subplot(1, 4, 4)"
v0.5,"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),"
v0.5,np.std(direct_te)))
v0.5,plt.hist(np.array(direct_te).flatten())
v0.5,plt.tight_layout()
v0.5,"plt.savefig(""r2_comparison.png"")"
v0.5,Plotting the results and saving
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,TODO: Add a __dir__ implementation?
v0.5,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.5,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.5,then we should be computing a confidence interval for the wrapped calls
v0.5,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.5,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.5,"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
v0.5,Remove children with nonwhite mothers from the treatment group
v0.5,Remove children with nonwhite mothers from the treatment group
v0.5,Select columns
v0.5,Scale the numeric variables
v0.5,"Change the binary variable 'first' takes values in {1,2}"
v0.5,Append a column of ones as intercept
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.5,creating notebooks that are annoying for our users to actually run themselves
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,TODO: make this test actually test something instead of generating images
v0.5,Sparse coefficients of treatment as a function of co-variates
v0.5,Coefficients of outcomes as a function of co-variates
v0.5,"DGP. Create samples of data (y, T, X) from known truth"
v0.5,"DGP. Create samples of data (y, T, X) from known truth"
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,simple DGP only for illustration
v0.5,Define the treatment model neural network architecture
v0.5,"This will take the concatenation of one-dimensional values z and x as input,"
v0.5,"so the input shape is (d_z + d_x,)"
v0.5,The exact shape of the final layer is not critical because the Deep IV framework will
v0.5,add extra layers on top for the mixture density network
v0.5,Define the response model neural network architecture
v0.5,"This will take the concatenation of one-dimensional values t and x as input,"
v0.5,"so the input shape is (d_t + d_x,)"
v0.5,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.5,"NOTE: For the response model, it is important to define the model *outside*"
v0.5,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.5,so that the same weights will be reused in each instantiation
v0.5,number of samples to use in second estimate of the response
v0.5,(to make loss estimate unbiased)
v0.5,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.5,do something with predictions...
v0.5,also test vector t and y
v0.5,simple DGP only for illustration
v0.5,Define the treatment model neural network architecture
v0.5,"This will take the concatenation of one-dimensional values z and x as input,"
v0.5,"so the input shape is (d_z + d_x,)"
v0.5,The exact shape of the final layer is not critical because the Deep IV framework will
v0.5,add extra layers on top for the mixture density network
v0.5,Define the response model neural network architecture
v0.5,"This will take the concatenation of one-dimensional values t and x as input,"
v0.5,"so the input shape is (d_t + d_x,)"
v0.5,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.5,"NOTE: For the response model, it is important to define the model *outside*"
v0.5,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.5,so that the same weights will be reused in each instantiation
v0.5,number of samples to use in second estimate of the response
v0.5,(to make loss estimate unbiased)
v0.5,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.5,do something with predictions...
v0.5,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.5,test = True ensures we draw test set images
v0.5,test = True ensures we draw test set images
v0.5,re-draw to get new independent treatment and implied response
v0.5,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.5,above is necesary so that reduced form doesn't win
v0.5,covariates: time and emotion
v0.5,random instrument
v0.5,z -> price
v0.5,true observable demand function
v0.5,errors
v0.5,response
v0.5,test = True ensures we draw test set images
v0.5,test = True ensures we draw test set images
v0.5,re-draw to get new independent treatment and implied response
v0.5,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.5,above is necesary so that reduced form doesn't win
v0.5,covariates: time and emotion
v0.5,random instrument
v0.5,z -> price
v0.5,true observable demand function
v0.5,errors
v0.5,response
v0.5,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.5,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.5,For some reason this doesn't work at all when run against the CNTK backend...
v0.5,"model.compile('nadam', loss=lambda _,l:l)"
v0.5,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.5,generate a valiation set
v0.5,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.5,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.5,"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
v0.5,"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
v0.5,so we need to transpose the result
v0.5,1-d output
v0.5,2-d output
v0.5,Single dimensional output y
v0.5,Multi-dimensional output y
v0.5,1-d y
v0.5,multi-d y
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,test that we can fit with the same arguments as the base estimator
v0.5,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.5,with the same shape for the lower and upper bounds
v0.5,test that the lower and upper bounds differ
v0.5,test that we can do the same thing once we provide percentile bounds
v0.5,test that the lower and upper bounds differ
v0.5,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.5,with the same shape for the lower and upper bounds
v0.5,test that the lower and upper bounds differ
v0.5,test that we can do the same thing once we provide percentile bounds
v0.5,test that the lower and upper bounds differ
v0.5,test that we can fit with the same arguments as the base estimator
v0.5,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.5,with the same shape for the lower and upper bounds
v0.5,test that the lower and upper bounds differ
v0.5,test that we can do the same thing once we provide percentile bounds
v0.5,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.5,with the same shape for the lower and upper bounds
v0.5,test that the lower and upper bounds differ
v0.5,test that we can do the same thing once we provide percentile bounds
v0.5,test that the lower and upper bounds differ
v0.5,test that we can fit with the same arguments as the base estimator
v0.5,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.5,with the same shape for the lower and upper bounds
v0.5,test that the lower and upper bounds differ
v0.5,test that we can do the same thing once we provide percentile bounds
v0.5,test that the lower and upper bounds differ
v0.5,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.5,with the same shape for the lower and upper bounds
v0.5,test that the lower and upper bounds differ
v0.5,test that we can do the same thing once we provide percentile bounds
v0.5,test that the lower and upper bounds differ
v0.5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.5,with the same shape for the lower and upper bounds
v0.5,test that the lower and upper bounds differ
v0.5,test that the estimated effect is usually within the bounds
v0.5,test that we can do the same thing once we provide alpha explicitly
v0.5,test that the lower and upper bounds differ
v0.5,test that the estimated effect is usually within the bounds
v0.5,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.5,with the same shape for the lower and upper bounds
v0.5,test that the lower and upper bounds differ
v0.5,TODO: test that the estimated effect is usually within the bounds
v0.5,and that the true effect is also usually within the bounds
v0.5,test that we can do the same thing once we provide percentile bounds
v0.5,test that the lower and upper bounds differ
v0.5,TODO: test that the estimated effect is usually within the bounds
v0.5,and that the true effect is also usually within the bounds
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,Test non keyword based calls to fit
v0.5,Test custom splitter
v0.5,Test incomplete set of test folds
v0.5,"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,make sure cross product varies more slowly with first array
v0.5,and that vectors are okay as inputs
v0.5,number of inputs in specification must match number of inputs
v0.5,must have an output
v0.5,output indices must be unique
v0.5,output indices must be present in an input
v0.5,number of indices must match number of dimensions for each input
v0.5,repeated indices must always have consistent sizes
v0.5,transpose
v0.5,tensordot
v0.5,trace
v0.5,TODO: set up proper flag for this
v0.5,pick indices at random with replacement from the first 7 letters of the alphabet
v0.5,"of all of the distinct indices that appear in any input,"
v0.5,pick a random subset of them (of size at most 5) to appear in the output
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,Preprocess data
v0.5,Convert 'week' to a date
v0.5,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.5,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.5,Take log of price
v0.5,Make brand numeric
v0.5,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.5,which have all zero coeffs
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.5,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.5,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,Set random seed
v0.5,Generate data
v0.5,DGP constants
v0.5,Test data
v0.5,Constant treatment effect
v0.5,Constant treatment with multi output Y
v0.5,Heterogeneous treatment
v0.5,Heterogeneous treatment with multi output Y
v0.5,TLearner test
v0.5,Instantiate TLearner
v0.5,Test inputs
v0.5,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.5,Instantiate SLearner
v0.5,Test inputs
v0.5,Test constant treatment effect
v0.5,Test constant treatment effect with multi output Y
v0.5,Test heterogeneous treatment effect
v0.5,Need interactions between T and features
v0.5,Test heterogeneous treatment effect with multi output Y
v0.5,Instantiate XLearner
v0.5,Test inputs
v0.5,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.5,Instantiate DomainAdaptationLearner
v0.5,Test inputs
v0.5,"Test constant and heterogeneous treatment effect, single and multi output y"
v0.5,Get the true treatment effect
v0.5,Get the true treatment effect
v0.5,Fit learner and get the effect and marginal effect
v0.5,Compute treatment effect residuals (absolute)
v0.5,Check that at least 90% of predictions are within tolerance interval
v0.5,Check whether the output shape is right
v0.5,Check that one can pass in regular lists
v0.5,Check that it fails correctly if lists of different shape are passed in
v0.5,"Check that it works when T, Y have shape (n, 1)"
v0.5,Generate covariates
v0.5,Generate treatment
v0.5,Calculate outcome
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,DGP constants
v0.5,DGP coefficients
v0.5,Generated outcomes
v0.5,################
v0.5,WeightedLasso #
v0.5,################
v0.5,Define weights
v0.5,Define extended datasets
v0.5,Range of alphas
v0.5,Compare with Lasso
v0.5,--> No intercept
v0.5,--> With intercept
v0.5,When DGP has no intercept
v0.5,When DGP has intercept
v0.5,--> Coerce coefficients to be positive
v0.5,--> Toggle max_iter & tol
v0.5,Define weights
v0.5,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.5,Mixed DGP scenario.
v0.5,Define extended datasets
v0.5,Define weights
v0.5,Define multioutput
v0.5,##################
v0.5,WeightedLassoCV #
v0.5,##################
v0.5,Define alphas to test
v0.5,Compare with LassoCV
v0.5,--> No intercept
v0.5,--> With intercept
v0.5,--> Force parameters to be positive
v0.5,Choose a smaller n to speed-up process
v0.5,Compare fold weights
v0.5,Define weights
v0.5,Define extended datasets
v0.5,Define splitters
v0.5,WeightedKFold splitter
v0.5,Map weighted splitter to an extended splitter
v0.5,Define alphas to test
v0.5,Compare with LassoCV
v0.5,--> No intercept
v0.5,--> With intercept
v0.5,--> Force parameters to be positive
v0.5,###########################
v0.5,MultiTaskWeightedLassoCV #
v0.5,###########################
v0.5,Define alphas to test
v0.5,Define splitter
v0.5,Compare with MultiTaskLassoCV
v0.5,--> No intercept
v0.5,--> With intercept
v0.5,Define weights
v0.5,Define extended datasets
v0.5,Define splitters
v0.5,WeightedKFold splitter
v0.5,Map weighted splitter to an extended splitter
v0.5,Define alphas to test
v0.5,Compare with LassoCV
v0.5,--> No intercept
v0.5,--> With intercept
v0.5,################
v0.5,DebiasedLasso #
v0.5,################
v0.5,Test DebiasedLasso without weights
v0.5,--> Check debiased coeffcients without intercept
v0.5,--> Check debiased coeffcients with intercept
v0.5,--> Check 5-95 CI coverage for unit vectors
v0.5,Test DebiasedLasso with weights for one DGP
v0.5,Define weights
v0.5,Define extended datasets
v0.5,--> Check debiased coefficients
v0.5,Define weights
v0.5,Data from one DGP has weight 0. Check that we recover correct coefficients
v0.5,--> Check debiased coeffcients
v0.5,Test that attributes propagate correctly
v0.5,Test MultiOutputDebiasedLasso without weights
v0.5,--> Check debiased coeffcients without intercept
v0.5,--> Check debiased coeffcients with intercept
v0.5,--> Check CI coverage
v0.5,Test MultiOutputDebiasedLasso with weights
v0.5,Define weights
v0.5,Define extended datasets
v0.5,--> Check debiased coefficients
v0.5,Unit vectors
v0.5,Unit vectors
v0.5,Check coeffcients and intercept are the same within tolerance
v0.5,Check results are similar with tolerance 1e-6
v0.5,Check if multitask
v0.5,Check that same alpha is chosen
v0.5,Check that the coefficients are similar
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,DGP constants
v0.5,Generate data
v0.5,Test data
v0.5,Remove warnings that might be raised by the models passed into the ORF
v0.5,Generate data with continuous treatments
v0.5,Instantiate model with most of the default parameters
v0.5,Test inputs for continuous treatments
v0.5,--> Check that one can pass in regular lists
v0.5,--> Check that it fails correctly if lists of different shape are passed in
v0.5,Check that outputs have the correct shape
v0.5,Test continuous treatments with controls
v0.5,Test continuous treatments without controls
v0.5,Generate data with binary treatments
v0.5,Instantiate model with default params
v0.5,Test inputs for binary treatments
v0.5,--> Check that one can pass in regular lists
v0.5,--> Check that it fails correctly if lists of different shape are passed in
v0.5,"--> Check that it works when T, Y have shape (n, 1)"
v0.5,"--> Check that it fails correctly when T has shape (n, 2)"
v0.5,--> Check that it fails correctly when the treatments are not numeric
v0.5,Check that outputs have the correct shape
v0.5,Test binary treatments with controls
v0.5,Test binary treatments without controls
v0.5,Only applicable to continuous treatments
v0.5,Generate data for 2 treatments
v0.5,Test multiple treatments with controls
v0.5,"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
v0.5,The rest for controls. Just as an example.
v0.5,Generating A/B test data
v0.5,Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
v0.5,We also have confounding on the first variable. We also have heteroskedastic errors.
v0.5,Generate data with continuous treatments
v0.5,Instantiate model with most of the default parameters
v0.5,Compute the treatment effect on test points
v0.5,Compute treatment effect residuals
v0.5,Multiple treatments
v0.5,Allow at most 10% test points to be outside of the tolerance interval
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.5,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.5,ensure that we've got at least two of every element
v0.5,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.5,TODO: add stratification to bootstrap so that we can use it even with discrete treatments
v0.5,make sure we can call the marginal_effect and effect methods
v0.5,"make sure we can call effect with implied scalar treatments, no matter the"
v0.5,"dimensions of T, and also that we warn when there are multiple treatments"
v0.5,create a simple artificial setup where effect of moving from treatment
v0.5,"1 -> 2 is 2,"
v0.5,"1 -> 3 is 1, and"
v0.5,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.5,"Using an uneven number of examples from different classes,"
v0.5,"and having the treatments in non-lexicographic order,"
v0.5,Should rule out some basic issues.
v0.5,test that we can fit with a KFold instance
v0.5,test that we can fit with a train/test iterable
v0.5,(incorrectly) use a final model with an intercept
v0.5,"Because final model is fixed, actual values of T and Y don't matter"
v0.5,Ensure reproducibility
v0.5,Sparse DGP
v0.5,Treatment effect coef
v0.5,Other coefs
v0.5,Features and controls
v0.5,Test sparse estimator
v0.5,"--> test coef_, intercept_"
v0.5,--> test treatment effects
v0.5,Restrict x_test to vectors of norm < 1
v0.5,--> check inference
v0.5,Check that a majority of true effects lie in the 5-95% CI
v0.5,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.5,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.5,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.5,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.5,sparse test case: heterogeneous effect by product
v0.5,need at least as many rows in e_y as there are distinct columns
v0.5,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,Set random seed
v0.5,Generate data
v0.5,DGP constants
v0.5,Test data
v0.5,Constant treatment effect and propensity
v0.5,Heterogeneous treatment and propensity
v0.5,ensure that we've got at least two of every element
v0.5,"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
v0.5,TODO: add stratification to bootstrap so that we can use it even with discrete treatments
v0.5,make sure we can call the marginal_effect and effect methods
v0.5,"make sure we can call effect with implied scalar treatments, no matter the"
v0.5,"dimensions of T, and also that we warn when there are multiple treatments"
v0.5,create a simple artificial setup where effect of moving from treatment
v0.5,"1 -> 2 is 2,"
v0.5,"1 -> 3 is 1, and"
v0.5,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.5,"Using an uneven number of examples from different classes,"
v0.5,"and having the treatments in non-lexicographic order,"
v0.5,Should rule out some basic issues.
v0.5,test that we can fit with a KFold instance
v0.5,test that we can fit with a train/test iterable
v0.5,Test inputs
v0.5,self._test_inputs(DR_learner)
v0.5,Test constant treatment effect
v0.5,Test heterogeneous treatment effect
v0.5,Test heterogenous treatment effect for W =/= None
v0.5,Sparse DGP
v0.5,Treatment effect coef
v0.5,Other coefs
v0.5,Features and controls
v0.5,Test sparse estimator
v0.5,"--> test coef_, intercept_"
v0.5,--> test treatment effects
v0.5,Restrict x_test to vectors of norm < 1
v0.5,--> check inference
v0.5,Check that a majority of true effects lie in the 5-95% CI
v0.5,Fit learner and get the effect
v0.5,Get the true treatment effect
v0.5,Compute treatment effect residuals (absolute)
v0.5,Check that at least 90% of predictions are within tolerance interval
v0.5,Only for heterogeneous TE
v0.5,Fit learner on X and W and get the effect
v0.5,Get the true treatment effect
v0.5,Compute treatment effect residuals (absolute)
v0.5,Check that at least 90% of predictions are within tolerance interval
v0.5,Check that one can pass in regular lists
v0.5,Check that it fails correctly if lists of different shape are passed in
v0.5,Check that it fails when T contains values other than 0 and 1
v0.5,"Check that it works when T, Y have shape (n, 1)"
v0.5,Generate covariates
v0.5,Generate treatment
v0.5,Calculate outcome
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"Found a good split, return."
v0.5,Record all splits in case the stratification by weight yeilds a worse partition
v0.5,Reseed random generator and try again
v0.5,"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
v0.5,"Found a good split, return."
v0.5,Did not find a good split
v0.5,Record the devaiation for the weight-stratified split to compare with KFold splits
v0.5,Return most weight-balanced partition
v0.5,Weight stratification algorithm
v0.5,Sort weights for weight strata search
v0.5,There are some leftover indices that have yet to be assigned
v0.5,Append stratum splits to overall splits
v0.5,Copyright (c) Microsoft Corporation. All rights reserved.
v0.5,Licensed under the MIT License.
v0.5,"Convert X, y into numpy arrays"
v0.5,Define fit parameters
v0.5,Some algorithms don't have a check_input option
v0.5,Check weights array
v0.5,Check that weights are size-compatible
v0.5,Normalize inputs
v0.5,Weight inputs
v0.5,Fit base class without intercept
v0.5,Fit Lasso
v0.5,Reset intercept
v0.5,The intercept is not calculated properly due the sqrt(weights) factor
v0.5,so it must be recomputed
v0.5,Fit lasso without weights
v0.5,Make weighted splitter
v0.5,Fit weighted model
v0.5,Make weighted splitter
v0.5,Fit weighted model
v0.5,Select optimal penalty
v0.5,Warn about consistency
v0.5,"Convert X, y into numpy arrays"
v0.5,Fit weighted lasso with user input
v0.5,"Center X, y"
v0.5,Calculate quantities that will be used later on. Account for centered data
v0.5,Calculate coefficient and error variance
v0.5,Add coefficient correction
v0.5,Set coefficients and intercept standard errors
v0.5,Set intercept
v0.5,Return alpha to 'auto' state
v0.5,"Note that in the case of no intercept, X_offset is 0"
v0.5,Calculate the variance of the predictions
v0.5,Calculate prediction confidence intervals
v0.5,Assumes flattened y
v0.5,Compute weighted residuals
v0.5,To be done once per target. Assumes y can be flattened.
v0.5,Assumes that X has already been offset
v0.5,Special case: n_features=1
v0.5,Compute Lasso coefficients for the columns of the design matrix
v0.5,Call weighted lasso on reduced design matrix
v0.5,Inherit some parameters from the parent
v0.5,Weighted tau
v0.5,Compute C_hat
v0.5,Compute theta_hat
v0.5,Allow for single output as well
v0.5,"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
v0.5,Set coef_ attribute
v0.5,Set intercept_ attribute
v0.5,Set selected_alpha_ attribute
v0.5,Set coef_std_err_
v0.5,intercept_std_err_
v0.5,set intercept_ attribute
v0.5,set coef_ attribute
v0.5,set alpha_ attribute
v0.5,set alphas_ attribute
v0.5,set n_iter_ attribute
v0.4,configuration is all pulled from setup.cfg
v0.4,-*- coding: utf-8 -*-
v0.4,
v0.4,Configuration file for the Sphinx documentation builder.
v0.4,
v0.4,This file does only contain a selection of the most common options. For a
v0.4,full list see the documentation:
v0.4,http://www.sphinx-doc.org/en/master/config
v0.4,-- Path setup --------------------------------------------------------------
v0.4,"If extensions (or modules to document with autodoc) are in another directory,"
v0.4,add these directories to sys.path here. If the directory is relative to the
v0.4,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.4,
v0.4,-- Project information -----------------------------------------------------
v0.4,-- General configuration ---------------------------------------------------
v0.4,"If your documentation needs a minimal Sphinx version, state it here."
v0.4,
v0.4,needs_sphinx = '1.0'
v0.4,"Add any Sphinx extension module names here, as strings. They can be"
v0.4,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.4,ones.
v0.4,"Add any paths that contain templates here, relative to this directory."
v0.4,The suffix(es) of source filenames.
v0.4,You can specify multiple suffix as a list of string:
v0.4,
v0.4,"source_suffix = ['.rst', '.md']"
v0.4,The master toctree document.
v0.4,The language for content autogenerated by Sphinx. Refer to documentation
v0.4,for a list of supported languages.
v0.4,
v0.4,This is also used if you do content translation via gettext catalogs.
v0.4,"Usually you set ""language"" from the command line for these cases."
v0.4,"List of patterns, relative to source directory, that match files and"
v0.4,directories to ignore when looking for source files.
v0.4,This pattern also affects html_static_path and html_extra_path.
v0.4,The name of the Pygments (syntax highlighting) style to use.
v0.4,-- Options for HTML output -------------------------------------------------
v0.4,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.4,a list of builtin themes.
v0.4,
v0.4,Theme options are theme-specific and customize the look and feel of a theme
v0.4,"further.  For a list of options available for each theme, see the"
v0.4,documentation.
v0.4,
v0.4,"Add any paths that contain custom static files (such as style sheets) here,"
v0.4,"relative to this directory. They are copied after the builtin static files,"
v0.4,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.4,html_static_path = ['_static']
v0.4,"Custom sidebar templates, must be a dictionary that maps document names"
v0.4,to template names.
v0.4,
v0.4,The default sidebars (for documents that don't match any pattern) are
v0.4,defined by theme itself.  Builtin themes are using these templates by
v0.4,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.4,'searchbox.html']``.
v0.4,
v0.4,html_sidebars = {}
v0.4,-- Options for HTMLHelp output ---------------------------------------------
v0.4,Output file base name for HTML help builder.
v0.4,-- Options for LaTeX output ------------------------------------------------
v0.4,The paper size ('letterpaper' or 'a4paper').
v0.4,
v0.4,"'papersize': 'letterpaper',"
v0.4,"The font size ('10pt', '11pt' or '12pt')."
v0.4,
v0.4,"'pointsize': '10pt',"
v0.4,Additional stuff for the LaTeX preamble.
v0.4,
v0.4,"'preamble': '',"
v0.4,Latex figure (float) alignment
v0.4,
v0.4,"'figure_align': 'htbp',"
v0.4,Grouping the document tree into LaTeX files. List of tuples
v0.4,"(source start file, target name, title,"
v0.4,"author, documentclass [howto, manual, or own class])."
v0.4,-- Options for manual page output ------------------------------------------
v0.4,One entry per manual page. List of tuples
v0.4,"(source start file, name, description, authors, manual section)."
v0.4,-- Options for Texinfo output ----------------------------------------------
v0.4,Grouping the document tree into Texinfo files. List of tuples
v0.4,"(source start file, target name, title, author,"
v0.4,"dir menu entry, description, category)"
v0.4,-- Options for Epub output -------------------------------------------------
v0.4,Bibliographic Dublin Core info.
v0.4,The unique identifier of the text. This can be a ISBN number
v0.4,or the project homepage.
v0.4,
v0.4,epub_identifier = ''
v0.4,A unique identification for the text.
v0.4,
v0.4,epub_uid = ''
v0.4,A list of files that should not be packed into the epub file.
v0.4,-- Extension configuration -------------------------------------------------
v0.4,-- Options for intersphinx extension ---------------------------------------
v0.4,Example configuration for intersphinx: refer to the Python standard library.
v0.4,-- Options for todo extension ----------------------------------------------
v0.4,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,Calculate residuals
v0.4,Estimate E[T_res | Z_res]
v0.4,TODO. Deal with multi-class instrument
v0.4,Calculate nuisances
v0.4,Estimate E[T_res | Z_res]
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,"We do a three way split, as typically a preliminary theta estimator would require"
v0.4,many samples. So having 2/3 of the sample to train model_theta seems appropriate.
v0.4,TODO. Deal with multi-class instrument
v0.4,Estimate final model of theta(X) by minimizing the square loss:
v0.4,"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
v0.4,"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
v0.4,at the expense of some small bias. For points with very small covariance we revert
v0.4,to the model-based preliminary estimate and do not add the correction term.
v0.4,Estimate preliminary theta in cross fitting manner
v0.4,Estimate p(X) = E[T | X] in cross fitting manner
v0.4,Estimate r(Z) = E[Z | X] in cross fitting manner
v0.4,Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
v0.4,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.4,"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
v0.4,TODO. The solution below is not really a valid cross-fitting
v0.4,as the test data are used to create the proj_t on the train
v0.4,which in the second train-test loop is used to create the nuisance
v0.4,cov on the test data. Hence the T variable of some sample
v0.4,"is implicitly correlated with its cov nuisance, through this flow"
v0.4,"of information. However, this seems a rather weak correlation."
v0.4,The more kosher would be to do an internal nested cv loop for the T_XZ
v0.4,model.
v0.4,"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
v0.4,Estimate preliminary theta in cross fitting manner
v0.4,Estimate p(X) = E[T | X] in cross fitting manner
v0.4,"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
v0.4,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.4,"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
v0.4,#############################################################################
v0.4,Classes for the DRIV implementation for the special case of intent-to-treat
v0.4,A/B test
v0.4,#############################################################################
v0.4,Estimate preliminary theta in cross fitting manner
v0.4,Estimate p(X) = E[T | X] in cross fitting manner
v0.4,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
v0.4,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.4,We can use statsmodel for all hypothesis testing capabilities
v0.4,"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
v0.4,"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
v0.4,the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
v0.4,model_T_XZ = lambda: model_clf()
v0.4,#'days_visited': lambda:
v0.4,"#X = np.random.uniform(-1, 1, size=(n, d))"
v0.4,Turn strings into categories for numeric mapping
v0.4,### Defining some generic regressors and classifiers
v0.4,This a generic non-parametric regressor
v0.4,"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.4,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
v0.4,"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.4,"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
v0.4,model = lambda: RandomForestRegressor(n_estimators=100)
v0.4,model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
v0.4,model = lambda: GradientBoostingRegressor(n_estimators=60)
v0.4,model = lambda: LinearRegression(n_jobs=-1)
v0.4,"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
v0.4,we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
v0.4,underlying model whenever predict is called.
v0.4,"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
v0.4,"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
v0.4,"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
v0.4,"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
v0.4,model_clf = lambda: RandomForestClassifier(n_estimators=100)
v0.4,model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
v0.4,"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
v0.4,We need to specify models to be used for each of these residualizations
v0.4,model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
v0.4,"E[T | X, Z]"
v0.4,E[TZ | X]
v0.4,We fit DMLATEIV with these models and then we call effect() to get the ATE.
v0.4,n_splits determines the number of splits to be used for cross-fitting.
v0.4,# Algorithm 2 - Current Method
v0.4,In[121]:
v0.4,# Algorithm 3 - DRIV ATE
v0.4,dmliv_model_effect = lambda: model()
v0.4,"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
v0.4,"dmliv_model_effect(),"
v0.4,n_splits=1)
v0.4,reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
v0.4,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
v0.4,"Once multiple treatments are supported, we'll need to fix this"
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.4,We can use statsmodel for all hypothesis testing capabilities
v0.4,"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
v0.4,We can use statsmodel for all hypothesis testing capabilities
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,TODO. Deal with multi-class instrument/treatment
v0.4,"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
v0.4,Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
v0.4,Estimate p(X) = E[T | X] in cross-fitting manner
v0.4,"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
v0.4,##################
v0.4,Global settings #
v0.4,##################
v0.4,Global plotting controls
v0.4,"Control for support size, can control for more"
v0.4,#################
v0.4,File utilities #
v0.4,#################
v0.4,#################
v0.4,Plotting utils #
v0.4,#################
v0.4,bias
v0.4,var
v0.4,rmse
v0.4,r2
v0.4,Infer feature dimension
v0.4,Metrics by support plots
v0.4,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.4,Vasilis Syrgkanis <vasy@microsoft.com>
v0.4,Steven Wu <zhiww@microsoft.com>
v0.4,Initialize causal tree parameters
v0.4,Create splits of causal tree
v0.4,Estimate treatment effects at the leafs
v0.4,Compute heterogeneous treatement effect for x's in x_list by finding
v0.4,the corresponding split and associating the effect computed on that leaf
v0.4,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.4,Safety check
v0.4,Weighted linear regression
v0.4,Calculates weights
v0.4,Bootstraping has repetitions in tree sample so we need to iterate
v0.4,over all indices
v0.4,Similar for `a` weights
v0.4,Doesn't have sample weights
v0.4,Is a linear model
v0.4,Weighted linear regression
v0.4,Calculates weights
v0.4,Bootstraping has repetitions in tree sample so we need to iterate
v0.4,over all indices
v0.4,Similar for `a` weights
v0.4,normalize weights
v0.4,"Split the data in half, train and test"
v0.4,Fit with LassoCV the treatment as a function of W and the outcome as
v0.4,"a function of W, using only the train fold"
v0.4,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.4,"Split the data in half, train and test"
v0.4,Fit with LassoCV the treatment as a function of W and the outcome as
v0.4,"a function of W, using only the train fold"
v0.4,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.4,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.4,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.4,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.4,"Split the data in half, train and test"
v0.4,Fit with LassoCV the treatment as a function of x and the outcome as
v0.4,"a function of x, using only the train fold"
v0.4,Then compute residuals p-g(x) and q-q(x) on test fold
v0.4,Compute coefficient by OLS on residuals
v0.4,"Split the data in half, train and test"
v0.4,Fit with LassoCV the treatment as a function of x and the outcome as
v0.4,"a function of x, using only the train fold"
v0.4,Then compute residuals p-g(x) and q-q(x) on test fold
v0.4,Estimate multipliers for second order orthogonal method
v0.4,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.4,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.4,Create local sample set
v0.4,compute the base estimate for the current node using double ml or second order double ml
v0.4,compute the influence functions here that are used for the criterion
v0.4,generate random proposals of dimensions to split
v0.4,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.4,compute criterion for each proposal
v0.4,if splitting creates valid leafs in terms of mean leaf size
v0.4,Calculate criterion for split
v0.4,Else set criterion to infinity so that this split is not chosen
v0.4,If no good split was found
v0.4,Find split that minimizes criterion
v0.4,Set the split attributes at the node
v0.4,Create child nodes with corresponding subsamples
v0.4,Recursively split children
v0.4,Return parent node
v0.4,estimate the local parameter at the leaf using the estimate data
v0.4,###################
v0.4,Argument parsing #
v0.4,###################
v0.4,#########################################
v0.4,Parameters constant across experiments #
v0.4,#########################################
v0.4,Outcome support
v0.4,Treatment support
v0.4,Evaluation grid
v0.4,Treatment effects array
v0.4,Other variables
v0.4,##########################
v0.4,Data Generating Process #
v0.4,##########################
v0.4,Log iteration
v0.4,"Generate controls, features, treatment and outcome"
v0.4,T and Y residuals to be used in later scripts
v0.4,Save generated dataset
v0.4,#################
v0.4,ORF parameters #
v0.4,#################
v0.4,######################################
v0.4,Train and evaluate treatment effect #
v0.4,######################################
v0.4,########
v0.4,Plots #
v0.4,########
v0.4,###############
v0.4,Save results #
v0.4,###############
v0.4,##############
v0.4,Run Rscript #
v0.4,##############
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,Check inputs
v0.4,Check inputs
v0.4,Check inputs
v0.4,Check inputs
v0.4,Check inputs
v0.4,Check inputs
v0.4,Check inputs
v0.4,Train model on controls. Assign higher weight to units resembling
v0.4,treated units.
v0.4,Train model on the treated. Assign higher weight to units resembling
v0.4,control units.
v0.4,Check inputs
v0.4,Check inputs
v0.4,Fit outcome model on X||W||T (concatenated)
v0.4,Check inputs
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,Create splits of causal tree
v0.4,Must make sure indices are merged correctly
v0.4,Require group assignment t to be one-hot-encoded
v0.4,Define an inner function that iterates over group predictions
v0.4,Convert rows to columns
v0.4,Get predictions for the 2 splits
v0.4,Must make sure indices are merged correctly
v0.4,Estimators
v0.4,OrthoForest parameters
v0.4,Sub-forests
v0.4,Fit check
v0.4,TODO: Check performance
v0.4,Must normalize weights
v0.4,Crossfitting
v0.4,Compute weighted nuisance estimates
v0.4,Generate subsample indices
v0.4,Safety check
v0.4,Build trees in parallel
v0.4,Calculates weights
v0.4,Bootstraping has repetitions in tree sample
v0.4,Similar for `a` weights
v0.4,Bootstraping has repetitions in tree sample
v0.4,Copy and/or define models
v0.4,Define nuisance estimators
v0.4,Define parameter estimators
v0.4,Define
v0.4,Nuissance estimates evaluated with cross-fitting
v0.4,Define 2-fold iterator
v0.4,need safe=False when cloning for WeightedModelWrapper
v0.4,Compute residuals
v0.4,Compute coefficient by OLS on residuals
v0.4,"Parameter returned by LinearRegression is (d_T, )"
v0.4,Compute residuals
v0.4,Compute coefficient by OLS on residuals
v0.4,ell_2 regularization
v0.4,Ridge regression estimate
v0.4,"Parameter returned by LinearRegression is (d_T, )"
v0.4,Return moments and gradients
v0.4,Compute residuals
v0.4,Compute moments
v0.4,"Moments shape is (n, d_T)"
v0.4,Compute moment gradients
v0.4,Copy and/or define models
v0.4,Nuisance estimators shall be defined during fitting because they need to know the number of distinct
v0.4,treatments
v0.4,Define parameter estimators
v0.4,Define moment and mean gradient estimator
v0.4,Define autoencoder
v0.4,"Check that T is shape (n, )"
v0.4,Check T is numeric
v0.4,Train label encoder
v0.4,Define number of classes
v0.4,Call `fit` from parent class
v0.4,"Test that T contains all treatments. If not, return None"
v0.4,Nuissance estimates evaluated with cross-fitting
v0.4,Define 2-fold iterator
v0.4,Check if there is only one example of some class
v0.4,No need to crossfit for internal nodes
v0.4,Compute partial moments
v0.4,"If any of the values in the parameter estimate is nan, return None"
v0.4,Compute partial moments
v0.4,Compute coefficient by OLS on residuals
v0.4,ell_2 regularization
v0.4,Ridge regression estimate
v0.4,"Parameter returned by LinearRegression is (d_T, )"
v0.4,Return moments and gradients
v0.4,Compute partial moments
v0.4,Compute moments
v0.4,"Moments shape is (n, d_T-1)"
v0.4,Compute moment gradients
v0.4,Need to calculate this in an elegant way for when propensity is 0
v0.4,This will flatten T
v0.4,Check that T is numeric
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector..."
v0.4,"Do we want to reshape to an nx1, or just trust the user's choice of input?"
v0.4,(Likewise for Y below)
v0.4,need to override effect in case of discrete treatments
v0.4,TODO: should this logic be moved up to the LinearCateEstimator class and
v0.4,removed from here and from the OrthoForest implementation?
v0.4,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.4,create an identity matrix of size d_t (or just a 1-element array if T was a vector)
v0.4,the nth row will allow us to compute the marginal effect of the nth component of treatment
v0.4,TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called
v0.4,"rather than just using coef_ seems silly, but one benefit is that we can use linear models"
v0.4,that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models)
v0.4,TODO: handle case where final model doesn't directly expose coef_?
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,#######################################
v0.4,Core DML Tests
v0.4,#######################################
v0.4,How many samples
v0.4,How many control features
v0.4,How many treatment variables
v0.4,Coefficients of how controls affect treatments
v0.4,Coefficients of how controls affect outcome
v0.4,Treatment effects that we want to estimate
v0.4,Run dml estimation
v0.4,How many samples
v0.4,How many control features
v0.4,How many treatment variables
v0.4,Coefficients of how controls affect treatments
v0.4,Coefficients of how controls affect outcome
v0.4,Treatment effects that we want to estimate
v0.4,Run dml estimation
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,"this will have dimension (d,) + shape(X)"
v0.4,send the first dimension to the end
v0.4,columns are featurized independently; partial derivatives are only non-zero
v0.4,when taken with respect to the same column each time
v0.4,don't fit intercept; manually add column of ones to the data instead;
v0.4,this allows us to ignore the intercept when computing marginal effects
v0.4,store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
v0.4,store number of columns of T so that we can pass scalars to effect
v0.4,two stage approximation
v0.4,"first, get basis expansions of T, X, and Z"
v0.4,"regress T expansion on X,Z expansions concatenated with W"
v0.4,"predict ft_T from interacted ft_X, ft_Z"
v0.4,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.4,dT may be only 2-dimensional)
v0.4,promote dT to 3D if necessary (e.g. if T was a vector)
v0.4,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,TODO: make sure to use random seeds wherever necessary
v0.4,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.4,"unfortunately with the Theano and Tensorflow backends,"
v0.4,the straightforward use of K.stop_gradient can cause an error
v0.4,because the parameters of the intermediate layers are now disconnected from the loss;
v0.4,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.4,so that those layers remain connected but with 0 gradient
v0.4,|| t - mu_i || ^2
v0.4,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.4,Use logsumexp for numeric stability:
v0.4,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.4,TODO: does the numeric stability actually make any difference?
v0.4,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.4,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.4,generate cumulative sum via matrix multiplication
v0.4,"Generate standard uniform values in shape (batch_size,1)"
v0.4,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.4,we use uniform_like instead with an input of an appropriate shape)
v0.4,convert to floats and multiply to perform equivalent of logical AND
v0.4,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.4,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.4,we use normal_like instead with an input of an appropriate shape)
v0.4,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.4,prevent gradient from passing through sampling
v0.4,three options: biased or upper-bound loss require a single number of samples;
v0.4,unbiased can take different numbers for the network and its gradient
v0.4,"sample: (() -> Layer, int) -> Layer"
v0.4,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.4,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.4,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.4,"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
v0.4,the dimensionality of the output of the network
v0.4,TODO: is there a more robust way to do this?
v0.4,TODO: do we need to give the user more control over other arguments to fit?
v0.4,"subtle point: we need to build a new model each time,"
v0.4,because each model encapsulates its randomness
v0.4,TODO: do we need to give the user more control over other arguments to fit?
v0.4,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.4,not a general tensor (because of how backprop works in every framework)
v0.4,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.4,but this seems annoying...)
v0.4,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.4,TODO: any way to get this to work on batches of arbitrary size?
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,#######################################################
v0.4,Perfect Data DGPs for Testing Correctness of Code
v0.4,#######################################################
v0.4,Generate random control co-variates
v0.4,Create epsilon residual treatments that deterministically sum up to
v0.4,zero
v0.4,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.4,conditional on each co-variate vector is equal to zero
v0.4,We simply subtract the conditional mean from the epsilons
v0.4,Construct treatments as T = X*A + epsilon
v0.4,Construct outcomes as y = X*beta + T*effect
v0.4,Generate random control co-variates
v0.4,Create epsilon residual treatments that deterministically sum up to
v0.4,zero
v0.4,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.4,conditional on each co-variate vector is equal to zero
v0.4,We simply subtract the conditional mean from the epsilons
v0.4,Construct treatments as T = X*A + epsilon
v0.4,Construct outcomes as y = X*beta + T*effect
v0.4,Generate random control co-variates
v0.4,Construct treatments as T = X*A + epsilon
v0.4,Construct outcomes as y = X*beta + T*effect
v0.4,Generate random control co-variates
v0.4,Create epsilon residual treatments
v0.4,Construct treatments as T = X*A + epsilon
v0.4,Construct outcomes as y = X*beta + T*effect + eta
v0.4,Generate random control co-variates
v0.4,Use the same treatment vector for each row
v0.4,Construct outcomes as y = X*beta + T*effect
v0.4,Licensed under the MIT License.
v0.4,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.4,but tensordot can't be applied to this problem because we don't sum over m
v0.4,"TODO: if T0 or T1 are scalars, we'll promote them to vectors;"
v0.4,should it be possible to promote them to 2D arrays if that's what we saw during training?
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,TODO: any way to avoid creating a copy if the array was already dense?
v0.4,"the call is necessary if the input was something like a list, though"
v0.4,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.4,so convert to pydata sparse first
v0.4,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.4,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.4,TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?
v0.4,(but note that changing this would necessitate changes to callers
v0.4,to switch the order to preserve behavior where order is important)
v0.4,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.4,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.4,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.4,same number of input definitions as arrays
v0.4,input definitions have same number of dimensions as each array
v0.4,all result indices are unique
v0.4,all result indices must match at least one input index
v0.4,"map indices to all array, axis pairs for that index"
v0.4,each index has the same cardinality wherever it appears
v0.4,"State: list of (set of letters, list of (corresponding indices, value))"
v0.4,Algo: while list contains more than one entry
v0.4,take two entries
v0.4,sort both lists by intersection of their indices
v0.4,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.4,"take the union of indices and the product of values), stepping through each list linearly"
v0.4,TODO: might be faster to break into connected components first
v0.4,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.4,"so compute their content separately, then take cartesian product"
v0.4,this would save a few pointless sorts by empty tuples
v0.4,TODO: Consider investigating other performance ideas for these cases
v0.4,where the dense method beat the sparse method (usually sparse is faster)
v0.4,"e,facd,c->cfed"
v0.4,sparse: 0.0335489
v0.4,dense:  0.011465999999999997
v0.4,"gbd,da,egb->da"
v0.4,sparse: 0.0791625
v0.4,dense:  0.007319099999999995
v0.4,"dcc,d,faedb,c->abe"
v0.4,sparse: 1.2868097
v0.4,dense:  0.44605229999999985
v0.4,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.4,TODO: would using einsum's paths to optimize the order of merging help?
v0.4,Normalize weights
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,TODO: allow different subsets for L1 and L2 regularization?
v0.4,TODO: any better way to deal with sparsity?
v0.4,TODO: any better way to deal with sparsity?
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,Estimators
v0.4,Causal tree parameters
v0.4,Tree structure
v0.4,No need for a random split since the data is already
v0.4,a random subsample from the original input
v0.4,node list stores the nodes that are yet to be splitted
v0.4,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.4,Create local sample set
v0.4,Compute nuisance estimates for the current node
v0.4,Nuisance estimate cannot be calculated
v0.4,Estimate parameter for current node
v0.4,Node estimate cannot be calculated
v0.4,Calculate moments and gradient of moments for current data
v0.4,Calculate inverse gradient
v0.4,The gradient matrix is not invertible.
v0.4,No good split can be found
v0.4,Calculate point-wise pseudo-outcomes rho
v0.4,a split is determined by a feature and a sample pair
v0.4,the number of possible splits is at most (number of features) * (number of node samples)
v0.4,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.4,parse row and column of random pair
v0.4,the sample of the pair is the integer division of the random number with n_feats
v0.4,calculate the binary indicator of whether sample i is on the left or the right
v0.4,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.4,calculate the number of samples on the left child for each proposed split
v0.4,calculate the analogous binary indicator for the samples in the estimation set
v0.4,calculate the number of estimation samples on the left child of each proposed split
v0.4,find the upper and lower bound on the size of the left split for the split
v0.4,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.4,on each side.
v0.4,similarly for the estimation sample set
v0.4,if there is no valid split then don't create any children
v0.4,filter only the valid splits
v0.4,calculate the average influence vector of the samples in the left child
v0.4,calculate the average influence vector of the samples in the right child
v0.4,take the square of each of the entries of the influence vectors and normalize
v0.4,by size of each child
v0.4,calculate the vector score of each candidate split as the average of left and right
v0.4,influence vectors
v0.4,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.4,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.4,where there might be large discontinuities in some parameter as the conditioning set varies
v0.4,calculate the scalar score of each split by aggregating across the vector of scores
v0.4,Find split that minimizes criterion
v0.4,Create child nodes with corresponding subsamples
v0.4,add the created children to the list of not yet split nodes
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,group by product; sum and subtract original; divide by (n_p-1)
v0.4,group by product; sum and subtract original; divide by (n_p-1)
v0.4,"for now, require one feature per store/product combination"
v0.4,TODO: would be nice to relax this somehow
v0.4,"alphas vary by product, not by store"
v0.4,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.4,"one cross-price term per product, which is based on the average price"
v0.4,of all other goods sold at the same store in the same week
v0.4,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.4,store-specific and product-specific gammas and betas (which are positively correlated)
v0.4,"features: product dummies, store dummies"
v0.4,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.4,"observe n_products * n_stores prices, same number of quantities"
v0.4,"for direct regression comparisons, we need a pivoted version"
v0.4,"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,"
v0.4,"plus the same for ""group treatments"" (average treatment of other products in the same store/week)"
v0.4,"for direct regression, we also need to append the features"
v0.4,"(both the ""constant features"" as well as the normal ones)"
v0.4,NOTE: need to set cv because default generic algorithm is super slow for sparse matrices
v0.4,"alphas vary by product, not by store"
v0.4,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.4,store-specific and product-specific gammas
v0.4,store-specific and product-specific betas
v0.4,"features: product dummies, store dummies"
v0.4,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.4,we need only the prices for the compound model; all dummies are created internally
v0.4,"observe n_products * n_stores prices, same number of quantities"
v0.4,"simple results include treatments, plus treatments interacted with product dummies,"
v0.4,for use with the direct methods
v0.4,X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor
v0.4,underspecified model
v0.4,Y = alpha T + \sum_i alpha_i T_i + X beta + eta
v0.4,T = X gamma + eps
v0.4,how to score? distance from line of all solutions?
v0.4,"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error"
v0.4,"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)"
v0.4,"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)"
v0.4,"features: one product dummy, one store dummy (each missing one level), constant"
v0.4,"alphas vary by product, not by store"
v0.4,store-specific and product-specific gammas
v0.4,store-specific and product-specific betas
v0.4,"features: product dummies, store dummies"
v0.4,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.4,"columns: prices interacted with products (and constant), features"
v0.4,"observe n_products * n_stores prices, same number of quantities"
v0.4,use features starting at index 1+n_products to skip all prices
v0.4,"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')"
v0.4,"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)"
v0.4,pickleFile.close()
v0.4,#############################################
v0.4,Defining the parameters of Monte Carlo
v0.4,#############################################
v0.4,Estimation parameters
v0.4,###################################################################
v0.4,Estimating the parameters of the DGP with DML. Running multiple
v0.4,Monte Carlo experiments.
v0.4,###################################################################
v0.4,Sparse coefficients of treatment as a function of co-variates
v0.4,Coefficients of outcomes as a function of co-variates
v0.4,"DGP. Create samples of data (y, T, X) from known truth"
v0.4,DML Estimation.
v0.4,Estimation with other methods for comparison
v0.4,#########################################################
v0.4,Plotting the results and saving
v0.4,#########################################################
v0.4,"plt.figure(figsize=(20, 10))"
v0.4,"plt.subplot(1, 4, 1)"
v0.4,"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))"
v0.4,plt.hist(dml_r2score)
v0.4,"plt.subplot(1, 4, 2)"
v0.4,"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),"
v0.4,np.mean(direct_r2score)))
v0.4,plt.hist(direct_r2score)
v0.4,"plt.subplot(1, 4, 3)"
v0.4,"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),"
v0.4,np.std(dml_te)))
v0.4,plt.hist(np.array(dml_te).flatten())
v0.4,"plt.subplot(1, 4, 4)"
v0.4,"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),"
v0.4,np.std(direct_te)))
v0.4,plt.hist(np.array(direct_te).flatten())
v0.4,plt.tight_layout()
v0.4,"plt.savefig(""r2_comparison.png"")"
v0.4,Plotting the results and saving
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,TODO: Add a __dir__ implementation?
v0.4,TODO: what if some args can be None?
v0.4,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.4,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.4,then we should be computing a confidence interval for the wrapped calls
v0.4,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.4,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.4,Remove children with nonwhite mothers from the treatment group
v0.4,Remove children with nonwhite mothers from the treatment group
v0.4,Select columns
v0.4,Scale the numeric variables
v0.4,"Change the binary variable 'first' takes values in {1,2}"
v0.4,Append a column of ones as intercept
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.4,creating notebooks that are annoying for our users to actually run themselves
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,TODO: make this test actually test something instead of generating images
v0.4,Sparse coefficients of treatment as a function of co-variates
v0.4,Coefficients of outcomes as a function of co-variates
v0.4,"DGP. Create samples of data (y, T, X) from known truth"
v0.4,"DGP. Create samples of data (y, T, X) from known truth"
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,simple DGP only for illustration
v0.4,Define the treatment model neural network architecture
v0.4,"This will take the concatenation of one-dimensional values z and x as input,"
v0.4,"so the input shape is (d_z + d_x,)"
v0.4,The exact shape of the final layer is not critical because the Deep IV framework will
v0.4,add extra layers on top for the mixture density network
v0.4,Define the response model neural network architecture
v0.4,"This will take the concatenation of one-dimensional values t and x as input,"
v0.4,"so the input shape is (d_t + d_x,)"
v0.4,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.4,"NOTE: For the response model, it is important to define the model *outside*"
v0.4,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.4,so that the same weights will be reused in each instantiation
v0.4,number of samples to use in second estimate of the response
v0.4,(to make loss estimate unbiased)
v0.4,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.4,do something with predictions...
v0.4,also test vector t and y
v0.4,simple DGP only for illustration
v0.4,Define the treatment model neural network architecture
v0.4,"This will take the concatenation of one-dimensional values z and x as input,"
v0.4,"so the input shape is (d_z + d_x,)"
v0.4,The exact shape of the final layer is not critical because the Deep IV framework will
v0.4,add extra layers on top for the mixture density network
v0.4,Define the response model neural network architecture
v0.4,"This will take the concatenation of one-dimensional values t and x as input,"
v0.4,"so the input shape is (d_t + d_x,)"
v0.4,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.4,"NOTE: For the response model, it is important to define the model *outside*"
v0.4,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.4,so that the same weights will be reused in each instantiation
v0.4,number of samples to use in second estimate of the response
v0.4,(to make loss estimate unbiased)
v0.4,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.4,do something with predictions...
v0.4,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.4,test = True ensures we draw test set images
v0.4,test = True ensures we draw test set images
v0.4,re-draw to get new independent treatment and implied response
v0.4,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.4,above is necesary so that reduced form doesn't win
v0.4,covariates: time and emotion
v0.4,random instrument
v0.4,z -> price
v0.4,true observable demand function
v0.4,errors
v0.4,response
v0.4,test = True ensures we draw test set images
v0.4,test = True ensures we draw test set images
v0.4,re-draw to get new independent treatment and implied response
v0.4,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.4,above is necesary so that reduced form doesn't win
v0.4,covariates: time and emotion
v0.4,random instrument
v0.4,z -> price
v0.4,true observable demand function
v0.4,errors
v0.4,response
v0.4,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.4,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.4,For some reason this doesn't work at all when run against the CNTK backend...
v0.4,"model.compile('nadam', loss=lambda _,l:l)"
v0.4,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.4,generate a valiation set
v0.4,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.4,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,test that we can fit with the same arguments as the base estimator
v0.4,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.4,with the same shape for the lower and upper bounds
v0.4,test that the lower and upper bounds differ
v0.4,test that we can do the same thing once we provide percentile bounds
v0.4,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.4,with the same shape for the lower and upper bounds
v0.4,test that we can do the same thing once we provide percentile bounds
v0.4,test that we can fit with the same arguments as the base estimator
v0.4,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.4,with the same shape for the lower and upper bounds
v0.4,test that we can do the same thing once we provide percentile bounds
v0.4,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.4,with the same shape for the lower and upper bounds
v0.4,test that we can do the same thing once we provide percentile bounds
v0.4,test that we can fit with the same arguments as the base estimator
v0.4,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.4,with the same shape for the lower and upper bounds
v0.4,test that the lower and upper bounds differ
v0.4,test that we can do the same thing once we provide percentile bounds
v0.4,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.4,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.4,with the same shape for the lower and upper bounds
v0.4,test that we can do the same thing once we provide percentile bounds
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,number of inputs in specification must match number of inputs
v0.4,must have an output
v0.4,output indices must be unique
v0.4,output indices must be present in an input
v0.4,number of indices must match number of dimensions for each input
v0.4,repeated indices must always have consistent sizes
v0.4,transpose
v0.4,tensordot
v0.4,trace
v0.4,TODO: set up proper flag for this
v0.4,pick indices at random with replacement from the first 7 letters of the alphabet
v0.4,"of all of the distinct indices that appear in any input,"
v0.4,pick a random subset of them (of size at most 5) to appear in the output
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,Preprocess data
v0.4,Convert 'week' to a date
v0.4,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.4,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.4,Take log of price
v0.4,Make brand numeric
v0.4,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.4,which have all zero coeffs
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.4,"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.4,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,Set random seed
v0.4,Generate data
v0.4,DGP constants
v0.4,Test data
v0.4,Constant treatment effect and propensity
v0.4,Heterogeneous treatment and propensity
v0.4,TLearner test
v0.4,Instantiate TLearner
v0.4,Test inputs
v0.4,Test constant treatment effect
v0.4,Test heterogeneous treatment effect
v0.4,Instantiate SLearner
v0.4,Test inputs
v0.4,Test constant treatment effect
v0.4,Test heterogeneous treatment effect
v0.4,Need interactions between T and features
v0.4,Instantiate XLearner
v0.4,Test inputs
v0.4,Test constant treatment effect
v0.4,Test heterogeneous treatment effect
v0.4,Instantiate DomainAdaptationLearner
v0.4,Test inputs
v0.4,Test constant treatment effect
v0.4,Test heterogeneous treatment effect
v0.4,Instantiate DomainAdaptationLearner
v0.4,Test inputs
v0.4,Test constant treatment effect
v0.4,Test heterogeneous treatment effect
v0.4,Test heterogenous treatment effect for W =/= None
v0.4,Fit learner and get the effect
v0.4,Get the true treatment effect
v0.4,Compute treatment effect residuals (absolute)
v0.4,Check that at least 90% of predictions are within tolerance interval
v0.4,Only for heterogeneous TE
v0.4,Fit learner on X and W and get the effect
v0.4,Get the true treatment effect
v0.4,Compute treatment effect residuals (absolute)
v0.4,Check that at least 90% of predictions are within tolerance interval
v0.4,Check that one can pass in regular lists
v0.4,Check that it fails correctly if lists of different shape are passed in
v0.4,Check that it fails when T contains values other than 0 and 1
v0.4,"Check that it works when T, Y have shape (n, 1)"
v0.4,Generate covariates
v0.4,Generate treatment
v0.4,Calculate outcome
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,DGP constants
v0.4,Generate data
v0.4,Test data
v0.4,Remove warnings that might be raised by the models passed into the ORF
v0.4,Generate data with continuous treatments
v0.4,Instantiate model with most of the default parameters
v0.4,Test inputs for continuous treatments
v0.4,--> Check that one can pass in regular lists
v0.4,--> Check that it fails correctly if lists of different shape are passed in
v0.4,Check that outputs have the correct shape
v0.4,Test continuous treatments with controls
v0.4,Test continuous treatments without controls
v0.4,Generate data with binary treatments
v0.4,Instantiate model with default params
v0.4,Test inputs for binary treatments
v0.4,--> Check that one can pass in regular lists
v0.4,--> Check that it fails correctly if lists of different shape are passed in
v0.4,"--> Check that it works when T, Y have shape (n, 1)"
v0.4,"--> Check that it fails correctly when T has shape (n, 2)"
v0.4,--> Check that it fails correctly when the treatments are not numeric
v0.4,Check that outputs have the correct shape
v0.4,Test binary treatments with controls
v0.4,Test binary treatments without controls
v0.4,Only applicable to continuous treatments
v0.4,Generate data for 2 treatments
v0.4,Test multiple treatments with controls
v0.4,Compute the treatment effect on test points
v0.4,Compute treatment effect residuals
v0.4,Multiple treatments
v0.4,Allow at most 10% test points to be outside of the tolerance interval
v0.4,Copyright (c) Microsoft Corporation. All rights reserved.
v0.4,Licensed under the MIT License.
v0.4,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.4,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.4,just make sure we can call the marginal_effect and effect methods
v0.4,"for vector-valued T, verify that default scalar T0 and T1 work"
v0.4,create a simple artificial setup where effect of moving from treatment
v0.4,"1 -> 2 is 2,"
v0.4,"1 -> 3 is 1, and"
v0.4,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.4,"Using an uneven number of examples from different classes,"
v0.4,"and having the treatments in non-lexicographic order,"
v0.4,Should rule out some basic issues.
v0.4,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.4,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.4,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.4,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.4,TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong
v0.4,sparse test case: heterogeneous effect by product
v0.4,need at least as many rows in e_y as there are distinct columns
v0.4,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.4,note that this would fail for the non-sparse DMLCateEstimator
v0.3,configuration is all pulled from setup.cfg
v0.3,-*- coding: utf-8 -*-
v0.3,
v0.3,Configuration file for the Sphinx documentation builder.
v0.3,
v0.3,This file does only contain a selection of the most common options. For a
v0.3,full list see the documentation:
v0.3,http://www.sphinx-doc.org/en/master/config
v0.3,-- Path setup --------------------------------------------------------------
v0.3,"If extensions (or modules to document with autodoc) are in another directory,"
v0.3,add these directories to sys.path here. If the directory is relative to the
v0.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.3,
v0.3,-- Project information -----------------------------------------------------
v0.3,-- General configuration ---------------------------------------------------
v0.3,"If your documentation needs a minimal Sphinx version, state it here."
v0.3,
v0.3,needs_sphinx = '1.0'
v0.3,"Add any Sphinx extension module names here, as strings. They can be"
v0.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.3,ones.
v0.3,"Add any paths that contain templates here, relative to this directory."
v0.3,The suffix(es) of source filenames.
v0.3,You can specify multiple suffix as a list of string:
v0.3,
v0.3,"source_suffix = ['.rst', '.md']"
v0.3,The master toctree document.
v0.3,The language for content autogenerated by Sphinx. Refer to documentation
v0.3,for a list of supported languages.
v0.3,
v0.3,This is also used if you do content translation via gettext catalogs.
v0.3,"Usually you set ""language"" from the command line for these cases."
v0.3,"List of patterns, relative to source directory, that match files and"
v0.3,directories to ignore when looking for source files.
v0.3,This pattern also affects html_static_path and html_extra_path.
v0.3,The name of the Pygments (syntax highlighting) style to use.
v0.3,-- Options for HTML output -------------------------------------------------
v0.3,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.3,a list of builtin themes.
v0.3,
v0.3,Theme options are theme-specific and customize the look and feel of a theme
v0.3,"further.  For a list of options available for each theme, see the"
v0.3,documentation.
v0.3,
v0.3,"Add any paths that contain custom static files (such as style sheets) here,"
v0.3,"relative to this directory. They are copied after the builtin static files,"
v0.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.3,html_static_path = ['_static']
v0.3,"Custom sidebar templates, must be a dictionary that maps document names"
v0.3,to template names.
v0.3,
v0.3,The default sidebars (for documents that don't match any pattern) are
v0.3,defined by theme itself.  Builtin themes are using these templates by
v0.3,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.3,'searchbox.html']``.
v0.3,
v0.3,html_sidebars = {}
v0.3,-- Options for HTMLHelp output ---------------------------------------------
v0.3,Output file base name for HTML help builder.
v0.3,-- Options for LaTeX output ------------------------------------------------
v0.3,The paper size ('letterpaper' or 'a4paper').
v0.3,
v0.3,"'papersize': 'letterpaper',"
v0.3,"The font size ('10pt', '11pt' or '12pt')."
v0.3,
v0.3,"'pointsize': '10pt',"
v0.3,Additional stuff for the LaTeX preamble.
v0.3,
v0.3,"'preamble': '',"
v0.3,Latex figure (float) alignment
v0.3,
v0.3,"'figure_align': 'htbp',"
v0.3,Grouping the document tree into LaTeX files. List of tuples
v0.3,"(source start file, target name, title,"
v0.3,"author, documentclass [howto, manual, or own class])."
v0.3,-- Options for manual page output ------------------------------------------
v0.3,One entry per manual page. List of tuples
v0.3,"(source start file, name, description, authors, manual section)."
v0.3,-- Options for Texinfo output ----------------------------------------------
v0.3,Grouping the document tree into Texinfo files. List of tuples
v0.3,"(source start file, target name, title, author,"
v0.3,"dir menu entry, description, category)"
v0.3,-- Options for Epub output -------------------------------------------------
v0.3,Bibliographic Dublin Core info.
v0.3,The unique identifier of the text. This can be a ISBN number
v0.3,or the project homepage.
v0.3,
v0.3,epub_identifier = ''
v0.3,A unique identification for the text.
v0.3,
v0.3,epub_uid = ''
v0.3,A list of files that should not be packed into the epub file.
v0.3,-- Extension configuration -------------------------------------------------
v0.3,-- Options for intersphinx extension ---------------------------------------
v0.3,Example configuration for intersphinx: refer to the Python standard library.
v0.3,-- Options for todo extension ----------------------------------------------
v0.3,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.3,##################
v0.3,Global settings #
v0.3,##################
v0.3,Global plotting controls
v0.3,"Control for support size, can control for more"
v0.3,#################
v0.3,File utilities #
v0.3,#################
v0.3,#################
v0.3,Plotting utils #
v0.3,#################
v0.3,bias
v0.3,var
v0.3,rmse
v0.3,r2
v0.3,Infer feature dimension
v0.3,Metrics by support plots
v0.3,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.3,Vasilis Syrgkanis <vasy@microsoft.com>
v0.3,Steven Wu <zhiww@microsoft.com>
v0.3,Initialize causal tree parameters
v0.3,Create splits of causal tree
v0.3,Estimate treatment effects at the leafs
v0.3,Compute heterogeneous treatement effect for x's in x_list by finding
v0.3,the corresponding split and associating the effect computed on that leaf
v0.3,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.3,Safety check
v0.3,Weighted linear regression
v0.3,Calculates weights
v0.3,Bootstraping has repetitions in tree sample so we need to iterate
v0.3,over all indices
v0.3,Similar for `a` weights
v0.3,Doesn't have sample weights
v0.3,Is a linear model
v0.3,Weighted linear regression
v0.3,Calculates weights
v0.3,Bootstraping has repetitions in tree sample so we need to iterate
v0.3,over all indices
v0.3,Similar for `a` weights
v0.3,normalize weights
v0.3,"Split the data in half, train and test"
v0.3,Fit with LassoCV the treatment as a function of W and the outcome as
v0.3,"a function of W, using only the train fold"
v0.3,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.3,"Split the data in half, train and test"
v0.3,Fit with LassoCV the treatment as a function of W and the outcome as
v0.3,"a function of W, using only the train fold"
v0.3,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.3,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.3,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.3,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.3,"Split the data in half, train and test"
v0.3,Fit with LassoCV the treatment as a function of x and the outcome as
v0.3,"a function of x, using only the train fold"
v0.3,Then compute residuals p-g(x) and q-q(x) on test fold
v0.3,Compute coefficient by OLS on residuals
v0.3,"Split the data in half, train and test"
v0.3,Fit with LassoCV the treatment as a function of x and the outcome as
v0.3,"a function of x, using only the train fold"
v0.3,Then compute residuals p-g(x) and q-q(x) on test fold
v0.3,Estimate multipliers for second order orthogonal method
v0.3,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.3,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.3,Create local sample set
v0.3,compute the base estimate for the current node using double ml or second order double ml
v0.3,compute the influence functions here that are used for the criterion
v0.3,generate random proposals of dimensions to split
v0.3,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.3,compute criterion for each proposal
v0.3,if splitting creates valid leafs in terms of mean leaf size
v0.3,Calculate criterion for split
v0.3,Else set criterion to infinity so that this split is not chosen
v0.3,If no good split was found
v0.3,Find split that minimizes criterion
v0.3,Set the split attributes at the node
v0.3,Create child nodes with corresponding subsamples
v0.3,Recursively split children
v0.3,Return parent node
v0.3,estimate the local parameter at the leaf using the estimate data
v0.3,###################
v0.3,Argument parsing #
v0.3,###################
v0.3,#########################################
v0.3,Parameters constant across experiments #
v0.3,#########################################
v0.3,Outcome support
v0.3,Treatment support
v0.3,Evaluation grid
v0.3,Treatment effects array
v0.3,Other variables
v0.3,##########################
v0.3,Data Generating Process #
v0.3,##########################
v0.3,Log iteration
v0.3,"Generate controls, features, treatment and outcome"
v0.3,T and Y residuals to be used in later scripts
v0.3,Save generated dataset
v0.3,#################
v0.3,ORF parameters #
v0.3,#################
v0.3,######################################
v0.3,Train and evaluate treatment effect #
v0.3,######################################
v0.3,########
v0.3,Plots #
v0.3,########
v0.3,###############
v0.3,Save results #
v0.3,###############
v0.3,##############
v0.3,Run Rscript #
v0.3,##############
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,Check inputs
v0.3,Check inputs
v0.3,Check inputs
v0.3,Check inputs
v0.3,Check inputs
v0.3,Check inputs
v0.3,Check inputs
v0.3,Train model on controls. Assign higher weight to units resembling
v0.3,treated units.
v0.3,Train model on the treated. Assign higher weight to units resembling
v0.3,control units.
v0.3,Check inputs
v0.3,Check inputs
v0.3,Fit outcome model on X||W||T (concatenated)
v0.3,Check inputs
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,Create splits of causal tree
v0.3,Must make sure indices are merged correctly
v0.3,Require group assignment t to be one-hot-encoded
v0.3,Define an inner function that iterates over group predictions
v0.3,Convert rows to columns
v0.3,Get predictions for the 2 splits
v0.3,Must make sure indices are merged correctly
v0.3,Estimators
v0.3,OrthoForest parameters
v0.3,Sub-forests
v0.3,Fit check
v0.3,TODO: Check performance
v0.3,Must normalize weights
v0.3,Crossfitting
v0.3,Compute weighted nuisance estimates
v0.3,Generate subsample indices
v0.3,Safety check
v0.3,Build trees in parallel
v0.3,Calculates weights
v0.3,Bootstraping has repetitions in tree sample
v0.3,Similar for `a` weights
v0.3,Bootstraping has repetitions in tree sample
v0.3,Copy and/or define models
v0.3,Define nuisance estimators
v0.3,Define parameter estimators
v0.3,Define
v0.3,Nuissance estimates evaluated with cross-fitting
v0.3,Define 2-fold iterator
v0.3,need safe=False when cloning for WeightedModelWrapper
v0.3,Compute residuals
v0.3,Compute coefficient by OLS on residuals
v0.3,"Parameter returned by LinearRegression is (d_T, )"
v0.3,Compute residuals
v0.3,Compute coefficient by OLS on residuals
v0.3,ell_2 regularization
v0.3,Ridge regression estimate
v0.3,"Parameter returned by LinearRegression is (d_T, )"
v0.3,Return moments and gradients
v0.3,Compute residuals
v0.3,Compute moments
v0.3,"Moments shape is (n, d_T)"
v0.3,Compute moment gradients
v0.3,Copy and/or define models
v0.3,Nuisance estimators shall be defined during fitting because they need to know the number of distinct
v0.3,treatments
v0.3,Define parameter estimators
v0.3,Define moment and mean gradient estimator
v0.3,Define autoencoder
v0.3,"Check that T is shape (n, )"
v0.3,Check T is numeric
v0.3,Train label encoder
v0.3,Define number of classes
v0.3,Call `fit` from parent class
v0.3,"Test that T contains all treatments. If not, return None"
v0.3,Nuissance estimates evaluated with cross-fitting
v0.3,Define 2-fold iterator
v0.3,Check if there is only one example of some class
v0.3,No need to crossfit for internal nodes
v0.3,Compute partial moments
v0.3,"If any of the values in the parameter estimate is nan, return None"
v0.3,Compute partial moments
v0.3,Compute coefficient by OLS on residuals
v0.3,ell_2 regularization
v0.3,Ridge regression estimate
v0.3,"Parameter returned by LinearRegression is (d_T, )"
v0.3,Return moments and gradients
v0.3,Compute partial moments
v0.3,Compute moments
v0.3,"Moments shape is (n, d_T-1)"
v0.3,Compute moment gradients
v0.3,Need to calculate this in an elegant way for when propensity is 0
v0.3,This will flatten T
v0.3,Check that T is numeric
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector..."
v0.3,"Do we want to reshape to an nx1, or just trust the user's choice of input?"
v0.3,(Likewise for Y below)
v0.3,need to override effect in case of discrete treatments
v0.3,TODO: should this logic be moved up to the LinearCateEstimator class and
v0.3,removed from here and from the OrthoForest implementation?
v0.3,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.3,create an identity matrix of size d_t (or just a 1-element array if T was a vector)
v0.3,the nth row will allow us to compute the marginal effect of the nth component of treatment
v0.3,TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called
v0.3,"rather than just using coef_ seems silly, but one benefit is that we can use linear models"
v0.3,that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models)
v0.3,TODO: handle case where final model doesn't directly expose coef_?
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,#######################################
v0.3,Core DML Tests
v0.3,#######################################
v0.3,How many samples
v0.3,How many control features
v0.3,How many treatment variables
v0.3,Coefficients of how controls affect treatments
v0.3,Coefficients of how controls affect outcome
v0.3,Treatment effects that we want to estimate
v0.3,Run dml estimation
v0.3,How many samples
v0.3,How many control features
v0.3,How many treatment variables
v0.3,Coefficients of how controls affect treatments
v0.3,Coefficients of how controls affect outcome
v0.3,Treatment effects that we want to estimate
v0.3,Run dml estimation
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,"this will have dimension (d,) + shape(X)"
v0.3,send the first dimension to the end
v0.3,columns are featurized independently; partial derivatives are only non-zero
v0.3,when taken with respect to the same column each time
v0.3,don't fit intercept; manually add column of ones to the data instead;
v0.3,this allows us to ignore the intercept when computing marginal effects
v0.3,two stage approximation
v0.3,"first, get basis expansions of T, X, and Z"
v0.3,"regress T expansion on X,Z expansions"
v0.3,"predict ft_T from interacted ft_X, ft_Z"
v0.3,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.3,dT may be only 2-dimensional)
v0.3,promote dT to 3D if necessary (e.g. if T was a vector)
v0.3,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,TODO: make sure to use random seeds wherever necessary
v0.3,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.3,"unfortunately with the Theano and Tensorflow backends,"
v0.3,the straightforward use of K.stop_gradient can cause an error
v0.3,because the parameters of the intermediate layers are now disconnected from the loss;
v0.3,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.3,so that those layers remain connected but with 0 gradient
v0.3,|| t - mu_i || ^2
v0.3,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.3,Use logsumexp for numeric stability:
v0.3,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.3,TODO: does the numeric stability actually make any difference?
v0.3,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.3,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.3,generate cumulative sum via matrix multiplication
v0.3,"Generate standard uniform values in shape (batch_size,1)"
v0.3,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.3,we use uniform_like instead with an input of an appropriate shape)
v0.3,convert to floats and multiply to perform equivalent of logical AND
v0.3,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.3,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.3,we use normal_like instead with an input of an appropriate shape)
v0.3,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.3,prevent gradient from passing through sampling
v0.3,three options: biased or upper-bound loss require a single number of samples;
v0.3,unbiased can take different numbers for the network and its gradient
v0.3,"sample: (() -> Layer, int) -> Layer"
v0.3,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.3,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.3,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.3,TODO: allow 1D arguments for Y and T
v0.3,the dimensionality of the output of the network
v0.3,TODO: is there a more robust way to do this?
v0.3,TODO: do we need to give the user more control over other arguments to fit?
v0.3,"subtle point: we need to build a new model each time,"
v0.3,because each model encapsulates its randomness
v0.3,TODO: do we need to give the user more control over other arguments to fit?
v0.3,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.3,not a general tensor (because of how backprop works in every framework)
v0.3,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.3,but this seems annoying...)
v0.3,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.3,TODO: any way to get this to work on batches of arbitrary size?
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,#######################################################
v0.3,Perfect Data DGPs for Testing Correctness of Code
v0.3,#######################################################
v0.3,Generate random control co-variates
v0.3,Create epsilon residual treatments that deterministically sum up to
v0.3,zero
v0.3,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.3,conditional on each co-variate vector is equal to zero
v0.3,We simply subtract the conditional mean from the epsilons
v0.3,Construct treatments as T = X*A + epsilon
v0.3,Construct outcomes as y = X*beta + T*effect
v0.3,Generate random control co-variates
v0.3,Create epsilon residual treatments that deterministically sum up to
v0.3,zero
v0.3,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.3,conditional on each co-variate vector is equal to zero
v0.3,We simply subtract the conditional mean from the epsilons
v0.3,Construct treatments as T = X*A + epsilon
v0.3,Construct outcomes as y = X*beta + T*effect
v0.3,Generate random control co-variates
v0.3,Construct treatments as T = X*A + epsilon
v0.3,Construct outcomes as y = X*beta + T*effect
v0.3,Generate random control co-variates
v0.3,Create epsilon residual treatments
v0.3,Construct treatments as T = X*A + epsilon
v0.3,Construct outcomes as y = X*beta + T*effect + eta
v0.3,Generate random control co-variates
v0.3,Use the same treatment vector for each row
v0.3,Construct outcomes as y = X*beta + T*effect
v0.3,Licensed under the MIT License.
v0.3,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.3,but tensordot can't be applied to this problem because we don't sum over m
v0.3,"TODO: if T0 or T1 are scalars, we'll promote them to vectors;"
v0.3,should it be possible to promote them to 2D arrays if that's what we saw during training?
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,TODO: any way to avoid creating a copy if the array was already dense?
v0.3,"the call is necessary if the input was something like a list, though"
v0.3,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.3,so convert to pydata sparse first
v0.3,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.3,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.3,TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?
v0.3,(but note that changing this would necessitate changes to callers
v0.3,to switch the order to preserve behavior where order is important)
v0.3,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.3,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.3,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.3,same number of input definitions as arrays
v0.3,input definitions have same number of dimensions as each array
v0.3,all result indices are unique
v0.3,all result indices must match at least one input index
v0.3,"map indices to all array, axis pairs for that index"
v0.3,each index has the same cardinality wherever it appears
v0.3,"State: list of (set of letters, list of (corresponding indices, value))"
v0.3,Algo: while list contains more than one entry
v0.3,take two entries
v0.3,sort both lists by intersection of their indices
v0.3,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.3,"take the union of indices and the product of values), stepping through each list linearly"
v0.3,TODO: might be faster to break into connected components first
v0.3,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.3,"so compute their content separately, then take cartesian product"
v0.3,this would save a few pointless sorts by empty tuples
v0.3,TODO: Consider investigating other performance ideas for these cases
v0.3,where the dense method beat the sparse method (usually sparse is faster)
v0.3,"e,facd,c->cfed"
v0.3,sparse: 0.0335489
v0.3,dense:  0.011465999999999997
v0.3,"gbd,da,egb->da"
v0.3,sparse: 0.0791625
v0.3,dense:  0.007319099999999995
v0.3,"dcc,d,faedb,c->abe"
v0.3,sparse: 1.2868097
v0.3,dense:  0.44605229999999985
v0.3,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.3,TODO: would using einsum's paths to optimize the order of merging help?
v0.3,Normalize weights
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,TODO: allow different subsets for L1 and L2 regularization?
v0.3,TODO: any better way to deal with sparsity?
v0.3,TODO: any better way to deal with sparsity?
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,Estimators
v0.3,Causal tree parameters
v0.3,Tree structure
v0.3,No need for a random split since the data is already
v0.3,a random subsample from the original input
v0.3,node list stores the nodes that are yet to be splitted
v0.3,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.3,Create local sample set
v0.3,Compute nuisance estimates for the current node
v0.3,Nuisance estimate cannot be calculated
v0.3,Estimate parameter for current node
v0.3,Node estimate cannot be calculated
v0.3,Calculate moments and gradient of moments for current data
v0.3,Calculate inverse gradient
v0.3,The gradient matrix is not invertible.
v0.3,No good split can be found
v0.3,Calculate point-wise pseudo-outcomes rho
v0.3,a split is determined by a feature and a sample pair
v0.3,the number of possible splits is at most (number of features) * (number of node samples)
v0.3,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.3,parse row and column of random pair
v0.3,the sample of the pair is the integer division of the random number with n_feats
v0.3,calculate the binary indicator of whether sample i is on the left or the right
v0.3,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.3,calculate the number of samples on the left child for each proposed split
v0.3,calculate the analogous binary indicator for the samples in the estimation set
v0.3,calculate the number of estimation samples on the left child of each proposed split
v0.3,find the upper and lower bound on the size of the left split for the split
v0.3,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.3,on each side.
v0.3,similarly for the estimation sample set
v0.3,if there is no valid split then don't create any children
v0.3,filter only the valid splits
v0.3,calculate the average influence vector of the samples in the left child
v0.3,calculate the average influence vector of the samples in the right child
v0.3,take the square of each of the entries of the influence vectors and normalize
v0.3,by size of each child
v0.3,calculate the vector score of each candidate split as the average of left and right
v0.3,influence vectors
v0.3,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.3,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.3,where there might be large discontinuities in some parameter as the conditioning set varies
v0.3,calculate the scalar score of each split by aggregating across the vector of scores
v0.3,Find split that minimizes criterion
v0.3,Create child nodes with corresponding subsamples
v0.3,add the created children to the list of not yet split nodes
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,group by product; sum and subtract original; divide by (n_p-1)
v0.3,group by product; sum and subtract original; divide by (n_p-1)
v0.3,"for now, require one feature per store/product combination"
v0.3,TODO: would be nice to relax this somehow
v0.3,"alphas vary by product, not by store"
v0.3,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.3,"one cross-price term per product, which is based on the average price"
v0.3,of all other goods sold at the same store in the same week
v0.3,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.3,store-specific and product-specific gammas and betas (which are positively correlated)
v0.3,"features: product dummies, store dummies"
v0.3,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.3,"observe n_products * n_stores prices, same number of quantities"
v0.3,"for direct regression comparisons, we need a pivoted version"
v0.3,"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,"
v0.3,"plus the same for ""group treatments"" (average treatment of other products in the same store/week)"
v0.3,"for direct regression, we also need to append the features"
v0.3,"(both the ""constant features"" as well as the normal ones)"
v0.3,NOTE: need to set cv because default generic algorithm is super slow for sparse matrices
v0.3,"alphas vary by product, not by store"
v0.3,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.3,store-specific and product-specific gammas
v0.3,store-specific and product-specific betas
v0.3,"features: product dummies, store dummies"
v0.3,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.3,we need only the prices for the compound model; all dummies are created internally
v0.3,"observe n_products * n_stores prices, same number of quantities"
v0.3,"simple results include treatments, plus treatments interacted with product dummies,"
v0.3,for use with the direct methods
v0.3,X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor
v0.3,underspecified model
v0.3,Y = alpha T + \sum_i alpha_i T_i + X beta + eta
v0.3,T = X gamma + eps
v0.3,how to score? distance from line of all solutions?
v0.3,"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error"
v0.3,"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)"
v0.3,"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)"
v0.3,"features: one product dummy, one store dummy (each missing one level), constant"
v0.3,"alphas vary by product, not by store"
v0.3,store-specific and product-specific gammas
v0.3,store-specific and product-specific betas
v0.3,"features: product dummies, store dummies"
v0.3,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.3,"columns: prices interacted with products (and constant), features"
v0.3,"observe n_products * n_stores prices, same number of quantities"
v0.3,use features starting at index 1+n_products to skip all prices
v0.3,"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')"
v0.3,"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)"
v0.3,pickleFile.close()
v0.3,#############################################
v0.3,Defining the parameters of Monte Carlo
v0.3,#############################################
v0.3,Estimation parameters
v0.3,###################################################################
v0.3,Estimating the parameters of the DGP with DML. Running multiple
v0.3,Monte Carlo experiments.
v0.3,###################################################################
v0.3,Sparse coefficients of treatment as a function of co-variates
v0.3,Coefficients of outcomes as a function of co-variates
v0.3,"DGP. Create samples of data (y, T, X) from known truth"
v0.3,DML Estimation.
v0.3,Estimation with other methods for comparison
v0.3,#########################################################
v0.3,Plotting the results and saving
v0.3,#########################################################
v0.3,"plt.figure(figsize=(20, 10))"
v0.3,"plt.subplot(1, 4, 1)"
v0.3,"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))"
v0.3,plt.hist(dml_r2score)
v0.3,"plt.subplot(1, 4, 2)"
v0.3,"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),"
v0.3,np.mean(direct_r2score)))
v0.3,plt.hist(direct_r2score)
v0.3,"plt.subplot(1, 4, 3)"
v0.3,"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),"
v0.3,np.std(dml_te)))
v0.3,plt.hist(np.array(dml_te).flatten())
v0.3,"plt.subplot(1, 4, 4)"
v0.3,"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),"
v0.3,np.std(direct_te)))
v0.3,plt.hist(np.array(direct_te).flatten())
v0.3,plt.tight_layout()
v0.3,"plt.savefig(""r2_comparison.png"")"
v0.3,Plotting the results and saving
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,TODO: Add a __dir__ implementation?
v0.3,TODO: what if some args can be None?
v0.3,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.3,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.3,then we should be computing a confidence interval for the wrapped calls
v0.3,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.3,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.3,Remove children with nonwhite mothers from the treatment group
v0.3,Remove children with nonwhite mothers from the treatment group
v0.3,Select columns
v0.3,Scale the numeric variables
v0.3,"Change the binary variable 'first' takes values in {1,2}"
v0.3,Append a column of ones as intercept
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.3,creating notebooks that are annoying for our users to actually run themselves
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,TODO: make this test actually test something instead of generating images
v0.3,Sparse coefficients of treatment as a function of co-variates
v0.3,Coefficients of outcomes as a function of co-variates
v0.3,"DGP. Create samples of data (y, T, X) from known truth"
v0.3,"DGP. Create samples of data (y, T, X) from known truth"
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,simple DGP only for illustration
v0.3,Define the treatment model neural network architecture
v0.3,"This will take the concatenation of one-dimensional values z and x as input,"
v0.3,"so the input shape is (d_z + d_x,)"
v0.3,The exact shape of the final layer is not critical because the Deep IV framework will
v0.3,add extra layers on top for the mixture density network
v0.3,Define the response model neural network architecture
v0.3,"This will take the concatenation of one-dimensional values t and x as input,"
v0.3,"so the input shape is (d_t + d_x,)"
v0.3,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.3,"NOTE: For the response model, it is important to define the model *outside*"
v0.3,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.3,so that the same weights will be reused in each instantiation
v0.3,number of samples to use in second estimate of the response
v0.3,(to make loss estimate unbiased)
v0.3,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.3,do something with predictions...
v0.3,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.3,test = True ensures we draw test set images
v0.3,test = True ensures we draw test set images
v0.3,re-draw to get new independent treatment and implied response
v0.3,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.3,above is necesary so that reduced form doesn't win
v0.3,covariates: time and emotion
v0.3,random instrument
v0.3,z -> price
v0.3,true observable demand function
v0.3,errors
v0.3,response
v0.3,test = True ensures we draw test set images
v0.3,test = True ensures we draw test set images
v0.3,re-draw to get new independent treatment and implied response
v0.3,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.3,above is necesary so that reduced form doesn't win
v0.3,covariates: time and emotion
v0.3,random instrument
v0.3,z -> price
v0.3,true observable demand function
v0.3,errors
v0.3,response
v0.3,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.3,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.3,For some reason this doesn't work at all when run against the CNTK backend...
v0.3,"model.compile('nadam', loss=lambda _,l:l)"
v0.3,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.3,generate a valiation set
v0.3,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.3,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,test that we can fit with the same arguments as the base estimator
v0.3,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.3,with the same shape for the lower and upper bounds
v0.3,test that we can do the same thing once we provide percentile bounds
v0.3,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.3,with the same shape for the lower and upper bounds
v0.3,test that we can do the same thing once we provide percentile bounds
v0.3,test that we can fit with the same arguments as the base estimator
v0.3,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.3,with the same shape for the lower and upper bounds
v0.3,test that we can do the same thing once we provide percentile bounds
v0.3,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.3,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.3,with the same shape for the lower and upper bounds
v0.3,test that we can do the same thing once we provide percentile bounds
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,number of inputs in specification must match number of inputs
v0.3,must have an output
v0.3,output indices must be unique
v0.3,output indices must be present in an input
v0.3,number of indices must match number of dimensions for each input
v0.3,repeated indices must always have consistent sizes
v0.3,transpose
v0.3,tensordot
v0.3,trace
v0.3,TODO: set up proper flag for this
v0.3,pick indices at random with replacement from the first 7 letters of the alphabet
v0.3,"of all of the distinct indices that appear in any input,"
v0.3,pick a random subset of them (of size at most 5) to appear in the output
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,Preprocess data
v0.3,Convert 'week' to a date
v0.3,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.3,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.3,Take log of price
v0.3,Make brand numeric
v0.3,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.3,which have all zero coeffs
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.3,"first derivitaves are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.3,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,Set random seed
v0.3,Generate data
v0.3,DGP constants
v0.3,Test data
v0.3,Constant treatment effect and propensity
v0.3,Heterogeneous treatment and propensity
v0.3,TLearner test
v0.3,Instantiate TLearner
v0.3,Test inputs
v0.3,Test constant treatment effect
v0.3,Test heterogeneous treatment effect
v0.3,Instantiate SLearner
v0.3,Test inputs
v0.3,Test constant treatment effect
v0.3,Test heterogeneous treatment effect
v0.3,Need interactions between T and features
v0.3,Instantiate XLearner
v0.3,Test inputs
v0.3,Test constant treatment effect
v0.3,Test heterogeneous treatment effect
v0.3,Instantiate DomainAdaptationLearner
v0.3,Test inputs
v0.3,Test constant treatment effect
v0.3,Test heterogeneous treatment effect
v0.3,Instantiate DomainAdaptationLearner
v0.3,Test inputs
v0.3,Test constant treatment effect
v0.3,Test heterogeneous treatment effect
v0.3,Test heterogenous treatment effect for W =/= None
v0.3,Fit learner and get the effect
v0.3,Get the true treatment effect
v0.3,Compute treatment effect residuals (absolute)
v0.3,Check that at least 90% of predictions are within tolerance interval
v0.3,Only for heterogeneous TE
v0.3,Fit learner on X and W and get the effect
v0.3,Get the true treatment effect
v0.3,Compute treatment effect residuals (absolute)
v0.3,Check that at least 90% of predictions are within tolerance interval
v0.3,Check that one can pass in regular lists
v0.3,Check that it fails correctly if lists of different shape are passed in
v0.3,Check that it fails when T contains values other than 0 and 1
v0.3,"Check that it works when T, Y have shape (n, 1)"
v0.3,Generate covariates
v0.3,Generate treatment
v0.3,Calculate outcome
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,DGP constants
v0.3,Generate data
v0.3,Test data
v0.3,Remove warnings that might be raised by the models passed into the ORF
v0.3,Generate data with continuous treatments
v0.3,Instantiate model with most of the default parameters
v0.3,Test inputs for continuous treatments
v0.3,--> Check that one can pass in regular lists
v0.3,--> Check that it fails correctly if lists of different shape are passed in
v0.3,Check that outputs have the correct shape
v0.3,Test continuous treatments with controls
v0.3,Test continuous treatments without controls
v0.3,Generate data with binary treatments
v0.3,Instantiate model with default params
v0.3,Test inputs for binary treatments
v0.3,--> Check that one can pass in regular lists
v0.3,--> Check that it fails correctly if lists of different shape are passed in
v0.3,"--> Check that it works when T, Y have shape (n, 1)"
v0.3,"--> Check that it fails correctly when T has shape (n, 2)"
v0.3,--> Check that it fails correctly when the treatments are not numeric
v0.3,Check that outputs have the correct shape
v0.3,Test binary treatments with controls
v0.3,Test binary treatments without controls
v0.3,Only applicable to continuous treatments
v0.3,Generate data for 2 treatments
v0.3,Test multiple treatments with controls
v0.3,Compute the treatment effect on test points
v0.3,Compute treatment effect residuals
v0.3,Multiple treatments
v0.3,Allow at most 10% test points to be outside of the tolerance interval
v0.3,Copyright (c) Microsoft Corporation. All rights reserved.
v0.3,Licensed under the MIT License.
v0.3,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.3,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.3,just make sure we can call the marginal_effect and effect methods
v0.3,"for vector-valued T, verify that default scalar T0 and T1 work"
v0.3,create a simple artificial setup where effect of moving from treatment
v0.3,"1 -> 2 is 2,"
v0.3,"1 -> 3 is 1, and"
v0.3,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.3,"Using an uneven number of examples from different classes,"
v0.3,"and having the treatments in non-lexicographic order,"
v0.3,Should rule out some basic issues.
v0.3,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.3,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.3,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.3,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.3,TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong
v0.3,sparse test case: heterogeneous effect by product
v0.3,need at least as many rows in e_y as there are distinct columns
v0.3,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.3,note that this would fail for the non-sparse DMLCateEstimator
v0.2,configuration is all pulled from setup.cfg
v0.2,-*- coding: utf-8 -*-
v0.2,
v0.2,Configuration file for the Sphinx documentation builder.
v0.2,
v0.2,This file does only contain a selection of the most common options. For a
v0.2,full list see the documentation:
v0.2,http://www.sphinx-doc.org/en/master/config
v0.2,-- Path setup --------------------------------------------------------------
v0.2,"If extensions (or modules to document with autodoc) are in another directory,"
v0.2,add these directories to sys.path here. If the directory is relative to the
v0.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.2,
v0.2,-- Project information -----------------------------------------------------
v0.2,-- General configuration ---------------------------------------------------
v0.2,"If your documentation needs a minimal Sphinx version, state it here."
v0.2,
v0.2,needs_sphinx = '1.0'
v0.2,"Add any Sphinx extension module names here, as strings. They can be"
v0.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.2,ones.
v0.2,"Add any paths that contain templates here, relative to this directory."
v0.2,The suffix(es) of source filenames.
v0.2,You can specify multiple suffix as a list of string:
v0.2,
v0.2,"source_suffix = ['.rst', '.md']"
v0.2,The master toctree document.
v0.2,The language for content autogenerated by Sphinx. Refer to documentation
v0.2,for a list of supported languages.
v0.2,
v0.2,This is also used if you do content translation via gettext catalogs.
v0.2,"Usually you set ""language"" from the command line for these cases."
v0.2,"List of patterns, relative to source directory, that match files and"
v0.2,directories to ignore when looking for source files.
v0.2,This pattern also affects html_static_path and html_extra_path.
v0.2,The name of the Pygments (syntax highlighting) style to use.
v0.2,-- Options for HTML output -------------------------------------------------
v0.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.2,a list of builtin themes.
v0.2,
v0.2,Theme options are theme-specific and customize the look and feel of a theme
v0.2,"further.  For a list of options available for each theme, see the"
v0.2,documentation.
v0.2,
v0.2,"Add any paths that contain custom static files (such as style sheets) here,"
v0.2,"relative to this directory. They are copied after the builtin static files,"
v0.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.2,html_static_path = ['_static']
v0.2,"Custom sidebar templates, must be a dictionary that maps document names"
v0.2,to template names.
v0.2,
v0.2,The default sidebars (for documents that don't match any pattern) are
v0.2,defined by theme itself.  Builtin themes are using these templates by
v0.2,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.2,'searchbox.html']``.
v0.2,
v0.2,html_sidebars = {}
v0.2,-- Options for HTMLHelp output ---------------------------------------------
v0.2,Output file base name for HTML help builder.
v0.2,-- Options for LaTeX output ------------------------------------------------
v0.2,The paper size ('letterpaper' or 'a4paper').
v0.2,
v0.2,"'papersize': 'letterpaper',"
v0.2,"The font size ('10pt', '11pt' or '12pt')."
v0.2,
v0.2,"'pointsize': '10pt',"
v0.2,Additional stuff for the LaTeX preamble.
v0.2,
v0.2,"'preamble': '',"
v0.2,Latex figure (float) alignment
v0.2,
v0.2,"'figure_align': 'htbp',"
v0.2,Grouping the document tree into LaTeX files. List of tuples
v0.2,"(source start file, target name, title,"
v0.2,"author, documentclass [howto, manual, or own class])."
v0.2,-- Options for manual page output ------------------------------------------
v0.2,One entry per manual page. List of tuples
v0.2,"(source start file, name, description, authors, manual section)."
v0.2,-- Options for Texinfo output ----------------------------------------------
v0.2,Grouping the document tree into Texinfo files. List of tuples
v0.2,"(source start file, target name, title, author,"
v0.2,"dir menu entry, description, category)"
v0.2,-- Options for Epub output -------------------------------------------------
v0.2,Bibliographic Dublin Core info.
v0.2,The unique identifier of the text. This can be a ISBN number
v0.2,or the project homepage.
v0.2,
v0.2,epub_identifier = ''
v0.2,A unique identification for the text.
v0.2,
v0.2,epub_uid = ''
v0.2,A list of files that should not be packed into the epub file.
v0.2,-- Extension configuration -------------------------------------------------
v0.2,-- Options for intersphinx extension ---------------------------------------
v0.2,Example configuration for intersphinx: refer to the Python standard library.
v0.2,-- Options for todo extension ----------------------------------------------
v0.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.2,##################
v0.2,Global settings #
v0.2,##################
v0.2,Global plotting controls
v0.2,"Control for support size, can control for more"
v0.2,#################
v0.2,File utilities #
v0.2,#################
v0.2,#################
v0.2,Plotting utils #
v0.2,#################
v0.2,bias
v0.2,var
v0.2,rmse
v0.2,r2
v0.2,Infer feature dimension
v0.2,Metrics by support plots
v0.2,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.2,Vasilis Syrgkanis <vasy@microsoft.com>
v0.2,Steven Wu <zhiww@microsoft.com>
v0.2,Initialize causal tree parameters
v0.2,Create splits of causal tree
v0.2,Estimate treatment effects at the leafs
v0.2,Compute heterogeneous treatement effect for x's in x_list by finding
v0.2,the corresponding split and associating the effect computed on that leaf
v0.2,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.2,Safety check
v0.2,Weighted linear regression
v0.2,Calculates weights
v0.2,Bootstraping has repetitions in tree sample so we need to iterate
v0.2,over all indices
v0.2,Similar for `a` weights
v0.2,Doesn't have sample weights
v0.2,Is a linear model
v0.2,Weighted linear regression
v0.2,Calculates weights
v0.2,Bootstraping has repetitions in tree sample so we need to iterate
v0.2,over all indices
v0.2,Similar for `a` weights
v0.2,normalize weights
v0.2,"Split the data in half, train and test"
v0.2,Fit with LassoCV the treatment as a function of W and the outcome as
v0.2,"a function of W, using only the train fold"
v0.2,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.2,"Split the data in half, train and test"
v0.2,Fit with LassoCV the treatment as a function of W and the outcome as
v0.2,"a function of W, using only the train fold"
v0.2,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.2,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.2,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.2,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.2,"Split the data in half, train and test"
v0.2,Fit with LassoCV the treatment as a function of x and the outcome as
v0.2,"a function of x, using only the train fold"
v0.2,Then compute residuals p-g(x) and q-q(x) on test fold
v0.2,Compute coefficient by OLS on residuals
v0.2,"Split the data in half, train and test"
v0.2,Fit with LassoCV the treatment as a function of x and the outcome as
v0.2,"a function of x, using only the train fold"
v0.2,Then compute residuals p-g(x) and q-q(x) on test fold
v0.2,Estimate multipliers for second order orthogonal method
v0.2,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.2,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.2,Create local sample set
v0.2,compute the base estimate for the current node using double ml or second order double ml
v0.2,compute the influence functions here that are used for the criterion
v0.2,generate random proposals of dimensions to split
v0.2,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.2,compute criterion for each proposal
v0.2,if splitting creates valid leafs in terms of mean leaf size
v0.2,Calculate criterion for split
v0.2,Else set criterion to infinity so that this split is not chosen
v0.2,If no good split was found
v0.2,Find split that minimizes criterion
v0.2,Set the split attributes at the node
v0.2,Create child nodes with corresponding subsamples
v0.2,Recursively split children
v0.2,Return parent node
v0.2,estimate the local parameter at the leaf using the estimate data
v0.2,###################
v0.2,Argument parsing #
v0.2,###################
v0.2,#########################################
v0.2,Parameters constant across experiments #
v0.2,#########################################
v0.2,Outcome support
v0.2,Treatment support
v0.2,Evaluation grid
v0.2,Treatment effects array
v0.2,Other variables
v0.2,##########################
v0.2,Data Generating Process #
v0.2,##########################
v0.2,Log iteration
v0.2,"Generate controls, features, treatment and outcome"
v0.2,T and Y residuals to be used in later scripts
v0.2,Save generated dataset
v0.2,#################
v0.2,ORF parameters #
v0.2,#################
v0.2,######################################
v0.2,Train and evaluate treatment effect #
v0.2,######################################
v0.2,########
v0.2,Plots #
v0.2,########
v0.2,###############
v0.2,Save results #
v0.2,###############
v0.2,##############
v0.2,Run Rscript #
v0.2,##############
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,Check inputs
v0.2,Check inputs
v0.2,Check inputs
v0.2,Check inputs
v0.2,Check inputs
v0.2,Check inputs
v0.2,Check inputs
v0.2,Train model on controls. Assign higher weight to units resembling
v0.2,treated units.
v0.2,Train model on the treated. Assign higher weight to units resembling
v0.2,control units.
v0.2,Check inputs
v0.2,Check inputs
v0.2,Fit outcome model on X||W||T (concatenated)
v0.2,Check inputs
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,Create splits of causal tree
v0.2,Must make sure indices are merged correctly
v0.2,Require group assignment t to be one-hot-encoded
v0.2,Define an inner function that iterates over group predictions
v0.2,Convert rows to columns
v0.2,Get predictions for the 2 splits
v0.2,Must make sure indices are merged correctly
v0.2,Estimators
v0.2,OrthoForest parameters
v0.2,Sub-forests
v0.2,Fit check
v0.2,TODO: Check performance
v0.2,Must normalize weights
v0.2,Crossfitting
v0.2,Compute weighted nuisance estimates
v0.2,Generate subsample indices
v0.2,Safety check
v0.2,Build trees in parallel
v0.2,Calculates weights
v0.2,Bootstraping has repetitions in tree sample
v0.2,Similar for `a` weights
v0.2,Bootstraping has repetitions in tree sample
v0.2,Copy and/or define models
v0.2,Define nuisance estimators
v0.2,Define parameter estimators
v0.2,Define
v0.2,Nuissance estimates evaluated with cross-fitting
v0.2,Define 2-fold iterator
v0.2,need safe=False when cloning for WeightedModelWrapper
v0.2,Compute residuals
v0.2,Compute coefficient by OLS on residuals
v0.2,"Parameter returned by LinearRegression is (d_T, )"
v0.2,Compute residuals
v0.2,Compute coefficient by OLS on residuals
v0.2,ell_2 regularization
v0.2,Ridge regression estimate
v0.2,"Parameter returned by LinearRegression is (d_T, )"
v0.2,Return moments and gradients
v0.2,Compute residuals
v0.2,Compute moments
v0.2,"Moments shape is (n, d_T)"
v0.2,Compute moment gradients
v0.2,Copy and/or define models
v0.2,Nuisance estimators shall be defined during fitting because they need to know the number of distinct
v0.2,treatments
v0.2,Define parameter estimators
v0.2,Define moment and mean gradient estimator
v0.2,Define autoencoder
v0.2,"Check that T is shape (n, )"
v0.2,Check T is numeric
v0.2,Train label encoder
v0.2,Define number of classes
v0.2,Call `fit` from parent class
v0.2,"Test that T contains all treatments. If not, return None"
v0.2,Nuissance estimates evaluated with cross-fitting
v0.2,Define 2-fold iterator
v0.2,Check if there is only one example of some class
v0.2,No need to crossfit for internal nodes
v0.2,Compute partial moments
v0.2,"If any of the values in the parameter estimate is nan, return None"
v0.2,Compute partial moments
v0.2,Compute coefficient by OLS on residuals
v0.2,ell_2 regularization
v0.2,Ridge regression estimate
v0.2,"Parameter returned by LinearRegression is (d_T, )"
v0.2,Return moments and gradients
v0.2,Compute partial moments
v0.2,Compute moments
v0.2,"Moments shape is (n, d_T-1)"
v0.2,Compute moment gradients
v0.2,Need to calculate this in an elegant way for when propensity is 0
v0.2,This will flatten T
v0.2,Check that T is numeric
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector..."
v0.2,"Do we want to reshape to an nx1, or just trust the user's choice of input?"
v0.2,(Likewise for Y below)
v0.2,need to override effect in case of discrete treatments
v0.2,TODO: should this logic be moved up to the LinearCateEstimator class and
v0.2,removed from here and from the OrthoForest implementation?
v0.2,Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
v0.2,create an identity matrix of size d_t (or just a 1-element array if T was a vector)
v0.2,the nth row will allow us to compute the marginal effect of the nth component of treatment
v0.2,TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called
v0.2,"rather than just using coef_ seems silly, but one benefit is that we can use linear models"
v0.2,that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models)
v0.2,TODO: handle case where final model doesn't directly expose coef_?
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,#######################################
v0.2,Core DML Tests
v0.2,#######################################
v0.2,How many samples
v0.2,How many control features
v0.2,How many treatment variables
v0.2,Coefficients of how controls affect treatments
v0.2,Coefficients of how controls affect outcome
v0.2,Treatment effects that we want to estimate
v0.2,Run dml estimation
v0.2,How many samples
v0.2,How many control features
v0.2,How many treatment variables
v0.2,Coefficients of how controls affect treatments
v0.2,Coefficients of how controls affect outcome
v0.2,Treatment effects that we want to estimate
v0.2,Run dml estimation
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,"this will have dimension (d,) + shape(X)"
v0.2,send the first dimension to the end
v0.2,columns are featurized independently; partial derivatives are only non-zero
v0.2,when taken with respect to the same column each time
v0.2,don't fit intercept; manually add column of ones to the data instead;
v0.2,this allows us to ignore the intercept when computing marginal effects
v0.2,two stage approximation
v0.2,"first, get basis expansions of T, X, and Z"
v0.2,"regress T expansion on X,Z expansions"
v0.2,"predict ft_T from interacted ft_X, ft_Z"
v0.2,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.2,dT may be only 2-dimensional)
v0.2,promote dT to 3D if necessary (e.g. if T was a vector)
v0.2,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,TODO: make sure to use random seeds wherever necessary
v0.2,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.2,"unfortunately with the Theano and Tensorflow backends,"
v0.2,the straightforward use of K.stop_gradient can cause an error
v0.2,because the parameters of the intermediate layers are now disconnected from the loss;
v0.2,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.2,so that those layers remain connected but with 0 gradient
v0.2,|| t - mu_i || ^2
v0.2,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.2,Use logsumexp for numeric stability:
v0.2,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.2,TODO: does the numeric stability actually make any difference?
v0.2,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.2,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.2,generate cumulative sum via matrix multiplication
v0.2,"Generate standard uniform values in shape (batch_size,1)"
v0.2,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.2,we use uniform_like instead with an input of an appropriate shape)
v0.2,convert to floats and multiply to perform equivalent of logical AND
v0.2,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.2,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.2,we use normal_like instead with an input of an appropriate shape)
v0.2,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.2,prevent gradient from passing through sampling
v0.2,three options: biased or upper-bound loss require a single number of samples;
v0.2,unbiased can take different numbers for the network and its gradient
v0.2,"sample: (() -> Layer, int) -> Layer"
v0.2,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.2,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.2,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.2,TODO: allow 1D arguments for Y and T
v0.2,the dimensionality of the output of the network
v0.2,TODO: is there a more robust way to do this?
v0.2,TODO: do we need to give the user more control over other arguments to fit?
v0.2,"subtle point: we need to build a new model each time,"
v0.2,because each model encapsulates its randomness
v0.2,TODO: do we need to give the user more control over other arguments to fit?
v0.2,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.2,not a general tensor (because of how backprop works in every framework)
v0.2,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.2,but this seems annoying...)
v0.2,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.2,TODO: any way to get this to work on batches of arbitrary size?
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,#######################################################
v0.2,Perfect Data DGPs for Testing Correctness of Code
v0.2,#######################################################
v0.2,Generate random control co-variates
v0.2,Create epsilon residual treatments that deterministically sum up to
v0.2,zero
v0.2,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.2,conditional on each co-variate vector is equal to zero
v0.2,We simply subtract the conditional mean from the epsilons
v0.2,Construct treatments as T = X*A + epsilon
v0.2,Construct outcomes as y = X*beta + T*effect
v0.2,Generate random control co-variates
v0.2,Create epsilon residual treatments that deterministically sum up to
v0.2,zero
v0.2,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.2,conditional on each co-variate vector is equal to zero
v0.2,We simply subtract the conditional mean from the epsilons
v0.2,Construct treatments as T = X*A + epsilon
v0.2,Construct outcomes as y = X*beta + T*effect
v0.2,Generate random control co-variates
v0.2,Construct treatments as T = X*A + epsilon
v0.2,Construct outcomes as y = X*beta + T*effect
v0.2,Generate random control co-variates
v0.2,Create epsilon residual treatments
v0.2,Construct treatments as T = X*A + epsilon
v0.2,Construct outcomes as y = X*beta + T*effect + eta
v0.2,Generate random control co-variates
v0.2,Use the same treatment vector for each row
v0.2,Construct outcomes as y = X*beta + T*effect
v0.2,Licensed under the MIT License.
v0.2,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.2,but tensordot can't be applied to this problem because we don't sum over m
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,TODO: any way to avoid creating a copy if the array was already dense?
v0.2,"the call is necessary if the input was something like a list, though"
v0.2,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.2,so convert to pydata sparse first
v0.2,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.2,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.2,TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?
v0.2,(but note that changing this would necessitate changes to callers
v0.2,to switch the order to preserve behavior where order is important)
v0.2,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.2,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.2,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.2,same number of input definitions as arrays
v0.2,input definitions have same number of dimensions as each array
v0.2,all result indices are unique
v0.2,all result indices must match at least one input index
v0.2,"map indices to all array, axis pairs for that index"
v0.2,each index has the same cardinality wherever it appears
v0.2,"State: list of (set of letters, list of (corresponding indices, value))"
v0.2,Algo: while list contains more than one entry
v0.2,take two entries
v0.2,sort both lists by intersection of their indices
v0.2,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.2,"take the union of indices and the product of values), stepping through each list linearly"
v0.2,TODO: might be faster to break into connected components first
v0.2,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.2,"so compute their content separately, then take cartesian product"
v0.2,this would save a few pointless sorts by empty tuples
v0.2,TODO: Consider investigating other performance ideas for these cases
v0.2,where the dense method beat the sparse method (usually sparse is faster)
v0.2,"e,facd,c->cfed"
v0.2,sparse: 0.0335489
v0.2,dense:  0.011465999999999997
v0.2,"gbd,da,egb->da"
v0.2,sparse: 0.0791625
v0.2,dense:  0.007319099999999995
v0.2,"dcc,d,faedb,c->abe"
v0.2,sparse: 1.2868097
v0.2,dense:  0.44605229999999985
v0.2,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.2,TODO: would using einsum's paths to optimize the order of merging help?
v0.2,Normalize weights
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,TODO: allow different subsets for L1 and L2 regularization?
v0.2,TODO: any better way to deal with sparsity?
v0.2,TODO: any better way to deal with sparsity?
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,Estimators
v0.2,Causal tree parameters
v0.2,Tree structure
v0.2,No need for a random split since the data is already
v0.2,a random subsample from the original input
v0.2,node list stores the nodes that are yet to be splitted
v0.2,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.2,Create local sample set
v0.2,Compute nuisance estimates for the current node
v0.2,Nuisance estimate cannot be calculated
v0.2,Estimate parameter for current node
v0.2,Node estimate cannot be calculated
v0.2,Calculate moments and gradient of moments for current data
v0.2,Calculate inverse gradient
v0.2,The gradient matrix is not invertible.
v0.2,No good split can be found
v0.2,Calculate point-wise pseudo-outcomes rho
v0.2,a split is determined by a feature and a sample pair
v0.2,the number of possible splits is at most (number of features) * (number of node samples)
v0.2,"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
v0.2,parse row and column of random pair
v0.2,the sample of the pair is the integer division of the random number with n_feats
v0.2,calculate the binary indicator of whether sample i is on the left or the right
v0.2,side of proposed split j. So this is an n_samples x n_proposals matrix
v0.2,calculate the number of samples on the left child for each proposed split
v0.2,calculate the analogous binary indicator for the samples in the estimation set
v0.2,calculate the number of estimation samples on the left child of each proposed split
v0.2,find the upper and lower bound on the size of the left split for the split
v0.2,to be valid so as for the split to be balanced and leave at least min_leaf_size
v0.2,on each side.
v0.2,similarly for the estimation sample set
v0.2,if there is no valid split then don't create any children
v0.2,filter only the valid splits
v0.2,calculate the average influence vector of the samples in the left child
v0.2,calculate the average influence vector of the samples in the right child
v0.2,take the square of each of the entries of the influence vectors and normalize
v0.2,by size of each child
v0.2,calculate the vector score of each candidate split as the average of left and right
v0.2,influence vectors
v0.2,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.2,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.2,where there might be large discontinuities in some parameter as the conditioning set varies
v0.2,calculate the scalar score of each split by aggregating across the vector of scores
v0.2,Find split that minimizes criterion
v0.2,Create child nodes with corresponding subsamples
v0.2,add the created children to the list of not yet split nodes
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,group by product; sum and subtract original; divide by (n_p-1)
v0.2,group by product; sum and subtract original; divide by (n_p-1)
v0.2,"for now, require one feature per store/product combination"
v0.2,TODO: would be nice to relax this somehow
v0.2,"alphas vary by product, not by store"
v0.2,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.2,"one cross-price term per product, which is based on the average price"
v0.2,of all other goods sold at the same store in the same week
v0.2,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.2,store-specific and product-specific gammas and betas (which are positively correlated)
v0.2,"features: product dummies, store dummies"
v0.2,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.2,"observe n_products * n_stores prices, same number of quantities"
v0.2,"for direct regression comparisons, we need a pivoted version"
v0.2,"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,"
v0.2,"plus the same for ""group treatments"" (average treatment of other products in the same store/week)"
v0.2,"for direct regression, we also need to append the features"
v0.2,"(both the ""constant features"" as well as the normal ones)"
v0.2,NOTE: need to set cv because default generic algorithm is super slow for sparse matrices
v0.2,"alphas vary by product, not by store"
v0.2,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.2,store-specific and product-specific gammas
v0.2,store-specific and product-specific betas
v0.2,"features: product dummies, store dummies"
v0.2,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.2,we need only the prices for the compound model; all dummies are created internally
v0.2,"observe n_products * n_stores prices, same number of quantities"
v0.2,"simple results include treatments, plus treatments interacted with product dummies,"
v0.2,for use with the direct methods
v0.2,X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor
v0.2,underspecified model
v0.2,Y = alpha T + \sum_i alpha_i T_i + X beta + eta
v0.2,T = X gamma + eps
v0.2,how to score? distance from line of all solutions?
v0.2,"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error"
v0.2,"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)"
v0.2,"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)"
v0.2,"features: one product dummy, one store dummy (each missing one level), constant"
v0.2,"alphas vary by product, not by store"
v0.2,store-specific and product-specific gammas
v0.2,store-specific and product-specific betas
v0.2,"features: product dummies, store dummies"
v0.2,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.2,"columns: prices interacted with products (and constant), features"
v0.2,"observe n_products * n_stores prices, same number of quantities"
v0.2,use features starting at index 1+n_products to skip all prices
v0.2,"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')"
v0.2,"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)"
v0.2,pickleFile.close()
v0.2,#############################################
v0.2,Defining the parameters of Monte Carlo
v0.2,#############################################
v0.2,Estimation parameters
v0.2,###################################################################
v0.2,Estimating the parameters of the DGP with DML. Running multiple
v0.2,Monte Carlo experiments.
v0.2,###################################################################
v0.2,Sparse coefficients of treatment as a function of co-variates
v0.2,Coefficients of outcomes as a function of co-variates
v0.2,"DGP. Create samples of data (y, T, X) from known truth"
v0.2,DML Estimation.
v0.2,Estimation with other methods for comparison
v0.2,#########################################################
v0.2,Plotting the results and saving
v0.2,#########################################################
v0.2,"plt.figure(figsize=(20, 10))"
v0.2,"plt.subplot(1, 4, 1)"
v0.2,"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))"
v0.2,plt.hist(dml_r2score)
v0.2,"plt.subplot(1, 4, 2)"
v0.2,"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),"
v0.2,np.mean(direct_r2score)))
v0.2,plt.hist(direct_r2score)
v0.2,"plt.subplot(1, 4, 3)"
v0.2,"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),"
v0.2,np.std(dml_te)))
v0.2,plt.hist(np.array(dml_te).flatten())
v0.2,"plt.subplot(1, 4, 4)"
v0.2,"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),"
v0.2,np.std(direct_te)))
v0.2,plt.hist(np.array(direct_te).flatten())
v0.2,plt.tight_layout()
v0.2,"plt.savefig(""r2_comparison.png"")"
v0.2,Plotting the results and saving
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,TODO: Add a __dir__ implementation?
v0.2,TODO: what if some args can be None?
v0.2,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.2,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.2,then we should be computing a confidence interval for the wrapped calls
v0.2,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.2,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.2,Remove children with nonwhite mothers from the treatment group
v0.2,Remove children with nonwhite mothers from the treatment group
v0.2,Select columns
v0.2,Scale the numeric variables
v0.2,"Change the binary variable 'first' takes values in {1,2}"
v0.2,Append a column of ones as intercept
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,"require all cells to complete within 15 minutes, which will help prevent us from"
v0.2,creating notebooks that are annoying for our users to actually run themselves
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,TODO: make this test actually test something instead of generating images
v0.2,Sparse coefficients of treatment as a function of co-variates
v0.2,Coefficients of outcomes as a function of co-variates
v0.2,"DGP. Create samples of data (y, T, X) from known truth"
v0.2,"DGP. Create samples of data (y, T, X) from known truth"
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,simple DGP only for illustration
v0.2,Define the treatment model neural network architecture
v0.2,"This will take the concatenation of one-dimensional values z and x as input,"
v0.2,"so the input shape is (d_z + d_x,)"
v0.2,The exact shape of the final layer is not critical because the Deep IV framework will
v0.2,add extra layers on top for the mixture density network
v0.2,Define the response model neural network architecture
v0.2,"This will take the concatenation of one-dimensional values t and x as input,"
v0.2,"so the input shape is (d_t + d_x,)"
v0.2,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.2,"NOTE: For the response model, it is important to define the model *outside*"
v0.2,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.2,so that the same weights will be reused in each instantiation
v0.2,number of samples to use in second estimate of the response
v0.2,(to make loss estimate unbiased)
v0.2,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.2,do something with predictions...
v0.2,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.2,test = True ensures we draw test set images
v0.2,test = True ensures we draw test set images
v0.2,re-draw to get new independent treatment and implied response
v0.2,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.2,above is necesary so that reduced form doesn't win
v0.2,covariates: time and emotion
v0.2,random instrument
v0.2,z -> price
v0.2,true observable demand function
v0.2,errors
v0.2,response
v0.2,test = True ensures we draw test set images
v0.2,test = True ensures we draw test set images
v0.2,re-draw to get new independent treatment and implied response
v0.2,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.2,above is necesary so that reduced form doesn't win
v0.2,covariates: time and emotion
v0.2,random instrument
v0.2,z -> price
v0.2,true observable demand function
v0.2,errors
v0.2,response
v0.2,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.2,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.2,For some reason this doesn't work at all when run against the CNTK backend...
v0.2,"model.compile('nadam', loss=lambda _,l:l)"
v0.2,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.2,generate a valiation set
v0.2,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.2,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,test that we can fit with the same arguments as the base estimator
v0.2,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.2,with the same shape for the lower and upper bounds
v0.2,test that we can do the same thing once we provide percentile bounds
v0.2,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.2,with the same shape for the lower and upper bounds
v0.2,test that we can do the same thing once we provide percentile bounds
v0.2,test that we can fit with the same arguments as the base estimator
v0.2,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.2,with the same shape for the lower and upper bounds
v0.2,test that we can do the same thing once we provide percentile bounds
v0.2,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.2,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.2,with the same shape for the lower and upper bounds
v0.2,test that we can do the same thing once we provide percentile bounds
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,number of inputs in specification must match number of inputs
v0.2,must have an output
v0.2,output indices must be unique
v0.2,output indices must be present in an input
v0.2,number of indices must match number of dimensions for each input
v0.2,repeated indices must always have consistent sizes
v0.2,transpose
v0.2,tensordot
v0.2,trace
v0.2,TODO: set up proper flag for this
v0.2,pick indices at random with replacement from the first 7 letters of the alphabet
v0.2,"of all of the distinct indices that appear in any input,"
v0.2,pick a random subset of them (of size at most 5) to appear in the output
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,Preprocess data
v0.2,Convert 'week' to a date
v0.2,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.2,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.2,Take log of price
v0.2,Make brand numeric
v0.2,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.2,which have all zero coeffs
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.2,"first derivitaves are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.2,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,Set random seed
v0.2,Generate data
v0.2,DGP constants
v0.2,Test data
v0.2,Constant treatment effect and propensity
v0.2,Heterogeneous treatment and propensity
v0.2,TLearner test
v0.2,Instantiate TLearner
v0.2,Test inputs
v0.2,Test constant treatment effect
v0.2,Test heterogeneous treatment effect
v0.2,Instantiate SLearner
v0.2,Test inputs
v0.2,Test constant treatment effect
v0.2,Test heterogeneous treatment effect
v0.2,Need interactions between T and features
v0.2,Instantiate XLearner
v0.2,Test inputs
v0.2,Test constant treatment effect
v0.2,Test heterogeneous treatment effect
v0.2,Instantiate DomainAdaptationLearner
v0.2,Test inputs
v0.2,Test constant treatment effect
v0.2,Test heterogeneous treatment effect
v0.2,Instantiate DomainAdaptationLearner
v0.2,Test inputs
v0.2,Test constant treatment effect
v0.2,Test heterogeneous treatment effect
v0.2,Test heterogenous treatment effect for W =/= None
v0.2,Fit learner and get the effect
v0.2,Get the true treatment effect
v0.2,Compute treatment effect residuals (absolute)
v0.2,Check that at least 90% of predictions are within tolerance interval
v0.2,Only for heterogeneous TE
v0.2,Fit learner on X and W and get the effect
v0.2,Get the true treatment effect
v0.2,Compute treatment effect residuals (absolute)
v0.2,Check that at least 90% of predictions are within tolerance interval
v0.2,Check that one can pass in regular lists
v0.2,Check that it fails correctly if lists of different shape are passed in
v0.2,Check that it fails when T contains values other than 0 and 1
v0.2,"Check that it works when T, Y have shape (n, 1)"
v0.2,Generate covariates
v0.2,Generate treatment
v0.2,Calculate outcome
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,DGP constants
v0.2,Generate data
v0.2,Test data
v0.2,Remove warnings that might be raised by the models passed into the ORF
v0.2,Generate data with continuous treatments
v0.2,Instantiate model with most of the default parameters
v0.2,Test inputs for continuous treatments
v0.2,--> Check that one can pass in regular lists
v0.2,--> Check that it fails correctly if lists of different shape are passed in
v0.2,Check that outputs have the correct shape
v0.2,Test continuous treatments with controls
v0.2,Test continuous treatments without controls
v0.2,Generate data with binary treatments
v0.2,Instantiate model with default params
v0.2,Test inputs for binary treatments
v0.2,--> Check that one can pass in regular lists
v0.2,--> Check that it fails correctly if lists of different shape are passed in
v0.2,"--> Check that it works when T, Y have shape (n, 1)"
v0.2,"--> Check that it fails correctly when T has shape (n, 2)"
v0.2,--> Check that it fails correctly when the treatments are not numeric
v0.2,Check that outputs have the correct shape
v0.2,Test binary treatments with controls
v0.2,Test binary treatments without controls
v0.2,Only applicable to continuous treatments
v0.2,Generate data for 2 treatments
v0.2,Test multiple treatments with controls
v0.2,Compute the treatment effect on test points
v0.2,Compute treatment effect residuals
v0.2,Multiple treatments
v0.2,Allow at most 10% test points to be outside of the tolerance interval
v0.2,Copyright (c) Microsoft Corporation. All rights reserved.
v0.2,Licensed under the MIT License.
v0.2,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.2,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.2,just make sure we can call the marginal_effect and effect methods
v0.2,create a simple artificial setup where effect of moving from treatment
v0.2,"1 -> 2 is 2,"
v0.2,"1 -> 3 is 1, and"
v0.2,"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
v0.2,"Using an uneven number of examples from different classes,"
v0.2,"and having the treatments in non-lexicographic order,"
v0.2,Should rule out some basic issues.
v0.2,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.2,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.2,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.2,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.2,TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong
v0.2,sparse test case: heterogeneous effect by product
v0.2,need at least as many rows in e_y as there are distinct columns
v0.2,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.2,note that this would fail for the non-sparse DMLCateEstimator
v0.1,configuration is all pulled from setup.cfg
v0.1,-*- coding: utf-8 -*-
v0.1,
v0.1,Configuration file for the Sphinx documentation builder.
v0.1,
v0.1,This file does only contain a selection of the most common options. For a
v0.1,full list see the documentation:
v0.1,http://www.sphinx-doc.org/en/master/config
v0.1,-- Path setup --------------------------------------------------------------
v0.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.1,add these directories to sys.path here. If the directory is relative to the
v0.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.1,
v0.1,-- Project information -----------------------------------------------------
v0.1,-- General configuration ---------------------------------------------------
v0.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.1,
v0.1,needs_sphinx = '1.0'
v0.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.1,ones.
v0.1,"Add any paths that contain templates here, relative to this directory."
v0.1,The suffix(es) of source filenames.
v0.1,You can specify multiple suffix as a list of string:
v0.1,
v0.1,"source_suffix = ['.rst', '.md']"
v0.1,The master toctree document.
v0.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.1,for a list of supported languages.
v0.1,
v0.1,This is also used if you do content translation via gettext catalogs.
v0.1,"Usually you set ""language"" from the command line for these cases."
v0.1,"List of patterns, relative to source directory, that match files and"
v0.1,directories to ignore when looking for source files.
v0.1,This pattern also affects html_static_path and html_extra_path.
v0.1,The name of the Pygments (syntax highlighting) style to use.
v0.1,-- Options for HTML output -------------------------------------------------
v0.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.1,a list of builtin themes.
v0.1,
v0.1,Theme options are theme-specific and customize the look and feel of a theme
v0.1,"further.  For a list of options available for each theme, see the"
v0.1,documentation.
v0.1,
v0.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.1,"relative to this directory. They are copied after the builtin static files,"
v0.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.1,html_static_path = ['_static']
v0.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.1,to template names.
v0.1,
v0.1,The default sidebars (for documents that don't match any pattern) are
v0.1,defined by theme itself.  Builtin themes are using these templates by
v0.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.1,'searchbox.html']``.
v0.1,
v0.1,html_sidebars = {}
v0.1,-- Options for HTMLHelp output ---------------------------------------------
v0.1,Output file base name for HTML help builder.
v0.1,-- Options for LaTeX output ------------------------------------------------
v0.1,The paper size ('letterpaper' or 'a4paper').
v0.1,
v0.1,"'papersize': 'letterpaper',"
v0.1,"The font size ('10pt', '11pt' or '12pt')."
v0.1,
v0.1,"'pointsize': '10pt',"
v0.1,Additional stuff for the LaTeX preamble.
v0.1,
v0.1,"'preamble': '',"
v0.1,Latex figure (float) alignment
v0.1,
v0.1,"'figure_align': 'htbp',"
v0.1,Grouping the document tree into LaTeX files. List of tuples
v0.1,"(source start file, target name, title,"
v0.1,"author, documentclass [howto, manual, or own class])."
v0.1,-- Options for manual page output ------------------------------------------
v0.1,One entry per manual page. List of tuples
v0.1,"(source start file, name, description, authors, manual section)."
v0.1,-- Options for Texinfo output ----------------------------------------------
v0.1,Grouping the document tree into Texinfo files. List of tuples
v0.1,"(source start file, target name, title, author,"
v0.1,"dir menu entry, description, category)"
v0.1,-- Options for Epub output -------------------------------------------------
v0.1,Bibliographic Dublin Core info.
v0.1,The unique identifier of the text. This can be a ISBN number
v0.1,or the project homepage.
v0.1,
v0.1,epub_identifier = ''
v0.1,A unique identification for the text.
v0.1,
v0.1,epub_uid = ''
v0.1,A list of files that should not be packed into the epub file.
v0.1,-- Extension configuration -------------------------------------------------
v0.1,-- Options for intersphinx extension ---------------------------------------
v0.1,Example configuration for intersphinx: refer to the Python standard library.
v0.1,-- Options for todo extension ----------------------------------------------
v0.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.1,##################
v0.1,Global settings #
v0.1,##################
v0.1,Global plotting controls
v0.1,"Control for support size, can control for more"
v0.1,#################
v0.1,File utilities #
v0.1,#################
v0.1,#################
v0.1,Plotting utils #
v0.1,#################
v0.1,bias
v0.1,var
v0.1,rmse
v0.1,r2
v0.1,Infer feature dimension
v0.1,Metrics by support plots
v0.1,Authors: Miruna Oprescu <moprescu@microsoft.com>
v0.1,Vasilis Syrgkanis <vasy@microsoft.com>
v0.1,Steven Wu <zhiww@microsoft.com>
v0.1,Initialize causal tree parameters
v0.1,Create splits of causal tree
v0.1,Estimate treatment effects at the leafs
v0.1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.1,the corresponding split and associating the effect computed on that leaf
v0.1,Find the leaf node that this x belongs too and parse the corresponding estimate
v0.1,Safety check
v0.1,Weighted linear regression
v0.1,Calculates weights
v0.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.1,over all indices
v0.1,Similar for `a` weights
v0.1,Doesn't have sample weights
v0.1,Is a linear model
v0.1,Weighted linear regression
v0.1,Calculates weights
v0.1,Bootstraping has repetitions in tree sample so we need to iterate
v0.1,over all indices
v0.1,Similar for `a` weights
v0.1,normalize weights
v0.1,"Split the data in half, train and test"
v0.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.1,"a function of W, using only the train fold"
v0.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.1,"Split the data in half, train and test"
v0.1,Fit with LassoCV the treatment as a function of W and the outcome as
v0.1,"a function of W, using only the train fold"
v0.1,Then compute residuals T-g(W) and Y-f(W) on test fold
v0.1,We create fake treatment points from the same distribution as the residuals created during the fit process
v0.1,"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
v0.1,"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
v0.1,"Split the data in half, train and test"
v0.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.1,"a function of x, using only the train fold"
v0.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.1,Compute coefficient by OLS on residuals
v0.1,"Split the data in half, train and test"
v0.1,Fit with LassoCV the treatment as a function of x and the outcome as
v0.1,"a function of x, using only the train fold"
v0.1,Then compute residuals p-g(x) and q-q(x) on test fold
v0.1,Estimate multipliers for second order orthogonal method
v0.1,"split the data into two parts: one for splitting, the other for estimation at the leafs"
v0.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.1,Create local sample set
v0.1,compute the base estimate for the current node using double ml or second order double ml
v0.1,compute the influence functions here that are used for the criterion
v0.1,generate random proposals of dimensions to split
v0.1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.1,compute criterion for each proposal
v0.1,if splitting creates valid leafs in terms of mean leaf size
v0.1,Calculate criterion for split
v0.1,Else set criterion to infinity so that this split is not chosen
v0.1,If no good split was found
v0.1,Find split that minimizes criterion
v0.1,Set the split attributes at the node
v0.1,Create child nodes with corresponding subsamples
v0.1,Recursively split children
v0.1,Return parent node
v0.1,estimate the local parameter at the leaf using the estimate data
v0.1,###################
v0.1,Argument parsing #
v0.1,###################
v0.1,#########################################
v0.1,Parameters constant across experiments #
v0.1,#########################################
v0.1,Outcome support
v0.1,Treatment support
v0.1,Evaluation grid
v0.1,Treatment effects array
v0.1,Other variables
v0.1,##########################
v0.1,Data Generating Process #
v0.1,##########################
v0.1,Log iteration
v0.1,"Generate controls, features, treatment and outcome"
v0.1,T and Y residuals to be used in later scripts
v0.1,Save generated dataset
v0.1,#################
v0.1,ORF parameters #
v0.1,#################
v0.1,######################################
v0.1,Train and evaluate treatment effect #
v0.1,######################################
v0.1,########
v0.1,Plots #
v0.1,########
v0.1,###############
v0.1,Save results #
v0.1,###############
v0.1,##############
v0.1,Run Rscript #
v0.1,##############
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,Check inputs
v0.1,Check inputs
v0.1,Check inputs
v0.1,Check inputs
v0.1,Check inputs
v0.1,Check inputs
v0.1,Check inputs
v0.1,Train model on controls. Assign higher weight to units resembling
v0.1,treated units.
v0.1,Train model on the treated. Assign higher weight to units resembling
v0.1,control units.
v0.1,Check inputs
v0.1,Check inputs
v0.1,Check inputs
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,Must make sure indices are merged correctly
v0.1,Require group assignment t to be one-hot-encoded
v0.1,Define an inner function that iterates over group predictions
v0.1,Convert rows to columns
v0.1,Get predictions for the 2 splits
v0.1,Must make sure indices are merged correctly
v0.1,Estimators
v0.1,OrthoTree parameters
v0.1,Tree structure
v0.1,Initialize causal tree parameters
v0.1,Create splits of causal tree
v0.1,Estimate treatment effects at the leafs
v0.1,Compute heterogeneous treatement effect for x's in x_list by finding
v0.1,the corresponding split and associating the effect computed on that leaf
v0.1,Estimators
v0.1,OrthoForest parameters
v0.1,Sub-forests
v0.1,Fit check
v0.1,TODO: Check performance
v0.1,Must normalize weights
v0.1,Crossfitting
v0.1,Compute weighted nuisance estimates
v0.1,Generate subsample indices
v0.1,Safety check
v0.1,Build trees in parallel
v0.1,Calculates weights
v0.1,Bootstraping has repetitions in tree sample
v0.1,Similar for `a` weights
v0.1,Bootstraping has repetitions in tree sample
v0.1,Copy and/or define models
v0.1,Define nuisance estimators
v0.1,Define parameter estimators
v0.1,Define
v0.1,Nuissance estimates evaluated with cross-fitting
v0.1,Define 2-fold iterator
v0.1,Compute residuals
v0.1,Compute coefficient by OLS on residuals
v0.1,"Parameter returned by LinearRegression is (d_T, )"
v0.1,Return moments and gradients
v0.1,Compute residuals
v0.1,Compute moments
v0.1,"Moments shape is (n, d_T)"
v0.1,Compute moment gradients
v0.1,Copy and/or define models
v0.1,Nuisance estimators shall be defined during fitting because they need to know the number of distinct
v0.1,treatments
v0.1,Define parameter estimators
v0.1,Define moment and mean gradient estimator
v0.1,Define autoencoder
v0.1,"Check that T is shape (n, )"
v0.1,Check T is numeric
v0.1,Train label encoder
v0.1,Define number of classes
v0.1,Call `fit` from parent class
v0.1,"Test that T contains all treatments. If not, return None"
v0.1,Nuissance estimates evaluated with cross-fitting
v0.1,Define 2-fold iterator
v0.1,Check if there is only one example of some class
v0.1,No need to crossfit for internal nodes
v0.1,Compute partial moments
v0.1,"If any of the values in the parameter estimate is nan, return None"
v0.1,Return moments and gradients
v0.1,Compute partial moments
v0.1,Compute moments
v0.1,"Moments shape is (n, d_T-1)"
v0.1,Compute moment gradients
v0.1,Need to calculate this in an elegant way for when propensity is 0
v0.1,This will flatten T
v0.1,Check that T is numeric
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,Handle case where Y or T is a vector instead of a 2-dimensional array
v0.1,"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector..."
v0.1,"Do we want to reshape to an nx1, or just trust the user's choice of input?"
v0.1,(Likewise for Y below)
v0.1,NOTE: the fact that we stack X first then W is relied upon
v0.1,by the SparseLinearDMLCateEstimator implementation;
v0.1,"if it's changed here then it needs to be changed there, too"
v0.1,TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called
v0.1,"rather than just using coef_ seems silly, but one benefit is that we can use linear models"
v0.1,that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models)
v0.1,TODO: handle case where final model doesn't directly expose coef_?
v0.1,"TODO: yuck, is there any way to avoid having to know the structure of XW"
v0.1,and the size of X to apply the features here?
v0.1,flatten the matrix features so that we can properly perform the cross product
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,#######################################
v0.1,Core DML Tests
v0.1,#######################################
v0.1,How many samples
v0.1,How many control features
v0.1,How many treatment variables
v0.1,Coefficients of how controls affect treatments
v0.1,Coefficients of how controls affect outcome
v0.1,Treatment effects that we want to estimate
v0.1,Run dml estimation
v0.1,How many samples
v0.1,How many control features
v0.1,How many treatment variables
v0.1,Coefficients of how controls affect treatments
v0.1,Coefficients of how controls affect outcome
v0.1,Treatment effects that we want to estimate
v0.1,Run dml estimation
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,"this will have dimension (d,) + shape(X)"
v0.1,send the first dimension to the end
v0.1,columns are featurized independently; partial derivatives are only non-zero
v0.1,when taken with respect to the same column each time
v0.1,don't fit intercept; manually add column of ones to the data instead;
v0.1,this allows us to ignore the intercept when computing marginal effects
v0.1,two stage approximation
v0.1,"first, get basis expansions of T, X, and Z"
v0.1,"regress T expansion on X,Z expansions"
v0.1,"predict ft_T from interacted ft_X, ft_Z"
v0.1,"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
v0.1,dT may be only 2-dimensional)
v0.1,promote dT to 3D if necessary (e.g. if T was a vector)
v0.1,reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,TODO: make sure to use random seeds wherever necessary
v0.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
v0.1,"unfortunately with the Theano and Tensorflow backends,"
v0.1,the straightforward use of K.stop_gradient can cause an error
v0.1,because the parameters of the intermediate layers are now disconnected from the loss;
v0.1,therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
v0.1,so that those layers remain connected but with 0 gradient
v0.1,|| t - mu_i || ^2
v0.1,LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
v0.1,Use logsumexp for numeric stability:
v0.1,LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
v0.1,TODO: does the numeric stability actually make any difference?
v0.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
v0.1,see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
v0.1,generate cumulative sum via matrix multiplication
v0.1,"Generate standard uniform values in shape (batch_size,1)"
v0.1,"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
v0.1,we use uniform_like instead with an input of an appropriate shape)
v0.1,convert to floats and multiply to perform equivalent of logical AND
v0.1,"Generate standard normal values in shape (batch_size,1,d_t)"
v0.1,"(since we can't use the dynamic batch_size with random.normal in CNTK,"
v0.1,we use normal_like instead with an input of an appropriate shape)
v0.1,"exactly one entry should be nonzero for each b,d combination; use sum to select it"
v0.1,prevent gradient from passing through sampling
v0.1,three options: biased or upper-bound loss require a single number of samples;
v0.1,unbiased can take different numbers for the network and its gradient
v0.1,"sample: (() -> Layer, int) -> Layer"
v0.1,we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
v0.1,"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
v0.1,"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
v0.1,TODO: allow 1D arguments for Y and T
v0.1,the dimensionality of the output of the network
v0.1,TODO: is there a more robust way to do this?
v0.1,TODO: do we need to give the user more control over other arguments to fit?
v0.1,"subtle point: we need to build a new model each time,"
v0.1,because each model encapsulates its randomness
v0.1,TODO: do we need to give the user more control over other arguments to fit?
v0.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
v0.1,not a general tensor (because of how backprop works in every framework)
v0.1,"(alternatively, we could iterate through the batch in addition to iterating through the output,"
v0.1,but this seems annoying...)
v0.1,"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
v0.1,TODO: any way to get this to work on batches of arbitrary size?
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,#######################################################
v0.1,Perfect Data DGPs for Testing Correctness of Code
v0.1,#######################################################
v0.1,Generate random control co-variates
v0.1,Create epsilon residual treatments that deterministically sum up to
v0.1,zero
v0.1,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.1,conditional on each co-variate vector is equal to zero
v0.1,We simply subtract the conditional mean from the epsilons
v0.1,Construct treatments as T = X*A + epsilon
v0.1,Construct outcomes as y = X*beta + T*effect
v0.1,Generate random control co-variates
v0.1,Create epsilon residual treatments that deterministically sum up to
v0.1,zero
v0.1,Re-calibrate epsilon to make sure that empirical distribution of epsilon
v0.1,conditional on each co-variate vector is equal to zero
v0.1,We simply subtract the conditional mean from the epsilons
v0.1,Construct treatments as T = X*A + epsilon
v0.1,Construct outcomes as y = X*beta + T*effect
v0.1,Generate random control co-variates
v0.1,Construct treatments as T = X*A + epsilon
v0.1,Construct outcomes as y = X*beta + T*effect
v0.1,Generate random control co-variates
v0.1,Create epsilon residual treatments
v0.1,Construct treatments as T = X*A + epsilon
v0.1,Construct outcomes as y = X*beta + T*effect + eta
v0.1,Generate random control co-variates
v0.1,Use the same treatment vector for each row
v0.1,Construct outcomes as y = X*beta + T*effect
v0.1,Licensed under the MIT License.
v0.1,"TODO: what if input is sparse? - there's no equivalent to einsum,"
v0.1,but tensordot can't be applied to this problem because we don't sum over m
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,TODO: any way to avoid creating a copy if the array was already dense?
v0.1,"the call is necessary if the input was something like a list, though"
v0.1,"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
v0.1,so convert to pydata sparse first
v0.1,"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
v0.1,both inputs were scipy and we can safely convert back to scipy because it's 2D
v0.1,TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?
v0.1,(but note that changing this would necessitate changes to callers
v0.1,to switch the order to preserve behavior where order is important)
v0.1,note: in contrast to np.hstack this only works with arrays of dimension at least 2
v0.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.1,"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
v0.1,same number of input definitions as arrays
v0.1,input definitions have same number of dimensions as each array
v0.1,all result indices are unique
v0.1,all result indices must match at least one input index
v0.1,"map indices to all array, axis pairs for that index"
v0.1,each index has the same cardinality wherever it appears
v0.1,"State: list of (set of letters, list of (corresponding indices, value))"
v0.1,Algo: while list contains more than one entry
v0.1,take two entries
v0.1,sort both lists by intersection of their indices
v0.1,"merge compatible entries (where intersection of indices is equal - in the resulting list,"
v0.1,"take the union of indices and the product of values), stepping through each list linearly"
v0.1,TODO: might be faster to break into connected components first
v0.1,"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
v0.1,"so compute their content separately, then take cartesian product"
v0.1,this would save a few pointless sorts by empty tuples
v0.1,TODO: Consider investigating other performance ideas for these cases
v0.1,where the dense method beat the sparse method (usually sparse is faster)
v0.1,"e,facd,c->cfed"
v0.1,sparse: 0.0335489
v0.1,dense:  0.011465999999999997
v0.1,"gbd,da,egb->da"
v0.1,sparse: 0.0791625
v0.1,dense:  0.007319099999999995
v0.1,"dcc,d,faedb,c->abe"
v0.1,sparse: 1.2868097
v0.1,dense:  0.44605229999999985
v0.1,"when indices are repeated within an array, pre-filter the coordinates and data"
v0.1,TODO: would using einsum's paths to optimize the order of merging help?
v0.1,Normalize weights
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,TODO: allow different subsets for L1 and L2 regularization?
v0.1,TODO: any better way to deal with sparsity?
v0.1,TODO: any better way to deal with sparsity?
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,Input datasets
v0.1,Estimators
v0.1,Causal tree parameters
v0.1,Tree structure
v0.1,If by splitting we have too small leaves or if we reached the maximum number of splits we stop
v0.1,Create local sample set
v0.1,Compute nuisance estimates for the current node
v0.1,Nuisance estimate cannot be calculated
v0.1,Estimate parameter for current node
v0.1,Node estimate cannot be calculated
v0.1,Calculate moments and gradient of moments for current data
v0.1,Calculate inverse gradient
v0.1,The gradient matrix is not invertible.
v0.1,No good split can be found
v0.1,Calculate point-wise pseudo-outcomes rho
v0.1,Generate random proposals of dimensions to split
v0.1,"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
v0.1,Compute criterion for each proposal
v0.1,eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
v0.1,across parameters. we give some benefit to individual heterogeneity factors for cases
v0.1,where there might be large discontinuities in some parameter as the conditioning set varies
v0.1,If splitting creates valid leafs in terms of mean leaf size
v0.1,Calculate criterion for split
v0.1,Else set criterion to infinity so that this split is not chosen
v0.1,If no good split was found
v0.1,Find split that minimizes criterion
v0.1,Set the split attributes at the node
v0.1,Create child nodes with corresponding subsamples
v0.1,Recursively split children
v0.1,Return parent node
v0.1,No need for a random split since the data is already
v0.1,a random subsample from the original input
v0.1,Estimate the local parameter at the leaf using the estimate data
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,group by product; sum and subtract original; divide by (n_p-1)
v0.1,group by product; sum and subtract original; divide by (n_p-1)
v0.1,"for now, require one feature per store/product combination"
v0.1,TODO: would be nice to relax this somehow
v0.1,"alphas vary by product, not by store"
v0.1,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.1,"one cross-price term per product, which is based on the average price"
v0.1,of all other goods sold at the same store in the same week
v0.1,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.1,store-specific and product-specific gammas and betas (which are positively correlated)
v0.1,"features: product dummies, store dummies"
v0.1,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.1,"observe n_products * n_stores prices, same number of quantities"
v0.1,"for direct regression comparisons, we need a pivoted version"
v0.1,"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,"
v0.1,"plus the same for ""group treatments"" (average treatment of other products in the same store/week)"
v0.1,"for direct regression, we also need to append the features"
v0.1,"(both the ""constant features"" as well as the normal ones)"
v0.1,NOTE: need to set cv because default generic algorithm is super slow for sparse matrices
v0.1,"alphas vary by product, not by store"
v0.1,"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
v0.1,store-specific and product-specific gammas
v0.1,store-specific and product-specific betas
v0.1,"features: product dummies, store dummies"
v0.1,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.1,we need only the prices for the compound model; all dummies are created internally
v0.1,"observe n_products * n_stores prices, same number of quantities"
v0.1,"simple results include treatments, plus treatments interacted with product dummies,"
v0.1,for use with the direct methods
v0.1,X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor
v0.1,underspecified model
v0.1,Y = alpha T + \sum_i alpha_i T_i + X beta + eta
v0.1,T = X gamma + eps
v0.1,how to score? distance from line of all solutions?
v0.1,"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error"
v0.1,"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)"
v0.1,"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)"
v0.1,"features: one product dummy, one store dummy (each missing one level), constant"
v0.1,"alphas vary by product, not by store"
v0.1,store-specific and product-specific gammas
v0.1,store-specific and product-specific betas
v0.1,"features: product dummies, store dummies"
v0.1,"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
v0.1,"columns: prices interacted with products (and constant), features"
v0.1,"observe n_products * n_stores prices, same number of quantities"
v0.1,use features starting at index 1+n_products to skip all prices
v0.1,"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')"
v0.1,"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)"
v0.1,pickleFile.close()
v0.1,#############################################
v0.1,Defining the parameters of Monte Carlo
v0.1,#############################################
v0.1,Estimation parameters
v0.1,###################################################################
v0.1,Estimating the parameters of the DGP with DML. Running multiple
v0.1,Monte Carlo experiments.
v0.1,###################################################################
v0.1,Sparse coefficients of treatment as a function of co-variates
v0.1,Coefficients of outcomes as a function of co-variates
v0.1,"DGP. Create samples of data (y, T, X) from known truth"
v0.1,DML Estimation.
v0.1,Estimation with other methods for comparison
v0.1,#########################################################
v0.1,Plotting the results and saving
v0.1,#########################################################
v0.1,"plt.figure(figsize=(20, 10))"
v0.1,"plt.subplot(1, 4, 1)"
v0.1,"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))"
v0.1,plt.hist(dml_r2score)
v0.1,"plt.subplot(1, 4, 2)"
v0.1,"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),"
v0.1,np.mean(direct_r2score)))
v0.1,plt.hist(direct_r2score)
v0.1,"plt.subplot(1, 4, 3)"
v0.1,"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),"
v0.1,np.std(dml_te)))
v0.1,plt.hist(np.array(dml_te).flatten())
v0.1,"plt.subplot(1, 4, 4)"
v0.1,"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),"
v0.1,np.std(direct_te)))
v0.1,plt.hist(np.array(direct_te).flatten())
v0.1,plt.tight_layout()
v0.1,"plt.savefig(""r2_comparison.png"")"
v0.1,Plotting the results and saving
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,TODO: Add a __dir__ implementation?
v0.1,TODO: what if some args can be None?
v0.1,"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
v0.1,"if the attribute exists on the wrapped object once we remove the suffix,"
v0.1,then we should be computing a confidence interval for the wrapped calls
v0.1,"collect extra arguments and pass them through, if the wrapped attribute was callable"
v0.1,don't pass extra arguments if the wrapped attribute wasn't callable to begin with
v0.1,Remove children with nonwhite mothers from the treatment group
v0.1,Remove children with nonwhite mothers from the treatment group
v0.1,Select columns
v0.1,Scale the numeric variables
v0.1,"Change the binary variable 'first' takes values in {1,2}"
v0.1,Append a column of ones as intercept
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,TODO: make this test actually test something instead of generating images
v0.1,Sparse coefficients of treatment as a function of co-variates
v0.1,Coefficients of outcomes as a function of co-variates
v0.1,"DGP. Create samples of data (y, T, X) from known truth"
v0.1,"DGP. Create samples of data (y, T, X) from known truth"
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,simple DGP only for illustration
v0.1,Define the treatment model neural network architecture
v0.1,"This will take the concatenation of one-dimensional values z and x as input,"
v0.1,"so the input shape is (d_z + d_x,)"
v0.1,The exact shape of the final layer is not critical because the Deep IV framework will
v0.1,add extra layers on top for the mixture density network
v0.1,Define the response model neural network architecture
v0.1,"This will take the concatenation of one-dimensional values t and x as input,"
v0.1,"so the input shape is (d_t + d_x,)"
v0.1,"The output should match the shape of y, so it must have shape (d_y,) in this case"
v0.1,"NOTE: For the response model, it is important to define the model *outside*"
v0.1,"of the lambda passed to the DeepIvEstimator, as we do here,"
v0.1,so that the same weights will be reused in each instantiation
v0.1,number of samples to use in second estimate of the response
v0.1,(to make loss estimate unbiased)
v0.1,Keras optimizer to use for training - see https://keras.io/optimizers/
v0.1,do something with predictions...
v0.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
v0.1,test = True ensures we draw test set images
v0.1,test = True ensures we draw test set images
v0.1,re-draw to get new independent treatment and implied response
v0.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.1,above is necesary so that reduced form doesn't win
v0.1,covariates: time and emotion
v0.1,random instrument
v0.1,z -> price
v0.1,true observable demand function
v0.1,errors
v0.1,response
v0.1,test = True ensures we draw test set images
v0.1,test = True ensures we draw test set images
v0.1,re-draw to get new independent treatment and implied response
v0.1,we need to make sure z _never_ does anything in these g functions (fitted and true)
v0.1,above is necesary so that reduced form doesn't win
v0.1,covariates: time and emotion
v0.1,random instrument
v0.1,z -> price
v0.1,true observable demand function
v0.1,errors
v0.1,response
v0.1,"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
v0.1,"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
v0.1,For some reason this doesn't work at all when run against the CNTK backend...
v0.1,"model.compile('nadam', loss=lambda _,l:l)"
v0.1,"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
v0.1,generate a valiation set
v0.1,"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
v0.1,convex combinations of semidefinite covariance matrices are themselves semidefinite
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,test that we can fit with the same arguments as the base estimator
v0.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.1,with the same shape for the lower and upper bounds
v0.1,test that we can do the same thing once we provide percentile bounds
v0.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.1,with the same shape for the lower and upper bounds
v0.1,test that we can do the same thing once we provide percentile bounds
v0.1,test that we can fit with the same arguments as the base estimator
v0.1,"test that we can get the same attribute for the bootstrap as the original, with the same shape"
v0.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.1,with the same shape for the lower and upper bounds
v0.1,test that we can do the same thing once we provide percentile bounds
v0.1,"test that we can do the same thing with the results of a method, rather than an attribute"
v0.1,"test that we can get an interval for the same attribute for the bootstrap as the original,"
v0.1,with the same shape for the lower and upper bounds
v0.1,test that we can do the same thing once we provide percentile bounds
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,number of inputs in specification must match number of inputs
v0.1,must have an output
v0.1,output indices must be unique
v0.1,output indices must be present in an input
v0.1,number of indices must match number of dimensions for each input
v0.1,repeated indices must always have consistent sizes
v0.1,transpose
v0.1,tensordot
v0.1,trace
v0.1,TODO: set up proper flag for this
v0.1,pick indices at random with replacement from the first 7 letters of the alphabet
v0.1,"of all of the distinct indices that appear in any input,"
v0.1,pick a random subset of them (of size at most 5) to appear in the output
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,Preprocess data
v0.1,Convert 'week' to a date
v0.1,"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
v0.1,"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
v0.1,Take log of price
v0.1,Make brand numeric
v0.1,"remove meaningless features (e.g. cross-price effects of products on themselves),"
v0.1,which have all zero coeffs
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,"first polynomials are 1, x, x*x-1, x*x*x-3*x"
v0.1,"first derivitaves are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
v0.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,Generate data
v0.1,DGP constants
v0.1,Test data
v0.1,Constant treatment effect and propensity
v0.1,Heterogeneous treatment and propensity
v0.1,TLearner test
v0.1,Instantiate TLearner
v0.1,Test inputs
v0.1,Test constant treatment effect
v0.1,Test heterogeneous treatment effect
v0.1,Instantiate SLearner
v0.1,Test inputs
v0.1,Test constant treatment effect
v0.1,Test heterogeneous treatment effect
v0.1,Need interactions between T and features
v0.1,Instantiate XLearner
v0.1,Test inputs
v0.1,Test constant treatment effect
v0.1,Test heterogeneous treatment effect
v0.1,Instantiate DomainAdaptationLearner
v0.1,Test inputs
v0.1,Test constant treatment effect
v0.1,Test heterogeneous treatment effect
v0.1,Instantiate DomainAdaptationLearner
v0.1,Test inputs
v0.1,Test constant treatment effect
v0.1,Test heterogeneous treatment effect
v0.1,Fit learner and get the effect
v0.1,Get the true treatment effect
v0.1,Compute treatment effect residuals (absolute)
v0.1,Check that at least 90% of predictions are within tolerance interval
v0.1,Check that one can pass in regular lists
v0.1,Check that it fails correctly if lists of different shape are passed in
v0.1,Check that it fails when T contains values other than 0 and 1
v0.1,"Check that it works when T, Y have shape (n, 1)"
v0.1,learner_instance.effect(TestMetalearners.X_test)
v0.1,Generate covariates
v0.1,Generate treatment
v0.1,Calculate outcome
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,DGP constants
v0.1,Generate data
v0.1,Test data
v0.1,Remove warnings that might be raised by the models passed into the ORF
v0.1,Generate data with continuous treatments
v0.1,Instantiate model with most of the default parameters
v0.1,Test inputs for continuous treatments
v0.1,--> Check that one can pass in regular lists
v0.1,--> Check that it fails correctly if lists of different shape are passed in
v0.1,Check that outputs have the correct shape
v0.1,Test continuous treatments with controls
v0.1,Test continuous treatments without controls
v0.1,Generate data with binary treatments
v0.1,Instantiate model with default params
v0.1,Test inputs for binary treatments
v0.1,--> Check that one can pass in regular lists
v0.1,--> Check that it fails correctly if lists of different shape are passed in
v0.1,"--> Check that it works when T, Y have shape (n, 1)"
v0.1,"--> Check that it fails correctly when T has shape (n, 2)"
v0.1,--> Check that it fails correctly when the treatments are not numeric
v0.1,Check that outputs have the correct shape
v0.1,Test binary treatments with controls
v0.1,Test binary treatments without controls
v0.1,Only applicable to continuous treatments
v0.1,Generate data for 2 treatments
v0.1,Test multiple treatments with controls
v0.1,Compute the treatment effect on test points
v0.1,Compute treatment effect residuals
v0.1,Multiple treatments
v0.1,Allow at most 10% test points to be outside of the tolerance interval
v0.1,Copyright (c) Microsoft Corporation. All rights reserved.
v0.1,Licensed under the MIT License.
v0.1,add column of ones to X
v0.1,"for each row, create the d_y*d_t*(d_x+1) features (which are matrices of size d_y by d_t)"
v0.1,all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
v0.1,"note that if Ax=b is overdetermined, this will raise an assertion error"
v0.1,just make sure we can call the marginal_effect and effect methods
v0.1,just make sure we can call the marginal_effect and effect methods
v0.1,"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
v0.1,"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
v0.1,-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
v0.1,"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
v0.1,TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong
v0.1,sparse test case: heterogeneous effect by product
v0.1,need at least as many rows in e_y as there are distinct columns
v0.1,in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
v0.1,note that this would fail for the non-sparse DMLCateEstimator
v0.1,recover simple features by initializing complex features appropriately
v0.1,using full set of matrix features should be equivalent to using non-matrix featurizer
