Version,Commit Message
v1.11.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.11.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.11.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.11.0,from unittest.mock import Mock
v1.11.0,sys.modules['numpy'] = Mock()
v1.11.0,sys.modules['numpy.linalg'] = Mock()
v1.11.0,sys.modules['scipy'] = Mock()
v1.11.0,sys.modules['scipy.optimize'] = Mock()
v1.11.0,sys.modules['scipy.interpolate'] = Mock()
v1.11.0,sys.modules['scipy.sparse'] = Mock()
v1.11.0,sys.modules['scipy.ndimage'] = Mock()
v1.11.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.11.0,sys.modules['tensorflow'] = Mock()
v1.11.0,sys.modules['theano'] = Mock()
v1.11.0,sys.modules['theano.tensor'] = Mock()
v1.11.0,sys.modules['torch'] = Mock()
v1.11.0,sys.modules['torch.optim'] = Mock()
v1.11.0,sys.modules['torch.nn'] = Mock()
v1.11.0,sys.modules['torch.nn.init'] = Mock()
v1.11.0,sys.modules['torch.autograd'] = Mock()
v1.11.0,sys.modules['sklearn'] = Mock()
v1.11.0,sys.modules['sklearn.model_selection'] = Mock()
v1.11.0,sys.modules['sklearn.utils'] = Mock()
v1.11.0,-- Project information -----------------------------------------------------
v1.11.0,"The full version, including alpha/beta/rc tags."
v1.11.0,The short X.Y version.
v1.11.0,-- General configuration ---------------------------------------------------
v1.11.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.11.0,
v1.11.0,needs_sphinx = '1.0'
v1.11.0,"If true, the current module name will be prepended to all description"
v1.11.0,unit titles (such as .. function::).
v1.11.0,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.11.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.11.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.11.0,ones.
v1.11.0,show todo's
v1.11.0,generate autosummary pages
v1.11.0,"Add any paths that contain templates here, relative to this directory."
v1.11.0,The suffix(es) of source filenames.
v1.11.0,The master toctree document.
v1.11.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.11.0,for a list of supported languages.
v1.11.0,
v1.11.0,This is also used if you do content translation via gettext catalogs.
v1.11.0,"Usually you set ""language"" from the command line for these cases."
v1.11.0,"List of patterns, relative to source directory, that match files and"
v1.11.0,directories to ignore when looking for source files.
v1.11.0,This pattern also affects html_static_path and html_extra_path.
v1.11.0,The name of the Pygments (syntax highlighting) style to use.
v1.11.0,-- Options for HTML output -------------------------------------------------
v1.11.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.11.0,a list of builtin themes.
v1.11.0,
v1.11.0,Theme options are theme-specific and customize the look and feel of a theme
v1.11.0,"further.  For a list of options available for each theme, see the"
v1.11.0,documentation.
v1.11.0,
v1.11.0,html_theme_options = {}
v1.11.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.11.0,"relative to this directory. They are copied after the builtin static files,"
v1.11.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.11.0,html_static_path = ['_static']
v1.11.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.11.0,to template names.
v1.11.0,
v1.11.0,The default sidebars (for documents that don't match any pattern) are
v1.11.0,defined by theme itself.  Builtin themes are using these templates by
v1.11.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.11.0,'searchbox.html']``.
v1.11.0,
v1.11.0,html_sidebars = {}
v1.11.0,The name of an image file (relative to this directory) to place at the top
v1.11.0,of the sidebar.
v1.11.0,
v1.11.0,-- Options for HTMLHelp output ---------------------------------------------
v1.11.0,Output file base name for HTML help builder.
v1.11.0,-- Options for LaTeX output ------------------------------------------------
v1.11.0,latex_elements = {
v1.11.0,The paper size ('letterpaper' or 'a4paper').
v1.11.0,
v1.11.0,"'papersize': 'letterpaper',"
v1.11.0,
v1.11.0,"The font size ('10pt', '11pt' or '12pt')."
v1.11.0,
v1.11.0,"'pointsize': '10pt',"
v1.11.0,
v1.11.0,Additional stuff for the LaTeX preamble.
v1.11.0,
v1.11.0,"'preamble': '',"
v1.11.0,
v1.11.0,Latex figure (float) alignment
v1.11.0,
v1.11.0,"'figure_align': 'htbp',"
v1.11.0,}
v1.11.0,Grouping the document tree into LaTeX files. List of tuples
v1.11.0,"(source start file, target name, title,"
v1.11.0,"author, documentclass [howto, manual, or own class])."
v1.11.0,latex_documents = [
v1.11.0,(
v1.11.0,"master_doc,"
v1.11.0,"'pykeen.tex',"
v1.11.0,"'PyKEEN Documentation',"
v1.11.0,"author,"
v1.11.0,"'manual',"
v1.11.0,"),"
v1.11.0,]
v1.11.0,-- Options for manual page output ------------------------------------------
v1.11.0,One entry per manual page. List of tuples
v1.11.0,"(source start file, name, description, authors, manual section)."
v1.11.0,-- Options for Texinfo output ----------------------------------------------
v1.11.0,Grouping the document tree into Texinfo files. List of tuples
v1.11.0,"(source start file, target name, title, author,"
v1.11.0,"dir menu entry, description, category)"
v1.11.0,-- Options for Epub output -------------------------------------------------
v1.11.0,Bibliographic Dublin Core info.
v1.11.0,epub_title = project
v1.11.0,The unique identifier of the text. This can be a ISBN number
v1.11.0,or the project homepage.
v1.11.0,
v1.11.0,epub_identifier = ''
v1.11.0,A unique identification for the text.
v1.11.0,
v1.11.0,epub_uid = ''
v1.11.0,A list of files that should not be packed into the epub file.
v1.11.0,epub_exclude_files = ['search.html']
v1.11.0,-- Extension configuration -------------------------------------------------
v1.11.0,-- Options for intersphinx extension ---------------------------------------
v1.11.0,Example configuration for intersphinx: refer to the Python standard library.
v1.11.0,"'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),"
v1.11.0,autodoc_member_order = 'bysource'
v1.11.0,autodoc_preserve_defaults = True
v1.11.0,we start by creating the representation for those entities where we have pre-trained features
v1.11.0,here we simulate this for a set of Asian countries
v1.11.0,"Next, we directly create representations for the remaining ones using the backfill representation."
v1.11.0,"To do this, we need to create an iterable (e.g., a set) of all of the entity IDs that are in the base"
v1.11.0,"representation. Then, the assignments to the base representation and an auxillary representation are"
v1.11.0,automatically generated for the base class.
v1.11.0,We assume that we do not have any pre-trained information for relations here for simplicity and train
v1.11.0,them from scratch.
v1.11.0,"The combined representation can now be used as any other representation, e.g., to train a model with"
v1.11.0,distmult interaction:
v1.11.0,Step 1: Get triples
v1.11.0,Step 2: Configure the model
v1.11.0,Step 3: Configure the loop
v1.11.0,Step 4: Train
v1.11.0,Step 5: Evaluate the model
v1.11.0,create one checkpoint every 10 epochs
v1.11.0,"create checkpoints every 5 epochs, and at epoch 7"
v1.11.0,create one checkpoint every 10 epochs
v1.11.0,"create checkpoints at epoch 1, 7, and 10"
v1.11.0,create a default result tracker (or use a proper one)
v1.11.0,"in this example, we just use the training loss"
v1.11.0,Important: use the same result tracker instance as in the checkpoint callback
v1.11.0,%%
v1.11.0,"As an example, we will use a small dataset that comes with entity and relation labels."
v1.11.0,This dataset provides entity names.
v1.11.0,%%
v1.11.0,"Direct access to the mapping, here for entities."
v1.11.0,a mapping from labels/strings to the Ids
v1.11.0,the inverse mapping
v1.11.0,%%
v1.11.0,The labeling object also offers convenience methods for converting ids in different formats to strings
v1.11.0,%%
v1.11.0,The triples factory exposes utility methods to normalize to ids
v1.11.0,TODO: we should move that to the labeling
v1.11.0,%%
v1.11.0,Get tensor of entity identifiers
v1.11.0,%%
v1.11.0,train a model
v1.11.0,access entity and relation representations
v1.11.0,%%
v1.11.0,TransE has one representation for entities and one for relations
v1.11.0,both are simple embedding matrices
v1.11.0,%%
v1.11.0,get representations for all entities/relations
v1.11.0,%%
v1.11.0,this corresponds to explicitly passing indices=None
v1.11.0,%%
v1.11.0,%%
v1.11.0,"detach tensor, move to cpu, and convert to numpy"
v1.11.0,Get a training dataset
v1.11.0,"The following applies to most packaged datasets,"
v1.11.0,although the dataset class itself makes `validation' optional.
v1.11.0,Pick a model
v1.11.0,Pick an optimizer from PyTorch
v1.11.0,Pick a training approach (sLCWA or LCWA)
v1.11.0,Train like Cristiano Ronaldo
v1.11.0,Pick an evaluator
v1.11.0,Evaluate
v1.11.0,print(results)
v1.11.0,get a dataset
v1.11.0,Pick a model
v1.11.0,Pick a training approach (sLCWA or LCWA)
v1.11.0,Train like Cristiano Ronaldo
v1.11.0,NEW: validation evaluation callback
v1.11.0,Pick an evaluation loop (NEW)
v1.11.0,Evaluate
v1.11.0,print(results)
v1.11.0,check probability distribution
v1.11.0,Check a model param is optimized
v1.11.0,Check a loss param is optimized
v1.11.0,Check a model param is NOT optimized
v1.11.0,Check a loss param is optimized
v1.11.0,Check a model param is optimized
v1.11.0,Check a loss param is NOT optimized
v1.11.0,Check a model param is NOT optimized
v1.11.0,Check a loss param is NOT optimized
v1.11.0,verify failure
v1.11.0,"Since custom data was passed, we can't store any of this"
v1.11.0,"currently, any custom data doesn't get stored."
v1.11.0,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.11.0,"Since there's no source path information, these shouldn't be"
v1.11.0,"added, even if it might be possible to infer path information"
v1.11.0,from the triples factories
v1.11.0,"Since paths were passed for training, testing, and validation,"
v1.11.0,they should be stored as study-level attributes
v1.11.0,Check a model param is optimized
v1.11.0,Check a loss param is optimized
v1.11.0,ignore abstract classes
v1.11.0,verify that all classes have the hpo_default dictionary
v1.11.0,verify that we can bind the keys to the __init__'s signature
v1.11.0,note: this is only of limited use since many have **kwargs which
v1.11.0,raise an OOM error whenever the batch size is larger than 1
v1.11.0,TODO: predict_target
v1.11.0,docstr-coverage: inherited
v1.11.0,check if within 0.5 std of observed
v1.11.0,test error is raised
v1.11.0,there is an extra test for this case
v1.11.0,docstr-coverage: inherited
v1.11.0,same size tensors
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,Tests that exception will be thrown when more than or less than two tensors are passed
v1.11.0,create broadcastable shapes
v1.11.0,check correct value range
v1.11.0,check maximum norm constraint
v1.11.0,unchanged values for small norms
v1.11.0,random entity embeddings & projections
v1.11.0,random relation embeddings & projections
v1.11.0,project
v1.11.0,check shape:
v1.11.0,check normalization
v1.11.0,check equivalence of re-formulation
v1.11.0,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.11.0,= r_p (e_p^T e) + e'
v1.11.0,"create random array, estimate the costs of addition, and measure some execution times."
v1.11.0,"then, compute correlation between the estimated cost, and the measured time."
v1.11.0,check for strong correlation between estimated costs and measured execution time
v1.11.0,get optimal sequence
v1.11.0,check caching
v1.11.0,get optimal sequence
v1.11.0,check correct cost
v1.11.0,check optimality
v1.11.0,compare result to sequential addition
v1.11.0,compare result to sequential addition
v1.11.0,ensure each node participates in at least one edge
v1.11.0,check type and shape
v1.11.0,number of colors is monotonically increasing
v1.11.0,ensure each node participates in at least one edge
v1.11.0,normalize
v1.11.0,equal value; larger is better
v1.11.0,equal value; smaller is better
v1.11.0,larger is better; improvement
v1.11.0,larger is better; improvement; but not significant
v1.11.0,negative number
v1.11.0,assert that reporting another metric for this epoch raises an error
v1.11.0,: The window size used by the early stopper
v1.11.0,: The (zeroed) index  - 1 at which stopping will occur
v1.11.0,: The minimum improvement
v1.11.0,: The random seed to use for reproducibility
v1.11.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.11.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.11.0,: The batch size to use.
v1.11.0,Fix seed for reproducibility
v1.11.0,Set automatic_memory_optimization to false during testing
v1.11.0,See https://github.com/pykeen/pykeen/pull/883
v1.11.0,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.11.0,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.11.0,"first triple, and passing create_inverse_triples=True."
v1.11.0,check for warning
v1.11.0,check for filtered triples
v1.11.0,check for correct inverse triples flag
v1.11.0,check correct translation
v1.11.0,check column order
v1.11.0,apply restriction
v1.11.0,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.11.0,check that inverse_triples is correctly carried over
v1.11.0,verify that the label-to-ID mapping has not been changed
v1.11.0,verify that triples have been filtered
v1.11.0,Test different combinations of restrictions
v1.11.0,check compressed triples
v1.11.0,reconstruct triples from compressed form
v1.11.0,check data loader
v1.11.0,set create inverse triple to true
v1.11.0,split factory
v1.11.0,check that in *training* inverse triple are to be created
v1.11.0,check that in all other splits no inverse triples are to be created
v1.11.0,verify that all entities and relations are present in the training factory
v1.11.0,verify that no triple got lost
v1.11.0,verify that the label-to-id mappings match
v1.11.0,Slightly larger number of triples to guarantee split can find coverage of all entities and relations.
v1.11.0,serialize
v1.11.0,de-serialize
v1.11.0,check for equality
v1.11.0,TODO: this could be (Core)TriplesFactory.__equal__
v1.11.0,cf. https://docs.pytest.org/en/7.1.x/example/parametrize.html#parametrizing-conditional-raising
v1.11.0,wrong ndim
v1.11.0,wrong last dim
v1.11.0,wrong dtype: float
v1.11.0,wrong dtype: complex
v1.11.0,correct
v1.11.0,>>> positional argument
v1.11.0,mapped_triples
v1.11.0,triples factory
v1.11.0,labeled triples + factory
v1.11.0,single labeled triple
v1.11.0,multiple labeled triples as list
v1.11.0,multiple labeled triples as array
v1.11.0,>>> keyword only
v1.11.0,fixme: find reason / enforce single-thread
v1.11.0,"DummyModel,"
v1.11.0,3x batch norm: bias + scale --> 6
v1.11.0,entity specific bias        --> 1
v1.11.0,==================================
v1.11.0,7
v1.11.0,"two bias terms, one conv-filter"
v1.11.0,Two linear layer biases
v1.11.0,"Two BN layers, bias & scale"
v1.11.0,Test that the weight in the MLP is trainable (i.e. requires grad)
v1.11.0,simulate creating a new triples factory with shared set of relations by shuffling
v1.11.0,quaternion have four components
v1.11.0,entity embeddings
v1.11.0,relation embeddings
v1.11.0,Compute Scores
v1.11.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.11.0,relation embeddings
v1.11.0,Compute Scores
v1.11.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.11.0,entity embeddings
v1.11.0,relation embeddings
v1.11.0,Compute Scores
v1.11.0,: 2xBN (bias & scale)
v1.11.0,the combination bias
v1.11.0,FIXME definitely a type mismatch going on here
v1.11.0,check shape
v1.11.0,check content
v1.11.0,empty lists are falsy
v1.11.0,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.11.0,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.11.0,pipeline is checked.
v1.11.0,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.11.0,Resume the previous pipeline
v1.11.0,The MockModel gives the highest score to the highest entity id
v1.11.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.11.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.11.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.11.0,test triple at hand
v1.11.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.11.0,test triple at hand
v1.11.0,cf. https://github.com/pykeen/pykeen/issues/1118
v1.11.0,save a reference to the old init *before* mocking
v1.11.0,run a small pipline
v1.11.0,use sampled training loop ...
v1.11.0,... without explicitly selecting a negative sampler ...
v1.11.0,... but providing custom kwargs
v1.11.0,other parameters for fast test
v1.11.0,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.11.0,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.11.0,Test looking up inverse triples
v1.11.0,test new label to ID
v1.11.0,type
v1.11.0,old labels
v1.11.0,"new, compact IDs"
v1.11.0,test vectorized lookup
v1.11.0,type
v1.11.0,shape
v1.11.0,value range
v1.11.0,only occurring Ids get mapped to non-negative numbers
v1.11.0,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.11.0,check type
v1.11.0,check shape
v1.11.0,check content
v1.11.0,check type
v1.11.0,check shape
v1.11.0,check 1-hot
v1.11.0,check type
v1.11.0,check shape
v1.11.0,check value range
v1.11.0,check self-similarity = 1
v1.11.0,base relation
v1.11.0,exact duplicate
v1.11.0,99% duplicate
v1.11.0,50% duplicate
v1.11.0,exact inverse
v1.11.0,99% inverse
v1.11.0,: The expected number of entities
v1.11.0,: The expected number of relations
v1.11.0,: The expected number of triples
v1.11.0,": The tolerance on expected number of triples, for randomized situations"
v1.11.0,: The dataset to test
v1.11.0,: The instantiated dataset
v1.11.0,: Should the validation be assumed to have been loaded with train/test?
v1.11.0,Not loaded
v1.11.0,Load
v1.11.0,Test caching
v1.11.0,assert (end - start) < 1.0e-02
v1.11.0,Test consistency of training / validation / testing mapping
v1.11.0,": The directory, if there is caching"
v1.11.0,: The batch size
v1.11.0,: The number of negatives per positive for sLCWA training loop.
v1.11.0,: The number of entities LCWA training loop / label smoothing.
v1.11.0,test reduction
v1.11.0,test finite loss value
v1.11.0,Test backward
v1.11.0,negative scores decreased compared to positive ones
v1.11.0,negative scores decreased compared to positive ones
v1.11.0,check for invalid keys
v1.11.0,check that each parameter without a default occurs
v1.11.0,try to instantiate loss for some configurations in the HPO search space
v1.11.0,: The number of entities.
v1.11.0,: The number of negative samples
v1.11.0,: The number of entities.
v1.11.0,"the relative tolerance for checking close results, cf. torch.allclose"
v1.11.0,"the absolute tolerance for checking close results, cf. torch.allclose"
v1.11.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.11.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.11.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.11.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.11.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.11.0,test multiple different initializations
v1.11.0,calculate by functional
v1.11.0,calculate manually
v1.11.0,allclose checks: | input - other | < atol + rtol * |other|
v1.11.0,simple
v1.11.0,nested
v1.11.0,nested
v1.11.0,prepare a temporary test directory
v1.11.0,check that file was created
v1.11.0,make sure to close file before trying to delete it
v1.11.0,delete intermediate files
v1.11.0,: The batch size
v1.11.0,: The device
v1.11.0,move test instance to device
v1.11.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.11.0,"verify that the regularizer is stored for both, entity and relation representations"
v1.11.0,Forward pass (should update regularizer)
v1.11.0,Call post_parameter_update (should reset regularizer)
v1.11.0,Check if regularization term is reset
v1.11.0,regularization term should be zero
v1.11.0,updated should be set to false
v1.11.0,call method
v1.11.0,generate random tensors
v1.11.0,generate inputs
v1.11.0,call update
v1.11.0,check shape
v1.11.0,check result
v1.11.0,generate single random tensor
v1.11.0,calculate penalty
v1.11.0,check shape
v1.11.0,check value
v1.11.0,update term
v1.11.0,check that the expected term is returned
v1.11.0,check that the regularizer is now reset
v1.11.0,create another instance with apply_only_once enabled
v1.11.0,test initial state
v1.11.0,"after first update, should change the term"
v1.11.0,"after second update, no change should happen"
v1.11.0,FIXME isn't any finite number allowed now?
v1.11.0,: Additional arguments passed to the training loop's constructor method
v1.11.0,: The triples factory instance
v1.11.0,: The batch size for use for forward_* tests
v1.11.0,: The embedding dimensionality
v1.11.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.11.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.11.0,: The batch size for use when testing training procedures
v1.11.0,: The number of epochs to train the model
v1.11.0,: A random number generator from torch
v1.11.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.11.0,initialization
v1.11.0,: Static extras to append to the CLI
v1.11.0,: the model's device
v1.11.0,: the inductive mode
v1.11.0,for reproducible testing
v1.11.0,insert shared parameters
v1.11.0,move model to correct device
v1.11.0,Check that all the parameters actually require a gradient
v1.11.0,Try to initialize an optimizer
v1.11.0,get model parameters
v1.11.0,re-initialize
v1.11.0,check that the operation works in-place
v1.11.0,check that the parameters where modified
v1.11.0,check for finite values by default
v1.11.0,check whether a gradient can be back-propgated
v1.11.0,TODO: look into score_r for inverse relations
v1.11.0,clear buffers for message passing models
v1.11.0,"For the high/low memory test cases of NTN, SE, etc."
v1.11.0,"else, leave to default"
v1.11.0,Make sure that inverse triples are created if create_inverse_triples=True
v1.11.0,triples factory is added by the pipeline
v1.11.0,TODO: Catch HolE MKL error?
v1.11.0,set regularizer term to something that isn't zero
v1.11.0,call post_parameter_update
v1.11.0,assert that the regularization term has been reset
v1.11.0,do one optimization step
v1.11.0,call post_parameter_update
v1.11.0,check model constraints
v1.11.0,Distance-based model
v1.11.0,dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)
v1.11.0,check type
v1.11.0,check shape
v1.11.0,create a new instance with guaranteed dropout
v1.11.0,set to training mode
v1.11.0,check for different output
v1.11.0,use more samples to make sure that enough values can be dropped
v1.11.0,this implicitly tests extra_repr / iter_extra_repr
v1.11.0,select random indices
v1.11.0,forward pass with full graph
v1.11.0,forward pass with restricted graph
v1.11.0,verify the results are similar
v1.11.0,: The number of entities
v1.11.0,: The number of triples
v1.11.0,: the message dim
v1.11.0,TODO: separation message vs. entity dim?
v1.11.0,check shape
v1.11.0,check dtype
v1.11.0,check finite values (e.g. due to division by zero)
v1.11.0,check non-negativity
v1.11.0,: the input dimension
v1.11.0,: the output dimension
v1.11.0,: the number of entities
v1.11.0,: the shape of the tensor to initialize
v1.11.0,: to be initialized / set in subclass
v1.11.0,: the interaction to use for testing a model
v1.11.0,initializers *may* work in-place => clone
v1.11.0,unfavourable split to ensure that cleanup is necessary
v1.11.0,check for unclean split
v1.11.0,check that no triple got lost
v1.11.0,check that triples where only moved from other to reference
v1.11.0,check that all entities occur in reference
v1.11.0,check that no triple got lost
v1.11.0,check that all entities are covered in first part
v1.11.0,the model
v1.11.0,Settings
v1.11.0,Use small model (untrained)
v1.11.0,Get batch
v1.11.0,Compute scores
v1.11.0,Compute mask only if required
v1.11.0,TODO: Re-use filtering code
v1.11.0,"shape: (batch_size, num_triples)"
v1.11.0,"shape: (batch_size, num_entities)"
v1.11.0,Process one batch
v1.11.0,shape
v1.11.0,value range
v1.11.0,no duplicates
v1.11.0,shape
v1.11.0,value range
v1.11.0,no duplicates
v1.11.0,shape
v1.11.0,value range
v1.11.0,"no repetition, except padding idx"
v1.11.0,inferred from triples factory
v1.11.0,: The batch size
v1.11.0,: the maximum number of candidates
v1.11.0,: the number of ranks
v1.11.0,: the number of samples to use for monte-carlo estimation
v1.11.0,: the number of candidates for each individual ranking task
v1.11.0,: the ranks for each individual ranking task
v1.11.0,data type
v1.11.0,value range
v1.11.0,original ranks
v1.11.0,better ranks
v1.11.0,variances are non-negative
v1.11.0,generate random weights such that sum = n
v1.11.0,for sanity checking: give the largest weight to best rank => should improve
v1.11.0,generate two versions
v1.11.0,1. repeat each rank/candidate pair a random number of times
v1.11.0,"2. do not repeat, but assign a corresponding weight"
v1.11.0,check flatness
v1.11.0,"TODO: does this suffice, or do we really need float as datatype?"
v1.11.0,generate random triples factories
v1.11.0,generate random alignment
v1.11.0,add label information if necessary
v1.11.0,prepare alignment data frame
v1.11.0,call
v1.11.0,check
v1.11.0,: The window size used by the early stopper
v1.11.0,: The mock losses the mock evaluator will return
v1.11.0,: The (zeroed) index  - 1 at which stopping will occur
v1.11.0,: The minimum improvement
v1.11.0,: The best results
v1.11.0,Step early stopper
v1.11.0,check storing of results
v1.11.0,not needed for test
v1.11.0,verify that the input is valid
v1.11.0,combine
v1.11.0,verify shape
v1.11.0,to be initialized in subclass
v1.11.0,no column has been removed
v1.11.0,all old columns are unmodified
v1.11.0,new columns are boolean
v1.11.0,no columns have been added
v1.11.0,check subset relation
v1.11.0,check for unique values
v1.11.0,check for subset property
v1.11.0,maybe additional checks
v1.11.0,TODO: this could be shared with the model tests
v1.11.0,fixme: CompGCN leads to an autograd runtime error...
v1.11.0,"models.CompGCN: dict(embedding_dim=EMBEDDING_DIM),"
v1.11.0,"FixedModel: dict(embedding_dim=EMBEDDING_DIM),"
v1.11.0,test combinations of models with training loops
v1.11.0,some models require inverse relations
v1.11.0,some model require access to the training triples
v1.11.0,"inductive models require an inductive mode to be set, and an inference factory to be passed"
v1.11.0,fake an inference factory
v1.11.0,automatically choose accelerator
v1.11.0,defaults to TensorBoard; explicitly disabled here
v1.11.0,disable checkpointing
v1.11.0,fast run
v1.11.0,automatically choose accelerator
v1.11.0,defaults to TensorBoard; explicitly disabled here
v1.11.0,disable checkpointing
v1.11.0,fast run
v1.11.0,check for finite values by default
v1.11.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.11.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.11.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.11.0,≈ result of softmax
v1.11.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.11.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.11.0,sum over the softmax dim as weights sum up to 1
v1.11.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.11.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.11.0,≈ result of sigmoid
v1.11.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.11.0,expected_loss ≈ 0.34
v1.11.0,abstract classes
v1.11.0,Create dummy dense labels
v1.11.0,Check if labels form a probability distribution
v1.11.0,Apply label smoothing
v1.11.0,Check if smooth labels form probability distribution
v1.11.0,Create dummy sLCWA labels
v1.11.0,Apply label smoothing
v1.11.0,generate random ratios
v1.11.0,check size
v1.11.0,check value range
v1.11.0,check total split
v1.11.0,check consistency with ratios
v1.11.0,the number of decimal digits equivalent to 1 / n_total
v1.11.0,check type
v1.11.0,check values
v1.11.0,compare against expected
v1.11.0,generated_triples = generate_triples()
v1.11.0,check type
v1.11.0,check format
v1.11.0,check coverage
v1.11.0,prediction post-processing
v1.11.0,mock prediction data frame
v1.11.0,score consumers
v1.11.0,"use a small model, since operation is expensive"
v1.11.0,"all scores, automatic batch size"
v1.11.0,top 3 scores
v1.11.0,"top 3 scores, fixed batch size, head scoring"
v1.11.0,"all scores, relation scoring"
v1.11.0,"all scores, relation scoring"
v1.11.0,model with inverse relations
v1.11.0,check type
v1.11.0,check shape
v1.11.0,check ID ranges
v1.11.0,"mapped triples, automatic batch size selection, no factory"
v1.11.0,"mapped triples, fixed batch size, no factory"
v1.11.0,labeled triples with factory
v1.11.0,labeled triples as list
v1.11.0,single labeled triple
v1.11.0,model with inverse relations
v1.11.0,"ID-based, no factory"
v1.11.0,string-based + factory
v1.11.0,mixed + factory
v1.11.0,"no restriction, no factory"
v1.11.0,"no restriction, factory"
v1.11.0,"id restriction, no factory ..."
v1.11.0,id restriction with factory
v1.11.0,"comment: we only use id-based input, since the normalization has already been tested"
v1.11.0,create model
v1.11.0,"id-based head/relation/tail prediction, no restriction"
v1.11.0,restriction by list of ints
v1.11.0,tail prediction
v1.11.0,try accessing each element
v1.11.0,"naive implementation, O(n2)"
v1.11.0,check correct output type
v1.11.0,check value range subset
v1.11.0,check value range side
v1.11.0,check columns
v1.11.0,check value range and type
v1.11.0,check value range entity IDs
v1.11.0,check value range entity labels
v1.11.0,check correct type
v1.11.0,check relation_id value range
v1.11.0,check pattern value range
v1.11.0,check confidence value range
v1.11.0,check support value range
v1.11.0,check correct type
v1.11.0,check relation_id value range
v1.11.0,check pattern value range
v1.11.0,check correct type
v1.11.0,check relation_id value range
v1.11.0,check case
v1.11.0,apply restriction
v1.11.0,check monotonicity (in counts)
v1.11.0,check factories
v1.11.0,clear
v1.11.0,docstr-coverage: inherited
v1.11.0,assumes deterministic entity to id mapping
v1.11.0,from left_tf
v1.11.0,from right_tf with offset
v1.11.0,docstr-coverage: inherited
v1.11.0,assumes deterministic entity to id mapping
v1.11.0,from left_tf
v1.11.0,from right_tf with offset
v1.11.0,extra-relation
v1.11.0,docstr-coverage: inherited
v1.11.0,assumes deterministic entity to id mapping
v1.11.0,docstr-coverage: inherited
v1.11.0,assumes deterministic entity to id mapping
v1.11.0,from left_tf
v1.11.0,from right_tf with offset
v1.11.0,additional
v1.11.0,verify shape
v1.11.0,verify dtype
v1.11.0,verify number of entities/relations
v1.11.0,verify offsets
v1.11.0,"create old, new pairs"
v1.11.0,simulate merging ids
v1.11.0,only a single pair
v1.11.0,apply
v1.11.0,every key is contained
v1.11.0,value range
v1.11.0,Check minimal statistics
v1.11.0,Check either a github link or author/publication information is given
v1.11.0,TODO: we could move this part into the interaction module itself
v1.11.0,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.11.0,"prepare conv input (N, C, H, W)"
v1.11.0,"f(h,r,t) = u^T act(h^T W t + V [h; t] + b)"
v1.11.0,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.11.0,hidden state:
v1.11.0,"1. ""h^T W t"""
v1.11.0,"2. ""V [h; t]"""
v1.11.0,"3. ""+ b"""
v1.11.0,activation
v1.11.0,projection
v1.11.0,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.11.0,"we calculate the scores using the hard-coded formula, instead of utilizing table + einsum"
v1.11.0,"f(h, r, t) = h @ r @ t"
v1.11.0,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.11.0,normalize rotations to unit modulus
v1.11.0,check for unit modulus
v1.11.0,entity embeddings
v1.11.0,relation embeddings
v1.11.0,Compute Scores
v1.11.0,entity embeddings
v1.11.0,relation embeddings
v1.11.0,Compute Scores
v1.11.0,Compute Scores
v1.11.0,-\|R_h h - R_t t\|
v1.11.0,-\|h - t\|
v1.11.0,"Since MuRE has offsets, the scores do not need to negative"
v1.11.0,"We do not need this, since we do not check for functional consistency anyway"
v1.11.0,intra-interaction comparison
v1.11.0,dimension needs to be divisible by num_heads
v1.11.0,FIXME
v1.11.0,multiple
v1.11.0,single
v1.11.0,head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid
v1.11.0,message_dim must be divisible by num_heads
v1.11.0,generate test data (with fixed seed for reproducibility)
v1.11.0,get result using argpartition
v1.11.0,check shape
v1.11.0,check type
v1.11.0,check value range
v1.11.0,check equality with argsort
v1.11.0,determine pool using anchor searcher
v1.11.0,determine expected pool using shortest path distances via scipy.sparse.csgraph
v1.11.0,generate random pool
v1.11.0,complex tensor
v1.11.0,check value range
v1.11.0,check modulus == 1
v1.11.0,quaternion needs shape to end on 4
v1.11.0,"check value range (actually [-s, +s] with s = 1/sqrt(2*n))"
v1.11.0,value range
v1.11.0,highest degree node has largest value
v1.11.0,Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf
v1.11.0,create triples with a dummy relation type 0
v1.11.0,"0: green: 2, 3, 7, 8"
v1.11.0,"1: red: 1, 4, 6, 9"
v1.11.0,"2: blue: 0, 5"
v1.11.0,the example includes the first power
v1.11.0,requires at least one complex tensor as input
v1.11.0,check type
v1.11.0,check size
v1.11.0,check value range
v1.11.0,inferred from triples factory
v1.11.0,inferred from assignment
v1.11.0,the representation module infers the max_id from the provided labels
v1.11.0,the following entity does not have an image -> will have to use backfill
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,the representation module infers the max_id from the provided labels
v1.11.0,docstr-coverage: inherited
v1.11.0,the representation module infers the max_id from the provided labels
v1.11.0,max_id is inferred from assignment
v1.11.0,create random assignment
v1.11.0,update kwargs
v1.11.0,empty bases
v1.11.0,inconsistent base shapes
v1.11.0,invalid base id
v1.11.0,invalid local index
v1.11.0,docstr-coverage: inherited
v1.11.0,allocate result
v1.11.0,prepare distributions
v1.11.0,ensure positivity for variance
v1.11.0,compute using pykeen
v1.11.0,"e: (batch_size, num_heads, num_tails, d)"
v1.11.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.11.0,divergence = 0 => similarity = -divergence = 0
v1.11.0,"(h - t), r"
v1.11.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.11.0,divergence >= 0 => similarity = -divergence <= 0
v1.11.0,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.11.0,filter vs. no filter thing.
v1.11.0,TODO: more tests
v1.11.0,check for empty batches
v1.11.0,: The window size used by the early stopper
v1.11.0,: The mock losses the mock evaluator will return
v1.11.0,: The (zeroed) index  - 1 at which stopping will occur
v1.11.0,: The minimum improvement
v1.11.0,: The best results
v1.11.0,Set automatic_memory_optimization to false for tests
v1.11.0,Train a model in one shot
v1.11.0,Train a model for the first half
v1.11.0,Continue training of the first part
v1.11.0,check non-empty metrics
v1.11.0,: Should negative samples be filtered?
v1.11.0,expectation = (1 + n) / 2
v1.11.0,variance = (n**2 - 1) / 12
v1.11.0,"x_i ~ N(mu_i, 1)"
v1.11.0,closed-form solution
v1.11.0,sampled confidence interval
v1.11.0,check that closed-form is in confidence interval of sampled
v1.11.0,positive values only
v1.11.0,positive and negative values
v1.11.0,Check for correct class
v1.11.0,check correct num_entities
v1.11.0,check type
v1.11.0,check length
v1.11.0,check type
v1.11.0,check length
v1.11.0,check confidence positivity
v1.11.0,Check for correct class
v1.11.0,"true_score: (2, 3, 3)"
v1.11.0,head based filter
v1.11.0,preprocessing for faster lookup
v1.11.0,check that all found positives are positive
v1.11.0,check in-place
v1.11.0,Test head scores
v1.11.0,Assert in-place modification
v1.11.0,Assert correct filtering
v1.11.0,Test tail scores
v1.11.0,Assert in-place modification
v1.11.0,Assert correct filtering
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,The MockModel gives the highest score to the highest entity id
v1.11.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.11.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.11.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.11.0,test triple at hand
v1.11.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.11.0,test triple at hand
v1.11.0,check true negatives
v1.11.0,TODO: check no repetitions (if possible)
v1.11.0,return type
v1.11.0,columns
v1.11.0,value range
v1.11.0,relation restriction
v1.11.0,with explicit num_entities
v1.11.0,with inferred num_entities
v1.11.0,test different shapes
v1.11.0,test different shapes
v1.11.0,value range
v1.11.0,value range
v1.11.0,check unique
v1.11.0,"strips off the ""k"" at the end"
v1.11.0,Populate with real results.
v1.11.0,"(-1, 1),"
v1.11.0,"(-1, -1),"
v1.11.0,"(-5, -3),"
v1.11.0,initialize
v1.11.0,update with batches
v1.11.0,Check whether filtering works correctly
v1.11.0,First giving an example where all triples have to be filtered
v1.11.0,The filter should remove all triples
v1.11.0,Create an example where no triples will be filtered
v1.11.0,The filter should not remove any triple
v1.11.0,same relation
v1.11.0,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.11.0,Test that half of the subjects and half of the objects are corrupted
v1.11.0,check that corrupted entities co-occur with the relation in training data
v1.11.0,: The batch size
v1.11.0,: The random seed
v1.11.0,: The triples factory
v1.11.0,: The instances
v1.11.0,: A positive batch
v1.11.0,: Kwargs
v1.11.0,Generate negative sample
v1.11.0,check filter shape if necessary
v1.11.0,check shape
v1.11.0,check bounds: heads
v1.11.0,check bounds: relations
v1.11.0,check bounds: tails
v1.11.0,test that the negative triple is not the original positive triple
v1.11.0,"shape: (batch_size, 1, num_neg)"
v1.11.0,Base Classes
v1.11.0,Concrete Classes
v1.11.0,Utils
v1.11.0,: synonyms of this loss
v1.11.0,: The default strategy for optimizing the loss's hyper-parameters
v1.11.0,flatten and stack
v1.11.0,apply label smoothing if necessary.
v1.11.0,TODO: Do label smoothing only once
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,Sanity check
v1.11.0,negative_scores have already been filtered in the sampler!
v1.11.0,"shape: (nnz,)"
v1.11.0,docstr-coverage: inherited
v1.11.0,Sanity check
v1.11.0,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.11.0,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.11.0,positive entries in the labels tensor.
v1.11.0,"This shows how often one row has to be repeated,"
v1.11.0,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.11.0,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.11.0,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.11.0,This tells us how often each true label should be repeated
v1.11.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.11.0,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.11.0,within the ablation pipeline.
v1.11.0,0. default
v1.11.0,1. positive & negative margin
v1.11.0,2. negative margin & offset
v1.11.0,3. positive margin & offset
v1.11.0,docstr-coverage: inherited
v1.11.0,Sanity check
v1.11.0,positive term
v1.11.0,implicitly repeat positive scores
v1.11.0,"shape: (nnz,)"
v1.11.0,negative term
v1.11.0,negative_scores have already been filtered in the sampler!
v1.11.0,docstr-coverage: inherited
v1.11.0,Sanity check
v1.11.0,"scale labels from [0, 1] to [-1, 1]"
v1.11.0,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.11.0,within the ablation pipeline.
v1.11.0,docstr-coverage: inherited
v1.11.0,negative_scores have already been filtered in the sampler!
v1.11.0,(dense) softmax requires unfiltered scores / masking
v1.11.0,we need to fill the scores with -inf for all filtered negative examples
v1.11.0,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.11.0,use filled negatives scores
v1.11.0,docstr-coverage: inherited
v1.11.0,we need dense negative scores => unfilter if necessary
v1.11.0,"we may have inf rows, since there will be one additional finite positive score per row"
v1.11.0,"combine scores: shape: (batch_size, num_negatives + 1)"
v1.11.0,use sparse version of cross entropy
v1.11.0,calculate cross entropy loss
v1.11.0,docstr-coverage: inherited
v1.11.0,make sure labels form a proper probability distribution
v1.11.0,calculate cross entropy loss
v1.11.0,docstr-coverage: inherited
v1.11.0,determine positive; do not check with == since the labels are floats
v1.11.0,subtract margin from positive scores
v1.11.0,divide by temperature
v1.11.0,docstr-coverage: inherited
v1.11.0,subtract margin from positive scores
v1.11.0,normalize positive score shape
v1.11.0,divide by temperature
v1.11.0,docstr-coverage: inherited
v1.11.0,determine positive; do not check with == since the labels are floats
v1.11.0,compute negative weights (without gradient tracking)
v1.11.0,clone is necessary since we modify in-place
v1.11.0,Split positive and negative scores
v1.11.0,"we pass *all* scores as negatives, but set the weight of positives to zero"
v1.11.0,this allows keeping a dense shape
v1.11.0,docstr-coverage: inherited
v1.11.0,Sanity check
v1.11.0,"we do not allow full -inf rows, since we compute the softmax over this tensor"
v1.11.0,compute weights (without gradient tracking)
v1.11.0,"fill negative scores with some finite value, e.g., 0 (they will get masked out anyway)"
v1.11.0,note: this is a reduction along the softmax dim; since the weights are already normalized
v1.11.0,"to sum to one, we want a sum reduction here, instead of using the self._reduction"
v1.11.0,docstr-coverage: inherited
v1.11.0,Sanity check
v1.11.0,docstr-coverage: inherited
v1.11.0,Sanity check
v1.11.0,negative loss part
v1.11.0,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.11.0,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.11.0,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: maybe we can make this more efficient?
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: maybe we can make this more efficient?
v1.11.0,docstr-coverage: inherited
v1.11.0,: A resolver for loss modules
v1.11.0,TODO: method is_inverse?
v1.11.0,TODO: inverse of inverse?
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.11.0,Id of inverse relation: relation + 1
v1.11.0,docstr-coverage: inherited
v1.11.0,: A resolver for relation inverter protocols
v1.11.0,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.11.0,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.11.0,": For more information, see https://github.com/cthoyt/pystow"
v1.11.0,: A path representing the PyKEEN data folder
v1.11.0,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.11.0,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.11.0,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.11.0,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.11.0,: A subdirectory for PyKEEN logs
v1.11.0,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.11.0,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.11.0,"TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728"
v1.11.0,"SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)"
v1.11.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.11.0,get datatype specific epsilon
v1.11.0,clamp minimum value
v1.11.0,try to resolve ambiguous device; there has to be at least one cuda device
v1.11.0,lower bound
v1.11.0,upper bound
v1.11.0,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.11.0,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.11.0,Determine optimal order and cost
v1.11.0,translate back to original order
v1.11.0,determine optimal processing order
v1.11.0,heuristic
v1.11.0,The dimensions affected by e'
v1.11.0,Project entities
v1.11.0,r_p (e_p.T e) + e'
v1.11.0,Enforce constraints
v1.11.0,upgrade to sequence
v1.11.0,broadcast
v1.11.0,"normalize ids: -> ids.shape: (batch_size, num_ids)"
v1.11.0,"normalize batch -> batch.shape: (batch_size, 1, 3)"
v1.11.0,allocate memory
v1.11.0,copy ids
v1.11.0,reshape
v1.11.0,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.11.0,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.11.0,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.11.0,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.11.0,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.11.0,"note: this is a hack, and should be fixed up-stream by making NodePiece"
v1.11.0,"use proper complex embeddings for rotate interaction; however, we also have representations"
v1.11.0,"that perform message passing, and we would need to propagate the base representation's complexity through it"
v1.11.0,ensure pathlib
v1.11.0,cf. https://stackoverflow.com/a/1176023
v1.11.0,check validity
v1.11.0,path compression
v1.11.0,get representatives
v1.11.0,already merged
v1.11.0,make x the smaller one
v1.11.0,merge
v1.11.0,extract partitions
v1.11.0,resolve path to make sure it is an absolute path
v1.11.0,ensure directory exists
v1.11.0,message passing: collect colors of neighbors
v1.11.0,"dense colors: shape: (n, c)"
v1.11.0,"adj:          shape: (n, n)"
v1.11.0,"values need to be float, since torch.sparse.mm does not support integer dtypes"
v1.11.0,size: will be correctly inferred
v1.11.0,concat with old colors
v1.11.0,hash
v1.11.0,create random indicator functions of low dimensionality
v1.11.0,collect neighbors' colors
v1.11.0,round to avoid numerical effects
v1.11.0,hash first
v1.11.0,concat with old colors
v1.11.0,re-hash
v1.11.0,"only keep connectivity, but remove multiplicity"
v1.11.0,"note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,"
v1.11.0,this is rather useless
v1.11.0,initial: degree
v1.11.0,"note: we calculate this separately, since we can use a more efficient implementation for the first step"
v1.11.0,hash
v1.11.0,determine small integer type for dense count array
v1.11.0,convergence check
v1.11.0,each node has a unique color
v1.11.0,the number of colors did not improve in the last iteration
v1.11.0,cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports
v1.11.0,docstr-coverage: excused `wrapped`
v1.11.0,cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset
v1.11.0,Circular correlation of entity embeddings
v1.11.0,complex conjugate
v1.11.0,Hadamard product in frequency domain
v1.11.0,inverse real FFT
v1.11.0,Base Class
v1.11.0,Child classes
v1.11.0,Utils
v1.11.0,: The overall regularization weight
v1.11.0,: The current regularization term (a scalar)
v1.11.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.11.0,: Has this regularizer been updated since last being reset?
v1.11.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.11.0,"If there are tracked parameters, update based on them"
v1.11.0,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.11.0,docstr-coverage: inherited
v1.11.0,no need to compute anything
v1.11.0,docstr-coverage: inherited
v1.11.0,always return zero
v1.11.0,: The dimension along which to compute the vector-based regularization terms.
v1.11.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.11.0,: This allows dimensionality-independent weight tuning.
v1.11.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.11.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.11.0,docstr-coverage: inherited
v1.11.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.11.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.11.0,docstr-coverage: inherited
v1.11.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.11.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.11.0,regularizer-specific parameters
v1.11.0,docstr-coverage: inherited
v1.11.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.11.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,orthogonality soft constraint: cosine similarity at most epsilon
v1.11.0,The normalization factor to balance individual regularizers' contribution.
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,: A resolver for regularizers
v1.11.0,high-level
v1.11.0,Low-Level
v1.11.0,cf. https://github.com/python/mypy/issues/5374
v1.11.0,": the dataframe; has to have a column named ""score"""
v1.11.0,: an optional factory to use for labeling
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,: the prediction target
v1.11.0,: the other column's fixed IDs
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,": the ID-based triples, shape: (n, 3)"
v1.11.0,: the scores
v1.11.0,3-tuple for return
v1.11.0,"extract label information, if possible"
v1.11.0,no restriction
v1.11.0,restriction is a tensor
v1.11.0,restriction is a sequence of integers or strings
v1.11.0,"now, restriction is a sequence of integers"
v1.11.0,"if explicit ids have been given, and label information is available, extract list of labels"
v1.11.0,exactly one of them is None
v1.11.0,create input batch
v1.11.0,"note type alias annotation required,"
v1.11.0,cf. https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases
v1.11.0,"batch, TODO: ids?"
v1.11.0,docstr-coverage: inherited
v1.11.0,initialize buffer on device
v1.11.0,docstr-coverage: inherited
v1.11.0,"reshape, shape: (batch_size * num_entities,)"
v1.11.0,get top scores within batch
v1.11.0,determine corresponding indices
v1.11.0,"batch_id, score_id = divmod(top_indices, num_scores)"
v1.11.0,combine to top triples
v1.11.0,append to global top scores
v1.11.0,reduce size if necessary
v1.11.0,initialize buffer on cpu
v1.11.0,Explicitly create triples
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: variable targets across batches/samples?
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"(?, r, t) => r.stride > t.stride"
v1.11.0,"(h, ?, t) => h.stride > t.stride"
v1.11.0,"(h, r, ?) => h.stride > r.stride"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,train model; note: needs larger number of epochs to do something useful ;-)
v1.11.0,"create prediction dataset, where the head entities is from a set of European countries,"
v1.11.0,and the relations are connected to tourism
v1.11.0,"calculate all scores for this restricted set, and keep k=3 largest"
v1.11.0,add labels
v1.11.0,: the choices for the first and second component of the input batch
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,calculate batch scores onces
v1.11.0,consume by all consumers
v1.11.0,TODO: Support partial dataset
v1.11.0,note: the models' predict method takes care of setting the model to evaluation mode
v1.11.0,exactly one of them is None
v1.11.0,
v1.11.0,TODO: add support for (automatic) slicing
v1.11.0,note: the models' predict method takes care of setting the model to evaluation mode
v1.11.0,get input & target
v1.11.0,get label-to-id mapping and prediction targets
v1.11.0,get scores
v1.11.0,"note: maybe we want to expose these scores, too?"
v1.11.0,create raw dataframe
v1.11.0,note: the models' predict method takes care of setting the model to evaluation mode
v1.11.0,normalize input
v1.11.0,calculate scores (with automatic memory optimization)
v1.11.0,determine fully qualified name
v1.11.0,shorten to main module
v1.11.0,verify that short name can be imported from the abbreviated reference
v1.11.0,get docdata and extract name & citation
v1.11.0,fallback for name: capitalized class name without base suffix
v1.11.0,extract citation information and warn about lack thereof
v1.11.0,compose reference
v1.11.0,cf. https://github.com/python/mypy/issues/5374
v1.11.0,"""Closed-Form Expectation"","
v1.11.0,"""Closed-Form Variance"","
v1.11.0,"""✓"" if metric.closed_expectation else """","
v1.11.0,"""✓"" if metric.closed_variance else """","
v1.11.0,Add HPO command
v1.11.0,Add NodePiece tokenization command
v1.11.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.11.0,General types
v1.11.0,Triples
v1.11.0,Others
v1.11.0,Tensor Functions
v1.11.0,Tensors
v1.11.0,Dataclasses
v1.11.0,prediction targets
v1.11.0,modes
v1.11.0,entity alignment sides
v1.11.0,utils
v1.11.0,"some PyTorch functions to not properly propagate types (e.g., float() does not return FloatTensor but Tensor)"
v1.11.0,however it is still useful to distinguish float tensors from long ones
v1.11.0,"to make the switch easier once PyTorch improves typing, we use a global type alias inside PyKEEN."
v1.11.0,: A function that mutates the input and returns a new object of the same type as output
v1.11.0,: A function that can be applied to a tensor to initialize it
v1.11.0,: A function that can be applied to a tensor to normalize it
v1.11.0,: A function that can be applied to a tensor to constrain it
v1.11.0,: A hint for a :class:`torch.device`
v1.11.0,: A hint for a :class:`torch.Generator`
v1.11.0,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.11.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.11.0,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.11.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.11.0,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.11.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.11.0,: the inductive prediction and training mode
v1.11.0,: the prediction target
v1.11.0,: the prediction target index
v1.11.0,: the rank types
v1.11.0,"RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8"
v1.11.0,entity alignment
v1.11.0,input normalization
v1.11.0,note: the base class does not have any parameters
v1.11.0,Heuristic for default value
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"note: the only parameters are inside the relation representation module, which has its own reset_parameters"
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: can we change the dimension order to make this contiguous?
v1.11.0,docstr-coverage: inherited
v1.11.0,normalize num blocks
v1.11.0,determine necessary padding
v1.11.0,determine block sizes
v1.11.0,"(R, nb, bsi, bso)"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,apply padding if necessary
v1.11.0,"(n, di) -> (n, nb, bsi)"
v1.11.0,"(n, nb, bsi), (R, nb, bsi, bso) -> (R, n, nb, bso)"
v1.11.0,"(R, n, nb, bso) -> (R * n, do)"
v1.11.0,"note: depending on the contracting order, the output may supporting viewing, or not"
v1.11.0,"(n, R * n), (R * n, do) -> (n, do)"
v1.11.0,remove padding if necessary
v1.11.0,docstr-coverage: inherited
v1.11.0,apply padding if necessary
v1.11.0,"(R * n, n), (n, di) -> (R * n, di)"
v1.11.0,"(R * n, di) -> (R, n, nb, bsi)"
v1.11.0,"(R, nb, bsi, bso), (R, n, nb, bsi) -> (n, nb, bso)"
v1.11.0,"(n, nb, bso) -> (n, do)"
v1.11.0,"note: depending on the contracting order, the output may supporting viewing, or not"
v1.11.0,remove padding if necessary
v1.11.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501
v1.11.0,there are separate decompositions for forward and backward relations.
v1.11.0,the self-loop weight is not decomposed.
v1.11.0,TODO: we could cache the stacked adjacency matrices
v1.11.0,self-loop
v1.11.0,forward messages
v1.11.0,backward messages
v1.11.0,activation
v1.11.0,input validation
v1.11.0,has to be imported now to avoid cyclic imports
v1.11.0,has to be assigned after call to nn.Module init
v1.11.0,Resolve edge weighting
v1.11.0,dropout
v1.11.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.11.0,no activation on last layer
v1.11.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501
v1.11.0,buffering of enriched representations
v1.11.0,docstr-coverage: inherited
v1.11.0,invalidate enriched embeddings
v1.11.0,docstr-coverage: inherited
v1.11.0,Bind fields
v1.11.0,"shape: (num_entities, embedding_dim)"
v1.11.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.11.0,Get random dropout mask
v1.11.0,Apply to edges
v1.11.0,fixed edges -> pre-compute weights
v1.11.0,Cache enriched representations
v1.11.0,Utils
v1.11.0,: the maximum ID (exclusively)
v1.11.0,: the shape of an individual representation
v1.11.0,: a normalizer for individual representations
v1.11.0,: a regularizer for individual representations
v1.11.0,: dropout
v1.11.0,heuristic
v1.11.0,normalize *before* repeating
v1.11.0,repeat if necessary
v1.11.0,regularize *after* repeating
v1.11.0,"dropout & regularizer will appear automatically, since it is a nn.Module"
v1.11.0,has to be imported here to avoid cyclic import
v1.11.0,docstr-coverage: inherited
v1.11.0,normalize num_embeddings vs. max_id
v1.11.0,normalize embedding_dim vs. shape
v1.11.0,work-around until full complex support (torch==1.10 still does not work)
v1.11.0,TODO: verify that this is our understanding of complex!
v1.11.0,"note: this seems to work, as finfo returns the datatype of the underlying floating"
v1.11.0,"point dtype, rather than the combined complex one"
v1.11.0,"use make for initializer since there's a default, and make_safe"
v1.11.0,for the others to pass through None values
v1.11.0,docstr-coverage: inherited
v1.11.0,initialize weights in-place
v1.11.0,docstr-coverage: inherited
v1.11.0,apply constraints in-place
v1.11.0,fixme: work-around until nn.Embedding supports complex
v1.11.0,docstr-coverage: inherited
v1.11.0,fixme: work-around until nn.Embedding supports complex
v1.11.0,verify that contiguity is preserved
v1.11.0,create low-rank approximation object
v1.11.0,"get base representations, shape: (n, *ds)"
v1.11.0,"calculate SVD, U.shape: (n, k), s.shape: (k,), u.shape: (k, prod(ds))"
v1.11.0,overwrite bases and weights
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"get all base representations, shape: (num_bases, *shape)"
v1.11.0,"get base weights, shape: (*batch_dims, num_bases)"
v1.11.0,"weighted linear combination of bases, shape: (*batch_dims, *shape)"
v1.11.0,: Constrainers
v1.11.0,:
v1.11.0,: - :func:`torch.nn.functional.normalize`
v1.11.0,: - :func:`complex_normalize`
v1.11.0,: - :func:`torch.clamp`
v1.11.0,: - :func:`clamp_norm`
v1.11.0,": Normalizers, which has by default:"
v1.11.0,:
v1.11.0,: - :func:`torch.nn.functional.normalize`
v1.11.0,normalize output dimension
v1.11.0,entity-relation composition
v1.11.0,edge weighting
v1.11.0,message passing weights
v1.11.0,linear relation transformation
v1.11.0,layer-specific self-loop relation representation
v1.11.0,other components
v1.11.0,initialize
v1.11.0,split
v1.11.0,compose
v1.11.0,transform
v1.11.0,normalization
v1.11.0,aggregate by sum
v1.11.0,dropout
v1.11.0,prepare for inverse relations
v1.11.0,update entity representations: mean over self-loops / forward edges / backward edges
v1.11.0,Relation transformation
v1.11.0,has to be imported here to avoid cyclic imports
v1.11.0,kwargs
v1.11.0,Buffered enriched entity and relation representations
v1.11.0,TODO: Check
v1.11.0,TODO: might not be true for all compositions
v1.11.0,hidden dimension normalization
v1.11.0,Create message passing layers
v1.11.0,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.11.0,TODO: This always uses all training triples for message passing
v1.11.0,initialize buffer of enriched representations
v1.11.0,docstr-coverage: inherited
v1.11.0,invalidate enriched embeddings
v1.11.0,docstr-coverage: inherited
v1.11.0,"when changing from evaluation to training mode, the buffered representations have been computed without"
v1.11.0,"gradient tracking. hence, we need to invalidate them."
v1.11.0,note: this occurs in practice when continuing training after evaluation.
v1.11.0,enrich
v1.11.0,docstr-coverage: inherited
v1.11.0,check max_id
v1.11.0,infer shape
v1.11.0,"assign after super, since they should be properly registered as submodules"
v1.11.0,docstr-coverage: inherited
v1.11.0,: the base representations
v1.11.0,: the combination module
v1.11.0,input normalization
v1.11.0,has to be imported here to avoid cyclic import
v1.11.0,create base representations
v1.11.0,verify same ID range
v1.11.0,"note: we could also relax the requiremen, and set max_id = min(max_ids)"
v1.11.0,shape inference
v1.11.0,assign base representations *after* super init
v1.11.0,docstr-coverage: inherited
v1.11.0,delegate to super class
v1.11.0,docstr-coverage: inherited
v1.11.0,Generate graph dataset from the Monarch Disease Ontology (MONDO)
v1.11.0,": the assignment from global ID to (representation, local id), shape: (max_id, 2)"
v1.11.0,import here to avoid cyclic import
v1.11.0,instantiate base representations if necessary
v1.11.0,there needs to be at least one base
v1.11.0,"while possible, this might be unintended"
v1.11.0,extract shape
v1.11.0,check for invalid base ids
v1.11.0,check for invalid local indices
v1.11.0,assign modules / buffers *after* super init
v1.11.0,docstr-coverage: inherited
v1.11.0,flatten assignment to ease construction of inverse indices
v1.11.0,we group indices by the representation which provides them
v1.11.0,"thus, we need an inverse to restore the correct order"
v1.11.0,get representations
v1.11.0,update inverse indices
v1.11.0,invert flattening
v1.11.0,import here to avoid cyclic import
v1.11.0,comment: not all representations support passing a shape parameter
v1.11.0,create assignment
v1.11.0,base
v1.11.0,other
v1.11.0,import here to avoid cyclic import
v1.11.0,infer shape
v1.11.0,infer max_id
v1.11.0,docstr-coverage: inherited
v1.11.0,"TODO: can be a combined representations, with appropriate tensor-train combination"
v1.11.0,": shape: (max_id, num_cores)"
v1.11.0,": the bases, length: num_cores, with compatible shapes"
v1.11.0,check shape
v1.11.0,check value range
v1.11.0,"do not increase counter i, since the dimension is shared with the following term"
v1.11.0,i += 1
v1.11.0,ids //= m_i
v1.11.0,import here to avoid cyclic import
v1.11.0,normalize ranks
v1.11.0,"determine M_k, N_k"
v1.11.0,TODO: allow to pass them from outside?
v1.11.0,normalize assignment
v1.11.0,determine shapes and einsum equation
v1.11.0,create base representations
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,abstract
v1.11.0,concrete classes
v1.11.0,: A resolver for PyG message passing layers
v1.11.0,default flow
v1.11.0,: the message passing layers
v1.11.0,: the flow direction of messages across layers
v1.11.0,": the edge index, shape: (2, num_edges)"
v1.11.0,fail if dependencies are missing
v1.11.0,avoid cyclic import
v1.11.0,"the base representations, e.g., entity embeddings or features"
v1.11.0,verify max_id
v1.11.0,verify shape
v1.11.0,assign sub-module *after* super call
v1.11.0,initialize layers
v1.11.0,normalize activation
v1.11.0,check consistency
v1.11.0,buffer edge index for message passing
v1.11.0,TODO: inductiveness; we need to
v1.11.0,* replace edge_index
v1.11.0,* replace base representations
v1.11.0,* keep layers & activations
v1.11.0,docstr-coverage: inherited
v1.11.0,we can restrict the message passing to the k-hop neighborhood of the desired indices;
v1.11.0,this does only make sense if we do not request *all* indices
v1.11.0,k_hop_subgraph returns:
v1.11.0,(1) the nodes involved in the subgraph
v1.11.0,(2) the filtered edge_index connectivity
v1.11.0,"(3) the mapping from node indices in node_idx to their new location, and"
v1.11.0,(4) the edge mask indicating which edges were preserved
v1.11.0,we only need the base representations for the neighbor indices
v1.11.0,get *all* base representations
v1.11.0,use *all* edges
v1.11.0,perform message passing
v1.11.0,select desired indices
v1.11.0,docstr-coverage: inherited
v1.11.0,": the edge type, shape: (num_edges,)"
v1.11.0,register an additional buffer for the categorical edge type
v1.11.0,docstr-coverage: inherited
v1.11.0,: the relation representations used to obtain initial edge features
v1.11.0,avoid cyclic import
v1.11.0,docstr-coverage: inherited
v1.11.0,get initial relation representations
v1.11.0,select edge attributes from relation representations according to relation type
v1.11.0,perform message passing
v1.11.0,"apply relation transformation, if necessary"
v1.11.0,Classes
v1.11.0,Resolver
v1.11.0,backwards compatibility
v1.11.0,scaling factor
v1.11.0,"modulus ~ Uniform[-s, s]"
v1.11.0,"phase ~ Uniform[0, 2*pi]"
v1.11.0,real part
v1.11.0,purely imaginary quaternions unitary
v1.11.0,this is usually loaded from somewhere else
v1.11.0,"the shape must match, as well as the entity-to-id mapping"
v1.11.0,"note: we explicitly need to provide a relation initializer here,"
v1.11.0,since ERMLPE shares initializers between entities and relations by default
v1.11.0,must be cloned if we want to do backprop
v1.11.0,the color initializer
v1.11.0,variants for the edge index
v1.11.0,additional parameters for iter_weisfeiler_lehman
v1.11.0,normalize shape
v1.11.0,get coloring
v1.11.0,make color initializer
v1.11.0,initialize color representations
v1.11.0,note: this could be a representation?
v1.11.0,init entity representations according to the color
v1.11.0,create random walk matrix
v1.11.0,TODO replace iter_matrix_power and safe_diagonal with torch_ppr functions?
v1.11.0,stack diagonal entries of powers of rw
v1.11.0,": A resolver for initializers, including both elements of :mod:`torch.nn.init` and"
v1.11.0,: custom additions in :mod:`pykeen.nn.init`
v1.11.0,: whether the edge weighting needs access to the message
v1.11.0,stub init to enable arbitrary arguments in subclasses
v1.11.0,"Calculate in-degree, i.e. number of incoming edges"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,backward compatibility with RGCN
v1.11.0,docstr-coverage: inherited
v1.11.0,view for heads
v1.11.0,"compute attention coefficients, shape: (num_edges, num_heads)"
v1.11.0,"TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter"
v1.11.0,: A resolver for R-GCN edge weighting implementations
v1.11.0,Caches
v1.11.0,"if the sparsity becomes too low, convert to a dense matrix"
v1.11.0,"note: this heuristic is based on the memory consumption,"
v1.11.0,"for a sparse matrix, we store 3 values per nnz (row index, column index, value)"
v1.11.0,"performance-wise, it likely makes sense to switch even earlier"
v1.11.0,`torch.sparse.mm` can also deal with dense 2nd argument
v1.11.0,note: torch.sparse.mm only works for COO matrices;
v1.11.0,@ only works for CSR matrices
v1.11.0,"convert to COO, if necessary"
v1.11.0,"we need to use indices here, since there may be zero diagonal entries"
v1.11.0,darglint does not like
v1.11.0,"raise cls(shape=shape, reference=reference)"
v1.11.0,Normalize relation embeddings
v1.11.0,1 * ? = ?; ? * 1 = ?
v1.11.0,i**2 = j**2 = k**2 = -1
v1.11.0,i * j = k; i * k = -j
v1.11.0,"j * i = -k, j * k = i"
v1.11.0,k * i = j; k * j = -i
v1.11.0,noqa: DAR401
v1.11.0,docstr-coverage: inherited
v1.11.0,: a = \mu^T\Sigma^{-1}\mu
v1.11.0,: b = \log \det \Sigma
v1.11.0,docstr-coverage: inherited
v1.11.0,1. Component
v1.11.0,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.11.0,2. Component
v1.11.0,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.11.0,with mu = (mu_1 - mu_0)
v1.11.0,= mu * Sigma_1^-1 mu
v1.11.0,since Sigma_1 is diagonal
v1.11.0,= mu**2 / sigma_1
v1.11.0,3. Component
v1.11.0,4. Component
v1.11.0,ln (det(\Sigma_1) / det(\Sigma_0))
v1.11.0,= ln det Sigma_1 - ln det Sigma_0
v1.11.0,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.11.0,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.11.0,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.11.0,: A resolver for similarities for :class:`pykeen.nn.modules.KG2EInteraction`
v1.11.0,REPRESENTATION
v1.11.0,base
v1.11.0,concrete
v1.11.0,INITIALIZER
v1.11.0,INTERACTIONS
v1.11.0,Adapter classes
v1.11.0,Concrete Classes
v1.11.0,combinations
v1.11.0,TODO: split file into multiple smaller ones?
v1.11.0,Base Classes
v1.11.0,Adapter classes
v1.11.0,Concrete Classes
v1.11.0,normalize input
v1.11.0,get number of head/relation/tail representations
v1.11.0,flatten list
v1.11.0,split tensors
v1.11.0,broadcasting
v1.11.0,yield batches
v1.11.0,complex typing
v1.11.0,docstr-coverage:excused `overload`
v1.11.0,docstr-coverage:excused `overload`
v1.11.0,: The symbolic shapes for entity representations
v1.11.0,: The symbolic shapes for relation representations
v1.11.0,if the interaction function's head parameter should only receive a subset of entity representations
v1.11.0,if the interaction function's tail parameter should only receive a subset of entity representations
v1.11.0,TODO: does not seem to be used
v1.11.0,: the interaction's value range (for unrestricted input)
v1.11.0,"TODO: annotate modelling capabilities? cf., e.g., https://arxiv.org/abs/1902.10197, Table 2"
v1.11.0,"TODO: annotate properties, e.g., symmetry, and use them for testing?"
v1.11.0,TODO: annotate complexity?
v1.11.0,: whether the interaction is defined on complex input
v1.11.0,: The functional interaction form
v1.11.0,docstr-coverage: inherited
v1.11.0,"TODO: we only allow single-tensor representations here, but could easily generalize"
v1.11.0,docstr-coverage: inherited
v1.11.0,"TODO: implement the unbalanced variant from the paper: f(h, r, t) = (h + r)^T t"
v1.11.0,TODO: update class docstring
v1.11.0,TODO: give this a better name?
v1.11.0,All are None -> try and make closest to square
v1.11.0,Only input channels is None
v1.11.0,Only width is None
v1.11.0,Only height is none
v1.11.0,Height and input_channels are None -> set input_channels to 1 and calculage height
v1.11.0,Width and input channels are None -> set input channels to 1 and calculate width
v1.11.0,: the embedding dimension
v1.11.0,: the number of input channels of the convolution
v1.11.0,": the embedding ""image"" height"
v1.11.0,": the embedding ""image"" width"
v1.11.0,: the number of output channels of the convolution
v1.11.0,: the convolution kernel height
v1.11.0,: the convolution kernel width
v1.11.0,resolve image shape
v1.11.0,Store initial input for error message
v1.11.0,infer open dimensions from the remainder
v1.11.0,resolve kernel size defaults
v1.11.0,vector & scalar offset
v1.11.0,the offset is only used for tails
v1.11.0,": The head-relation encoder operating on 2D ""images"""
v1.11.0,: The head-relation encoder operating on the 1D flattened version
v1.11.0,Parameter need to fulfil:
v1.11.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.11.0,encoders
v1.11.0,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.11.0,"2: 1D encoder: FC, DO, BN?, Act"
v1.11.0,"repeat if necessary, and concat head and relation"
v1.11.0,"shape: -1, num_input_channels, 2*height, width"
v1.11.0,"shape: -1, num_input_channels, 2*height, width"
v1.11.0,"-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.11.0,"reshape: (-1, dim) -> (*batch_dims, dim)"
v1.11.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.11.0,output_shape: batch_dims
v1.11.0,add bias term
v1.11.0,The interaction model
v1.11.0,docstr-coverage: inherited
v1.11.0,Use Xavier initialization for weight; bias to zero
v1.11.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.11.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.11.0,"cat into shape (..., 1, d, 3)"
v1.11.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.11.0,"Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)"
v1.11.0,normalize hidden_dim
v1.11.0,shortcut for same shape
v1.11.0,split weight into head-/relation-/tail-specific sub-matrices
v1.11.0,docstr-coverage: inherited
v1.11.0,Initialize biases with zero
v1.11.0,"In the original formulation,"
v1.11.0,"repeat if necessary, and concat head and relation, (*batch_dims, 2 * embedding_dim)"
v1.11.0,"Predict t embedding, shape: (*batch_dims, d)"
v1.11.0,dot product
v1.11.0,project to relation specific subspace
v1.11.0,ensure constraints
v1.11.0,TODO: update docstring
v1.11.0,TODO: give this a better name?
v1.11.0,r expresses a rotation in complex plane.
v1.11.0,rotate head by relation (=Hadamard product in complex space)
v1.11.0,rotate tail by inverse of relation
v1.11.0,The inverse rotation is expressed by the complex conjugate of r.
v1.11.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.11.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.11.0,|h * r - t| = |h - conj(r) * t|
v1.11.0,"composite: (*batch_dims, d)"
v1.11.0,inner product with relation embedding
v1.11.0,Global entity projection
v1.11.0,Global relation projection
v1.11.0,Global combination bias
v1.11.0,Global combination bias
v1.11.0,global projections
v1.11.0,"combination, shape: (*batch_dims, d)"
v1.11.0,dot product with t
v1.11.0,docstr-coverage: inherited
v1.11.0,projections
v1.11.0,default core tensor initialization
v1.11.0,cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12
v1.11.0,normalize initializer
v1.11.0,normalize relation dimension
v1.11.0,Core tensor
v1.11.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.11.0,Dropout
v1.11.0,docstr-coverage: inherited
v1.11.0,instantiate here to make module easily serializable
v1.11.0,"batch norm gets reset automatically, since it defines reset_parameters"
v1.11.0,x_2 contraction
v1.11.0,x_1 contraction
v1.11.0,shapes
v1.11.0,Project entities
v1.11.0,h projection to hyperplane
v1.11.0,r
v1.11.0,-t projection to hyperplane
v1.11.0,there are separate biases for entities in head and tail position
v1.11.0,docstr-coverage: inherited
v1.11.0,with k=4
v1.11.0,"TODO: this sign is in the official code, too, but why do we need it?"
v1.11.0,note: this is a fused kernel for computing the Hamilton product and the inner product at once
v1.11.0,the base interaction
v1.11.0,forward entity/relation shapes
v1.11.0,The parameters of the affine transformation: bias
v1.11.0,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: expose initialization?
v1.11.0,head interaction
v1.11.0,relation interaction (notice that h has been updated)
v1.11.0,combination
v1.11.0,similarity
v1.11.0,relation box head; relation box tail
v1.11.0,head position and bump
v1.11.0,tail position and bump
v1.11.0,head score
v1.11.0,head box score
v1.11.0,tail box score
v1.11.0,compute width plus 1
v1.11.0,compute box midpoints
v1.11.0,"TODO: we already had this before, as `base`"
v1.11.0,inside box?
v1.11.0,yes: |p - c| / (w + 1)
v1.11.0,no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))
v1.11.0,Step 1: Apply the other entity bump
v1.11.0,Step 2: Apply tanh if tanh_map is set to True.
v1.11.0,Compute the distance function output element-wise
v1.11.0,"Finally, compute the norm"
v1.11.0,Enforce that sizes are strictly positive by passing through ELU
v1.11.0,Shape vector is normalized using the above helper function
v1.11.0,Size is learned separately and applied to normalized shape
v1.11.0,Compute potential boundaries by applying the shape in substraction
v1.11.0,and in addition
v1.11.0,Compute box upper bounds using min and max respectively
v1.11.0,input normalization
v1.11.0,Core tensor
v1.11.0,docstr-coverage: inherited
v1.11.0,initialize core tensor
v1.11.0,"stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)"
v1.11.0,"remember shape for output, but reshape for transformer to (2, prod(batch_dims), dim)"
v1.11.0,"get position embeddings, shape: (seq_len, dim)"
v1.11.0,Now we are position-dependent w.r.t qualifier pairs.
v1.11.0,"seq_length, batch_size, dim"
v1.11.0,"Pool output along sequence dimension, (prod(batch_dims), dim)"
v1.11.0,"output shape: (prod(batch_dims), dim)"
v1.11.0,reshape
v1.11.0,"r_head, r_mid, r_tail"
v1.11.0,note: normalization should be done from the representations
v1.11.0,cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501
v1.11.0,version 2
v1.11.0,r_head = r_head + u * torch.ones_like(r_head)
v1.11.0,r_tail = r_tail + u * torch.ones_like(r_tail)
v1.11.0,type alias for AutoSF block description
v1.11.0,"head_index, relation_index, tail_index, sign"
v1.11.0,: a description of the block structure
v1.11.0,convert to tuple
v1.11.0,infer the number of entity and relation representations
v1.11.0,verify coefficients
v1.11.0,dynamic entity / relation shapes
v1.11.0,docstr-coverage: inherited
v1.11.0,"r_head, r_bias, r_tail"
v1.11.0,: A resolver for stateful interaction functions
v1.11.0,Concrete classes
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,input normalization
v1.11.0,instantiate separate combinations
v1.11.0,docstr-coverage: inherited
v1.11.0,split complex; repeat real
v1.11.0,separately combine real and imaginary parts
v1.11.0,combine
v1.11.0,docstr-coverage: inherited
v1.11.0,symbolic output to avoid dtype issue
v1.11.0,we only need to consider real part here
v1.11.0,the gate
v1.11.0,the combination
v1.11.0,docstr-coverage: inherited
v1.11.0,Base
v1.11.0,Concrete
v1.11.0,Resolver
v1.11.0,: The stateless function that gets composed
v1.11.0,docstr-coverage: inherited
v1.11.0,NOTE: wrapping torch.sub and torch.mul since their docstrings cause an issue...
v1.11.0,: Subtracts with :func:`torch.sub`
v1.11.0,: Multiplies with :func:`torch.mul`
v1.11.0,: A path to an image file or a tensor representation of the image
v1.11.0,: A sequence of image hints
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,infer shape
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO extract out a shared base class if we ever get a second image source
v1.11.0,we can have multiple images per entity -> collect image URLs per image
v1.11.0,entity ID
v1.11.0,relation ID
v1.11.0,image URL
v1.11.0,check whether images are still missing
v1.11.0,select on image url per image in a reproducible way
v1.11.0,traverse relations in order of preference
v1.11.0,now there is an image available -> select reproducible by URL sorting
v1.11.0,did not break -> no image
v1.11.0,abstract
v1.11.0,concrete
v1.11.0,docstr-coverage: inherited
v1.11.0,tokenize
v1.11.0,pad
v1.11.0,get character embeddings
v1.11.0,pool
v1.11.0,docstr-coverage: inherited
v1.11.0,": A resolver for text encoders. By default, can use 'characterembedding'"
v1.11.0,: for :class:`CharacterEmbeddingTextEncoder` or 'transformer' for
v1.11.0,: :class:`TransformerTextEncoder`.
v1.11.0,Concrete classes
v1.11.0,docstr-coverage: inherited
v1.11.0,This import doesn't need a wrapper since it's a transitive
v1.11.0,requirement of PyOBO
v1.11.0,: Wikidata SPARQL endpoint. See https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service#Interfacing
v1.11.0,cf. https://meta.wikimedia.org/wiki/User-Agent_policy
v1.11.0,cf. https://wikitech.wikimedia.org/wiki/Robot_policy
v1.11.0,break into smaller requests
v1.11.0,try to load cached first
v1.11.0,determine missing entries
v1.11.0,retrieve information via SPARQL
v1.11.0,save entries
v1.11.0,fill missing descriptions
v1.11.0,for mypy
v1.11.0,get labels & descriptions
v1.11.0,compose labels
v1.11.0,: A resolver for text caches
v1.11.0,Text Cache
v1.11.0,Text Encoder
v1.11.0,Resolver
v1.11.0,Base classes
v1.11.0,Concrete classes
v1.11.0,TODO: allow relative
v1.11.0,isin() preserves the sorted order
v1.11.0,docstr-coverage: inherited
v1.11.0,sort by decreasing degree
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,sort by decreasing page rank
v1.11.0,docstr-coverage: inherited
v1.11.0,input normalization
v1.11.0,determine absolute number of anchors for each strategy
v1.11.0,if pre-instantiated
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,: A resolver for NodePiece anchor selectors
v1.11.0,: the token ID of the padding token
v1.11.0,: the token representations
v1.11.0,: the assigned tokens for each entity
v1.11.0,needs to be lazily imported to avoid cyclic imports
v1.11.0,fill padding (nn.Embedding cannot deal with negative indices)
v1.11.0,"sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs"
v1.11.0,"contain a subset of training relations) - for that, the padding index is the last index of the Representation"
v1.11.0,resolve token representation
v1.11.0,input validation
v1.11.0,register as buffer
v1.11.0,assign sub-module
v1.11.0,apply tokenizer
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"get token IDs, shape: (*, num_chosen_tokens)"
v1.11.0,"lookup token representations, shape: (*, num_chosen_tokens, *shape)"
v1.11.0,": A list with ratios per representation in their creation order,"
v1.11.0,": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`"
v1.11.0,": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95"
v1.11.0,normalize triples
v1.11.0,inverse triples are created afterwards implicitly
v1.11.0,tokenize
v1.11.0,Create an MLP for string aggregation
v1.11.0,note: the token representations' shape includes the number of tokens as leading dim
v1.11.0,unique hashes per representation
v1.11.0,unique hashes if we concatenate all representations together
v1.11.0,TODO: vectorization?
v1.11.0,remove self-loops
v1.11.0,add inverse edges and remove duplicates
v1.11.0,Resolver
v1.11.0,Base classes
v1.11.0,Concrete classes
v1.11.0,docstr-coverage: inherited
v1.11.0,tokenize: represent entities by bag of relations
v1.11.0,collect candidates
v1.11.0,randomly sample without replacement num_tokens relations for each entity
v1.11.0,TODO: expose num_anchors?
v1.11.0,select anchors
v1.11.0,find closest anchors
v1.11.0,convert to torch
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"To prevent possible segfaults in the METIS C code, METIS expects a graph"
v1.11.0,(1) without self-loops; (2) with inverse edges added; (3) with unique edges only
v1.11.0,https://github.com/KarypisLab/METIS/blob/94c03a6e2d1860128c2d0675cbbb86ad4f261256/libmetis/checkgraph.c#L18
v1.11.0,select independently per partition
v1.11.0,select adjacency part;
v1.11.0,"note: the indices will automatically be in [0, ..., high - low), since they are *local* indices"
v1.11.0,offset
v1.11.0,the -1 comes from the shared padding token
v1.11.0,note: permutation will be later on reverted
v1.11.0,add back 1 for the shared padding token
v1.11.0,TODO: check if perm is used correctly
v1.11.0,verify pool
v1.11.0,docstr-coverage: inherited
v1.11.0,choose first num_tokens
v1.11.0,TODO: vectorization?
v1.11.0,: A resolver for NodePiece tokenizers
v1.11.0,heuristic
v1.11.0,heuristic
v1.11.0,calculate configuration digest
v1.11.0,create anchor selection instance
v1.11.0,select anchors
v1.11.0,anchor search (=anchor assignment?)
v1.11.0,assign anchors
v1.11.0,save
v1.11.0,Resolver
v1.11.0,Base classes
v1.11.0,Concrete classes
v1.11.0,docstr-coverage: inherited
v1.11.0,"contains: anchor_ids, entity_ids, mapping {entity_id -> {""ancs"": anchors, ""dists"": distances}}"
v1.11.0,normalize anchor_ids
v1.11.0,cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541
v1.11.0,TODO: keep distances?
v1.11.0,ensure parent directory exists
v1.11.0,save via torch.save
v1.11.0,docstr-coverage: inherited
v1.11.0,"TODO: since we save a contiguous array of (num_entities, num_anchors),"
v1.11.0,"it would be more efficient to not convert to a mapping, but directly select from the tensor"
v1.11.0,: A resolver for NodePiece precomputed tokenizer loaders
v1.11.0,Anchor Searchers
v1.11.0,Anchor Selection
v1.11.0,Tokenizers
v1.11.0,Token Loaders
v1.11.0,Representations
v1.11.0,Data containers
v1.11.0,"TODO: use graph library, such as igraph, graph-tool, or networkit"
v1.11.0,Resolver
v1.11.0,Base classes
v1.11.0,Concrete classes
v1.11.0,"this array contains the indices of the k closest anchors nodes, but without guarantee that they are sorted"
v1.11.0,"now we want to sort these top-k entries, (O(k log k)) (and only those)"
v1.11.0,docstr-coverage: inherited
v1.11.0,convert to adjacency matrix
v1.11.0,convert to scipy sparse csr
v1.11.0,"compute distances between anchors and all nodes, shape: (num_anchors, num_entities)"
v1.11.0,TODO: padding for unreachable?
v1.11.0,docstr-coverage: inherited
v1.11.0,infer shape
v1.11.0,create adjacency matrix
v1.11.0,symmetric + self-loops
v1.11.0,"for each entity, determine anchor pool by BFS"
v1.11.0,an array storing whether node i is reachable by anchor j
v1.11.0,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.11.0,the output
v1.11.0,anchor nodes have themselves as a starting found anchor
v1.11.0,TODO: take all (q-1) hop neighbors before selecting from q-hop
v1.11.0,propagate one hop
v1.11.0,convergence check
v1.11.0,copy pool if we have seen enough anchors and have not yet stopped
v1.11.0,stop once we have enough
v1.11.0,TODO: can we replace this loop with something vectorized?
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,symmetric + self-loops
v1.11.0,"for each entity, determine anchor pool by BFS"
v1.11.0,an array storing whether node i is reachable by anchor j
v1.11.0,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.11.0,the output that track the distance to each found anchor
v1.11.0,"dtype is unsigned int 8 bit, so we initialize the maximum distance to 255 (or max default)"
v1.11.0,initial anchors are 0-hop away from themselves
v1.11.0,propagate one hop
v1.11.0,TODO the float() trick for GPU result stability until the torch_sparse issue is resolved
v1.11.0,https://github.com/rusty1s/pytorch_sparse/issues/243
v1.11.0,convergence check
v1.11.0,newly reached is a mask that points to newly discovered anchors at this particular step
v1.11.0,implemented as element-wise XOR (will only give True in 0 XOR 1 or 1 XOR 0)
v1.11.0,"in our case we enrich the set of found anchors, so we can only have values turning 0 to 1, eg 0 XOR 1"
v1.11.0,copy pool if we have seen enough anchors and have not yet stopped
v1.11.0,"update the value in the pool by the current hop value (we start from 0, so +1 be default)"
v1.11.0,stop once we have enough
v1.11.0,sort the pool by nearest to farthest anchors
v1.11.0,values with distance 255 (or max for unsigned int8 type) are padding tokens
v1.11.0,"since the output is sorted, no need for random sampling, we just take top-k nearest"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"select k anchors with largest ppr, shape: (batch_size, k)"
v1.11.0,prepare adjacency matrix only once
v1.11.0,prepare result
v1.11.0,progress bar?
v1.11.0,batch-wise computation of PPR
v1.11.0,"run page-rank calculation, shape: (batch_size, n)"
v1.11.0,"select PPR values for the anchors, shape: (batch_size, num_anchors)"
v1.11.0,: A resolver for NodePiece anchor searchers
v1.11.0,Base classes
v1.11.0,Concrete classes
v1.11.0,
v1.11.0,
v1.11.0,
v1.11.0,
v1.11.0,
v1.11.0,Misc
v1.11.0,
v1.11.0,rank based metrics do not need binarized scores
v1.11.0,: the supported rank types. Most of the time equal to all rank types
v1.11.0,: whether the metric requires the number of candidates for each ranking task
v1.11.0,normalize confidence level
v1.11.0,sample metric values
v1.11.0,"bootstrap estimator (i.e., compute on sample with replacement)"
v1.11.0,cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function
v1.11.0,: The rank-based metric class that this derived metric extends
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"since scale and offset are constant for a given number of candidates, we have"
v1.11.0,E[scale * M + offset] = scale * E[M] + offset
v1.11.0,docstr-coverage: inherited
v1.11.0,"since scale and offset are constant for a given number of candidates, we have"
v1.11.0,V[scale * M + offset] = scale^2 * V[M]
v1.11.0,: Z-adjusted metrics are formulated to be increasing
v1.11.0,: Z-adjusted metrics can only be applied to realistic ranks
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,should be exactly 0.0
v1.11.0,docstr-coverage: inherited
v1.11.0,should be exactly 1.0
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,: Expectation/maximum reindexed metrics are formulated to be increasing
v1.11.0,: Expectation/maximum reindexed metrics can only be applied to realistic ranks
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,should be exactly 0.0
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2
v1.11.0,use V[x] = E[x^2] - E[x]^2
v1.11.0,group by same weight -> compute H_w(n) for multiple n at once
v1.11.0,we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once
v1.11.0,now select from precomputed cumulative sums and aggregate
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective"
v1.11.0,cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: should we return the sum of weights?
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.11.0,docstr-coverage: inherited
v1.11.0,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.11.0,: the lower bound
v1.11.0,: whether the lower bound is inclusive
v1.11.0,: the upper bound
v1.11.0,: whether the upper bound is inclusive
v1.11.0,: The name of the metric
v1.11.0,: a link to further information
v1.11.0,: whether the metric needs binarized scores
v1.11.0,": whether it is increasing, i.e., larger values are better"
v1.11.0,: the value range
v1.11.0,: synonyms for this metric
v1.11.0,: whether the metric supports weights
v1.11.0,: whether there is a closed-form solution of the expectation
v1.11.0,: whether there is a closed-form solution of the variance
v1.11.0,normalize weights
v1.11.0,calculate weighted harmonic mean
v1.11.0,calculate cdf
v1.11.0,determine value at p=0.5
v1.11.0,special case for exactly 0.5
v1.11.0,see also: https://cran.r-project.org/web/packages/metrica/vignettes/available_metrics_classification.html
v1.11.0,todo: do we need numpy support?
v1.11.0,TODO: re-consider threshold
v1.11.0,noqa:DAR202
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: can we directly include sklearn's docstring here?
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,noqa: DAR202
v1.11.0,todo: it would make sense to have a separate evaluator which constructs the confusion matrix only once
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,todo: https://en.wikipedia.org/wiki/Diagnostic_odds_ratio#Confidence_interval
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,todo: improve doc
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,: A resolver for classification metrics
v1.11.0,don't worry about functions because they can't be specified by JSON.
v1.11.0,Could make a better mo
v1.11.0,later could extend for other non-JSON valid types
v1.11.0,todo: read from config instead
v1.11.0,Score with original triples
v1.11.0,Score with inverse triples
v1.11.0,noqa:DAR101
v1.11.0,noqa:DAR401
v1.11.0,Create directory in which all experimental artifacts are saved
v1.11.0,noqa:DAR101
v1.11.0,clip for node piece configurations
v1.11.0,"""pykeen experiments reproduce"" expects ""model reference dataset"""
v1.11.0,TODO: take care that triples aren't removed that are the only ones with any given entity
v1.11.0,distribute the deteriorated triples across the remaining factories
v1.11.0,"'kinships',"
v1.11.0,"'umls',"
v1.11.0,"'codexsmall',"
v1.11.0,"'wn18',"
v1.11.0,ensure that each entity & relation occurs at least once
v1.11.0,: Functions for specifying exotic resources with a given prefix
v1.11.0,: Functions for specifying exotic resources based on their file extension
v1.11.0,Input validation
v1.11.0,convert to numpy
v1.11.0,Additional columns
v1.11.0,convert PyTorch tensors to numpy
v1.11.0,convert to dataframe
v1.11.0,Re-order columns
v1.11.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.11.0,TODO vectorize code
v1.11.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,save literal-to-id mapping
v1.11.0,save numeric literals
v1.11.0,load literal-to-id
v1.11.0,load literals
v1.11.0,Split triples
v1.11.0,Sorting ensures consistent results when the triples are permuted
v1.11.0,Create mapping
v1.11.0,Sorting ensures consistent results when the triples are permuted
v1.11.0,Create mapping
v1.11.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.11.0,Filter all non-existent triples
v1.11.0,Note: Unique changes the order of the triples
v1.11.0,Note: Using unique means implicit balancing of training samples
v1.11.0,normalize input
v1.11.0,: The mapping from labels to IDs.
v1.11.0,: The inverse mapping for label_to_id; initialized automatically
v1.11.0,: A vectorized version of entity_label_to_id; initialized automatically
v1.11.0,: A vectorized version of entity_id_to_label; initialized automatically
v1.11.0,Normalize input
v1.11.0,label
v1.11.0,Filter for entities
v1.11.0,Filter for relations
v1.11.0,No filter
v1.11.0,: the number of unique entities
v1.11.0,": the number of relations (maybe including ""artificial"" inverse relations)"
v1.11.0,: whether to create inverse triples
v1.11.0,": the number of real relations, i.e., without artificial inverses"
v1.11.0,ensure torch.Tensor
v1.11.0,input validation
v1.11.0,"always store as torch.long, i.e., torch's default integer dtype"
v1.11.0,check new label to ID mappings
v1.11.0,Make new triples factories for each group
v1.11.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.11.0,prepare metadata
v1.11.0,Delegate to function
v1.11.0,"restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed"
v1.11.0,docstr-coverage: inherited
v1.11.0,load base
v1.11.0,load numeric triples
v1.11.0,store numeric triples
v1.11.0,store metadata
v1.11.0,note: num_relations will be doubled again when instantiating with create_inverse_triples=True
v1.11.0,Check if the triples are inverted already
v1.11.0,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.11.0,contained if and only if create_inverse_triples is True.
v1.11.0,Generate entity mapping if necessary
v1.11.0,Generate relation mapping if necessary
v1.11.0,Map triples of labels to triples of IDs.
v1.11.0,TODO: Check if lazy evaluation would make sense
v1.11.0,docstr-coverage: inherited
v1.11.0,store entity/relation to ID
v1.11.0,load entity/relation to ID
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,pre-filter to keep only topk
v1.11.0,if top is larger than the number of available options
v1.11.0,Generate a word cloud image
v1.11.0,docstr-coverage: inherited
v1.11.0,vectorized label lookup
v1.11.0,Re-order columns
v1.11.0,docstr-coverage: inherited
v1.11.0,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.11.0,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.11.0,ID-based triples
v1.11.0,labeled triples
v1.11.0,make sure triples are a numpy array
v1.11.0,make sure triples are 2d
v1.11.0,convert to ID-based
v1.11.0,triples factory
v1.11.0,all keyword-based options have been none
v1.11.0,delegate to keyword-based get_mapped_triples to re-use optional validation logic
v1.11.0,delegate to keyword-based get_mapped_triples to re-use optional validation logic
v1.11.0,only labeled triples are remaining
v1.11.0,Cleaners
v1.11.0,Splitters
v1.11.0,Utils
v1.11.0,Split indices
v1.11.0,Split triples
v1.11.0,select one triple per relation
v1.11.0,"Select one triple for each head/tail entity, which is not yet covered."
v1.11.0,create mask
v1.11.0,Prepare split index
v1.11.0,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.11.0,base cases
v1.11.0,IDs not in training
v1.11.0,triples with exclusive test IDs
v1.11.0,docstr-coverage: inherited
v1.11.0,While there are still triples that should be moved to the training set
v1.11.0,Pick a random triple to move over to the training triples
v1.11.0,TODO: this could easily be extended to select a batch of triples
v1.11.0,-> speeds up the process at the cost of slightly larger movements
v1.11.0,add to training
v1.11.0,remove from testing
v1.11.0,Recalculate the move_id_mask
v1.11.0,docstr-coverage: inherited
v1.11.0,: A resolver for triple cleaners
v1.11.0,docstr-coverage: inherited
v1.11.0,Make sure that the first element has all the right stuff in it
v1.11.0,docstr-coverage: inherited
v1.11.0,: A resolver for triple splitters
v1.11.0,backwards compatibility
v1.11.0,constants
v1.11.0,constants
v1.11.0,unary
v1.11.0,binary
v1.11.0,ternary
v1.11.0,column names
v1.11.0,return candidates
v1.11.0,index triples
v1.11.0,incoming relations per entity
v1.11.0,outgoing relations per entity
v1.11.0,indexing triples for fast join r1 & r2
v1.11.0,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.11.0,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.11.0,actual evaluation of the pattern
v1.11.0,skip empty support
v1.11.0,TODO: Can this happen after pre-filtering?
v1.11.0,"sort first, for triple order invariance"
v1.11.0,TODO: what is the support?
v1.11.0,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.11.0,sort decreasingly. i dominates j for all j > i in x-dimension
v1.11.0,"if it is also dominated by any y, it is not part of the skyline"
v1.11.0,"group by (relation id, pattern type)"
v1.11.0,"for each group, yield from skyline"
v1.11.0,determine patterns from triples
v1.11.0,drop zero-confidence
v1.11.0,keep only skyline
v1.11.0,create data frame
v1.11.0,iterate relation types
v1.11.0,drop zero-confidence
v1.11.0,keep only skyline
v1.11.0,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.11.0,base = skyline(base)
v1.11.0,create data frame
v1.11.0,TODO: the same
v1.11.0,": the positive triples, shape: (batch_size, 3)"
v1.11.0,": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)"
v1.11.0,": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)"
v1.11.0,noqa:DAR202
v1.11.0,noqa:DAR401
v1.11.0,TODO: some negative samplers require batches
v1.11.0,"shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.11.0,"each shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,indexing
v1.11.0,initialize
v1.11.0,sample iteratively
v1.11.0,determine weights
v1.11.0,randomly choose a vertex which has not been chosen yet
v1.11.0,normalize to probabilities
v1.11.0,sample a start node
v1.11.0,get list of neighbors
v1.11.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.11.0,visit target node
v1.11.0,decrease sample counts
v1.11.0,docstr-coverage: inherited
v1.11.0,convert to csr for fast row slicing
v1.11.0,safe division for empty sets
v1.11.0,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.11.0,duplicates
v1.11.0,we are not interested in self-similarity
v1.11.0,compute similarities
v1.11.0,Calculate which relations are the inverse ones
v1.11.0,get existing IDs
v1.11.0,remove non-existing ID from label mapping
v1.11.0,create translation tensor
v1.11.0,get entities and relations occurring in triples
v1.11.0,generate ID translation and new label to Id mappings
v1.11.0,note: this seems to be a pretty unsafe method to derive __init__ kwargs...
v1.11.0,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.11.0,seamless loading and saving of training checkpoints
v1.11.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.11.0,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.11.0,"If a checkpoint file is given, it must be loaded if it exists already"
v1.11.0,"If the stopper dict has any keys, those are written back to the stopper"
v1.11.0,The checkpoint frequency needs to be set to save checkpoints
v1.11.0,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.11.0,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.11.0,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.11.0,send model to device before going into the internal training loop
v1.11.0,the exit stack ensure that we clean up temporary files when an error occurs
v1.11.0,"When using early stopping models have to be saved separately at the best epoch, since the training"
v1.11.0,loop will due to the patience continue to train after the best epoch and thus alter the model
v1.11.0,note: NamedTemporaryFile does not seem to work
v1.11.0,Create a path
v1.11.0,Ensure the release of memory
v1.11.0,Clear optimizer
v1.11.0,Accumulate loss over epoch
v1.11.0,Flag to check when to quit the size probing
v1.11.0,apply callbacks before starting with batch
v1.11.0,Get batch size of current batch (last batch may be incomplete)
v1.11.0,accumulate gradients for whole batch
v1.11.0,forward pass call
v1.11.0,For testing purposes we're only interested in processing one batch
v1.11.0,"note: this epoch loss can be slightly biased towards the last batch, if this is smaller than the rest"
v1.11.0,"in practice, this should have a minor effect, since typically batch_size << num_instances"
v1.11.0,TODO: is this necessary?
v1.11.0,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.11.0,due to the patience continue to train after the best epoch and thus alter the model
v1.11.0,"-> the temporay file has to be created outside, which we assert here"
v1.11.0,Prepare all of the callbacks
v1.11.0,"Register a callback for the result tracker, if given"
v1.11.0,"Register a callback for the early stopper, if given"
v1.11.0,TODO should mode be passed here?
v1.11.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.11.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.11.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.11.0,return the relevant parameters slice_size and batch_size
v1.11.0,Force weight initialization if training continuation is not explicitly requested.
v1.11.0,Reset the weights
v1.11.0,"afterwards, some parameters may be on the wrong device"
v1.11.0,Create new optimizer
v1.11.0,Create a new lr scheduler and add the optimizer
v1.11.0,Ensure the model is on the correct device
v1.11.0,"When size probing, we don't want progress bars"
v1.11.0,Create progress bar
v1.11.0,optimizer callbacks
v1.11.0,Save the time to track when the saved point was available
v1.11.0,Training Loop
v1.11.0,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.11.0,Enforce training mode
v1.11.0,Batching
v1.11.0,Only create a progress bar when not in size probing mode
v1.11.0,When size probing we don't need the losses
v1.11.0,Track epoch loss
v1.11.0,Print loss information to console
v1.11.0,Save the last successful finished epoch
v1.11.0,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.11.0,During automatic memory optimization only the error message is of interest
v1.11.0,When there wasn't a best epoch the checkpoint path should be None
v1.11.0,Delete temporary best epoch model
v1.11.0,Includes a call to result_tracker.log_metrics
v1.11.0,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.11.0,MyPy overrides are because you should
v1.11.0,When there wasn't a best epoch the checkpoint path should be None
v1.11.0,Delete temporary best epoch model
v1.11.0,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.11.0,at that state
v1.11.0,Delete temporary best epoch model
v1.11.0,forward pass
v1.11.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.11.0,correction for loss reduction
v1.11.0,backward pass
v1.11.0,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.11.0,Set upper bound
v1.11.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.11.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.11.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.11.0,"Only if a cuda device is available, the random state is accessed"
v1.11.0,This is an entire checkpoint for the optional best model when using early stopping
v1.11.0,Saving triples factory related states
v1.11.0,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.11.0,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.11.0,Check whether the triples factory mappings match those from the checkpoints
v1.11.0,docstr-coverage: inherited
v1.11.0,disable automatic batching
v1.11.0,docstr-coverage: inherited
v1.11.0,Slicing is not possible in sLCWA training loops
v1.11.0,split batch
v1.11.0,send to device
v1.11.0,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.11.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.11.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.11.0,Compute negative and positive scores
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,Slicing is not possible for sLCWA
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,lazy init
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO how to pass inductive mode
v1.11.0,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.11.0,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.11.0,TODO: we may want to separate TrainingCallback from pre-step callbacks in the future
v1.11.0,docstr-coverage: inherited
v1.11.0,Recall that torch *accumulates* gradients. Before passing in a
v1.11.0,"new instance, you need to zero out the gradients from the old instance"
v1.11.0,note: we want to run this step during size probing to cleanup any remaining grads
v1.11.0,docstr-coverage: inherited
v1.11.0,pre-step callbacks
v1.11.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.11.0,update parameters according to optimizer
v1.11.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.11.0,constraints are no longer applied
v1.11.0,"note: we want to apply this during size probing to properly account for the memory necessary for e.g.,"
v1.11.0,regularization
v1.11.0,docstr-coverage: inherited
v1.11.0,do not share optimal parameters across different training loops
v1.11.0,todo: create dataset only once
v1.11.0,"no sub-batching (for evaluation, we can just reduce batch size without any effect)"
v1.11.0,this is handled by the AMO wrapper
v1.11.0,no backward passes
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,set to evaluation mode
v1.11.0,determine maximum batch size
v1.11.0,TODO: this should be num_instances rather than num_triples
v1.11.0,note: slicing is only effective for LCWA training
v1.11.0,: A hint for constructing a :class:`MultiTrainingCallback`
v1.11.0,: A collection of callbacks
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,use 1-based epochs
v1.11.0,save checkpoint
v1.11.0,None corresponds to no clean-up
v1.11.0,add newly saved checkpoint to the store
v1.11.0,delete checkpoints which we do not want to keep
v1.11.0,: A resolver for training callbacks
v1.11.0,
v1.11.0,: A resolver for training loops
v1.11.0,normalize target column
v1.11.0,The type inference is so confusing between the function switching
v1.11.0,and polymorphism introduced by slicability that these need to be ignored
v1.11.0,Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number
v1.11.0,of total entities from another inductive inference factory
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,Split batch components
v1.11.0,Send batch to device
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.11.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.11.0,"note: we use Tuple[Tensor] here, so we can re-use TensorDataset instead of having to create a custom one"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,unpack
v1.11.0,Send batch to device
v1.11.0,head prediction
v1.11.0,TODO: exploit sparsity
v1.11.0,"note: this is different to what we do for LCWA, where we collect *all* training entities"
v1.11.0,for which the combination is true
v1.11.0,tail prediction
v1.11.0,TODO: exploit sparsity
v1.11.0,regularization
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO?
v1.11.0,To make MyPy happy
v1.11.0,: the number of reported results with no improvement after which training will be stopped
v1.11.0,the minimum relative improvement necessary to consider it an improved result
v1.11.0,"whether a larger value is better, or a smaller."
v1.11.0,: The epoch at which the best result occurred
v1.11.0,: The best result so far
v1.11.0,: The remaining patience
v1.11.0,check for improvement
v1.11.0,stop if the result did not improve more than delta for patience evaluations
v1.11.0,: The model
v1.11.0,: The evaluator
v1.11.0,: The triples to use for training (to be used during filtered evaluation)
v1.11.0,: The triples to use for evaluation
v1.11.0,: Size of the evaluation batches
v1.11.0,: Slice size of the evaluation batches
v1.11.0,: The number of epochs after which the model is evaluated on validation set
v1.11.0,: The number of iterations (one iteration can correspond to various epochs)
v1.11.0,: with no improvement after which training will be stopped.
v1.11.0,: The name of the metric to use
v1.11.0,: The minimum relative improvement necessary to consider it an improved result
v1.11.0,: The metric results from all evaluations
v1.11.0,": Whether a larger value is better, or a smaller"
v1.11.0,: The result tracker
v1.11.0,: Callbacks when after results are calculated
v1.11.0,: Callbacks when training gets continued
v1.11.0,: Callbacks when training is stopped early
v1.11.0,: Did the stopper ever decide to stop?
v1.11.0,: The path to the weights of the best model
v1.11.0,: Whether to delete the file with the best model weights after termination
v1.11.0,: note: the weights will be re-loaded into the model before
v1.11.0,: Whether to use a tqdm progress bar for evaluation
v1.11.0,: Keyword arguments for the tqdm progress bar
v1.11.0,TODO: Fix this
v1.11.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.11.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.11.0,for mypy
v1.11.0,Evaluate
v1.11.0,Only perform time-consuming checks for the first call.
v1.11.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.11.0,Append to history
v1.11.0,TODO need a test that this all re-instantiates properly
v1.11.0,Utils
v1.11.0,: A resolver for stoppers
v1.11.0,dataset
v1.11.0,model
v1.11.0,stored outside of the training loop / optimizer to give access to auto-tuning from Lightning
v1.11.0,optimizer
v1.11.0,"TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA"
v1.11.0,docstr-coverage: inherited
v1.11.0,call post_parameter_update
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: sub-batching / slicing
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO:
v1.11.0,"shuffle=shuffle,"
v1.11.0,"drop_last=drop_last,"
v1.11.0,"sampler=sampler,"
v1.11.0,"shuffle=shuffle,"
v1.11.0,disable automatic batching in data loader
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: sub-batching / slicing
v1.11.0,docstr-coverage: inherited
v1.11.0,: A resolver for PyTorch Lightning training modules
v1.11.0,"note: since this file is executed via __main__, its module name is replaced by __name__"
v1.11.0,"hence, the two classes' fully qualified names start with ""_"" and are considered private"
v1.11.0,cf. https://github.com/cthoyt/class-resolver/issues/39
v1.11.0,automatically choose accelerator
v1.11.0,defaults to TensorBoard; explicitly disabled here
v1.11.0,disable checkpointing
v1.11.0,mixed precision training
v1.11.0,docstr-coverage: inherited
v1.11.0,side?.metric
v1.11.0,individual side
v1.11.0,"Because the order of the values of a dictionary is not guaranteed,"
v1.11.0,we need to retrieve scores and masks using the exact same key order.
v1.11.0,combined
v1.11.0,docstr-coverage: inherited
v1.11.0,Transfer to cpu and convert to numpy
v1.11.0,Ensure that each key gets counted only once
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,: The optimistic rank is the rank when assuming all options with an equal score are placed
v1.11.0,: behind the current test triple.
v1.11.0,": shape: (batch_size,)"
v1.11.0,": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank"
v1.11.0,: over all permutations of the elements with the same score as the currently considered option.
v1.11.0,": shape: (batch_size,)"
v1.11.0,: The pessimistic rank is the rank when assuming all options with an equal score are placed
v1.11.0,: in front of current test triple.
v1.11.0,": shape: (batch_size,)"
v1.11.0,: The number of options is the number of items considered in the ranking. It may change for
v1.11.0,: filtered evaluation
v1.11.0,": shape: (batch_size,)"
v1.11.0,The optimistic rank is the rank when assuming all options with an
v1.11.0,"equal score are placed behind the currently considered. Hence, the"
v1.11.0,"rank is the number of options with better scores, plus one, as the"
v1.11.0,rank is one-based.
v1.11.0,The pessimistic rank is the rank when assuming all options with an
v1.11.0,"equal score are placed in front of the currently considered. Hence,"
v1.11.0,the rank is the number of options which have at least the same score
v1.11.0,minus one (as the currently considered option in included in all
v1.11.0,"options). As the rank is one-based, we have to add 1, which nullifies"
v1.11.0,"the ""minus 1"" from before."
v1.11.0,The realistic rank is the average of the optimistic and pessimistic
v1.11.0,"rank, and hence the expected rank over all permutations of the elements"
v1.11.0,with the same score as the currently considered option.
v1.11.0,"We set values which should be ignored to NaN, hence the number of options"
v1.11.0,which should be considered is given by
v1.11.0,TODO: unused?
v1.11.0,": the scores of the true choice, shape: (*bs), dtype: float"
v1.11.0,": the number of scores which were larger than the true score, shape: (*bs), dtype: long"
v1.11.0,": the number of scores which were not smaller than the true score, shape: (*bs), dtype: long"
v1.11.0,": the total number of compared scores, shape: (*bs), dtype: long"
v1.11.0,TODO: maybe move into separate module?
v1.11.0,actual type: nested dictionary with string keys
v1.11.0,"assert isinstance(one_key, NamedTuple)"
v1.11.0,TODO: should we enforce this?
v1.11.0,verify that the triples have been filtered
v1.11.0,Filter triples if necessary
v1.11.0,Prepare for result filtering
v1.11.0,Ensure evaluation mode
v1.11.0,Send model & tensors to device
v1.11.0,no batch size -> automatic memory optimization
v1.11.0,no slice size -> automatic memory optimization
v1.11.0,Show progressbar
v1.11.0,note: we provide the *maximum* batch and slice size here; it is reduced if necessary
v1.11.0,kwargs
v1.11.0,"if inverse triples are used, we only do score_t (TODO: by default; can this be changed?)"
v1.11.0,"otherwise, i.e., without inverse triples, we also need score_h"
v1.11.0,"if relations are to be predicted, we need to slice score_r"
v1.11.0,"raise an error, if any of the required methods cannot slice"
v1.11.0,we ignore keys which clearly do not have an effect on the memory consumptions
v1.11.0,we ignore batch_size and slice_size as those are optimized
v1.11.0,we use mapped_triples' shape instead
v1.11.0,we want to separate optimize for each evaluator instance
v1.11.0,todo: maybe we want to have some more keys outside of kwargs for hashing / have more visibility about
v1.11.0,what is passed around
v1.11.0,clear evaluator and reset progress bar (necessary for size-probing / evaluation fallback)
v1.11.0,batch-wise processing
v1.11.0,the relation_filter can be re-used (per batch) when we evaluate head *and* tail predictions
v1.11.0,"(which is the standard setting), cf. create_sparse_positive_filter_"
v1.11.0,update progress bar with actual batch size
v1.11.0,Split batch
v1.11.0,Bind shape
v1.11.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.11.0,Warn if all entities will be filtered
v1.11.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.11.0,Create filter
v1.11.0,Select scores of true
v1.11.0,overwrite filtered scores
v1.11.0,The scores for the true triples have to be rewritten to the scores tensor
v1.11.0,the rank-based evaluators needs the true scores with trailing 1-dim
v1.11.0,Create a positive mask with the size of the scores from the positive filter
v1.11.0,Restrict to entities of interest
v1.11.0,process scores
v1.11.0,optionally restrict triples (nop if no restriction)
v1.11.0,evaluation triples as dataframe
v1.11.0,determine filter triples
v1.11.0,infer num_entities if not given
v1.11.0,"TODO: unique, or max ID + 1?"
v1.11.0,optionally restrict triples
v1.11.0,compute candidate set sizes for different targets
v1.11.0,TODO: extend to relations?
v1.11.0,avoid cyclic imports
v1.11.0,normalize keys
v1.11.0,TODO: find a better way to handle this
v1.11.0,Evaluation loops
v1.11.0,Evaluation datasets
v1.11.0,batch
v1.11.0,tqdm
v1.11.0,data loader
v1.11.0,set upper limit of batch size for automatic memory optimization
v1.11.0,set model to evaluation mode
v1.11.0,delegate to AMO wrapper
v1.11.0,"The key-id for each triple, shape: (num_triples,)"
v1.11.0,": the number of targets for each key, shape: (num_unique_keys + 1,)"
v1.11.0,: the concatenation of unique targets for each key (use bounds to select appropriate sub-array)
v1.11.0,input verification
v1.11.0,group key = everything except the prediction target
v1.11.0,initialize data structure
v1.11.0,group by key
v1.11.0,convert lists to arrays
v1.11.0,instantiate
v1.11.0,return indices corresponding to the `item`-th triple
v1.11.0,input normalization
v1.11.0,prepare filter indices if required
v1.11.0,sorted by target -> most of the batches only have a single target
v1.11.0,group by target
v1.11.0,stack groups into a single tensor
v1.11.0,avoid cyclic imports
v1.11.0,TODO: it would be better to allow separate batch sizes for entity/relation prediction
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"note: most of the time, this loop will only make a single iteration, since the evaluation dataset typically is"
v1.11.0,"not shuffled, and contains evaluation ranking tasks sorted by target"
v1.11.0,"TODO: in theory, we could make a single score calculation for e.g.,"
v1.11.0,"{(h, r, t1), (h, r, t1), ..., (h, r, tk)}"
v1.11.0,predict scores for all candidates
v1.11.0,filter scores
v1.11.0,extract true scores
v1.11.0,replace by nan
v1.11.0,rewrite true scores
v1.11.0,create dense positive masks
v1.11.0,"TODO: afaik, dense positive masks are not used on GPU -> we do not need to move the masks around"
v1.11.0,delegate processing of scores to the evaluator
v1.11.0,: A resolver for evaluators
v1.11.0,: A resolver for metric results
v1.11.0,docstr-coverage: inherited
v1.11.0,delay declaration
v1.11.0,"note: OGB's evaluator needs a dataset name as input, and uses it to lookup the standard evaluation"
v1.11.0,metric. we do want to support user-selected metrics on arbitrary datasets instead
v1.11.0,"this setting is equivalent to the WikiKG2 setting, and will calculate MRR *and* H@k for k in {1, 3, 10}"
v1.11.0,check targets
v1.11.0,filter supported metrics
v1.11.0,"prepare input format, cf. `evaluator.expected_input``"
v1.11.0,"y_pred_pos: shape: (num_edge,)"
v1.11.0,"y_pred_neg: shape: (num_edge, num_nodes_neg)"
v1.11.0,move tensor to device
v1.11.0,iterate over prediction targets
v1.11.0,cf. https://github.com/snap-stanford/ogb/pull/357
v1.11.0,combine to input dictionary
v1.11.0,delegate to OGB evaluator
v1.11.0,post-processing
v1.11.0,normalize name
v1.11.0,OGB does not aggregate values across triples
v1.11.0,todo: maybe we can merge this code with the AMO code of the base evaluator?
v1.11.0,pre-allocate
v1.11.0,TODO: maybe we want to collect scores on CPU / add an option?
v1.11.0,iterate over batches
v1.11.0,"combine ids, shape: (batch_size, num_negatives + 1)"
v1.11.0,"get scores, shape: (batch_size, num_negatives + 1)"
v1.11.0,store positive and negative scores
v1.11.0,flatten dictionaries
v1.11.0,individual side
v1.11.0,combined
v1.11.0,parsing metrics
v1.11.0,metric pattern = side?.type?.metric.k?
v1.11.0,normalize metric name
v1.11.0,normalize side
v1.11.0,normalize rank type
v1.11.0,ensure that rank-opt <= rank-pess
v1.11.0,assert that rank-real = (opt + pess)/2
v1.11.0,fixme: the annotation of ClassResolver.__iter__ seems to be broken (X instead of Type[X])
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,repeat
v1.11.0,default for inductive LP by [teru2020]
v1.11.0,verify input
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: do not require to compute all scores beforehand
v1.11.0,cf. Model.score_t(ts=...)
v1.11.0,super.evaluation assumes that the true scores are part of all_scores
v1.11.0,write back correct num_entities
v1.11.0,TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?
v1.11.0,combine key batches
v1.11.0,calculate key frequency
v1.11.0,weight = inverse frequency
v1.11.0,broadcast to samples
v1.11.0,docstr-coverage: inherited
v1.11.0,store keys for calculating macro weights
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,compute macro weights
v1.11.0,note: we wrap the array into a list to be able to re-use _iter_ranks
v1.11.0,calculate weighted metrics
v1.11.0,Clear buffers
v1.11.0,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.11.0,to create the class
v1.11.0,TODO: update to hint + kwargs
v1.11.0,: The default regularizer class
v1.11.0,: The default parameters for the default regularizer class
v1.11.0,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.11.0,Make sure that all modules with parameters do have a reset_parameters method.
v1.11.0,Recursively visit all sub-modules
v1.11.0,skip self
v1.11.0,Track parents for blaming
v1.11.0,call reset_parameters if possible
v1.11.0,initialize from bottom to top
v1.11.0,This ensures that specialized initializations will take priority over the default ones of its components.
v1.11.0,emit warning if there where parameters which were not initialised by reset_parameters.
v1.11.0,Additional debug information
v1.11.0,docstr-coverage: inherited
v1.11.0,TODO: allow max_id being present in representation_kwargs; if it matches max_id
v1.11.0,TODO: we could infer some shapes from the given interaction shape information
v1.11.0,check max-id
v1.11.0,check shapes
v1.11.0,: The entity representations
v1.11.0,: The relation representations
v1.11.0,: The weight regularizers
v1.11.0,: The interaction function
v1.11.0,"TODO: support ""broadcasting"" representation regularizers?"
v1.11.0,e.g. re-use the same regularizer for everything; or
v1.11.0,"pass a dictionary with keys ""entity""/""relation"";"
v1.11.0,values are either a regularizer hint (=the same regularizer for all repr); or a sequence of appropriate length
v1.11.0,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.11.0,"model.modules(). Thereby, we can collect them automatically."
v1.11.0,Explicitly call reset_parameters to trigger initialization
v1.11.0,"note, triples_factory is required instead of just using self.num_entities"
v1.11.0,and self.num_relations for the inductive case when this is different
v1.11.0,instantiate regularizer
v1.11.0,normalize input
v1.11.0,Note: slicing cannot be used here: the indices for score_hrt only have a batch
v1.11.0,"dimension, and slicing along this dimension is already considered by sub-batching."
v1.11.0,Note: we do not delegate to the general method for performance reasons
v1.11.0,Note: repetition is not necessary here
v1.11.0,batch normalization modules use batch statistics in training mode
v1.11.0,-> different batch divisions lead to different results
v1.11.0,docstr-coverage: inherited
v1.11.0,normalize before checking
v1.11.0,slice early to allow lazy computation of target representations
v1.11.0,add broadcast dimension
v1.11.0,unsqueeze if necessary
v1.11.0,docstr-coverage: inherited
v1.11.0,normalize before checking
v1.11.0,slice early to allow lazy computation of target representations
v1.11.0,add broadcast dimension
v1.11.0,unsqueeze if necessary
v1.11.0,docstr-coverage: inherited
v1.11.0,normalize before checking
v1.11.0,slice early to allow lazy computation of target representations
v1.11.0,add broadcast dimension
v1.11.0,unsqueeze if necessary
v1.11.0,normalization
v1.11.0,train model
v1.11.0,"note: as this is an example, the model is only trained for a few epochs,"
v1.11.0,"but not until convergence. In practice, you would usually first verify that"
v1.11.0,"the model is sufficiently good in prediction, before looking at uncertainty scores"
v1.11.0,predict triple scores with uncertainty
v1.11.0,"use a larger number of samples, to increase quality of uncertainty estimate"
v1.11.0,get most and least uncertain prediction on training set
v1.11.0,: The scores
v1.11.0,": The uncertainty, in the same shape as scores"
v1.11.0,Enforce evaluation mode
v1.11.0,set dropout layers to training mode
v1.11.0,draw samples
v1.11.0,compute mean and std
v1.11.0,"This empty 1-element tensor doesn't actually do anything,"
v1.11.0,but is necessary since models with no grad params blow
v1.11.0,up the optimizer
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default loss function class
v1.11.0,: The default parameters for the default loss function class
v1.11.0,: The instance of the loss
v1.11.0,: the number of entities
v1.11.0,: the number of relations
v1.11.0,: whether to use inverse relations
v1.11.0,: utility for generating inverse relations
v1.11.0,": When predict_with_sigmoid is set to True, the sigmoid function is"
v1.11.0,: applied to the logits during evaluation and also for predictions
v1.11.0,": after training, but has no effect on the training."
v1.11.0,Random seeds have to set before the embeddings are initialized
v1.11.0,Loss
v1.11.0,TODO: why do we need to empty the cache?
v1.11.0,"TODO: this currently compute (batch_size, num_relations) instead,"
v1.11.0,"i.e., scores for normal and inverse relations"
v1.11.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.11.0,send to device
v1.11.0,special handling of inverse relations
v1.11.0,"when trained on inverse relations, the internal relation ID is twice the original relation ID"
v1.11.0,Base Models
v1.11.0,Concrete Models
v1.11.0,Inductive Models
v1.11.0,Evaluation-only models
v1.11.0,Meta Models
v1.11.0,Utils
v1.11.0,: A resolver for knowledge graph embedding models
v1.11.0,Abstract Models
v1.11.0,We might be able to relax this later
v1.11.0,baseline models behave differently
v1.11.0,always create representations for normal and inverse relations and padding
v1.11.0,"note: we need to share the aggregation across representations, since the aggregation may have"
v1.11.0,trainable parameters
v1.11.0,note: we cannot ensure the mapping also matches...
v1.11.0,get relation representations
v1.11.0,get combination
v1.11.0,get token representations
v1.11.0,relation representations are shared
v1.11.0,share combination weights
v1.11.0,: a mapping from inductive mode to corresponding entity representations
v1.11.0,": note: there may be duplicate values, if entity representations are shared between validation and testing"
v1.11.0,inductive factories
v1.11.0,"entity representation kwargs may contain a triples factory, which needs to be replaced"
v1.11.0,"entity_representations_kwargs.pop(""triples_factory"", None)"
v1.11.0,note: this is *not* a nn.ModuleDict; the modules have to be registered elsewhere
v1.11.0,shared
v1.11.0,non-shared
v1.11.0,"note: ""training"" is an attribute of nn.Module -> need to rename to avoid name collision"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,default composition is DistMult-style
v1.11.0,Saving edge indices for all the supplied splits
v1.11.0,Extract all entity and relation representations
v1.11.0,Perform message passing and get updated states
v1.11.0,Use updated entity and relation states to extract requested IDs
v1.11.0,TODO I got lost in all the Representation Modules and shape casting and wrote this ;(
v1.11.0,normalization
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,": the indexed filter triples, i.e., sparse masks"
v1.11.0,avoid cyclic imports
v1.11.0,create base model
v1.11.0,assign *after* nn.Module.__init__
v1.11.0,save constants
v1.11.0,index triples
v1.11.0,initialize base model's parameters
v1.11.0,"get masks, shape: (batch_size, num_entities/num_relations)"
v1.11.0,combine masks
v1.11.0,"note: * is an elementwise and, and + and elementwise or"
v1.11.0,get non-zero entries
v1.11.0,set scores for fill value for every non-occuring entry
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,NodePiece
v1.11.0,TODO rethink after RGCN update
v1.11.0,TODO: other parameters?
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default loss function class
v1.11.0,: The default parameters for the default loss function class
v1.11.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.11.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.11.0,ConvE should be trained with inverse triples
v1.11.0,entity embedding
v1.11.0,ConvE uses one bias for each entity
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,head representation
v1.11.0,tail representation
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default loss function class
v1.11.0,: The default parameters for the default loss function class
v1.11.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.11.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.11.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.11.0,use torch's native complex data type
v1.11.0,use torch's native complex data type
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.11.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.11.0,: a normalized weight of 10 is used.
v1.11.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.11.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.11.0,In the code base only the weights of the output layer are used for regularization
v1.11.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,comment:
v1.11.0,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.11.0,uses float64
v1.11.0,entity bias for head
v1.11.0,entity bias for tail
v1.11.0,relation offset
v1.11.0,diagonal relation transformation matrix
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: the default loss function is the self-adversarial negative sampling loss
v1.11.0,: The default parameters for the default loss function class
v1.11.0,: The default entity normalizer parameters
v1.11.0,: The entity representations are normalized to L2 unit length
v1.11.0,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.11.0,"update initializer settings, cf."
v1.11.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.11.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.11.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.11.0,in the original implementation the embeddings are initialized in one parameter
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,"w: (k, d, d)"
v1.11.0,"vh: (k, d)"
v1.11.0,"vt: (k, d)"
v1.11.0,"b: (k,)"
v1.11.0,"u: (k,)"
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The regularizer used by [yang2014]_ for DistMult
v1.11.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.11.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.11.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.11.0,: The LP settings used by [yang2014]_ for DistMult
v1.11.0,note: DistMult only regularizes the relation embeddings;
v1.11.0,entity embeddings are hard constrained instead
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default settings for the entity constrainer
v1.11.0,mean
v1.11.0,diagonal covariance
v1.11.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.11.0,mean
v1.11.0,diagonal covariance
v1.11.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.11.0,diagonal entries
v1.11.0,off-diagonal
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The custom regularizer used by [wang2014]_ for TransH
v1.11.0,: The settings used by [wang2014]_ for TransH
v1.11.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.11.0,"Therefore, apply_only_once is always set to True."
v1.11.0,: The custom regularizer used by [wang2014]_ for TransH
v1.11.0,: The settings used by [wang2014]_ for TransH
v1.11.0,"note: this parameter is not named ""entity_regularizer"" for compatability with the"
v1.11.0,regularizer-specific HPO code
v1.11.0,translation vector in hyperplane
v1.11.0,normal vector of hyperplane
v1.11.0,normalise the normal vectors to unit l2 length
v1.11.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.11.0,which enforces the defined soft constraints.
v1.11.0,"thus, we need to use a weight regularizer instead of having an Embedding regularizer,"
v1.11.0,which only regularizes the weights used in a batch
v1.11.0,note: the following is already the default
v1.11.0,"default_regularizer=self.regularizer_default,"
v1.11.0,"default_regularizer_kwargs=self.regularizer_default_kwargs,"
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,interaction function kwargs
v1.11.0,entity embedding
v1.11.0,relation embedding
v1.11.0,relation projection
v1.11.0,TODO: Initialize from TransE
v1.11.0,relation embedding
v1.11.0,relation projection
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,TODO: Decomposition kwargs
v1.11.0,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.11.0,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.11.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.11.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.11.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,combined representation
v1.11.0,Resolve interaction function
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default loss function class
v1.11.0,: The default parameters for the default loss function class
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,entity bias for head
v1.11.0,relation position head
v1.11.0,relation shape head
v1.11.0,relation size head
v1.11.0,relation position tail
v1.11.0,relation shape tail
v1.11.0,relation size tail
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default loss function class
v1.11.0,: The default parameters for the default loss function class
v1.11.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.11.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.11.0,": regularization term by the number of elements, which is 200."
v1.11.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.11.0,TODO: what about using the default regularizer?
v1.11.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.11.0,"That is not mentioned in the paper, so it is made optional here."
v1.11.0,(head) entity
v1.11.0,tail entity
v1.11.0,relations
v1.11.0,inverse relations
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,Regular relation embeddings
v1.11.0,The relation-specific interaction vector
v1.11.0,always create representations for normal and inverse relations and padding
v1.11.0,normalize embedding specification
v1.11.0,prepare token representations & kwargs
v1.11.0,"max_id=triples_factory.num_relations,  # will get added by ERModel"
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default loss function class
v1.11.0,: The default parameters for the default loss function class
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default loss function class
v1.11.0,: The default parameters for the default loss function class
v1.11.0,: The LP settings used by [zhang2019]_ for QuatE.
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default settings for the entity constrainer
v1.11.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default loss function class
v1.11.0,: The default parameters for the default loss function class
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default parameters for the default loss function class
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default loss function class
v1.11.0,: The default parameters for the default loss function class
v1.11.0,the individual combination for real/complex parts
v1.11.0,: The default strategy for optimizing the model's hyper-parameters
v1.11.0,: The default parameters for the default loss function class
v1.11.0,no activation
v1.11.0,: the interaction class (for generating the overview table)
v1.11.0,added by ERModel
v1.11.0,"max_id=triples_factory.num_entities,"
v1.11.0,create sparse matrix of absolute counts
v1.11.0,normalize to relative counts
v1.11.0,base case
v1.11.0,"note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could"
v1.11.0,"stay sparse here, with a potential of a huge memory benefit on large datasets!"
v1.11.0,These operations are deterministic and a random seed can be fixed
v1.11.0,just to avoid warnings
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,compute relation similarity matrix
v1.11.0,mapping from relations to head/tail entities
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"if we really need access to the path later, we can expose it as a property"
v1.11.0,via self.writer.log_dir
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,: The WANDB run
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,: The name of the run
v1.11.0,": The configuration dictionary, a mapping from name -> value"
v1.11.0,: Should metrics be stored when running ``log_metrics()``?
v1.11.0,": The metrics, a mapping from step -> (name -> value)"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,: A hint for constructing a :class:`MultiResultTracker`
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,Base classes
v1.11.0,Concrete classes
v1.11.0,Utilities
v1.11.0,: A resolver for result trackers
v1.11.0,always add a Python result tracker for storing the configuration
v1.11.0,: The file extension for this writer (do not include dot)
v1.11.0,: The file where the results are written to.
v1.11.0,docstr-coverage: inherited
v1.11.0,: The column names
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,store set of triples
v1.11.0,docstr-coverage: inherited
v1.11.0,: some prime numbers for tuple hashing
v1.11.0,: The bit-array for the Bloom filter data structure
v1.11.0,Allocate bit array
v1.11.0,calculate number of hashing rounds
v1.11.0,index triples
v1.11.0,Store some meta-data
v1.11.0,pre-hash
v1.11.0,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.11.0,: A resolver for mapping filterers
v1.11.0,At least make sure to not replace the triples by the original value
v1.11.0,"To make sure we don't replace the {head, relation, tail} by the"
v1.11.0,original value we shift all values greater or equal than the original value by one up
v1.11.0,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.11.0,Set the indices
v1.11.0,docstr-coverage: inherited
v1.11.0,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.11.0,Bind the total number of negatives to sample in this batch
v1.11.0,Equally corrupt all sides
v1.11.0,"Do not detach, as no gradients should flow into the indices."
v1.11.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.11.0,: A filterer for negative batches
v1.11.0,create unfiltered negative batch by corruption
v1.11.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.11.0,Utils
v1.11.0,: A resolver for negative samplers
v1.11.0,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.11.0,create index structure
v1.11.0,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.11.0,: The concatenated sorted sets of head/tail entities
v1.11.0,docstr-coverage: inherited
v1.11.0,"shape: (batch_size, num_neg_per_pos, 3)"
v1.11.0,Uniformly sample from head/tail offsets
v1.11.0,get corresponding entity
v1.11.0,"and position within triple (0: head, 2: tail)"
v1.11.0,write into negative batch
v1.11.0,Preprocessing: Compute corruption probabilities
v1.11.0,"compute tph, i.e. the average number of tail entities per head"
v1.11.0,"compute hpt, i.e. the average number of head entities per tail"
v1.11.0,Set parameter for Bernoulli distribution
v1.11.0,docstr-coverage: inherited
v1.11.0,Decide whether to corrupt head or tail
v1.11.0,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.11.0,flatten mask
v1.11.0,Tails are corrupted if heads are not corrupted
v1.11.0,: The random seed used at the beginning of the pipeline
v1.11.0,: The model trained by the pipeline
v1.11.0,: The training triples
v1.11.0,: The training loop used by the pipeline
v1.11.0,: The losses during training
v1.11.0,: The results evaluated by the pipeline
v1.11.0,: How long in seconds did training take?
v1.11.0,: How long in seconds did evaluation take?
v1.11.0,: An early stopper
v1.11.0,: The configuration
v1.11.0,: Any additional metadata as a dictionary
v1.11.0,: The version of PyKEEN used to create these results
v1.11.0,: The git hash of PyKEEN used to create these results
v1.11.0,file names for storing results
v1.11.0,TODO: rename param?
v1.11.0,always save results as json file
v1.11.0,"save other components only if requested (which they are, by default)"
v1.11.0,TODO use pathlib here
v1.11.0,"note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:"
v1.11.0,"when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless"
v1.11.0,note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()
v1.11.0,only one original value => assume this to be the mean
v1.11.0,multiple values => assume they correspond to individual trials
v1.11.0,metrics accumulates rows for a dataframe for comparison against the original reported results (if any)
v1.11.0,"TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)"
v1.11.0,summarize
v1.11.0,skip special parameters
v1.11.0,FIXME this should never happen.
v1.11.0,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.11.0,used random_seed to ensure reproducible results
v1.11.0,We have to set clear optimizer to False since training should be continued
v1.11.0,TODO: checkpoint_dict not further used; later loaded again by TrainingLoop.train
v1.11.0,TODO: allow empty validation / testing
v1.11.0,evaluation restriction to a subset of entities/relations
v1.11.0,2. Model
v1.11.0,3. Loss
v1.11.0,4. Regularizer
v1.11.0,TODO should training be reset?
v1.11.0,TODO should kwargs for loss and regularizer be checked and raised for?
v1.11.0,Log model parameters
v1.11.0,Log loss parameters
v1.11.0,the loss was already logged as part of the model kwargs
v1.11.0,"loss=loss_resolver.normalize_inst(model_instance.loss),"
v1.11.0,Log regularizer parameters
v1.11.0,5. Optimizer
v1.11.0,5.1 Learning Rate Scheduler
v1.11.0,6. Training Loop
v1.11.0,8. Evaluation
v1.11.0,7. Training (ronaldo style)
v1.11.0,Misc
v1.11.0,Stopping
v1.11.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.11.0,Add logging for debugging
v1.11.0,Train like Cristiano Ronaldo
v1.11.0,Misc
v1.11.0,Build up a list of triples if we want to be in the filtered setting
v1.11.0,"If the user gave custom ""additional_filter_triples"""
v1.11.0,Determine whether the validation triples should also be filtered while performing test evaluation
v1.11.0,TODO consider implications of duplicates
v1.11.0,Evaluate
v1.11.0,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.11.0,Add logging about evaluator for debugging
v1.11.0,1. Dataset
v1.11.0,2. Model
v1.11.0,3. Loss
v1.11.0,4. Regularizer
v1.11.0,5. Optimizer
v1.11.0,5.1 Learning Rate Scheduler
v1.11.0,6. Training Loop
v1.11.0,7. Training (ronaldo style)
v1.11.0,8. Evaluation
v1.11.0,9. Tracking
v1.11.0,Misc
v1.11.0,Start tracking
v1.11.0,cf. also https://github.com/pykeen/pykeen/issues/1071
v1.11.0,TODO: use a class-resolver?
v1.11.0,": The default strategy for optimizing the lr_schedulers' hyper-parameters,"
v1.11.0,: based on :class:`torch.optim.lr_scheduler.LRScheduler`
v1.11.0,"note: for some reason, mypy does not properly recognize the tuple[T1, T2, T3] notation,"
v1.11.0,"but rather uses tuple[T1 | T2 | T3, ...]"
v1.11.0,dataset
v1.11.0,create inverse triples
v1.11.0,"models, losses"
v1.11.0,regularizers
v1.11.0,"optimizers, training loops"
v1.11.0,TODO what happens if already exists?
v1.11.0,TODO incorporate setting of random seed
v1.11.0,pipeline_kwargs=dict(
v1.11.0,"random_seed=random_non_negative_int(),"
v1.11.0,"),"
v1.11.0,Add dataset to current_pipeline
v1.11.0,"Training, test, and validation paths are provided"
v1.11.0,Add loss function to current_pipeline
v1.11.0,Add regularizer to current_pipeline
v1.11.0,Add optimizer to current_pipeline
v1.11.0,Add training approach to current_pipeline
v1.11.0,Add evaluation
v1.11.0,paths need to be encoded as strings to make them JSON-serializable
v1.11.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.11.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.11.0,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.11.0,the columns are not ordered properly.
v1.11.0,convert class to string to use caching
v1.11.0,Assume it's a file path
v1.11.0,note: we only need to set the create_inverse_triples in the training factory.
v1.11.0,normalize dataset kwargs
v1.11.0,enable passing force option via dataset_kwargs
v1.11.0,hash kwargs
v1.11.0,normalize dataset name
v1.11.0,get canonic path
v1.11.0,try to use cached dataset
v1.11.0,load dataset without cache
v1.11.0,store cache
v1.11.0,Type annotation for split types
v1.11.0,type variables for dictionaries of preprocessed data loaded through torch.load
v1.11.0,: The name of the dataset to download
v1.11.0,docstr-coverage: inherited
v1.11.0,label mapping is in dataset.root/mapping
v1.11.0,docstr-coverage: inherited
v1.11.0,"note: we do not use the built-in constants here, since those refer to OGB nomenclature"
v1.11.0,(which happens to coincide with ours)
v1.11.0,"dtype: numpy.int64, shape: (m,)"
v1.11.0,"dtype: numpy.int64, shape: (n, k)"
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,noqa: D102
v1.11.0,docstr-coverage: inherited
v1.11.0,: the node types
v1.11.0,"shape: (n,)"
v1.11.0,"dtype: numpy.int64, shape: (n, k)"
v1.11.0,disease: UMLS CUI (https://www.nlm.nih.gov/research/umls/index.html).
v1.11.0,drug: STITCH ID (http://stitch.embl.de/).
v1.11.0,function: Gene Ontology ID (http://geneontology.org/).
v1.11.0,protein: Proteins: Entrez Gene ID (https://www.genenames.org/).
v1.11.0,side effects: UMLS CUI (https://www.nlm.nih.gov/research/umls/index.html).
v1.11.0,todo(@cthoyt): proper prefixing?
v1.11.0,docstr-coverage: inherited
v1.11.0,entity mappings are separate for each node type -> combine
v1.11.0,convert entity_name to categorical for fast joins
v1.11.0,we need the entity dataframe for fast re-mapping later on
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,compose temporary df
v1.11.0,add extra column with old index to revert sort order change by merge
v1.11.0,convert to categorical dtype
v1.11.0,join with entity mapping
v1.11.0,revert change in order
v1.11.0,select global ID
v1.11.0,relation typing
v1.11.0,constants
v1.11.0,unique
v1.11.0,compute over all triples
v1.11.0,Determine group key
v1.11.0,Add labels if requested
v1.11.0,TODO: Merge with _common?
v1.11.0,include hash over triples into cache-file name
v1.11.0,include part hash into cache-file name
v1.11.0,re-use cached file if possible
v1.11.0,select triples
v1.11.0,save to file
v1.11.0,Prune by support and confidence
v1.11.0,TODO: Consider merging with other analysis methods
v1.11.0,TODO: Consider merging with other analysis methods
v1.11.0,TODO: Consider merging with other analysis methods
v1.11.0,"num_triples_validation: Optional[int],"
v1.11.0,Raise matplotlib level
v1.11.0,expected metrics
v1.11.0,Needs simulation
v1.11.0,See https://zenodo.org/record/6331629
v1.11.0,TODO: maybe merge into analyze / make sub-command
v1.11.0,only save full data
v1.11.0,Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)
v1.11.0,Plot: difference between mean head and tail degree
v1.11.0,don't call this function by itself. assumes called through the `validation`
v1.11.0,property and the _training factory has already been loaded
v1.11.0,Normalize path
v1.11.0,Base classes
v1.11.0,Utilities
v1.11.0,note: this needs `O(old_max_id)` memory.
v1.11.0,note: this is quite similar to pykeen.triples.triples_factory._map_triples_elements_to_ids
v1.11.0,: A factory wrapping the training triples
v1.11.0,": A factory wrapping the testing triples, that share indices with the training triples"
v1.11.0,": A factory wrapping the validation triples, that share indices with the training triples"
v1.11.0,: the dataset's name
v1.11.0,TODO: Make a constant for the names
v1.11.0,early termination for simple case
v1.11.0,restrict triples factories (without modifying the entity to id mapping)
v1.11.0,collapse entity and relation ids
v1.11.0,help mypy
v1.11.0,update factories
v1.11.0,also update testing and validation
v1.11.0,update metadata
v1.11.0,note:
v1.11.0,- we convert to list to make sure that the metadata is JSON-serializable
v1.11.0,- we sort because the order does not matter for the functionality of this method
v1.11.0,compose restricted dataset
v1.11.0,docstr-coverage: inherited
v1.11.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.11.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.11.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.11.0,: The directory in which the cached data is stored
v1.11.0,TODO: use class-resolver normalize?
v1.11.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.11.0,don't call this function by itself. assumes called through the `validation`
v1.11.0,property and the _training factory has already been loaded
v1.11.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,"relative paths within zip file's always follow Posix path, even on Windows"
v1.11.0,tarfile does not like pathlib
v1.11.0,: URL to the data to download
v1.11.0,Utilities
v1.11.0,Base Classes
v1.11.0,Concrete Classes
v1.11.0,: A resolver for datasets
v1.11.0,"ZENODO_URL = ""https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip"""
v1.11.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.11.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.11.0,Base class
v1.11.0,Mid-level classes
v1.11.0,: A factory wrapping the training triples
v1.11.0,: A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT
v1.11.0,share indices with the transductive training
v1.11.0,": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.11.0,": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.11.0,: All datasets should take care of inverse triple creation
v1.11.0,": The actual instance of the training factory, which is exposed to the user through `transductive_training`"
v1.11.0,": The actual instance of the inductive inference factory,"
v1.11.0,: which is exposed to the user through `inductive_inference`
v1.11.0,": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`"
v1.11.0,": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`"
v1.11.0,: The directory in which the cached data is stored
v1.11.0,generate subfolders 'training' and  'inference'
v1.11.0,TODO: use class-resolver normalize?
v1.11.0,add v1 / v2 / v3 / v4 for inductive splits if available
v1.11.0,important: inductive_inference shares the same RELATIONS with the transductive training graph
v1.11.0,inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph
v1.11.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.11.0,inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph
v1.11.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.11.0,Base class
v1.11.0,Mid-level classes
v1.11.0,Datasets
v1.11.0,: A resolver for inductive datasets
v1.11.0,graph pairs
v1.11.0,graph sizes
v1.11.0,graph versions
v1.11.0,: The link to the zip file
v1.11.0,: The hex digest for the zip file
v1.11.0,Input validation.
v1.11.0,ensure zip file is present
v1.11.0,save relative paths beforehand so they are present for loading
v1.11.0,delegate to super class
v1.11.0,docstr-coverage: inherited
v1.11.0,"left side has files ending with 1, right side with 2"
v1.11.0,docstr-coverage: inherited
v1.11.0,": The mapping from (graph-pair, side) to triple file name"
v1.11.0,: The internal dataset name
v1.11.0,: The hex digest for the zip file
v1.11.0,input validation
v1.11.0,store *before* calling super to have it available when loading the graphs
v1.11.0,ensure zip file is present
v1.11.0,shared directory for multiple datasets.
v1.11.0,docstr-coverage: inherited
v1.11.0,create triples factory
v1.11.0,docstr-coverage: inherited
v1.11.0,load mappings for both sides
v1.11.0,load triple alignments
v1.11.0,extract entity alignments
v1.11.0,"(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2"
v1.11.0,TODO: support ID-only graphs
v1.11.0,load both graphs
v1.11.0,load alignment
v1.11.0,drop duplicates
v1.11.0,combine
v1.11.0,store for repr
v1.11.0,split
v1.11.0,create inverse triples only for training
v1.11.0,docstr-coverage: inherited
v1.11.0,base
v1.11.0,concrete
v1.11.0,Abstract class
v1.11.0,Concrete classes
v1.11.0,Data Structures
v1.11.0,a buffer for the triples
v1.11.0,the offsets
v1.11.0,normalization
v1.11.0,append shifted mapped triples
v1.11.0,update offsets
v1.11.0,merge labels with same ID
v1.11.0,for mypy
v1.11.0,reconstruct label-to-id
v1.11.0,optional
v1.11.0,merge entity mapping
v1.11.0,merge relation mapping
v1.11.0,convert labels to IDs
v1.11.0,"map labels, using -1 as fill-value for invalid labels"
v1.11.0,"we cannot drop them here, since the two columns need to stay aligned"
v1.11.0,filter alignment
v1.11.0,map alignment from old IDs to new IDs
v1.11.0,determine swapping partner
v1.11.0,only keep triples where we have a swapping partner
v1.11.0,replace by swapping partner
v1.11.0,": the merged id-based triples, shape: (n, 3)"
v1.11.0,": the updated alignment, shape: (2, m)"
v1.11.0,: additional keyword-based parameters for adjusting label-to-id mappings
v1.11.0,concatenate triples
v1.11.0,filter alignment and translate to IDs
v1.11.0,process
v1.11.0,TODO: restrict to only using training alignments?
v1.11.0,merge mappings
v1.11.0,TODO: unreachable code
v1.11.0,docstr-coverage: inherited
v1.11.0,docstr-coverage: inherited
v1.11.0,add swap triples
v1.11.0,"e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)"
v1.11.0,create dense entity remapping for swap
v1.11.0,add swapped triples
v1.11.0,swap head
v1.11.0,swap tail
v1.11.0,: the name of the additional alignment relation
v1.11.0,docstr-coverage: inherited
v1.11.0,add alignment triples with extra relation
v1.11.0,docstr-coverage: inherited
v1.11.0,"determine connected components regarding the same-as relation (i.e., applies transitivity)"
v1.11.0,apply id mapping
v1.11.0,ensure consecutive IDs
v1.11.0,only use training alignments?
v1.11.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.11.0,1. Dataset
v1.11.0,2. Model
v1.11.0,3. Loss
v1.11.0,4. Regularizer
v1.11.0,5. Optimizer
v1.11.0,5.1 Learning Rate Scheduler
v1.11.0,6. Training Loop
v1.11.0,7. Training
v1.11.0,8. Evaluation
v1.11.0,9. Trackers
v1.11.0,Misc.
v1.11.0,log pruning
v1.11.0,"trial was successful, but has to be ended"
v1.11.0,also show info
v1.11.0,2. Model
v1.11.0,3. Loss
v1.11.0,4. Regularizer
v1.11.0,5. Optimizer
v1.11.0,5.1 Learning Rate Scheduler
v1.11.0,"TODO this fixes the issue for negative samplers, but does not generally address it."
v1.11.0,"For example, some of them obscure their arguments with **kwargs, so should we look"
v1.11.0,at the parent class? Sounds like something to put in class resolver by using the
v1.11.0,"inspect module. For now, this solution will rely on the fact that the sampler is a"
v1.11.0,direct descendent of a parent NegativeSampler
v1.11.0,a fixed checkpoint_name leads avoid collision across trials
v1.11.0,create result tracker to allow to gracefully close failed trials
v1.11.0,1. Dataset
v1.11.0,2. Model
v1.11.0,3. Loss
v1.11.0,4. Regularizer
v1.11.0,5. Optimizer
v1.11.0,5.1 Learning Rate Scheduler
v1.11.0,6. Training Loop
v1.11.0,7. Training
v1.11.0,8. Evaluation
v1.11.0,9. Tracker
v1.11.0,Misc.
v1.11.0,close run in result tracker
v1.11.0,raise the error again (which will be catched in study.optimize)
v1.11.0,: The :mod:`optuna` study object
v1.11.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.11.0,Output study information
v1.11.0,Output all trials
v1.11.0,Output best trial as pipeline configuration file
v1.11.0,1. Dataset
v1.11.0,2. Model
v1.11.0,3. Loss
v1.11.0,4. Regularizer
v1.11.0,5. Optimizer
v1.11.0,5.1 Learning Rate Scheduler
v1.11.0,6. Training Loop
v1.11.0,7. Training
v1.11.0,8. Evaluation
v1.11.0,9. Tracking
v1.11.0,6. Misc
v1.11.0,Optuna Study Settings
v1.11.0,Optuna Optimization Settings
v1.11.0,TODO: use metric.increasing to determine default direction
v1.11.0,0. Metadata/Provenance
v1.11.0,1. Dataset
v1.11.0,2. Model
v1.11.0,3. Loss
v1.11.0,4. Regularizer
v1.11.0,5. Optimizer
v1.11.0,5.1 Learning Rate Scheduler
v1.11.0,6. Training Loop
v1.11.0,7. Training
v1.11.0,8. Evaluation
v1.11.0,9. Tracking
v1.11.0,1. Dataset
v1.11.0,2. Model
v1.11.0,3. Loss
v1.11.0,4. Regularizer
v1.11.0,5. Optimizer
v1.11.0,5.1 Learning Rate Scheduler
v1.11.0,6. Training Loop
v1.11.0,7. Training
v1.11.0,8. Evaluation
v1.11.0,9. Tracker
v1.11.0,Optuna Misc.
v1.11.0,Pipeline Misc.
v1.11.0,Invoke optimization of the objective function.
v1.11.0,TODO: make it even easier to specify categorical strategies just as lists
v1.11.0,"if isinstance(info, (tuple, list, set)):"
v1.11.0,"info = dict(type='categorical', choices=list(info))"
v1.11.0,get log from info - could either be a boolean or string
v1.11.0,"otherwise, dataset refers to a file that should be automatically split"
v1.11.0,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.11.0,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.11.0,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.11.0,if has_dataset(dataset_name):
v1.11.0,"study.set_user_attr('dataset', dataset_name)"
v1.11.0,noqa: DAR101
v1.11.0,: The checkpoint frequency
v1.11.0,: the result tracker which receives updates on metrics
v1.11.0,": since the same tracker instance needs to receive results from the training loop, we do require a pre-instantiated"
v1.11.0,": one rather than offering to provide hints, too"
v1.11.0,: the metric selection
v1.11.0,note: internal detail
v1.11.0,: a resolver for checkpoint schedules
v1.11.0,determine when checkpoints are written
v1.11.0,simulate cleanup
v1.11.0,"TODO: for some reason, this field is missing in the documentation"
v1.11.0,: the normalized metric name (as seen by the result tracker)
v1.11.0,": the metric prefix; if None, do not check prefix"
v1.11.0,: whether to maximize or minimize the metric
v1.11.0,docstr-coverage: inherited
v1.11.0,prefix filter
v1.11.0,metric filter
v1.11.0,: the number of checkpoints to keep
v1.11.0,convert to set for better lookup speed
v1.11.0,the set operation should be a nop of sets
v1.11.0,: the result tracker which receives updates on metrics
v1.11.0,": since the same tracker instance needs to receive results from the training loop, we do require a pre-instantiated"
v1.11.0,": one rather than offering to provide hints, too"
v1.11.0,: the metric selection
v1.11.0,note: internal detail
v1.11.0,: a resolver for checkpoint keepers
v1.11.0,TODO: without label to id mapping a model might be pretty use-less
v1.11.0,TODO: it would be nice to get a configuration to re-construct the model
v1.11.0,save model's weights to a file
v1.11.0,load weights again
v1.11.0,update the model
v1.11.0,%%
v1.11.0,%% [markdown]
v1.11.0,## Training a model with PyKEEN
v1.11.0,%%
v1.11.0,"this will log a metric with name ""validation.loss"" to the configured result tracker"
v1.11.0,%% [markdown]
v1.11.0,## Evaluation with seaborn
v1.11.0,%%
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,
v1.10.2,Configuration file for the Sphinx documentation builder.
v1.10.2,
v1.10.2,This file does only contain a selection of the most common options. For a
v1.10.2,full list see the documentation:
v1.10.2,http://www.sphinx-doc.org/en/master/config
v1.10.2,-- Path setup --------------------------------------------------------------
v1.10.2,"If extensions (or modules to document with autodoc) are in another directory,"
v1.10.2,add these directories to sys.path here. If the directory is relative to the
v1.10.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.10.2,
v1.10.2,"sys.path.insert(0, os.path.abspath('..'))"
v1.10.2,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.10.2,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.10.2,from unittest.mock import Mock
v1.10.2,sys.modules['numpy'] = Mock()
v1.10.2,sys.modules['numpy.linalg'] = Mock()
v1.10.2,sys.modules['scipy'] = Mock()
v1.10.2,sys.modules['scipy.optimize'] = Mock()
v1.10.2,sys.modules['scipy.interpolate'] = Mock()
v1.10.2,sys.modules['scipy.sparse'] = Mock()
v1.10.2,sys.modules['scipy.ndimage'] = Mock()
v1.10.2,sys.modules['scipy.ndimage.filters'] = Mock()
v1.10.2,sys.modules['tensorflow'] = Mock()
v1.10.2,sys.modules['theano'] = Mock()
v1.10.2,sys.modules['theano.tensor'] = Mock()
v1.10.2,sys.modules['torch'] = Mock()
v1.10.2,sys.modules['torch.optim'] = Mock()
v1.10.2,sys.modules['torch.nn'] = Mock()
v1.10.2,sys.modules['torch.nn.init'] = Mock()
v1.10.2,sys.modules['torch.autograd'] = Mock()
v1.10.2,sys.modules['sklearn'] = Mock()
v1.10.2,sys.modules['sklearn.model_selection'] = Mock()
v1.10.2,sys.modules['sklearn.utils'] = Mock()
v1.10.2,-- Project information -----------------------------------------------------
v1.10.2,"The full version, including alpha/beta/rc tags."
v1.10.2,The short X.Y version.
v1.10.2,-- General configuration ---------------------------------------------------
v1.10.2,"If your documentation needs a minimal Sphinx version, state it here."
v1.10.2,
v1.10.2,needs_sphinx = '1.0'
v1.10.2,"If true, the current module name will be prepended to all description"
v1.10.2,unit titles (such as .. function::).
v1.10.2,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.10.2,"Add any Sphinx extension module names here, as strings. They can be"
v1.10.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.10.2,ones.
v1.10.2,show todo's
v1.10.2,generate autosummary pages
v1.10.2,"Add any paths that contain templates here, relative to this directory."
v1.10.2,The suffix(es) of source filenames.
v1.10.2,You can specify multiple suffix as a list of string:
v1.10.2,
v1.10.2,"source_suffix = ['.rst', '.md']"
v1.10.2,The master toctree document.
v1.10.2,The language for content autogenerated by Sphinx. Refer to documentation
v1.10.2,for a list of supported languages.
v1.10.2,
v1.10.2,This is also used if you do content translation via gettext catalogs.
v1.10.2,"Usually you set ""language"" from the command line for these cases."
v1.10.2,"List of patterns, relative to source directory, that match files and"
v1.10.2,directories to ignore when looking for source files.
v1.10.2,This pattern also affects html_static_path and html_extra_path.
v1.10.2,The name of the Pygments (syntax highlighting) style to use.
v1.10.2,-- Options for HTML output -------------------------------------------------
v1.10.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.10.2,a list of builtin themes.
v1.10.2,
v1.10.2,Theme options are theme-specific and customize the look and feel of a theme
v1.10.2,"further.  For a list of options available for each theme, see the"
v1.10.2,documentation.
v1.10.2,
v1.10.2,html_theme_options = {}
v1.10.2,"Add any paths that contain custom static files (such as style sheets) here,"
v1.10.2,"relative to this directory. They are copied after the builtin static files,"
v1.10.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.10.2,html_static_path = ['_static']
v1.10.2,"Custom sidebar templates, must be a dictionary that maps document names"
v1.10.2,to template names.
v1.10.2,
v1.10.2,The default sidebars (for documents that don't match any pattern) are
v1.10.2,defined by theme itself.  Builtin themes are using these templates by
v1.10.2,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.10.2,'searchbox.html']``.
v1.10.2,
v1.10.2,html_sidebars = {}
v1.10.2,The name of an image file (relative to this directory) to place at the top
v1.10.2,of the sidebar.
v1.10.2,
v1.10.2,-- Options for HTMLHelp output ---------------------------------------------
v1.10.2,Output file base name for HTML help builder.
v1.10.2,-- Options for LaTeX output ------------------------------------------------
v1.10.2,latex_elements = {
v1.10.2,The paper size ('letterpaper' or 'a4paper').
v1.10.2,
v1.10.2,"'papersize': 'letterpaper',"
v1.10.2,
v1.10.2,"The font size ('10pt', '11pt' or '12pt')."
v1.10.2,
v1.10.2,"'pointsize': '10pt',"
v1.10.2,
v1.10.2,Additional stuff for the LaTeX preamble.
v1.10.2,
v1.10.2,"'preamble': '',"
v1.10.2,
v1.10.2,Latex figure (float) alignment
v1.10.2,
v1.10.2,"'figure_align': 'htbp',"
v1.10.2,}
v1.10.2,Grouping the document tree into LaTeX files. List of tuples
v1.10.2,"(source start file, target name, title,"
v1.10.2,"author, documentclass [howto, manual, or own class])."
v1.10.2,latex_documents = [
v1.10.2,(
v1.10.2,"master_doc,"
v1.10.2,"'pykeen.tex',"
v1.10.2,"'PyKEEN Documentation',"
v1.10.2,"author,"
v1.10.2,"'manual',"
v1.10.2,"),"
v1.10.2,]
v1.10.2,-- Options for manual page output ------------------------------------------
v1.10.2,One entry per manual page. List of tuples
v1.10.2,"(source start file, name, description, authors, manual section)."
v1.10.2,-- Options for Texinfo output ----------------------------------------------
v1.10.2,Grouping the document tree into Texinfo files. List of tuples
v1.10.2,"(source start file, target name, title, author,"
v1.10.2,"dir menu entry, description, category)"
v1.10.2,-- Options for Epub output -------------------------------------------------
v1.10.2,Bibliographic Dublin Core info.
v1.10.2,epub_title = project
v1.10.2,The unique identifier of the text. This can be a ISBN number
v1.10.2,or the project homepage.
v1.10.2,
v1.10.2,epub_identifier = ''
v1.10.2,A unique identification for the text.
v1.10.2,
v1.10.2,epub_uid = ''
v1.10.2,A list of files that should not be packed into the epub file.
v1.10.2,epub_exclude_files = ['search.html']
v1.10.2,-- Extension configuration -------------------------------------------------
v1.10.2,-- Options for intersphinx extension ---------------------------------------
v1.10.2,Example configuration for intersphinx: refer to the Python standard library.
v1.10.2,"'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),"
v1.10.2,See discussion for adding huggingface intersphinx docs at
v1.10.2,https://github.com/huggingface/transformers/issues/14728#issuecomment-1133521776
v1.10.2,autodoc_member_order = 'bysource'
v1.10.2,autodoc_preserve_defaults = True
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,check probability distribution
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Check a model param is optimized
v1.10.2,Check a loss param is optimized
v1.10.2,Check a model param is NOT optimized
v1.10.2,Check a loss param is optimized
v1.10.2,Check a model param is optimized
v1.10.2,Check a loss param is NOT optimized
v1.10.2,Check a model param is NOT optimized
v1.10.2,Check a loss param is NOT optimized
v1.10.2,verify failure
v1.10.2,"Since custom data was passed, we can't store any of this"
v1.10.2,"currently, any custom data doesn't get stored."
v1.10.2,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.10.2,"Since there's no source path information, these shouldn't be"
v1.10.2,"added, even if it might be possible to infer path information"
v1.10.2,from the triples factories
v1.10.2,"Since paths were passed for training, testing, and validation,"
v1.10.2,they should be stored as study-level attributes
v1.10.2,Check a model param is optimized
v1.10.2,Check a loss param is optimized
v1.10.2,ignore abstract classes
v1.10.2,verify that all classes have the hpo_default dictionary
v1.10.2,verify that we can bind the keys to the __init__'s signature
v1.10.2,note: this is only of limited use since many have **kwargs which
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,docstr-coverage: inherited
v1.10.2,check if within 0.5 std of observed
v1.10.2,test error is raised
v1.10.2,there is an extra test for this case
v1.10.2,docstr-coverage: inherited
v1.10.2,same size tensors
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,Tests that exception will be thrown when more than or less than two tensors are passed
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,create broadcastable shapes
v1.10.2,check correct value range
v1.10.2,check maximum norm constraint
v1.10.2,unchanged values for small norms
v1.10.2,random entity embeddings & projections
v1.10.2,random relation embeddings & projections
v1.10.2,project
v1.10.2,check shape:
v1.10.2,check normalization
v1.10.2,check equivalence of re-formulation
v1.10.2,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.10.2,= r_p (e_p^T e) + e'
v1.10.2,"create random array, estimate the costs of addition, and measure some execution times."
v1.10.2,"then, compute correlation between the estimated cost, and the measured time."
v1.10.2,check for strong correlation between estimated costs and measured execution time
v1.10.2,get optimal sequence
v1.10.2,check caching
v1.10.2,get optimal sequence
v1.10.2,check correct cost
v1.10.2,check optimality
v1.10.2,compare result to sequential addition
v1.10.2,compare result to sequential addition
v1.10.2,ensure each node participates in at least one edge
v1.10.2,check type and shape
v1.10.2,number of colors is monotonically increasing
v1.10.2,ensure each node participates in at least one edge
v1.10.2,normalize
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,equal value; larger is better
v1.10.2,equal value; smaller is better
v1.10.2,larger is better; improvement
v1.10.2,larger is better; improvement; but not significant
v1.10.2,negative number
v1.10.2,assert that reporting another metric for this epoch raises an error
v1.10.2,: The window size used by the early stopper
v1.10.2,: The (zeroed) index  - 1 at which stopping will occur
v1.10.2,: The minimum improvement
v1.10.2,: The random seed to use for reproducibility
v1.10.2,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.10.2,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.10.2,: The batch size to use.
v1.10.2,Fix seed for reproducibility
v1.10.2,Set automatic_memory_optimization to false during testing
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,See https://github.com/pykeen/pykeen/pull/883
v1.10.2,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.10.2,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.10.2,"first triple, and passing create_inverse_triples=True."
v1.10.2,check for warning
v1.10.2,check for filtered triples
v1.10.2,check for correct inverse triples flag
v1.10.2,check correct translation
v1.10.2,check column order
v1.10.2,apply restriction
v1.10.2,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.10.2,check that inverse_triples is correctly carried over
v1.10.2,verify that the label-to-ID mapping has not been changed
v1.10.2,verify that triples have been filtered
v1.10.2,Test different combinations of restrictions
v1.10.2,check compressed triples
v1.10.2,reconstruct triples from compressed form
v1.10.2,check data loader
v1.10.2,set create inverse triple to true
v1.10.2,split factory
v1.10.2,check that in *training* inverse triple are to be created
v1.10.2,check that in all other splits no inverse triples are to be created
v1.10.2,verify that all entities and relations are present in the training factory
v1.10.2,verify that no triple got lost
v1.10.2,verify that the label-to-id mappings match
v1.10.2,Slightly larger number of triples to guarantee split can find coverage of all entities and relations.
v1.10.2,serialize
v1.10.2,de-serialize
v1.10.2,check for equality
v1.10.2,TODO: this could be (Core)TriplesFactory.__equal__
v1.10.2,cf. https://docs.pytest.org/en/7.1.x/example/parametrize.html#parametrizing-conditional-raising
v1.10.2,wrong ndim
v1.10.2,wrong last dim
v1.10.2,wrong dtype: float
v1.10.2,wrong dtype: complex
v1.10.2,correct
v1.10.2,>>> positional argument
v1.10.2,mapped_triples
v1.10.2,triples factory
v1.10.2,labeled triples + factory
v1.10.2,single labeled triple
v1.10.2,multiple labeled triples as list
v1.10.2,multiple labeled triples as array
v1.10.2,>>> keyword only
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,fixme: find reason / enforce single-thread
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,"DummyModel,"
v1.10.2,3x batch norm: bias + scale --> 6
v1.10.2,entity specific bias        --> 1
v1.10.2,==================================
v1.10.2,7
v1.10.2,"two bias terms, one conv-filter"
v1.10.2,Two linear layer biases
v1.10.2,"Two BN layers, bias & scale"
v1.10.2,Test that the weight in the MLP is trainable (i.e. requires grad)
v1.10.2,simulate creating a new triples factory with shared set of relations by shuffling
v1.10.2,quaternion have four components
v1.10.2,entity embeddings
v1.10.2,relation embeddings
v1.10.2,Compute Scores
v1.10.2,Use different dimension for relation embedding: relation_dim > entity_dim
v1.10.2,relation embeddings
v1.10.2,Compute Scores
v1.10.2,Use different dimension for relation embedding: relation_dim < entity_dim
v1.10.2,entity embeddings
v1.10.2,relation embeddings
v1.10.2,Compute Scores
v1.10.2,: 2xBN (bias & scale)
v1.10.2,the combination bias
v1.10.2,FIXME definitely a type mismatch going on here
v1.10.2,check shape
v1.10.2,check content
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,empty lists are falsy
v1.10.2,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.10.2,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.10.2,pipeline is checked.
v1.10.2,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.10.2,Resume the previous pipeline
v1.10.2,The MockModel gives the highest score to the highest entity id
v1.10.2,The test triples are created to yield the third highest score on both head and tail prediction
v1.10.2,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.10.2,These triples are created to yield the highest score on both head and tail prediction for the
v1.10.2,test triple at hand
v1.10.2,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.10.2,test triple at hand
v1.10.2,cf. https://github.com/pykeen/pykeen/issues/1118
v1.10.2,save a reference to the old init *before* mocking
v1.10.2,run a small pipline
v1.10.2,use sampled training loop ...
v1.10.2,... without explicitly selecting a negative sampler ...
v1.10.2,... but providing custom kwargs
v1.10.2,other parameters for fast test
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.10.2,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.10.2,Test looking up inverse triples
v1.10.2,test new label to ID
v1.10.2,type
v1.10.2,old labels
v1.10.2,"new, compact IDs"
v1.10.2,test vectorized lookup
v1.10.2,type
v1.10.2,shape
v1.10.2,value range
v1.10.2,only occurring Ids get mapped to non-negative numbers
v1.10.2,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.10.2,check type
v1.10.2,check shape
v1.10.2,check content
v1.10.2,check type
v1.10.2,check shape
v1.10.2,check 1-hot
v1.10.2,check type
v1.10.2,check shape
v1.10.2,check value range
v1.10.2,check self-similarity = 1
v1.10.2,base relation
v1.10.2,exact duplicate
v1.10.2,99% duplicate
v1.10.2,50% duplicate
v1.10.2,exact inverse
v1.10.2,99% inverse
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The expected number of entities
v1.10.2,: The expected number of relations
v1.10.2,: The expected number of triples
v1.10.2,": The tolerance on expected number of triples, for randomized situations"
v1.10.2,: The dataset to test
v1.10.2,: The instantiated dataset
v1.10.2,: Should the validation be assumed to have been loaded with train/test?
v1.10.2,Not loaded
v1.10.2,Load
v1.10.2,Test caching
v1.10.2,assert (end - start) < 1.0e-02
v1.10.2,Test consistency of training / validation / testing mapping
v1.10.2,": The directory, if there is caching"
v1.10.2,: The batch size
v1.10.2,: The number of negatives per positive for sLCWA training loop.
v1.10.2,: The number of entities LCWA training loop / label smoothing.
v1.10.2,test reduction
v1.10.2,test finite loss value
v1.10.2,Test backward
v1.10.2,negative scores decreased compared to positive ones
v1.10.2,negative scores decreased compared to positive ones
v1.10.2,check for invalid keys
v1.10.2,check that each parameter without a default occurs
v1.10.2,try to instantiate loss for some configurations in the HPO search space
v1.10.2,: The number of entities.
v1.10.2,: The number of negative samples
v1.10.2,: The number of entities.
v1.10.2,"the relative tolerance for checking close results, cf. torch.allclose"
v1.10.2,"the absolute tolerance for checking close results, cf. torch.allclose"
v1.10.2,: The equivalence for models with batch norm only holds in evaluation mode
v1.10.2,: The equivalence for models with batch norm only holds in evaluation mode
v1.10.2,: The equivalence for models with batch norm only holds in evaluation mode
v1.10.2,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.10.2,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.10.2,test multiple different initializations
v1.10.2,calculate by functional
v1.10.2,calculate manually
v1.10.2,allclose checks: | input - other | < atol + rtol * |other|
v1.10.2,simple
v1.10.2,nested
v1.10.2,nested
v1.10.2,prepare a temporary test directory
v1.10.2,check that file was created
v1.10.2,make sure to close file before trying to delete it
v1.10.2,delete intermediate files
v1.10.2,: The batch size
v1.10.2,: The device
v1.10.2,move test instance to device
v1.10.2,Use RESCAL as it regularizes multiple tensors of different shape.
v1.10.2,"verify that the regularizer is stored for both, entity and relation representations"
v1.10.2,Forward pass (should update regularizer)
v1.10.2,Call post_parameter_update (should reset regularizer)
v1.10.2,Check if regularization term is reset
v1.10.2,regularization term should be zero
v1.10.2,updated should be set to false
v1.10.2,call method
v1.10.2,generate random tensors
v1.10.2,generate inputs
v1.10.2,call update
v1.10.2,check shape
v1.10.2,check result
v1.10.2,generate single random tensor
v1.10.2,calculate penalty
v1.10.2,check shape
v1.10.2,check value
v1.10.2,update term
v1.10.2,check that the expected term is returned
v1.10.2,check that the regularizer is now reset
v1.10.2,create another instance with apply_only_once enabled
v1.10.2,test initial state
v1.10.2,"after first update, should change the term"
v1.10.2,"after second update, no change should happen"
v1.10.2,FIXME isn't any finite number allowed now?
v1.10.2,: Additional arguments passed to the training loop's constructor method
v1.10.2,: The triples factory instance
v1.10.2,: The batch size for use for forward_* tests
v1.10.2,: The embedding dimensionality
v1.10.2,: Whether to create inverse triples (needed e.g. by ConvE)
v1.10.2,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.10.2,: The batch size for use when testing training procedures
v1.10.2,: The number of epochs to train the model
v1.10.2,: A random number generator from torch
v1.10.2,: The number of parameters which receive a constant (i.e. non-randomized)
v1.10.2,initialization
v1.10.2,: Static extras to append to the CLI
v1.10.2,: the model's device
v1.10.2,: the inductive mode
v1.10.2,for reproducible testing
v1.10.2,insert shared parameters
v1.10.2,move model to correct device
v1.10.2,Check that all the parameters actually require a gradient
v1.10.2,Try to initialize an optimizer
v1.10.2,get model parameters
v1.10.2,re-initialize
v1.10.2,check that the operation works in-place
v1.10.2,check that the parameters where modified
v1.10.2,check for finite values by default
v1.10.2,check whether a gradient can be back-propgated
v1.10.2,TODO: look into score_r for inverse relations
v1.10.2,clear buffers for message passing models
v1.10.2,"For the high/low memory test cases of NTN, SE, etc."
v1.10.2,"else, leave to default"
v1.10.2,Make sure that inverse triples are created if create_inverse_triples=True
v1.10.2,triples factory is added by the pipeline
v1.10.2,TODO: Catch HolE MKL error?
v1.10.2,set regularizer term to something that isn't zero
v1.10.2,call post_parameter_update
v1.10.2,assert that the regularization term has been reset
v1.10.2,do one optimization step
v1.10.2,call post_parameter_update
v1.10.2,check model constraints
v1.10.2,Distance-based model
v1.10.2,dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)
v1.10.2,check type
v1.10.2,check shape
v1.10.2,create a new instance with guaranteed dropout
v1.10.2,set to training mode
v1.10.2,check for different output
v1.10.2,use more samples to make sure that enough values can be dropped
v1.10.2,this implicitly tests extra_repr / iter_extra_repr
v1.10.2,select random indices
v1.10.2,forward pass with full graph
v1.10.2,forward pass with restricted graph
v1.10.2,verify the results are similar
v1.10.2,: The number of entities
v1.10.2,: The number of triples
v1.10.2,: the message dim
v1.10.2,TODO: separation message vs. entity dim?
v1.10.2,check shape
v1.10.2,check dtype
v1.10.2,check finite values (e.g. due to division by zero)
v1.10.2,check non-negativity
v1.10.2,: the input dimension
v1.10.2,: the output dimension
v1.10.2,: the number of entities
v1.10.2,: the shape of the tensor to initialize
v1.10.2,: to be initialized / set in subclass
v1.10.2,: the interaction to use for testing a model
v1.10.2,initializers *may* work in-place => clone
v1.10.2,actual number may be different...
v1.10.2,unfavourable split to ensure that cleanup is necessary
v1.10.2,check for unclean split
v1.10.2,check that no triple got lost
v1.10.2,check that triples where only moved from other to reference
v1.10.2,check that all entities occur in reference
v1.10.2,check that no triple got lost
v1.10.2,check that all entities are covered in first part
v1.10.2,the model
v1.10.2,Settings
v1.10.2,Use small model (untrained)
v1.10.2,Get batch
v1.10.2,Compute scores
v1.10.2,Compute mask only if required
v1.10.2,TODO: Re-use filtering code
v1.10.2,"shape: (batch_size, num_triples)"
v1.10.2,"shape: (batch_size, num_entities)"
v1.10.2,Process one batch
v1.10.2,shape
v1.10.2,value range
v1.10.2,no duplicates
v1.10.2,shape
v1.10.2,value range
v1.10.2,no duplicates
v1.10.2,shape
v1.10.2,value range
v1.10.2,"no repetition, except padding idx"
v1.10.2,inferred from triples factory
v1.10.2,: The batch size
v1.10.2,: the maximum number of candidates
v1.10.2,: the number of ranks
v1.10.2,: the number of samples to use for monte-carlo estimation
v1.10.2,: the number of candidates for each individual ranking task
v1.10.2,: the ranks for each individual ranking task
v1.10.2,data type
v1.10.2,value range
v1.10.2,original ranks
v1.10.2,better ranks
v1.10.2,variances are non-negative
v1.10.2,generate random weights such that sum = n
v1.10.2,for sanity checking: give the largest weight to best rank => should improve
v1.10.2,generate two versions
v1.10.2,1. repeat each rank/candidate pair a random number of times
v1.10.2,"2. do not repeat, but assign a corresponding weight"
v1.10.2,check flatness
v1.10.2,"TODO: does this suffice, or do we really need float as datatype?"
v1.10.2,generate random triples factories
v1.10.2,generate random alignment
v1.10.2,add label information if necessary
v1.10.2,prepare alignment data frame
v1.10.2,call
v1.10.2,check
v1.10.2,: The window size used by the early stopper
v1.10.2,: The mock losses the mock evaluator will return
v1.10.2,: The (zeroed) index  - 1 at which stopping will occur
v1.10.2,: The minimum improvement
v1.10.2,: The best results
v1.10.2,Set automatic_memory_optimization to false for tests
v1.10.2,Step early stopper
v1.10.2,check storing of results
v1.10.2,not needed for test
v1.10.2,verify that the input is valid
v1.10.2,combine
v1.10.2,verify shape
v1.10.2,to be initialized in subclass
v1.10.2,no column has been removed
v1.10.2,all old columns are unmodified
v1.10.2,new columns are boolean
v1.10.2,no columns have been added
v1.10.2,check subset relation
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO: this could be shared with the model tests
v1.10.2,fixme: CompGCN leads to an autograd runtime error...
v1.10.2,"models.CompGCN: dict(embedding_dim=EMBEDDING_DIM),"
v1.10.2,"FixedModel: dict(embedding_dim=EMBEDDING_DIM),"
v1.10.2,test combinations of models with training loops
v1.10.2,some models require inverse relations
v1.10.2,some model require access to the training triples
v1.10.2,"inductive models require an inductive mode to be set, and an inference factory to be passed"
v1.10.2,fake an inference factory
v1.10.2,automatically choose accelerator
v1.10.2,defaults to TensorBoard; explicitly disabled here
v1.10.2,disable checkpointing
v1.10.2,fast run
v1.10.2,automatically choose accelerator
v1.10.2,defaults to TensorBoard; explicitly disabled here
v1.10.2,disable checkpointing
v1.10.2,fast run
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,check for finite values by default
v1.10.2,Set into training mode to check if it is correctly set to evaluation mode.
v1.10.2,Set into training mode to check if it is correctly set to evaluation mode.
v1.10.2,Set into training mode to check if it is correctly set to evaluation mode.
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,≈ result of softmax
v1.10.2,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.10.2,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.10.2,sum over the softmax dim as weights sum up to 1
v1.10.2,"pos_distances = [0., 0., 0.5, 0.5]"
v1.10.2,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.10.2,≈ result of sigmoid
v1.10.2,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.10.2,expected_loss ≈ 0.34
v1.10.2,abstract classes
v1.10.2,Create dummy dense labels
v1.10.2,Check if labels form a probability distribution
v1.10.2,Apply label smoothing
v1.10.2,Check if smooth labels form probability distribution
v1.10.2,Create dummy sLCWA labels
v1.10.2,Apply label smoothing
v1.10.2,generate random ratios
v1.10.2,check size
v1.10.2,check value range
v1.10.2,check total split
v1.10.2,check consistency with ratios
v1.10.2,the number of decimal digits equivalent to 1 / n_total
v1.10.2,check type
v1.10.2,check values
v1.10.2,compare against expected
v1.10.2,generated_triples = generate_triples()
v1.10.2,check type
v1.10.2,check format
v1.10.2,check coverage
v1.10.2,prediction post-processing
v1.10.2,mock prediction data frame
v1.10.2,score consumers
v1.10.2,"use a small model, since operation is expensive"
v1.10.2,"all scores, automatic batch size"
v1.10.2,top 3 scores
v1.10.2,"top 3 scores, fixed batch size, head scoring"
v1.10.2,"all scores, relation scoring"
v1.10.2,"all scores, relation scoring"
v1.10.2,model with inverse relations
v1.10.2,check type
v1.10.2,check shape
v1.10.2,check ID ranges
v1.10.2,"mapped triples, automatic batch size selection, no factory"
v1.10.2,"mapped triples, fixed batch size, no factory"
v1.10.2,labeled triples with factory
v1.10.2,labeled triples as list
v1.10.2,single labeled triple
v1.10.2,model with inverse relations
v1.10.2,"ID-based, no factory"
v1.10.2,string-based + factory
v1.10.2,mixed + factory
v1.10.2,"no restriction, no factory"
v1.10.2,"no restriction, factory"
v1.10.2,"id restriction, no factory ..."
v1.10.2,id restriction with factory
v1.10.2,"comment: we only use id-based input, since the normalization has already been tested"
v1.10.2,create model
v1.10.2,"id-based head/relation/tail prediction, no restriction"
v1.10.2,restriction by list of ints
v1.10.2,tail prediction
v1.10.2,try accessing each element
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,"naive implementation, O(n2)"
v1.10.2,check correct output type
v1.10.2,check value range subset
v1.10.2,check value range side
v1.10.2,check columns
v1.10.2,check value range and type
v1.10.2,check value range entity IDs
v1.10.2,check value range entity labels
v1.10.2,check correct type
v1.10.2,check relation_id value range
v1.10.2,check pattern value range
v1.10.2,check confidence value range
v1.10.2,check support value range
v1.10.2,check correct type
v1.10.2,check relation_id value range
v1.10.2,check pattern value range
v1.10.2,check correct type
v1.10.2,check relation_id value range
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,clear
v1.10.2,docstr-coverage: inherited
v1.10.2,assumes deterministic entity to id mapping
v1.10.2,from left_tf
v1.10.2,from right_tf with offset
v1.10.2,docstr-coverage: inherited
v1.10.2,assumes deterministic entity to id mapping
v1.10.2,from left_tf
v1.10.2,from right_tf with offset
v1.10.2,extra-relation
v1.10.2,docstr-coverage: inherited
v1.10.2,assumes deterministic entity to id mapping
v1.10.2,docstr-coverage: inherited
v1.10.2,assumes deterministic entity to id mapping
v1.10.2,from left_tf
v1.10.2,from right_tf with offset
v1.10.2,additional
v1.10.2,verify shape
v1.10.2,verify dtype
v1.10.2,verify number of entities/relations
v1.10.2,verify offsets
v1.10.2,"create old, new pairs"
v1.10.2,simulate merging ids
v1.10.2,only a single pair
v1.10.2,apply
v1.10.2,every key is contained
v1.10.2,value range
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Check minimal statistics
v1.10.2,Check either a github link or author/publication information is given
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO: we could move this part into the interaction module itself
v1.10.2,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.10.2,"prepare conv input (N, C, H, W)"
v1.10.2,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.10.2,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.10.2,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.10.2,Rotate (=Hamilton product in quaternion space).
v1.10.2,"we calculate the scores using the hard-coded formula, instead of utilizing table + einsum"
v1.10.2,"f(h, r, t) = h @ r @ t"
v1.10.2,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.10.2,normalize rotations to unit modulus
v1.10.2,check for unit modulus
v1.10.2,entity embeddings
v1.10.2,relation embeddings
v1.10.2,Compute Scores
v1.10.2,entity embeddings
v1.10.2,relation embeddings
v1.10.2,Compute Scores
v1.10.2,Compute Scores
v1.10.2,-\|R_h h - R_t t\|
v1.10.2,-\|h - t\|
v1.10.2,"Since MuRE has offsets, the scores do not need to negative"
v1.10.2,"We do not need this, since we do not check for functional consistency anyway"
v1.10.2,intra-interaction comparison
v1.10.2,dimension needs to be divisible by num_heads
v1.10.2,FIXME
v1.10.2,multiple
v1.10.2,single
v1.10.2,head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid
v1.10.2,check type
v1.10.2,check size
v1.10.2,check value range
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,message_dim must be divisible by num_heads
v1.10.2,generate test data (with fixed seed for reproducibility)
v1.10.2,get result using argpartition
v1.10.2,check shape
v1.10.2,check type
v1.10.2,check value range
v1.10.2,check equality with argsort
v1.10.2,determine pool using anchor searcher
v1.10.2,determine expected pool using shortest path distances via scipy.sparse.csgraph
v1.10.2,generate random pool
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,complex tensor
v1.10.2,check value range
v1.10.2,check modulus == 1
v1.10.2,quaternion needs shape to end on 4
v1.10.2,"check value range (actually [-s, +s] with s = 1/sqrt(2*n))"
v1.10.2,value range
v1.10.2,highest degree node has largest value
v1.10.2,Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf
v1.10.2,create triples with a dummy relation type 0
v1.10.2,"0: green: 2, 3, 7, 8"
v1.10.2,"1: red: 1, 4, 6, 9"
v1.10.2,"2: blue: 0, 5"
v1.10.2,the example includes the first power
v1.10.2,requires at least one complex tensor as input
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,inferred from triples factory
v1.10.2,inferred from assignment
v1.10.2,the representation module infers the max_id from the provided labels
v1.10.2,the following entity does not have an image -> will have to use backfill
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,the representation module infers the max_id from the provided labels
v1.10.2,docstr-coverage: inherited
v1.10.2,the representation module infers the max_id from the provided labels
v1.10.2,max_id is inferred from assignment
v1.10.2,create random assignment
v1.10.2,update kwargs
v1.10.2,empty bases
v1.10.2,inconsistent base shapes
v1.10.2,invalid base id
v1.10.2,invalid local index
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO this is the only place this function is used.
v1.10.2,Is there an alternative so we can remove it?
v1.10.2,ensure positivity
v1.10.2,compute using pytorch
v1.10.2,prepare distributions
v1.10.2,compute using pykeen
v1.10.2,"e: (batch_size, num_heads, num_tails, d)"
v1.10.2,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.10.2,divergence = 0 => similarity = -divergence = 0
v1.10.2,"(h - t), r"
v1.10.2,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.10.2,divergence >= 0 => similarity = -divergence <= 0
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.10.2,filter vs. no filter thing.
v1.10.2,TODO: more tests
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,check for empty batches
v1.10.2,: The window size used by the early stopper
v1.10.2,: The mock losses the mock evaluator will return
v1.10.2,: The (zeroed) index  - 1 at which stopping will occur
v1.10.2,: The minimum improvement
v1.10.2,: The best results
v1.10.2,Set automatic_memory_optimization to false for tests
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Train a model in one shot
v1.10.2,Train a model for the first half
v1.10.2,Continue training of the first part
v1.10.2,check non-empty metrics
v1.10.2,: Should negative samples be filtered?
v1.10.2,expectation = (1 + n) / 2
v1.10.2,variance = (n**2 - 1) / 12
v1.10.2,"x_i ~ N(mu_i, 1)"
v1.10.2,closed-form solution
v1.10.2,sampled confidence interval
v1.10.2,check that closed-form is in confidence interval of sampled
v1.10.2,positive values only
v1.10.2,positive and negative values
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Check for correct class
v1.10.2,check correct num_entities
v1.10.2,check type
v1.10.2,check length
v1.10.2,check type
v1.10.2,check length
v1.10.2,check confidence positivity
v1.10.2,Check for correct class
v1.10.2,"true_score: (2, 3, 3)"
v1.10.2,head based filter
v1.10.2,preprocessing for faster lookup
v1.10.2,check that all found positives are positive
v1.10.2,check in-place
v1.10.2,Test head scores
v1.10.2,Assert in-place modification
v1.10.2,Assert correct filtering
v1.10.2,Test tail scores
v1.10.2,Assert in-place modification
v1.10.2,Assert correct filtering
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,The MockModel gives the highest score to the highest entity id
v1.10.2,The test triples are created to yield the third highest score on both head and tail prediction
v1.10.2,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.10.2,These triples are created to yield the highest score on both head and tail prediction for the
v1.10.2,test triple at hand
v1.10.2,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.10.2,test triple at hand
v1.10.2,check true negatives
v1.10.2,TODO: check no repetitions (if possible)
v1.10.2,return type
v1.10.2,columns
v1.10.2,value range
v1.10.2,relation restriction
v1.10.2,with explicit num_entities
v1.10.2,with inferred num_entities
v1.10.2,test different shapes
v1.10.2,test different shapes
v1.10.2,value range
v1.10.2,value range
v1.10.2,check unique
v1.10.2,"strips off the ""k"" at the end"
v1.10.2,Populate with real results.
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,"(-1, 1),"
v1.10.2,"(-1, -1),"
v1.10.2,"(-5, -3),"
v1.10.2,initialize
v1.10.2,update with batches
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Check whether filtering works correctly
v1.10.2,First giving an example where all triples have to be filtered
v1.10.2,The filter should remove all triples
v1.10.2,Create an example where no triples will be filtered
v1.10.2,The filter should not remove any triple
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,same relation
v1.10.2,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.10.2,Test that half of the subjects and half of the objects are corrupted
v1.10.2,check that corrupted entities co-occur with the relation in training data
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The batch size
v1.10.2,: The random seed
v1.10.2,: The triples factory
v1.10.2,: The instances
v1.10.2,: A positive batch
v1.10.2,: Kwargs
v1.10.2,Generate negative sample
v1.10.2,check filter shape if necessary
v1.10.2,check shape
v1.10.2,check bounds: heads
v1.10.2,check bounds: relations
v1.10.2,check bounds: tails
v1.10.2,test that the negative triple is not the original positive triple
v1.10.2,"shape: (batch_size, 1, num_neg)"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Base Classes
v1.10.2,Concrete Classes
v1.10.2,Utils
v1.10.2,: synonyms of this loss
v1.10.2,: The default strategy for optimizing the loss's hyper-parameters
v1.10.2,flatten and stack
v1.10.2,apply label smoothing if necessary.
v1.10.2,TODO: Do label smoothing only once
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,Sanity check
v1.10.2,negative_scores have already been filtered in the sampler!
v1.10.2,"shape: (nnz,)"
v1.10.2,docstr-coverage: inherited
v1.10.2,Sanity check
v1.10.2,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.10.2,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.10.2,positive entries in the labels tensor.
v1.10.2,"This shows how often one row has to be repeated,"
v1.10.2,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.10.2,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.10.2,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.10.2,This tells us how often each true label should be repeated
v1.10.2,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.10.2,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.10.2,within the ablation pipeline.
v1.10.2,0. default
v1.10.2,1. positive & negative margin
v1.10.2,2. negative margin & offset
v1.10.2,3. positive margin & offset
v1.10.2,docstr-coverage: inherited
v1.10.2,Sanity check
v1.10.2,positive term
v1.10.2,implicitly repeat positive scores
v1.10.2,"shape: (nnz,)"
v1.10.2,negative term
v1.10.2,negative_scores have already been filtered in the sampler!
v1.10.2,docstr-coverage: inherited
v1.10.2,Sanity check
v1.10.2,"scale labels from [0, 1] to [-1, 1]"
v1.10.2,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.10.2,within the ablation pipeline.
v1.10.2,docstr-coverage: inherited
v1.10.2,negative_scores have already been filtered in the sampler!
v1.10.2,(dense) softmax requires unfiltered scores / masking
v1.10.2,we need to fill the scores with -inf for all filtered negative examples
v1.10.2,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.10.2,use filled negatives scores
v1.10.2,docstr-coverage: inherited
v1.10.2,we need dense negative scores => unfilter if necessary
v1.10.2,"we may have inf rows, since there will be one additional finite positive score per row"
v1.10.2,"combine scores: shape: (batch_size, num_negatives + 1)"
v1.10.2,use sparse version of cross entropy
v1.10.2,calculate cross entropy loss
v1.10.2,docstr-coverage: inherited
v1.10.2,make sure labels form a proper probability distribution
v1.10.2,calculate cross entropy loss
v1.10.2,docstr-coverage: inherited
v1.10.2,determine positive; do not check with == since the labels are floats
v1.10.2,subtract margin from positive scores
v1.10.2,divide by temperature
v1.10.2,docstr-coverage: inherited
v1.10.2,subtract margin from positive scores
v1.10.2,normalize positive score shape
v1.10.2,divide by temperature
v1.10.2,docstr-coverage: inherited
v1.10.2,determine positive; do not check with == since the labels are floats
v1.10.2,compute negative weights (without gradient tracking)
v1.10.2,clone is necessary since we modify in-place
v1.10.2,Split positive and negative scores
v1.10.2,"we pass *all* scores as negatives, but set the weight of positives to zero"
v1.10.2,this allows keeping a dense shape
v1.10.2,docstr-coverage: inherited
v1.10.2,Sanity check
v1.10.2,"we do not allow full -inf rows, since we compute the softmax over this tensor"
v1.10.2,compute weights (without gradient tracking)
v1.10.2,"fill negative scores with some finite value, e.g., 0 (they will get masked out anyway)"
v1.10.2,note: this is a reduction along the softmax dim; since the weights are already normalized
v1.10.2,"to sum to one, we want a sum reduction here, instead of using the self._reduction"
v1.10.2,docstr-coverage: inherited
v1.10.2,Sanity check
v1.10.2,docstr-coverage: inherited
v1.10.2,Sanity check
v1.10.2,negative loss part
v1.10.2,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.10.2,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.10.2,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: maybe we can make this more efficient?
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: maybe we can make this more efficient?
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO: method is_inverse?
v1.10.2,TODO: inverse of inverse?
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,The number of relations stored in the triples factory includes the number of inverse relations
v1.10.2,Id of inverse relation: relation + 1
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.10.2,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.10.2,": For more information, see https://github.com/cthoyt/pystow"
v1.10.2,: A path representing the PyKEEN data folder
v1.10.2,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.10.2,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.10.2,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.10.2,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.10.2,: A subdirectory for PyKEEN logs
v1.10.2,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.10.2,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.10.2,"TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728"
v1.10.2,"SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.10.2,get datatype specific epsilon
v1.10.2,clamp minimum value
v1.10.2,try to resolve ambiguous device; there has to be at least one cuda device
v1.10.2,lower bound
v1.10.2,upper bound
v1.10.2,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.10.2,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.10.2,Determine optimal order and cost
v1.10.2,translate back to original order
v1.10.2,determine optimal processing order
v1.10.2,heuristic
v1.10.2,The dimensions affected by e'
v1.10.2,Project entities
v1.10.2,r_p (e_p.T e) + e'
v1.10.2,Enforce constraints
v1.10.2,TODO delete when deleting _normalize_dim (below)
v1.10.2,TODO delete when deleting convert_to_canonical_shape (below)
v1.10.2,TODO delete? See note in test_sim.py on its only usage
v1.10.2,upgrade to sequence
v1.10.2,broadcast
v1.10.2,"normalize ids: -> ids.shape: (batch_size, num_ids)"
v1.10.2,"normalize batch -> batch.shape: (batch_size, 1, 3)"
v1.10.2,allocate memory
v1.10.2,copy ids
v1.10.2,reshape
v1.10.2,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.10.2,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.10.2,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.10.2,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.10.2,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.10.2,"note: this is a hack, and should be fixed up-stream by making NodePiece"
v1.10.2,"use proper complex embeddings for rotate interaction; however, we also have representations"
v1.10.2,"that perform message passing, and we would need to propagate the base representation's complexity through it"
v1.10.2,ensure pathlib
v1.10.2,cf. https://stackoverflow.com/a/1176023
v1.10.2,check validity
v1.10.2,path compression
v1.10.2,get representatives
v1.10.2,already merged
v1.10.2,make x the smaller one
v1.10.2,merge
v1.10.2,extract partitions
v1.10.2,resolve path to make sure it is an absolute path
v1.10.2,ensure directory exists
v1.10.2,message passing: collect colors of neighbors
v1.10.2,"dense colors: shape: (n, c)"
v1.10.2,"adj:          shape: (n, n)"
v1.10.2,"values need to be float, since torch.sparse.mm does not support integer dtypes"
v1.10.2,size: will be correctly inferred
v1.10.2,concat with old colors
v1.10.2,hash
v1.10.2,create random indicator functions of low dimensionality
v1.10.2,collect neighbors' colors
v1.10.2,round to avoid numerical effects
v1.10.2,hash first
v1.10.2,concat with old colors
v1.10.2,re-hash
v1.10.2,"only keep connectivity, but remove multiplicity"
v1.10.2,"note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,"
v1.10.2,this is rather useless
v1.10.2,initial: degree
v1.10.2,"note: we calculate this separately, since we can use a more efficient implementation for the first step"
v1.10.2,hash
v1.10.2,determine small integer type for dense count array
v1.10.2,convergence check
v1.10.2,each node has a unique color
v1.10.2,the number of colors did not improve in the last iteration
v1.10.2,cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Base Class
v1.10.2,Child classes
v1.10.2,Utils
v1.10.2,: The overall regularization weight
v1.10.2,: The current regularization term (a scalar)
v1.10.2,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.10.2,: Has this regularizer been updated since last being reset?
v1.10.2,: The default strategy for optimizing the regularizer's hyper-parameters
v1.10.2,"If there are tracked parameters, update based on them"
v1.10.2,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.10.2,docstr-coverage: inherited
v1.10.2,no need to compute anything
v1.10.2,docstr-coverage: inherited
v1.10.2,always return zero
v1.10.2,: The dimension along which to compute the vector-based regularization terms.
v1.10.2,: Whether to normalize the regularization term by the dimension of the vectors.
v1.10.2,: This allows dimensionality-independent weight tuning.
v1.10.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.2,docstr-coverage: inherited
v1.10.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.2,docstr-coverage: inherited
v1.10.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.2,regularizer-specific parameters
v1.10.2,docstr-coverage: inherited
v1.10.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,orthogonality soft constraint: cosine similarity at most epsilon
v1.10.2,The normalization factor to balance individual regularizers' contribution.
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,high-level
v1.10.2,Low-Level
v1.10.2,cf. https://github.com/python/mypy/issues/5374
v1.10.2,": the dataframe; has to have a column named ""score"""
v1.10.2,: an optional factory to use for labeling
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,: the prediction target
v1.10.2,: the other column's fixed IDs
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,": the ID-based triples, shape: (n, 3)"
v1.10.2,: the scores
v1.10.2,3-tuple for return
v1.10.2,"extract label information, if possible"
v1.10.2,no restriction
v1.10.2,restriction is a tensor
v1.10.2,restriction is a sequence of integers or strings
v1.10.2,"now, restriction is a sequence of integers"
v1.10.2,"if explicit ids have been given, and label information is available, extract list of labels"
v1.10.2,exactly one of them is None
v1.10.2,create input batch
v1.10.2,"note type alias annotation required,"
v1.10.2,cf. https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases
v1.10.2,"batch, TODO: ids?"
v1.10.2,docstr-coverage: inherited
v1.10.2,initialize buffer on device
v1.10.2,docstr-coverage: inherited
v1.10.2,"reshape, shape: (batch_size * num_entities,)"
v1.10.2,get top scores within batch
v1.10.2,determine corresponding indices
v1.10.2,"batch_id, score_id = divmod(top_indices, num_scores)"
v1.10.2,combine to top triples
v1.10.2,append to global top scores
v1.10.2,reduce size if necessary
v1.10.2,initialize buffer on cpu
v1.10.2,Explicitly create triples
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: variable targets across batches/samples?
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"(?, r, t) => r.stride > t.stride"
v1.10.2,"(h, ?, t) => h.stride > t.stride"
v1.10.2,"(h, r, ?) => h.stride > r.stride"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,train model; note: needs larger number of epochs to do something useful ;-)
v1.10.2,"create prediction dataset, where the head entities is from a set of European countries,"
v1.10.2,and the relations are connected to tourism
v1.10.2,"calculate all scores for this restricted set, and keep k=3 largest"
v1.10.2,add labels
v1.10.2,: the choices for the first and second component of the input batch
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,calculate batch scores onces
v1.10.2,consume by all consumers
v1.10.2,TODO: Support partial dataset
v1.10.2,note: the models' predict method takes care of setting the model to evaluation mode
v1.10.2,exactly one of them is None
v1.10.2,
v1.10.2,note: the models' predict method takes care of setting the model to evaluation mode
v1.10.2,get input & target
v1.10.2,get label-to-id mapping and prediction targets
v1.10.2,get scores
v1.10.2,"note: maybe we want to expose these scores, too?"
v1.10.2,create raw dataframe
v1.10.2,note: the models' predict method takes care of setting the model to evaluation mode
v1.10.2,normalize input
v1.10.2,calculate scores (with automatic memory optimization)
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,determine fully qualified name
v1.10.2,shorten to main module
v1.10.2,verify that short name can be imported from the abbreviated reference
v1.10.2,get docdata and extract name & citation
v1.10.2,fallback for name: capitalized class name without base suffix
v1.10.2,extract citation information and warn about lack thereof
v1.10.2,compose reference
v1.10.2,cf. https://github.com/python/mypy/issues/5374
v1.10.2,"""Closed-Form Expectation"","
v1.10.2,"""Closed-Form Variance"","
v1.10.2,"""✓"" if metric.closed_expectation else """","
v1.10.2,"""✓"" if metric.closed_variance else """","
v1.10.2,Add HPO command
v1.10.2,Add NodePiece tokenization command
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,General types
v1.10.2,Triples
v1.10.2,Others
v1.10.2,Tensor Functions
v1.10.2,Tensors
v1.10.2,Dataclasses
v1.10.2,prediction targets
v1.10.2,modes
v1.10.2,entity alignment sides
v1.10.2,utils
v1.10.2,: A function that mutates the input and returns a new object of the same type as output
v1.10.2,: A function that can be applied to a tensor to initialize it
v1.10.2,: A function that can be applied to a tensor to normalize it
v1.10.2,: A function that can be applied to a tensor to constrain it
v1.10.2,: A hint for a :class:`torch.device`
v1.10.2,: A hint for a :class:`torch.Generator`
v1.10.2,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.10.2,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.10.2,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.10.2,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.10.2,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.10.2,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.10.2,: the inductive prediction and training mode
v1.10.2,: the prediction target
v1.10.2,: the prediction target index
v1.10.2,: the rank types
v1.10.2,"RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8"
v1.10.2,entity alignment
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,infer shape
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,input normalization
v1.10.2,note: the base class does not have any parameters
v1.10.2,Heuristic for default value
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"note: the only parameters are inside the relation representation module, which has its own reset_parameters"
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: can we change the dimension order to make this contiguous?
v1.10.2,docstr-coverage: inherited
v1.10.2,normalize num blocks
v1.10.2,determine necessary padding
v1.10.2,determine block sizes
v1.10.2,"(R, nb, bsi, bso)"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,apply padding if necessary
v1.10.2,"(n, di) -> (n, nb, bsi)"
v1.10.2,"(n, nb, bsi), (R, nb, bsi, bso) -> (R, n, nb, bso)"
v1.10.2,"(R, n, nb, bso) -> (R * n, do)"
v1.10.2,"note: depending on the contracting order, the output may supporting viewing, or not"
v1.10.2,"(n, R * n), (R * n, do) -> (n, do)"
v1.10.2,remove padding if necessary
v1.10.2,docstr-coverage: inherited
v1.10.2,apply padding if necessary
v1.10.2,"(R * n, n), (n, di) -> (R * n, di)"
v1.10.2,"(R * n, di) -> (R, n, nb, bsi)"
v1.10.2,"(R, nb, bsi, bso), (R, n, nb, bsi) -> (n, nb, bso)"
v1.10.2,"(n, nb, bso) -> (n, do)"
v1.10.2,"note: depending on the contracting order, the output may supporting viewing, or not"
v1.10.2,remove padding if necessary
v1.10.2,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501
v1.10.2,there are separate decompositions for forward and backward relations.
v1.10.2,the self-loop weight is not decomposed.
v1.10.2,TODO: we could cache the stacked adjacency matrices
v1.10.2,self-loop
v1.10.2,forward messages
v1.10.2,backward messages
v1.10.2,activation
v1.10.2,input validation
v1.10.2,has to be imported now to avoid cyclic imports
v1.10.2,has to be assigned after call to nn.Module init
v1.10.2,Resolve edge weighting
v1.10.2,dropout
v1.10.2,"Save graph using buffers, such that the tensors are moved together with the model"
v1.10.2,no activation on last layer
v1.10.2,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501
v1.10.2,buffering of enriched representations
v1.10.2,docstr-coverage: inherited
v1.10.2,invalidate enriched embeddings
v1.10.2,docstr-coverage: inherited
v1.10.2,Bind fields
v1.10.2,"shape: (num_entities, embedding_dim)"
v1.10.2,Edge dropout: drop the same edges on all layers (only in training mode)
v1.10.2,Get random dropout mask
v1.10.2,Apply to edges
v1.10.2,fixed edges -> pre-compute weights
v1.10.2,Cache enriched representations
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Utils
v1.10.2,: the maximum ID (exclusively)
v1.10.2,: the shape of an individual representation
v1.10.2,: a normalizer for individual representations
v1.10.2,: a regularizer for individual representations
v1.10.2,: dropout
v1.10.2,heuristic
v1.10.2,normalize *before* repeating
v1.10.2,repeat if necessary
v1.10.2,regularize *after* repeating
v1.10.2,"dropout & regularizer will appear automatically, since it is a nn.Module"
v1.10.2,has to be imported here to avoid cyclic import
v1.10.2,docstr-coverage: inherited
v1.10.2,normalize num_embeddings vs. max_id
v1.10.2,normalize embedding_dim vs. shape
v1.10.2,work-around until full complex support (torch==1.10 still does not work)
v1.10.2,TODO: verify that this is our understanding of complex!
v1.10.2,"note: this seems to work, as finfo returns the datatype of the underlying floating"
v1.10.2,"point dtype, rather than the combined complex one"
v1.10.2,"use make for initializer since there's a default, and make_safe"
v1.10.2,for the others to pass through None values
v1.10.2,docstr-coverage: inherited
v1.10.2,initialize weights in-place
v1.10.2,docstr-coverage: inherited
v1.10.2,apply constraints in-place
v1.10.2,fixme: work-around until nn.Embedding supports complex
v1.10.2,docstr-coverage: inherited
v1.10.2,fixme: work-around until nn.Embedding supports complex
v1.10.2,verify that contiguity is preserved
v1.10.2,create low-rank approximation object
v1.10.2,"get base representations, shape: (n, *ds)"
v1.10.2,"calculate SVD, U.shape: (n, k), s.shape: (k,), u.shape: (k, prod(ds))"
v1.10.2,overwrite bases and weights
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"get all base representations, shape: (num_bases, *shape)"
v1.10.2,"get base weights, shape: (*batch_dims, num_bases)"
v1.10.2,"weighted linear combination of bases, shape: (*batch_dims, *shape)"
v1.10.2,normalize output dimension
v1.10.2,entity-relation composition
v1.10.2,edge weighting
v1.10.2,message passing weights
v1.10.2,linear relation transformation
v1.10.2,layer-specific self-loop relation representation
v1.10.2,other components
v1.10.2,initialize
v1.10.2,split
v1.10.2,compose
v1.10.2,transform
v1.10.2,normalization
v1.10.2,aggregate by sum
v1.10.2,dropout
v1.10.2,prepare for inverse relations
v1.10.2,update entity representations: mean over self-loops / forward edges / backward edges
v1.10.2,Relation transformation
v1.10.2,has to be imported here to avoid cyclic imports
v1.10.2,kwargs
v1.10.2,Buffered enriched entity and relation representations
v1.10.2,TODO: Check
v1.10.2,TODO: might not be true for all compositions
v1.10.2,hidden dimension normalization
v1.10.2,Create message passing layers
v1.10.2,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.10.2,TODO: This always uses all training triples for message passing
v1.10.2,initialize buffer of enriched representations
v1.10.2,docstr-coverage: inherited
v1.10.2,invalidate enriched embeddings
v1.10.2,docstr-coverage: inherited
v1.10.2,"when changing from evaluation to training mode, the buffered representations have been computed without"
v1.10.2,"gradient tracking. hence, we need to invalidate them."
v1.10.2,note: this occurs in practice when continuing training after evaluation.
v1.10.2,enrich
v1.10.2,docstr-coverage: inherited
v1.10.2,check max_id
v1.10.2,infer shape
v1.10.2,"assign after super, since they should be properly registered as submodules"
v1.10.2,docstr-coverage: inherited
v1.10.2,: the base representations
v1.10.2,: the combination module
v1.10.2,input normalization
v1.10.2,has to be imported here to avoid cyclic import
v1.10.2,create base representations
v1.10.2,verify same ID range
v1.10.2,"note: we could also relax the requiremen, and set max_id = min(max_ids)"
v1.10.2,shape inference
v1.10.2,assign base representations *after* super init
v1.10.2,docstr-coverage: inherited
v1.10.2,delegate to super class
v1.10.2,docstr-coverage: inherited
v1.10.2,Generate graph dataset from the Monarch Disease Ontology (MONDO)
v1.10.2,": the assignment from global ID to (representation, local id), shape: (max_id, 2)"
v1.10.2,import here to avoid cyclic import
v1.10.2,instantiate base representations if necessary
v1.10.2,there needs to be at least one base
v1.10.2,"while possible, this might be unintended"
v1.10.2,extract shape
v1.10.2,check for invalid base ids
v1.10.2,check for invalid local indices
v1.10.2,assign modules / buffers *after* super init
v1.10.2,docstr-coverage: inherited
v1.10.2,flatten assignment to ease construction of inverse indices
v1.10.2,we group indices by the representation which provides them
v1.10.2,"thus, we need an inverse to restore the correct order"
v1.10.2,get representations
v1.10.2,update inverse indices
v1.10.2,invert flattening
v1.10.2,import here to avoid cyclic import
v1.10.2,comment: not all representations support passing a shape parameter
v1.10.2,create assignment
v1.10.2,base
v1.10.2,other
v1.10.2,import here to avoid cyclic import
v1.10.2,infer shape
v1.10.2,infer max_id
v1.10.2,docstr-coverage: inherited
v1.10.2,"TODO: can be a combined representations, with appropriate tensor-train combination"
v1.10.2,": shape: (max_id, num_cores)"
v1.10.2,": the bases, length: num_cores, with compatible shapes"
v1.10.2,check shape
v1.10.2,check value range
v1.10.2,"do not increase counter i, since the dimension is shared with the following term"
v1.10.2,i += 1
v1.10.2,ids //= m_i
v1.10.2,import here to avoid cyclic import
v1.10.2,normalize ranks
v1.10.2,"determine M_k, N_k"
v1.10.2,TODO: allow to pass them from outside?
v1.10.2,normalize assignment
v1.10.2,determine shapes and einsum equation
v1.10.2,create base representations
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,abstract
v1.10.2,concrete classes
v1.10.2,default flow
v1.10.2,: the message passing layers
v1.10.2,: the flow direction of messages across layers
v1.10.2,": the edge index, shape: (2, num_edges)"
v1.10.2,fail if dependencies are missing
v1.10.2,avoid cyclic import
v1.10.2,"the base representations, e.g., entity embeddings or features"
v1.10.2,verify max_id
v1.10.2,verify shape
v1.10.2,assign sub-module *after* super call
v1.10.2,initialize layers
v1.10.2,normalize activation
v1.10.2,check consistency
v1.10.2,buffer edge index for message passing
v1.10.2,TODO: inductiveness; we need to
v1.10.2,* replace edge_index
v1.10.2,* replace base representations
v1.10.2,* keep layers & activations
v1.10.2,docstr-coverage: inherited
v1.10.2,we can restrict the message passing to the k-hop neighborhood of the desired indices;
v1.10.2,this does only make sense if we do not request *all* indices
v1.10.2,k_hop_subgraph returns:
v1.10.2,(1) the nodes involved in the subgraph
v1.10.2,(2) the filtered edge_index connectivity
v1.10.2,"(3) the mapping from node indices in node_idx to their new location, and"
v1.10.2,(4) the edge mask indicating which edges were preserved
v1.10.2,we only need the base representations for the neighbor indices
v1.10.2,get *all* base representations
v1.10.2,use *all* edges
v1.10.2,perform message passing
v1.10.2,select desired indices
v1.10.2,docstr-coverage: inherited
v1.10.2,": the edge type, shape: (num_edges,)"
v1.10.2,register an additional buffer for the categorical edge type
v1.10.2,docstr-coverage: inherited
v1.10.2,: the relation representations used to obtain initial edge features
v1.10.2,avoid cyclic import
v1.10.2,docstr-coverage: inherited
v1.10.2,get initial relation representations
v1.10.2,select edge attributes from relation representations according to relation type
v1.10.2,perform message passing
v1.10.2,"apply relation transformation, if necessary"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Classes
v1.10.2,Resolver
v1.10.2,backwards compatibility
v1.10.2,scaling factor
v1.10.2,"modulus ~ Uniform[-s, s]"
v1.10.2,"phase ~ Uniform[0, 2*pi]"
v1.10.2,real part
v1.10.2,purely imaginary quaternions unitary
v1.10.2,this is usually loaded from somewhere else
v1.10.2,"the shape must match, as well as the entity-to-id mapping"
v1.10.2,must be cloned if we want to do backprop
v1.10.2,the color initializer
v1.10.2,variants for the edge index
v1.10.2,additional parameters for iter_weisfeiler_lehman
v1.10.2,normalize shape
v1.10.2,get coloring
v1.10.2,make color initializer
v1.10.2,initialize color representations
v1.10.2,note: this could be a representation?
v1.10.2,init entity representations according to the color
v1.10.2,create random walk matrix
v1.10.2,stack diagonal entries of powers of rw
v1.10.2,abstract
v1.10.2,concrete
v1.10.2,docstr-coverage: inherited
v1.10.2,tokenize
v1.10.2,pad
v1.10.2,get character embeddings
v1.10.2,pool
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: whether the edge weighting needs access to the message
v1.10.2,stub init to enable arbitrary arguments in subclasses
v1.10.2,"Calculate in-degree, i.e. number of incoming edges"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,backward compatibility with RGCN
v1.10.2,docstr-coverage: inherited
v1.10.2,view for heads
v1.10.2,"compute attention coefficients, shape: (num_edges, num_heads)"
v1.10.2,"TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Caches
v1.10.2,"if the sparsity becomes too low, convert to a dense matrix"
v1.10.2,"note: this heuristic is based on the memory consumption,"
v1.10.2,"for a sparse matrix, we store 3 values per nnz (row index, column index, value)"
v1.10.2,"performance-wise, it likely makes sense to switch even earlier"
v1.10.2,`torch.sparse.mm` can also deal with dense 2nd argument
v1.10.2,note: torch.sparse.mm only works for COO matrices;
v1.10.2,@ only works for CSR matrices
v1.10.2,"convert to COO, if necessary"
v1.10.2,"we need to use indices here, since there may be zero diagonal entries"
v1.10.2,docstr-coverage: inherited
v1.10.2,: Wikidata SPARQL endpoint. See https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service#Interfacing
v1.10.2,cf. https://meta.wikimedia.org/wiki/User-Agent_policy
v1.10.2,cf. https://wikitech.wikimedia.org/wiki/Robot_policy
v1.10.2,break into smaller requests
v1.10.2,try to load cached first
v1.10.2,determine missing entries
v1.10.2,retrieve information via SPARQL
v1.10.2,save entries
v1.10.2,fill missing descriptions
v1.10.2,for mypy
v1.10.2,get labels & descriptions
v1.10.2,compose labels
v1.10.2,we can have multiple images per entity -> collect image URLs per image
v1.10.2,entity ID
v1.10.2,relation ID
v1.10.2,image URL
v1.10.2,check whether images are still missing
v1.10.2,select on image url per image in a reproducible way
v1.10.2,traverse relations in order of preference
v1.10.2,now there is an image available -> select reproducible by URL sorting
v1.10.2,did not break -> no image
v1.10.2,This import doesn't need a wrapper since it's a transitive
v1.10.2,requirement of PyOBO
v1.10.2,darglint does not like
v1.10.2,"raise cls(shape=shape, reference=reference)"
v1.10.2,1 * ? = ?; ? * 1 = ?
v1.10.2,i**2 = j**2 = k**2 = -1
v1.10.2,i * j = k; i * k = -j
v1.10.2,"j * i = -k, j * k = i"
v1.10.2,k * i = j; k * j = -i
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO test
v1.10.2,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.10.2,: a = \mu^T\Sigma^{-1}\mu
v1.10.2,: b = \log \det \Sigma
v1.10.2,1. Component
v1.10.2,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.10.2,2. Component
v1.10.2,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.10.2,with mu = (mu_1 - mu_0)
v1.10.2,= mu * Sigma_1^-1 mu
v1.10.2,since Sigma_1 is diagonal
v1.10.2,= mu**2 / sigma_1
v1.10.2,3. Component
v1.10.2,4. Component
v1.10.2,ln (det(\Sigma_1) / det(\Sigma_0))
v1.10.2,= ln det Sigma_1 - ln det Sigma_0
v1.10.2,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.10.2,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.10.2,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.10.2,allocate result
v1.10.2,prepare distributions
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO benchmark
v1.10.2,TODO benchmark
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,REPRESENTATION
v1.10.2,base
v1.10.2,concrete
v1.10.2,INITIALIZER
v1.10.2,INTERACTIONS
v1.10.2,Adapter classes
v1.10.2,Concrete Classes
v1.10.2,combinations
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO: split file into multiple smaller ones?
v1.10.2,Base Classes
v1.10.2,Adapter classes
v1.10.2,Concrete Classes
v1.10.2,normalize input
v1.10.2,get number of head/relation/tail representations
v1.10.2,flatten list
v1.10.2,split tensors
v1.10.2,broadcasting
v1.10.2,yield batches
v1.10.2,complex typing
v1.10.2,: The symbolic shapes for entity representations
v1.10.2,": The symbolic shapes for entity representations for tail entities, if different."
v1.10.2,": Otherwise, the entity_shape is used for head & tail entities"
v1.10.2,: The symbolic shapes for relation representations
v1.10.2,if the interaction function's head parameter should only receive a subset of entity representations
v1.10.2,if the interaction function's tail parameter should only receive a subset of entity representations
v1.10.2,: the interaction's value range (for unrestricted input)
v1.10.2,"TODO: annotate modelling capabilities? cf., e.g., https://arxiv.org/abs/1902.10197, Table 2"
v1.10.2,"TODO: annotate properties, e.g., symmetry, and use them for testing?"
v1.10.2,TODO: annotate complexity?
v1.10.2,: whether the interaction is defined on complex input
v1.10.2,"TODO: cannot cover dynamic shapes, e.g., AutoSF"
v1.10.2,"TODO: we could change that to slicing along multiple dimensions, if necessary"
v1.10.2,: The functional interaction form
v1.10.2,docstr-coverage: inherited
v1.10.2,"TODO: we only allow single-tensor representations here, but could easily generalize"
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: update class docstring
v1.10.2,TODO: give this a better name?
v1.10.2,Store initial input for error message
v1.10.2,All are None -> try and make closest to square
v1.10.2,Only input channels is None
v1.10.2,Only width is None
v1.10.2,Only height is none
v1.10.2,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.10.2,Width and input channels are None -> set input channels to 1 and calculate width
v1.10.2,vector & scalar offset
v1.10.2,": The head-relation encoder operating on 2D ""images"""
v1.10.2,: The head-relation encoder operating on the 1D flattened version
v1.10.2,: The interaction function
v1.10.2,Automatic calculation of remaining dimensions
v1.10.2,Parameter need to fulfil:
v1.10.2,input_channels * embedding_height * embedding_width = embedding_dim
v1.10.2,normalize kernel height
v1.10.2,encoders
v1.10.2,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.10.2,"2: 1D encoder: FC, DO, BN?, Act"
v1.10.2,store reshaping dimensions
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,The interaction model
v1.10.2,docstr-coverage: inherited
v1.10.2,Use Xavier initialization for weight; bias to zero
v1.10.2,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.10.2,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.10.2,docstr-coverage: inherited
v1.10.2,normalize hidden_dim
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,Initialize biases with zero
v1.10.2,"In the original formulation,"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: update docstring
v1.10.2,TODO: give this a better name?
v1.10.2,r expresses a rotation in complex plane.
v1.10.2,rotate head by relation (=Hadamard product in complex space)
v1.10.2,rotate tail by inverse of relation
v1.10.2,The inverse rotation is expressed by the complex conjugate of r.
v1.10.2,The score is computed as the distance of the relation-rotated head to the tail.
v1.10.2,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.10.2,|h * r - t| = |h - conj(r) * t|
v1.10.2,Global entity projection
v1.10.2,Global relation projection
v1.10.2,Global combination bias
v1.10.2,Global combination bias
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,default core tensor initialization
v1.10.2,cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12
v1.10.2,normalize initializer
v1.10.2,normalize relation dimension
v1.10.2,Core tensor
v1.10.2,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.10.2,Dropout
v1.10.2,docstr-coverage: inherited
v1.10.2,instantiate here to make module easily serializable
v1.10.2,"batch norm gets reset automatically, since it defines reset_parameters"
v1.10.2,shapes
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,there are separate biases for entities in head and tail position
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,with k=4
v1.10.2,the base interaction
v1.10.2,forward entity/relation shapes
v1.10.2,The parameters of the affine transformation: bias
v1.10.2,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,head position and bump
v1.10.2,relation box: head
v1.10.2,relation box: tail
v1.10.2,tail position and bump
v1.10.2,docstr-coverage: inherited
v1.10.2,compute width plus 1
v1.10.2,compute box midpoints
v1.10.2,"TODO: we already had this before, as `base`"
v1.10.2,inside box?
v1.10.2,yes: |p - c| / (w + 1)
v1.10.2,no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))
v1.10.2,Step 1: Apply the other entity bump
v1.10.2,Step 2: Apply tanh if tanh_map is set to True.
v1.10.2,Compute the distance function output element-wise
v1.10.2,"Finally, compute the norm"
v1.10.2,Enforce that sizes are strictly positive by passing through ELU
v1.10.2,Shape vector is normalized using the above helper function
v1.10.2,Size is learned separately and applied to normalized shape
v1.10.2,Compute potential boundaries by applying the shape in substraction
v1.10.2,and in addition
v1.10.2,Compute box upper bounds using min and max respectively
v1.10.2,head
v1.10.2,relation box: head
v1.10.2,relation box: tail
v1.10.2,tail
v1.10.2,power norm
v1.10.2,the relation-specific head box base shape (normalized to have a volume of 1):
v1.10.2,the relation-specific tail box base shape (normalized to have a volume of 1):
v1.10.2,input normalization
v1.10.2,Core tensor
v1.10.2,docstr-coverage: inherited
v1.10.2,initialize core tensor
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"r_head, r_mid, r_tail"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,type alias for AutoSF block description
v1.10.2,"head_index, relation_index, tail_index, sign"
v1.10.2,: a description of the block structure
v1.10.2,convert to tuple
v1.10.2,infer the number of entity and relation representations
v1.10.2,verify coefficients
v1.10.2,dynamic entity / relation shapes
v1.10.2,docstr-coverage: inherited
v1.10.2,"r_head, r_bias, r_tail"
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,docstr-coverage: excused `wrapped`
v1.10.2,"repeat if necessary, and concat head and relation"
v1.10.2,"shape: -1, num_input_channels, 2*height, width"
v1.10.2,"shape: -1, num_input_channels, 2*height, width"
v1.10.2,"-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.10.2,"reshape: (-1, dim) -> (*batch_dims, dim)"
v1.10.2,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.10.2,output_shape: batch_dims
v1.10.2,add bias term
v1.10.2,"cat into shape (..., 1, d, 3)"
v1.10.2,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.10.2,"Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)"
v1.10.2,shortcut for same shape
v1.10.2,split weight into head-/relation-/tail-specific sub-matrices
v1.10.2,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.10.2,"Predict t embedding, shape: (*batch_dims, d)"
v1.10.2,dot product
v1.10.2,"composite: (*batch_dims, d)"
v1.10.2,inner product with relation embedding
v1.10.2,Circular correlation of entity embeddings
v1.10.2,complex conjugate
v1.10.2,Hadamard product in frequency domain
v1.10.2,inverse real FFT
v1.10.2,global projections
v1.10.2,"combination, shape: (*batch_dims, d)"
v1.10.2,dot product with t
v1.10.2,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.10.2,"That is not mentioned in the paper, so it is made optional here."
v1.10.2,Project entities
v1.10.2,h projection to hyperplane
v1.10.2,r
v1.10.2,-t projection to hyperplane
v1.10.2,project to relation specific subspace
v1.10.2,ensure constraints
v1.10.2,x_1 contraction
v1.10.2,x_2 contraction
v1.10.2,"TODO: this sign is in the official code, too, but why do we need it?"
v1.10.2,head interaction
v1.10.2,relation interaction (notice that h has been updated)
v1.10.2,combination
v1.10.2,similarity
v1.10.2,head
v1.10.2,relation
v1.10.2,tail
v1.10.2,version 2: relation factor offset
v1.10.2,extension: negative (power) norm
v1.10.2,note: normalization should be done from the representations
v1.10.2,cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501
v1.10.2,version 2
v1.10.2,r_head = r_head + u * torch.ones_like(r_head)
v1.10.2,r_tail = r_tail + u * torch.ones_like(r_tail)
v1.10.2,"stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)"
v1.10.2,"remember shape for output, but reshape for transformer"
v1.10.2,"get position embeddings, shape: (seq_len, dim)"
v1.10.2,Now we are position-dependent w.r.t qualifier pairs.
v1.10.2,"seq_length, batch_size, dim"
v1.10.2,Pool output
v1.10.2,"output shape: (batch_size, dim)"
v1.10.2,reshape
v1.10.2,head
v1.10.2,relation
v1.10.2,tail
v1.10.2,extension: negative (power) norm
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Concrete classes
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,input normalization
v1.10.2,instantiate separate combinations
v1.10.2,docstr-coverage: inherited
v1.10.2,split complex; repeat real
v1.10.2,separately combine real and imaginary parts
v1.10.2,combine
v1.10.2,docstr-coverage: inherited
v1.10.2,symbolic output to avoid dtype issue
v1.10.2,we only need to consider real part here
v1.10.2,the gate
v1.10.2,the combination
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Resolver
v1.10.2,Base classes
v1.10.2,Concrete classes
v1.10.2,TODO: allow relative
v1.10.2,isin() preserves the sorted order
v1.10.2,docstr-coverage: inherited
v1.10.2,sort by decreasing degree
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,sort by decreasing page rank
v1.10.2,docstr-coverage: inherited
v1.10.2,input normalization
v1.10.2,determine absolute number of anchors for each strategy
v1.10.2,if pre-instantiated
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: the token ID of the padding token
v1.10.2,: the token representations
v1.10.2,: the assigned tokens for each entity
v1.10.2,needs to be lazily imported to avoid cyclic imports
v1.10.2,fill padding (nn.Embedding cannot deal with negative indices)
v1.10.2,"sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs"
v1.10.2,"contain a subset of training relations) - for that, the padding index is the last index of the Representation"
v1.10.2,resolve token representation
v1.10.2,input validation
v1.10.2,register as buffer
v1.10.2,assign sub-module
v1.10.2,apply tokenizer
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"get token IDs, shape: (*, num_chosen_tokens)"
v1.10.2,"lookup token representations, shape: (*, num_chosen_tokens, *shape)"
v1.10.2,": A list with ratios per representation in their creation order,"
v1.10.2,": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`"
v1.10.2,": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95"
v1.10.2,normalize triples
v1.10.2,inverse triples are created afterwards implicitly
v1.10.2,tokenize
v1.10.2,Create an MLP for string aggregation
v1.10.2,note: the token representations' shape includes the number of tokens as leading dim
v1.10.2,unique hashes per representation
v1.10.2,unique hashes if we concatenate all representations together
v1.10.2,TODO: vectorization?
v1.10.2,remove self-loops
v1.10.2,add inverse edges and remove duplicates
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Resolver
v1.10.2,Base classes
v1.10.2,Concrete classes
v1.10.2,docstr-coverage: inherited
v1.10.2,tokenize: represent entities by bag of relations
v1.10.2,collect candidates
v1.10.2,randomly sample without replacement num_tokens relations for each entity
v1.10.2,TODO: expose num_anchors?
v1.10.2,select anchors
v1.10.2,find closest anchors
v1.10.2,convert to torch
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"To prevent possible segfaults in the METIS C code, METIS expects a graph"
v1.10.2,(1) without self-loops; (2) with inverse edges added; (3) with unique edges only
v1.10.2,https://github.com/KarypisLab/METIS/blob/94c03a6e2d1860128c2d0675cbbb86ad4f261256/libmetis/checkgraph.c#L18
v1.10.2,select independently per partition
v1.10.2,select adjacency part;
v1.10.2,"note: the indices will automatically be in [0, ..., high - low), since they are *local* indices"
v1.10.2,offset
v1.10.2,the -1 comes from the shared padding token
v1.10.2,note: permutation will be later on reverted
v1.10.2,add back 1 for the shared padding token
v1.10.2,TODO: check if perm is used correctly
v1.10.2,verify pool
v1.10.2,docstr-coverage: inherited
v1.10.2,choose first num_tokens
v1.10.2,TODO: vectorization?
v1.10.2,heuristic
v1.10.2,heuristic
v1.10.2,calculate configuration digest
v1.10.2,create anchor selection instance
v1.10.2,select anchors
v1.10.2,anchor search (=anchor assignment?)
v1.10.2,assign anchors
v1.10.2,save
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Resolver
v1.10.2,Base classes
v1.10.2,Concrete classes
v1.10.2,docstr-coverage: inherited
v1.10.2,"contains: anchor_ids, entity_ids, mapping {entity_id -> {""ancs"": anchors, ""dists"": distances}}"
v1.10.2,normalize anchor_ids
v1.10.2,cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541
v1.10.2,TODO: keep distances?
v1.10.2,ensure parent directory exists
v1.10.2,save via torch.save
v1.10.2,docstr-coverage: inherited
v1.10.2,"TODO: since we save a contiguous array of (num_entities, num_anchors),"
v1.10.2,"it would be more efficient to not convert to a mapping, but directly select from the tensor"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Anchor Searchers
v1.10.2,Anchor Selection
v1.10.2,Tokenizers
v1.10.2,Token Loaders
v1.10.2,Representations
v1.10.2,Data containers
v1.10.2,"TODO: use graph library, such as igraph, graph-tool, or networkit"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Resolver
v1.10.2,Base classes
v1.10.2,Concrete classes
v1.10.2,"this array contains the indices of the k closest anchors nodes, but without guarantee that they are sorted"
v1.10.2,"now we want to sort these top-k entries, (O(k log k)) (and only those)"
v1.10.2,docstr-coverage: inherited
v1.10.2,convert to adjacency matrix
v1.10.2,convert to scipy sparse csr
v1.10.2,"compute distances between anchors and all nodes, shape: (num_anchors, num_entities)"
v1.10.2,TODO: padding for unreachable?
v1.10.2,docstr-coverage: inherited
v1.10.2,infer shape
v1.10.2,create adjacency matrix
v1.10.2,symmetric + self-loops
v1.10.2,"for each entity, determine anchor pool by BFS"
v1.10.2,an array storing whether node i is reachable by anchor j
v1.10.2,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.10.2,the output
v1.10.2,anchor nodes have themselves as a starting found anchor
v1.10.2,TODO: take all (q-1) hop neighbors before selecting from q-hop
v1.10.2,propagate one hop
v1.10.2,convergence check
v1.10.2,copy pool if we have seen enough anchors and have not yet stopped
v1.10.2,stop once we have enough
v1.10.2,TODO: can we replace this loop with something vectorized?
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,symmetric + self-loops
v1.10.2,"for each entity, determine anchor pool by BFS"
v1.10.2,an array storing whether node i is reachable by anchor j
v1.10.2,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.10.2,the output that track the distance to each found anchor
v1.10.2,"dtype is unsigned int 8 bit, so we initialize the maximum distance to 255 (or max default)"
v1.10.2,initial anchors are 0-hop away from themselves
v1.10.2,propagate one hop
v1.10.2,TODO the float() trick for GPU result stability until the torch_sparse issue is resolved
v1.10.2,https://github.com/rusty1s/pytorch_sparse/issues/243
v1.10.2,convergence check
v1.10.2,newly reached is a mask that points to newly discovered anchors at this particular step
v1.10.2,implemented as element-wise XOR (will only give True in 0 XOR 1 or 1 XOR 0)
v1.10.2,"in our case we enrich the set of found anchors, so we can only have values turning 0 to 1, eg 0 XOR 1"
v1.10.2,copy pool if we have seen enough anchors and have not yet stopped
v1.10.2,"update the value in the pool by the current hop value (we start from 0, so +1 be default)"
v1.10.2,stop once we have enough
v1.10.2,sort the pool by nearest to farthest anchors
v1.10.2,values with distance 255 (or max for unsigned int8 type) are padding tokens
v1.10.2,"since the output is sorted, no need for random sampling, we just take top-k nearest"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"select k anchors with largest ppr, shape: (batch_size, k)"
v1.10.2,prepare adjacency matrix only once
v1.10.2,prepare result
v1.10.2,progress bar?
v1.10.2,batch-wise computation of PPR
v1.10.2,"run page-rank calculation, shape: (batch_size, n)"
v1.10.2,"select PPR values for the anchors, shape: (batch_size, num_anchors)"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Base classes
v1.10.2,Concrete classes
v1.10.2,
v1.10.2,
v1.10.2,
v1.10.2,
v1.10.2,
v1.10.2,Misc
v1.10.2,
v1.10.2,rank based metrics do not need binarized scores
v1.10.2,: the supported rank types. Most of the time equal to all rank types
v1.10.2,: whether the metric requires the number of candidates for each ranking task
v1.10.2,normalize confidence level
v1.10.2,sample metric values
v1.10.2,"bootstrap estimator (i.e., compute on sample with replacement)"
v1.10.2,cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function
v1.10.2,: The rank-based metric class that this derived metric extends
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"since scale and offset are constant for a given number of candidates, we have"
v1.10.2,E[scale * M + offset] = scale * E[M] + offset
v1.10.2,docstr-coverage: inherited
v1.10.2,"since scale and offset are constant for a given number of candidates, we have"
v1.10.2,V[scale * M + offset] = scale^2 * V[M]
v1.10.2,: Z-adjusted metrics are formulated to be increasing
v1.10.2,: Z-adjusted metrics can only be applied to realistic ranks
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,should be exactly 0.0
v1.10.2,docstr-coverage: inherited
v1.10.2,should be exactly 1.0
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,: Expectation/maximum reindexed metrics are formulated to be increasing
v1.10.2,: Expectation/maximum reindexed metrics can only be applied to realistic ranks
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,should be exactly 0.0
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2
v1.10.2,use V[x] = E[x^2] - E[x]^2
v1.10.2,group by same weight -> compute H_w(n) for multiple n at once
v1.10.2,we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once
v1.10.2,now select from precomputed cumulative sums and aggregate
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective"
v1.10.2,cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: should we return the sum of weights?
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.10.2,docstr-coverage: inherited
v1.10.2,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: the lower bound
v1.10.2,: whether the lower bound is inclusive
v1.10.2,: the upper bound
v1.10.2,: whether the upper bound is inclusive
v1.10.2,: The name of the metric
v1.10.2,: a link to further information
v1.10.2,: whether the metric needs binarized scores
v1.10.2,": whether it is increasing, i.e., larger values are better"
v1.10.2,: the value range
v1.10.2,: synonyms for this metric
v1.10.2,: whether the metric supports weights
v1.10.2,: whether there is a closed-form solution of the expectation
v1.10.2,: whether there is a closed-form solution of the variance
v1.10.2,normalize weights
v1.10.2,calculate weighted harmonic mean
v1.10.2,calculate cdf
v1.10.2,determine value at p=0.5
v1.10.2,special case for exactly 0.5
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,see also: https://cran.r-project.org/web/packages/metrica/vignettes/available_metrics_classification.html
v1.10.2,todo: do we need numpy support?
v1.10.2,TODO: re-consider threshold
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: can we directly include sklearn's docstring here?
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,todo: it would make sense to have a separate evaluator which constructs the confusion matrix only once
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,todo: https://en.wikipedia.org/wiki/Diagnostic_odds_ratio#Confidence_interval
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,todo: improve doc
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,don't worry about functions because they can't be specified by JSON.
v1.10.2,Could make a better mo
v1.10.2,later could extend for other non-JSON valid types
v1.10.2,todo: read from config instead
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Score with original triples
v1.10.2,Score with inverse triples
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,noqa:DAR101
v1.10.2,noqa:DAR401
v1.10.2,Create directory in which all experimental artifacts are saved
v1.10.2,noqa:DAR101
v1.10.2,clip for node piece configurations
v1.10.2,"""pykeen experiments reproduce"" expects ""model reference dataset"""
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO: take care that triples aren't removed that are the only ones with any given entity
v1.10.2,distribute the deteriorated triples across the remaining factories
v1.10.2,"'kinships',"
v1.10.2,"'umls',"
v1.10.2,"'codexsmall',"
v1.10.2,"'wn18',"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: Functions for specifying exotic resources with a given prefix
v1.10.2,: Functions for specifying exotic resources based on their file extension
v1.10.2,Input validation
v1.10.2,convert to numpy
v1.10.2,Additional columns
v1.10.2,convert PyTorch tensors to numpy
v1.10.2,convert to dataframe
v1.10.2,Re-order columns
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.10.2,TODO vectorize code
v1.10.2,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,save literal-to-id mapping
v1.10.2,save numeric literals
v1.10.2,load literal-to-id
v1.10.2,load literals
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Split triples
v1.10.2,Sorting ensures consistent results when the triples are permuted
v1.10.2,Create mapping
v1.10.2,Sorting ensures consistent results when the triples are permuted
v1.10.2,Create mapping
v1.10.2,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.10.2,Filter all non-existent triples
v1.10.2,Note: Unique changes the order of the triples
v1.10.2,Note: Using unique means implicit balancing of training samples
v1.10.2,normalize input
v1.10.2,: The mapping from labels to IDs.
v1.10.2,: The inverse mapping for label_to_id; initialized automatically
v1.10.2,: A vectorized version of entity_label_to_id; initialized automatically
v1.10.2,: A vectorized version of entity_id_to_label; initialized automatically
v1.10.2,Normalize input
v1.10.2,label
v1.10.2,Filter for entities
v1.10.2,Filter for relations
v1.10.2,No filter
v1.10.2,: the number of unique entities
v1.10.2,": the number of relations (maybe including ""artificial"" inverse relations)"
v1.10.2,: whether to create inverse triples
v1.10.2,": the number of real relations, i.e., without artificial inverses"
v1.10.2,ensure torch.Tensor
v1.10.2,input validation
v1.10.2,"always store as torch.long, i.e., torch's default integer dtype"
v1.10.2,check new label to ID mappings
v1.10.2,Make new triples factories for each group
v1.10.2,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.2,prepare metadata
v1.10.2,Delegate to function
v1.10.2,"restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed"
v1.10.2,docstr-coverage: inherited
v1.10.2,load base
v1.10.2,load numeric triples
v1.10.2,store numeric triples
v1.10.2,store metadata
v1.10.2,note: num_relations will be doubled again when instantiating with create_inverse_triples=True
v1.10.2,Check if the triples are inverted already
v1.10.2,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.10.2,contained if and only if create_inverse_triples is True.
v1.10.2,Generate entity mapping if necessary
v1.10.2,Generate relation mapping if necessary
v1.10.2,Map triples of labels to triples of IDs.
v1.10.2,TODO: Check if lazy evaluation would make sense
v1.10.2,docstr-coverage: inherited
v1.10.2,store entity/relation to ID
v1.10.2,load entity/relation to ID
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,pre-filter to keep only topk
v1.10.2,if top is larger than the number of available options
v1.10.2,Generate a word cloud image
v1.10.2,docstr-coverage: inherited
v1.10.2,vectorized label lookup
v1.10.2,Re-order columns
v1.10.2,docstr-coverage: inherited
v1.10.2,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.10.2,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.10.2,ID-based triples
v1.10.2,labeled triples
v1.10.2,make sure triples are a numpy array
v1.10.2,make sure triples are 2d
v1.10.2,convert to ID-based
v1.10.2,triples factory
v1.10.2,all keyword-based options have been none
v1.10.2,delegate to keyword-based get_mapped_triples to re-use optional validation logic
v1.10.2,delegate to keyword-based get_mapped_triples to re-use optional validation logic
v1.10.2,only labeled triples are remaining
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Split indices
v1.10.2,Split triples
v1.10.2,select one triple per relation
v1.10.2,maintain set of covered entities
v1.10.2,"Select one triple for each head/tail entity, which is not yet covered."
v1.10.2,create mask
v1.10.2,Prepare split index
v1.10.2,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.10.2,base cases
v1.10.2,IDs not in training
v1.10.2,triples with exclusive test IDs
v1.10.2,docstr-coverage: inherited
v1.10.2,While there are still triples that should be moved to the training set
v1.10.2,Pick a random triple to move over to the training triples
v1.10.2,add to training
v1.10.2,remove from testing
v1.10.2,Recalculate the move_id_mask
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,Make sure that the first element has all the right stuff in it
v1.10.2,docstr-coverage: inherited
v1.10.2,backwards compatibility
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,constants
v1.10.2,constants
v1.10.2,unary
v1.10.2,binary
v1.10.2,ternary
v1.10.2,column names
v1.10.2,return candidates
v1.10.2,index triples
v1.10.2,incoming relations per entity
v1.10.2,outgoing relations per entity
v1.10.2,indexing triples for fast join r1 & r2
v1.10.2,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.10.2,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.10.2,actual evaluation of the pattern
v1.10.2,skip empty support
v1.10.2,TODO: Can this happen after pre-filtering?
v1.10.2,"sort first, for triple order invariance"
v1.10.2,TODO: what is the support?
v1.10.2,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.10.2,sort decreasingly. i dominates j for all j > i in x-dimension
v1.10.2,"if it is also dominated by any y, it is not part of the skyline"
v1.10.2,"group by (relation id, pattern type)"
v1.10.2,"for each group, yield from skyline"
v1.10.2,determine patterns from triples
v1.10.2,drop zero-confidence
v1.10.2,keep only skyline
v1.10.2,create data frame
v1.10.2,iterate relation types
v1.10.2,drop zero-confidence
v1.10.2,keep only skyline
v1.10.2,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.10.2,base = skyline(base)
v1.10.2,create data frame
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO: the same
v1.10.2,": the positive triples, shape: (batch_size, 3)"
v1.10.2,": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)"
v1.10.2,": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)"
v1.10.2,noqa:DAR202
v1.10.2,noqa:DAR401
v1.10.2,TODO: some negative samplers require batches
v1.10.2,"shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.10.2,"each shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset
v1.10.2,docstr-coverage: inherited
v1.10.2,indexing
v1.10.2,initialize
v1.10.2,sample iteratively
v1.10.2,determine weights
v1.10.2,randomly choose a vertex which has not been chosen yet
v1.10.2,normalize to probabilities
v1.10.2,sample a start node
v1.10.2,get list of neighbors
v1.10.2,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.10.2,visit target node
v1.10.2,decrease sample counts
v1.10.2,docstr-coverage: inherited
v1.10.2,convert to csr for fast row slicing
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,safe division for empty sets
v1.10.2,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.10.2,duplicates
v1.10.2,we are not interested in self-similarity
v1.10.2,compute similarities
v1.10.2,Calculate which relations are the inverse ones
v1.10.2,get existing IDs
v1.10.2,remove non-existing ID from label mapping
v1.10.2,create translation tensor
v1.10.2,get entities and relations occurring in triples
v1.10.2,generate ID translation and new label to Id mappings
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.10.2,seamless loading and saving of training checkpoints
v1.10.2,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.10.2,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.10.2,"If a checkpoint file is given, it must be loaded if it exists already"
v1.10.2,"If the stopper dict has any keys, those are written back to the stopper"
v1.10.2,The checkpoint frequency needs to be set to save checkpoints
v1.10.2,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.10.2,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.10.2,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.10.2,send model to device before going into the internal training loop
v1.10.2,the exit stack ensure that we clean up temporary files when an error occurs
v1.10.2,"When using early stopping models have to be saved separately at the best epoch, since the training"
v1.10.2,loop will due to the patience continue to train after the best epoch and thus alter the model
v1.10.2,note: NamedTemporaryFile does not seem to work
v1.10.2,Create a path
v1.10.2,Ensure the release of memory
v1.10.2,Clear optimizer
v1.10.2,Accumulate loss over epoch
v1.10.2,Flag to check when to quit the size probing
v1.10.2,apply callbacks before starting with batch
v1.10.2,Get batch size of current batch (last batch may be incomplete)
v1.10.2,accumulate gradients for whole batch
v1.10.2,forward pass call
v1.10.2,For testing purposes we're only interested in processing one batch
v1.10.2,"note: this epoch loss can be slightly biased towards the last batch, if this is smaller than the rest"
v1.10.2,"in practice, this should have a minor effect, since typically batch_size << num_instances"
v1.10.2,TODO: is this necessary?
v1.10.2,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.10.2,due to the patience continue to train after the best epoch and thus alter the model
v1.10.2,"-> the temporay file has to be created outside, which we assert here"
v1.10.2,Prepare all of the callbacks
v1.10.2,"Register a callback for the result tracker, if given"
v1.10.2,"Register a callback for the early stopper, if given"
v1.10.2,TODO should mode be passed here?
v1.10.2,"Take the biggest possible training batch_size, if batch_size not set"
v1.10.2,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.10.2,This will find necessary parameters to optimize the use of the hardware at hand
v1.10.2,return the relevant parameters slice_size and batch_size
v1.10.2,Force weight initialization if training continuation is not explicitly requested.
v1.10.2,Reset the weights
v1.10.2,"afterwards, some parameters may be on the wrong device"
v1.10.2,Create new optimizer
v1.10.2,Create a new lr scheduler and add the optimizer
v1.10.2,Ensure the model is on the correct device
v1.10.2,"When size probing, we don't want progress bars"
v1.10.2,Create progress bar
v1.10.2,optimizer callbacks
v1.10.2,Save the time to track when the saved point was available
v1.10.2,Training Loop
v1.10.2,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.10.2,Enforce training mode
v1.10.2,Batching
v1.10.2,Only create a progress bar when not in size probing mode
v1.10.2,When size probing we don't need the losses
v1.10.2,Track epoch loss
v1.10.2,Print loss information to console
v1.10.2,Save the last successful finished epoch
v1.10.2,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.10.2,During automatic memory optimization only the error message is of interest
v1.10.2,When there wasn't a best epoch the checkpoint path should be None
v1.10.2,Delete temporary best epoch model
v1.10.2,Includes a call to result_tracker.log_metrics
v1.10.2,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.10.2,MyPy overrides are because you should
v1.10.2,When there wasn't a best epoch the checkpoint path should be None
v1.10.2,Delete temporary best epoch model
v1.10.2,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.10.2,at that state
v1.10.2,Delete temporary best epoch model
v1.10.2,forward pass
v1.10.2,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.10.2,correction for loss reduction
v1.10.2,backward pass
v1.10.2,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.10.2,Set upper bound
v1.10.2,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.10.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.2,"Only if a cuda device is available, the random state is accessed"
v1.10.2,This is an entire checkpoint for the optional best model when using early stopping
v1.10.2,Saving triples factory related states
v1.10.2,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.10.2,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.10.2,Check whether the triples factory mappings match those from the checkpoints
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,docstr-coverage: inherited
v1.10.2,disable automatic batching
v1.10.2,docstr-coverage: inherited
v1.10.2,Slicing is not possible in sLCWA training loops
v1.10.2,split batch
v1.10.2,send to device
v1.10.2,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.10.2,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.10.2,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.10.2,Compute negative and positive scores
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,Slicing is not possible for sLCWA
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,lazy init
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO how to pass inductive mode
v1.10.2,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.10.2,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.10.2,TODO: we may want to separate TrainingCallback from pre-step callbacks in the future
v1.10.2,docstr-coverage: inherited
v1.10.2,Recall that torch *accumulates* gradients. Before passing in a
v1.10.2,"new instance, you need to zero out the gradients from the old instance"
v1.10.2,note: we want to run this step during size probing to cleanup any remaining grads
v1.10.2,docstr-coverage: inherited
v1.10.2,pre-step callbacks
v1.10.2,"when called by batch_size_search(), the parameter update should not be applied."
v1.10.2,update parameters according to optimizer
v1.10.2,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.10.2,constraints are no longer applied
v1.10.2,"note: we want to apply this during size probing to properly account for the memory necessary for e.g.,"
v1.10.2,regularization
v1.10.2,docstr-coverage: inherited
v1.10.2,do not share optimal parameters across different training loops
v1.10.2,todo: create dataset only once
v1.10.2,"no sub-batching (for evaluation, we can just reduce batch size without any effect)"
v1.10.2,this is handled by the AMO wrapper
v1.10.2,no backward passes
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,set to evaluation mode
v1.10.2,determine maximum batch size
v1.10.2,try to avoid OOM kills on cpu for large datasets
v1.10.2,"TODO: this should be num_instances rather than num_triples; also for cpu, we may want to reduce this"
v1.10.2,note: slicing is only effective for LCWA training
v1.10.2,: A hint for constructing a :class:`MultiTrainingCallback`
v1.10.2,: A collection of callbacks
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,normalize target column
v1.10.2,The type inference is so confusing between the function switching
v1.10.2,and polymorphism introduced by slicability that these need to be ignored
v1.10.2,Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number
v1.10.2,of total entities from another inductive inference factory
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,Split batch components
v1.10.2,Send batch to device
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.10.2,"must have failed to start slice_size search, we start with trying half the entities."
v1.10.2,"note: we use Tuple[Tensor] here, so we can re-use TensorDataset instead of having to create a custom one"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,unpack
v1.10.2,Send batch to device
v1.10.2,head prediction
v1.10.2,TODO: exploit sparsity
v1.10.2,"note: this is different to what we do for LCWA, where we collect *all* training entities"
v1.10.2,for which the combination is true
v1.10.2,tail prediction
v1.10.2,TODO: exploit sparsity
v1.10.2,regularization
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO?
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,To make MyPy happy
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: the number of reported results with no improvement after which training will be stopped
v1.10.2,the minimum relative improvement necessary to consider it an improved result
v1.10.2,"whether a larger value is better, or a smaller."
v1.10.2,: The epoch at which the best result occurred
v1.10.2,: The best result so far
v1.10.2,: The remaining patience
v1.10.2,check for improvement
v1.10.2,stop if the result did not improve more than delta for patience evaluations
v1.10.2,: The model
v1.10.2,: The evaluator
v1.10.2,: The triples to use for training (to be used during filtered evaluation)
v1.10.2,: The triples to use for evaluation
v1.10.2,: Size of the evaluation batches
v1.10.2,: Slice size of the evaluation batches
v1.10.2,: The number of epochs after which the model is evaluated on validation set
v1.10.2,: The number of iterations (one iteration can correspond to various epochs)
v1.10.2,: with no improvement after which training will be stopped.
v1.10.2,: The name of the metric to use
v1.10.2,: The minimum relative improvement necessary to consider it an improved result
v1.10.2,: The metric results from all evaluations
v1.10.2,": Whether a larger value is better, or a smaller"
v1.10.2,: The result tracker
v1.10.2,: Callbacks when after results are calculated
v1.10.2,: Callbacks when training gets continued
v1.10.2,: Callbacks when training is stopped early
v1.10.2,: Did the stopper ever decide to stop?
v1.10.2,: the path to the weights of the best model
v1.10.2,: whether to delete the file with the best model weights after termination
v1.10.2,: note: the weights will be re-loaded into the model before
v1.10.2,TODO: Fix this
v1.10.2,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.10.2,raise ValueError(f'Invalid metric name: {self.metric}')
v1.10.2,for mypy
v1.10.2,Evaluate
v1.10.2,Only perform time-consuming checks for the first call.
v1.10.2,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.10.2,Append to history
v1.10.2,TODO need a test that this all re-instantiates properly
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Utils
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,dataset
v1.10.2,model
v1.10.2,stored outside of the training loop / optimizer to give access to auto-tuning from Lightning
v1.10.2,optimizer
v1.10.2,"TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA"
v1.10.2,docstr-coverage: inherited
v1.10.2,call post_parameter_update
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: sub-batching / slicing
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO:
v1.10.2,"shuffle=shuffle,"
v1.10.2,"drop_last=drop_last,"
v1.10.2,"sampler=sampler,"
v1.10.2,"shuffle=shuffle,"
v1.10.2,disable automatic batching in data loader
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: sub-batching / slicing
v1.10.2,docstr-coverage: inherited
v1.10.2,"note: since this file is executed via __main__, its module name is replaced by __name__"
v1.10.2,"hence, the two classes' fully qualified names start with ""_"" and are considered private"
v1.10.2,cf. https://github.com/cthoyt/class-resolver/issues/39
v1.10.2,automatically choose accelerator
v1.10.2,defaults to TensorBoard; explicitly disabled here
v1.10.2,disable checkpointing
v1.10.2,mixed precision training
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,docstr-coverage: inherited
v1.10.2,side?.metric
v1.10.2,individual side
v1.10.2,"Because the order of the values of a dictionary is not guaranteed,"
v1.10.2,we need to retrieve scores and masks using the exact same key order.
v1.10.2,combined
v1.10.2,docstr-coverage: inherited
v1.10.2,Transfer to cpu and convert to numpy
v1.10.2,Ensure that each key gets counted only once
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The optimistic rank is the rank when assuming all options with an equal score are placed
v1.10.2,: behind the current test triple.
v1.10.2,": shape: (batch_size,)"
v1.10.2,": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank"
v1.10.2,: over all permutations of the elements with the same score as the currently considered option.
v1.10.2,": shape: (batch_size,)"
v1.10.2,: The pessimistic rank is the rank when assuming all options with an equal score are placed
v1.10.2,: in front of current test triple.
v1.10.2,": shape: (batch_size,)"
v1.10.2,: The number of options is the number of items considered in the ranking. It may change for
v1.10.2,: filtered evaluation
v1.10.2,": shape: (batch_size,)"
v1.10.2,The optimistic rank is the rank when assuming all options with an
v1.10.2,"equal score are placed behind the currently considered. Hence, the"
v1.10.2,"rank is the number of options with better scores, plus one, as the"
v1.10.2,rank is one-based.
v1.10.2,The pessimistic rank is the rank when assuming all options with an
v1.10.2,"equal score are placed in front of the currently considered. Hence,"
v1.10.2,the rank is the number of options which have at least the same score
v1.10.2,minus one (as the currently considered option in included in all
v1.10.2,"options). As the rank is one-based, we have to add 1, which nullifies"
v1.10.2,"the ""minus 1"" from before."
v1.10.2,The realistic rank is the average of the optimistic and pessimistic
v1.10.2,"rank, and hence the expected rank over all permutations of the elements"
v1.10.2,with the same score as the currently considered option.
v1.10.2,"We set values which should be ignored to NaN, hence the number of options"
v1.10.2,which should be considered is given by
v1.10.2,": the scores of the true choice, shape: (*bs), dtype: float"
v1.10.2,": the number of scores which were larger than the true score, shape: (*bs), dtype: long"
v1.10.2,": the number of scores which were not smaller than the true score, shape: (*bs), dtype: long"
v1.10.2,": the total number of compared scores, shape: (*bs), dtype: long"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO: maybe move into separate module?
v1.10.2,actual type: nested dictionary with string keys
v1.10.2,"assert isinstance(one_key, NamedTuple)"
v1.10.2,verify that the triples have been filtered
v1.10.2,Filter triples if necessary
v1.10.2,Prepare for result filtering
v1.10.2,Ensure evaluation mode
v1.10.2,Send model & tensors to device
v1.10.2,no batch size -> automatic memory optimization
v1.10.2,no slice size -> automatic memory optimization
v1.10.2,Show progressbar
v1.10.2,note: we provide the *maximum* batch and slice size here; it is reduced if necessary
v1.10.2,kwargs
v1.10.2,"if inverse triples are used, we only do score_t (TODO: by default; can this be changed?)"
v1.10.2,"otherwise, i.e., without inverse triples, we also need score_h"
v1.10.2,"if relations are to be predicted, we need to slice score_r"
v1.10.2,"raise an error, if any of the required methods cannot slice"
v1.10.2,todo: maybe we want to have some more keys outside of kwargs for hashing / have more visibility about
v1.10.2,what is passed around
v1.10.2,clear evaluator and reset progress bar (necessary for size-probing / evaluation fallback)
v1.10.2,batch-wise processing
v1.10.2,the relation_filter can be re-used (per batch) when we evaluate head *and* tail predictions
v1.10.2,"(which is the standard setting), cf. create_sparse_positive_filter_"
v1.10.2,update progress bar with actual batch size
v1.10.2,Split batch
v1.10.2,Bind shape
v1.10.2,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.10.2,Warn if all entities will be filtered
v1.10.2,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.10.2,Create filter
v1.10.2,Select scores of true
v1.10.2,overwrite filtered scores
v1.10.2,The scores for the true triples have to be rewritten to the scores tensor
v1.10.2,the rank-based evaluators needs the true scores with trailing 1-dim
v1.10.2,Create a positive mask with the size of the scores from the positive filter
v1.10.2,Restrict to entities of interest
v1.10.2,process scores
v1.10.2,optionally restrict triples (nop if no restriction)
v1.10.2,evaluation triples as dataframe
v1.10.2,determine filter triples
v1.10.2,infer num_entities if not given
v1.10.2,"TODO: unique, or max ID + 1?"
v1.10.2,optionally restrict triples
v1.10.2,compute candidate set sizes for different targets
v1.10.2,TODO: extend to relations?
v1.10.2,avoid cyclic imports
v1.10.2,normalize keys
v1.10.2,TODO: find a better way to handle this
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Evaluation loops
v1.10.2,Evaluation datasets
v1.10.2,batch
v1.10.2,tqdm
v1.10.2,data loader
v1.10.2,set upper limit of batch size for automatic memory optimization
v1.10.2,set model to evaluation mode
v1.10.2,delegate to AMO wrapper
v1.10.2,"The key-id for each triple, shape: (num_triples,)"
v1.10.2,": the number of targets for each key, shape: (num_unique_keys + 1,)"
v1.10.2,: the concatenation of unique targets for each key (use bounds to select appropriate sub-array)
v1.10.2,input verification
v1.10.2,group key = everything except the prediction target
v1.10.2,initialize data structure
v1.10.2,group by key
v1.10.2,convert lists to arrays
v1.10.2,instantiate
v1.10.2,return indices corresponding to the `item`-th triple
v1.10.2,input normalization
v1.10.2,prepare filter indices if required
v1.10.2,sorted by target -> most of the batches only have a single target
v1.10.2,group by target
v1.10.2,stack groups into a single tensor
v1.10.2,avoid cyclic imports
v1.10.2,TODO: it would be better to allow separate batch sizes for entity/relation prediction
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"note: most of the time, this loop will only make a single iteration, since the evaluation dataset typically is"
v1.10.2,"not shuffled, and contains evaluation ranking tasks sorted by target"
v1.10.2,"TODO: in theory, we could make a single score calculation for e.g.,"
v1.10.2,"{(h, r, t1), (h, r, t1), ..., (h, r, tk)}"
v1.10.2,predict scores for all candidates
v1.10.2,filter scores
v1.10.2,extract true scores
v1.10.2,replace by nan
v1.10.2,rewrite true scores
v1.10.2,create dense positive masks
v1.10.2,"TODO: afaik, dense positive masks are not used on GPU -> we do not need to move the masks around"
v1.10.2,delegate processing of scores to the evaluator
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,docstr-coverage: inherited
v1.10.2,delay declaration
v1.10.2,"note: OGB's evaluator needs a dataset name as input, and uses it to lookup the standard evaluation"
v1.10.2,metric. we do want to support user-selected metrics on arbitrary datasets instead
v1.10.2,"this setting is equivalent to the WikiKG2 setting, and will calculate MRR *and* H@k for k in {1, 3, 10}"
v1.10.2,check targets
v1.10.2,filter supported metrics
v1.10.2,"prepare input format, cf. `evaluator.expected_input``"
v1.10.2,"y_pred_pos: shape: (num_edge,)"
v1.10.2,"y_pred_neg: shape: (num_edge, num_nodes_neg)"
v1.10.2,move tensor to device
v1.10.2,iterate over prediction targets
v1.10.2,cf. https://github.com/snap-stanford/ogb/pull/357
v1.10.2,combine to input dictionary
v1.10.2,delegate to OGB evaluator
v1.10.2,post-processing
v1.10.2,normalize name
v1.10.2,OGB does not aggregate values across triples
v1.10.2,todo: maybe we can merge this code with the AMO code of the base evaluator?
v1.10.2,pre-allocate
v1.10.2,TODO: maybe we want to collect scores on CPU / add an option?
v1.10.2,iterate over batches
v1.10.2,"combine ids, shape: (batch_size, num_negatives + 1)"
v1.10.2,"get scores, shape: (batch_size, num_negatives + 1)"
v1.10.2,store positive and negative scores
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,flatten dictionaries
v1.10.2,individual side
v1.10.2,combined
v1.10.2,parsing metrics
v1.10.2,metric pattern = side?.type?.metric.k?
v1.10.2,normalize metric name
v1.10.2,normalize side
v1.10.2,normalize rank type
v1.10.2,ensure that rank-opt <= rank-pess
v1.10.2,assert that rank-real = (opt + pess)/2
v1.10.2,fixme: the annotation of ClassResolver.__iter__ seems to be broken (X instead of Type[X])
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,repeat
v1.10.2,default for inductive LP by [teru2020]
v1.10.2,verify input
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: do not require to compute all scores beforehand
v1.10.2,cf. Model.score_t(ts=...)
v1.10.2,super.evaluation assumes that the true scores are part of all_scores
v1.10.2,write back correct num_entities
v1.10.2,TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?
v1.10.2,combine key batches
v1.10.2,calculate key frequency
v1.10.2,weight = inverse frequency
v1.10.2,broadcast to samples
v1.10.2,docstr-coverage: inherited
v1.10.2,store keys for calculating macro weights
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,compute macro weights
v1.10.2,note: we wrap the array into a list to be able to re-use _iter_ranks
v1.10.2,calculate weighted metrics
v1.10.2,Clear buffers
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.10.2,to create the class
v1.10.2,TODO: update to hint + kwargs
v1.10.2,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default regularizer class
v1.10.2,: The default parameters for the default regularizer class
v1.10.2,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.10.2,Make sure that all modules with parameters do have a reset_parameters method.
v1.10.2,Recursively visit all sub-modules
v1.10.2,skip self
v1.10.2,Track parents for blaming
v1.10.2,call reset_parameters if possible
v1.10.2,initialize from bottom to top
v1.10.2,This ensures that specialized initializations will take priority over the default ones of its components.
v1.10.2,emit warning if there where parameters which were not initialised by reset_parameters.
v1.10.2,Additional debug information
v1.10.2,docstr-coverage: inherited
v1.10.2,TODO: allow max_id being present in representation_kwargs; if it matches max_id
v1.10.2,TODO: we could infer some shapes from the given interaction shape information
v1.10.2,check max-id
v1.10.2,check shapes
v1.10.2,: The entity representations
v1.10.2,: The relation representations
v1.10.2,: The weight regularizers
v1.10.2,: The interaction function
v1.10.2,"TODO: support ""broadcasting"" representation regularizers?"
v1.10.2,e.g. re-use the same regularizer for everything; or
v1.10.2,"pass a dictionary with keys ""entity""/""relation"";"
v1.10.2,values are either a regularizer hint (=the same regularizer for all repr); or a sequence of appropriate length
v1.10.2,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.10.2,"model.modules(). Thereby, we can collect them automatically."
v1.10.2,Explicitly call reset_parameters to trigger initialization
v1.10.2,"note, triples_factory is required instead of just using self.num_entities"
v1.10.2,and self.num_relations for the inductive case when this is different
v1.10.2,instantiate regularizer
v1.10.2,normalize input
v1.10.2,Note: slicing cannot be used here: the indices for score_hrt only have a batch
v1.10.2,"dimension, and slicing along this dimension is already considered by sub-batching."
v1.10.2,Note: we do not delegate to the general method for performance reasons
v1.10.2,Note: repetition is not necessary here
v1.10.2,batch normalization modules use batch statistics in training mode
v1.10.2,-> different batch divisions lead to different results
v1.10.2,docstr-coverage: inherited
v1.10.2,normalize before checking
v1.10.2,slice early to allow lazy computation of target representations
v1.10.2,add broadcast dimension
v1.10.2,unsqueeze if necessary
v1.10.2,docstr-coverage: inherited
v1.10.2,normalize before checking
v1.10.2,slice early to allow lazy computation of target representations
v1.10.2,add broadcast dimension
v1.10.2,unsqueeze if necessary
v1.10.2,docstr-coverage: inherited
v1.10.2,normalize before checking
v1.10.2,slice early to allow lazy computation of target representations
v1.10.2,add broadcast dimension
v1.10.2,unsqueeze if necessary
v1.10.2,normalization
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,train model
v1.10.2,"note: as this is an example, the model is only trained for a few epochs,"
v1.10.2,"but not until convergence. In practice, you would usually first verify that"
v1.10.2,"the model is sufficiently good in prediction, before looking at uncertainty scores"
v1.10.2,predict triple scores with uncertainty
v1.10.2,"use a larger number of samples, to increase quality of uncertainty estimate"
v1.10.2,get most and least uncertain prediction on training set
v1.10.2,: The scores
v1.10.2,": The uncertainty, in the same shape as scores"
v1.10.2,Enforce evaluation mode
v1.10.2,set dropout layers to training mode
v1.10.2,draw samples
v1.10.2,compute mean and std
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,"This empty 1-element tensor doesn't actually do anything,"
v1.10.2,but is necessary since models with no grad params blow
v1.10.2,up the optimizer
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default loss function class
v1.10.2,: The default parameters for the default loss function class
v1.10.2,: The instance of the loss
v1.10.2,: the number of entities
v1.10.2,: the number of relations
v1.10.2,: whether to use inverse relations
v1.10.2,: utility for generating inverse relations
v1.10.2,": When predict_with_sigmoid is set to True, the sigmoid function is"
v1.10.2,: applied to the logits during evaluation and also for predictions
v1.10.2,": after training, but has no effect on the training."
v1.10.2,Random seeds have to set before the embeddings are initialized
v1.10.2,Loss
v1.10.2,TODO: why do we need to empty the cache?
v1.10.2,"TODO: this currently compute (batch_size, num_relations) instead,"
v1.10.2,"i.e., scores for normal and inverse relations"
v1.10.2,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.10.2,send to device
v1.10.2,special handling of inverse relations
v1.10.2,"when trained on inverse relations, the internal relation ID is twice the original relation ID"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Base Models
v1.10.2,Concrete Models
v1.10.2,Inductive Models
v1.10.2,Evaluation-only models
v1.10.2,Meta Models
v1.10.2,Utils
v1.10.2,Abstract Models
v1.10.2,We might be able to relax this later
v1.10.2,baseline models behave differently
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,always create representations for normal and inverse relations and padding
v1.10.2,"note: we need to share the aggregation across representations, since the aggregation may have"
v1.10.2,trainable parameters
v1.10.2,note: we cannot ensure the mapping also matches...
v1.10.2,get relation representations
v1.10.2,get combination
v1.10.2,get token representations
v1.10.2,relation representations are shared
v1.10.2,share combination weights
v1.10.2,: a mapping from inductive mode to corresponding entity representations
v1.10.2,": note: there may be duplicate values, if entity representations are shared between validation and testing"
v1.10.2,inductive factories
v1.10.2,"entity representation kwargs may contain a triples factory, which needs to be replaced"
v1.10.2,"entity_representations_kwargs.pop(""triples_factory"", None)"
v1.10.2,note: this is *not* a nn.ModuleDict; the modules have to be registered elsewhere
v1.10.2,shared
v1.10.2,non-shared
v1.10.2,"note: ""training"" is an attribute of nn.Module -> need to rename to avoid name collision"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,default composition is DistMult-style
v1.10.2,Saving edge indices for all the supplied splits
v1.10.2,Extract all entity and relation representations
v1.10.2,Perform message passing and get updated states
v1.10.2,Use updated entity and relation states to extract requested IDs
v1.10.2,TODO I got lost in all the Representation Modules and shape casting and wrote this ;(
v1.10.2,normalization
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,": the indexed filter triples, i.e., sparse masks"
v1.10.2,avoid cyclic imports
v1.10.2,create base model
v1.10.2,assign *after* nn.Module.__init__
v1.10.2,save constants
v1.10.2,index triples
v1.10.2,initialize base model's parameters
v1.10.2,"get masks, shape: (batch_size, num_entities/num_relations)"
v1.10.2,combine masks
v1.10.2,"note: * is an elementwise and, and + and elementwise or"
v1.10.2,get non-zero entries
v1.10.2,set scores for fill value for every non-occuring entry
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,NodePiece
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO rethink after RGCN update
v1.10.2,TODO: other parameters?
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default loss function class
v1.10.2,: The default parameters for the default loss function class
v1.10.2,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.10.2,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.10.2,ConvE should be trained with inverse triples
v1.10.2,entity embedding
v1.10.2,ConvE uses one bias for each entity
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,head representation
v1.10.2,tail representation
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default loss function class
v1.10.2,: The default parameters for the default loss function class
v1.10.2,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.10.2,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.10.2,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.10.2,use torch's native complex data type
v1.10.2,use torch's native complex data type
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The regularizer used by [nickel2011]_ for for RESCAL
v1.10.2,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.10.2,: a normalized weight of 10 is used.
v1.10.2,: The LP settings used by [nickel2011]_ for for RESCAL
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.10.2,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.10.2,In the code base only the weights of the output layer are used for regularization
v1.10.2,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,comment:
v1.10.2,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.10.2,uses float64
v1.10.2,entity bias for head
v1.10.2,entity bias for tail
v1.10.2,relation offset
v1.10.2,diagonal relation transformation matrix
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: the default loss function is the self-adversarial negative sampling loss
v1.10.2,: The default parameters for the default loss function class
v1.10.2,: The default entity normalizer parameters
v1.10.2,: The entity representations are normalized to L2 unit length
v1.10.2,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.10.2,"update initializer settings, cf."
v1.10.2,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.10.2,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.10.2,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.10.2,in the original implementation the embeddings are initialized in one parameter
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,"w: (k, d, d)"
v1.10.2,"vh: (k, d)"
v1.10.2,"vt: (k, d)"
v1.10.2,"b: (k,)"
v1.10.2,"u: (k,)"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The regularizer used by [yang2014]_ for DistMult
v1.10.2,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.10.2,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.10.2,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.10.2,: The LP settings used by [yang2014]_ for DistMult
v1.10.2,note: DistMult only regularizes the relation embeddings;
v1.10.2,entity embeddings are hard constrained instead
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default settings for the entity constrainer
v1.10.2,mean
v1.10.2,diagonal covariance
v1.10.2,Ensure positive definite covariances matrices and appropriate size by clamping
v1.10.2,mean
v1.10.2,diagonal covariance
v1.10.2,Ensure positive definite covariances matrices and appropriate size by clamping
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,diagonal entries
v1.10.2,off-diagonal
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The custom regularizer used by [wang2014]_ for TransH
v1.10.2,: The settings used by [wang2014]_ for TransH
v1.10.2,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.10.2,"Therefore, apply_only_once is always set to True."
v1.10.2,: The custom regularizer used by [wang2014]_ for TransH
v1.10.2,: The settings used by [wang2014]_ for TransH
v1.10.2,"note: this parameter is not named ""entity_regularizer"" for compatability with the"
v1.10.2,regularizer-specific HPO code
v1.10.2,translation vector in hyperplane
v1.10.2,normal vector of hyperplane
v1.10.2,normalise the normal vectors to unit l2 length
v1.10.2,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.10.2,which enforces the defined soft constraints.
v1.10.2,"thus, we need to use a weight regularizer instead of having an Embedding regularizer,"
v1.10.2,which only regularizes the weights used in a batch
v1.10.2,note: the following is already the default
v1.10.2,"default_regularizer=self.regularizer_default,"
v1.10.2,"default_regularizer_kwargs=self.regularizer_default_kwargs,"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,TODO: Initialize from TransE
v1.10.2,relation embedding
v1.10.2,relation projection
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,TODO: Decomposition kwargs
v1.10.2,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.10.2,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.10.2,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.10.2,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.10.2,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,combined representation
v1.10.2,Resolve interaction function
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default loss function class
v1.10.2,: The default parameters for the default loss function class
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,entity bias for head
v1.10.2,relation position head
v1.10.2,relation shape head
v1.10.2,relation size head
v1.10.2,relation position tail
v1.10.2,relation shape tail
v1.10.2,relation size tail
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default loss function class
v1.10.2,: The default parameters for the default loss function class
v1.10.2,: The regularizer used by [trouillon2016]_ for SimplE
v1.10.2,": In the paper, they use weight of 0.1, and do not normalize the"
v1.10.2,": regularization term by the number of elements, which is 200."
v1.10.2,: The power sum settings used by [trouillon2016]_ for SimplE
v1.10.2,(head) entity
v1.10.2,tail entity
v1.10.2,relations
v1.10.2,inverse relations
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,input normalization
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,Regular relation embeddings
v1.10.2,The relation-specific interaction vector
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,always create representations for normal and inverse relations and padding
v1.10.2,normalize embedding specification
v1.10.2,prepare token representations & kwargs
v1.10.2,"max_id=triples_factory.num_relations,  # will get added by ERModel"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default loss function class
v1.10.2,: The default parameters for the default loss function class
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Normalize relation embeddings
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default loss function class
v1.10.2,: The default parameters for the default loss function class
v1.10.2,: The LP settings used by [zhang2019]_ for QuatE.
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default settings for the entity constrainer
v1.10.2,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default loss function class
v1.10.2,: The default parameters for the default loss function class
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default parameters for the default loss function class
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default loss function class
v1.10.2,: The default parameters for the default loss function class
v1.10.2,the individual combination for real/complex parts
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the model's hyper-parameters
v1.10.2,: The default parameters for the default loss function class
v1.10.2,no activation
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: the interaction class (for generating the overview table)
v1.10.2,added by ERModel
v1.10.2,"max_id=triples_factory.num_entities,"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,create sparse matrix of absolute counts
v1.10.2,normalize to relative counts
v1.10.2,base case
v1.10.2,"note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could"
v1.10.2,"stay sparse here, with a potential of a huge memory benefit on large datasets!"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,These operations are deterministic and a random seed can be fixed
v1.10.2,just to avoid warnings
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,compute relation similarity matrix
v1.10.2,mapping from relations to head/tail entities
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,"if we really need access to the path later, we can expose it as a property"
v1.10.2,via self.writer.log_dir
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The WANDB run
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The name of the run
v1.10.2,": The configuration dictionary, a mapping from name -> value"
v1.10.2,: Should metrics be stored when running ``log_metrics()``?
v1.10.2,": The metrics, a mapping from step -> (name -> value)"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,: A hint for constructing a :class:`MultiResultTracker`
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Base classes
v1.10.2,Concrete classes
v1.10.2,Utilities
v1.10.2,always add a Python result tracker for storing the configuration
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The file extension for this writer (do not include dot)
v1.10.2,: The file where the results are written to.
v1.10.2,docstr-coverage: inherited
v1.10.2,: The column names
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,store set of triples
v1.10.2,docstr-coverage: inherited
v1.10.2,: some prime numbers for tuple hashing
v1.10.2,: The bit-array for the Bloom filter data structure
v1.10.2,Allocate bit array
v1.10.2,calculate number of hashing rounds
v1.10.2,index triples
v1.10.2,Store some meta-data
v1.10.2,pre-hash
v1.10.2,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,At least make sure to not replace the triples by the original value
v1.10.2,"To make sure we don't replace the {head, relation, tail} by the"
v1.10.2,original value we shift all values greater or equal than the original value by one up
v1.10.2,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.10.2,Set the indices
v1.10.2,docstr-coverage: inherited
v1.10.2,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.10.2,Bind the total number of negatives to sample in this batch
v1.10.2,Equally corrupt all sides
v1.10.2,"Do not detach, as no gradients should flow into the indices."
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.10.2,: A filterer for negative batches
v1.10.2,create unfiltered negative batch by corruption
v1.10.2,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Utils
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.10.2,create index structure
v1.10.2,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.10.2,: The concatenated sorted sets of head/tail entities
v1.10.2,docstr-coverage: inherited
v1.10.2,"shape: (batch_size, num_neg_per_pos, 3)"
v1.10.2,Uniformly sample from head/tail offsets
v1.10.2,get corresponding entity
v1.10.2,"and position within triple (0: head, 2: tail)"
v1.10.2,write into negative batch
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Preprocessing: Compute corruption probabilities
v1.10.2,"compute tph, i.e. the average number of tail entities per head"
v1.10.2,"compute hpt, i.e. the average number of head entities per tail"
v1.10.2,Set parameter for Bernoulli distribution
v1.10.2,docstr-coverage: inherited
v1.10.2,Decide whether to corrupt head or tail
v1.10.2,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.10.2,flatten mask
v1.10.2,Tails are corrupted if heads are not corrupted
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The random seed used at the beginning of the pipeline
v1.10.2,: The model trained by the pipeline
v1.10.2,: The training triples
v1.10.2,: The training loop used by the pipeline
v1.10.2,: The losses during training
v1.10.2,: The results evaluated by the pipeline
v1.10.2,: How long in seconds did training take?
v1.10.2,: How long in seconds did evaluation take?
v1.10.2,: An early stopper
v1.10.2,: The configuration
v1.10.2,: Any additional metadata as a dictionary
v1.10.2,: The version of PyKEEN used to create these results
v1.10.2,: The git hash of PyKEEN used to create these results
v1.10.2,file names for storing results
v1.10.2,TODO: rename param?
v1.10.2,always save results as json file
v1.10.2,"save other components only if requested (which they are, by default)"
v1.10.2,TODO use pathlib here
v1.10.2,"note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:"
v1.10.2,"when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless"
v1.10.2,note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()
v1.10.2,only one original value => assume this to be the mean
v1.10.2,multiple values => assume they correspond to individual trials
v1.10.2,metrics accumulates rows for a dataframe for comparison against the original reported results (if any)
v1.10.2,"TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)"
v1.10.2,summarize
v1.10.2,skip special parameters
v1.10.2,FIXME this should never happen.
v1.10.2,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.10.2,used random_seed to ensure reproducible results
v1.10.2,We have to set clear optimizer to False since training should be continued
v1.10.2,TODO: checkpoint_dict not further used; later loaded again by TrainingLoop.train
v1.10.2,TODO: allow empty validation / testing
v1.10.2,evaluation restriction to a subset of entities/relations
v1.10.2,2. Model
v1.10.2,3. Loss
v1.10.2,4. Regularizer
v1.10.2,TODO should training be reset?
v1.10.2,TODO should kwargs for loss and regularizer be checked and raised for?
v1.10.2,Log model parameters
v1.10.2,Log loss parameters
v1.10.2,the loss was already logged as part of the model kwargs
v1.10.2,"loss=loss_resolver.normalize_inst(model_instance.loss),"
v1.10.2,Log regularizer parameters
v1.10.2,5. Optimizer
v1.10.2,5.1 Learning Rate Scheduler
v1.10.2,6. Training Loop
v1.10.2,8. Evaluation
v1.10.2,7. Training (ronaldo style)
v1.10.2,Misc
v1.10.2,Stopping
v1.10.2,"Load the evaluation batch size for the stopper, if it has been set"
v1.10.2,Add logging for debugging
v1.10.2,Train like Cristiano Ronaldo
v1.10.2,Misc
v1.10.2,Build up a list of triples if we want to be in the filtered setting
v1.10.2,"If the user gave custom ""additional_filter_triples"""
v1.10.2,Determine whether the validation triples should also be filtered while performing test evaluation
v1.10.2,TODO consider implications of duplicates
v1.10.2,Evaluate
v1.10.2,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.10.2,Add logging about evaluator for debugging
v1.10.2,1. Dataset
v1.10.2,2. Model
v1.10.2,3. Loss
v1.10.2,4. Regularizer
v1.10.2,5. Optimizer
v1.10.2,5.1 Learning Rate Scheduler
v1.10.2,6. Training Loop
v1.10.2,7. Training (ronaldo style)
v1.10.2,8. Evaluation
v1.10.2,9. Tracking
v1.10.2,Misc
v1.10.2,Start tracking
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,cf. also https://github.com/pykeen/pykeen/issues/1071
v1.10.2,TODO: use a class-resolver?
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Imported from PyTorch
v1.10.2,: The default strategy for optimizing the lr_schedulers' hyper-parameters
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,TODO what happens if already exists?
v1.10.2,TODO incorporate setting of random seed
v1.10.2,pipeline_kwargs=dict(
v1.10.2,"random_seed=random_non_negative_int(),"
v1.10.2,"),"
v1.10.2,Add dataset to current_pipeline
v1.10.2,"Training, test, and validation paths are provided"
v1.10.2,Add loss function to current_pipeline
v1.10.2,Add regularizer to current_pipeline
v1.10.2,Add optimizer to current_pipeline
v1.10.2,Add training approach to current_pipeline
v1.10.2,Add evaluation
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.10.2,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.10.2,the columns are not ordered properly.
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,convert class to string to use caching
v1.10.2,Assume it's a file path
v1.10.2,note: we only need to set the create_inverse_triples in the training factory.
v1.10.2,normalize dataset kwargs
v1.10.2,enable passing force option via dataset_kwargs
v1.10.2,hash kwargs
v1.10.2,normalize dataset name
v1.10.2,get canonic path
v1.10.2,try to use cached dataset
v1.10.2,load dataset without cache
v1.10.2,store cache
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Type annotation for split types
v1.10.2,type variables for dictionaries of preprocessed data loaded through torch.load
v1.10.2,: The name of the dataset to download
v1.10.2,docstr-coverage: inherited
v1.10.2,label mapping is in dataset.root/mapping
v1.10.2,docstr-coverage: inherited
v1.10.2,"note: we do not use the built-in constants here, since those refer to OGB nomenclature"
v1.10.2,(which happens to coincide with ours)
v1.10.2,"dtype: numpy.int64, shape: (m,)"
v1.10.2,"dtype: numpy.int64, shape: (n, k)"
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,noqa: D102
v1.10.2,docstr-coverage: inherited
v1.10.2,: the node types
v1.10.2,"shape: (n,)"
v1.10.2,"dtype: numpy.int64, shape: (n, k)"
v1.10.2,disease: UMLS CUI (https://www.nlm.nih.gov/research/umls/index.html).
v1.10.2,drug: STITCH ID (http://stitch.embl.de/).
v1.10.2,function: Gene Ontology ID (http://geneontology.org/).
v1.10.2,protein: Proteins: Entrez Gene ID (https://www.genenames.org/).
v1.10.2,side effects: UMLS CUI (https://www.nlm.nih.gov/research/umls/index.html).
v1.10.2,todo(@cthoyt): proper prefixing?
v1.10.2,docstr-coverage: inherited
v1.10.2,entity mappings are separate for each node type -> combine
v1.10.2,convert entity_name to categorical for fast joins
v1.10.2,we need the entity dataframe for fast re-mapping later on
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,compose temporary df
v1.10.2,add extra column with old index to revert sort order change by merge
v1.10.2,convert to categorical dtype
v1.10.2,join with entity mapping
v1.10.2,revert change in order
v1.10.2,select global ID
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,relation typing
v1.10.2,constants
v1.10.2,unique
v1.10.2,compute over all triples
v1.10.2,Determine group key
v1.10.2,Add labels if requested
v1.10.2,TODO: Merge with _common?
v1.10.2,include hash over triples into cache-file name
v1.10.2,include part hash into cache-file name
v1.10.2,re-use cached file if possible
v1.10.2,select triples
v1.10.2,save to file
v1.10.2,Prune by support and confidence
v1.10.2,TODO: Consider merging with other analysis methods
v1.10.2,TODO: Consider merging with other analysis methods
v1.10.2,TODO: Consider merging with other analysis methods
v1.10.2,"num_triples_validation: Optional[int],"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Raise matplotlib level
v1.10.2,expected metrics
v1.10.2,Needs simulation
v1.10.2,See https://zenodo.org/record/6331629
v1.10.2,TODO: maybe merge into analyze / make sub-command
v1.10.2,only save full data
v1.10.2,Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)
v1.10.2,Plot: difference between mean head and tail degree
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,don't call this function by itself. assumes called through the `validation`
v1.10.2,property and the _training factory has already been loaded
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Normalize path
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Base classes
v1.10.2,Utilities
v1.10.2,: A factory wrapping the training triples
v1.10.2,": A factory wrapping the testing triples, that share indices with the training triples"
v1.10.2,": A factory wrapping the validation triples, that share indices with the training triples"
v1.10.2,: the dataset's name
v1.10.2,TODO: Make a constant for the names
v1.10.2,docstr-coverage: inherited
v1.10.2,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.10.2,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.10.2,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.10.2,: The directory in which the cached data is stored
v1.10.2,TODO: use class-resolver normalize?
v1.10.2,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.2,don't call this function by itself. assumes called through the `validation`
v1.10.2,property and the _training factory has already been loaded
v1.10.2,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,"relative paths within zip file's always follow Posix path, even on Windows"
v1.10.2,tarfile does not like pathlib
v1.10.2,: URL to the data to download
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Utilities
v1.10.2,Base Classes
v1.10.2,Concrete Classes
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,"ZENODO_URL = ""https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip"""
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.10.2,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Base class
v1.10.2,Mid-level classes
v1.10.2,: A factory wrapping the training triples
v1.10.2,: A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT
v1.10.2,share indices with the transductive training
v1.10.2,": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.10.2,": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.10.2,: All datasets should take care of inverse triple creation
v1.10.2,": The actual instance of the training factory, which is exposed to the user through `transductive_training`"
v1.10.2,": The actual instance of the inductive inference factory,"
v1.10.2,: which is exposed to the user through `inductive_inference`
v1.10.2,": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`"
v1.10.2,": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`"
v1.10.2,: The directory in which the cached data is stored
v1.10.2,generate subfolders 'training' and  'inference'
v1.10.2,TODO: use class-resolver normalize?
v1.10.2,add v1 / v2 / v3 / v4 for inductive splits if available
v1.10.2,important: inductive_inference shares the same RELATIONS with the transductive training graph
v1.10.2,inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph
v1.10.2,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.2,inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph
v1.10.2,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,Base class
v1.10.2,Mid-level classes
v1.10.2,Datasets
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,graph pairs
v1.10.2,graph sizes
v1.10.2,graph versions
v1.10.2,: The link to the zip file
v1.10.2,: The hex digest for the zip file
v1.10.2,Input validation.
v1.10.2,ensure zip file is present
v1.10.2,save relative paths beforehand so they are present for loading
v1.10.2,delegate to super class
v1.10.2,docstr-coverage: inherited
v1.10.2,"left side has files ending with 1, right side with 2"
v1.10.2,docstr-coverage: inherited
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,": The mapping from (graph-pair, side) to triple file name"
v1.10.2,: The internal dataset name
v1.10.2,: The hex digest for the zip file
v1.10.2,input validation
v1.10.2,store *before* calling super to have it available when loading the graphs
v1.10.2,ensure zip file is present
v1.10.2,shared directory for multiple datasets.
v1.10.2,docstr-coverage: inherited
v1.10.2,create triples factory
v1.10.2,docstr-coverage: inherited
v1.10.2,load mappings for both sides
v1.10.2,load triple alignments
v1.10.2,extract entity alignments
v1.10.2,"(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2"
v1.10.2,TODO: support ID-only graphs
v1.10.2,load both graphs
v1.10.2,load alignment
v1.10.2,drop duplicates
v1.10.2,combine
v1.10.2,store for repr
v1.10.2,split
v1.10.2,create inverse triples only for training
v1.10.2,docstr-coverage: inherited
v1.10.2,base
v1.10.2,concrete
v1.10.2,Abstract class
v1.10.2,Concrete classes
v1.10.2,Data Structures
v1.10.2,a buffer for the triples
v1.10.2,the offsets
v1.10.2,normalization
v1.10.2,append shifted mapped triples
v1.10.2,update offsets
v1.10.2,merge labels with same ID
v1.10.2,for mypy
v1.10.2,reconstruct label-to-id
v1.10.2,optional
v1.10.2,merge entity mapping
v1.10.2,merge relation mapping
v1.10.2,convert labels to IDs
v1.10.2,"map labels, using -1 as fill-value for invalid labels"
v1.10.2,"we cannot drop them here, since the two columns need to stay aligned"
v1.10.2,filter alignment
v1.10.2,map alignment from old IDs to new IDs
v1.10.2,determine swapping partner
v1.10.2,only keep triples where we have a swapping partner
v1.10.2,replace by swapping partner
v1.10.2,": the merged id-based triples, shape: (n, 3)"
v1.10.2,": the updated alignment, shape: (2, m)"
v1.10.2,: additional keyword-based parameters for adjusting label-to-id mappings
v1.10.2,concatenate triples
v1.10.2,filter alignment and translate to IDs
v1.10.2,process
v1.10.2,TODO: restrict to only using training alignments?
v1.10.2,merge mappings
v1.10.2,docstr-coverage: inherited
v1.10.2,docstr-coverage: inherited
v1.10.2,add swap triples
v1.10.2,"e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)"
v1.10.2,create dense entity remapping for swap
v1.10.2,add swapped triples
v1.10.2,swap head
v1.10.2,swap tail
v1.10.2,: the name of the additional alignment relation
v1.10.2,docstr-coverage: inherited
v1.10.2,add alignment triples with extra relation
v1.10.2,docstr-coverage: inherited
v1.10.2,"determine connected components regarding the same-as relation (i.e., applies transitivity)"
v1.10.2,apply id mapping
v1.10.2,ensure consecutive IDs
v1.10.2,only use training alignments?
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,1. Dataset
v1.10.2,2. Model
v1.10.2,3. Loss
v1.10.2,4. Regularizer
v1.10.2,5. Optimizer
v1.10.2,5.1 Learning Rate Scheduler
v1.10.2,6. Training Loop
v1.10.2,7. Training
v1.10.2,8. Evaluation
v1.10.2,9. Trackers
v1.10.2,Misc.
v1.10.2,log pruning
v1.10.2,"trial was successful, but has to be ended"
v1.10.2,also show info
v1.10.2,2. Model
v1.10.2,3. Loss
v1.10.2,4. Regularizer
v1.10.2,5. Optimizer
v1.10.2,5.1 Learning Rate Scheduler
v1.10.2,"TODO this fixes the issue for negative samplers, but does not generally address it."
v1.10.2,"For example, some of them obscure their arguments with **kwargs, so should we look"
v1.10.2,at the parent class? Sounds like something to put in class resolver by using the
v1.10.2,"inspect module. For now, this solution will rely on the fact that the sampler is a"
v1.10.2,direct descendent of a parent NegativeSampler
v1.10.2,a fixed checkpoint_name leads avoid collision across trials
v1.10.2,create result tracker to allow to gracefully close failed trials
v1.10.2,1. Dataset
v1.10.2,2. Model
v1.10.2,3. Loss
v1.10.2,4. Regularizer
v1.10.2,5. Optimizer
v1.10.2,5.1 Learning Rate Scheduler
v1.10.2,6. Training Loop
v1.10.2,7. Training
v1.10.2,8. Evaluation
v1.10.2,9. Tracker
v1.10.2,Misc.
v1.10.2,close run in result tracker
v1.10.2,raise the error again (which will be catched in study.optimize)
v1.10.2,: The :mod:`optuna` study object
v1.10.2,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.10.2,Output study information
v1.10.2,Output all trials
v1.10.2,Output best trial as pipeline configuration file
v1.10.2,1. Dataset
v1.10.2,2. Model
v1.10.2,3. Loss
v1.10.2,4. Regularizer
v1.10.2,5. Optimizer
v1.10.2,5.1 Learning Rate Scheduler
v1.10.2,6. Training Loop
v1.10.2,7. Training
v1.10.2,8. Evaluation
v1.10.2,9. Tracking
v1.10.2,6. Misc
v1.10.2,Optuna Study Settings
v1.10.2,Optuna Optimization Settings
v1.10.2,TODO: use metric.increasing to determine default direction
v1.10.2,0. Metadata/Provenance
v1.10.2,1. Dataset
v1.10.2,2. Model
v1.10.2,3. Loss
v1.10.2,4. Regularizer
v1.10.2,5. Optimizer
v1.10.2,5.1 Learning Rate Scheduler
v1.10.2,6. Training Loop
v1.10.2,7. Training
v1.10.2,8. Evaluation
v1.10.2,9. Tracking
v1.10.2,1. Dataset
v1.10.2,2. Model
v1.10.2,3. Loss
v1.10.2,4. Regularizer
v1.10.2,5. Optimizer
v1.10.2,5.1 Learning Rate Scheduler
v1.10.2,6. Training Loop
v1.10.2,7. Training
v1.10.2,8. Evaluation
v1.10.2,9. Tracker
v1.10.2,Optuna Misc.
v1.10.2,Pipeline Misc.
v1.10.2,Invoke optimization of the objective function.
v1.10.2,TODO: make it even easier to specify categorical strategies just as lists
v1.10.2,"if isinstance(info, (tuple, list, set)):"
v1.10.2,"info = dict(type='categorical', choices=list(info))"
v1.10.2,get log from info - could either be a boolean or string
v1.10.2,"otherwise, dataset refers to a file that should be automatically split"
v1.10.2,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.10.2,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.10.2,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.10.2,if has_dataset(dataset_name):
v1.10.2,"study.set_user_attr('dataset', dataset_name)"
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,noqa: DAR101
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,-*- coding: utf-8 -*-
v1.10.2,%%
v1.10.2,%% [markdown]
v1.10.2,## Training a model with PyKEEN
v1.10.2,%%
v1.10.2,"this will log a metric with name ""validation.loss"" to the configured result tracker"
v1.10.2,%% [markdown]
v1.10.2,## Evaluation with seaborn
v1.10.2,%%
v1.10.2,%%
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,
v1.10.1,Configuration file for the Sphinx documentation builder.
v1.10.1,
v1.10.1,This file does only contain a selection of the most common options. For a
v1.10.1,full list see the documentation:
v1.10.1,http://www.sphinx-doc.org/en/master/config
v1.10.1,-- Path setup --------------------------------------------------------------
v1.10.1,"If extensions (or modules to document with autodoc) are in another directory,"
v1.10.1,add these directories to sys.path here. If the directory is relative to the
v1.10.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.10.1,
v1.10.1,"sys.path.insert(0, os.path.abspath('..'))"
v1.10.1,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.10.1,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.10.1,from unittest.mock import Mock
v1.10.1,sys.modules['numpy'] = Mock()
v1.10.1,sys.modules['numpy.linalg'] = Mock()
v1.10.1,sys.modules['scipy'] = Mock()
v1.10.1,sys.modules['scipy.optimize'] = Mock()
v1.10.1,sys.modules['scipy.interpolate'] = Mock()
v1.10.1,sys.modules['scipy.sparse'] = Mock()
v1.10.1,sys.modules['scipy.ndimage'] = Mock()
v1.10.1,sys.modules['scipy.ndimage.filters'] = Mock()
v1.10.1,sys.modules['tensorflow'] = Mock()
v1.10.1,sys.modules['theano'] = Mock()
v1.10.1,sys.modules['theano.tensor'] = Mock()
v1.10.1,sys.modules['torch'] = Mock()
v1.10.1,sys.modules['torch.optim'] = Mock()
v1.10.1,sys.modules['torch.nn'] = Mock()
v1.10.1,sys.modules['torch.nn.init'] = Mock()
v1.10.1,sys.modules['torch.autograd'] = Mock()
v1.10.1,sys.modules['sklearn'] = Mock()
v1.10.1,sys.modules['sklearn.model_selection'] = Mock()
v1.10.1,sys.modules['sklearn.utils'] = Mock()
v1.10.1,-- Project information -----------------------------------------------------
v1.10.1,"The full version, including alpha/beta/rc tags."
v1.10.1,The short X.Y version.
v1.10.1,-- General configuration ---------------------------------------------------
v1.10.1,"If your documentation needs a minimal Sphinx version, state it here."
v1.10.1,
v1.10.1,needs_sphinx = '1.0'
v1.10.1,"If true, the current module name will be prepended to all description"
v1.10.1,unit titles (such as .. function::).
v1.10.1,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.10.1,"Add any Sphinx extension module names here, as strings. They can be"
v1.10.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.10.1,ones.
v1.10.1,show todo's
v1.10.1,generate autosummary pages
v1.10.1,"Add any paths that contain templates here, relative to this directory."
v1.10.1,The suffix(es) of source filenames.
v1.10.1,You can specify multiple suffix as a list of string:
v1.10.1,
v1.10.1,"source_suffix = ['.rst', '.md']"
v1.10.1,The master toctree document.
v1.10.1,The language for content autogenerated by Sphinx. Refer to documentation
v1.10.1,for a list of supported languages.
v1.10.1,
v1.10.1,This is also used if you do content translation via gettext catalogs.
v1.10.1,"Usually you set ""language"" from the command line for these cases."
v1.10.1,"List of patterns, relative to source directory, that match files and"
v1.10.1,directories to ignore when looking for source files.
v1.10.1,This pattern also affects html_static_path and html_extra_path.
v1.10.1,The name of the Pygments (syntax highlighting) style to use.
v1.10.1,-- Options for HTML output -------------------------------------------------
v1.10.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.10.1,a list of builtin themes.
v1.10.1,
v1.10.1,Theme options are theme-specific and customize the look and feel of a theme
v1.10.1,"further.  For a list of options available for each theme, see the"
v1.10.1,documentation.
v1.10.1,
v1.10.1,html_theme_options = {}
v1.10.1,"Add any paths that contain custom static files (such as style sheets) here,"
v1.10.1,"relative to this directory. They are copied after the builtin static files,"
v1.10.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.10.1,html_static_path = ['_static']
v1.10.1,"Custom sidebar templates, must be a dictionary that maps document names"
v1.10.1,to template names.
v1.10.1,
v1.10.1,The default sidebars (for documents that don't match any pattern) are
v1.10.1,defined by theme itself.  Builtin themes are using these templates by
v1.10.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.10.1,'searchbox.html']``.
v1.10.1,
v1.10.1,html_sidebars = {}
v1.10.1,The name of an image file (relative to this directory) to place at the top
v1.10.1,of the sidebar.
v1.10.1,
v1.10.1,-- Options for HTMLHelp output ---------------------------------------------
v1.10.1,Output file base name for HTML help builder.
v1.10.1,-- Options for LaTeX output ------------------------------------------------
v1.10.1,latex_elements = {
v1.10.1,The paper size ('letterpaper' or 'a4paper').
v1.10.1,
v1.10.1,"'papersize': 'letterpaper',"
v1.10.1,
v1.10.1,"The font size ('10pt', '11pt' or '12pt')."
v1.10.1,
v1.10.1,"'pointsize': '10pt',"
v1.10.1,
v1.10.1,Additional stuff for the LaTeX preamble.
v1.10.1,
v1.10.1,"'preamble': '',"
v1.10.1,
v1.10.1,Latex figure (float) alignment
v1.10.1,
v1.10.1,"'figure_align': 'htbp',"
v1.10.1,}
v1.10.1,Grouping the document tree into LaTeX files. List of tuples
v1.10.1,"(source start file, target name, title,"
v1.10.1,"author, documentclass [howto, manual, or own class])."
v1.10.1,latex_documents = [
v1.10.1,(
v1.10.1,"master_doc,"
v1.10.1,"'pykeen.tex',"
v1.10.1,"'PyKEEN Documentation',"
v1.10.1,"author,"
v1.10.1,"'manual',"
v1.10.1,"),"
v1.10.1,]
v1.10.1,-- Options for manual page output ------------------------------------------
v1.10.1,One entry per manual page. List of tuples
v1.10.1,"(source start file, name, description, authors, manual section)."
v1.10.1,-- Options for Texinfo output ----------------------------------------------
v1.10.1,Grouping the document tree into Texinfo files. List of tuples
v1.10.1,"(source start file, target name, title, author,"
v1.10.1,"dir menu entry, description, category)"
v1.10.1,-- Options for Epub output -------------------------------------------------
v1.10.1,Bibliographic Dublin Core info.
v1.10.1,epub_title = project
v1.10.1,The unique identifier of the text. This can be a ISBN number
v1.10.1,or the project homepage.
v1.10.1,
v1.10.1,epub_identifier = ''
v1.10.1,A unique identification for the text.
v1.10.1,
v1.10.1,epub_uid = ''
v1.10.1,A list of files that should not be packed into the epub file.
v1.10.1,epub_exclude_files = ['search.html']
v1.10.1,-- Extension configuration -------------------------------------------------
v1.10.1,-- Options for intersphinx extension ---------------------------------------
v1.10.1,Example configuration for intersphinx: refer to the Python standard library.
v1.10.1,"'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),"
v1.10.1,See discussion for adding huggingface intersphinx docs at
v1.10.1,https://github.com/huggingface/transformers/issues/14728#issuecomment-1133521776
v1.10.1,autodoc_member_order = 'bysource'
v1.10.1,autodoc_preserve_defaults = True
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,check probability distribution
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Check a model param is optimized
v1.10.1,Check a loss param is optimized
v1.10.1,Check a model param is NOT optimized
v1.10.1,Check a loss param is optimized
v1.10.1,Check a model param is optimized
v1.10.1,Check a loss param is NOT optimized
v1.10.1,Check a model param is NOT optimized
v1.10.1,Check a loss param is NOT optimized
v1.10.1,verify failure
v1.10.1,"Since custom data was passed, we can't store any of this"
v1.10.1,"currently, any custom data doesn't get stored."
v1.10.1,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.10.1,"Since there's no source path information, these shouldn't be"
v1.10.1,"added, even if it might be possible to infer path information"
v1.10.1,from the triples factories
v1.10.1,"Since paths were passed for training, testing, and validation,"
v1.10.1,they should be stored as study-level attributes
v1.10.1,Check a model param is optimized
v1.10.1,Check a loss param is optimized
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,docstr-coverage: inherited
v1.10.1,check if within 0.5 std of observed
v1.10.1,test error is raised
v1.10.1,there is an extra test for this case
v1.10.1,docstr-coverage: inherited
v1.10.1,same size tensors
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,Tests that exception will be thrown when more than or less than two tensors are passed
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,create broadcastable shapes
v1.10.1,check correct value range
v1.10.1,check maximum norm constraint
v1.10.1,unchanged values for small norms
v1.10.1,random entity embeddings & projections
v1.10.1,random relation embeddings & projections
v1.10.1,project
v1.10.1,check shape:
v1.10.1,check normalization
v1.10.1,check equivalence of re-formulation
v1.10.1,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.10.1,= r_p (e_p^T e) + e'
v1.10.1,"create random array, estimate the costs of addition, and measure some execution times."
v1.10.1,"then, compute correlation between the estimated cost, and the measured time."
v1.10.1,check for strong correlation between estimated costs and measured execution time
v1.10.1,get optimal sequence
v1.10.1,check caching
v1.10.1,get optimal sequence
v1.10.1,check correct cost
v1.10.1,check optimality
v1.10.1,compare result to sequential addition
v1.10.1,compare result to sequential addition
v1.10.1,ensure each node participates in at least one edge
v1.10.1,check type and shape
v1.10.1,number of colors is monotonically increasing
v1.10.1,ensure each node participates in at least one edge
v1.10.1,normalize
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,equal value; larger is better
v1.10.1,equal value; smaller is better
v1.10.1,larger is better; improvement
v1.10.1,larger is better; improvement; but not significant
v1.10.1,assert that reporting another metric for this epoch raises an error
v1.10.1,: The window size used by the early stopper
v1.10.1,: The (zeroed) index  - 1 at which stopping will occur
v1.10.1,: The minimum improvement
v1.10.1,: The random seed to use for reproducibility
v1.10.1,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.10.1,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.10.1,: The batch size to use.
v1.10.1,Fix seed for reproducibility
v1.10.1,Set automatic_memory_optimization to false during testing
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,See https://github.com/pykeen/pykeen/pull/883
v1.10.1,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.10.1,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.10.1,"first triple, and passing create_inverse_triples=True."
v1.10.1,check for warning
v1.10.1,check for filtered triples
v1.10.1,check for correct inverse triples flag
v1.10.1,check correct translation
v1.10.1,check column order
v1.10.1,apply restriction
v1.10.1,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.10.1,check that inverse_triples is correctly carried over
v1.10.1,verify that the label-to-ID mapping has not been changed
v1.10.1,verify that triples have been filtered
v1.10.1,Test different combinations of restrictions
v1.10.1,check compressed triples
v1.10.1,reconstruct triples from compressed form
v1.10.1,check data loader
v1.10.1,set create inverse triple to true
v1.10.1,split factory
v1.10.1,check that in *training* inverse triple are to be created
v1.10.1,check that in all other splits no inverse triples are to be created
v1.10.1,verify that all entities and relations are present in the training factory
v1.10.1,verify that no triple got lost
v1.10.1,verify that the label-to-id mappings match
v1.10.1,Slightly larger number of triples to guarantee split can find coverage of all entities and relations.
v1.10.1,serialize
v1.10.1,de-serialize
v1.10.1,check for equality
v1.10.1,TODO: this could be (Core)TriplesFactory.__equal__
v1.10.1,cf. https://docs.pytest.org/en/7.1.x/example/parametrize.html#parametrizing-conditional-raising
v1.10.1,wrong ndim
v1.10.1,wrong last dim
v1.10.1,wrong dtype: float
v1.10.1,wrong dtype: complex
v1.10.1,correct
v1.10.1,>>> positional argument
v1.10.1,mapped_triples
v1.10.1,triples factory
v1.10.1,labeled triples + factory
v1.10.1,single labeled triple
v1.10.1,multiple labeled triples as list
v1.10.1,multiple labeled triples as array
v1.10.1,>>> keyword only
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"DummyModel,"
v1.10.1,3x batch norm: bias + scale --> 6
v1.10.1,entity specific bias        --> 1
v1.10.1,==================================
v1.10.1,7
v1.10.1,"two bias terms, one conv-filter"
v1.10.1,Two linear layer biases
v1.10.1,"Two BN layers, bias & scale"
v1.10.1,Test that the weight in the MLP is trainable (i.e. requires grad)
v1.10.1,quaternion have four components
v1.10.1,entity embeddings
v1.10.1,relation embeddings
v1.10.1,Compute Scores
v1.10.1,Use different dimension for relation embedding: relation_dim > entity_dim
v1.10.1,relation embeddings
v1.10.1,Compute Scores
v1.10.1,Use different dimension for relation embedding: relation_dim < entity_dim
v1.10.1,entity embeddings
v1.10.1,relation embeddings
v1.10.1,Compute Scores
v1.10.1,: 2xBN (bias & scale)
v1.10.1,the combination bias
v1.10.1,FIXME definitely a type mismatch going on here
v1.10.1,check shape
v1.10.1,check content
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,empty lists are falsy
v1.10.1,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.10.1,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.10.1,pipeline is checked.
v1.10.1,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.10.1,Resume the previous pipeline
v1.10.1,The MockModel gives the highest score to the highest entity id
v1.10.1,The test triples are created to yield the third highest score on both head and tail prediction
v1.10.1,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.10.1,These triples are created to yield the highest score on both head and tail prediction for the
v1.10.1,test triple at hand
v1.10.1,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.10.1,test triple at hand
v1.10.1,cf. https://github.com/pykeen/pykeen/issues/1118
v1.10.1,save a reference to the old init *before* mocking
v1.10.1,run a small pipline
v1.10.1,use sampled training loop ...
v1.10.1,... without explicitly selecting a negative sampler ...
v1.10.1,... but providing custom kwargs
v1.10.1,other parameters for fast test
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.10.1,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.10.1,Test looking up inverse triples
v1.10.1,test new label to ID
v1.10.1,type
v1.10.1,old labels
v1.10.1,"new, compact IDs"
v1.10.1,test vectorized lookup
v1.10.1,type
v1.10.1,shape
v1.10.1,value range
v1.10.1,only occurring Ids get mapped to non-negative numbers
v1.10.1,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.10.1,check type
v1.10.1,check shape
v1.10.1,check content
v1.10.1,check type
v1.10.1,check shape
v1.10.1,check 1-hot
v1.10.1,check type
v1.10.1,check shape
v1.10.1,check value range
v1.10.1,check self-similarity = 1
v1.10.1,base relation
v1.10.1,exact duplicate
v1.10.1,99% duplicate
v1.10.1,50% duplicate
v1.10.1,exact inverse
v1.10.1,99% inverse
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The expected number of entities
v1.10.1,: The expected number of relations
v1.10.1,: The expected number of triples
v1.10.1,": The tolerance on expected number of triples, for randomized situations"
v1.10.1,: The dataset to test
v1.10.1,: The instantiated dataset
v1.10.1,: Should the validation be assumed to have been loaded with train/test?
v1.10.1,Not loaded
v1.10.1,Load
v1.10.1,Test caching
v1.10.1,assert (end - start) < 1.0e-02
v1.10.1,Test consistency of training / validation / testing mapping
v1.10.1,": The directory, if there is caching"
v1.10.1,: The batch size
v1.10.1,: The number of negatives per positive for sLCWA training loop.
v1.10.1,: The number of entities LCWA training loop / label smoothing.
v1.10.1,test reduction
v1.10.1,test finite loss value
v1.10.1,Test backward
v1.10.1,negative scores decreased compared to positive ones
v1.10.1,negative scores decreased compared to positive ones
v1.10.1,: The number of entities.
v1.10.1,: The number of negative samples
v1.10.1,: The number of entities.
v1.10.1,"the relative tolerance for checking close results, cf. torch.allclose"
v1.10.1,"the absolute tolerance for checking close results, cf. torch.allclose"
v1.10.1,: The equivalence for models with batch norm only holds in evaluation mode
v1.10.1,: The equivalence for models with batch norm only holds in evaluation mode
v1.10.1,: The equivalence for models with batch norm only holds in evaluation mode
v1.10.1,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.10.1,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.10.1,test multiple different initializations
v1.10.1,calculate by functional
v1.10.1,calculate manually
v1.10.1,allclose checks: | input - other | < atol + rtol * |other|
v1.10.1,simple
v1.10.1,nested
v1.10.1,nested
v1.10.1,prepare a temporary test directory
v1.10.1,check that file was created
v1.10.1,make sure to close file before trying to delete it
v1.10.1,delete intermediate files
v1.10.1,: The batch size
v1.10.1,: The device
v1.10.1,move test instance to device
v1.10.1,Use RESCAL as it regularizes multiple tensors of different shape.
v1.10.1,"verify that the regularizer is stored for both, entity and relation representations"
v1.10.1,Forward pass (should update regularizer)
v1.10.1,Call post_parameter_update (should reset regularizer)
v1.10.1,Check if regularization term is reset
v1.10.1,regularization term should be zero
v1.10.1,updated should be set to false
v1.10.1,call method
v1.10.1,generate random tensors
v1.10.1,generate inputs
v1.10.1,call update
v1.10.1,check shape
v1.10.1,check result
v1.10.1,generate single random tensor
v1.10.1,calculate penalty
v1.10.1,check shape
v1.10.1,check value
v1.10.1,update term
v1.10.1,check that the expected term is returned
v1.10.1,check that the regularizer is now reset
v1.10.1,create another instance with apply_only_once enabled
v1.10.1,test initial state
v1.10.1,"after first update, should change the term"
v1.10.1,"after second update, no change should happen"
v1.10.1,FIXME isn't any finite number allowed now?
v1.10.1,: Additional arguments passed to the training loop's constructor method
v1.10.1,: The triples factory instance
v1.10.1,: The batch size for use for forward_* tests
v1.10.1,: The embedding dimensionality
v1.10.1,: Whether to create inverse triples (needed e.g. by ConvE)
v1.10.1,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.10.1,: The batch size for use when testing training procedures
v1.10.1,: The number of epochs to train the model
v1.10.1,: A random number generator from torch
v1.10.1,: The number of parameters which receive a constant (i.e. non-randomized)
v1.10.1,initialization
v1.10.1,: Static extras to append to the CLI
v1.10.1,: the model's device
v1.10.1,: the inductive mode
v1.10.1,for reproducible testing
v1.10.1,insert shared parameters
v1.10.1,move model to correct device
v1.10.1,Check that all the parameters actually require a gradient
v1.10.1,Try to initialize an optimizer
v1.10.1,get model parameters
v1.10.1,re-initialize
v1.10.1,check that the operation works in-place
v1.10.1,check that the parameters where modified
v1.10.1,check for finite values by default
v1.10.1,check whether a gradient can be back-propgated
v1.10.1,TODO: look into score_r for inverse relations
v1.10.1,clear buffers for message passing models
v1.10.1,"For the high/low memory test cases of NTN, SE, etc."
v1.10.1,"else, leave to default"
v1.10.1,Make sure that inverse triples are created if create_inverse_triples=True
v1.10.1,triples factory is added by the pipeline
v1.10.1,TODO: Catch HolE MKL error?
v1.10.1,set regularizer term to something that isn't zero
v1.10.1,call post_parameter_update
v1.10.1,assert that the regularization term has been reset
v1.10.1,do one optimization step
v1.10.1,call post_parameter_update
v1.10.1,check model constraints
v1.10.1,Distance-based model
v1.10.1,dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)
v1.10.1,check type
v1.10.1,check shape
v1.10.1,create a new instance with guaranteed dropout
v1.10.1,set to training mode
v1.10.1,check for different output
v1.10.1,use more samples to make sure that enough values can be dropped
v1.10.1,this implicitly tests extra_repr / iter_extra_repr
v1.10.1,select random indices
v1.10.1,forward pass with full graph
v1.10.1,forward pass with restricted graph
v1.10.1,verify the results are similar
v1.10.1,: The number of entities
v1.10.1,: The number of triples
v1.10.1,: the message dim
v1.10.1,TODO: separation message vs. entity dim?
v1.10.1,check shape
v1.10.1,check dtype
v1.10.1,check finite values (e.g. due to division by zero)
v1.10.1,check non-negativity
v1.10.1,: the input dimension
v1.10.1,: the output dimension
v1.10.1,: the number of entities
v1.10.1,: the shape of the tensor to initialize
v1.10.1,: to be initialized / set in subclass
v1.10.1,: the interaction to use for testing a model
v1.10.1,initializers *may* work in-place => clone
v1.10.1,actual number may be different...
v1.10.1,unfavourable split to ensure that cleanup is necessary
v1.10.1,check for unclean split
v1.10.1,check that no triple got lost
v1.10.1,check that triples where only moved from other to reference
v1.10.1,check that all entities occur in reference
v1.10.1,check that no triple got lost
v1.10.1,check that all entities are covered in first part
v1.10.1,the model
v1.10.1,Settings
v1.10.1,Use small model (untrained)
v1.10.1,Get batch
v1.10.1,Compute scores
v1.10.1,Compute mask only if required
v1.10.1,TODO: Re-use filtering code
v1.10.1,"shape: (batch_size, num_triples)"
v1.10.1,"shape: (batch_size, num_entities)"
v1.10.1,Process one batch
v1.10.1,shape
v1.10.1,value range
v1.10.1,no duplicates
v1.10.1,shape
v1.10.1,value range
v1.10.1,no duplicates
v1.10.1,shape
v1.10.1,value range
v1.10.1,"no repetition, except padding idx"
v1.10.1,inferred from triples factory
v1.10.1,: The batch size
v1.10.1,: the maximum number of candidates
v1.10.1,: the number of ranks
v1.10.1,: the number of samples to use for monte-carlo estimation
v1.10.1,: the number of candidates for each individual ranking task
v1.10.1,: the ranks for each individual ranking task
v1.10.1,data type
v1.10.1,value range
v1.10.1,original ranks
v1.10.1,better ranks
v1.10.1,variances are non-negative
v1.10.1,generate random weights such that sum = n
v1.10.1,for sanity checking: give the largest weight to best rank => should improve
v1.10.1,generate two versions
v1.10.1,1. repeat each rank/candidate pair a random number of times
v1.10.1,"2. do not repeat, but assign a corresponding weight"
v1.10.1,check flatness
v1.10.1,"TODO: does this suffice, or do we really need float as datatype?"
v1.10.1,generate random triples factories
v1.10.1,generate random alignment
v1.10.1,add label information if necessary
v1.10.1,prepare alignment data frame
v1.10.1,call
v1.10.1,check
v1.10.1,: The window size used by the early stopper
v1.10.1,: The mock losses the mock evaluator will return
v1.10.1,: The (zeroed) index  - 1 at which stopping will occur
v1.10.1,: The minimum improvement
v1.10.1,: The best results
v1.10.1,Set automatic_memory_optimization to false for tests
v1.10.1,Step early stopper
v1.10.1,check storing of results
v1.10.1,not needed for test
v1.10.1,verify that the input is valid
v1.10.1,combine
v1.10.1,verify shape
v1.10.1,to be initialized in subclass
v1.10.1,no column has been removed
v1.10.1,all old columns are unmodified
v1.10.1,new columns are boolean
v1.10.1,no columns have been added
v1.10.1,check subset relation
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO: this could be shared with the model tests
v1.10.1,"FixedModel: dict(embedding_dim=EMBEDDING_DIM),"
v1.10.1,test combinations of models with training loops
v1.10.1,some models require inverse relations
v1.10.1,some model require access to the training triples
v1.10.1,"inductive models require an inductive mode to be set, and an inference factory to be passed"
v1.10.1,fake an inference factory
v1.10.1,automatically choose accelerator
v1.10.1,defaults to TensorBoard; explicitly disabled here
v1.10.1,disable checkpointing
v1.10.1,fast run
v1.10.1,automatically choose accelerator
v1.10.1,defaults to TensorBoard; explicitly disabled here
v1.10.1,disable checkpointing
v1.10.1,fast run
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,check for finite values by default
v1.10.1,Set into training mode to check if it is correctly set to evaluation mode.
v1.10.1,Set into training mode to check if it is correctly set to evaluation mode.
v1.10.1,Set into training mode to check if it is correctly set to evaluation mode.
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,≈ result of softmax
v1.10.1,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.10.1,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.10.1,sum over the softmax dim as weights sum up to 1
v1.10.1,"pos_distances = [0., 0., 0.5, 0.5]"
v1.10.1,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.10.1,≈ result of sigmoid
v1.10.1,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.10.1,expected_loss ≈ 0.34
v1.10.1,abstract classes
v1.10.1,Create dummy dense labels
v1.10.1,Check if labels form a probability distribution
v1.10.1,Apply label smoothing
v1.10.1,Check if smooth labels form probability distribution
v1.10.1,Create dummy sLCWA labels
v1.10.1,Apply label smoothing
v1.10.1,generate random ratios
v1.10.1,check size
v1.10.1,check value range
v1.10.1,check total split
v1.10.1,check consistency with ratios
v1.10.1,the number of decimal digits equivalent to 1 / n_total
v1.10.1,check type
v1.10.1,check values
v1.10.1,compare against expected
v1.10.1,generated_triples = generate_triples()
v1.10.1,check type
v1.10.1,check format
v1.10.1,check coverage
v1.10.1,prediction post-processing
v1.10.1,mock prediction data frame
v1.10.1,score consumers
v1.10.1,"use a small model, since operation is expensive"
v1.10.1,"all scores, automatic batch size"
v1.10.1,top 3 scores
v1.10.1,"top 3 scores, fixed batch size, head scoring"
v1.10.1,"all scores, relation scoring"
v1.10.1,"all scores, relation scoring"
v1.10.1,model with inverse relations
v1.10.1,check type
v1.10.1,check shape
v1.10.1,check ID ranges
v1.10.1,"mapped triples, automatic batch size selection, no factory"
v1.10.1,"mapped triples, fixed batch size, no factory"
v1.10.1,labeled triples with factory
v1.10.1,labeled triples as list
v1.10.1,single labeled triple
v1.10.1,model with inverse relations
v1.10.1,"ID-based, no factory"
v1.10.1,string-based + factory
v1.10.1,mixed + factory
v1.10.1,"no restriction, no factory"
v1.10.1,"no restriction, factory"
v1.10.1,"id restriction, no factory ..."
v1.10.1,id restriction with factory
v1.10.1,"comment: we only use id-based input, since the normalization has already been tested"
v1.10.1,create model
v1.10.1,"id-based head/relation/tail prediction, no restriction"
v1.10.1,restriction by list of ints
v1.10.1,tail prediction
v1.10.1,try accessing each element
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"naive implementation, O(n2)"
v1.10.1,check correct output type
v1.10.1,check value range subset
v1.10.1,check value range side
v1.10.1,check columns
v1.10.1,check value range and type
v1.10.1,check value range entity IDs
v1.10.1,check value range entity labels
v1.10.1,check correct type
v1.10.1,check relation_id value range
v1.10.1,check pattern value range
v1.10.1,check confidence value range
v1.10.1,check support value range
v1.10.1,check correct type
v1.10.1,check relation_id value range
v1.10.1,check pattern value range
v1.10.1,check correct type
v1.10.1,check relation_id value range
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,clear
v1.10.1,docstr-coverage: inherited
v1.10.1,assumes deterministic entity to id mapping
v1.10.1,from left_tf
v1.10.1,from right_tf with offset
v1.10.1,docstr-coverage: inherited
v1.10.1,assumes deterministic entity to id mapping
v1.10.1,from left_tf
v1.10.1,from right_tf with offset
v1.10.1,extra-relation
v1.10.1,docstr-coverage: inherited
v1.10.1,assumes deterministic entity to id mapping
v1.10.1,docstr-coverage: inherited
v1.10.1,assumes deterministic entity to id mapping
v1.10.1,from left_tf
v1.10.1,from right_tf with offset
v1.10.1,additional
v1.10.1,verify shape
v1.10.1,verify dtype
v1.10.1,verify number of entities/relations
v1.10.1,verify offsets
v1.10.1,"create old, new pairs"
v1.10.1,simulate merging ids
v1.10.1,only a single pair
v1.10.1,apply
v1.10.1,every key is contained
v1.10.1,value range
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Check minimal statistics
v1.10.1,Check either a github link or author/publication information is given
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO: we could move this part into the interaction module itself
v1.10.1,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.10.1,"prepare conv input (N, C, H, W)"
v1.10.1,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.10.1,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.10.1,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.10.1,Rotate (=Hamilton product in quaternion space).
v1.10.1,"we calculate the scores using the hard-coded formula, instead of utilizing table + einsum"
v1.10.1,"f(h, r, t) = h @ r @ t"
v1.10.1,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.10.1,normalize rotations to unit modulus
v1.10.1,check for unit modulus
v1.10.1,entity embeddings
v1.10.1,relation embeddings
v1.10.1,Compute Scores
v1.10.1,entity embeddings
v1.10.1,relation embeddings
v1.10.1,Compute Scores
v1.10.1,Compute Scores
v1.10.1,-\|R_h h - R_t t\|
v1.10.1,-\|h - t\|
v1.10.1,"Since MuRE has offsets, the scores do not need to negative"
v1.10.1,"We do not need this, since we do not check for functional consistency anyway"
v1.10.1,intra-interaction comparison
v1.10.1,dimension needs to be divisible by num_heads
v1.10.1,FIXME
v1.10.1,multiple
v1.10.1,single
v1.10.1,head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid
v1.10.1,check type
v1.10.1,check size
v1.10.1,check value range
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,message_dim must be divisible by num_heads
v1.10.1,determine pool using anchor searcher
v1.10.1,determine expected pool using shortest path distances via scipy.sparse.csgraph
v1.10.1,generate random pool
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,complex tensor
v1.10.1,check value range
v1.10.1,check modulus == 1
v1.10.1,quaternion needs shape to end on 4
v1.10.1,"check value range (actually [-s, +s] with s = 1/sqrt(2*n))"
v1.10.1,value range
v1.10.1,highest degree node has largest value
v1.10.1,Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf
v1.10.1,create triples with a dummy relation type 0
v1.10.1,"0: green: 2, 3, 7, 8"
v1.10.1,"1: red: 1, 4, 6, 9"
v1.10.1,"2: blue: 0, 5"
v1.10.1,the example includes the first power
v1.10.1,requires at least one complex tensor as input
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,inferred from triples factory
v1.10.1,inferred from assignment
v1.10.1,the representation module infers the max_id from the provided labels
v1.10.1,the following entity does not have an image -> will have to use backfill
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,the representation module infers the max_id from the provided labels
v1.10.1,docstr-coverage: inherited
v1.10.1,the representation module infers the max_id from the provided labels
v1.10.1,max_id is inferred from assignment
v1.10.1,create random assignment
v1.10.1,update kwargs
v1.10.1,empty bases
v1.10.1,inconsistent base shapes
v1.10.1,invalid base id
v1.10.1,invalid local index
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO this is the only place this function is used.
v1.10.1,Is there an alternative so we can remove it?
v1.10.1,ensure positivity
v1.10.1,compute using pytorch
v1.10.1,prepare distributions
v1.10.1,compute using pykeen
v1.10.1,"e: (batch_size, num_heads, num_tails, d)"
v1.10.1,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.10.1,divergence = 0 => similarity = -divergence = 0
v1.10.1,"(h - t), r"
v1.10.1,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.10.1,divergence >= 0 => similarity = -divergence <= 0
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.10.1,filter vs. no filter thing.
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,check for empty batches
v1.10.1,: The window size used by the early stopper
v1.10.1,: The mock losses the mock evaluator will return
v1.10.1,: The (zeroed) index  - 1 at which stopping will occur
v1.10.1,: The minimum improvement
v1.10.1,: The best results
v1.10.1,Set automatic_memory_optimization to false for tests
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Train a model in one shot
v1.10.1,Train a model for the first half
v1.10.1,Continue training of the first part
v1.10.1,check non-empty metrics
v1.10.1,: Should negative samples be filtered?
v1.10.1,expectation = (1 + n) / 2
v1.10.1,variance = (n**2 - 1) / 12
v1.10.1,"x_i ~ N(mu_i, 1)"
v1.10.1,closed-form solution
v1.10.1,sampled confidence interval
v1.10.1,check that closed-form is in confidence interval of sampled
v1.10.1,positive values only
v1.10.1,positive and negative values
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Check for correct class
v1.10.1,check correct num_entities
v1.10.1,check type
v1.10.1,check length
v1.10.1,check type
v1.10.1,check length
v1.10.1,check confidence positivity
v1.10.1,Check for correct class
v1.10.1,check value
v1.10.1,filtering
v1.10.1,"true_score: (2, 3, 3)"
v1.10.1,head based filter
v1.10.1,preprocessing for faster lookup
v1.10.1,check that all found positives are positive
v1.10.1,check in-place
v1.10.1,Test head scores
v1.10.1,Assert in-place modification
v1.10.1,Assert correct filtering
v1.10.1,Test tail scores
v1.10.1,Assert in-place modification
v1.10.1,Assert correct filtering
v1.10.1,The MockModel gives the highest score to the highest entity id
v1.10.1,The test triples are created to yield the third highest score on both head and tail prediction
v1.10.1,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.10.1,These triples are created to yield the highest score on both head and tail prediction for the
v1.10.1,test triple at hand
v1.10.1,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.10.1,test triple at hand
v1.10.1,check true negatives
v1.10.1,TODO: check no repetitions (if possible)
v1.10.1,return type
v1.10.1,columns
v1.10.1,value range
v1.10.1,relation restriction
v1.10.1,with explicit num_entities
v1.10.1,with inferred num_entities
v1.10.1,test different shapes
v1.10.1,test different shapes
v1.10.1,value range
v1.10.1,value range
v1.10.1,check unique
v1.10.1,"strips off the ""k"" at the end"
v1.10.1,Populate with real results.
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"(-1, 1),"
v1.10.1,"(-1, -1),"
v1.10.1,"(-5, -3),"
v1.10.1,initialize
v1.10.1,update with batches
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Check whether filtering works correctly
v1.10.1,First giving an example where all triples have to be filtered
v1.10.1,The filter should remove all triples
v1.10.1,Create an example where no triples will be filtered
v1.10.1,The filter should not remove any triple
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,same relation
v1.10.1,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.10.1,Test that half of the subjects and half of the objects are corrupted
v1.10.1,check that corrupted entities co-occur with the relation in training data
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The batch size
v1.10.1,: The random seed
v1.10.1,: The triples factory
v1.10.1,: The instances
v1.10.1,: A positive batch
v1.10.1,: Kwargs
v1.10.1,Generate negative sample
v1.10.1,check filter shape if necessary
v1.10.1,check shape
v1.10.1,check bounds: heads
v1.10.1,check bounds: relations
v1.10.1,check bounds: tails
v1.10.1,test that the negative triple is not the original positive triple
v1.10.1,"shape: (batch_size, 1, num_neg)"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Base Classes
v1.10.1,Concrete Classes
v1.10.1,Utils
v1.10.1,: synonyms of this loss
v1.10.1,: The default strategy for optimizing the loss's hyper-parameters
v1.10.1,flatten and stack
v1.10.1,apply label smoothing if necessary.
v1.10.1,TODO: Do label smoothing only once
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,Sanity check
v1.10.1,negative_scores have already been filtered in the sampler!
v1.10.1,"shape: (nnz,)"
v1.10.1,docstr-coverage: inherited
v1.10.1,Sanity check
v1.10.1,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.10.1,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.10.1,positive entries in the labels tensor.
v1.10.1,"This shows how often one row has to be repeated,"
v1.10.1,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.10.1,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.10.1,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.10.1,This tells us how often each true label should be repeated
v1.10.1,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.10.1,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.10.1,within the ablation pipeline.
v1.10.1,1. positive & negative margin
v1.10.1,2. negative margin & offset
v1.10.1,3. positive margin & offset
v1.10.1,docstr-coverage: inherited
v1.10.1,Sanity check
v1.10.1,positive term
v1.10.1,implicitly repeat positive scores
v1.10.1,"shape: (nnz,)"
v1.10.1,negative term
v1.10.1,negative_scores have already been filtered in the sampler!
v1.10.1,docstr-coverage: inherited
v1.10.1,Sanity check
v1.10.1,"scale labels from [0, 1] to [-1, 1]"
v1.10.1,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.10.1,within the ablation pipeline.
v1.10.1,docstr-coverage: inherited
v1.10.1,negative_scores have already been filtered in the sampler!
v1.10.1,(dense) softmax requires unfiltered scores / masking
v1.10.1,we need to fill the scores with -inf for all filtered negative examples
v1.10.1,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.10.1,use filled negatives scores
v1.10.1,docstr-coverage: inherited
v1.10.1,we need dense negative scores => unfilter if necessary
v1.10.1,"we may have inf rows, since there will be one additional finite positive score per row"
v1.10.1,"combine scores: shape: (batch_size, num_negatives + 1)"
v1.10.1,use sparse version of cross entropy
v1.10.1,calculate cross entropy loss
v1.10.1,docstr-coverage: inherited
v1.10.1,make sure labels form a proper probability distribution
v1.10.1,calculate cross entropy loss
v1.10.1,docstr-coverage: inherited
v1.10.1,determine positive; do not check with == since the labels are floats
v1.10.1,subtract margin from positive scores
v1.10.1,divide by temperature
v1.10.1,docstr-coverage: inherited
v1.10.1,subtract margin from positive scores
v1.10.1,normalize positive score shape
v1.10.1,divide by temperature
v1.10.1,docstr-coverage: inherited
v1.10.1,determine positive; do not check with == since the labels are floats
v1.10.1,compute negative weights (without gradient tracking)
v1.10.1,clone is necessary since we modify in-place
v1.10.1,Split positive and negative scores
v1.10.1,"we pass *all* scores as negatives, but set the weight of positives to zero"
v1.10.1,this allows keeping a dense shape
v1.10.1,docstr-coverage: inherited
v1.10.1,Sanity check
v1.10.1,"we do not allow full -inf rows, since we compute the softmax over this tensor"
v1.10.1,compute weights (without gradient tracking)
v1.10.1,"fill negative scores with some finite value, e.g., 0 (they will get masked out anyway)"
v1.10.1,note: this is a reduction along the softmax dim; since the weights are already normalized
v1.10.1,"to sum to one, we want a sum reduction here, instead of using the self._reduction"
v1.10.1,docstr-coverage: inherited
v1.10.1,Sanity check
v1.10.1,docstr-coverage: inherited
v1.10.1,Sanity check
v1.10.1,negative loss part
v1.10.1,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.10.1,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.10.1,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO: maybe we can make this more efficient?
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO: maybe we can make this more efficient?
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO: method is_inverse?
v1.10.1,TODO: inverse of inverse?
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,The number of relations stored in the triples factory includes the number of inverse relations
v1.10.1,Id of inverse relation: relation + 1
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.10.1,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.10.1,": For more information, see https://github.com/cthoyt/pystow"
v1.10.1,: A path representing the PyKEEN data folder
v1.10.1,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.10.1,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.10.1,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.10.1,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.10.1,: A subdirectory for PyKEEN logs
v1.10.1,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.10.1,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.10.1,"TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728"
v1.10.1,"SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.10.1,get datatype specific epsilon
v1.10.1,clamp minimum value
v1.10.1,try to resolve ambiguous device; there has to be at least one cuda device
v1.10.1,lower bound
v1.10.1,upper bound
v1.10.1,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.10.1,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.10.1,Determine optimal order and cost
v1.10.1,translate back to original order
v1.10.1,determine optimal processing order
v1.10.1,heuristic
v1.10.1,The dimensions affected by e'
v1.10.1,Project entities
v1.10.1,r_p (e_p.T e) + e'
v1.10.1,Enforce constraints
v1.10.1,TODO delete when deleting _normalize_dim (below)
v1.10.1,TODO delete when deleting convert_to_canonical_shape (below)
v1.10.1,TODO delete? See note in test_sim.py on its only usage
v1.10.1,upgrade to sequence
v1.10.1,broadcast
v1.10.1,"normalize ids: -> ids.shape: (batch_size, num_ids)"
v1.10.1,"normalize batch -> batch.shape: (batch_size, 1, 3)"
v1.10.1,allocate memory
v1.10.1,copy ids
v1.10.1,reshape
v1.10.1,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.10.1,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.10.1,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.10.1,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.10.1,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.10.1,ensure pathlib
v1.10.1,Enforce that sizes are strictly positive by passing through ELU
v1.10.1,Shape vector is normalized using the above helper function
v1.10.1,Size is learned separately and applied to normalized shape
v1.10.1,Compute potential boundaries by applying the shape in substraction
v1.10.1,and in addition
v1.10.1,Compute box upper bounds using min and max respectively
v1.10.1,compute width plus 1
v1.10.1,compute box midpoints
v1.10.1,"TODO: we already had this before, as `base`"
v1.10.1,inside box?
v1.10.1,yes: |p - c| / (w + 1)
v1.10.1,no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))
v1.10.1,Step 1: Apply the other entity bump
v1.10.1,Step 2: Apply tanh if tanh_map is set to True.
v1.10.1,Compute the distance function output element-wise
v1.10.1,"Finally, compute the norm"
v1.10.1,cf. https://stackoverflow.com/a/1176023
v1.10.1,check validity
v1.10.1,path compression
v1.10.1,get representatives
v1.10.1,already merged
v1.10.1,make x the smaller one
v1.10.1,merge
v1.10.1,extract partitions
v1.10.1,resolve path to make sure it is an absolute path
v1.10.1,ensure directory exists
v1.10.1,message passing: collect colors of neighbors
v1.10.1,"dense colors: shape: (n, c)"
v1.10.1,"adj:          shape: (n, n)"
v1.10.1,"values need to be float, since torch.sparse.mm does not support integer dtypes"
v1.10.1,size: will be correctly inferred
v1.10.1,concat with old colors
v1.10.1,hash
v1.10.1,create random indicator functions of low dimensionality
v1.10.1,collect neighbors' colors
v1.10.1,round to avoid numerical effects
v1.10.1,hash first
v1.10.1,concat with old colors
v1.10.1,re-hash
v1.10.1,"only keep connectivity, but remove multiplicity"
v1.10.1,"note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,"
v1.10.1,this is rather useless
v1.10.1,initial: degree
v1.10.1,"note: we calculate this separately, since we can use a more efficient implementation for the first step"
v1.10.1,hash
v1.10.1,determine small integer type for dense count array
v1.10.1,convergence check
v1.10.1,each node has a unique color
v1.10.1,the number of colors did not improve in the last iteration
v1.10.1,cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Base Class
v1.10.1,Child classes
v1.10.1,Utils
v1.10.1,: The overall regularization weight
v1.10.1,: The current regularization term (a scalar)
v1.10.1,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.10.1,: Has this regularizer been updated since last being reset?
v1.10.1,: The default strategy for optimizing the regularizer's hyper-parameters
v1.10.1,"If there are tracked parameters, update based on them"
v1.10.1,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.10.1,docstr-coverage: inherited
v1.10.1,no need to compute anything
v1.10.1,docstr-coverage: inherited
v1.10.1,always return zero
v1.10.1,: The dimension along which to compute the vector-based regularization terms.
v1.10.1,: Whether to normalize the regularization term by the dimension of the vectors.
v1.10.1,: This allows dimensionality-independent weight tuning.
v1.10.1,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.10.1,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.1,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.1,docstr-coverage: inherited
v1.10.1,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.10.1,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.1,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.1,docstr-coverage: inherited
v1.10.1,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.1,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.1,regularizer-specific parameters
v1.10.1,docstr-coverage: inherited
v1.10.1,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.10.1,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.1,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,orthogonality soft constraint: cosine similarity at most epsilon
v1.10.1,The normalization factor to balance individual regularizers' contribution.
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,high-level
v1.10.1,Low-Level
v1.10.1,cf. https://github.com/python/mypy/issues/5374
v1.10.1,": the dataframe; has to have a column named ""score"""
v1.10.1,: an optional factory to use for labeling
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,: the prediction target
v1.10.1,: the other column's fixed IDs
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,": the ID-based triples, shape: (n, 3)"
v1.10.1,: the scores
v1.10.1,3-tuple for return
v1.10.1,"extract label information, if possible"
v1.10.1,no restriction
v1.10.1,restriction is a tensor
v1.10.1,restriction is a sequence of integers or strings
v1.10.1,"now, restriction is a sequence of integers"
v1.10.1,"if explicit ids have been given, and label information is available, extract list of labels"
v1.10.1,exactly one of them is None
v1.10.1,create input batch
v1.10.1,"note type alias annotation required,"
v1.10.1,cf. https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases
v1.10.1,"batch, TODO: ids?"
v1.10.1,docstr-coverage: inherited
v1.10.1,initialize buffer on device
v1.10.1,docstr-coverage: inherited
v1.10.1,"reshape, shape: (batch_size * num_entities,)"
v1.10.1,get top scores within batch
v1.10.1,determine corresponding indices
v1.10.1,"batch_id, score_id = divmod(top_indices, num_scores)"
v1.10.1,combine to top triples
v1.10.1,append to global top scores
v1.10.1,reduce size if necessary
v1.10.1,initialize buffer on cpu
v1.10.1,Explicitly create triples
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO: variable targets across batches/samples?
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"(?, r, t) => r.stride > t.stride"
v1.10.1,"(h, ?, t) => h.stride > t.stride"
v1.10.1,"(h, r, ?) => h.stride > r.stride"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,train model; note: needs larger number of epochs to do something useful ;-)
v1.10.1,"create prediction dataset, where the head entities is from a set of European countries,"
v1.10.1,and the relations are connected to tourism
v1.10.1,"calculate all scores for this restricted set, and keep k=3 largest"
v1.10.1,add labels
v1.10.1,: the choices for the first and second component of the input batch
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,calculate batch scores onces
v1.10.1,consume by all consumers
v1.10.1,TODO: Support partial dataset
v1.10.1,note: the models' predict method takes care of setting the model to evaluation mode
v1.10.1,exactly one of them is None
v1.10.1,
v1.10.1,note: the models' predict method takes care of setting the model to evaluation mode
v1.10.1,get input & target
v1.10.1,get label-to-id mapping and prediction targets
v1.10.1,get scores
v1.10.1,create raw dataframe
v1.10.1,note: the models' predict method takes care of setting the model to evaluation mode
v1.10.1,normalize input
v1.10.1,calculate scores (with automatic memory optimization)
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"""Closed-Form Expectation"","
v1.10.1,"""Closed-Form Variance"","
v1.10.1,"""✓"" if metric.closed_expectation else """","
v1.10.1,"""✓"" if metric.closed_variance else """","
v1.10.1,Add HPO command
v1.10.1,Add NodePiece tokenization command
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,General types
v1.10.1,Triples
v1.10.1,Others
v1.10.1,Tensor Functions
v1.10.1,Tensors
v1.10.1,Dataclasses
v1.10.1,prediction targets
v1.10.1,modes
v1.10.1,entity alignment sides
v1.10.1,: A function that mutates the input and returns a new object of the same type as output
v1.10.1,: A function that can be applied to a tensor to initialize it
v1.10.1,: A function that can be applied to a tensor to normalize it
v1.10.1,: A function that can be applied to a tensor to constrain it
v1.10.1,: A hint for a :class:`torch.device`
v1.10.1,: A hint for a :class:`torch.Generator`
v1.10.1,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.10.1,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.10.1,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.10.1,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.10.1,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.10.1,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.10.1,: the inductive prediction and training mode
v1.10.1,: the prediction target
v1.10.1,: the prediction target index
v1.10.1,: the rank types
v1.10.1,"RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8"
v1.10.1,entity alignment
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,infer shape
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,input normalization
v1.10.1,note: the base class does not have any parameters
v1.10.1,Heuristic for default value
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"note: the only parameters are inside the relation representation module, which has its own reset_parameters"
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO: can we change the dimension order to make this contiguous?
v1.10.1,docstr-coverage: inherited
v1.10.1,normalize num blocks
v1.10.1,determine necessary padding
v1.10.1,determine block sizes
v1.10.1,"(R, nb, bsi, bso)"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,apply padding if necessary
v1.10.1,"(n, di) -> (n, nb, bsi)"
v1.10.1,"(n, nb, bsi), (R, nb, bsi, bso) -> (R, n, nb, bso)"
v1.10.1,"(R, n, nb, bso) -> (R * n, do)"
v1.10.1,"note: depending on the contracting order, the output may supporting viewing, or not"
v1.10.1,"(n, R * n), (R * n, do) -> (n, do)"
v1.10.1,remove padding if necessary
v1.10.1,docstr-coverage: inherited
v1.10.1,apply padding if necessary
v1.10.1,"(R * n, n), (n, di) -> (R * n, di)"
v1.10.1,"(R * n, di) -> (R, n, nb, bsi)"
v1.10.1,"(R, nb, bsi, bso), (R, n, nb, bsi) -> (n, nb, bso)"
v1.10.1,"(n, nb, bso) -> (n, do)"
v1.10.1,"note: depending on the contracting order, the output may supporting viewing, or not"
v1.10.1,remove padding if necessary
v1.10.1,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501
v1.10.1,there are separate decompositions for forward and backward relations.
v1.10.1,the self-loop weight is not decomposed.
v1.10.1,TODO: we could cache the stacked adjacency matrices
v1.10.1,self-loop
v1.10.1,forward messages
v1.10.1,backward messages
v1.10.1,activation
v1.10.1,input validation
v1.10.1,has to be imported now to avoid cyclic imports
v1.10.1,has to be assigned after call to nn.Module init
v1.10.1,Resolve edge weighting
v1.10.1,dropout
v1.10.1,"Save graph using buffers, such that the tensors are moved together with the model"
v1.10.1,no activation on last layer
v1.10.1,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501
v1.10.1,buffering of enriched representations
v1.10.1,docstr-coverage: inherited
v1.10.1,invalidate enriched embeddings
v1.10.1,docstr-coverage: inherited
v1.10.1,Bind fields
v1.10.1,"shape: (num_entities, embedding_dim)"
v1.10.1,Edge dropout: drop the same edges on all layers (only in training mode)
v1.10.1,Get random dropout mask
v1.10.1,Apply to edges
v1.10.1,fixed edges -> pre-compute weights
v1.10.1,Cache enriched representations
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Utils
v1.10.1,: the maximum ID (exclusively)
v1.10.1,: the shape of an individual representation
v1.10.1,: a normalizer for individual representations
v1.10.1,: a regularizer for individual representations
v1.10.1,: dropout
v1.10.1,heuristic
v1.10.1,normalize *before* repeating
v1.10.1,repeat if necessary
v1.10.1,regularize *after* repeating
v1.10.1,"dropout & regularizer will appear automatically, since it is a nn.Module"
v1.10.1,has to be imported here to avoid cyclic import
v1.10.1,docstr-coverage: inherited
v1.10.1,normalize num_embeddings vs. max_id
v1.10.1,normalize embedding_dim vs. shape
v1.10.1,work-around until full complex support (torch==1.10 still does not work)
v1.10.1,TODO: verify that this is our understanding of complex!
v1.10.1,"note: this seems to work, as finfo returns the datatype of the underlying floating"
v1.10.1,"point dtype, rather than the combined complex one"
v1.10.1,"use make for initializer since there's a default, and make_safe"
v1.10.1,for the others to pass through None values
v1.10.1,docstr-coverage: inherited
v1.10.1,initialize weights in-place
v1.10.1,docstr-coverage: inherited
v1.10.1,apply constraints in-place
v1.10.1,fixme: work-around until nn.Embedding supports complex
v1.10.1,docstr-coverage: inherited
v1.10.1,fixme: work-around until nn.Embedding supports complex
v1.10.1,verify that contiguity is preserved
v1.10.1,create low-rank approximation object
v1.10.1,"get base representations, shape: (n, *ds)"
v1.10.1,"calculate SVD, U.shape: (n, k), s.shape: (k,), u.shape: (k, prod(ds))"
v1.10.1,overwrite bases and weights
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"get all base representations, shape: (num_bases, *shape)"
v1.10.1,"get base weights, shape: (*batch_dims, num_bases)"
v1.10.1,"weighted linear combination of bases, shape: (*batch_dims, *shape)"
v1.10.1,normalize output dimension
v1.10.1,entity-relation composition
v1.10.1,edge weighting
v1.10.1,message passing weights
v1.10.1,linear relation transformation
v1.10.1,layer-specific self-loop relation representation
v1.10.1,other components
v1.10.1,initialize
v1.10.1,split
v1.10.1,compose
v1.10.1,transform
v1.10.1,normalization
v1.10.1,aggregate by sum
v1.10.1,dropout
v1.10.1,prepare for inverse relations
v1.10.1,update entity representations: mean over self-loops / forward edges / backward edges
v1.10.1,Relation transformation
v1.10.1,has to be imported here to avoid cyclic imports
v1.10.1,kwargs
v1.10.1,Buffered enriched entity and relation representations
v1.10.1,TODO: Check
v1.10.1,TODO: might not be true for all compositions
v1.10.1,hidden dimension normalization
v1.10.1,Create message passing layers
v1.10.1,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.10.1,TODO: This always uses all training triples for message passing
v1.10.1,initialize buffer of enriched representations
v1.10.1,docstr-coverage: inherited
v1.10.1,invalidate enriched embeddings
v1.10.1,docstr-coverage: inherited
v1.10.1,"when changing from evaluation to training mode, the buffered representations have been computed without"
v1.10.1,"gradient tracking. hence, we need to invalidate them."
v1.10.1,note: this occurs in practice when continuing training after evaluation.
v1.10.1,enrich
v1.10.1,docstr-coverage: inherited
v1.10.1,check max_id
v1.10.1,infer shape
v1.10.1,"assign after super, since they should be properly registered as submodules"
v1.10.1,docstr-coverage: inherited
v1.10.1,: the base representations
v1.10.1,: the combination module
v1.10.1,input normalization
v1.10.1,has to be imported here to avoid cyclic import
v1.10.1,create base representations
v1.10.1,verify same ID range
v1.10.1,"note: we could also relax the requiremen, and set max_id = min(max_ids)"
v1.10.1,shape inference
v1.10.1,assign base representations *after* super init
v1.10.1,docstr-coverage: inherited
v1.10.1,delegate to super class
v1.10.1,Generate graph dataset from the Monarch Disease Ontology (MONDO)
v1.10.1,": the assignment from global ID to (representation, local id), shape: (max_id, 2)"
v1.10.1,import here to avoid cyclic import
v1.10.1,instantiate base representations if necessary
v1.10.1,there needs to be at least one base
v1.10.1,"while possible, this might be unintended"
v1.10.1,extract shape
v1.10.1,check for invalid base ids
v1.10.1,check for invalid local indices
v1.10.1,assign modules / buffers *after* super init
v1.10.1,docstr-coverage: inherited
v1.10.1,flatten assignment to ease construction of inverse indices
v1.10.1,we group indices by the representation which provides them
v1.10.1,"thus, we need an inverse to restore the correct order"
v1.10.1,get representations
v1.10.1,update inverse indices
v1.10.1,invert flattening
v1.10.1,import here to avoid cyclic import
v1.10.1,comment: not all representations support passing a shape parameter
v1.10.1,create assignment
v1.10.1,base
v1.10.1,other
v1.10.1,import here to avoid cyclic import
v1.10.1,infer shape
v1.10.1,infer max_id
v1.10.1,docstr-coverage: inherited
v1.10.1,"TODO: can be a combined representations, with appropriate tensor-train combination"
v1.10.1,": shape: (max_id, num_cores)"
v1.10.1,": the bases, length: num_cores, with compatible shapes"
v1.10.1,check shape
v1.10.1,check value range
v1.10.1,"do not increase counter i, since the dimension is shared with the following term"
v1.10.1,i += 1
v1.10.1,ids //= m_i
v1.10.1,import here to avoid cyclic import
v1.10.1,normalize ranks
v1.10.1,"determine M_k, N_k"
v1.10.1,TODO: allow to pass them from outside?
v1.10.1,normalize assignment
v1.10.1,determine shapes and einsum equation
v1.10.1,create base representations
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,abstract
v1.10.1,concrete classes
v1.10.1,default flow
v1.10.1,: the message passing layers
v1.10.1,: the flow direction of messages across layers
v1.10.1,": the edge index, shape: (2, num_edges)"
v1.10.1,fail if dependencies are missing
v1.10.1,avoid cyclic import
v1.10.1,"the base representations, e.g., entity embeddings or features"
v1.10.1,verify max_id
v1.10.1,verify shape
v1.10.1,assign sub-module *after* super call
v1.10.1,initialize layers
v1.10.1,normalize activation
v1.10.1,check consistency
v1.10.1,buffer edge index for message passing
v1.10.1,TODO: inductiveness; we need to
v1.10.1,* replace edge_index
v1.10.1,* replace base representations
v1.10.1,* keep layers & activations
v1.10.1,docstr-coverage: inherited
v1.10.1,we can restrict the message passing to the k-hop neighborhood of the desired indices;
v1.10.1,this does only make sense if we do not request *all* indices
v1.10.1,k_hop_subgraph returns:
v1.10.1,(1) the nodes involved in the subgraph
v1.10.1,(2) the filtered edge_index connectivity
v1.10.1,"(3) the mapping from node indices in node_idx to their new location, and"
v1.10.1,(4) the edge mask indicating which edges were preserved
v1.10.1,we only need the base representations for the neighbor indices
v1.10.1,get *all* base representations
v1.10.1,use *all* edges
v1.10.1,perform message passing
v1.10.1,select desired indices
v1.10.1,docstr-coverage: inherited
v1.10.1,": the edge type, shape: (num_edges,)"
v1.10.1,register an additional buffer for the categorical edge type
v1.10.1,docstr-coverage: inherited
v1.10.1,: the relation representations used to obtain initial edge features
v1.10.1,avoid cyclic import
v1.10.1,docstr-coverage: inherited
v1.10.1,get initial relation representations
v1.10.1,select edge attributes from relation representations according to relation type
v1.10.1,perform message passing
v1.10.1,"apply relation transformation, if necessary"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Classes
v1.10.1,Resolver
v1.10.1,backwards compatibility
v1.10.1,scaling factor
v1.10.1,"modulus ~ Uniform[-s, s]"
v1.10.1,"phase ~ Uniform[0, 2*pi]"
v1.10.1,real part
v1.10.1,purely imaginary quaternions unitary
v1.10.1,this is usually loaded from somewhere else
v1.10.1,"the shape must match, as well as the entity-to-id mapping"
v1.10.1,must be cloned if we want to do backprop
v1.10.1,the color initializer
v1.10.1,variants for the edge index
v1.10.1,additional parameters for iter_weisfeiler_lehman
v1.10.1,normalize shape
v1.10.1,get coloring
v1.10.1,make color initializer
v1.10.1,initialize color representations
v1.10.1,note: this could be a representation?
v1.10.1,init entity representations according to the color
v1.10.1,create random walk matrix
v1.10.1,stack diagonal entries of powers of rw
v1.10.1,abstract
v1.10.1,concrete
v1.10.1,docstr-coverage: inherited
v1.10.1,tokenize
v1.10.1,pad
v1.10.1,get character embeddings
v1.10.1,pool
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: whether the edge weighting needs access to the message
v1.10.1,stub init to enable arbitrary arguments in subclasses
v1.10.1,"Calculate in-degree, i.e. number of incoming edges"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,backward compatibility with RGCN
v1.10.1,docstr-coverage: inherited
v1.10.1,view for heads
v1.10.1,"compute attention coefficients, shape: (num_edges, num_heads)"
v1.10.1,"TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Caches
v1.10.1,"if the sparsity becomes too low, convert to a dense matrix"
v1.10.1,"note: this heuristic is based on the memory consumption,"
v1.10.1,"for a sparse matrix, we store 3 values per nnz (row index, column index, value)"
v1.10.1,"performance-wise, it likely makes sense to switch even earlier"
v1.10.1,`torch.sparse.mm` can also deal with dense 2nd argument
v1.10.1,note: torch.sparse.mm only works for COO matrices;
v1.10.1,@ only works for CSR matrices
v1.10.1,"convert to COO, if necessary"
v1.10.1,"we need to use indices here, since there may be zero diagonal entries"
v1.10.1,: Wikidata SPARQL endpoint. See https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service#Interfacing
v1.10.1,cf. https://meta.wikimedia.org/wiki/User-Agent_policy
v1.10.1,cf. https://wikitech.wikimedia.org/wiki/Robot_policy
v1.10.1,break into smaller requests
v1.10.1,try to load cached first
v1.10.1,determine missing entries
v1.10.1,retrieve information via SPARQL
v1.10.1,save entries
v1.10.1,fill missing descriptions
v1.10.1,for mypy
v1.10.1,get labels & descriptions
v1.10.1,compose labels
v1.10.1,we can have multiple images per entity -> collect image URLs per image
v1.10.1,entity ID
v1.10.1,relation ID
v1.10.1,image URL
v1.10.1,check whether images are still missing
v1.10.1,select on image url per image in a reproducible way
v1.10.1,traverse relations in order of preference
v1.10.1,now there is an image available -> select reproducible by URL sorting
v1.10.1,did not break -> no image
v1.10.1,This import doesn't need a wrapper since it's a transitive
v1.10.1,requirement of PyOBO
v1.10.1,darglint does not like
v1.10.1,"raise cls(shape=shape, reference=reference)"
v1.10.1,1 * ? = ?; ? * 1 = ?
v1.10.1,i**2 = j**2 = k**2 = -1
v1.10.1,i * j = k; i * k = -j
v1.10.1,"j * i = -k, j * k = i"
v1.10.1,k * i = j; k * j = -i
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO test
v1.10.1,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.10.1,: a = \mu^T\Sigma^{-1}\mu
v1.10.1,: b = \log \det \Sigma
v1.10.1,1. Component
v1.10.1,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.10.1,2. Component
v1.10.1,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.10.1,with mu = (mu_1 - mu_0)
v1.10.1,= mu * Sigma_1^-1 mu
v1.10.1,since Sigma_1 is diagonal
v1.10.1,= mu**2 / sigma_1
v1.10.1,3. Component
v1.10.1,4. Component
v1.10.1,ln (det(\Sigma_1) / det(\Sigma_0))
v1.10.1,= ln det Sigma_1 - ln det Sigma_0
v1.10.1,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.10.1,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.10.1,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.10.1,allocate result
v1.10.1,prepare distributions
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO benchmark
v1.10.1,TODO benchmark
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,REPRESENTATION
v1.10.1,base
v1.10.1,concrete
v1.10.1,INITIALIZER
v1.10.1,INTERACTIONS
v1.10.1,Adapter classes
v1.10.1,Concrete Classes
v1.10.1,combinations
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Base Classes
v1.10.1,Adapter classes
v1.10.1,Concrete Classes
v1.10.1,normalize input
v1.10.1,get number of head/relation/tail representations
v1.10.1,flatten list
v1.10.1,split tensors
v1.10.1,broadcasting
v1.10.1,yield batches
v1.10.1,complex typing
v1.10.1,: The symbolic shapes for entity representations
v1.10.1,": The symbolic shapes for entity representations for tail entities, if different."
v1.10.1,": Otherwise, the entity_shape is used for head & tail entities"
v1.10.1,: The symbolic shapes for relation representations
v1.10.1,if the interaction function's head parameter should only receive a subset of entity representations
v1.10.1,if the interaction function's tail parameter should only receive a subset of entity representations
v1.10.1,: the interaction's value range (for unrestricted input)
v1.10.1,"TODO: annotate modelling capabilities? cf., e.g., https://arxiv.org/abs/1902.10197, Table 2"
v1.10.1,"TODO: annotate properties, e.g., symmetry, and use them for testing?"
v1.10.1,TODO: annotate complexity?
v1.10.1,"TODO: cannot cover dynamic shapes, e.g., AutoSF"
v1.10.1,"TODO: we could change that to slicing along multiple dimensions, if necessary"
v1.10.1,: The functional interaction form
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO: update class docstring
v1.10.1,TODO: give this a better name?
v1.10.1,Store initial input for error message
v1.10.1,All are None -> try and make closest to square
v1.10.1,Only input channels is None
v1.10.1,Only width is None
v1.10.1,Only height is none
v1.10.1,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.10.1,Width and input channels are None -> set input channels to 1 and calculate width
v1.10.1,vector & scalar offset
v1.10.1,": The head-relation encoder operating on 2D ""images"""
v1.10.1,: The head-relation encoder operating on the 1D flattened version
v1.10.1,: The interaction function
v1.10.1,Automatic calculation of remaining dimensions
v1.10.1,Parameter need to fulfil:
v1.10.1,input_channels * embedding_height * embedding_width = embedding_dim
v1.10.1,normalize kernel height
v1.10.1,encoders
v1.10.1,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.10.1,"2: 1D encoder: FC, DO, BN?, Act"
v1.10.1,store reshaping dimensions
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,The interaction model
v1.10.1,docstr-coverage: inherited
v1.10.1,Use Xavier initialization for weight; bias to zero
v1.10.1,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.10.1,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.10.1,docstr-coverage: inherited
v1.10.1,normalize hidden_dim
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,Initialize biases with zero
v1.10.1,"In the original formulation,"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,Global entity projection
v1.10.1,Global relation projection
v1.10.1,Global combination bias
v1.10.1,Global combination bias
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,default core tensor initialization
v1.10.1,cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12
v1.10.1,normalize initializer
v1.10.1,normalize relation dimension
v1.10.1,Core tensor
v1.10.1,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.10.1,Dropout
v1.10.1,docstr-coverage: inherited
v1.10.1,instantiate here to make module easily serializable
v1.10.1,"batch norm gets reset automatically, since it defines reset_parameters"
v1.10.1,shapes
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,there are separate biases for entities in head and tail position
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,with k=4
v1.10.1,the base interaction
v1.10.1,forward entity/relation shapes
v1.10.1,The parameters of the affine transformation: bias
v1.10.1,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,head position and bump
v1.10.1,relation box: head
v1.10.1,relation box: tail
v1.10.1,tail position and bump
v1.10.1,docstr-coverage: inherited
v1.10.1,input normalization
v1.10.1,Core tensor
v1.10.1,docstr-coverage: inherited
v1.10.1,initialize core tensor
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"r_head, r_mid, r_tail"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,type alias for AutoSF block description
v1.10.1,"head_index, relation_index, tail_index, sign"
v1.10.1,: a description of the block structure
v1.10.1,convert to tuple
v1.10.1,infer the number of entity and relation representations
v1.10.1,verify coefficients
v1.10.1,dynamic entity / relation shapes
v1.10.1,docstr-coverage: inherited
v1.10.1,"r_head, r_bias, r_tail"
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,docstr-coverage: excused `wrapped`
v1.10.1,"repeat if necessary, and concat head and relation"
v1.10.1,"shape: -1, num_input_channels, 2*height, width"
v1.10.1,"shape: -1, num_input_channels, 2*height, width"
v1.10.1,"-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.10.1,"reshape: (-1, dim) -> (*batch_dims, dim)"
v1.10.1,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.10.1,output_shape: batch_dims
v1.10.1,add bias term
v1.10.1,"cat into shape (..., 1, d, 3)"
v1.10.1,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.10.1,"Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)"
v1.10.1,shortcut for same shape
v1.10.1,split weight into head-/relation-/tail-specific sub-matrices
v1.10.1,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.10.1,"Predict t embedding, shape: (*batch_dims, d)"
v1.10.1,dot product
v1.10.1,"composite: (*batch_dims, d)"
v1.10.1,inner product with relation embedding
v1.10.1,Circular correlation of entity embeddings
v1.10.1,complex conjugate
v1.10.1,Hadamard product in frequency domain
v1.10.1,inverse real FFT
v1.10.1,global projections
v1.10.1,"combination, shape: (*batch_dims, d)"
v1.10.1,dot product with t
v1.10.1,r expresses a rotation in complex plane.
v1.10.1,rotate head by relation (=Hadamard product in complex space)
v1.10.1,rotate tail by inverse of relation
v1.10.1,The inverse rotation is expressed by the complex conjugate of r.
v1.10.1,The score is computed as the distance of the relation-rotated head to the tail.
v1.10.1,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.10.1,|h * r - t| = |h - conj(r) * t|
v1.10.1,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.10.1,"That is not mentioned in the paper, so it is made optional here."
v1.10.1,Project entities
v1.10.1,h projection to hyperplane
v1.10.1,r
v1.10.1,-t projection to hyperplane
v1.10.1,project to relation specific subspace
v1.10.1,ensure constraints
v1.10.1,x_1 contraction
v1.10.1,x_2 contraction
v1.10.1,"TODO: this sign is in the official code, too, but why do we need it?"
v1.10.1,head interaction
v1.10.1,relation interaction (notice that h has been updated)
v1.10.1,combination
v1.10.1,similarity
v1.10.1,head
v1.10.1,relation box: head
v1.10.1,relation box: tail
v1.10.1,tail
v1.10.1,power norm
v1.10.1,the relation-specific head box base shape (normalized to have a volume of 1):
v1.10.1,the relation-specific tail box base shape (normalized to have a volume of 1):
v1.10.1,head
v1.10.1,relation
v1.10.1,tail
v1.10.1,version 2: relation factor offset
v1.10.1,extension: negative (power) norm
v1.10.1,note: normalization should be done from the representations
v1.10.1,cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501
v1.10.1,version 2
v1.10.1,r_head = r_head + u * torch.ones_like(r_head)
v1.10.1,r_tail = r_tail + u * torch.ones_like(r_tail)
v1.10.1,"stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)"
v1.10.1,"remember shape for output, but reshape for transformer"
v1.10.1,"get position embeddings, shape: (seq_len, dim)"
v1.10.1,Now we are position-dependent w.r.t qualifier pairs.
v1.10.1,"seq_length, batch_size, dim"
v1.10.1,Pool output
v1.10.1,"output shape: (batch_size, dim)"
v1.10.1,reshape
v1.10.1,head
v1.10.1,relation
v1.10.1,tail
v1.10.1,extension: negative (power) norm
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Concrete classes
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,input normalization
v1.10.1,instantiate separate combinations
v1.10.1,docstr-coverage: inherited
v1.10.1,split complex; repeat real
v1.10.1,separately combine real and imaginary parts
v1.10.1,combine
v1.10.1,docstr-coverage: inherited
v1.10.1,symbolic output to avoid dtype issue
v1.10.1,we only need to consider real part here
v1.10.1,the gate
v1.10.1,the combination
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Resolver
v1.10.1,Base classes
v1.10.1,Concrete classes
v1.10.1,TODO: allow relative
v1.10.1,isin() preserves the sorted order
v1.10.1,docstr-coverage: inherited
v1.10.1,sort by decreasing degree
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,sort by decreasing page rank
v1.10.1,docstr-coverage: inherited
v1.10.1,input normalization
v1.10.1,determine absolute number of anchors for each strategy
v1.10.1,if pre-instantiated
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: the token ID of the padding token
v1.10.1,: the token representations
v1.10.1,: the assigned tokens for each entity
v1.10.1,needs to be lazily imported to avoid cyclic imports
v1.10.1,fill padding (nn.Embedding cannot deal with negative indices)
v1.10.1,"sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs"
v1.10.1,"contain a subset of training relations) - for that, the padding index is the last index of the Representation"
v1.10.1,resolve token representation
v1.10.1,input validation
v1.10.1,register as buffer
v1.10.1,assign sub-module
v1.10.1,apply tokenizer
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"get token IDs, shape: (*, num_chosen_tokens)"
v1.10.1,"lookup token representations, shape: (*, num_chosen_tokens, *shape)"
v1.10.1,": A list with ratios per representation in their creation order,"
v1.10.1,": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`"
v1.10.1,": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95"
v1.10.1,normalize triples
v1.10.1,inverse triples are created afterwards implicitly
v1.10.1,tokenize
v1.10.1,Create an MLP for string aggregation
v1.10.1,note: the token representations' shape includes the number of tokens as leading dim
v1.10.1,unique hashes per representation
v1.10.1,unique hashes if we concatenate all representations together
v1.10.1,TODO: vectorization?
v1.10.1,remove self-loops
v1.10.1,add inverse edges and remove duplicates
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Resolver
v1.10.1,Base classes
v1.10.1,Concrete classes
v1.10.1,docstr-coverage: inherited
v1.10.1,tokenize: represent entities by bag of relations
v1.10.1,collect candidates
v1.10.1,randomly sample without replacement num_tokens relations for each entity
v1.10.1,TODO: expose num_anchors?
v1.10.1,select anchors
v1.10.1,find closest anchors
v1.10.1,convert to torch
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"To prevent possible segfaults in the METIS C code, METIS expects a graph"
v1.10.1,(1) without self-loops; (2) with inverse edges added; (3) with unique edges only
v1.10.1,https://github.com/KarypisLab/METIS/blob/94c03a6e2d1860128c2d0675cbbb86ad4f261256/libmetis/checkgraph.c#L18
v1.10.1,select independently per partition
v1.10.1,select adjacency part;
v1.10.1,"note: the indices will automatically be in [0, ..., high - low), since they are *local* indices"
v1.10.1,offset
v1.10.1,the -1 comes from the shared padding token
v1.10.1,note: permutation will be later on reverted
v1.10.1,add back 1 for the shared padding token
v1.10.1,TODO: check if perm is used correctly
v1.10.1,verify pool
v1.10.1,docstr-coverage: inherited
v1.10.1,choose first num_tokens
v1.10.1,TODO: vectorization?
v1.10.1,heuristic
v1.10.1,heuristic
v1.10.1,calculate configuration digest
v1.10.1,create anchor selection instance
v1.10.1,select anchors
v1.10.1,anchor search (=anchor assignment?)
v1.10.1,assign anchors
v1.10.1,save
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Resolver
v1.10.1,Base classes
v1.10.1,Concrete classes
v1.10.1,docstr-coverage: inherited
v1.10.1,"contains: anchor_ids, entity_ids, mapping {entity_id -> {""ancs"": anchors, ""dists"": distances}}"
v1.10.1,normalize anchor_ids
v1.10.1,cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541
v1.10.1,TODO: keep distances?
v1.10.1,ensure parent directory exists
v1.10.1,save via torch.save
v1.10.1,docstr-coverage: inherited
v1.10.1,"TODO: since we save a contiguous array of (num_entities, num_anchors),"
v1.10.1,"it would be more efficient to not convert to a mapping, but directly select from the tensor"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Anchor Searchers
v1.10.1,Anchor Selection
v1.10.1,Tokenizers
v1.10.1,Token Loaders
v1.10.1,Representations
v1.10.1,Data containers
v1.10.1,"TODO: use graph library, such as igraph, graph-tool, or networkit"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Resolver
v1.10.1,Base classes
v1.10.1,Concrete classes
v1.10.1,docstr-coverage: inherited
v1.10.1,convert to adjacency matrix
v1.10.1,convert to scipy sparse csr
v1.10.1,"compute distances between anchors and all nodes, shape: (num_anchors, num_entities)"
v1.10.1,TODO: padding for unreachable?
v1.10.1,select anchor IDs with smallest distance
v1.10.1,docstr-coverage: inherited
v1.10.1,infer shape
v1.10.1,create adjacency matrix
v1.10.1,symmetric + self-loops
v1.10.1,"for each entity, determine anchor pool by BFS"
v1.10.1,an array storing whether node i is reachable by anchor j
v1.10.1,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.10.1,the output
v1.10.1,anchor nodes have themselves as a starting found anchor
v1.10.1,TODO: take all (q-1) hop neighbors before selecting from q-hop
v1.10.1,propagate one hop
v1.10.1,convergence check
v1.10.1,copy pool if we have seen enough anchors and have not yet stopped
v1.10.1,stop once we have enough
v1.10.1,TODO: can we replace this loop with something vectorized?
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,symmetric + self-loops
v1.10.1,"for each entity, determine anchor pool by BFS"
v1.10.1,an array storing whether node i is reachable by anchor j
v1.10.1,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.10.1,the output that track the distance to each found anchor
v1.10.1,"dtype is unsigned int 8 bit, so we initialize the maximum distance to 255 (or max default)"
v1.10.1,initial anchors are 0-hop away from themselves
v1.10.1,propagate one hop
v1.10.1,TODO the float() trick for GPU result stability until the torch_sparse issue is resolved
v1.10.1,https://github.com/rusty1s/pytorch_sparse/issues/243
v1.10.1,convergence check
v1.10.1,newly reached is a mask that points to newly discovered anchors at this particular step
v1.10.1,implemented as element-wise XOR (will only give True in 0 XOR 1 or 1 XOR 0)
v1.10.1,"in our case we enrich the set of found anchors, so we can only have values turning 0 to 1, eg 0 XOR 1"
v1.10.1,copy pool if we have seen enough anchors and have not yet stopped
v1.10.1,"update the value in the pool by the current hop value (we start from 0, so +1 be default)"
v1.10.1,stop once we have enough
v1.10.1,sort the pool by nearest to farthest anchors
v1.10.1,values with distance 255 (or max for unsigned int8 type) are padding tokens
v1.10.1,"since the output is sorted, no need for random sampling, we just take top-k nearest"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"select k anchors with largest ppr, shape: (batch_size, k)"
v1.10.1,prepare adjacency matrix only once
v1.10.1,prepare result
v1.10.1,progress bar?
v1.10.1,batch-wise computation of PPR
v1.10.1,"run page-rank calculation, shape: (batch_size, n)"
v1.10.1,"select PPR values for the anchors, shape: (batch_size, num_anchors)"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Base classes
v1.10.1,Concrete classes
v1.10.1,
v1.10.1,
v1.10.1,
v1.10.1,
v1.10.1,
v1.10.1,Misc
v1.10.1,
v1.10.1,rank based metrics do not need binarized scores
v1.10.1,: the supported rank types. Most of the time equal to all rank types
v1.10.1,: whether the metric requires the number of candidates for each ranking task
v1.10.1,normalize confidence level
v1.10.1,sample metric values
v1.10.1,"bootstrap estimator (i.e., compute on sample with replacement)"
v1.10.1,cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function
v1.10.1,: The rank-based metric class that this derived metric extends
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"since scale and offset are constant for a given number of candidates, we have"
v1.10.1,E[scale * M + offset] = scale * E[M] + offset
v1.10.1,docstr-coverage: inherited
v1.10.1,"since scale and offset are constant for a given number of candidates, we have"
v1.10.1,V[scale * M + offset] = scale^2 * V[M]
v1.10.1,: Z-adjusted metrics are formulated to be increasing
v1.10.1,: Z-adjusted metrics can only be applied to realistic ranks
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,should be exactly 0.0
v1.10.1,docstr-coverage: inherited
v1.10.1,should be exactly 1.0
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,: Expectation/maximum reindexed metrics are formulated to be increasing
v1.10.1,: Expectation/maximum reindexed metrics can only be applied to realistic ranks
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,should be exactly 0.0
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2
v1.10.1,use V[x] = E[x^2] - E[x]^2
v1.10.1,group by same weight -> compute H_w(n) for multiple n at once
v1.10.1,we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once
v1.10.1,now select from precomputed cumulative sums and aggregate
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective"
v1.10.1,cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO: should we return the sum of weights?
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.10.1,docstr-coverage: inherited
v1.10.1,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: the lower bound
v1.10.1,: whether the lower bound is inclusive
v1.10.1,: the upper bound
v1.10.1,: whether the upper bound is inclusive
v1.10.1,: The name of the metric
v1.10.1,: a link to further information
v1.10.1,: whether the metric needs binarized scores
v1.10.1,": whether it is increasing, i.e., larger values are better"
v1.10.1,: the value range
v1.10.1,: synonyms for this metric
v1.10.1,: whether the metric supports weights
v1.10.1,: whether there is a closed-form solution of the expectation
v1.10.1,: whether there is a closed-form solution of the variance
v1.10.1,normalize weights
v1.10.1,calculate weighted harmonic mean
v1.10.1,calculate cdf
v1.10.1,determine value at p=0.5
v1.10.1,special case for exactly 0.5
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: A description of the metric
v1.10.1,: The function that runs the metric
v1.10.1,docstr-coverage: inherited
v1.10.1,: Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics
v1.10.1,: This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`
v1.10.1,"TODO there's something wrong with this, so add it later"
v1.10.1,classifier_annotator.higher(
v1.10.1,"rmc.pr_auc_score,"
v1.10.1,"name=""AUC-PR"","
v1.10.1,"description=""Area Under the Precision-Recall Curve"","
v1.10.1,"link=""https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score"","
v1.10.1,)
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,don't worry about functions because they can't be specified by JSON.
v1.10.1,Could make a better mo
v1.10.1,later could extend for other non-JSON valid types
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Score with original triples
v1.10.1,Score with inverse triples
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,noqa:DAR101
v1.10.1,noqa:DAR401
v1.10.1,Create directory in which all experimental artifacts are saved
v1.10.1,noqa:DAR101
v1.10.1,clip for node piece configurations
v1.10.1,"""pykeen experiments reproduce"" expects ""model reference dataset"""
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO: take care that triples aren't removed that are the only ones with any given entity
v1.10.1,distribute the deteriorated triples across the remaining factories
v1.10.1,"'kinships',"
v1.10.1,"'umls',"
v1.10.1,"'codexsmall',"
v1.10.1,"'wn18',"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: Functions for specifying exotic resources with a given prefix
v1.10.1,: Functions for specifying exotic resources based on their file extension
v1.10.1,Input validation
v1.10.1,convert to numpy
v1.10.1,Additional columns
v1.10.1,convert PyTorch tensors to numpy
v1.10.1,convert to dataframe
v1.10.1,Re-order columns
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.10.1,TODO vectorize code
v1.10.1,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,save literal-to-id mapping
v1.10.1,save numeric literals
v1.10.1,load literal-to-id
v1.10.1,load literals
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Split triples
v1.10.1,Sorting ensures consistent results when the triples are permuted
v1.10.1,Create mapping
v1.10.1,Sorting ensures consistent results when the triples are permuted
v1.10.1,Create mapping
v1.10.1,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.10.1,Filter all non-existent triples
v1.10.1,Note: Unique changes the order of the triples
v1.10.1,Note: Using unique means implicit balancing of training samples
v1.10.1,normalize input
v1.10.1,: The mapping from labels to IDs.
v1.10.1,: The inverse mapping for label_to_id; initialized automatically
v1.10.1,: A vectorized version of entity_label_to_id; initialized automatically
v1.10.1,: A vectorized version of entity_id_to_label; initialized automatically
v1.10.1,Normalize input
v1.10.1,label
v1.10.1,Filter for entities
v1.10.1,Filter for relations
v1.10.1,No filter
v1.10.1,: the number of unique entities
v1.10.1,": the number of relations (maybe including ""artificial"" inverse relations)"
v1.10.1,: whether to create inverse triples
v1.10.1,": the number of real relations, i.e., without artificial inverses"
v1.10.1,ensure torch.Tensor
v1.10.1,input validation
v1.10.1,"always store as torch.long, i.e., torch's default integer dtype"
v1.10.1,check new label to ID mappings
v1.10.1,Make new triples factories for each group
v1.10.1,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.1,prepare metadata
v1.10.1,Delegate to function
v1.10.1,"restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed"
v1.10.1,docstr-coverage: inherited
v1.10.1,load base
v1.10.1,load numeric triples
v1.10.1,store numeric triples
v1.10.1,store metadata
v1.10.1,note: num_relations will be doubled again when instantiating with create_inverse_triples=True
v1.10.1,Check if the triples are inverted already
v1.10.1,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.10.1,contained if and only if create_inverse_triples is True.
v1.10.1,Generate entity mapping if necessary
v1.10.1,Generate relation mapping if necessary
v1.10.1,Map triples of labels to triples of IDs.
v1.10.1,TODO: Check if lazy evaluation would make sense
v1.10.1,docstr-coverage: inherited
v1.10.1,store entity/relation to ID
v1.10.1,load entity/relation to ID
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,pre-filter to keep only topk
v1.10.1,if top is larger than the number of available options
v1.10.1,Generate a word cloud image
v1.10.1,docstr-coverage: inherited
v1.10.1,vectorized label lookup
v1.10.1,Re-order columns
v1.10.1,docstr-coverage: inherited
v1.10.1,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.10.1,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.10.1,ID-based triples
v1.10.1,labeled triples
v1.10.1,make sure triples are a numpy array
v1.10.1,make sure triples are 2d
v1.10.1,convert to ID-based
v1.10.1,triples factory
v1.10.1,all keyword-based options have been none
v1.10.1,delegate to keyword-based get_mapped_triples to re-use optional validation logic
v1.10.1,delegate to keyword-based get_mapped_triples to re-use optional validation logic
v1.10.1,only labeled triples are remaining
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Split indices
v1.10.1,Split triples
v1.10.1,select one triple per relation
v1.10.1,maintain set of covered entities
v1.10.1,"Select one triple for each head/tail entity, which is not yet covered."
v1.10.1,create mask
v1.10.1,Prepare split index
v1.10.1,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.10.1,base cases
v1.10.1,IDs not in training
v1.10.1,triples with exclusive test IDs
v1.10.1,docstr-coverage: inherited
v1.10.1,While there are still triples that should be moved to the training set
v1.10.1,Pick a random triple to move over to the training triples
v1.10.1,add to training
v1.10.1,remove from testing
v1.10.1,Recalculate the move_id_mask
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,Make sure that the first element has all the right stuff in it
v1.10.1,docstr-coverage: inherited
v1.10.1,backwards compatibility
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,constants
v1.10.1,constants
v1.10.1,unary
v1.10.1,binary
v1.10.1,ternary
v1.10.1,column names
v1.10.1,return candidates
v1.10.1,index triples
v1.10.1,incoming relations per entity
v1.10.1,outgoing relations per entity
v1.10.1,indexing triples for fast join r1 & r2
v1.10.1,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.10.1,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.10.1,actual evaluation of the pattern
v1.10.1,skip empty support
v1.10.1,TODO: Can this happen after pre-filtering?
v1.10.1,"sort first, for triple order invariance"
v1.10.1,TODO: what is the support?
v1.10.1,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.10.1,sort decreasingly. i dominates j for all j > i in x-dimension
v1.10.1,"if it is also dominated by any y, it is not part of the skyline"
v1.10.1,"group by (relation id, pattern type)"
v1.10.1,"for each group, yield from skyline"
v1.10.1,determine patterns from triples
v1.10.1,drop zero-confidence
v1.10.1,keep only skyline
v1.10.1,create data frame
v1.10.1,iterate relation types
v1.10.1,drop zero-confidence
v1.10.1,keep only skyline
v1.10.1,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.10.1,base = skyline(base)
v1.10.1,create data frame
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO: the same
v1.10.1,": the positive triples, shape: (batch_size, 3)"
v1.10.1,": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)"
v1.10.1,": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)"
v1.10.1,noqa:DAR202
v1.10.1,noqa:DAR401
v1.10.1,TODO: some negative samplers require batches
v1.10.1,"shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.10.1,"each shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset
v1.10.1,docstr-coverage: inherited
v1.10.1,indexing
v1.10.1,initialize
v1.10.1,sample iteratively
v1.10.1,determine weights
v1.10.1,randomly choose a vertex which has not been chosen yet
v1.10.1,normalize to probabilities
v1.10.1,sample a start node
v1.10.1,get list of neighbors
v1.10.1,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.10.1,visit target node
v1.10.1,decrease sample counts
v1.10.1,docstr-coverage: inherited
v1.10.1,convert to csr for fast row slicing
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,safe division for empty sets
v1.10.1,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.10.1,duplicates
v1.10.1,we are not interested in self-similarity
v1.10.1,compute similarities
v1.10.1,Calculate which relations are the inverse ones
v1.10.1,get existing IDs
v1.10.1,remove non-existing ID from label mapping
v1.10.1,create translation tensor
v1.10.1,get entities and relations occurring in triples
v1.10.1,generate ID translation and new label to Id mappings
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.10.1,seamless loading and saving of training checkpoints
v1.10.1,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.10.1,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.10.1,"If a checkpoint file is given, it must be loaded if it exists already"
v1.10.1,"If the stopper dict has any keys, those are written back to the stopper"
v1.10.1,The checkpoint frequency needs to be set to save checkpoints
v1.10.1,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.10.1,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.10.1,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.10.1,send model to device before going into the internal training loop
v1.10.1,Ensure the release of memory
v1.10.1,Clear optimizer
v1.10.1,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.10.1,due to the patience continue to train after the best epoch and thus alter the model
v1.10.1,Create a path
v1.10.1,Prepare all of the callbacks
v1.10.1,"Register a callback for the result tracker, if given"
v1.10.1,"Register a callback for the early stopper, if given"
v1.10.1,TODO should mode be passed here?
v1.10.1,"Take the biggest possible training batch_size, if batch_size not set"
v1.10.1,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.10.1,This will find necessary parameters to optimize the use of the hardware at hand
v1.10.1,return the relevant parameters slice_size and batch_size
v1.10.1,Force weight initialization if training continuation is not explicitly requested.
v1.10.1,Reset the weights
v1.10.1,"afterwards, some parameters may be on the wrong device"
v1.10.1,Create new optimizer
v1.10.1,Create a new lr scheduler and add the optimizer
v1.10.1,Ensure the model is on the correct device
v1.10.1,"When size probing, we don't want progress bars"
v1.10.1,Create progress bar
v1.10.1,Save the time to track when the saved point was available
v1.10.1,Training Loop
v1.10.1,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.10.1,Enforce training mode
v1.10.1,Accumulate loss over epoch
v1.10.1,Batching
v1.10.1,Only create a progress bar when not in size probing mode
v1.10.1,Flag to check when to quit the size probing
v1.10.1,Recall that torch *accumulates* gradients. Before passing in a
v1.10.1,"new instance, you need to zero out the gradients from the old instance"
v1.10.1,Get batch size of current batch (last batch may be incomplete)
v1.10.1,accumulate gradients for whole batch
v1.10.1,forward pass call
v1.10.1,"when called by batch_size_search(), the parameter update should not be applied."
v1.10.1,update parameters according to optimizer
v1.10.1,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.10.1,constraints are no longer applied
v1.10.1,For testing purposes we're only interested in processing one batch
v1.10.1,When size probing we don't need the losses
v1.10.1,Update learning rate scheduler
v1.10.1,Track epoch loss
v1.10.1,"note: this epoch loss can be slightly biased towards the last batch, if this is smaller than the rest"
v1.10.1,"in practice, this should have a minor effect, since typically batch_size << num_instances"
v1.10.1,Print loss information to console
v1.10.1,Save the last successful finished epoch
v1.10.1,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.10.1,During automatic memory optimization only the error message is of interest
v1.10.1,When there wasn't a best epoch the checkpoint path should be None
v1.10.1,Delete temporary best epoch model
v1.10.1,Includes a call to result_tracker.log_metrics
v1.10.1,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.10.1,MyPy overrides are because you should
v1.10.1,When there wasn't a best epoch the checkpoint path should be None
v1.10.1,Delete temporary best epoch model
v1.10.1,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.10.1,at that state
v1.10.1,Delete temporary best epoch model
v1.10.1,forward pass
v1.10.1,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.10.1,correction for loss reduction
v1.10.1,backward pass
v1.10.1,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.10.1,Set upper bound
v1.10.1,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.10.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.1,"Only if a cuda device is available, the random state is accessed"
v1.10.1,This is an entire checkpoint for the optional best model when using early stopping
v1.10.1,Saving triples factory related states
v1.10.1,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.10.1,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.10.1,Check whether the triples factory mappings match those from the checkpoints
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Shuffle each epoch
v1.10.1,Lazy-splitting into batches
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,docstr-coverage: inherited
v1.10.1,disable automatic batching
v1.10.1,docstr-coverage: inherited
v1.10.1,Slicing is not possible in sLCWA training loops
v1.10.1,split batch
v1.10.1,send to device
v1.10.1,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.10.1,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.10.1,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.10.1,Compute negative and positive scores
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,Slicing is not possible for sLCWA
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,lazy init
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO how to pass inductive mode
v1.10.1,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.10.1,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.10.1,: A hint for constructing a :class:`MultiTrainingCallback`
v1.10.1,: A collection of callbacks
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,normalize target column
v1.10.1,The type inference is so confusing between the function switching
v1.10.1,and polymorphism introduced by slicability that these need to be ignored
v1.10.1,Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number
v1.10.1,of total entities from another inductive inference factory
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,Split batch components
v1.10.1,Send batch to device
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.10.1,"must have failed to start slice_size search, we start with trying half the entities."
v1.10.1,"note: we use Tuple[Tensor] here, so we can re-use TensorDataset instead of having to create a custom one"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,unpack
v1.10.1,Send batch to device
v1.10.1,head prediction
v1.10.1,TODO: exploit sparsity
v1.10.1,"note: this is different to what we do for LCWA, where we collect *all* training entities"
v1.10.1,for which the combination is true
v1.10.1,tail prediction
v1.10.1,TODO: exploit sparsity
v1.10.1,regularization
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO?
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,To make MyPy happy
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,now: smaller is better
v1.10.1,: the number of reported results with no improvement after which training will be stopped
v1.10.1,the minimum relative improvement necessary to consider it an improved result
v1.10.1,"whether a larger value is better, or a smaller."
v1.10.1,: The epoch at which the best result occurred
v1.10.1,: The best result so far
v1.10.1,: The remaining patience
v1.10.1,check for improvement
v1.10.1,stop if the result did not improve more than delta for patience evaluations
v1.10.1,: The model
v1.10.1,: The evaluator
v1.10.1,: The triples to use for training (to be used during filtered evaluation)
v1.10.1,: The triples to use for evaluation
v1.10.1,: Size of the evaluation batches
v1.10.1,: Slice size of the evaluation batches
v1.10.1,: The number of epochs after which the model is evaluated on validation set
v1.10.1,: The number of iterations (one iteration can correspond to various epochs)
v1.10.1,: with no improvement after which training will be stopped.
v1.10.1,: The name of the metric to use
v1.10.1,: The minimum relative improvement necessary to consider it an improved result
v1.10.1,: The metric results from all evaluations
v1.10.1,": Whether a larger value is better, or a smaller"
v1.10.1,: The result tracker
v1.10.1,: Callbacks when after results are calculated
v1.10.1,: Callbacks when training gets continued
v1.10.1,: Callbacks when training is stopped early
v1.10.1,: Did the stopper ever decide to stop?
v1.10.1,: the path to the weights of the best model
v1.10.1,: whether to delete the file with the best model weights after termination
v1.10.1,: note: the weights will be re-loaded into the model before
v1.10.1,TODO: Fix this
v1.10.1,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.10.1,raise ValueError(f'Invalid metric name: {self.metric}')
v1.10.1,for mypy
v1.10.1,Evaluate
v1.10.1,Only perform time consuming checks for the first call.
v1.10.1,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.10.1,Append to history
v1.10.1,TODO need a test that this all re-instantiates properly
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Utils
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,dataset
v1.10.1,model
v1.10.1,stored outside of the training loop / optimizer to give access to auto-tuning from Lightning
v1.10.1,optimizer
v1.10.1,"TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA"
v1.10.1,docstr-coverage: inherited
v1.10.1,call post_parameter_update
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO: sub-batching / slicing
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO:
v1.10.1,"shuffle=shuffle,"
v1.10.1,"drop_last=drop_last,"
v1.10.1,"sampler=sampler,"
v1.10.1,"shuffle=shuffle,"
v1.10.1,disable automatic batching in data loader
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO: sub-batching / slicing
v1.10.1,docstr-coverage: inherited
v1.10.1,"note: since this file is executed via __main__, its module name is replaced by __name__"
v1.10.1,"hence, the two classes' fully qualified names start with ""_"" and are considered private"
v1.10.1,cf. https://github.com/cthoyt/class-resolver/issues/39
v1.10.1,automatically choose accelerator
v1.10.1,defaults to TensorBoard; explicitly disabled here
v1.10.1,disable checkpointing
v1.10.1,mixed precision training
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,parsing metrics
v1.10.1,metric pattern = side?.type?.metric.k?
v1.10.1,: The metric key
v1.10.1,": Side of the metric, or ""both"""
v1.10.1,: The rank type
v1.10.1,normalize metric name
v1.10.1,normalize side
v1.10.1,normalize rank type
v1.10.1,normalize keys
v1.10.1,TODO: this can only normalize rank-based metrics!
v1.10.1,TODO: find a better way to handle this
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO: fix this upstream / make metric.score comply to signature
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,Transfer to cpu and convert to numpy
v1.10.1,Ensure that each key gets counted only once
v1.10.1,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"Because the order of the values of an dictionary is not guaranteed,"
v1.10.1,we need to retrieve scores and masks using the exact same key order.
v1.10.1,Clear buffers
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The optimistic rank is the rank when assuming all options with an equal score are placed
v1.10.1,: behind the current test triple.
v1.10.1,": shape: (batch_size,)"
v1.10.1,": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank"
v1.10.1,: over all permutations of the elements with the same score as the currently considered option.
v1.10.1,": shape: (batch_size,)"
v1.10.1,: The pessimistic rank is the rank when assuming all options with an equal score are placed
v1.10.1,: in front of current test triple.
v1.10.1,": shape: (batch_size,)"
v1.10.1,: The number of options is the number of items considered in the ranking. It may change for
v1.10.1,: filtered evaluation
v1.10.1,": shape: (batch_size,)"
v1.10.1,The optimistic rank is the rank when assuming all options with an
v1.10.1,"equal score are placed behind the currently considered. Hence, the"
v1.10.1,"rank is the number of options with better scores, plus one, as the"
v1.10.1,rank is one-based.
v1.10.1,The pessimistic rank is the rank when assuming all options with an
v1.10.1,"equal score are placed in front of the currently considered. Hence,"
v1.10.1,the rank is the number of options which have at least the same score
v1.10.1,minus one (as the currently considered option in included in all
v1.10.1,"options). As the rank is one-based, we have to add 1, which nullifies"
v1.10.1,"the ""minus 1"" from before."
v1.10.1,The realistic rank is the average of the optimistic and pessimistic
v1.10.1,"rank, and hence the expected rank over all permutations of the elements"
v1.10.1,with the same score as the currently considered option.
v1.10.1,"We set values which should be ignored to NaN, hence the number of options"
v1.10.1,which should be considered is given by
v1.10.1,": the scores of the true choice, shape: (*bs), dtype: float"
v1.10.1,": the number of scores which were larger than the true score, shape: (*bs), dtype: long"
v1.10.1,": the number of scores which were not smaller than the true score, shape: (*bs), dtype: long"
v1.10.1,": the total number of compared scores, shape: (*bs), dtype: long"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"TODO remove this, it makes code much harder to reason about"
v1.10.1,add mode parameter
v1.10.1,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.10.1,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.10.1,Clear the ranks from the current evaluator
v1.10.1,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.10.1,do not display progress bar while searching
v1.10.1,start by searching for batch_size
v1.10.1,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.10.1,we do not need to repeat time-consuming checks
v1.10.1,infer start value
v1.10.1,"Since the batch_size search with size 1, i.e. one tuple ((h, r), (h, t) or (r, t)) scored on all"
v1.10.1,"entities/relations, must have failed to start slice_size search, we start with trying half the"
v1.10.1,entities/relations.
v1.10.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.1,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.10.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.1,values_dict[key] will always be an int at this point
v1.10.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.1,"if inverse triples are used, we only do score_t (TODO: by default; can this be changed?)"
v1.10.1,"otherwise, i.e., without inverse triples, we also need score_h"
v1.10.1,"if relations are to be predicted, we need to slice score_r"
v1.10.1,"raise an error, if any of the required methods cannot slice"
v1.10.1,Split batch
v1.10.1,Bind shape
v1.10.1,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.10.1,Warn if all entities will be filtered
v1.10.1,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.10.1,TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place
v1.10.1,verify that the triples have been filtered
v1.10.1,Filter triples if necessary
v1.10.1,Send to device
v1.10.1,Ensure evaluation mode
v1.10.1,Prepare for result filtering
v1.10.1,Send tensors to device
v1.10.1,Prepare batches
v1.10.1,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.10.1,Show progressbar
v1.10.1,Flag to check when to quit the size probing
v1.10.1,Disable gradient tracking
v1.10.1,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.10.1,batch-wise processing
v1.10.1,If we only probe sizes we do not need more than one batch
v1.10.1,Finalize
v1.10.1,Create filter
v1.10.1,Select scores of true
v1.10.1,overwrite filtered scores
v1.10.1,The scores for the true triples have to be rewritten to the scores tensor
v1.10.1,the rank-based evaluators needs the true scores with trailing 1-dim
v1.10.1,Create a positive mask with the size of the scores from the positive filter
v1.10.1,Restrict to entities of interest
v1.10.1,process scores
v1.10.1,optionally restrict triples (nop if no restriction)
v1.10.1,evaluation triples as dataframe
v1.10.1,determine filter triples
v1.10.1,infer num_entities if not given
v1.10.1,"TODO: unique, or max ID + 1?"
v1.10.1,optionally restrict triples
v1.10.1,compute candidate set sizes for different targets
v1.10.1,TODO: extend to relations?
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Evaluation loops
v1.10.1,Evaluation datasets
v1.10.1,: the MemoryUtilizationMaximizer instance for :func:`_evaluate`.
v1.10.1,batch
v1.10.1,tqdm
v1.10.1,data loader
v1.10.1,set upper limit of batch size for automatic memory optimization
v1.10.1,set model to evaluation mode
v1.10.1,delegate to AMO wrapper
v1.10.1,"The key-id for each triple, shape: (num_triples,)"
v1.10.1,": the number of targets for each key, shape: (num_unique_keys + 1,)"
v1.10.1,: the concatenation of unique targets for each key (use bounds to select appropriate sub-array)
v1.10.1,input verification
v1.10.1,group key = everything except the prediction target
v1.10.1,initialize data structure
v1.10.1,group by key
v1.10.1,convert lists to arrays
v1.10.1,instantiate
v1.10.1,return indices corresponding to the `item`-th triple
v1.10.1,input normalization
v1.10.1,prepare filter indices if required
v1.10.1,sorted by target -> most of the batches only have a single target
v1.10.1,group by target
v1.10.1,stack groups into a single tensor
v1.10.1,avoid cyclic imports
v1.10.1,TODO: it would be better to allow separate batch sizes for entity/relation prediction
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"note: most of the time, this loop will only make a single iteration, since the evaluation dataset typically is"
v1.10.1,"not shuffled, and contains evaluation ranking tasks sorted by target"
v1.10.1,"TODO: in theory, we could make a single score calculation for e.g.,"
v1.10.1,"{(h, r, t1), (h, r, t1), ..., (h, r, tk)}"
v1.10.1,predict scores for all candidates
v1.10.1,filter scores
v1.10.1,extract true scores
v1.10.1,replace by nan
v1.10.1,rewrite true scores
v1.10.1,create dense positive masks
v1.10.1,"TODO: afaik, dense positive masks are not used on GPU -> we do not need to move the masks around"
v1.10.1,delegate processing of scores to the evaluator
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,docstr-coverage: inherited
v1.10.1,"note: OGB's evaluator needs a dataset name as input, and uses it to lookup the standard evaluation"
v1.10.1,metric. we do want to support user-selected metrics on arbitrary datasets instead
v1.10.1,"this setting is equivalent to the WikiKG2 setting, and will calculate MRR *and* H@k for k in {1, 3, 10}"
v1.10.1,filter supported metrics
v1.10.1,"prepare input format, cf. `evaluator.expected_input``"
v1.10.1,"y_pred_pos: shape: (num_edge,)"
v1.10.1,"y_pred_neg: shape: (num_edge, num_nodes_neg)"
v1.10.1,iterate over prediction targets
v1.10.1,pre-allocate
v1.10.1,TODO: maybe we want to collect scores on CPU / add an option?
v1.10.1,iterate over batches
v1.10.1,"combine ids, shape: (batch_size, num_negatives + 1)"
v1.10.1,"get scores, shape: (batch_size, num_negatives + 1)"
v1.10.1,store positive and negative scores
v1.10.1,cf. https://github.com/snap-stanford/ogb/pull/357
v1.10.1,combine to input dictionary
v1.10.1,delegate to OGB evaluator
v1.10.1,post-processing
v1.10.1,normalize name
v1.10.1,OGB does not aggregate values across triples
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,flatten dictionaries
v1.10.1,individual side
v1.10.1,combined
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,repeat
v1.10.1,default for inductive LP by [teru2020]
v1.10.1,verify input
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO: do not require to compute all scores beforehand
v1.10.1,cf. Model.score_t(ts=...)
v1.10.1,super.evaluation assumes that the true scores are part of all_scores
v1.10.1,write back correct num_entities
v1.10.1,TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?
v1.10.1,combine key batches
v1.10.1,calculate key frequency
v1.10.1,weight = inverse frequency
v1.10.1,broadcast to samples
v1.10.1,docstr-coverage: inherited
v1.10.1,store keys for calculating macro weights
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,compute macro weights
v1.10.1,note: we wrap the array into a list to be able to re-use _iter_ranks
v1.10.1,calculate weighted metrics
v1.10.1,Clear buffers
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.10.1,to create the class
v1.10.1,TODO: update to hint + kwargs
v1.10.1,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default regularizer class
v1.10.1,: The default parameters for the default regularizer class
v1.10.1,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.10.1,Make sure that all modules with parameters do have a reset_parameters method.
v1.10.1,Recursively visit all sub-modules
v1.10.1,skip self
v1.10.1,Track parents for blaming
v1.10.1,call reset_parameters if possible
v1.10.1,initialize from bottom to top
v1.10.1,This ensures that specialized initializations will take priority over the default ones of its components.
v1.10.1,emit warning if there where parameters which were not initialised by reset_parameters.
v1.10.1,Additional debug information
v1.10.1,docstr-coverage: inherited
v1.10.1,TODO: allow max_id being present in representation_kwargs; if it matches max_id
v1.10.1,TODO: we could infer some shapes from the given interaction shape information
v1.10.1,check max-id
v1.10.1,check shapes
v1.10.1,: The entity representations
v1.10.1,: The relation representations
v1.10.1,: The weight regularizers
v1.10.1,: The interaction function
v1.10.1,"TODO: support ""broadcasting"" representation regularizers?"
v1.10.1,e.g. re-use the same regularizer for everything; or
v1.10.1,"pass a dictionary with keys ""entity""/""relation"";"
v1.10.1,values are either a regularizer hint (=the same regularizer for all repr); or a sequence of appropriate length
v1.10.1,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.10.1,"model.modules(). Thereby, we can collect them automatically."
v1.10.1,Explicitly call reset_parameters to trigger initialization
v1.10.1,"note, triples_factory is required instead of just using self.num_entities"
v1.10.1,and self.num_relations for the inductive case when this is different
v1.10.1,instantiate regularizer
v1.10.1,normalize input
v1.10.1,Note: slicing cannot be used here: the indices for score_hrt only have a batch
v1.10.1,"dimension, and slicing along this dimension is already considered by sub-batching."
v1.10.1,Note: we do not delegate to the general method for performance reasons
v1.10.1,Note: repetition is not necessary here
v1.10.1,batch normalization modules use batch statistics in training mode
v1.10.1,-> different batch divisions lead to different results
v1.10.1,docstr-coverage: inherited
v1.10.1,add broadcast dimension
v1.10.1,unsqueeze if necessary
v1.10.1,docstr-coverage: inherited
v1.10.1,add broadcast dimension
v1.10.1,unsqueeze if necessary
v1.10.1,docstr-coverage: inherited
v1.10.1,add broadcast dimension
v1.10.1,unsqueeze if necessary
v1.10.1,normalization
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,train model
v1.10.1,"note: as this is an example, the model is only trained for a few epochs,"
v1.10.1,"but not until convergence. In practice, you would usually first verify that"
v1.10.1,"the model is sufficiently good in prediction, before looking at uncertainty scores"
v1.10.1,predict triple scores with uncertainty
v1.10.1,"use a larger number of samples, to increase quality of uncertainty estimate"
v1.10.1,get most and least uncertain prediction on training set
v1.10.1,: The scores
v1.10.1,": The uncertainty, in the same shape as scores"
v1.10.1,Enforce evaluation mode
v1.10.1,set dropout layers to training mode
v1.10.1,draw samples
v1.10.1,compute mean and std
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"This empty 1-element tensor doesn't actually do anything,"
v1.10.1,but is necessary since models with no grad params blow
v1.10.1,up the optimizer
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default loss function class
v1.10.1,: The default parameters for the default loss function class
v1.10.1,: The instance of the loss
v1.10.1,: the number of entities
v1.10.1,: the number of relations
v1.10.1,: whether to use inverse relations
v1.10.1,: utility for generating inverse relations
v1.10.1,": When predict_with_sigmoid is set to True, the sigmoid function is"
v1.10.1,: applied to the logits during evaluation and also for predictions
v1.10.1,": after training, but has no effect on the training."
v1.10.1,Random seeds have to set before the embeddings are initialized
v1.10.1,Loss
v1.10.1,TODO: why do we need to empty the cache?
v1.10.1,"TODO: this currently compute (batch_size, num_relations) instead,"
v1.10.1,"i.e., scores for normal and inverse relations"
v1.10.1,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.10.1,send to device
v1.10.1,special handling of inverse relations
v1.10.1,"when trained on inverse relations, the internal relation ID is twice the original relation ID"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Base Models
v1.10.1,Concrete Models
v1.10.1,Inductive Models
v1.10.1,Evaluation-only models
v1.10.1,Meta Models
v1.10.1,Utils
v1.10.1,Abstract Models
v1.10.1,We might be able to relax this later
v1.10.1,baseline models behave differently
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,always create representations for normal and inverse relations and padding
v1.10.1,"note: we need to share the aggregation across representations, since the aggregation may have"
v1.10.1,trainable parameters
v1.10.1,: a mapping from inductive mode to corresponding entity representations
v1.10.1,": note: there may be duplicate values, if entity representations are shared between validation and testing"
v1.10.1,inductive factories
v1.10.1,"entity representation kwargs may contain a triples factory, which needs to be replaced"
v1.10.1,"entity_representations_kwargs.pop(""triples_factory"", None)"
v1.10.1,note: this is *not* a nn.ModuleDict; the modules have to be registered elsewhere
v1.10.1,shared
v1.10.1,non-shared
v1.10.1,"note: ""training"" is an attribute of nn.Module -> need to rename to avoid name collision"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,default composition is DistMult-style
v1.10.1,Saving edge indices for all the supplied splits
v1.10.1,Extract all entity and relation representations
v1.10.1,Perform message passing and get updated states
v1.10.1,Use updated entity and relation states to extract requested IDs
v1.10.1,TODO I got lost in all the Representation Modules and shape casting and wrote this ;(
v1.10.1,normalization
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,": the indexed filter triples, i.e., sparse masks"
v1.10.1,avoid cyclic imports
v1.10.1,create base model
v1.10.1,assign *after* nn.Module.__init__
v1.10.1,save constants
v1.10.1,index triples
v1.10.1,initialize base model's parameters
v1.10.1,"get masks, shape: (batch_size, num_entities/num_relations)"
v1.10.1,combine masks
v1.10.1,"note: * is an elementwise and, and + and elementwise or"
v1.10.1,get non-zero entries
v1.10.1,set scores for fill value for every non-occuring entry
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,NodePiece
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO rethink after RGCN update
v1.10.1,TODO: other parameters?
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default loss function class
v1.10.1,: The default parameters for the default loss function class
v1.10.1,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.10.1,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.10.1,ConvE should be trained with inverse triples
v1.10.1,entity embedding
v1.10.1,ConvE uses one bias for each entity
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,head representation
v1.10.1,tail representation
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default loss function class
v1.10.1,: The default parameters for the default loss function class
v1.10.1,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.10.1,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.10.1,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.10.1,use torch's native complex data type
v1.10.1,use torch's native complex data type
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The regularizer used by [nickel2011]_ for for RESCAL
v1.10.1,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.10.1,: a normalized weight of 10 is used.
v1.10.1,: The LP settings used by [nickel2011]_ for for RESCAL
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.10.1,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.10.1,In the code base only the weights of the output layer are used for regularization
v1.10.1,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,comment:
v1.10.1,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.10.1,uses float64
v1.10.1,entity bias for head
v1.10.1,entity bias for tail
v1.10.1,relation offset
v1.10.1,diagonal relation transformation matrix
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: the default loss function is the self-adversarial negative sampling loss
v1.10.1,: The default parameters for the default loss function class
v1.10.1,: The default entity normalizer parameters
v1.10.1,: The entity representations are normalized to L2 unit length
v1.10.1,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.10.1,"update initializer settings, cf."
v1.10.1,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.10.1,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.10.1,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.10.1,in the original implementation the embeddings are initialized in one parameter
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,"w: (k, d, d)"
v1.10.1,"vh: (k, d)"
v1.10.1,"vt: (k, d)"
v1.10.1,"b: (k,)"
v1.10.1,"u: (k,)"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The regularizer used by [yang2014]_ for DistMult
v1.10.1,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.10.1,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.10.1,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.10.1,: The LP settings used by [yang2014]_ for DistMult
v1.10.1,note: DistMult only regularizes the relation embeddings;
v1.10.1,entity embeddings are hard constrained instead
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default settings for the entity constrainer
v1.10.1,mean
v1.10.1,diagonal covariance
v1.10.1,Ensure positive definite covariances matrices and appropriate size by clamping
v1.10.1,mean
v1.10.1,diagonal covariance
v1.10.1,Ensure positive definite covariances matrices and appropriate size by clamping
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,diagonal entries
v1.10.1,off-diagonal
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The custom regularizer used by [wang2014]_ for TransH
v1.10.1,: The settings used by [wang2014]_ for TransH
v1.10.1,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.10.1,"Therefore, apply_only_once is always set to True."
v1.10.1,: The custom regularizer used by [wang2014]_ for TransH
v1.10.1,: The settings used by [wang2014]_ for TransH
v1.10.1,translation vector in hyperplane
v1.10.1,normal vector of hyperplane
v1.10.1,normalise the normal vectors to unit l2 length
v1.10.1,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.10.1,which enforces the defined soft constraints.
v1.10.1,"thus, we need to use a weight regularizer instead of having an Embedding regularizer,"
v1.10.1,which only regularizes the weights used in a batch
v1.10.1,note: the following is already the default
v1.10.1,"default_regularizer=self.regularizer_default,"
v1.10.1,"default_regularizer_kwargs=self.regularizer_default_kwargs,"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,TODO: Initialize from TransE
v1.10.1,relation embedding
v1.10.1,relation projection
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,": The default strategy for optimizing the model""s hyper-parameters"
v1.10.1,TODO: Decomposition kwargs
v1.10.1,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.10.1,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.10.1,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.10.1,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.10.1,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,combined representation
v1.10.1,Resolve interaction function
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default loss function class
v1.10.1,: The default parameters for the default loss function class
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,entity bias for head
v1.10.1,relation position head
v1.10.1,relation shape head
v1.10.1,relation size head
v1.10.1,relation position tail
v1.10.1,relation shape tail
v1.10.1,relation size tail
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default loss function class
v1.10.1,: The default parameters for the default loss function class
v1.10.1,: The regularizer used by [trouillon2016]_ for SimplE
v1.10.1,": In the paper, they use weight of 0.1, and do not normalize the"
v1.10.1,": regularization term by the number of elements, which is 200."
v1.10.1,: The power sum settings used by [trouillon2016]_ for SimplE
v1.10.1,(head) entity
v1.10.1,tail entity
v1.10.1,relations
v1.10.1,inverse relations
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,input normalization
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,Regular relation embeddings
v1.10.1,The relation-specific interaction vector
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,always create representations for normal and inverse relations and padding
v1.10.1,normalize embedding specification
v1.10.1,prepare token representations & kwargs
v1.10.1,"max_id=triples_factory.num_relations,  # will get added by ERModel"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default loss function class
v1.10.1,: The default parameters for the default loss function class
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Normalize relation embeddings
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default loss function class
v1.10.1,: The default parameters for the default loss function class
v1.10.1,: The LP settings used by [zhang2019]_ for QuatE.
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default settings for the entity constrainer
v1.10.1,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default loss function class
v1.10.1,: The default parameters for the default loss function class
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default parameters for the default loss function class
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default loss function class
v1.10.1,: The default parameters for the default loss function class
v1.10.1,the individual combination for real/complex parts
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the model's hyper-parameters
v1.10.1,: The default parameters for the default loss function class
v1.10.1,no activation
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: the interaction class (for generating the overview table)
v1.10.1,added by ERModel
v1.10.1,"max_id=triples_factory.num_entities,"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,create sparse matrix of absolute counts
v1.10.1,normalize to relative counts
v1.10.1,base case
v1.10.1,"note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could"
v1.10.1,"stay sparse here, with a potential of a huge memory benefit on large datasets!"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,These operations are deterministic and a random seed can be fixed
v1.10.1,just to avoid warnings
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,compute relation similarity matrix
v1.10.1,mapping from relations to head/tail entities
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"if we really need access to the path later, we can expose it as a property"
v1.10.1,via self.writer.log_dir
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The WANDB run
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The name of the run
v1.10.1,": The configuration dictionary, a mapping from name -> value"
v1.10.1,: Should metrics be stored when running ``log_metrics()``?
v1.10.1,": The metrics, a mapping from step -> (name -> value)"
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,: A hint for constructing a :class:`MultiResultTracker`
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Base classes
v1.10.1,Concrete classes
v1.10.1,Utilities
v1.10.1,always add a Python result tracker for storing the configuration
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The file extension for this writer (do not include dot)
v1.10.1,: The file where the results are written to.
v1.10.1,docstr-coverage: inherited
v1.10.1,: The column names
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,store set of triples
v1.10.1,docstr-coverage: inherited
v1.10.1,: some prime numbers for tuple hashing
v1.10.1,: The bit-array for the Bloom filter data structure
v1.10.1,Allocate bit array
v1.10.1,calculate number of hashing rounds
v1.10.1,index triples
v1.10.1,Store some meta-data
v1.10.1,pre-hash
v1.10.1,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,At least make sure to not replace the triples by the original value
v1.10.1,"To make sure we don't replace the {head, relation, tail} by the"
v1.10.1,original value we shift all values greater or equal than the original value by one up
v1.10.1,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.10.1,Set the indices
v1.10.1,docstr-coverage: inherited
v1.10.1,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.10.1,Bind the total number of negatives to sample in this batch
v1.10.1,Equally corrupt all sides
v1.10.1,"Do not detach, as no gradients should flow into the indices."
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.10.1,: A filterer for negative batches
v1.10.1,create unfiltered negative batch by corruption
v1.10.1,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Utils
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.10.1,create index structure
v1.10.1,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.10.1,: The concatenated sorted sets of head/tail entities
v1.10.1,docstr-coverage: inherited
v1.10.1,"shape: (batch_size, num_neg_per_pos, 3)"
v1.10.1,Uniformly sample from head/tail offsets
v1.10.1,get corresponding entity
v1.10.1,"and position within triple (0: head, 2: tail)"
v1.10.1,write into negative batch
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Preprocessing: Compute corruption probabilities
v1.10.1,"compute tph, i.e. the average number of tail entities per head"
v1.10.1,"compute hpt, i.e. the average number of head entities per tail"
v1.10.1,Set parameter for Bernoulli distribution
v1.10.1,docstr-coverage: inherited
v1.10.1,Decide whether to corrupt head or tail
v1.10.1,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.10.1,flatten mask
v1.10.1,Tails are corrupted if heads are not corrupted
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The random seed used at the beginning of the pipeline
v1.10.1,: The model trained by the pipeline
v1.10.1,: The training triples
v1.10.1,: The training loop used by the pipeline
v1.10.1,: The losses during training
v1.10.1,: The results evaluated by the pipeline
v1.10.1,: How long in seconds did training take?
v1.10.1,: How long in seconds did evaluation take?
v1.10.1,: An early stopper
v1.10.1,: The configuration
v1.10.1,: Any additional metadata as a dictionary
v1.10.1,: The version of PyKEEN used to create these results
v1.10.1,: The git hash of PyKEEN used to create these results
v1.10.1,file names for storing results
v1.10.1,TODO: rename param?
v1.10.1,always save results as json file
v1.10.1,"save other components only if requested (which they are, by default)"
v1.10.1,TODO use pathlib here
v1.10.1,"note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:"
v1.10.1,"when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless"
v1.10.1,note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()
v1.10.1,only one original value => assume this to be the mean
v1.10.1,multiple values => assume they correspond to individual trials
v1.10.1,metrics accumulates rows for a dataframe for comparison against the original reported results (if any)
v1.10.1,"TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)"
v1.10.1,summarize
v1.10.1,skip special parameters
v1.10.1,FIXME this should never happen.
v1.10.1,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.10.1,used random_seed to ensure reproducible results
v1.10.1,We have to set clear optimizer to False since training should be continued
v1.10.1,TODO: checkpoint_dict not further used; later loaded again by TrainingLoop.train
v1.10.1,TODO: allow empty validation / testing
v1.10.1,evaluation restriction to a subset of entities/relations
v1.10.1,2. Model
v1.10.1,3. Loss
v1.10.1,4. Regularizer
v1.10.1,TODO should training be reset?
v1.10.1,TODO should kwargs for loss and regularizer be checked and raised for?
v1.10.1,Log model parameters
v1.10.1,Log loss parameters
v1.10.1,the loss was already logged as part of the model kwargs
v1.10.1,"loss=loss_resolver.normalize_inst(model_instance.loss),"
v1.10.1,Log regularizer parameters
v1.10.1,5. Optimizer
v1.10.1,5.1 Learning Rate Scheduler
v1.10.1,6. Training Loop
v1.10.1,8. Evaluation
v1.10.1,7. Training (ronaldo style)
v1.10.1,Misc
v1.10.1,Stopping
v1.10.1,"Load the evaluation batch size for the stopper, if it has been set"
v1.10.1,Add logging for debugging
v1.10.1,Train like Cristiano Ronaldo
v1.10.1,Misc
v1.10.1,Build up a list of triples if we want to be in the filtered setting
v1.10.1,"If the user gave custom ""additional_filter_triples"""
v1.10.1,Determine whether the validation triples should also be filtered while performing test evaluation
v1.10.1,TODO consider implications of duplicates
v1.10.1,Evaluate
v1.10.1,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.10.1,Add logging about evaluator for debugging
v1.10.1,1. Dataset
v1.10.1,2. Model
v1.10.1,3. Loss
v1.10.1,4. Regularizer
v1.10.1,5. Optimizer
v1.10.1,5.1 Learning Rate Scheduler
v1.10.1,6. Training Loop
v1.10.1,7. Training (ronaldo style)
v1.10.1,8. Evaluation
v1.10.1,9. Tracking
v1.10.1,Misc
v1.10.1,Start tracking
v1.10.1,"If the evaluation still fail using the CPU, the error is raised"
v1.10.1,"When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is"
v1.10.1,restarted with PyKEEN's automatic memory optimization
v1.10.1,"When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation"
v1.10.1,is restarted using the cpu
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,cf. also https://github.com/pykeen/pykeen/issues/1071
v1.10.1,TODO: use a class-resolver?
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Imported from PyTorch
v1.10.1,: A wrapper around the hidden scheduler base class
v1.10.1,: The default strategy for optimizing the lr_schedulers' hyper-parameters
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,TODO what happens if already exists?
v1.10.1,TODO incorporate setting of random seed
v1.10.1,pipeline_kwargs=dict(
v1.10.1,"random_seed=random_non_negative_int(),"
v1.10.1,"),"
v1.10.1,Add dataset to current_pipeline
v1.10.1,"Training, test, and validation paths are provided"
v1.10.1,Add loss function to current_pipeline
v1.10.1,Add regularizer to current_pipeline
v1.10.1,Add optimizer to current_pipeline
v1.10.1,Add training approach to current_pipeline
v1.10.1,Add evaluation
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.10.1,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.10.1,the columns are not ordered properly.
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,convert class to string to use caching
v1.10.1,Assume it's a file path
v1.10.1,note: we only need to set the create_inverse_triples in the training factory.
v1.10.1,normalize dataset kwargs
v1.10.1,enable passing force option via dataset_kwargs
v1.10.1,hash kwargs
v1.10.1,normalize dataset name
v1.10.1,get canonic path
v1.10.1,try to use cached dataset
v1.10.1,load dataset without cache
v1.10.1,store cache
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The name of the dataset to download
v1.10.1,"note: we do not use the built-in constants here, since those refer to OGB nomenclature"
v1.10.1,(which happens to coincide with ours)
v1.10.1,FIXME these are already identifiers
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,relation typing
v1.10.1,constants
v1.10.1,unique
v1.10.1,compute over all triples
v1.10.1,Determine group key
v1.10.1,Add labels if requested
v1.10.1,TODO: Merge with _common?
v1.10.1,include hash over triples into cache-file name
v1.10.1,include part hash into cache-file name
v1.10.1,re-use cached file if possible
v1.10.1,select triples
v1.10.1,save to file
v1.10.1,Prune by support and confidence
v1.10.1,TODO: Consider merging with other analysis methods
v1.10.1,TODO: Consider merging with other analysis methods
v1.10.1,TODO: Consider merging with other analysis methods
v1.10.1,"num_triples_validation: Optional[int],"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Raise matplotlib level
v1.10.1,expected metrics
v1.10.1,Needs simulation
v1.10.1,See https://zenodo.org/record/6331629
v1.10.1,TODO: maybe merge into analyze / make sub-command
v1.10.1,only save full data
v1.10.1,Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)
v1.10.1,Plot: difference between mean head and tail degree
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,don't call this function by itself. assumes called through the `validation`
v1.10.1,property and the _training factory has already been loaded
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Normalize path
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Base classes
v1.10.1,Utilities
v1.10.1,: A factory wrapping the training triples
v1.10.1,": A factory wrapping the testing triples, that share indices with the training triples"
v1.10.1,": A factory wrapping the validation triples, that share indices with the training triples"
v1.10.1,: the dataset's name
v1.10.1,TODO: Make a constant for the names
v1.10.1,docstr-coverage: inherited
v1.10.1,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.10.1,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.10.1,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.10.1,: The directory in which the cached data is stored
v1.10.1,TODO: use class-resolver normalize?
v1.10.1,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.1,don't call this function by itself. assumes called through the `validation`
v1.10.1,property and the _training factory has already been loaded
v1.10.1,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,"relative paths within zip file's always follow Posix path, even on Windows"
v1.10.1,tarfile does not like pathlib
v1.10.1,: URL to the data to download
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Utilities
v1.10.1,Base Classes
v1.10.1,Concrete Classes
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"ZENODO_URL = ""https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip"""
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.10.1,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Base class
v1.10.1,Mid-level classes
v1.10.1,: A factory wrapping the training triples
v1.10.1,: A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT
v1.10.1,share indices with the transductive training
v1.10.1,": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.10.1,": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.10.1,: All datasets should take care of inverse triple creation
v1.10.1,": The actual instance of the training factory, which is exposed to the user through `transductive_training`"
v1.10.1,": The actual instance of the inductive inference factory,"
v1.10.1,: which is exposed to the user through `inductive_inference`
v1.10.1,": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`"
v1.10.1,": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`"
v1.10.1,: The directory in which the cached data is stored
v1.10.1,generate subfolders 'training' and  'inference'
v1.10.1,TODO: use class-resolver normalize?
v1.10.1,add v1 / v2 / v3 / v4 for inductive splits if available
v1.10.1,important: inductive_inference shares the same RELATIONS with the transductive training graph
v1.10.1,inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph
v1.10.1,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.1,inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph
v1.10.1,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,Base class
v1.10.1,Mid-level classes
v1.10.1,Datasets
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,graph pairs
v1.10.1,graph sizes
v1.10.1,graph versions
v1.10.1,: The link to the zip file
v1.10.1,: The hex digest for the zip file
v1.10.1,Input validation.
v1.10.1,ensure zip file is present
v1.10.1,save relative paths beforehand so they are present for loading
v1.10.1,delegate to super class
v1.10.1,docstr-coverage: inherited
v1.10.1,"left side has files ending with 1, right side with 2"
v1.10.1,docstr-coverage: inherited
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,": The mapping from (graph-pair, side) to triple file name"
v1.10.1,: The internal dataset name
v1.10.1,: The hex digest for the zip file
v1.10.1,input validation
v1.10.1,store *before* calling super to have it available when loading the graphs
v1.10.1,ensure zip file is present
v1.10.1,shared directory for multiple datasets.
v1.10.1,docstr-coverage: inherited
v1.10.1,create triples factory
v1.10.1,docstr-coverage: inherited
v1.10.1,load mappings for both sides
v1.10.1,load triple alignments
v1.10.1,extract entity alignments
v1.10.1,"(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2"
v1.10.1,TODO: support ID-only graphs
v1.10.1,load both graphs
v1.10.1,load alignment
v1.10.1,drop duplicates
v1.10.1,combine
v1.10.1,store for repr
v1.10.1,split
v1.10.1,create inverse triples only for training
v1.10.1,docstr-coverage: inherited
v1.10.1,base
v1.10.1,concrete
v1.10.1,Abstract class
v1.10.1,Concrete classes
v1.10.1,Data Structures
v1.10.1,a buffer for the triples
v1.10.1,the offsets
v1.10.1,normalization
v1.10.1,append shifted mapped triples
v1.10.1,update offsets
v1.10.1,merge labels with same ID
v1.10.1,for mypy
v1.10.1,reconstruct label-to-id
v1.10.1,optional
v1.10.1,merge entity mapping
v1.10.1,merge relation mapping
v1.10.1,convert labels to IDs
v1.10.1,"map labels, using -1 as fill-value for invalid labels"
v1.10.1,"we cannot drop them here, since the two columns need to stay aligned"
v1.10.1,filter alignment
v1.10.1,map alignment from old IDs to new IDs
v1.10.1,determine swapping partner
v1.10.1,only keep triples where we have a swapping partner
v1.10.1,replace by swapping partner
v1.10.1,": the merged id-based triples, shape: (n, 3)"
v1.10.1,": the updated alignment, shape: (2, m)"
v1.10.1,: additional keyword-based parameters for adjusting label-to-id mappings
v1.10.1,concatenate triples
v1.10.1,filter alignment and translate to IDs
v1.10.1,process
v1.10.1,TODO: restrict to only using training alignments?
v1.10.1,merge mappings
v1.10.1,docstr-coverage: inherited
v1.10.1,docstr-coverage: inherited
v1.10.1,add swap triples
v1.10.1,"e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)"
v1.10.1,create dense entity remapping for swap
v1.10.1,add swapped triples
v1.10.1,swap head
v1.10.1,swap tail
v1.10.1,: the name of the additional alignment relation
v1.10.1,docstr-coverage: inherited
v1.10.1,add alignment triples with extra relation
v1.10.1,docstr-coverage: inherited
v1.10.1,"determine connected components regarding the same-as relation (i.e., applies transitivity)"
v1.10.1,apply id mapping
v1.10.1,ensure consecutive IDs
v1.10.1,only use training alignments?
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,1. Dataset
v1.10.1,2. Model
v1.10.1,3. Loss
v1.10.1,4. Regularizer
v1.10.1,5. Optimizer
v1.10.1,5.1 Learning Rate Scheduler
v1.10.1,6. Training Loop
v1.10.1,7. Training
v1.10.1,8. Evaluation
v1.10.1,9. Trackers
v1.10.1,Misc.
v1.10.1,log pruning
v1.10.1,"trial was successful, but has to be ended"
v1.10.1,also show info
v1.10.1,2. Model
v1.10.1,3. Loss
v1.10.1,4. Regularizer
v1.10.1,5. Optimizer
v1.10.1,5.1 Learning Rate Scheduler
v1.10.1,"TODO this fixes the issue for negative samplers, but does not generally address it."
v1.10.1,"For example, some of them obscure their arguments with **kwargs, so should we look"
v1.10.1,at the parent class? Sounds like something to put in class resolver by using the
v1.10.1,"inspect module. For now, this solution will rely on the fact that the sampler is a"
v1.10.1,direct descendent of a parent NegativeSampler
v1.10.1,create result tracker to allow to gracefully close failed trials
v1.10.1,1. Dataset
v1.10.1,2. Model
v1.10.1,3. Loss
v1.10.1,4. Regularizer
v1.10.1,5. Optimizer
v1.10.1,5.1 Learning Rate Scheduler
v1.10.1,6. Training Loop
v1.10.1,7. Training
v1.10.1,8. Evaluation
v1.10.1,9. Tracker
v1.10.1,Misc.
v1.10.1,close run in result tracker
v1.10.1,raise the error again (which will be catched in study.optimize)
v1.10.1,: The :mod:`optuna` study object
v1.10.1,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.10.1,Output study information
v1.10.1,Output all trials
v1.10.1,Output best trial as pipeline configuration file
v1.10.1,1. Dataset
v1.10.1,2. Model
v1.10.1,3. Loss
v1.10.1,4. Regularizer
v1.10.1,5. Optimizer
v1.10.1,5.1 Learning Rate Scheduler
v1.10.1,6. Training Loop
v1.10.1,7. Training
v1.10.1,8. Evaluation
v1.10.1,9. Tracking
v1.10.1,6. Misc
v1.10.1,Optuna Study Settings
v1.10.1,Optuna Optimization Settings
v1.10.1,TODO: use metric.increasing to determine default direction
v1.10.1,0. Metadata/Provenance
v1.10.1,1. Dataset
v1.10.1,2. Model
v1.10.1,3. Loss
v1.10.1,4. Regularizer
v1.10.1,5. Optimizer
v1.10.1,5.1 Learning Rate Scheduler
v1.10.1,6. Training Loop
v1.10.1,7. Training
v1.10.1,8. Evaluation
v1.10.1,9. Tracking
v1.10.1,1. Dataset
v1.10.1,2. Model
v1.10.1,3. Loss
v1.10.1,4. Regularizer
v1.10.1,5. Optimizer
v1.10.1,5.1 Learning Rate Scheduler
v1.10.1,6. Training Loop
v1.10.1,7. Training
v1.10.1,8. Evaluation
v1.10.1,9. Tracker
v1.10.1,Optuna Misc.
v1.10.1,Pipeline Misc.
v1.10.1,Invoke optimization of the objective function.
v1.10.1,TODO: make it even easier to specify categorical strategies just as lists
v1.10.1,"if isinstance(info, (tuple, list, set)):"
v1.10.1,"info = dict(type='categorical', choices=list(info))"
v1.10.1,get log from info - could either be a boolean or string
v1.10.1,"otherwise, dataset refers to a file that should be automatically split"
v1.10.1,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.10.1,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.10.1,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.10.1,if has_dataset(dataset_name):
v1.10.1,"study.set_user_attr('dataset', dataset_name)"
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,noqa: DAR101
v1.10.1,-*- coding: utf-8 -*-
v1.10.1,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,
v1.10.0,Configuration file for the Sphinx documentation builder.
v1.10.0,
v1.10.0,This file does only contain a selection of the most common options. For a
v1.10.0,full list see the documentation:
v1.10.0,http://www.sphinx-doc.org/en/master/config
v1.10.0,-- Path setup --------------------------------------------------------------
v1.10.0,"If extensions (or modules to document with autodoc) are in another directory,"
v1.10.0,add these directories to sys.path here. If the directory is relative to the
v1.10.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.10.0,
v1.10.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.10.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.10.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.10.0,from unittest.mock import Mock
v1.10.0,sys.modules['numpy'] = Mock()
v1.10.0,sys.modules['numpy.linalg'] = Mock()
v1.10.0,sys.modules['scipy'] = Mock()
v1.10.0,sys.modules['scipy.optimize'] = Mock()
v1.10.0,sys.modules['scipy.interpolate'] = Mock()
v1.10.0,sys.modules['scipy.sparse'] = Mock()
v1.10.0,sys.modules['scipy.ndimage'] = Mock()
v1.10.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.10.0,sys.modules['tensorflow'] = Mock()
v1.10.0,sys.modules['theano'] = Mock()
v1.10.0,sys.modules['theano.tensor'] = Mock()
v1.10.0,sys.modules['torch'] = Mock()
v1.10.0,sys.modules['torch.optim'] = Mock()
v1.10.0,sys.modules['torch.nn'] = Mock()
v1.10.0,sys.modules['torch.nn.init'] = Mock()
v1.10.0,sys.modules['torch.autograd'] = Mock()
v1.10.0,sys.modules['sklearn'] = Mock()
v1.10.0,sys.modules['sklearn.model_selection'] = Mock()
v1.10.0,sys.modules['sklearn.utils'] = Mock()
v1.10.0,-- Project information -----------------------------------------------------
v1.10.0,"The full version, including alpha/beta/rc tags."
v1.10.0,The short X.Y version.
v1.10.0,-- General configuration ---------------------------------------------------
v1.10.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.10.0,
v1.10.0,needs_sphinx = '1.0'
v1.10.0,"If true, the current module name will be prepended to all description"
v1.10.0,unit titles (such as .. function::).
v1.10.0,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.10.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.10.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.10.0,ones.
v1.10.0,show todo's
v1.10.0,generate autosummary pages
v1.10.0,"Add any paths that contain templates here, relative to this directory."
v1.10.0,The suffix(es) of source filenames.
v1.10.0,You can specify multiple suffix as a list of string:
v1.10.0,
v1.10.0,"source_suffix = ['.rst', '.md']"
v1.10.0,The master toctree document.
v1.10.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.10.0,for a list of supported languages.
v1.10.0,
v1.10.0,This is also used if you do content translation via gettext catalogs.
v1.10.0,"Usually you set ""language"" from the command line for these cases."
v1.10.0,"List of patterns, relative to source directory, that match files and"
v1.10.0,directories to ignore when looking for source files.
v1.10.0,This pattern also affects html_static_path and html_extra_path.
v1.10.0,The name of the Pygments (syntax highlighting) style to use.
v1.10.0,-- Options for HTML output -------------------------------------------------
v1.10.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.10.0,a list of builtin themes.
v1.10.0,
v1.10.0,Theme options are theme-specific and customize the look and feel of a theme
v1.10.0,"further.  For a list of options available for each theme, see the"
v1.10.0,documentation.
v1.10.0,
v1.10.0,html_theme_options = {}
v1.10.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.10.0,"relative to this directory. They are copied after the builtin static files,"
v1.10.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.10.0,html_static_path = ['_static']
v1.10.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.10.0,to template names.
v1.10.0,
v1.10.0,The default sidebars (for documents that don't match any pattern) are
v1.10.0,defined by theme itself.  Builtin themes are using these templates by
v1.10.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.10.0,'searchbox.html']``.
v1.10.0,
v1.10.0,html_sidebars = {}
v1.10.0,The name of an image file (relative to this directory) to place at the top
v1.10.0,of the sidebar.
v1.10.0,
v1.10.0,-- Options for HTMLHelp output ---------------------------------------------
v1.10.0,Output file base name for HTML help builder.
v1.10.0,-- Options for LaTeX output ------------------------------------------------
v1.10.0,latex_elements = {
v1.10.0,The paper size ('letterpaper' or 'a4paper').
v1.10.0,
v1.10.0,"'papersize': 'letterpaper',"
v1.10.0,
v1.10.0,"The font size ('10pt', '11pt' or '12pt')."
v1.10.0,
v1.10.0,"'pointsize': '10pt',"
v1.10.0,
v1.10.0,Additional stuff for the LaTeX preamble.
v1.10.0,
v1.10.0,"'preamble': '',"
v1.10.0,
v1.10.0,Latex figure (float) alignment
v1.10.0,
v1.10.0,"'figure_align': 'htbp',"
v1.10.0,}
v1.10.0,Grouping the document tree into LaTeX files. List of tuples
v1.10.0,"(source start file, target name, title,"
v1.10.0,"author, documentclass [howto, manual, or own class])."
v1.10.0,latex_documents = [
v1.10.0,(
v1.10.0,"master_doc,"
v1.10.0,"'pykeen.tex',"
v1.10.0,"'PyKEEN Documentation',"
v1.10.0,"author,"
v1.10.0,"'manual',"
v1.10.0,"),"
v1.10.0,]
v1.10.0,-- Options for manual page output ------------------------------------------
v1.10.0,One entry per manual page. List of tuples
v1.10.0,"(source start file, name, description, authors, manual section)."
v1.10.0,-- Options for Texinfo output ----------------------------------------------
v1.10.0,Grouping the document tree into Texinfo files. List of tuples
v1.10.0,"(source start file, target name, title, author,"
v1.10.0,"dir menu entry, description, category)"
v1.10.0,-- Options for Epub output -------------------------------------------------
v1.10.0,Bibliographic Dublin Core info.
v1.10.0,epub_title = project
v1.10.0,The unique identifier of the text. This can be a ISBN number
v1.10.0,or the project homepage.
v1.10.0,
v1.10.0,epub_identifier = ''
v1.10.0,A unique identification for the text.
v1.10.0,
v1.10.0,epub_uid = ''
v1.10.0,A list of files that should not be packed into the epub file.
v1.10.0,epub_exclude_files = ['search.html']
v1.10.0,-- Extension configuration -------------------------------------------------
v1.10.0,-- Options for intersphinx extension ---------------------------------------
v1.10.0,Example configuration for intersphinx: refer to the Python standard library.
v1.10.0,"'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),"
v1.10.0,See discussion for adding huggingface intersphinx docs at
v1.10.0,https://github.com/huggingface/transformers/issues/14728#issuecomment-1133521776
v1.10.0,autodoc_member_order = 'bysource'
v1.10.0,autodoc_preserve_defaults = True
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,check probability distribution
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Check a model param is optimized
v1.10.0,Check a loss param is optimized
v1.10.0,Check a model param is NOT optimized
v1.10.0,Check a loss param is optimized
v1.10.0,Check a model param is optimized
v1.10.0,Check a loss param is NOT optimized
v1.10.0,Check a model param is NOT optimized
v1.10.0,Check a loss param is NOT optimized
v1.10.0,verify failure
v1.10.0,"Since custom data was passed, we can't store any of this"
v1.10.0,"currently, any custom data doesn't get stored."
v1.10.0,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.10.0,"Since there's no source path information, these shouldn't be"
v1.10.0,"added, even if it might be possible to infer path information"
v1.10.0,from the triples factories
v1.10.0,"Since paths were passed for training, testing, and validation,"
v1.10.0,they should be stored as study-level attributes
v1.10.0,Check a model param is optimized
v1.10.0,Check a loss param is optimized
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,docstr-coverage: inherited
v1.10.0,check if within 0.5 std of observed
v1.10.0,test error is raised
v1.10.0,there is an extra test for this case
v1.10.0,docstr-coverage: inherited
v1.10.0,same size tensors
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,Tests that exception will be thrown when more than or less than two tensors are passed
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,create broadcastable shapes
v1.10.0,check correct value range
v1.10.0,check maximum norm constraint
v1.10.0,unchanged values for small norms
v1.10.0,random entity embeddings & projections
v1.10.0,random relation embeddings & projections
v1.10.0,project
v1.10.0,check shape:
v1.10.0,check normalization
v1.10.0,check equivalence of re-formulation
v1.10.0,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.10.0,= r_p (e_p^T e) + e'
v1.10.0,"create random array, estimate the costs of addition, and measure some execution times."
v1.10.0,"then, compute correlation between the estimated cost, and the measured time."
v1.10.0,check for strong correlation between estimated costs and measured execution time
v1.10.0,get optimal sequence
v1.10.0,check caching
v1.10.0,get optimal sequence
v1.10.0,check correct cost
v1.10.0,check optimality
v1.10.0,compare result to sequential addition
v1.10.0,compare result to sequential addition
v1.10.0,ensure each node participates in at least one edge
v1.10.0,check type and shape
v1.10.0,number of colors is monotonically increasing
v1.10.0,ensure each node participates in at least one edge
v1.10.0,normalize
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,equal value; larger is better
v1.10.0,equal value; smaller is better
v1.10.0,larger is better; improvement
v1.10.0,larger is better; improvement; but not significant
v1.10.0,assert that reporting another metric for this epoch raises an error
v1.10.0,: The window size used by the early stopper
v1.10.0,: The (zeroed) index  - 1 at which stopping will occur
v1.10.0,: The minimum improvement
v1.10.0,: The random seed to use for reproducibility
v1.10.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.10.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.10.0,: The batch size to use.
v1.10.0,Fix seed for reproducibility
v1.10.0,Set automatic_memory_optimization to false during testing
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,See https://github.com/pykeen/pykeen/pull/883
v1.10.0,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.10.0,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.10.0,"first triple, and passing create_inverse_triples=True."
v1.10.0,check for warning
v1.10.0,check for filtered triples
v1.10.0,check for correct inverse triples flag
v1.10.0,check correct translation
v1.10.0,check column order
v1.10.0,apply restriction
v1.10.0,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.10.0,check that inverse_triples is correctly carried over
v1.10.0,verify that the label-to-ID mapping has not been changed
v1.10.0,verify that triples have been filtered
v1.10.0,Test different combinations of restrictions
v1.10.0,check compressed triples
v1.10.0,reconstruct triples from compressed form
v1.10.0,check data loader
v1.10.0,set create inverse triple to true
v1.10.0,split factory
v1.10.0,check that in *training* inverse triple are to be created
v1.10.0,check that in all other splits no inverse triples are to be created
v1.10.0,verify that all entities and relations are present in the training factory
v1.10.0,verify that no triple got lost
v1.10.0,verify that the label-to-id mappings match
v1.10.0,Slightly larger number of triples to guarantee split can find coverage of all entities and relations.
v1.10.0,serialize
v1.10.0,de-serialize
v1.10.0,check for equality
v1.10.0,TODO: this could be (Core)TriplesFactory.__equal__
v1.10.0,cf. https://docs.pytest.org/en/7.1.x/example/parametrize.html#parametrizing-conditional-raising
v1.10.0,wrong ndim
v1.10.0,wrong last dim
v1.10.0,wrong dtype: float
v1.10.0,wrong dtype: complex
v1.10.0,correct
v1.10.0,>>> positional argument
v1.10.0,mapped_triples
v1.10.0,triples factory
v1.10.0,labeled triples + factory
v1.10.0,single labeled triple
v1.10.0,multiple labeled triples as list
v1.10.0,multiple labeled triples as array
v1.10.0,>>> keyword only
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"DummyModel,"
v1.10.0,3x batch norm: bias + scale --> 6
v1.10.0,entity specific bias        --> 1
v1.10.0,==================================
v1.10.0,7
v1.10.0,"two bias terms, one conv-filter"
v1.10.0,Two linear layer biases
v1.10.0,"Two BN layers, bias & scale"
v1.10.0,Test that the weight in the MLP is trainable (i.e. requires grad)
v1.10.0,quaternion have four components
v1.10.0,entity embeddings
v1.10.0,relation embeddings
v1.10.0,Compute Scores
v1.10.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.10.0,relation embeddings
v1.10.0,Compute Scores
v1.10.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.10.0,entity embeddings
v1.10.0,relation embeddings
v1.10.0,Compute Scores
v1.10.0,: 2xBN (bias & scale)
v1.10.0,the combination bias
v1.10.0,FIXME definitely a type mismatch going on here
v1.10.0,check shape
v1.10.0,check content
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,empty lists are falsy
v1.10.0,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.10.0,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.10.0,pipeline is checked.
v1.10.0,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.10.0,Resume the previous pipeline
v1.10.0,The MockModel gives the highest score to the highest entity id
v1.10.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.10.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.10.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.10.0,test triple at hand
v1.10.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.10.0,test triple at hand
v1.10.0,cf. https://github.com/pykeen/pykeen/issues/1118
v1.10.0,save a reference to the old init *before* mocking
v1.10.0,run a small pipline
v1.10.0,use sampled training loop ...
v1.10.0,... without explicitly selecting a negative sampler ...
v1.10.0,... but providing custom kwargs
v1.10.0,other parameters for fast test
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.10.0,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.10.0,Test looking up inverse triples
v1.10.0,test new label to ID
v1.10.0,type
v1.10.0,old labels
v1.10.0,"new, compact IDs"
v1.10.0,test vectorized lookup
v1.10.0,type
v1.10.0,shape
v1.10.0,value range
v1.10.0,only occurring Ids get mapped to non-negative numbers
v1.10.0,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.10.0,check type
v1.10.0,check shape
v1.10.0,check content
v1.10.0,check type
v1.10.0,check shape
v1.10.0,check 1-hot
v1.10.0,check type
v1.10.0,check shape
v1.10.0,check value range
v1.10.0,check self-similarity = 1
v1.10.0,base relation
v1.10.0,exact duplicate
v1.10.0,99% duplicate
v1.10.0,50% duplicate
v1.10.0,exact inverse
v1.10.0,99% inverse
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The expected number of entities
v1.10.0,: The expected number of relations
v1.10.0,: The expected number of triples
v1.10.0,": The tolerance on expected number of triples, for randomized situations"
v1.10.0,: The dataset to test
v1.10.0,: The instantiated dataset
v1.10.0,: Should the validation be assumed to have been loaded with train/test?
v1.10.0,Not loaded
v1.10.0,Load
v1.10.0,Test caching
v1.10.0,assert (end - start) < 1.0e-02
v1.10.0,Test consistency of training / validation / testing mapping
v1.10.0,": The directory, if there is caching"
v1.10.0,: The batch size
v1.10.0,: The number of negatives per positive for sLCWA training loop.
v1.10.0,: The number of entities LCWA training loop / label smoothing.
v1.10.0,test reduction
v1.10.0,test finite loss value
v1.10.0,Test backward
v1.10.0,negative scores decreased compared to positive ones
v1.10.0,negative scores decreased compared to positive ones
v1.10.0,: The number of entities.
v1.10.0,: The number of negative samples
v1.10.0,: The number of entities.
v1.10.0,"the relative tolerance for checking close results, cf. torch.allclose"
v1.10.0,"the absolute tolerance for checking close results, cf. torch.allclose"
v1.10.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.10.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.10.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.10.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.10.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.10.0,test multiple different initializations
v1.10.0,calculate by functional
v1.10.0,calculate manually
v1.10.0,allclose checks: | input - other | < atol + rtol * |other|
v1.10.0,simple
v1.10.0,nested
v1.10.0,nested
v1.10.0,prepare a temporary test directory
v1.10.0,check that file was created
v1.10.0,make sure to close file before trying to delete it
v1.10.0,delete intermediate files
v1.10.0,: The batch size
v1.10.0,: The device
v1.10.0,move test instance to device
v1.10.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.10.0,"verify that the regularizer is stored for both, entity and relation representations"
v1.10.0,Forward pass (should update regularizer)
v1.10.0,Call post_parameter_update (should reset regularizer)
v1.10.0,Check if regularization term is reset
v1.10.0,regularization term should be zero
v1.10.0,updated should be set to false
v1.10.0,call method
v1.10.0,generate random tensors
v1.10.0,generate inputs
v1.10.0,call update
v1.10.0,check shape
v1.10.0,check result
v1.10.0,generate single random tensor
v1.10.0,calculate penalty
v1.10.0,check shape
v1.10.0,check value
v1.10.0,update term
v1.10.0,check that the expected term is returned
v1.10.0,check that the regularizer is now reset
v1.10.0,create another instance with apply_only_once enabled
v1.10.0,test initial state
v1.10.0,"after first update, should change the term"
v1.10.0,"after second update, no change should happen"
v1.10.0,FIXME isn't any finite number allowed now?
v1.10.0,: Additional arguments passed to the training loop's constructor method
v1.10.0,: The triples factory instance
v1.10.0,: The batch size for use for forward_* tests
v1.10.0,: The embedding dimensionality
v1.10.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.10.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.10.0,: The batch size for use when testing training procedures
v1.10.0,: The number of epochs to train the model
v1.10.0,: A random number generator from torch
v1.10.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.10.0,initialization
v1.10.0,: Static extras to append to the CLI
v1.10.0,: the model's device
v1.10.0,: the inductive mode
v1.10.0,for reproducible testing
v1.10.0,insert shared parameters
v1.10.0,move model to correct device
v1.10.0,Check that all the parameters actually require a gradient
v1.10.0,Try to initialize an optimizer
v1.10.0,get model parameters
v1.10.0,re-initialize
v1.10.0,check that the operation works in-place
v1.10.0,check that the parameters where modified
v1.10.0,check for finite values by default
v1.10.0,check whether a gradient can be back-propgated
v1.10.0,TODO: look into score_r for inverse relations
v1.10.0,clear buffers for message passing models
v1.10.0,"For the high/low memory test cases of NTN, SE, etc."
v1.10.0,"else, leave to default"
v1.10.0,Make sure that inverse triples are created if create_inverse_triples=True
v1.10.0,triples factory is added by the pipeline
v1.10.0,TODO: Catch HolE MKL error?
v1.10.0,set regularizer term to something that isn't zero
v1.10.0,call post_parameter_update
v1.10.0,assert that the regularization term has been reset
v1.10.0,do one optimization step
v1.10.0,call post_parameter_update
v1.10.0,check model constraints
v1.10.0,Distance-based model
v1.10.0,dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)
v1.10.0,check type
v1.10.0,check shape
v1.10.0,create a new instance with guaranteed dropout
v1.10.0,set to training mode
v1.10.0,check for different output
v1.10.0,use more samples to make sure that enough values can be dropped
v1.10.0,this implicitly tests extra_repr / iter_extra_repr
v1.10.0,select random indices
v1.10.0,forward pass with full graph
v1.10.0,forward pass with restricted graph
v1.10.0,verify the results are similar
v1.10.0,: The number of entities
v1.10.0,: The number of triples
v1.10.0,: the message dim
v1.10.0,TODO: separation message vs. entity dim?
v1.10.0,check shape
v1.10.0,check dtype
v1.10.0,check finite values (e.g. due to division by zero)
v1.10.0,check non-negativity
v1.10.0,: the input dimension
v1.10.0,: the output dimension
v1.10.0,: the number of entities
v1.10.0,: the shape of the tensor to initialize
v1.10.0,: to be initialized / set in subclass
v1.10.0,: the interaction to use for testing a model
v1.10.0,initializers *may* work in-place => clone
v1.10.0,actual number may be different...
v1.10.0,unfavourable split to ensure that cleanup is necessary
v1.10.0,check for unclean split
v1.10.0,check that no triple got lost
v1.10.0,check that triples where only moved from other to reference
v1.10.0,check that all entities occur in reference
v1.10.0,check that no triple got lost
v1.10.0,check that all entities are covered in first part
v1.10.0,the model
v1.10.0,Settings
v1.10.0,Use small model (untrained)
v1.10.0,Get batch
v1.10.0,Compute scores
v1.10.0,Compute mask only if required
v1.10.0,TODO: Re-use filtering code
v1.10.0,"shape: (batch_size, num_triples)"
v1.10.0,"shape: (batch_size, num_entities)"
v1.10.0,Process one batch
v1.10.0,shape
v1.10.0,value range
v1.10.0,no duplicates
v1.10.0,shape
v1.10.0,value range
v1.10.0,no duplicates
v1.10.0,shape
v1.10.0,value range
v1.10.0,"no repetition, except padding idx"
v1.10.0,inferred from triples factory
v1.10.0,: The batch size
v1.10.0,: the maximum number of candidates
v1.10.0,: the number of ranks
v1.10.0,: the number of samples to use for monte-carlo estimation
v1.10.0,: the number of candidates for each individual ranking task
v1.10.0,: the ranks for each individual ranking task
v1.10.0,data type
v1.10.0,value range
v1.10.0,original ranks
v1.10.0,better ranks
v1.10.0,variances are non-negative
v1.10.0,generate random weights such that sum = n
v1.10.0,for sanity checking: give the largest weight to best rank => should improve
v1.10.0,generate two versions
v1.10.0,1. repeat each rank/candidate pair a random number of times
v1.10.0,"2. do not repeat, but assign a corresponding weight"
v1.10.0,check flatness
v1.10.0,"TODO: does this suffice, or do we really need float as datatype?"
v1.10.0,generate random triples factories
v1.10.0,generate random alignment
v1.10.0,add label information if necessary
v1.10.0,prepare alignment data frame
v1.10.0,call
v1.10.0,check
v1.10.0,: The window size used by the early stopper
v1.10.0,: The mock losses the mock evaluator will return
v1.10.0,: The (zeroed) index  - 1 at which stopping will occur
v1.10.0,: The minimum improvement
v1.10.0,: The best results
v1.10.0,Set automatic_memory_optimization to false for tests
v1.10.0,Step early stopper
v1.10.0,check storing of results
v1.10.0,not needed for test
v1.10.0,verify that the input is valid
v1.10.0,combine
v1.10.0,verify shape
v1.10.0,to be initialized in subclass
v1.10.0,no column has been removed
v1.10.0,all old columns are unmodified
v1.10.0,new columns are boolean
v1.10.0,no columns have been added
v1.10.0,check subset relation
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO: this could be shared with the model tests
v1.10.0,"FixedModel: dict(embedding_dim=EMBEDDING_DIM),"
v1.10.0,test combinations of models with training loops
v1.10.0,some models require inverse relations
v1.10.0,some model require access to the training triples
v1.10.0,"inductive models require an inductive mode to be set, and an inference factory to be passed"
v1.10.0,fake an inference factory
v1.10.0,automatically choose accelerator
v1.10.0,defaults to TensorBoard; explicitly disabled here
v1.10.0,disable checkpointing
v1.10.0,fast run
v1.10.0,automatically choose accelerator
v1.10.0,defaults to TensorBoard; explicitly disabled here
v1.10.0,disable checkpointing
v1.10.0,fast run
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,check for finite values by default
v1.10.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.10.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.10.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,≈ result of softmax
v1.10.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.10.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.10.0,sum over the softmax dim as weights sum up to 1
v1.10.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.10.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.10.0,≈ result of sigmoid
v1.10.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.10.0,expected_loss ≈ 0.34
v1.10.0,abstract classes
v1.10.0,Create dummy dense labels
v1.10.0,Check if labels form a probability distribution
v1.10.0,Apply label smoothing
v1.10.0,Check if smooth labels form probability distribution
v1.10.0,Create dummy sLCWA labels
v1.10.0,Apply label smoothing
v1.10.0,generate random ratios
v1.10.0,check size
v1.10.0,check value range
v1.10.0,check total split
v1.10.0,check consistency with ratios
v1.10.0,the number of decimal digits equivalent to 1 / n_total
v1.10.0,check type
v1.10.0,check values
v1.10.0,compare against expected
v1.10.0,generated_triples = generate_triples()
v1.10.0,check type
v1.10.0,check format
v1.10.0,check coverage
v1.10.0,prediction post-processing
v1.10.0,mock prediction data frame
v1.10.0,score consumers
v1.10.0,"use a small model, since operation is expensive"
v1.10.0,"all scores, automatic batch size"
v1.10.0,top 3 scores
v1.10.0,"top 3 scores, fixed batch size, head scoring"
v1.10.0,"all scores, relation scoring"
v1.10.0,"all scores, relation scoring"
v1.10.0,model with inverse relations
v1.10.0,check type
v1.10.0,check shape
v1.10.0,check ID ranges
v1.10.0,"mapped triples, automatic batch size selection, no factory"
v1.10.0,"mapped triples, fixed batch size, no factory"
v1.10.0,labeled triples with factory
v1.10.0,labeled triples as list
v1.10.0,single labeled triple
v1.10.0,model with inverse relations
v1.10.0,"ID-based, no factory"
v1.10.0,string-based + factory
v1.10.0,mixed + factory
v1.10.0,"no restriction, no factory"
v1.10.0,"no restriction, factory"
v1.10.0,"id restriction, no factory ..."
v1.10.0,id restriction with factory
v1.10.0,"comment: we only use id-based input, since the normalization has already been tested"
v1.10.0,create model
v1.10.0,"id-based head/relation/tail prediction, no restriction"
v1.10.0,restriction by list of ints
v1.10.0,tail prediction
v1.10.0,try accessing each element
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"naive implementation, O(n2)"
v1.10.0,check correct output type
v1.10.0,check value range subset
v1.10.0,check value range side
v1.10.0,check columns
v1.10.0,check value range and type
v1.10.0,check value range entity IDs
v1.10.0,check value range entity labels
v1.10.0,check correct type
v1.10.0,check relation_id value range
v1.10.0,check pattern value range
v1.10.0,check confidence value range
v1.10.0,check support value range
v1.10.0,check correct type
v1.10.0,check relation_id value range
v1.10.0,check pattern value range
v1.10.0,check correct type
v1.10.0,check relation_id value range
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,clear
v1.10.0,docstr-coverage: inherited
v1.10.0,assumes deterministic entity to id mapping
v1.10.0,from left_tf
v1.10.0,from right_tf with offset
v1.10.0,docstr-coverage: inherited
v1.10.0,assumes deterministic entity to id mapping
v1.10.0,from left_tf
v1.10.0,from right_tf with offset
v1.10.0,extra-relation
v1.10.0,docstr-coverage: inherited
v1.10.0,assumes deterministic entity to id mapping
v1.10.0,docstr-coverage: inherited
v1.10.0,assumes deterministic entity to id mapping
v1.10.0,from left_tf
v1.10.0,from right_tf with offset
v1.10.0,additional
v1.10.0,verify shape
v1.10.0,verify dtype
v1.10.0,verify number of entities/relations
v1.10.0,verify offsets
v1.10.0,"create old, new pairs"
v1.10.0,simulate merging ids
v1.10.0,only a single pair
v1.10.0,apply
v1.10.0,every key is contained
v1.10.0,value range
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Check minimal statistics
v1.10.0,Check either a github link or author/publication information is given
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO: we could move this part into the interaction module itself
v1.10.0,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.10.0,"prepare conv input (N, C, H, W)"
v1.10.0,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.10.0,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.10.0,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.10.0,Rotate (=Hamilton product in quaternion space).
v1.10.0,"we calculate the scores using the hard-coded formula, instead of utilizing table + einsum"
v1.10.0,"f(h, r, t) = h @ r @ t"
v1.10.0,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.10.0,normalize rotations to unit modulus
v1.10.0,check for unit modulus
v1.10.0,entity embeddings
v1.10.0,relation embeddings
v1.10.0,Compute Scores
v1.10.0,entity embeddings
v1.10.0,relation embeddings
v1.10.0,Compute Scores
v1.10.0,Compute Scores
v1.10.0,-\|R_h h - R_t t\|
v1.10.0,-\|h - t\|
v1.10.0,"Since MuRE has offsets, the scores do not need to negative"
v1.10.0,"We do not need this, since we do not check for functional consistency anyway"
v1.10.0,intra-interaction comparison
v1.10.0,dimension needs to be divisible by num_heads
v1.10.0,FIXME
v1.10.0,multiple
v1.10.0,single
v1.10.0,head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid
v1.10.0,check type
v1.10.0,check size
v1.10.0,check value range
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,message_dim must be divisible by num_heads
v1.10.0,determine pool using anchor searcher
v1.10.0,determine expected pool using shortest path distances via scipy.sparse.csgraph
v1.10.0,generate random pool
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,complex tensor
v1.10.0,check value range
v1.10.0,check modulus == 1
v1.10.0,quaternion needs shape to end on 4
v1.10.0,"check value range (actually [-s, +s] with s = 1/sqrt(2*n))"
v1.10.0,value range
v1.10.0,highest degree node has largest value
v1.10.0,Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf
v1.10.0,create triples with a dummy relation type 0
v1.10.0,"0: green: 2, 3, 7, 8"
v1.10.0,"1: red: 1, 4, 6, 9"
v1.10.0,"2: blue: 0, 5"
v1.10.0,the example includes the first power
v1.10.0,requires at least one complex tensor as input
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,inferred from triples factory
v1.10.0,inferred from assignment
v1.10.0,the representation module infers the max_id from the provided labels
v1.10.0,the following entity does not have an image -> will have to use backfill
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,the representation module infers the max_id from the provided labels
v1.10.0,docstr-coverage: inherited
v1.10.0,the representation module infers the max_id from the provided labels
v1.10.0,max_id is inferred from assignment
v1.10.0,create random assignment
v1.10.0,update kwargs
v1.10.0,empty bases
v1.10.0,inconsistent base shapes
v1.10.0,invalid base id
v1.10.0,invalid local index
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO this is the only place this function is used.
v1.10.0,Is there an alternative so we can remove it?
v1.10.0,ensure positivity
v1.10.0,compute using pytorch
v1.10.0,prepare distributions
v1.10.0,compute using pykeen
v1.10.0,"e: (batch_size, num_heads, num_tails, d)"
v1.10.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.10.0,divergence = 0 => similarity = -divergence = 0
v1.10.0,"(h - t), r"
v1.10.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.10.0,divergence >= 0 => similarity = -divergence <= 0
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.10.0,filter vs. no filter thing.
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,check for empty batches
v1.10.0,: The window size used by the early stopper
v1.10.0,: The mock losses the mock evaluator will return
v1.10.0,: The (zeroed) index  - 1 at which stopping will occur
v1.10.0,: The minimum improvement
v1.10.0,: The best results
v1.10.0,Set automatic_memory_optimization to false for tests
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Train a model in one shot
v1.10.0,Train a model for the first half
v1.10.0,Continue training of the first part
v1.10.0,check non-empty metrics
v1.10.0,: Should negative samples be filtered?
v1.10.0,expectation = (1 + n) / 2
v1.10.0,variance = (n**2 - 1) / 12
v1.10.0,"x_i ~ N(mu_i, 1)"
v1.10.0,closed-form solution
v1.10.0,sampled confidence interval
v1.10.0,check that closed-form is in confidence interval of sampled
v1.10.0,positive values only
v1.10.0,positive and negative values
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Check for correct class
v1.10.0,check correct num_entities
v1.10.0,check type
v1.10.0,check length
v1.10.0,check type
v1.10.0,check length
v1.10.0,check confidence positivity
v1.10.0,Check for correct class
v1.10.0,check value
v1.10.0,filtering
v1.10.0,"true_score: (2, 3, 3)"
v1.10.0,head based filter
v1.10.0,preprocessing for faster lookup
v1.10.0,check that all found positives are positive
v1.10.0,check in-place
v1.10.0,Test head scores
v1.10.0,Assert in-place modification
v1.10.0,Assert correct filtering
v1.10.0,Test tail scores
v1.10.0,Assert in-place modification
v1.10.0,Assert correct filtering
v1.10.0,The MockModel gives the highest score to the highest entity id
v1.10.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.10.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.10.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.10.0,test triple at hand
v1.10.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.10.0,test triple at hand
v1.10.0,check true negatives
v1.10.0,TODO: check no repetitions (if possible)
v1.10.0,return type
v1.10.0,columns
v1.10.0,value range
v1.10.0,relation restriction
v1.10.0,with explicit num_entities
v1.10.0,with inferred num_entities
v1.10.0,test different shapes
v1.10.0,test different shapes
v1.10.0,value range
v1.10.0,value range
v1.10.0,check unique
v1.10.0,"strips off the ""k"" at the end"
v1.10.0,Populate with real results.
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"(-1, 1),"
v1.10.0,"(-1, -1),"
v1.10.0,"(-5, -3),"
v1.10.0,initialize
v1.10.0,update with batches
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Check whether filtering works correctly
v1.10.0,First giving an example where all triples have to be filtered
v1.10.0,The filter should remove all triples
v1.10.0,Create an example where no triples will be filtered
v1.10.0,The filter should not remove any triple
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,same relation
v1.10.0,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.10.0,Test that half of the subjects and half of the objects are corrupted
v1.10.0,check that corrupted entities co-occur with the relation in training data
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The batch size
v1.10.0,: The random seed
v1.10.0,: The triples factory
v1.10.0,: The instances
v1.10.0,: A positive batch
v1.10.0,: Kwargs
v1.10.0,Generate negative sample
v1.10.0,check filter shape if necessary
v1.10.0,check shape
v1.10.0,check bounds: heads
v1.10.0,check bounds: relations
v1.10.0,check bounds: tails
v1.10.0,test that the negative triple is not the original positive triple
v1.10.0,"shape: (batch_size, 1, num_neg)"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Base Classes
v1.10.0,Concrete Classes
v1.10.0,Utils
v1.10.0,: synonyms of this loss
v1.10.0,: The default strategy for optimizing the loss's hyper-parameters
v1.10.0,flatten and stack
v1.10.0,apply label smoothing if necessary.
v1.10.0,TODO: Do label smoothing only once
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,Sanity check
v1.10.0,negative_scores have already been filtered in the sampler!
v1.10.0,"shape: (nnz,)"
v1.10.0,docstr-coverage: inherited
v1.10.0,Sanity check
v1.10.0,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.10.0,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.10.0,positive entries in the labels tensor.
v1.10.0,"This shows how often one row has to be repeated,"
v1.10.0,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.10.0,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.10.0,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.10.0,This tells us how often each true label should be repeated
v1.10.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.10.0,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.10.0,within the ablation pipeline.
v1.10.0,1. positive & negative margin
v1.10.0,2. negative margin & offset
v1.10.0,3. positive margin & offset
v1.10.0,docstr-coverage: inherited
v1.10.0,Sanity check
v1.10.0,positive term
v1.10.0,implicitly repeat positive scores
v1.10.0,"shape: (nnz,)"
v1.10.0,negative term
v1.10.0,negative_scores have already been filtered in the sampler!
v1.10.0,docstr-coverage: inherited
v1.10.0,Sanity check
v1.10.0,"scale labels from [0, 1] to [-1, 1]"
v1.10.0,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.10.0,within the ablation pipeline.
v1.10.0,docstr-coverage: inherited
v1.10.0,negative_scores have already been filtered in the sampler!
v1.10.0,(dense) softmax requires unfiltered scores / masking
v1.10.0,we need to fill the scores with -inf for all filtered negative examples
v1.10.0,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.10.0,use filled negatives scores
v1.10.0,docstr-coverage: inherited
v1.10.0,we need dense negative scores => unfilter if necessary
v1.10.0,"we may have inf rows, since there will be one additional finite positive score per row"
v1.10.0,"combine scores: shape: (batch_size, num_negatives + 1)"
v1.10.0,use sparse version of cross entropy
v1.10.0,calculate cross entropy loss
v1.10.0,docstr-coverage: inherited
v1.10.0,make sure labels form a proper probability distribution
v1.10.0,calculate cross entropy loss
v1.10.0,docstr-coverage: inherited
v1.10.0,determine positive; do not check with == since the labels are floats
v1.10.0,subtract margin from positive scores
v1.10.0,divide by temperature
v1.10.0,docstr-coverage: inherited
v1.10.0,subtract margin from positive scores
v1.10.0,normalize positive score shape
v1.10.0,divide by temperature
v1.10.0,docstr-coverage: inherited
v1.10.0,determine positive; do not check with == since the labels are floats
v1.10.0,compute negative weights (without gradient tracking)
v1.10.0,clone is necessary since we modify in-place
v1.10.0,Split positive and negative scores
v1.10.0,"we pass *all* scores as negatives, but set the weight of positives to zero"
v1.10.0,this allows keeping a dense shape
v1.10.0,docstr-coverage: inherited
v1.10.0,Sanity check
v1.10.0,"we do not allow full -inf rows, since we compute the softmax over this tensor"
v1.10.0,compute weights (without gradient tracking)
v1.10.0,"fill negative scores with some finite value, e.g., 0 (they will get masked out anyway)"
v1.10.0,note: this is a reduction along the softmax dim; since the weights are already normalized
v1.10.0,"to sum to one, we want a sum reduction here, instead of using the self._reduction"
v1.10.0,docstr-coverage: inherited
v1.10.0,Sanity check
v1.10.0,docstr-coverage: inherited
v1.10.0,Sanity check
v1.10.0,negative loss part
v1.10.0,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.10.0,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.10.0,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO: maybe we can make this more efficient?
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO: maybe we can make this more efficient?
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO: method is_inverse?
v1.10.0,TODO: inverse of inverse?
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.10.0,Id of inverse relation: relation + 1
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.10.0,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.10.0,": For more information, see https://github.com/cthoyt/pystow"
v1.10.0,: A path representing the PyKEEN data folder
v1.10.0,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.10.0,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.10.0,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.10.0,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.10.0,: A subdirectory for PyKEEN logs
v1.10.0,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.10.0,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.10.0,"TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728"
v1.10.0,"SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.10.0,get datatype specific epsilon
v1.10.0,clamp minimum value
v1.10.0,try to resolve ambiguous device; there has to be at least one cuda device
v1.10.0,lower bound
v1.10.0,upper bound
v1.10.0,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.10.0,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.10.0,Determine optimal order and cost
v1.10.0,translate back to original order
v1.10.0,determine optimal processing order
v1.10.0,heuristic
v1.10.0,The dimensions affected by e'
v1.10.0,Project entities
v1.10.0,r_p (e_p.T e) + e'
v1.10.0,Enforce constraints
v1.10.0,TODO delete when deleting _normalize_dim (below)
v1.10.0,TODO delete when deleting convert_to_canonical_shape (below)
v1.10.0,TODO delete? See note in test_sim.py on its only usage
v1.10.0,upgrade to sequence
v1.10.0,broadcast
v1.10.0,"normalize ids: -> ids.shape: (batch_size, num_ids)"
v1.10.0,"normalize batch -> batch.shape: (batch_size, 1, 3)"
v1.10.0,allocate memory
v1.10.0,copy ids
v1.10.0,reshape
v1.10.0,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.10.0,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.10.0,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.10.0,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.10.0,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.10.0,ensure pathlib
v1.10.0,Enforce that sizes are strictly positive by passing through ELU
v1.10.0,Shape vector is normalized using the above helper function
v1.10.0,Size is learned separately and applied to normalized shape
v1.10.0,Compute potential boundaries by applying the shape in substraction
v1.10.0,and in addition
v1.10.0,Compute box upper bounds using min and max respectively
v1.10.0,compute width plus 1
v1.10.0,compute box midpoints
v1.10.0,"TODO: we already had this before, as `base`"
v1.10.0,inside box?
v1.10.0,yes: |p - c| / (w + 1)
v1.10.0,no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))
v1.10.0,Step 1: Apply the other entity bump
v1.10.0,Step 2: Apply tanh if tanh_map is set to True.
v1.10.0,Compute the distance function output element-wise
v1.10.0,"Finally, compute the norm"
v1.10.0,cf. https://stackoverflow.com/a/1176023
v1.10.0,check validity
v1.10.0,path compression
v1.10.0,get representatives
v1.10.0,already merged
v1.10.0,make x the smaller one
v1.10.0,merge
v1.10.0,extract partitions
v1.10.0,resolve path to make sure it is an absolute path
v1.10.0,ensure directory exists
v1.10.0,message passing: collect colors of neighbors
v1.10.0,"dense colors: shape: (n, c)"
v1.10.0,"adj:          shape: (n, n)"
v1.10.0,"values need to be float, since torch.sparse.mm does not support integer dtypes"
v1.10.0,size: will be correctly inferred
v1.10.0,concat with old colors
v1.10.0,hash
v1.10.0,create random indicator functions of low dimensionality
v1.10.0,collect neighbors' colors
v1.10.0,round to avoid numerical effects
v1.10.0,hash first
v1.10.0,concat with old colors
v1.10.0,re-hash
v1.10.0,"only keep connectivity, but remove multiplicity"
v1.10.0,"note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,"
v1.10.0,this is rather useless
v1.10.0,initial: degree
v1.10.0,"note: we calculate this separately, since we can use a more efficient implementation for the first step"
v1.10.0,hash
v1.10.0,determine small integer type for dense count array
v1.10.0,convergence check
v1.10.0,each node has a unique color
v1.10.0,the number of colors did not improve in the last iteration
v1.10.0,cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Base Class
v1.10.0,Child classes
v1.10.0,Utils
v1.10.0,: The overall regularization weight
v1.10.0,: The current regularization term (a scalar)
v1.10.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.10.0,: Has this regularizer been updated since last being reset?
v1.10.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.10.0,"If there are tracked parameters, update based on them"
v1.10.0,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.10.0,docstr-coverage: inherited
v1.10.0,no need to compute anything
v1.10.0,docstr-coverage: inherited
v1.10.0,always return zero
v1.10.0,: The dimension along which to compute the vector-based regularization terms.
v1.10.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.10.0,: This allows dimensionality-independent weight tuning.
v1.10.0,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.10.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.0,docstr-coverage: inherited
v1.10.0,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.10.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.0,docstr-coverage: inherited
v1.10.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.0,regularizer-specific parameters
v1.10.0,docstr-coverage: inherited
v1.10.0,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.10.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,orthogonality soft constraint: cosine similarity at most epsilon
v1.10.0,The normalization factor to balance individual regularizers' contribution.
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,high-level
v1.10.0,Low-Level
v1.10.0,cf. https://github.com/python/mypy/issues/5374
v1.10.0,": the dataframe; has to have a column named ""score"""
v1.10.0,: an optional factory to use for labeling
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,: the prediction target
v1.10.0,: the other column's fixed IDs
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,": the ID-based triples, shape: (n, 3)"
v1.10.0,: the scores
v1.10.0,3-tuple for return
v1.10.0,"extract label information, if possible"
v1.10.0,no restriction
v1.10.0,restriction is a tensor
v1.10.0,restriction is a sequence of integers or strings
v1.10.0,"now, restriction is a sequence of integers"
v1.10.0,"if explicit ids have been given, and label information is available, extract list of labels"
v1.10.0,exactly one of them is None
v1.10.0,create input batch
v1.10.0,"note type alias annotation required,"
v1.10.0,cf. https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases
v1.10.0,"batch, TODO: ids?"
v1.10.0,docstr-coverage: inherited
v1.10.0,initialize buffer on device
v1.10.0,docstr-coverage: inherited
v1.10.0,"reshape, shape: (batch_size * num_entities,)"
v1.10.0,get top scores within batch
v1.10.0,determine corresponding indices
v1.10.0,"batch_id, score_id = divmod(top_indices, num_scores)"
v1.10.0,combine to top triples
v1.10.0,append to global top scores
v1.10.0,reduce size if necessary
v1.10.0,initialize buffer on cpu
v1.10.0,Explicitly create triples
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO: variable targets across batches/samples?
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"(?, r, t) => r.stride > t.stride"
v1.10.0,"(h, ?, t) => h.stride > t.stride"
v1.10.0,"(h, r, ?) => h.stride > r.stride"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,train model; note: needs larger number of epochs to do something useful ;-)
v1.10.0,"create prediction dataset, where the head entities is from a set of European countries,"
v1.10.0,and the relations are connected to tourism
v1.10.0,"calculate all scores for this restricted set, and keep k=3 largest"
v1.10.0,add labels
v1.10.0,: the choices for the first and second component of the input batch
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,calculate batch scores onces
v1.10.0,consume by all consumers
v1.10.0,TODO: Support partial dataset
v1.10.0,note: the models' predict method takes care of setting the model to evaluation mode
v1.10.0,exactly one of them is None
v1.10.0,
v1.10.0,note: the models' predict method takes care of setting the model to evaluation mode
v1.10.0,get input & target
v1.10.0,get label-to-id mapping and prediction targets
v1.10.0,get scores
v1.10.0,create raw dataframe
v1.10.0,note: the models' predict method takes care of setting the model to evaluation mode
v1.10.0,normalize input
v1.10.0,calculate scores (with automatic memory optimization)
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"""Closed-Form Expectation"","
v1.10.0,"""Closed-Form Variance"","
v1.10.0,"""✓"" if metric.closed_expectation else """","
v1.10.0,"""✓"" if metric.closed_variance else """","
v1.10.0,Add HPO command
v1.10.0,Add NodePiece tokenization command
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,General types
v1.10.0,Triples
v1.10.0,Others
v1.10.0,Tensor Functions
v1.10.0,Tensors
v1.10.0,Dataclasses
v1.10.0,prediction targets
v1.10.0,modes
v1.10.0,entity alignment sides
v1.10.0,: A function that mutates the input and returns a new object of the same type as output
v1.10.0,: A function that can be applied to a tensor to initialize it
v1.10.0,: A function that can be applied to a tensor to normalize it
v1.10.0,: A function that can be applied to a tensor to constrain it
v1.10.0,: A hint for a :class:`torch.device`
v1.10.0,: A hint for a :class:`torch.Generator`
v1.10.0,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.10.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.10.0,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.10.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.10.0,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.10.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.10.0,: the inductive prediction and training mode
v1.10.0,: the prediction target
v1.10.0,: the prediction target index
v1.10.0,: the rank types
v1.10.0,"RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8"
v1.10.0,entity alignment
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,infer shape
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,input normalization
v1.10.0,note: the base class does not have any parameters
v1.10.0,Heuristic for default value
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"note: the only parameters are inside the relation representation module, which has its own reset_parameters"
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO: can we change the dimension order to make this contiguous?
v1.10.0,docstr-coverage: inherited
v1.10.0,normalize num blocks
v1.10.0,determine necessary padding
v1.10.0,determine block sizes
v1.10.0,"(R, nb, bsi, bso)"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,apply padding if necessary
v1.10.0,"(n, di) -> (n, nb, bsi)"
v1.10.0,"(n, nb, bsi), (R, nb, bsi, bso) -> (R, n, nb, bso)"
v1.10.0,"(R, n, nb, bso) -> (R * n, do)"
v1.10.0,"note: depending on the contracting order, the output may supporting viewing, or not"
v1.10.0,"(n, R * n), (R * n, do) -> (n, do)"
v1.10.0,remove padding if necessary
v1.10.0,docstr-coverage: inherited
v1.10.0,apply padding if necessary
v1.10.0,"(R * n, n), (n, di) -> (R * n, di)"
v1.10.0,"(R * n, di) -> (R, n, nb, bsi)"
v1.10.0,"(R, nb, bsi, bso), (R, n, nb, bsi) -> (n, nb, bso)"
v1.10.0,"(n, nb, bso) -> (n, do)"
v1.10.0,"note: depending on the contracting order, the output may supporting viewing, or not"
v1.10.0,remove padding if necessary
v1.10.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501
v1.10.0,there are separate decompositions for forward and backward relations.
v1.10.0,the self-loop weight is not decomposed.
v1.10.0,TODO: we could cache the stacked adjacency matrices
v1.10.0,self-loop
v1.10.0,forward messages
v1.10.0,backward messages
v1.10.0,activation
v1.10.0,input validation
v1.10.0,has to be imported now to avoid cyclic imports
v1.10.0,has to be assigned after call to nn.Module init
v1.10.0,Resolve edge weighting
v1.10.0,dropout
v1.10.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.10.0,no activation on last layer
v1.10.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501
v1.10.0,buffering of enriched representations
v1.10.0,docstr-coverage: inherited
v1.10.0,invalidate enriched embeddings
v1.10.0,docstr-coverage: inherited
v1.10.0,Bind fields
v1.10.0,"shape: (num_entities, embedding_dim)"
v1.10.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.10.0,Get random dropout mask
v1.10.0,Apply to edges
v1.10.0,fixed edges -> pre-compute weights
v1.10.0,Cache enriched representations
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Utils
v1.10.0,: the maximum ID (exclusively)
v1.10.0,: the shape of an individual representation
v1.10.0,: a normalizer for individual representations
v1.10.0,: a regularizer for individual representations
v1.10.0,: dropout
v1.10.0,heuristic
v1.10.0,normalize *before* repeating
v1.10.0,repeat if necessary
v1.10.0,regularize *after* repeating
v1.10.0,"dropout & regularizer will appear automatically, since it is a nn.Module"
v1.10.0,has to be imported here to avoid cyclic import
v1.10.0,docstr-coverage: inherited
v1.10.0,normalize num_embeddings vs. max_id
v1.10.0,normalize embedding_dim vs. shape
v1.10.0,work-around until full complex support (torch==1.10 still does not work)
v1.10.0,TODO: verify that this is our understanding of complex!
v1.10.0,"note: this seems to work, as finfo returns the datatype of the underlying floating"
v1.10.0,"point dtype, rather than the combined complex one"
v1.10.0,"use make for initializer since there's a default, and make_safe"
v1.10.0,for the others to pass through None values
v1.10.0,docstr-coverage: inherited
v1.10.0,initialize weights in-place
v1.10.0,docstr-coverage: inherited
v1.10.0,apply constraints in-place
v1.10.0,fixme: work-around until nn.Embedding supports complex
v1.10.0,docstr-coverage: inherited
v1.10.0,fixme: work-around until nn.Embedding supports complex
v1.10.0,verify that contiguity is preserved
v1.10.0,create low-rank approximation object
v1.10.0,"get base representations, shape: (n, *ds)"
v1.10.0,"calculate SVD, U.shape: (n, k), s.shape: (k,), u.shape: (k, prod(ds))"
v1.10.0,overwrite bases and weights
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"get all base representations, shape: (num_bases, *shape)"
v1.10.0,"get base weights, shape: (*batch_dims, num_bases)"
v1.10.0,"weighted linear combination of bases, shape: (*batch_dims, *shape)"
v1.10.0,normalize output dimension
v1.10.0,entity-relation composition
v1.10.0,edge weighting
v1.10.0,message passing weights
v1.10.0,linear relation transformation
v1.10.0,layer-specific self-loop relation representation
v1.10.0,other components
v1.10.0,initialize
v1.10.0,split
v1.10.0,compose
v1.10.0,transform
v1.10.0,normalization
v1.10.0,aggregate by sum
v1.10.0,dropout
v1.10.0,prepare for inverse relations
v1.10.0,update entity representations: mean over self-loops / forward edges / backward edges
v1.10.0,Relation transformation
v1.10.0,has to be imported here to avoid cyclic imports
v1.10.0,kwargs
v1.10.0,Buffered enriched entity and relation representations
v1.10.0,TODO: Check
v1.10.0,TODO: might not be true for all compositions
v1.10.0,hidden dimension normalization
v1.10.0,Create message passing layers
v1.10.0,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.10.0,TODO: This always uses all training triples for message passing
v1.10.0,initialize buffer of enriched representations
v1.10.0,docstr-coverage: inherited
v1.10.0,invalidate enriched embeddings
v1.10.0,docstr-coverage: inherited
v1.10.0,"when changing from evaluation to training mode, the buffered representations have been computed without"
v1.10.0,"gradient tracking. hence, we need to invalidate them."
v1.10.0,note: this occurs in practice when continuing training after evaluation.
v1.10.0,enrich
v1.10.0,docstr-coverage: inherited
v1.10.0,check max_id
v1.10.0,infer shape
v1.10.0,"assign after super, since they should be properly registered as submodules"
v1.10.0,docstr-coverage: inherited
v1.10.0,: the base representations
v1.10.0,: the combination module
v1.10.0,input normalization
v1.10.0,has to be imported here to avoid cyclic import
v1.10.0,create base representations
v1.10.0,verify same ID range
v1.10.0,"note: we could also relax the requiremen, and set max_id = min(max_ids)"
v1.10.0,shape inference
v1.10.0,assign base representations *after* super init
v1.10.0,docstr-coverage: inherited
v1.10.0,delegate to super class
v1.10.0,Generate graph dataset from the Monarch Disease Ontology (MONDO)
v1.10.0,": the assignment from global ID to (representation, local id), shape: (max_id, 2)"
v1.10.0,import here to avoid cyclic import
v1.10.0,instantiate base representations if necessary
v1.10.0,there needs to be at least one base
v1.10.0,"while possible, this might be unintended"
v1.10.0,extract shape
v1.10.0,check for invalid base ids
v1.10.0,check for invalid local indices
v1.10.0,assign modules / buffers *after* super init
v1.10.0,docstr-coverage: inherited
v1.10.0,flatten assignment to ease construction of inverse indices
v1.10.0,we group indices by the representation which provides them
v1.10.0,"thus, we need an inverse to restore the correct order"
v1.10.0,get representations
v1.10.0,update inverse indices
v1.10.0,invert flattening
v1.10.0,import here to avoid cyclic import
v1.10.0,comment: not all representations support passing a shape parameter
v1.10.0,create assignment
v1.10.0,base
v1.10.0,other
v1.10.0,import here to avoid cyclic import
v1.10.0,infer shape
v1.10.0,infer max_id
v1.10.0,docstr-coverage: inherited
v1.10.0,"TODO: can be a combined representations, with appropriate tensor-train combination"
v1.10.0,": shape: (max_id, num_cores)"
v1.10.0,": the bases, length: num_cores, with compatible shapes"
v1.10.0,check shape
v1.10.0,check value range
v1.10.0,"do not increase counter i, since the dimension is shared with the following term"
v1.10.0,i += 1
v1.10.0,ids //= m_i
v1.10.0,import here to avoid cyclic import
v1.10.0,normalize ranks
v1.10.0,"determine M_k, N_k"
v1.10.0,TODO: allow to pass them from outside?
v1.10.0,normalize assignment
v1.10.0,determine shapes and einsum equation
v1.10.0,create base representations
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,abstract
v1.10.0,concrete classes
v1.10.0,default flow
v1.10.0,: the message passing layers
v1.10.0,: the flow direction of messages across layers
v1.10.0,": the edge index, shape: (2, num_edges)"
v1.10.0,fail if dependencies are missing
v1.10.0,avoid cyclic import
v1.10.0,"the base representations, e.g., entity embeddings or features"
v1.10.0,verify max_id
v1.10.0,verify shape
v1.10.0,assign sub-module *after* super call
v1.10.0,initialize layers
v1.10.0,normalize activation
v1.10.0,check consistency
v1.10.0,buffer edge index for message passing
v1.10.0,TODO: inductiveness; we need to
v1.10.0,* replace edge_index
v1.10.0,* replace base representations
v1.10.0,* keep layers & activations
v1.10.0,docstr-coverage: inherited
v1.10.0,we can restrict the message passing to the k-hop neighborhood of the desired indices;
v1.10.0,this does only make sense if we do not request *all* indices
v1.10.0,k_hop_subgraph returns:
v1.10.0,(1) the nodes involved in the subgraph
v1.10.0,(2) the filtered edge_index connectivity
v1.10.0,"(3) the mapping from node indices in node_idx to their new location, and"
v1.10.0,(4) the edge mask indicating which edges were preserved
v1.10.0,we only need the base representations for the neighbor indices
v1.10.0,get *all* base representations
v1.10.0,use *all* edges
v1.10.0,perform message passing
v1.10.0,select desired indices
v1.10.0,docstr-coverage: inherited
v1.10.0,": the edge type, shape: (num_edges,)"
v1.10.0,register an additional buffer for the categorical edge type
v1.10.0,docstr-coverage: inherited
v1.10.0,: the relation representations used to obtain initial edge features
v1.10.0,avoid cyclic import
v1.10.0,docstr-coverage: inherited
v1.10.0,get initial relation representations
v1.10.0,select edge attributes from relation representations according to relation type
v1.10.0,perform message passing
v1.10.0,"apply relation transformation, if necessary"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Classes
v1.10.0,Resolver
v1.10.0,backwards compatibility
v1.10.0,scaling factor
v1.10.0,"modulus ~ Uniform[-s, s]"
v1.10.0,"phase ~ Uniform[0, 2*pi]"
v1.10.0,real part
v1.10.0,purely imaginary quaternions unitary
v1.10.0,this is usually loaded from somewhere else
v1.10.0,"the shape must match, as well as the entity-to-id mapping"
v1.10.0,must be cloned if we want to do backprop
v1.10.0,the color initializer
v1.10.0,variants for the edge index
v1.10.0,additional parameters for iter_weisfeiler_lehman
v1.10.0,normalize shape
v1.10.0,get coloring
v1.10.0,make color initializer
v1.10.0,initialize color representations
v1.10.0,note: this could be a representation?
v1.10.0,init entity representations according to the color
v1.10.0,create random walk matrix
v1.10.0,stack diagonal entries of powers of rw
v1.10.0,abstract
v1.10.0,concrete
v1.10.0,docstr-coverage: inherited
v1.10.0,tokenize
v1.10.0,pad
v1.10.0,get character embeddings
v1.10.0,pool
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: whether the edge weighting needs access to the message
v1.10.0,stub init to enable arbitrary arguments in subclasses
v1.10.0,"Calculate in-degree, i.e. number of incoming edges"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,backward compatibility with RGCN
v1.10.0,docstr-coverage: inherited
v1.10.0,view for heads
v1.10.0,"compute attention coefficients, shape: (num_edges, num_heads)"
v1.10.0,"TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Caches
v1.10.0,"if the sparsity becomes too low, convert to a dense matrix"
v1.10.0,"note: this heuristic is based on the memory consumption,"
v1.10.0,"for a sparse matrix, we store 3 values per nnz (row index, column index, value)"
v1.10.0,"performance-wise, it likely makes sense to switch even earlier"
v1.10.0,`torch.sparse.mm` can also deal with dense 2nd argument
v1.10.0,note: torch.sparse.mm only works for COO matrices;
v1.10.0,@ only works for CSR matrices
v1.10.0,"convert to COO, if necessary"
v1.10.0,"we need to use indices here, since there may be zero diagonal entries"
v1.10.0,: Wikidata SPARQL endpoint. See https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service#Interfacing
v1.10.0,cf. https://meta.wikimedia.org/wiki/User-Agent_policy
v1.10.0,cf. https://wikitech.wikimedia.org/wiki/Robot_policy
v1.10.0,break into smaller requests
v1.10.0,try to load cached first
v1.10.0,determine missing entries
v1.10.0,retrieve information via SPARQL
v1.10.0,save entries
v1.10.0,fill missing descriptions
v1.10.0,for mypy
v1.10.0,get labels & descriptions
v1.10.0,compose labels
v1.10.0,we can have multiple images per entity -> collect image URLs per image
v1.10.0,entity ID
v1.10.0,relation ID
v1.10.0,image URL
v1.10.0,check whether images are still missing
v1.10.0,select on image url per image in a reproducible way
v1.10.0,traverse relations in order of preference
v1.10.0,now there is an image available -> select reproducible by URL sorting
v1.10.0,did not break -> no image
v1.10.0,This import doesn't need a wrapper since it's a transitive
v1.10.0,requirement of PyOBO
v1.10.0,darglint does not like
v1.10.0,"raise cls(shape=shape, reference=reference)"
v1.10.0,1 * ? = ?; ? * 1 = ?
v1.10.0,i**2 = j**2 = k**2 = -1
v1.10.0,i * j = k; i * k = -j
v1.10.0,"j * i = -k, j * k = i"
v1.10.0,k * i = j; k * j = -i
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO test
v1.10.0,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.10.0,: a = \mu^T\Sigma^{-1}\mu
v1.10.0,: b = \log \det \Sigma
v1.10.0,1. Component
v1.10.0,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.10.0,2. Component
v1.10.0,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.10.0,with mu = (mu_1 - mu_0)
v1.10.0,= mu * Sigma_1^-1 mu
v1.10.0,since Sigma_1 is diagonal
v1.10.0,= mu**2 / sigma_1
v1.10.0,3. Component
v1.10.0,4. Component
v1.10.0,ln (det(\Sigma_1) / det(\Sigma_0))
v1.10.0,= ln det Sigma_1 - ln det Sigma_0
v1.10.0,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.10.0,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.10.0,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.10.0,allocate result
v1.10.0,prepare distributions
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO benchmark
v1.10.0,TODO benchmark
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,REPRESENTATION
v1.10.0,base
v1.10.0,concrete
v1.10.0,INITIALIZER
v1.10.0,INTERACTIONS
v1.10.0,Adapter classes
v1.10.0,Concrete Classes
v1.10.0,combinations
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Base Classes
v1.10.0,Adapter classes
v1.10.0,Concrete Classes
v1.10.0,normalize input
v1.10.0,get number of head/relation/tail representations
v1.10.0,flatten list
v1.10.0,split tensors
v1.10.0,broadcasting
v1.10.0,yield batches
v1.10.0,complex typing
v1.10.0,: The symbolic shapes for entity representations
v1.10.0,": The symbolic shapes for entity representations for tail entities, if different."
v1.10.0,": Otherwise, the entity_shape is used for head & tail entities"
v1.10.0,: The symbolic shapes for relation representations
v1.10.0,if the interaction function's head parameter should only receive a subset of entity representations
v1.10.0,if the interaction function's tail parameter should only receive a subset of entity representations
v1.10.0,: the interaction's value range (for unrestricted input)
v1.10.0,"TODO: annotate modelling capabilities? cf., e.g., https://arxiv.org/abs/1902.10197, Table 2"
v1.10.0,"TODO: annotate properties, e.g., symmetry, and use them for testing?"
v1.10.0,TODO: annotate complexity?
v1.10.0,"TODO: cannot cover dynamic shapes, e.g., AutoSF"
v1.10.0,"TODO: we could change that to slicing along multiple dimensions, if necessary"
v1.10.0,: The functional interaction form
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO: update class docstring
v1.10.0,TODO: give this a better name?
v1.10.0,Store initial input for error message
v1.10.0,All are None -> try and make closest to square
v1.10.0,Only input channels is None
v1.10.0,Only width is None
v1.10.0,Only height is none
v1.10.0,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.10.0,Width and input channels are None -> set input channels to 1 and calculate width
v1.10.0,vector & scalar offset
v1.10.0,": The head-relation encoder operating on 2D ""images"""
v1.10.0,: The head-relation encoder operating on the 1D flattened version
v1.10.0,: The interaction function
v1.10.0,Automatic calculation of remaining dimensions
v1.10.0,Parameter need to fulfil:
v1.10.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.10.0,normalize kernel height
v1.10.0,encoders
v1.10.0,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.10.0,"2: 1D encoder: FC, DO, BN?, Act"
v1.10.0,store reshaping dimensions
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,The interaction model
v1.10.0,docstr-coverage: inherited
v1.10.0,Use Xavier initialization for weight; bias to zero
v1.10.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.10.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.10.0,docstr-coverage: inherited
v1.10.0,normalize hidden_dim
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,Initialize biases with zero
v1.10.0,"In the original formulation,"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,Global entity projection
v1.10.0,Global relation projection
v1.10.0,Global combination bias
v1.10.0,Global combination bias
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,default core tensor initialization
v1.10.0,cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12
v1.10.0,normalize initializer
v1.10.0,normalize relation dimension
v1.10.0,Core tensor
v1.10.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.10.0,Dropout
v1.10.0,docstr-coverage: inherited
v1.10.0,instantiate here to make module easily serializable
v1.10.0,"batch norm gets reset automatically, since it defines reset_parameters"
v1.10.0,shapes
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,there are separate biases for entities in head and tail position
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,with k=4
v1.10.0,the base interaction
v1.10.0,forward entity/relation shapes
v1.10.0,The parameters of the affine transformation: bias
v1.10.0,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,head position and bump
v1.10.0,relation box: head
v1.10.0,relation box: tail
v1.10.0,tail position and bump
v1.10.0,docstr-coverage: inherited
v1.10.0,input normalization
v1.10.0,Core tensor
v1.10.0,docstr-coverage: inherited
v1.10.0,initialize core tensor
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"r_head, r_mid, r_tail"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,type alias for AutoSF block description
v1.10.0,"head_index, relation_index, tail_index, sign"
v1.10.0,: a description of the block structure
v1.10.0,convert to tuple
v1.10.0,infer the number of entity and relation representations
v1.10.0,verify coefficients
v1.10.0,dynamic entity / relation shapes
v1.10.0,docstr-coverage: inherited
v1.10.0,"r_head, r_bias, r_tail"
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,docstr-coverage: excused `wrapped`
v1.10.0,"repeat if necessary, and concat head and relation"
v1.10.0,"shape: -1, num_input_channels, 2*height, width"
v1.10.0,"shape: -1, num_input_channels, 2*height, width"
v1.10.0,"-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.10.0,"reshape: (-1, dim) -> (*batch_dims, dim)"
v1.10.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.10.0,output_shape: batch_dims
v1.10.0,add bias term
v1.10.0,"cat into shape (..., 1, d, 3)"
v1.10.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.10.0,"Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)"
v1.10.0,shortcut for same shape
v1.10.0,split weight into head-/relation-/tail-specific sub-matrices
v1.10.0,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.10.0,"Predict t embedding, shape: (*batch_dims, d)"
v1.10.0,dot product
v1.10.0,"composite: (*batch_dims, d)"
v1.10.0,inner product with relation embedding
v1.10.0,Circular correlation of entity embeddings
v1.10.0,complex conjugate
v1.10.0,Hadamard product in frequency domain
v1.10.0,inverse real FFT
v1.10.0,global projections
v1.10.0,"combination, shape: (*batch_dims, d)"
v1.10.0,dot product with t
v1.10.0,r expresses a rotation in complex plane.
v1.10.0,rotate head by relation (=Hadamard product in complex space)
v1.10.0,rotate tail by inverse of relation
v1.10.0,The inverse rotation is expressed by the complex conjugate of r.
v1.10.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.10.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.10.0,|h * r - t| = |h - conj(r) * t|
v1.10.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.10.0,"That is not mentioned in the paper, so it is made optional here."
v1.10.0,Project entities
v1.10.0,h projection to hyperplane
v1.10.0,r
v1.10.0,-t projection to hyperplane
v1.10.0,project to relation specific subspace
v1.10.0,ensure constraints
v1.10.0,x_1 contraction
v1.10.0,x_2 contraction
v1.10.0,"TODO: this sign is in the official code, too, but why do we need it?"
v1.10.0,head interaction
v1.10.0,relation interaction (notice that h has been updated)
v1.10.0,combination
v1.10.0,similarity
v1.10.0,head
v1.10.0,relation box: head
v1.10.0,relation box: tail
v1.10.0,tail
v1.10.0,power norm
v1.10.0,the relation-specific head box base shape (normalized to have a volume of 1):
v1.10.0,the relation-specific tail box base shape (normalized to have a volume of 1):
v1.10.0,head
v1.10.0,relation
v1.10.0,tail
v1.10.0,version 2: relation factor offset
v1.10.0,extension: negative (power) norm
v1.10.0,note: normalization should be done from the representations
v1.10.0,cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501
v1.10.0,version 2
v1.10.0,r_head = r_head + u * torch.ones_like(r_head)
v1.10.0,r_tail = r_tail + u * torch.ones_like(r_tail)
v1.10.0,"stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)"
v1.10.0,"remember shape for output, but reshape for transformer"
v1.10.0,"get position embeddings, shape: (seq_len, dim)"
v1.10.0,Now we are position-dependent w.r.t qualifier pairs.
v1.10.0,"seq_length, batch_size, dim"
v1.10.0,Pool output
v1.10.0,"output shape: (batch_size, dim)"
v1.10.0,reshape
v1.10.0,head
v1.10.0,relation
v1.10.0,tail
v1.10.0,extension: negative (power) norm
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Concrete classes
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,input normalization
v1.10.0,instantiate separate combinations
v1.10.0,docstr-coverage: inherited
v1.10.0,split complex; repeat real
v1.10.0,separately combine real and imaginary parts
v1.10.0,combine
v1.10.0,docstr-coverage: inherited
v1.10.0,symbolic output to avoid dtype issue
v1.10.0,we only need to consider real part here
v1.10.0,the gate
v1.10.0,the combination
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Resolver
v1.10.0,Base classes
v1.10.0,Concrete classes
v1.10.0,TODO: allow relative
v1.10.0,isin() preserves the sorted order
v1.10.0,docstr-coverage: inherited
v1.10.0,sort by decreasing degree
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,sort by decreasing page rank
v1.10.0,docstr-coverage: inherited
v1.10.0,input normalization
v1.10.0,determine absolute number of anchors for each strategy
v1.10.0,if pre-instantiated
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: the token ID of the padding token
v1.10.0,: the token representations
v1.10.0,: the assigned tokens for each entity
v1.10.0,needs to be lazily imported to avoid cyclic imports
v1.10.0,fill padding (nn.Embedding cannot deal with negative indices)
v1.10.0,"sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs"
v1.10.0,"contain a subset of training relations) - for that, the padding index is the last index of the Representation"
v1.10.0,resolve token representation
v1.10.0,input validation
v1.10.0,register as buffer
v1.10.0,assign sub-module
v1.10.0,apply tokenizer
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"get token IDs, shape: (*, num_chosen_tokens)"
v1.10.0,"lookup token representations, shape: (*, num_chosen_tokens, *shape)"
v1.10.0,": A list with ratios per representation in their creation order,"
v1.10.0,": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`"
v1.10.0,": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95"
v1.10.0,normalize triples
v1.10.0,inverse triples are created afterwards implicitly
v1.10.0,tokenize
v1.10.0,Create an MLP for string aggregation
v1.10.0,note: the token representations' shape includes the number of tokens as leading dim
v1.10.0,unique hashes per representation
v1.10.0,unique hashes if we concatenate all representations together
v1.10.0,TODO: vectorization?
v1.10.0,remove self-loops
v1.10.0,add inverse edges and remove duplicates
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Resolver
v1.10.0,Base classes
v1.10.0,Concrete classes
v1.10.0,docstr-coverage: inherited
v1.10.0,tokenize: represent entities by bag of relations
v1.10.0,collect candidates
v1.10.0,randomly sample without replacement num_tokens relations for each entity
v1.10.0,TODO: expose num_anchors?
v1.10.0,select anchors
v1.10.0,find closest anchors
v1.10.0,convert to torch
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"To prevent possible segfaults in the METIS C code, METIS expects a graph"
v1.10.0,(1) without self-loops; (2) with inverse edges added; (3) with unique edges only
v1.10.0,https://github.com/KarypisLab/METIS/blob/94c03a6e2d1860128c2d0675cbbb86ad4f261256/libmetis/checkgraph.c#L18
v1.10.0,select independently per partition
v1.10.0,select adjacency part;
v1.10.0,"note: the indices will automatically be in [0, ..., high - low), since they are *local* indices"
v1.10.0,offset
v1.10.0,the -1 comes from the shared padding token
v1.10.0,note: permutation will be later on reverted
v1.10.0,add back 1 for the shared padding token
v1.10.0,TODO: check if perm is used correctly
v1.10.0,verify pool
v1.10.0,docstr-coverage: inherited
v1.10.0,choose first num_tokens
v1.10.0,TODO: vectorization?
v1.10.0,heuristic
v1.10.0,heuristic
v1.10.0,calculate configuration digest
v1.10.0,create anchor selection instance
v1.10.0,select anchors
v1.10.0,anchor search (=anchor assignment?)
v1.10.0,assign anchors
v1.10.0,save
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Resolver
v1.10.0,Base classes
v1.10.0,Concrete classes
v1.10.0,docstr-coverage: inherited
v1.10.0,"contains: anchor_ids, entity_ids, mapping {entity_id -> {""ancs"": anchors, ""dists"": distances}}"
v1.10.0,normalize anchor_ids
v1.10.0,cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541
v1.10.0,TODO: keep distances?
v1.10.0,ensure parent directory exists
v1.10.0,save via torch.save
v1.10.0,docstr-coverage: inherited
v1.10.0,"TODO: since we save a contiguous array of (num_entities, num_anchors),"
v1.10.0,"it would be more efficient to not convert to a mapping, but directly select from the tensor"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Anchor Searchers
v1.10.0,Anchor Selection
v1.10.0,Tokenizers
v1.10.0,Token Loaders
v1.10.0,Representations
v1.10.0,Data containers
v1.10.0,"TODO: use graph library, such as igraph, graph-tool, or networkit"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Resolver
v1.10.0,Base classes
v1.10.0,Concrete classes
v1.10.0,docstr-coverage: inherited
v1.10.0,convert to adjacency matrix
v1.10.0,convert to scipy sparse csr
v1.10.0,"compute distances between anchors and all nodes, shape: (num_anchors, num_entities)"
v1.10.0,TODO: padding for unreachable?
v1.10.0,select anchor IDs with smallest distance
v1.10.0,docstr-coverage: inherited
v1.10.0,infer shape
v1.10.0,create adjacency matrix
v1.10.0,symmetric + self-loops
v1.10.0,"for each entity, determine anchor pool by BFS"
v1.10.0,an array storing whether node i is reachable by anchor j
v1.10.0,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.10.0,the output
v1.10.0,anchor nodes have themselves as a starting found anchor
v1.10.0,TODO: take all (q-1) hop neighbors before selecting from q-hop
v1.10.0,propagate one hop
v1.10.0,convergence check
v1.10.0,copy pool if we have seen enough anchors and have not yet stopped
v1.10.0,stop once we have enough
v1.10.0,TODO: can we replace this loop with something vectorized?
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,symmetric + self-loops
v1.10.0,"for each entity, determine anchor pool by BFS"
v1.10.0,an array storing whether node i is reachable by anchor j
v1.10.0,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.10.0,the output that track the distance to each found anchor
v1.10.0,"dtype is unsigned int 8 bit, so we initialize the maximum distance to 255 (or max default)"
v1.10.0,initial anchors are 0-hop away from themselves
v1.10.0,propagate one hop
v1.10.0,TODO the float() trick for GPU result stability until the torch_sparse issue is resolved
v1.10.0,https://github.com/rusty1s/pytorch_sparse/issues/243
v1.10.0,convergence check
v1.10.0,newly reached is a mask that points to newly discovered anchors at this particular step
v1.10.0,implemented as element-wise XOR (will only give True in 0 XOR 1 or 1 XOR 0)
v1.10.0,"in our case we enrich the set of found anchors, so we can only have values turning 0 to 1, eg 0 XOR 1"
v1.10.0,copy pool if we have seen enough anchors and have not yet stopped
v1.10.0,"update the value in the pool by the current hop value (we start from 0, so +1 be default)"
v1.10.0,stop once we have enough
v1.10.0,sort the pool by nearest to farthest anchors
v1.10.0,values with distance 255 (or max for unsigned int8 type) are padding tokens
v1.10.0,"since the output is sorted, no need for random sampling, we just take top-k nearest"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"select k anchors with largest ppr, shape: (batch_size, k)"
v1.10.0,prepare adjacency matrix only once
v1.10.0,prepare result
v1.10.0,progress bar?
v1.10.0,batch-wise computation of PPR
v1.10.0,"run page-rank calculation, shape: (batch_size, n)"
v1.10.0,"select PPR values for the anchors, shape: (batch_size, num_anchors)"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Base classes
v1.10.0,Concrete classes
v1.10.0,
v1.10.0,
v1.10.0,
v1.10.0,
v1.10.0,
v1.10.0,Misc
v1.10.0,
v1.10.0,rank based metrics do not need binarized scores
v1.10.0,: the supported rank types. Most of the time equal to all rank types
v1.10.0,: whether the metric requires the number of candidates for each ranking task
v1.10.0,normalize confidence level
v1.10.0,sample metric values
v1.10.0,"bootstrap estimator (i.e., compute on sample with replacement)"
v1.10.0,cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function
v1.10.0,: The rank-based metric class that this derived metric extends
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"since scale and offset are constant for a given number of candidates, we have"
v1.10.0,E[scale * M + offset] = scale * E[M] + offset
v1.10.0,docstr-coverage: inherited
v1.10.0,"since scale and offset are constant for a given number of candidates, we have"
v1.10.0,V[scale * M + offset] = scale^2 * V[M]
v1.10.0,: Z-adjusted metrics are formulated to be increasing
v1.10.0,: Z-adjusted metrics can only be applied to realistic ranks
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,should be exactly 0.0
v1.10.0,docstr-coverage: inherited
v1.10.0,should be exactly 1.0
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,: Expectation/maximum reindexed metrics are formulated to be increasing
v1.10.0,: Expectation/maximum reindexed metrics can only be applied to realistic ranks
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,should be exactly 0.0
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2
v1.10.0,use V[x] = E[x^2] - E[x]^2
v1.10.0,group by same weight -> compute H_w(n) for multiple n at once
v1.10.0,we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once
v1.10.0,now select from precomputed cumulative sums and aggregate
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective"
v1.10.0,cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO: should we return the sum of weights?
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.10.0,docstr-coverage: inherited
v1.10.0,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: the lower bound
v1.10.0,: whether the lower bound is inclusive
v1.10.0,: the upper bound
v1.10.0,: whether the upper bound is inclusive
v1.10.0,: The name of the metric
v1.10.0,: a link to further information
v1.10.0,: whether the metric needs binarized scores
v1.10.0,": whether it is increasing, i.e., larger values are better"
v1.10.0,: the value range
v1.10.0,: synonyms for this metric
v1.10.0,: whether the metric supports weights
v1.10.0,: whether there is a closed-form solution of the expectation
v1.10.0,: whether there is a closed-form solution of the variance
v1.10.0,normalize weights
v1.10.0,calculate weighted harmonic mean
v1.10.0,calculate cdf
v1.10.0,determine value at p=0.5
v1.10.0,special case for exactly 0.5
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: A description of the metric
v1.10.0,: The function that runs the metric
v1.10.0,docstr-coverage: inherited
v1.10.0,: Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics
v1.10.0,: This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`
v1.10.0,"TODO there's something wrong with this, so add it later"
v1.10.0,classifier_annotator.higher(
v1.10.0,"rmc.pr_auc_score,"
v1.10.0,"name=""AUC-PR"","
v1.10.0,"description=""Area Under the Precision-Recall Curve"","
v1.10.0,"link=""https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score"","
v1.10.0,)
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,don't worry about functions because they can't be specified by JSON.
v1.10.0,Could make a better mo
v1.10.0,later could extend for other non-JSON valid types
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Score with original triples
v1.10.0,Score with inverse triples
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,noqa:DAR101
v1.10.0,noqa:DAR401
v1.10.0,Create directory in which all experimental artifacts are saved
v1.10.0,noqa:DAR101
v1.10.0,clip for node piece configurations
v1.10.0,"""pykeen experiments reproduce"" expects ""model reference dataset"""
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO: take care that triples aren't removed that are the only ones with any given entity
v1.10.0,distribute the deteriorated triples across the remaining factories
v1.10.0,"'kinships',"
v1.10.0,"'umls',"
v1.10.0,"'codexsmall',"
v1.10.0,"'wn18',"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: Functions for specifying exotic resources with a given prefix
v1.10.0,: Functions for specifying exotic resources based on their file extension
v1.10.0,Input validation
v1.10.0,convert to numpy
v1.10.0,Additional columns
v1.10.0,convert PyTorch tensors to numpy
v1.10.0,convert to dataframe
v1.10.0,Re-order columns
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.10.0,TODO vectorize code
v1.10.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,save literal-to-id mapping
v1.10.0,save numeric literals
v1.10.0,load literal-to-id
v1.10.0,load literals
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Split triples
v1.10.0,Sorting ensures consistent results when the triples are permuted
v1.10.0,Create mapping
v1.10.0,Sorting ensures consistent results when the triples are permuted
v1.10.0,Create mapping
v1.10.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.10.0,Filter all non-existent triples
v1.10.0,Note: Unique changes the order of the triples
v1.10.0,Note: Using unique means implicit balancing of training samples
v1.10.0,normalize input
v1.10.0,: The mapping from labels to IDs.
v1.10.0,: The inverse mapping for label_to_id; initialized automatically
v1.10.0,: A vectorized version of entity_label_to_id; initialized automatically
v1.10.0,: A vectorized version of entity_id_to_label; initialized automatically
v1.10.0,Normalize input
v1.10.0,label
v1.10.0,Filter for entities
v1.10.0,Filter for relations
v1.10.0,No filter
v1.10.0,: the number of unique entities
v1.10.0,": the number of relations (maybe including ""artificial"" inverse relations)"
v1.10.0,: whether to create inverse triples
v1.10.0,": the number of real relations, i.e., without artificial inverses"
v1.10.0,ensure torch.Tensor
v1.10.0,input validation
v1.10.0,"always store as torch.long, i.e., torch's default integer dtype"
v1.10.0,check new label to ID mappings
v1.10.0,Make new triples factories for each group
v1.10.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.0,prepare metadata
v1.10.0,Delegate to function
v1.10.0,"restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed"
v1.10.0,docstr-coverage: inherited
v1.10.0,load base
v1.10.0,load numeric triples
v1.10.0,store numeric triples
v1.10.0,store metadata
v1.10.0,note: num_relations will be doubled again when instantiating with create_inverse_triples=True
v1.10.0,Check if the triples are inverted already
v1.10.0,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.10.0,contained if and only if create_inverse_triples is True.
v1.10.0,Generate entity mapping if necessary
v1.10.0,Generate relation mapping if necessary
v1.10.0,Map triples of labels to triples of IDs.
v1.10.0,TODO: Check if lazy evaluation would make sense
v1.10.0,docstr-coverage: inherited
v1.10.0,store entity/relation to ID
v1.10.0,load entity/relation to ID
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,pre-filter to keep only topk
v1.10.0,if top is larger than the number of available options
v1.10.0,Generate a word cloud image
v1.10.0,docstr-coverage: inherited
v1.10.0,vectorized label lookup
v1.10.0,Re-order columns
v1.10.0,docstr-coverage: inherited
v1.10.0,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.10.0,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.10.0,ID-based triples
v1.10.0,labeled triples
v1.10.0,make sure triples are a numpy array
v1.10.0,make sure triples are 2d
v1.10.0,convert to ID-based
v1.10.0,triples factory
v1.10.0,all keyword-based options have been none
v1.10.0,delegate to keyword-based get_mapped_triples to re-use optional validation logic
v1.10.0,delegate to keyword-based get_mapped_triples to re-use optional validation logic
v1.10.0,only labeled triples are remaining
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Split indices
v1.10.0,Split triples
v1.10.0,select one triple per relation
v1.10.0,maintain set of covered entities
v1.10.0,"Select one triple for each head/tail entity, which is not yet covered."
v1.10.0,create mask
v1.10.0,Prepare split index
v1.10.0,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.10.0,base cases
v1.10.0,IDs not in training
v1.10.0,triples with exclusive test IDs
v1.10.0,docstr-coverage: inherited
v1.10.0,While there are still triples that should be moved to the training set
v1.10.0,Pick a random triple to move over to the training triples
v1.10.0,add to training
v1.10.0,remove from testing
v1.10.0,Recalculate the move_id_mask
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,Make sure that the first element has all the right stuff in it
v1.10.0,docstr-coverage: inherited
v1.10.0,backwards compatibility
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,constants
v1.10.0,constants
v1.10.0,unary
v1.10.0,binary
v1.10.0,ternary
v1.10.0,column names
v1.10.0,return candidates
v1.10.0,index triples
v1.10.0,incoming relations per entity
v1.10.0,outgoing relations per entity
v1.10.0,indexing triples for fast join r1 & r2
v1.10.0,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.10.0,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.10.0,actual evaluation of the pattern
v1.10.0,skip empty support
v1.10.0,TODO: Can this happen after pre-filtering?
v1.10.0,"sort first, for triple order invariance"
v1.10.0,TODO: what is the support?
v1.10.0,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.10.0,sort decreasingly. i dominates j for all j > i in x-dimension
v1.10.0,"if it is also dominated by any y, it is not part of the skyline"
v1.10.0,"group by (relation id, pattern type)"
v1.10.0,"for each group, yield from skyline"
v1.10.0,determine patterns from triples
v1.10.0,drop zero-confidence
v1.10.0,keep only skyline
v1.10.0,create data frame
v1.10.0,iterate relation types
v1.10.0,drop zero-confidence
v1.10.0,keep only skyline
v1.10.0,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.10.0,base = skyline(base)
v1.10.0,create data frame
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO: the same
v1.10.0,": the positive triples, shape: (batch_size, 3)"
v1.10.0,": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)"
v1.10.0,": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)"
v1.10.0,noqa:DAR202
v1.10.0,noqa:DAR401
v1.10.0,TODO: some negative samplers require batches
v1.10.0,"shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.10.0,"each shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset
v1.10.0,docstr-coverage: inherited
v1.10.0,indexing
v1.10.0,initialize
v1.10.0,sample iteratively
v1.10.0,determine weights
v1.10.0,randomly choose a vertex which has not been chosen yet
v1.10.0,normalize to probabilities
v1.10.0,sample a start node
v1.10.0,get list of neighbors
v1.10.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.10.0,visit target node
v1.10.0,decrease sample counts
v1.10.0,docstr-coverage: inherited
v1.10.0,convert to csr for fast row slicing
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,safe division for empty sets
v1.10.0,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.10.0,duplicates
v1.10.0,we are not interested in self-similarity
v1.10.0,compute similarities
v1.10.0,Calculate which relations are the inverse ones
v1.10.0,get existing IDs
v1.10.0,remove non-existing ID from label mapping
v1.10.0,create translation tensor
v1.10.0,get entities and relations occurring in triples
v1.10.0,generate ID translation and new label to Id mappings
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.10.0,seamless loading and saving of training checkpoints
v1.10.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.10.0,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.10.0,"If a checkpoint file is given, it must be loaded if it exists already"
v1.10.0,"If the stopper dict has any keys, those are written back to the stopper"
v1.10.0,The checkpoint frequency needs to be set to save checkpoints
v1.10.0,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.10.0,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.10.0,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.10.0,send model to device before going into the internal training loop
v1.10.0,Ensure the release of memory
v1.10.0,Clear optimizer
v1.10.0,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.10.0,due to the patience continue to train after the best epoch and thus alter the model
v1.10.0,Create a path
v1.10.0,Prepare all of the callbacks
v1.10.0,"Register a callback for the result tracker, if given"
v1.10.0,"Register a callback for the early stopper, if given"
v1.10.0,TODO should mode be passed here?
v1.10.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.10.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.10.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.10.0,return the relevant parameters slice_size and batch_size
v1.10.0,Force weight initialization if training continuation is not explicitly requested.
v1.10.0,Reset the weights
v1.10.0,"afterwards, some parameters may be on the wrong device"
v1.10.0,Create new optimizer
v1.10.0,Create a new lr scheduler and add the optimizer
v1.10.0,Ensure the model is on the correct device
v1.10.0,"When size probing, we don't want progress bars"
v1.10.0,Create progress bar
v1.10.0,Save the time to track when the saved point was available
v1.10.0,Training Loop
v1.10.0,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.10.0,Enforce training mode
v1.10.0,Accumulate loss over epoch
v1.10.0,Batching
v1.10.0,Only create a progress bar when not in size probing mode
v1.10.0,Flag to check when to quit the size probing
v1.10.0,Recall that torch *accumulates* gradients. Before passing in a
v1.10.0,"new instance, you need to zero out the gradients from the old instance"
v1.10.0,Get batch size of current batch (last batch may be incomplete)
v1.10.0,accumulate gradients for whole batch
v1.10.0,forward pass call
v1.10.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.10.0,update parameters according to optimizer
v1.10.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.10.0,constraints are no longer applied
v1.10.0,For testing purposes we're only interested in processing one batch
v1.10.0,When size probing we don't need the losses
v1.10.0,Update learning rate scheduler
v1.10.0,Track epoch loss
v1.10.0,"note: this epoch loss can be slightly biased towards the last batch, if this is smaller than the rest"
v1.10.0,"in practice, this should have a minor effect, since typically batch_size << num_instances"
v1.10.0,Print loss information to console
v1.10.0,Save the last successful finished epoch
v1.10.0,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.10.0,During automatic memory optimization only the error message is of interest
v1.10.0,When there wasn't a best epoch the checkpoint path should be None
v1.10.0,Delete temporary best epoch model
v1.10.0,Includes a call to result_tracker.log_metrics
v1.10.0,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.10.0,MyPy overrides are because you should
v1.10.0,When there wasn't a best epoch the checkpoint path should be None
v1.10.0,Delete temporary best epoch model
v1.10.0,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.10.0,at that state
v1.10.0,Delete temporary best epoch model
v1.10.0,forward pass
v1.10.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.10.0,correction for loss reduction
v1.10.0,backward pass
v1.10.0,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.10.0,Set upper bound
v1.10.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.10.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.0,"Only if a cuda device is available, the random state is accessed"
v1.10.0,This is an entire checkpoint for the optional best model when using early stopping
v1.10.0,Saving triples factory related states
v1.10.0,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.10.0,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.10.0,Check whether the triples factory mappings match those from the checkpoints
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Shuffle each epoch
v1.10.0,Lazy-splitting into batches
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,docstr-coverage: inherited
v1.10.0,disable automatic batching
v1.10.0,docstr-coverage: inherited
v1.10.0,Slicing is not possible in sLCWA training loops
v1.10.0,split batch
v1.10.0,send to device
v1.10.0,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.10.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.10.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.10.0,Compute negative and positive scores
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,Slicing is not possible for sLCWA
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,lazy init
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO how to pass inductive mode
v1.10.0,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.10.0,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.10.0,: A hint for constructing a :class:`MultiTrainingCallback`
v1.10.0,: A collection of callbacks
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,normalize target column
v1.10.0,The type inference is so confusing between the function switching
v1.10.0,and polymorphism introduced by slicability that these need to be ignored
v1.10.0,Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number
v1.10.0,of total entities from another inductive inference factory
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,Split batch components
v1.10.0,Send batch to device
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.10.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.10.0,"note: we use Tuple[Tensor] here, so we can re-use TensorDataset instead of having to create a custom one"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,unpack
v1.10.0,Send batch to device
v1.10.0,head prediction
v1.10.0,TODO: exploit sparsity
v1.10.0,"note: this is different to what we do for LCWA, where we collect *all* training entities"
v1.10.0,for which the combination is true
v1.10.0,tail prediction
v1.10.0,TODO: exploit sparsity
v1.10.0,regularization
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO?
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,To make MyPy happy
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,now: smaller is better
v1.10.0,: the number of reported results with no improvement after which training will be stopped
v1.10.0,the minimum relative improvement necessary to consider it an improved result
v1.10.0,"whether a larger value is better, or a smaller."
v1.10.0,: The epoch at which the best result occurred
v1.10.0,: The best result so far
v1.10.0,: The remaining patience
v1.10.0,check for improvement
v1.10.0,stop if the result did not improve more than delta for patience evaluations
v1.10.0,: The model
v1.10.0,: The evaluator
v1.10.0,: The triples to use for training (to be used during filtered evaluation)
v1.10.0,: The triples to use for evaluation
v1.10.0,: Size of the evaluation batches
v1.10.0,: Slice size of the evaluation batches
v1.10.0,: The number of epochs after which the model is evaluated on validation set
v1.10.0,: The number of iterations (one iteration can correspond to various epochs)
v1.10.0,: with no improvement after which training will be stopped.
v1.10.0,: The name of the metric to use
v1.10.0,: The minimum relative improvement necessary to consider it an improved result
v1.10.0,: The metric results from all evaluations
v1.10.0,": Whether a larger value is better, or a smaller"
v1.10.0,: The result tracker
v1.10.0,: Callbacks when after results are calculated
v1.10.0,: Callbacks when training gets continued
v1.10.0,: Callbacks when training is stopped early
v1.10.0,: Did the stopper ever decide to stop?
v1.10.0,: the path to the weights of the best model
v1.10.0,: whether to delete the file with the best model weights after termination
v1.10.0,: note: the weights will be re-loaded into the model before
v1.10.0,TODO: Fix this
v1.10.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.10.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.10.0,for mypy
v1.10.0,Evaluate
v1.10.0,Only perform time consuming checks for the first call.
v1.10.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.10.0,Append to history
v1.10.0,TODO need a test that this all re-instantiates properly
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Utils
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,dataset
v1.10.0,model
v1.10.0,stored outside of the training loop / optimizer to give access to auto-tuning from Lightning
v1.10.0,optimizer
v1.10.0,"TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA"
v1.10.0,docstr-coverage: inherited
v1.10.0,call post_parameter_update
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO: sub-batching / slicing
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO:
v1.10.0,"shuffle=shuffle,"
v1.10.0,"drop_last=drop_last,"
v1.10.0,"sampler=sampler,"
v1.10.0,"shuffle=shuffle,"
v1.10.0,disable automatic batching in data loader
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO: sub-batching / slicing
v1.10.0,docstr-coverage: inherited
v1.10.0,"note: since this file is executed via __main__, its module name is replaced by __name__"
v1.10.0,"hence, the two classes' fully qualified names start with ""_"" and are considered private"
v1.10.0,cf. https://github.com/cthoyt/class-resolver/issues/39
v1.10.0,automatically choose accelerator
v1.10.0,defaults to TensorBoard; explicitly disabled here
v1.10.0,disable checkpointing
v1.10.0,mixed precision training
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,parsing metrics
v1.10.0,metric pattern = side?.type?.metric.k?
v1.10.0,: The metric key
v1.10.0,": Side of the metric, or ""both"""
v1.10.0,: The rank type
v1.10.0,normalize metric name
v1.10.0,normalize side
v1.10.0,normalize rank type
v1.10.0,normalize keys
v1.10.0,TODO: this can only normalize rank-based metrics!
v1.10.0,TODO: find a better way to handle this
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO: fix this upstream / make metric.score comply to signature
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,Transfer to cpu and convert to numpy
v1.10.0,Ensure that each key gets counted only once
v1.10.0,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"Because the order of the values of an dictionary is not guaranteed,"
v1.10.0,we need to retrieve scores and masks using the exact same key order.
v1.10.0,Clear buffers
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The optimistic rank is the rank when assuming all options with an equal score are placed
v1.10.0,: behind the current test triple.
v1.10.0,": shape: (batch_size,)"
v1.10.0,": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank"
v1.10.0,: over all permutations of the elements with the same score as the currently considered option.
v1.10.0,": shape: (batch_size,)"
v1.10.0,: The pessimistic rank is the rank when assuming all options with an equal score are placed
v1.10.0,: in front of current test triple.
v1.10.0,": shape: (batch_size,)"
v1.10.0,: The number of options is the number of items considered in the ranking. It may change for
v1.10.0,: filtered evaluation
v1.10.0,": shape: (batch_size,)"
v1.10.0,The optimistic rank is the rank when assuming all options with an
v1.10.0,"equal score are placed behind the currently considered. Hence, the"
v1.10.0,"rank is the number of options with better scores, plus one, as the"
v1.10.0,rank is one-based.
v1.10.0,The pessimistic rank is the rank when assuming all options with an
v1.10.0,"equal score are placed in front of the currently considered. Hence,"
v1.10.0,the rank is the number of options which have at least the same score
v1.10.0,minus one (as the currently considered option in included in all
v1.10.0,"options). As the rank is one-based, we have to add 1, which nullifies"
v1.10.0,"the ""minus 1"" from before."
v1.10.0,The realistic rank is the average of the optimistic and pessimistic
v1.10.0,"rank, and hence the expected rank over all permutations of the elements"
v1.10.0,with the same score as the currently considered option.
v1.10.0,"We set values which should be ignored to NaN, hence the number of options"
v1.10.0,which should be considered is given by
v1.10.0,": the scores of the true choice, shape: (*bs), dtype: float"
v1.10.0,": the number of scores which were larger than the true score, shape: (*bs), dtype: long"
v1.10.0,": the number of scores which were not smaller than the true score, shape: (*bs), dtype: long"
v1.10.0,": the total number of compared scores, shape: (*bs), dtype: long"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"TODO remove this, it makes code much harder to reason about"
v1.10.0,add mode parameter
v1.10.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.10.0,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.10.0,Clear the ranks from the current evaluator
v1.10.0,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.10.0,do not display progress bar while searching
v1.10.0,start by searching for batch_size
v1.10.0,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.10.0,we do not need to repeat time-consuming checks
v1.10.0,infer start value
v1.10.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r), (h, t) or (r, t)) scored on all"
v1.10.0,"entities/relations, must have failed to start slice_size search, we start with trying half the"
v1.10.0,entities/relations.
v1.10.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.0,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.10.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.0,values_dict[key] will always be an int at this point
v1.10.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.10.0,"if inverse triples are used, we only do score_t (TODO: by default; can this be changed?)"
v1.10.0,"otherwise, i.e., without inverse triples, we also need score_h"
v1.10.0,"if relations are to be predicted, we need to slice score_r"
v1.10.0,"raise an error, if any of the required methods cannot slice"
v1.10.0,Split batch
v1.10.0,Bind shape
v1.10.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.10.0,Warn if all entities will be filtered
v1.10.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.10.0,TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place
v1.10.0,verify that the triples have been filtered
v1.10.0,Filter triples if necessary
v1.10.0,Send to device
v1.10.0,Ensure evaluation mode
v1.10.0,Prepare for result filtering
v1.10.0,Send tensors to device
v1.10.0,Prepare batches
v1.10.0,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.10.0,Show progressbar
v1.10.0,Flag to check when to quit the size probing
v1.10.0,Disable gradient tracking
v1.10.0,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.10.0,batch-wise processing
v1.10.0,If we only probe sizes we do not need more than one batch
v1.10.0,Finalize
v1.10.0,Create filter
v1.10.0,Select scores of true
v1.10.0,overwrite filtered scores
v1.10.0,The scores for the true triples have to be rewritten to the scores tensor
v1.10.0,the rank-based evaluators needs the true scores with trailing 1-dim
v1.10.0,Create a positive mask with the size of the scores from the positive filter
v1.10.0,Restrict to entities of interest
v1.10.0,process scores
v1.10.0,optionally restrict triples (nop if no restriction)
v1.10.0,evaluation triples as dataframe
v1.10.0,determine filter triples
v1.10.0,infer num_entities if not given
v1.10.0,"TODO: unique, or max ID + 1?"
v1.10.0,optionally restrict triples
v1.10.0,compute candidate set sizes for different targets
v1.10.0,TODO: extend to relations?
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Evaluation loops
v1.10.0,Evaluation datasets
v1.10.0,: the MemoryUtilizationMaximizer instance for :func:`_evaluate`.
v1.10.0,batch
v1.10.0,tqdm
v1.10.0,data loader
v1.10.0,set upper limit of batch size for automatic memory optimization
v1.10.0,set model to evaluation mode
v1.10.0,delegate to AMO wrapper
v1.10.0,"The key-id for each triple, shape: (num_triples,)"
v1.10.0,": the number of targets for each key, shape: (num_unique_keys + 1,)"
v1.10.0,: the concatenation of unique targets for each key (use bounds to select appropriate sub-array)
v1.10.0,input verification
v1.10.0,group key = everything except the prediction target
v1.10.0,initialize data structure
v1.10.0,group by key
v1.10.0,convert lists to arrays
v1.10.0,instantiate
v1.10.0,return indices corresponding to the `item`-th triple
v1.10.0,input normalization
v1.10.0,prepare filter indices if required
v1.10.0,sorted by target -> most of the batches only have a single target
v1.10.0,group by target
v1.10.0,stack groups into a single tensor
v1.10.0,avoid cyclic imports
v1.10.0,TODO: it would be better to allow separate batch sizes for entity/relation prediction
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"note: most of the time, this loop will only make a single iteration, since the evaluation dataset typically is"
v1.10.0,"not shuffled, and contains evaluation ranking tasks sorted by target"
v1.10.0,"TODO: in theory, we could make a single score calculation for e.g.,"
v1.10.0,"{(h, r, t1), (h, r, t1), ..., (h, r, tk)}"
v1.10.0,predict scores for all candidates
v1.10.0,filter scores
v1.10.0,extract true scores
v1.10.0,replace by nan
v1.10.0,rewrite true scores
v1.10.0,create dense positive masks
v1.10.0,"TODO: afaik, dense positive masks are not used on GPU -> we do not need to move the masks around"
v1.10.0,delegate processing of scores to the evaluator
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,docstr-coverage: inherited
v1.10.0,"note: OGB's evaluator needs a dataset name as input, and uses it to lookup the standard evaluation"
v1.10.0,metric. we do want to support user-selected metrics on arbitrary datasets instead
v1.10.0,"this setting is equivalent to the WikiKG2 setting, and will calculate MRR *and* H@k for k in {1, 3, 10}"
v1.10.0,filter supported metrics
v1.10.0,"prepare input format, cf. `evaluator.expected_input``"
v1.10.0,"y_pred_pos: shape: (num_edge,)"
v1.10.0,"y_pred_neg: shape: (num_edge, num_nodes_neg)"
v1.10.0,iterate over prediction targets
v1.10.0,pre-allocate
v1.10.0,TODO: maybe we want to collect scores on CPU / add an option?
v1.10.0,iterate over batches
v1.10.0,"combine ids, shape: (batch_size, num_negatives + 1)"
v1.10.0,"get scores, shape: (batch_size, num_negatives + 1)"
v1.10.0,store positive and negative scores
v1.10.0,cf. https://github.com/snap-stanford/ogb/pull/357
v1.10.0,combine to input dictionary
v1.10.0,delegate to OGB evaluator
v1.10.0,post-processing
v1.10.0,normalize name
v1.10.0,OGB does not aggregate values across triples
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,flatten dictionaries
v1.10.0,individual side
v1.10.0,combined
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,repeat
v1.10.0,default for inductive LP by [teru2020]
v1.10.0,verify input
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO: do not require to compute all scores beforehand
v1.10.0,cf. Model.score_t(ts=...)
v1.10.0,super.evaluation assumes that the true scores are part of all_scores
v1.10.0,write back correct num_entities
v1.10.0,TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?
v1.10.0,combine key batches
v1.10.0,calculate key frequency
v1.10.0,weight = inverse frequency
v1.10.0,broadcast to samples
v1.10.0,docstr-coverage: inherited
v1.10.0,store keys for calculating macro weights
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,compute macro weights
v1.10.0,note: we wrap the array into a list to be able to re-use _iter_ranks
v1.10.0,calculate weighted metrics
v1.10.0,Clear buffers
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.10.0,to create the class
v1.10.0,TODO: update to hint + kwargs
v1.10.0,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default regularizer class
v1.10.0,: The default parameters for the default regularizer class
v1.10.0,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.10.0,Make sure that all modules with parameters do have a reset_parameters method.
v1.10.0,Recursively visit all sub-modules
v1.10.0,skip self
v1.10.0,Track parents for blaming
v1.10.0,call reset_parameters if possible
v1.10.0,initialize from bottom to top
v1.10.0,This ensures that specialized initializations will take priority over the default ones of its components.
v1.10.0,emit warning if there where parameters which were not initialised by reset_parameters.
v1.10.0,Additional debug information
v1.10.0,docstr-coverage: inherited
v1.10.0,TODO: allow max_id being present in representation_kwargs; if it matches max_id
v1.10.0,TODO: we could infer some shapes from the given interaction shape information
v1.10.0,check max-id
v1.10.0,check shapes
v1.10.0,: The entity representations
v1.10.0,: The relation representations
v1.10.0,: The weight regularizers
v1.10.0,: The interaction function
v1.10.0,"TODO: support ""broadcasting"" representation regularizers?"
v1.10.0,e.g. re-use the same regularizer for everything; or
v1.10.0,"pass a dictionary with keys ""entity""/""relation"";"
v1.10.0,values are either a regularizer hint (=the same regularizer for all repr); or a sequence of appropriate length
v1.10.0,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.10.0,"model.modules(). Thereby, we can collect them automatically."
v1.10.0,Explicitly call reset_parameters to trigger initialization
v1.10.0,"note, triples_factory is required instead of just using self.num_entities"
v1.10.0,and self.num_relations for the inductive case when this is different
v1.10.0,instantiate regularizer
v1.10.0,normalize input
v1.10.0,Note: slicing cannot be used here: the indices for score_hrt only have a batch
v1.10.0,"dimension, and slicing along this dimension is already considered by sub-batching."
v1.10.0,Note: we do not delegate to the general method for performance reasons
v1.10.0,Note: repetition is not necessary here
v1.10.0,batch normalization modules use batch statistics in training mode
v1.10.0,-> different batch divisions lead to different results
v1.10.0,docstr-coverage: inherited
v1.10.0,add broadcast dimension
v1.10.0,unsqueeze if necessary
v1.10.0,docstr-coverage: inherited
v1.10.0,add broadcast dimension
v1.10.0,unsqueeze if necessary
v1.10.0,docstr-coverage: inherited
v1.10.0,add broadcast dimension
v1.10.0,unsqueeze if necessary
v1.10.0,normalization
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,train model
v1.10.0,"note: as this is an example, the model is only trained for a few epochs,"
v1.10.0,"but not until convergence. In practice, you would usually first verify that"
v1.10.0,"the model is sufficiently good in prediction, before looking at uncertainty scores"
v1.10.0,predict triple scores with uncertainty
v1.10.0,"use a larger number of samples, to increase quality of uncertainty estimate"
v1.10.0,get most and least uncertain prediction on training set
v1.10.0,: The scores
v1.10.0,": The uncertainty, in the same shape as scores"
v1.10.0,Enforce evaluation mode
v1.10.0,set dropout layers to training mode
v1.10.0,draw samples
v1.10.0,compute mean and std
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"This empty 1-element tensor doesn't actually do anything,"
v1.10.0,but is necessary since models with no grad params blow
v1.10.0,up the optimizer
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default loss function class
v1.10.0,: The default parameters for the default loss function class
v1.10.0,: The instance of the loss
v1.10.0,: the number of entities
v1.10.0,: the number of relations
v1.10.0,: whether to use inverse relations
v1.10.0,: utility for generating inverse relations
v1.10.0,": When predict_with_sigmoid is set to True, the sigmoid function is"
v1.10.0,: applied to the logits during evaluation and also for predictions
v1.10.0,": after training, but has no effect on the training."
v1.10.0,Random seeds have to set before the embeddings are initialized
v1.10.0,Loss
v1.10.0,TODO: why do we need to empty the cache?
v1.10.0,"TODO: this currently compute (batch_size, num_relations) instead,"
v1.10.0,"i.e., scores for normal and inverse relations"
v1.10.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.10.0,send to device
v1.10.0,special handling of inverse relations
v1.10.0,"when trained on inverse relations, the internal relation ID is twice the original relation ID"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Base Models
v1.10.0,Concrete Models
v1.10.0,Inductive Models
v1.10.0,Evaluation-only models
v1.10.0,Meta Models
v1.10.0,Utils
v1.10.0,Abstract Models
v1.10.0,We might be able to relax this later
v1.10.0,baseline models behave differently
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,always create representations for normal and inverse relations and padding
v1.10.0,"note: we need to share the aggregation across representations, since the aggregation may have"
v1.10.0,trainable parameters
v1.10.0,: a mapping from inductive mode to corresponding entity representations
v1.10.0,": note: there may be duplicate values, if entity representations are shared between validation and testing"
v1.10.0,inductive factories
v1.10.0,"entity representation kwargs may contain a triples factory, which needs to be replaced"
v1.10.0,"entity_representations_kwargs.pop(""triples_factory"", None)"
v1.10.0,note: this is *not* a nn.ModuleDict; the modules have to be registered elsewhere
v1.10.0,shared
v1.10.0,non-shared
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,default composition is DistMult-style
v1.10.0,Saving edge indices for all the supplied splits
v1.10.0,Extract all entity and relation representations
v1.10.0,Perform message passing and get updated states
v1.10.0,Use updated entity and relation states to extract requested IDs
v1.10.0,TODO I got lost in all the Representation Modules and shape casting and wrote this ;(
v1.10.0,normalization
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,": the indexed filter triples, i.e., sparse masks"
v1.10.0,avoid cyclic imports
v1.10.0,create base model
v1.10.0,assign *after* nn.Module.__init__
v1.10.0,save constants
v1.10.0,index triples
v1.10.0,initialize base model's parameters
v1.10.0,"get masks, shape: (batch_size, num_entities/num_relations)"
v1.10.0,combine masks
v1.10.0,"note: * is an elementwise and, and + and elementwise or"
v1.10.0,get non-zero entries
v1.10.0,set scores for fill value for every non-occuring entry
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,NodePiece
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO rethink after RGCN update
v1.10.0,TODO: other parameters?
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default loss function class
v1.10.0,: The default parameters for the default loss function class
v1.10.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.10.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.10.0,ConvE should be trained with inverse triples
v1.10.0,entity embedding
v1.10.0,ConvE uses one bias for each entity
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,head representation
v1.10.0,tail representation
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default loss function class
v1.10.0,: The default parameters for the default loss function class
v1.10.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.10.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.10.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.10.0,use torch's native complex data type
v1.10.0,use torch's native complex data type
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.10.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.10.0,: a normalized weight of 10 is used.
v1.10.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.10.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.10.0,In the code base only the weights of the output layer are used for regularization
v1.10.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,comment:
v1.10.0,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.10.0,uses float64
v1.10.0,entity bias for head
v1.10.0,entity bias for tail
v1.10.0,relation offset
v1.10.0,diagonal relation transformation matrix
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: the default loss function is the self-adversarial negative sampling loss
v1.10.0,: The default parameters for the default loss function class
v1.10.0,: The default entity normalizer parameters
v1.10.0,: The entity representations are normalized to L2 unit length
v1.10.0,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.10.0,"update initializer settings, cf."
v1.10.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.10.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.10.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.10.0,in the original implementation the embeddings are initialized in one parameter
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,"w: (k, d, d)"
v1.10.0,"vh: (k, d)"
v1.10.0,"vt: (k, d)"
v1.10.0,"b: (k,)"
v1.10.0,"u: (k,)"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The regularizer used by [yang2014]_ for DistMult
v1.10.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.10.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.10.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.10.0,: The LP settings used by [yang2014]_ for DistMult
v1.10.0,note: DistMult only regularizes the relation embeddings;
v1.10.0,entity embeddings are hard constrained instead
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default settings for the entity constrainer
v1.10.0,mean
v1.10.0,diagonal covariance
v1.10.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.10.0,mean
v1.10.0,diagonal covariance
v1.10.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,diagonal entries
v1.10.0,off-diagonal
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The custom regularizer used by [wang2014]_ for TransH
v1.10.0,: The settings used by [wang2014]_ for TransH
v1.10.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.10.0,"Therefore, apply_only_once is always set to True."
v1.10.0,: The custom regularizer used by [wang2014]_ for TransH
v1.10.0,: The settings used by [wang2014]_ for TransH
v1.10.0,translation vector in hyperplane
v1.10.0,normal vector of hyperplane
v1.10.0,normalise the normal vectors to unit l2 length
v1.10.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.10.0,which enforces the defined soft constraints.
v1.10.0,"thus, we need to use a weight regularizer instead of having an Embedding regularizer,"
v1.10.0,which only regularizes the weights used in a batch
v1.10.0,note: the following is already the default
v1.10.0,"default_regularizer=self.regularizer_default,"
v1.10.0,"default_regularizer_kwargs=self.regularizer_default_kwargs,"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,TODO: Initialize from TransE
v1.10.0,relation embedding
v1.10.0,relation projection
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,": The default strategy for optimizing the model""s hyper-parameters"
v1.10.0,TODO: Decomposition kwargs
v1.10.0,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.10.0,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.10.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.10.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.10.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,combined representation
v1.10.0,Resolve interaction function
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default loss function class
v1.10.0,: The default parameters for the default loss function class
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,entity bias for head
v1.10.0,relation position head
v1.10.0,relation shape head
v1.10.0,relation size head
v1.10.0,relation position tail
v1.10.0,relation shape tail
v1.10.0,relation size tail
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default loss function class
v1.10.0,: The default parameters for the default loss function class
v1.10.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.10.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.10.0,": regularization term by the number of elements, which is 200."
v1.10.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.10.0,(head) entity
v1.10.0,tail entity
v1.10.0,relations
v1.10.0,inverse relations
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,input normalization
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,Regular relation embeddings
v1.10.0,The relation-specific interaction vector
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,always create representations for normal and inverse relations and padding
v1.10.0,normalize embedding specification
v1.10.0,prepare token representations & kwargs
v1.10.0,"max_id=triples_factory.num_relations,  # will get added by ERModel"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default loss function class
v1.10.0,: The default parameters for the default loss function class
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Normalize relation embeddings
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default loss function class
v1.10.0,: The default parameters for the default loss function class
v1.10.0,: The LP settings used by [zhang2019]_ for QuatE.
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default settings for the entity constrainer
v1.10.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default loss function class
v1.10.0,: The default parameters for the default loss function class
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default parameters for the default loss function class
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default loss function class
v1.10.0,: The default parameters for the default loss function class
v1.10.0,the individual combination for real/complex parts
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the model's hyper-parameters
v1.10.0,: The default parameters for the default loss function class
v1.10.0,no activation
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: the interaction class (for generating the overview table)
v1.10.0,added by ERModel
v1.10.0,"max_id=triples_factory.num_entities,"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,create sparse matrix of absolute counts
v1.10.0,normalize to relative counts
v1.10.0,base case
v1.10.0,"note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could"
v1.10.0,"stay sparse here, with a potential of a huge memory benefit on large datasets!"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,These operations are deterministic and a random seed can be fixed
v1.10.0,just to avoid warnings
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,compute relation similarity matrix
v1.10.0,mapping from relations to head/tail entities
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"if we really need access to the path later, we can expose it as a property"
v1.10.0,via self.writer.log_dir
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The WANDB run
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The name of the run
v1.10.0,": The configuration dictionary, a mapping from name -> value"
v1.10.0,: Should metrics be stored when running ``log_metrics()``?
v1.10.0,": The metrics, a mapping from step -> (name -> value)"
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,: A hint for constructing a :class:`MultiResultTracker`
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Base classes
v1.10.0,Concrete classes
v1.10.0,Utilities
v1.10.0,always add a Python result tracker for storing the configuration
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The file extension for this writer (do not include dot)
v1.10.0,: The file where the results are written to.
v1.10.0,docstr-coverage: inherited
v1.10.0,: The column names
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,store set of triples
v1.10.0,docstr-coverage: inherited
v1.10.0,: some prime numbers for tuple hashing
v1.10.0,: The bit-array for the Bloom filter data structure
v1.10.0,Allocate bit array
v1.10.0,calculate number of hashing rounds
v1.10.0,index triples
v1.10.0,Store some meta-data
v1.10.0,pre-hash
v1.10.0,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,At least make sure to not replace the triples by the original value
v1.10.0,"To make sure we don't replace the {head, relation, tail} by the"
v1.10.0,original value we shift all values greater or equal than the original value by one up
v1.10.0,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.10.0,Set the indices
v1.10.0,docstr-coverage: inherited
v1.10.0,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.10.0,Bind the total number of negatives to sample in this batch
v1.10.0,Equally corrupt all sides
v1.10.0,"Do not detach, as no gradients should flow into the indices."
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.10.0,: A filterer for negative batches
v1.10.0,create unfiltered negative batch by corruption
v1.10.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Utils
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.10.0,create index structure
v1.10.0,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.10.0,: The concatenated sorted sets of head/tail entities
v1.10.0,docstr-coverage: inherited
v1.10.0,"shape: (batch_size, num_neg_per_pos, 3)"
v1.10.0,Uniformly sample from head/tail offsets
v1.10.0,get corresponding entity
v1.10.0,"and position within triple (0: head, 2: tail)"
v1.10.0,write into negative batch
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Preprocessing: Compute corruption probabilities
v1.10.0,"compute tph, i.e. the average number of tail entities per head"
v1.10.0,"compute hpt, i.e. the average number of head entities per tail"
v1.10.0,Set parameter for Bernoulli distribution
v1.10.0,docstr-coverage: inherited
v1.10.0,Decide whether to corrupt head or tail
v1.10.0,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.10.0,flatten mask
v1.10.0,Tails are corrupted if heads are not corrupted
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The random seed used at the beginning of the pipeline
v1.10.0,: The model trained by the pipeline
v1.10.0,: The training triples
v1.10.0,: The training loop used by the pipeline
v1.10.0,: The losses during training
v1.10.0,: The results evaluated by the pipeline
v1.10.0,: How long in seconds did training take?
v1.10.0,: How long in seconds did evaluation take?
v1.10.0,: An early stopper
v1.10.0,: The configuration
v1.10.0,: Any additional metadata as a dictionary
v1.10.0,: The version of PyKEEN used to create these results
v1.10.0,: The git hash of PyKEEN used to create these results
v1.10.0,file names for storing results
v1.10.0,TODO: rename param?
v1.10.0,always save results as json file
v1.10.0,"save other components only if requested (which they are, by default)"
v1.10.0,TODO use pathlib here
v1.10.0,"note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:"
v1.10.0,"when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless"
v1.10.0,note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()
v1.10.0,only one original value => assume this to be the mean
v1.10.0,multiple values => assume they correspond to individual trials
v1.10.0,metrics accumulates rows for a dataframe for comparison against the original reported results (if any)
v1.10.0,"TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)"
v1.10.0,summarize
v1.10.0,skip special parameters
v1.10.0,FIXME this should never happen.
v1.10.0,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.10.0,used random_seed to ensure reproducible results
v1.10.0,We have to set clear optimizer to False since training should be continued
v1.10.0,TODO: checkpoint_dict not further used; later loaded again by TrainingLoop.train
v1.10.0,TODO: allow empty validation / testing
v1.10.0,evaluation restriction to a subset of entities/relations
v1.10.0,2. Model
v1.10.0,3. Loss
v1.10.0,4. Regularizer
v1.10.0,TODO should training be reset?
v1.10.0,TODO should kwargs for loss and regularizer be checked and raised for?
v1.10.0,Log model parameters
v1.10.0,Log loss parameters
v1.10.0,the loss was already logged as part of the model kwargs
v1.10.0,"loss=loss_resolver.normalize_inst(model_instance.loss),"
v1.10.0,Log regularizer parameters
v1.10.0,5. Optimizer
v1.10.0,5.1 Learning Rate Scheduler
v1.10.0,6. Training Loop
v1.10.0,8. Evaluation
v1.10.0,7. Training (ronaldo style)
v1.10.0,Misc
v1.10.0,Stopping
v1.10.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.10.0,Add logging for debugging
v1.10.0,Train like Cristiano Ronaldo
v1.10.0,Misc
v1.10.0,Build up a list of triples if we want to be in the filtered setting
v1.10.0,"If the user gave custom ""additional_filter_triples"""
v1.10.0,Determine whether the validation triples should also be filtered while performing test evaluation
v1.10.0,TODO consider implications of duplicates
v1.10.0,Evaluate
v1.10.0,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.10.0,Add logging about evaluator for debugging
v1.10.0,1. Dataset
v1.10.0,2. Model
v1.10.0,3. Loss
v1.10.0,4. Regularizer
v1.10.0,5. Optimizer
v1.10.0,5.1 Learning Rate Scheduler
v1.10.0,6. Training Loop
v1.10.0,7. Training (ronaldo style)
v1.10.0,8. Evaluation
v1.10.0,9. Tracking
v1.10.0,Misc
v1.10.0,Start tracking
v1.10.0,"If the evaluation still fail using the CPU, the error is raised"
v1.10.0,"When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is"
v1.10.0,restarted with PyKEEN's automatic memory optimization
v1.10.0,"When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation"
v1.10.0,is restarted using the cpu
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,cf. also https://github.com/pykeen/pykeen/issues/1071
v1.10.0,TODO: use a class-resolver?
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Imported from PyTorch
v1.10.0,: A wrapper around the hidden scheduler base class
v1.10.0,: The default strategy for optimizing the lr_schedulers' hyper-parameters
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,TODO what happens if already exists?
v1.10.0,TODO incorporate setting of random seed
v1.10.0,pipeline_kwargs=dict(
v1.10.0,"random_seed=random_non_negative_int(),"
v1.10.0,"),"
v1.10.0,Add dataset to current_pipeline
v1.10.0,"Training, test, and validation paths are provided"
v1.10.0,Add loss function to current_pipeline
v1.10.0,Add regularizer to current_pipeline
v1.10.0,Add optimizer to current_pipeline
v1.10.0,Add training approach to current_pipeline
v1.10.0,Add evaluation
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.10.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.10.0,the columns are not ordered properly.
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,convert class to string to use caching
v1.10.0,Assume it's a file path
v1.10.0,note: we only need to set the create_inverse_triples in the training factory.
v1.10.0,normalize dataset kwargs
v1.10.0,enable passing force option via dataset_kwargs
v1.10.0,hash kwargs
v1.10.0,normalize dataset name
v1.10.0,get canonic path
v1.10.0,try to use cached dataset
v1.10.0,load dataset without cache
v1.10.0,store cache
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The name of the dataset to download
v1.10.0,"note: we do not use the built-in constants here, since those refer to OGB nomenclature"
v1.10.0,(which happens to coincide with ours)
v1.10.0,FIXME these are already identifiers
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,relation typing
v1.10.0,constants
v1.10.0,unique
v1.10.0,compute over all triples
v1.10.0,Determine group key
v1.10.0,Add labels if requested
v1.10.0,TODO: Merge with _common?
v1.10.0,include hash over triples into cache-file name
v1.10.0,include part hash into cache-file name
v1.10.0,re-use cached file if possible
v1.10.0,select triples
v1.10.0,save to file
v1.10.0,Prune by support and confidence
v1.10.0,TODO: Consider merging with other analysis methods
v1.10.0,TODO: Consider merging with other analysis methods
v1.10.0,TODO: Consider merging with other analysis methods
v1.10.0,"num_triples_validation: Optional[int],"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Raise matplotlib level
v1.10.0,expected metrics
v1.10.0,Needs simulation
v1.10.0,See https://zenodo.org/record/6331629
v1.10.0,TODO: maybe merge into analyze / make sub-command
v1.10.0,only save full data
v1.10.0,Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)
v1.10.0,Plot: difference between mean head and tail degree
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,don't call this function by itself. assumes called through the `validation`
v1.10.0,property and the _training factory has already been loaded
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Normalize path
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Base classes
v1.10.0,Utilities
v1.10.0,: A factory wrapping the training triples
v1.10.0,": A factory wrapping the testing triples, that share indices with the training triples"
v1.10.0,": A factory wrapping the validation triples, that share indices with the training triples"
v1.10.0,: the dataset's name
v1.10.0,TODO: Make a constant for the names
v1.10.0,docstr-coverage: inherited
v1.10.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.10.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.10.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.10.0,: The directory in which the cached data is stored
v1.10.0,TODO: use class-resolver normalize?
v1.10.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.0,don't call this function by itself. assumes called through the `validation`
v1.10.0,property and the _training factory has already been loaded
v1.10.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,"relative paths within zip file's always follow Posix path, even on Windows"
v1.10.0,tarfile does not like pathlib
v1.10.0,: URL to the data to download
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Utilities
v1.10.0,Base Classes
v1.10.0,Concrete Classes
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"ZENODO_URL = ""https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip"""
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.10.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Base class
v1.10.0,Mid-level classes
v1.10.0,: A factory wrapping the training triples
v1.10.0,: A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT
v1.10.0,share indices with the transductive training
v1.10.0,": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.10.0,": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.10.0,: All datasets should take care of inverse triple creation
v1.10.0,": The actual instance of the training factory, which is exposed to the user through `transductive_training`"
v1.10.0,": The actual instance of the inductive inference factory,"
v1.10.0,: which is exposed to the user through `inductive_inference`
v1.10.0,": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`"
v1.10.0,": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`"
v1.10.0,: The directory in which the cached data is stored
v1.10.0,generate subfolders 'training' and  'inference'
v1.10.0,TODO: use class-resolver normalize?
v1.10.0,add v1 / v2 / v3 / v4 for inductive splits if available
v1.10.0,important: inductive_inference shares the same RELATIONS with the transductive training graph
v1.10.0,inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph
v1.10.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.0,inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph
v1.10.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,Base class
v1.10.0,Mid-level classes
v1.10.0,Datasets
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,graph pairs
v1.10.0,graph sizes
v1.10.0,graph versions
v1.10.0,: The link to the zip file
v1.10.0,: The hex digest for the zip file
v1.10.0,Input validation.
v1.10.0,ensure zip file is present
v1.10.0,save relative paths beforehand so they are present for loading
v1.10.0,delegate to super class
v1.10.0,docstr-coverage: inherited
v1.10.0,"left side has files ending with 1, right side with 2"
v1.10.0,docstr-coverage: inherited
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,": The mapping from (graph-pair, side) to triple file name"
v1.10.0,: The internal dataset name
v1.10.0,: The hex digest for the zip file
v1.10.0,input validation
v1.10.0,store *before* calling super to have it available when loading the graphs
v1.10.0,ensure zip file is present
v1.10.0,shared directory for multiple datasets.
v1.10.0,docstr-coverage: inherited
v1.10.0,create triples factory
v1.10.0,docstr-coverage: inherited
v1.10.0,load mappings for both sides
v1.10.0,load triple alignments
v1.10.0,extract entity alignments
v1.10.0,"(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2"
v1.10.0,TODO: support ID-only graphs
v1.10.0,load both graphs
v1.10.0,load alignment
v1.10.0,drop duplicates
v1.10.0,combine
v1.10.0,store for repr
v1.10.0,split
v1.10.0,create inverse triples only for training
v1.10.0,docstr-coverage: inherited
v1.10.0,base
v1.10.0,concrete
v1.10.0,Abstract class
v1.10.0,Concrete classes
v1.10.0,Data Structures
v1.10.0,a buffer for the triples
v1.10.0,the offsets
v1.10.0,normalization
v1.10.0,append shifted mapped triples
v1.10.0,update offsets
v1.10.0,merge labels with same ID
v1.10.0,for mypy
v1.10.0,reconstruct label-to-id
v1.10.0,optional
v1.10.0,merge entity mapping
v1.10.0,merge relation mapping
v1.10.0,convert labels to IDs
v1.10.0,"map labels, using -1 as fill-value for invalid labels"
v1.10.0,"we cannot drop them here, since the two columns need to stay aligned"
v1.10.0,filter alignment
v1.10.0,map alignment from old IDs to new IDs
v1.10.0,determine swapping partner
v1.10.0,only keep triples where we have a swapping partner
v1.10.0,replace by swapping partner
v1.10.0,": the merged id-based triples, shape: (n, 3)"
v1.10.0,": the updated alignment, shape: (2, m)"
v1.10.0,: additional keyword-based parameters for adjusting label-to-id mappings
v1.10.0,concatenate triples
v1.10.0,filter alignment and translate to IDs
v1.10.0,process
v1.10.0,TODO: restrict to only using training alignments?
v1.10.0,merge mappings
v1.10.0,docstr-coverage: inherited
v1.10.0,docstr-coverage: inherited
v1.10.0,add swap triples
v1.10.0,"e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)"
v1.10.0,create dense entity remapping for swap
v1.10.0,add swapped triples
v1.10.0,swap head
v1.10.0,swap tail
v1.10.0,: the name of the additional alignment relation
v1.10.0,docstr-coverage: inherited
v1.10.0,add alignment triples with extra relation
v1.10.0,docstr-coverage: inherited
v1.10.0,"determine connected components regarding the same-as relation (i.e., applies transitivity)"
v1.10.0,apply id mapping
v1.10.0,ensure consecutive IDs
v1.10.0,only use training alignments?
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,1. Dataset
v1.10.0,2. Model
v1.10.0,3. Loss
v1.10.0,4. Regularizer
v1.10.0,5. Optimizer
v1.10.0,5.1 Learning Rate Scheduler
v1.10.0,6. Training Loop
v1.10.0,7. Training
v1.10.0,8. Evaluation
v1.10.0,9. Trackers
v1.10.0,Misc.
v1.10.0,log pruning
v1.10.0,"trial was successful, but has to be ended"
v1.10.0,also show info
v1.10.0,2. Model
v1.10.0,3. Loss
v1.10.0,4. Regularizer
v1.10.0,5. Optimizer
v1.10.0,5.1 Learning Rate Scheduler
v1.10.0,"TODO this fixes the issue for negative samplers, but does not generally address it."
v1.10.0,"For example, some of them obscure their arguments with **kwargs, so should we look"
v1.10.0,at the parent class? Sounds like something to put in class resolver by using the
v1.10.0,"inspect module. For now, this solution will rely on the fact that the sampler is a"
v1.10.0,direct descendent of a parent NegativeSampler
v1.10.0,create result tracker to allow to gracefully close failed trials
v1.10.0,1. Dataset
v1.10.0,2. Model
v1.10.0,3. Loss
v1.10.0,4. Regularizer
v1.10.0,5. Optimizer
v1.10.0,5.1 Learning Rate Scheduler
v1.10.0,6. Training Loop
v1.10.0,7. Training
v1.10.0,8. Evaluation
v1.10.0,9. Tracker
v1.10.0,Misc.
v1.10.0,close run in result tracker
v1.10.0,raise the error again (which will be catched in study.optimize)
v1.10.0,: The :mod:`optuna` study object
v1.10.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.10.0,Output study information
v1.10.0,Output all trials
v1.10.0,Output best trial as pipeline configuration file
v1.10.0,1. Dataset
v1.10.0,2. Model
v1.10.0,3. Loss
v1.10.0,4. Regularizer
v1.10.0,5. Optimizer
v1.10.0,5.1 Learning Rate Scheduler
v1.10.0,6. Training Loop
v1.10.0,7. Training
v1.10.0,8. Evaluation
v1.10.0,9. Tracking
v1.10.0,6. Misc
v1.10.0,Optuna Study Settings
v1.10.0,Optuna Optimization Settings
v1.10.0,TODO: use metric.increasing to determine default direction
v1.10.0,0. Metadata/Provenance
v1.10.0,1. Dataset
v1.10.0,2. Model
v1.10.0,3. Loss
v1.10.0,4. Regularizer
v1.10.0,5. Optimizer
v1.10.0,5.1 Learning Rate Scheduler
v1.10.0,6. Training Loop
v1.10.0,7. Training
v1.10.0,8. Evaluation
v1.10.0,9. Tracking
v1.10.0,1. Dataset
v1.10.0,2. Model
v1.10.0,3. Loss
v1.10.0,4. Regularizer
v1.10.0,5. Optimizer
v1.10.0,5.1 Learning Rate Scheduler
v1.10.0,6. Training Loop
v1.10.0,7. Training
v1.10.0,8. Evaluation
v1.10.0,9. Tracker
v1.10.0,Optuna Misc.
v1.10.0,Pipeline Misc.
v1.10.0,Invoke optimization of the objective function.
v1.10.0,TODO: make it even easier to specify categorical strategies just as lists
v1.10.0,"if isinstance(info, (tuple, list, set)):"
v1.10.0,"info = dict(type='categorical', choices=list(info))"
v1.10.0,get log from info - could either be a boolean or string
v1.10.0,"otherwise, dataset refers to a file that should be automatically split"
v1.10.0,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.10.0,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.10.0,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.10.0,if has_dataset(dataset_name):
v1.10.0,"study.set_user_attr('dataset', dataset_name)"
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,noqa: DAR101
v1.10.0,-*- coding: utf-8 -*-
v1.10.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,
v1.9.0,Configuration file for the Sphinx documentation builder.
v1.9.0,
v1.9.0,This file does only contain a selection of the most common options. For a
v1.9.0,full list see the documentation:
v1.9.0,http://www.sphinx-doc.org/en/master/config
v1.9.0,-- Path setup --------------------------------------------------------------
v1.9.0,"If extensions (or modules to document with autodoc) are in another directory,"
v1.9.0,add these directories to sys.path here. If the directory is relative to the
v1.9.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.9.0,
v1.9.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.9.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.9.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.9.0,from unittest.mock import Mock
v1.9.0,sys.modules['numpy'] = Mock()
v1.9.0,sys.modules['numpy.linalg'] = Mock()
v1.9.0,sys.modules['scipy'] = Mock()
v1.9.0,sys.modules['scipy.optimize'] = Mock()
v1.9.0,sys.modules['scipy.interpolate'] = Mock()
v1.9.0,sys.modules['scipy.sparse'] = Mock()
v1.9.0,sys.modules['scipy.ndimage'] = Mock()
v1.9.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.9.0,sys.modules['tensorflow'] = Mock()
v1.9.0,sys.modules['theano'] = Mock()
v1.9.0,sys.modules['theano.tensor'] = Mock()
v1.9.0,sys.modules['torch'] = Mock()
v1.9.0,sys.modules['torch.optim'] = Mock()
v1.9.0,sys.modules['torch.nn'] = Mock()
v1.9.0,sys.modules['torch.nn.init'] = Mock()
v1.9.0,sys.modules['torch.autograd'] = Mock()
v1.9.0,sys.modules['sklearn'] = Mock()
v1.9.0,sys.modules['sklearn.model_selection'] = Mock()
v1.9.0,sys.modules['sklearn.utils'] = Mock()
v1.9.0,-- Project information -----------------------------------------------------
v1.9.0,"The full version, including alpha/beta/rc tags."
v1.9.0,The short X.Y version.
v1.9.0,-- General configuration ---------------------------------------------------
v1.9.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.9.0,
v1.9.0,needs_sphinx = '1.0'
v1.9.0,"If true, the current module name will be prepended to all description"
v1.9.0,unit titles (such as .. function::).
v1.9.0,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.9.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.9.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.9.0,ones.
v1.9.0,show todo's
v1.9.0,generate autosummary pages
v1.9.0,"Add any paths that contain templates here, relative to this directory."
v1.9.0,The suffix(es) of source filenames.
v1.9.0,You can specify multiple suffix as a list of string:
v1.9.0,
v1.9.0,"source_suffix = ['.rst', '.md']"
v1.9.0,The master toctree document.
v1.9.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.9.0,for a list of supported languages.
v1.9.0,
v1.9.0,This is also used if you do content translation via gettext catalogs.
v1.9.0,"Usually you set ""language"" from the command line for these cases."
v1.9.0,"List of patterns, relative to source directory, that match files and"
v1.9.0,directories to ignore when looking for source files.
v1.9.0,This pattern also affects html_static_path and html_extra_path.
v1.9.0,The name of the Pygments (syntax highlighting) style to use.
v1.9.0,-- Options for HTML output -------------------------------------------------
v1.9.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.9.0,a list of builtin themes.
v1.9.0,
v1.9.0,Theme options are theme-specific and customize the look and feel of a theme
v1.9.0,"further.  For a list of options available for each theme, see the"
v1.9.0,documentation.
v1.9.0,
v1.9.0,html_theme_options = {}
v1.9.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.9.0,"relative to this directory. They are copied after the builtin static files,"
v1.9.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.9.0,html_static_path = ['_static']
v1.9.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.9.0,to template names.
v1.9.0,
v1.9.0,The default sidebars (for documents that don't match any pattern) are
v1.9.0,defined by theme itself.  Builtin themes are using these templates by
v1.9.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.9.0,'searchbox.html']``.
v1.9.0,
v1.9.0,html_sidebars = {}
v1.9.0,The name of an image file (relative to this directory) to place at the top
v1.9.0,of the sidebar.
v1.9.0,
v1.9.0,-- Options for HTMLHelp output ---------------------------------------------
v1.9.0,Output file base name for HTML help builder.
v1.9.0,-- Options for LaTeX output ------------------------------------------------
v1.9.0,latex_elements = {
v1.9.0,The paper size ('letterpaper' or 'a4paper').
v1.9.0,
v1.9.0,"'papersize': 'letterpaper',"
v1.9.0,
v1.9.0,"The font size ('10pt', '11pt' or '12pt')."
v1.9.0,
v1.9.0,"'pointsize': '10pt',"
v1.9.0,
v1.9.0,Additional stuff for the LaTeX preamble.
v1.9.0,
v1.9.0,"'preamble': '',"
v1.9.0,
v1.9.0,Latex figure (float) alignment
v1.9.0,
v1.9.0,"'figure_align': 'htbp',"
v1.9.0,}
v1.9.0,Grouping the document tree into LaTeX files. List of tuples
v1.9.0,"(source start file, target name, title,"
v1.9.0,"author, documentclass [howto, manual, or own class])."
v1.9.0,latex_documents = [
v1.9.0,(
v1.9.0,"master_doc,"
v1.9.0,"'pykeen.tex',"
v1.9.0,"'PyKEEN Documentation',"
v1.9.0,"author,"
v1.9.0,"'manual',"
v1.9.0,"),"
v1.9.0,]
v1.9.0,-- Options for manual page output ------------------------------------------
v1.9.0,One entry per manual page. List of tuples
v1.9.0,"(source start file, name, description, authors, manual section)."
v1.9.0,-- Options for Texinfo output ----------------------------------------------
v1.9.0,Grouping the document tree into Texinfo files. List of tuples
v1.9.0,"(source start file, target name, title, author,"
v1.9.0,"dir menu entry, description, category)"
v1.9.0,-- Options for Epub output -------------------------------------------------
v1.9.0,Bibliographic Dublin Core info.
v1.9.0,epub_title = project
v1.9.0,The unique identifier of the text. This can be a ISBN number
v1.9.0,or the project homepage.
v1.9.0,
v1.9.0,epub_identifier = ''
v1.9.0,A unique identification for the text.
v1.9.0,
v1.9.0,epub_uid = ''
v1.9.0,A list of files that should not be packed into the epub file.
v1.9.0,epub_exclude_files = ['search.html']
v1.9.0,-- Extension configuration -------------------------------------------------
v1.9.0,-- Options for intersphinx extension ---------------------------------------
v1.9.0,Example configuration for intersphinx: refer to the Python standard library.
v1.9.0,"'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),"
v1.9.0,See discussion for adding huggingface intersphinx docs at
v1.9.0,https://github.com/huggingface/transformers/issues/14728#issuecomment-1133521776
v1.9.0,autodoc_member_order = 'bysource'
v1.9.0,autodoc_preserve_defaults = True
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,check probability distribution
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Check a model param is optimized
v1.9.0,Check a loss param is optimized
v1.9.0,Check a model param is NOT optimized
v1.9.0,Check a loss param is optimized
v1.9.0,Check a model param is optimized
v1.9.0,Check a loss param is NOT optimized
v1.9.0,Check a model param is NOT optimized
v1.9.0,Check a loss param is NOT optimized
v1.9.0,verify failure
v1.9.0,"Since custom data was passed, we can't store any of this"
v1.9.0,"currently, any custom data doesn't get stored."
v1.9.0,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.9.0,"Since there's no source path information, these shouldn't be"
v1.9.0,"added, even if it might be possible to infer path information"
v1.9.0,from the triples factories
v1.9.0,"Since paths were passed for training, testing, and validation,"
v1.9.0,they should be stored as study-level attributes
v1.9.0,Check a model param is optimized
v1.9.0,Check a loss param is optimized
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,docstr-coverage: inherited
v1.9.0,check if within 0.5 std of observed
v1.9.0,test error is raised
v1.9.0,there is an extra test for this case
v1.9.0,docstr-coverage: inherited
v1.9.0,same size tensors
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Tests that exception will be thrown when more than or less than two tensors are passed
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,create broadcastable shapes
v1.9.0,check correct value range
v1.9.0,check maximum norm constraint
v1.9.0,unchanged values for small norms
v1.9.0,random entity embeddings & projections
v1.9.0,random relation embeddings & projections
v1.9.0,project
v1.9.0,check shape:
v1.9.0,check normalization
v1.9.0,check equivalence of re-formulation
v1.9.0,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.9.0,= r_p (e_p^T e) + e'
v1.9.0,"create random array, estimate the costs of addition, and measure some execution times."
v1.9.0,"then, compute correlation between the estimated cost, and the measured time."
v1.9.0,check for strong correlation between estimated costs and measured execution time
v1.9.0,get optimal sequence
v1.9.0,check caching
v1.9.0,get optimal sequence
v1.9.0,check correct cost
v1.9.0,check optimality
v1.9.0,compare result to sequential addition
v1.9.0,compare result to sequential addition
v1.9.0,ensure each node participates in at least one edge
v1.9.0,check type and shape
v1.9.0,number of colors is monotonically increasing
v1.9.0,ensure each node participates in at least one edge
v1.9.0,normalize
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,equal value; larger is better
v1.9.0,equal value; smaller is better
v1.9.0,larger is better; improvement
v1.9.0,larger is better; improvement; but not significant
v1.9.0,assert that reporting another metric for this epoch raises an error
v1.9.0,: The window size used by the early stopper
v1.9.0,: The (zeroed) index  - 1 at which stopping will occur
v1.9.0,: The minimum improvement
v1.9.0,: The random seed to use for reproducibility
v1.9.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.9.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.9.0,: The batch size to use.
v1.9.0,Fix seed for reproducibility
v1.9.0,Set automatic_memory_optimization to false during testing
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,See https://github.com/pykeen/pykeen/pull/883
v1.9.0,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.9.0,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.9.0,"first triple, and passing create_inverse_triples=True."
v1.9.0,check for warning
v1.9.0,check for filtered triples
v1.9.0,check for correct inverse triples flag
v1.9.0,check correct translation
v1.9.0,check column order
v1.9.0,apply restriction
v1.9.0,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.9.0,check that inverse_triples is correctly carried over
v1.9.0,verify that the label-to-ID mapping has not been changed
v1.9.0,verify that triples have been filtered
v1.9.0,Test different combinations of restrictions
v1.9.0,check compressed triples
v1.9.0,reconstruct triples from compressed form
v1.9.0,check data loader
v1.9.0,set create inverse triple to true
v1.9.0,split factory
v1.9.0,check that in *training* inverse triple are to be created
v1.9.0,check that in all other splits no inverse triples are to be created
v1.9.0,verify that all entities and relations are present in the training factory
v1.9.0,verify that no triple got lost
v1.9.0,verify that the label-to-id mappings match
v1.9.0,Slightly larger number of triples to guarantee split can find coverage of all entities and relations.
v1.9.0,serialize
v1.9.0,de-serialize
v1.9.0,check for equality
v1.9.0,TODO: this could be (Core)TriplesFactory.__equal__
v1.9.0,cf. https://docs.pytest.org/en/7.1.x/example/parametrize.html#parametrizing-conditional-raising
v1.9.0,wrong ndim
v1.9.0,wrong last dim
v1.9.0,wrong dtype: float
v1.9.0,wrong dtype: complex
v1.9.0,correct
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"DummyModel,"
v1.9.0,3x batch norm: bias + scale --> 6
v1.9.0,entity specific bias        --> 1
v1.9.0,==================================
v1.9.0,7
v1.9.0,"two bias terms, one conv-filter"
v1.9.0,check type
v1.9.0,check shape
v1.9.0,check ID ranges
v1.9.0,this is only done in one of the models
v1.9.0,this is only done in one of the models
v1.9.0,Two linear layer biases
v1.9.0,"Two BN layers, bias & scale"
v1.9.0,Test that the weight in the MLP is trainable (i.e. requires grad)
v1.9.0,quaternion have four components
v1.9.0,entity embeddings
v1.9.0,relation embeddings
v1.9.0,Compute Scores
v1.9.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.9.0,relation embeddings
v1.9.0,Compute Scores
v1.9.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.9.0,entity embeddings
v1.9.0,relation embeddings
v1.9.0,Compute Scores
v1.9.0,: 2xBN (bias & scale)
v1.9.0,the combination bias
v1.9.0,FIXME definitely a type mismatch going on here
v1.9.0,check shape
v1.9.0,check content
v1.9.0,create triples factory with inverse relations
v1.9.0,head prediction via inverse tail prediction
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,empty lists are falsy
v1.9.0,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.9.0,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.9.0,pipeline is checked.
v1.9.0,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.9.0,Resume the previous pipeline
v1.9.0,The MockModel gives the highest score to the highest entity id
v1.9.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.9.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.9.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.9.0,test triple at hand
v1.9.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.9.0,test triple at hand
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.9.0,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.9.0,Test looking up inverse triples
v1.9.0,test new label to ID
v1.9.0,type
v1.9.0,old labels
v1.9.0,"new, compact IDs"
v1.9.0,test vectorized lookup
v1.9.0,type
v1.9.0,shape
v1.9.0,value range
v1.9.0,only occurring Ids get mapped to non-negative numbers
v1.9.0,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.9.0,check type
v1.9.0,check shape
v1.9.0,check content
v1.9.0,check type
v1.9.0,check shape
v1.9.0,check 1-hot
v1.9.0,check type
v1.9.0,check shape
v1.9.0,check value range
v1.9.0,check self-similarity = 1
v1.9.0,base relation
v1.9.0,exact duplicate
v1.9.0,99% duplicate
v1.9.0,50% duplicate
v1.9.0,exact inverse
v1.9.0,99% inverse
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The expected number of entities
v1.9.0,: The expected number of relations
v1.9.0,: The expected number of triples
v1.9.0,": The tolerance on expected number of triples, for randomized situations"
v1.9.0,: The dataset to test
v1.9.0,: The instantiated dataset
v1.9.0,: Should the validation be assumed to have been loaded with train/test?
v1.9.0,Not loaded
v1.9.0,Load
v1.9.0,Test caching
v1.9.0,assert (end - start) < 1.0e-02
v1.9.0,Test consistency of training / validation / testing mapping
v1.9.0,": The directory, if there is caching"
v1.9.0,: The batch size
v1.9.0,: The number of negatives per positive for sLCWA training loop.
v1.9.0,: The number of entities LCWA training loop / label smoothing.
v1.9.0,test reduction
v1.9.0,test finite loss value
v1.9.0,Test backward
v1.9.0,negative scores decreased compared to positive ones
v1.9.0,negative scores decreased compared to positive ones
v1.9.0,: The number of entities.
v1.9.0,: The number of negative samples
v1.9.0,: The number of entities.
v1.9.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.9.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.9.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.9.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.9.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.9.0,test multiple different initializations
v1.9.0,calculate by functional
v1.9.0,calculate manually
v1.9.0,simple
v1.9.0,nested
v1.9.0,nested
v1.9.0,prepare a temporary test directory
v1.9.0,check that file was created
v1.9.0,make sure to close file before trying to delete it
v1.9.0,delete intermediate files
v1.9.0,: The batch size
v1.9.0,: The device
v1.9.0,move test instance to device
v1.9.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.9.0,"verify that the regularizer is stored for both, entity and relation representations"
v1.9.0,Forward pass (should update regularizer)
v1.9.0,Call post_parameter_update (should reset regularizer)
v1.9.0,Check if regularization term is reset
v1.9.0,regularization term should be zero
v1.9.0,updated should be set to false
v1.9.0,call method
v1.9.0,generate random tensors
v1.9.0,generate inputs
v1.9.0,call update
v1.9.0,check shape
v1.9.0,check result
v1.9.0,generate single random tensor
v1.9.0,calculate penalty
v1.9.0,check shape
v1.9.0,check value
v1.9.0,update term
v1.9.0,check that the expected term is returned
v1.9.0,check that the regularizer is now reset
v1.9.0,create another instance with apply_only_once enabled
v1.9.0,test initial state
v1.9.0,"after first update, should change the term"
v1.9.0,"after second update, no change should happen"
v1.9.0,FIXME isn't any finite number allowed now?
v1.9.0,: Additional arguments passed to the training loop's constructor method
v1.9.0,: The triples factory instance
v1.9.0,: The batch size for use for forward_* tests
v1.9.0,: The embedding dimensionality
v1.9.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.9.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.9.0,: The batch size for use when testing training procedures
v1.9.0,: The number of epochs to train the model
v1.9.0,: A random number generator from torch
v1.9.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.9.0,initialization
v1.9.0,: Static extras to append to the CLI
v1.9.0,: the model's device
v1.9.0,: the inductive mode
v1.9.0,for reproducible testing
v1.9.0,insert shared parameters
v1.9.0,move model to correct device
v1.9.0,Check that all the parameters actually require a gradient
v1.9.0,Try to initialize an optimizer
v1.9.0,get model parameters
v1.9.0,re-initialize
v1.9.0,check that the operation works in-place
v1.9.0,check that the parameters where modified
v1.9.0,check for finite values by default
v1.9.0,check whether a gradient can be back-propgated
v1.9.0,TODO: look into score_r for inverse relations
v1.9.0,clear buffers for message passing models
v1.9.0,"For the high/low memory test cases of NTN, SE, etc."
v1.9.0,"else, leave to default"
v1.9.0,Make sure that inverse triples are created if create_inverse_triples=True
v1.9.0,triples factory is added by the pipeline
v1.9.0,TODO: Catch HolE MKL error?
v1.9.0,set regularizer term to something that isn't zero
v1.9.0,call post_parameter_update
v1.9.0,assert that the regularization term has been reset
v1.9.0,do one optimization step
v1.9.0,call post_parameter_update
v1.9.0,check model constraints
v1.9.0,Distance-based model
v1.9.0,dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)
v1.9.0,check type
v1.9.0,check shape
v1.9.0,create a new instance with guaranteed dropout
v1.9.0,set to training mode
v1.9.0,check for different output
v1.9.0,use more samples to make sure that enough values can be dropped
v1.9.0,this implicitly tests extra_repr / iter_extra_repr
v1.9.0,select random indices
v1.9.0,forward pass with full graph
v1.9.0,forward pass with restricted graph
v1.9.0,verify the results are similar
v1.9.0,: The number of entities
v1.9.0,: The number of triples
v1.9.0,: the message dim
v1.9.0,TODO: separation message vs. entity dim?
v1.9.0,check shape
v1.9.0,check dtype
v1.9.0,check finite values (e.g. due to division by zero)
v1.9.0,check non-negativity
v1.9.0,: the input dimension
v1.9.0,: the output dimension
v1.9.0,: the number of entities
v1.9.0,: the shape of the tensor to initialize
v1.9.0,: to be initialized / set in subclass
v1.9.0,: the interaction to use for testing a model
v1.9.0,initializers *may* work in-place => clone
v1.9.0,actual number may be different...
v1.9.0,unfavourable split to ensure that cleanup is necessary
v1.9.0,check for unclean split
v1.9.0,check that no triple got lost
v1.9.0,check that triples where only moved from other to reference
v1.9.0,check that all entities occur in reference
v1.9.0,check that no triple got lost
v1.9.0,check that all entities are covered in first part
v1.9.0,the model
v1.9.0,Settings
v1.9.0,Use small model (untrained)
v1.9.0,Get batch
v1.9.0,Compute scores
v1.9.0,Compute mask only if required
v1.9.0,TODO: Re-use filtering code
v1.9.0,"shape: (batch_size, num_triples)"
v1.9.0,"shape: (batch_size, num_entities)"
v1.9.0,Process one batch
v1.9.0,shape
v1.9.0,value range
v1.9.0,no duplicates
v1.9.0,shape
v1.9.0,value range
v1.9.0,no duplicates
v1.9.0,shape
v1.9.0,value range
v1.9.0,"no repetition, except padding idx"
v1.9.0,inferred from triples factory
v1.9.0,: The batch size
v1.9.0,: the maximum number of candidates
v1.9.0,: the number of ranks
v1.9.0,: the number of samples to use for monte-carlo estimation
v1.9.0,: the number of candidates for each individual ranking task
v1.9.0,: the ranks for each individual ranking task
v1.9.0,data type
v1.9.0,value range
v1.9.0,original ranks
v1.9.0,better ranks
v1.9.0,variances are non-negative
v1.9.0,generate random weights such that sum = n
v1.9.0,for sanity checking: give the largest weight to best rank => should improve
v1.9.0,generate two versions
v1.9.0,1. repeat each rank/candidate pair a random number of times
v1.9.0,"2. do not repeat, but assign a corresponding weight"
v1.9.0,check flatness
v1.9.0,"TODO: does this suffice, or do we really need float as datatype?"
v1.9.0,generate random triples factories
v1.9.0,generate random alignment
v1.9.0,add label information if necessary
v1.9.0,prepare alignment data frame
v1.9.0,call
v1.9.0,check
v1.9.0,: The window size used by the early stopper
v1.9.0,: The mock losses the mock evaluator will return
v1.9.0,: The (zeroed) index  - 1 at which stopping will occur
v1.9.0,: The minimum improvement
v1.9.0,: The best results
v1.9.0,Set automatic_memory_optimization to false for tests
v1.9.0,Step early stopper
v1.9.0,check storing of results
v1.9.0,not needed for test
v1.9.0,verify that the input is valid
v1.9.0,combine
v1.9.0,verify shape
v1.9.0,to be initialize in subclass
v1.9.0,TODO: check subset
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO: this could be shared with the model tests
v1.9.0,"FixedModel: dict(embedding_dim=EMBEDDING_DIM),"
v1.9.0,test combinations of models with training loops
v1.9.0,some models require inverse relations
v1.9.0,some model require access to the training triples
v1.9.0,"inductive models require an inductive mode to be set, and an inference factory to be passed"
v1.9.0,fake an inference factory
v1.9.0,automatically choose accelerator
v1.9.0,defaults to TensorBoard; explicitly disabled here
v1.9.0,disable checkpointing
v1.9.0,fast run
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,check for finite values by default
v1.9.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.9.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.9.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,≈ result of softmax
v1.9.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.9.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.9.0,sum over the softmax dim as weights sum up to 1
v1.9.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.9.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.9.0,≈ result of sigmoid
v1.9.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.9.0,expected_loss ≈ 0.34
v1.9.0,abstract classes
v1.9.0,Create dummy dense labels
v1.9.0,Check if labels form a probability distribution
v1.9.0,Apply label smoothing
v1.9.0,Check if smooth labels form probability distribution
v1.9.0,Create dummy sLCWA labels
v1.9.0,Apply label smoothing
v1.9.0,generate random ratios
v1.9.0,check size
v1.9.0,check value range
v1.9.0,check total split
v1.9.0,check consistency with ratios
v1.9.0,the number of decimal digits equivalent to 1 / n_total
v1.9.0,check type
v1.9.0,check values
v1.9.0,compare against expected
v1.9.0,generated_triples = generate_triples()
v1.9.0,check type
v1.9.0,check format
v1.9.0,check coverage
v1.9.0,mock prediction data frame
v1.9.0,set other parameters
v1.9.0,mock prediction data frame
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"naive implementation, O(n2)"
v1.9.0,check correct output type
v1.9.0,check value range subset
v1.9.0,check value range side
v1.9.0,check columns
v1.9.0,check value range and type
v1.9.0,check value range entity IDs
v1.9.0,check value range entity labels
v1.9.0,check correct type
v1.9.0,check relation_id value range
v1.9.0,check pattern value range
v1.9.0,check confidence value range
v1.9.0,check support value range
v1.9.0,check correct type
v1.9.0,check relation_id value range
v1.9.0,check pattern value range
v1.9.0,check correct type
v1.9.0,check relation_id value range
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,clear
v1.9.0,docstr-coverage: inherited
v1.9.0,assumes deterministic entity to id mapping
v1.9.0,from left_tf
v1.9.0,from right_tf with offset
v1.9.0,docstr-coverage: inherited
v1.9.0,assumes deterministic entity to id mapping
v1.9.0,from left_tf
v1.9.0,from right_tf with offset
v1.9.0,extra-relation
v1.9.0,docstr-coverage: inherited
v1.9.0,assumes deterministic entity to id mapping
v1.9.0,docstr-coverage: inherited
v1.9.0,assumes deterministic entity to id mapping
v1.9.0,from left_tf
v1.9.0,from right_tf with offset
v1.9.0,additional
v1.9.0,verify shape
v1.9.0,verify dtype
v1.9.0,verify number of entities/relations
v1.9.0,verify offsets
v1.9.0,"create old, new pairs"
v1.9.0,simulate merging ids
v1.9.0,only a single pair
v1.9.0,apply
v1.9.0,every key is contained
v1.9.0,value range
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Check minimal statistics
v1.9.0,Check either a github link or author/publication information is given
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.9.0,"prepare conv input (N, C, H, W)"
v1.9.0,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.9.0,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.9.0,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.9.0,"f(h, r, t) = h @ r @ t"
v1.9.0,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.9.0,normalize rotations to unit modulus
v1.9.0,check for unit modulus
v1.9.0,entity embeddings
v1.9.0,relation embeddings
v1.9.0,Compute Scores
v1.9.0,entity embeddings
v1.9.0,relation embeddings
v1.9.0,Compute Scores
v1.9.0,Compute Scores
v1.9.0,-\|R_h h - R_t t\|
v1.9.0,-\|h - t\|
v1.9.0,"Since MuRE has offsets, the scores do not need to negative"
v1.9.0,"We do not need this, since we do not check for functional consistency anyway"
v1.9.0,intra-interaction comparison
v1.9.0,dimension needs to be divisible by num_heads
v1.9.0,FIXME
v1.9.0,multiple
v1.9.0,single
v1.9.0,head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,message_dim must be divisible by num_heads
v1.9.0,determine pool using anchor searcher
v1.9.0,determine expected pool using shortest path distances via scipy.sparse.csgraph
v1.9.0,generate random pool
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,complex tensor
v1.9.0,check value range
v1.9.0,check modulus == 1
v1.9.0,quaternion needs dim divisible by 4
v1.9.0,"check value range (actually [-s, +s] with s = 1/sqrt(2*n))"
v1.9.0,value range
v1.9.0,highest degree node has largest value
v1.9.0,Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf
v1.9.0,create triples with a dummy relation type 0
v1.9.0,"0: green: 2, 3, 7, 8"
v1.9.0,"1: red: 1, 4, 6, 9"
v1.9.0,"2: blue: 0, 5"
v1.9.0,the example includes the first power
v1.9.0,requires at least one complex tensor as input
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,inferred from triples factory
v1.9.0,inferred from assignment
v1.9.0,the representation module infers the max_id from the provided labels
v1.9.0,the following entity does not have an image -> will have to use backfill
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,the representation module infers the max_id from the provided labels
v1.9.0,max_id is inferred from assignment
v1.9.0,create random assignment
v1.9.0,update kwargs
v1.9.0,empty bases
v1.9.0,inconsistent base shapes
v1.9.0,invalid base id
v1.9.0,invalid local index
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO this is the only place this function is used.
v1.9.0,Is there an alternative so we can remove it?
v1.9.0,ensure positivity
v1.9.0,compute using pytorch
v1.9.0,prepare distributions
v1.9.0,compute using pykeen
v1.9.0,"e: (batch_size, num_heads, num_tails, d)"
v1.9.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.9.0,divergence = 0 => similarity = -divergence = 0
v1.9.0,"(h - t), r"
v1.9.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.9.0,divergence >= 0 => similarity = -divergence <= 0
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.9.0,filter vs. no filter thing.
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,check for empty batches
v1.9.0,: The window size used by the early stopper
v1.9.0,: The mock losses the mock evaluator will return
v1.9.0,: The (zeroed) index  - 1 at which stopping will occur
v1.9.0,: The minimum improvement
v1.9.0,: The best results
v1.9.0,Set automatic_memory_optimization to false for tests
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Train a model in one shot
v1.9.0,Train a model for the first half
v1.9.0,Continue training of the first part
v1.9.0,check non-empty metrics
v1.9.0,: Should negative samples be filtered?
v1.9.0,expectation = (1 + n) / 2
v1.9.0,variance = (n**2 - 1) / 12
v1.9.0,"x_i ~ N(mu_i, 1)"
v1.9.0,closed-form solution
v1.9.0,sampled confidence interval
v1.9.0,check that closed-form is in confidence interval of sampled
v1.9.0,positive values only
v1.9.0,positive and negative values
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Check for correct class
v1.9.0,check correct num_entities
v1.9.0,Check for correct class
v1.9.0,check value
v1.9.0,filtering
v1.9.0,"true_score: (2, 3, 3)"
v1.9.0,head based filter
v1.9.0,preprocessing for faster lookup
v1.9.0,check that all found positives are positive
v1.9.0,check in-place
v1.9.0,Test head scores
v1.9.0,Assert in-place modification
v1.9.0,Assert correct filtering
v1.9.0,Test tail scores
v1.9.0,Assert in-place modification
v1.9.0,Assert correct filtering
v1.9.0,The MockModel gives the highest score to the highest entity id
v1.9.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.9.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.9.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.9.0,test triple at hand
v1.9.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.9.0,test triple at hand
v1.9.0,check true negatives
v1.9.0,TODO: check no repetitions (if possible)
v1.9.0,return type
v1.9.0,columns
v1.9.0,value range
v1.9.0,relation restriction
v1.9.0,with explicit num_entities
v1.9.0,with inferred num_entities
v1.9.0,test different shapes
v1.9.0,test different shapes
v1.9.0,value range
v1.9.0,value range
v1.9.0,check unique
v1.9.0,"strips off the ""k"" at the end"
v1.9.0,Populate with real results.
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"(-1, 1),"
v1.9.0,"(-1, -1),"
v1.9.0,"(-5, -3),"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Check whether filtering works correctly
v1.9.0,First giving an example where all triples have to be filtered
v1.9.0,The filter should remove all triples
v1.9.0,Create an example where no triples will be filtered
v1.9.0,The filter should not remove any triple
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,same relation
v1.9.0,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.9.0,Test that half of the subjects and half of the objects are corrupted
v1.9.0,check that corrupted entities co-occur with the relation in training data
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The batch size
v1.9.0,: The random seed
v1.9.0,: The triples factory
v1.9.0,: The instances
v1.9.0,: A positive batch
v1.9.0,: Kwargs
v1.9.0,Generate negative sample
v1.9.0,check filter shape if necessary
v1.9.0,check shape
v1.9.0,check bounds: heads
v1.9.0,check bounds: relations
v1.9.0,check bounds: tails
v1.9.0,test that the negative triple is not the original positive triple
v1.9.0,"shape: (batch_size, 1, num_neg)"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Base Classes
v1.9.0,Concrete Classes
v1.9.0,Utils
v1.9.0,: synonyms of this loss
v1.9.0,: The default strategy for optimizing the loss's hyper-parameters
v1.9.0,flatten and stack
v1.9.0,apply label smoothing if necessary.
v1.9.0,TODO: Do label smoothing only once
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Sanity check
v1.9.0,"prepare for broadcasting, shape: (batch_size, 1, 3)"
v1.9.0,negative_scores have already been filtered in the sampler!
v1.9.0,"shape: (nnz,)"
v1.9.0,docstr-coverage: inherited
v1.9.0,Sanity check
v1.9.0,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.9.0,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.9.0,positive entries in the labels tensor.
v1.9.0,"This shows how often one row has to be repeated,"
v1.9.0,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.9.0,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.9.0,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.9.0,This tells us how often each true label should be repeated
v1.9.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.9.0,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.9.0,within the ablation pipeline.
v1.9.0,1. positive & negative margin
v1.9.0,2. negative margin & offset
v1.9.0,3. positive margin & offset
v1.9.0,docstr-coverage: inherited
v1.9.0,Sanity check
v1.9.0,positive term
v1.9.0,implicitly repeat positive scores
v1.9.0,"shape: (nnz,)"
v1.9.0,negative term
v1.9.0,negative_scores have already been filtered in the sampler!
v1.9.0,docstr-coverage: inherited
v1.9.0,Sanity check
v1.9.0,"scale labels from [0, 1] to [-1, 1]"
v1.9.0,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.9.0,within the ablation pipeline.
v1.9.0,docstr-coverage: inherited
v1.9.0,negative_scores have already been filtered in the sampler!
v1.9.0,(dense) softmax requires unfiltered scores / masking
v1.9.0,we need to fill the scores with -inf for all filtered negative examples
v1.9.0,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.9.0,use filled negatives scores
v1.9.0,docstr-coverage: inherited
v1.9.0,we need dense negative scores => unfilter if necessary
v1.9.0,"we may have inf rows, since there will be one additional finite positive score per row"
v1.9.0,"combine scores: shape: (batch_size, num_negatives + 1)"
v1.9.0,use sparse version of cross entropy
v1.9.0,calculate cross entropy loss
v1.9.0,docstr-coverage: inherited
v1.9.0,make sure labels form a proper probability distribution
v1.9.0,calculate cross entropy loss
v1.9.0,docstr-coverage: inherited
v1.9.0,determine positive; do not check with == since the labels are floats
v1.9.0,subtract margin from positive scores
v1.9.0,divide by temperature
v1.9.0,docstr-coverage: inherited
v1.9.0,subtract margin from positive scores
v1.9.0,normalize positive score shape
v1.9.0,divide by temperature
v1.9.0,docstr-coverage: inherited
v1.9.0,determine positive; do not check with == since the labels are floats
v1.9.0,compute negative weights (without gradient tracking)
v1.9.0,clone is necessary since we modify in-place
v1.9.0,Split positive and negative scores
v1.9.0,"we pass *all* scores as negatives, but set the weight of positives to zero"
v1.9.0,this allows keeping a dense shape
v1.9.0,docstr-coverage: inherited
v1.9.0,Sanity check
v1.9.0,"we do not allow full -inf rows, since we compute the softmax over this tensor"
v1.9.0,compute weights (without gradient tracking)
v1.9.0,"fill negative scores with some finite value, e.g., 0 (they will get masked out anyway)"
v1.9.0,note: this is a reduction along the softmax dim; since the weights are already normalized
v1.9.0,"to sum to one, we want a sum reduction here, instead of using the self._reduction"
v1.9.0,docstr-coverage: inherited
v1.9.0,Sanity check
v1.9.0,docstr-coverage: inherited
v1.9.0,Sanity check
v1.9.0,negative loss part
v1.9.0,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.9.0,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.9.0,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.9.0,docstr-coverage: inherited
v1.9.0,TODO: maybe we can make this more efficient?
v1.9.0,docstr-coverage: inherited
v1.9.0,TODO: maybe we can make this more efficient?
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.9.0,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.9.0,": For more information, see https://github.com/cthoyt/pystow"
v1.9.0,: A path representing the PyKEEN data folder
v1.9.0,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.9.0,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.9.0,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.9.0,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.9.0,: A subdirectory for PyKEEN logs
v1.9.0,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.9.0,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.9.0,"TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728"
v1.9.0,"SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.9.0,get datatype specific epsilon
v1.9.0,clamp minimum value
v1.9.0,try to resolve ambiguous device; there has to be at least one cuda device
v1.9.0,lower bound
v1.9.0,upper bound
v1.9.0,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.9.0,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.9.0,Determine optimal order and cost
v1.9.0,translate back to original order
v1.9.0,determine optimal processing order
v1.9.0,heuristic
v1.9.0,TODO: check if einsum is still very slow.
v1.9.0,TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation
v1.9.0,unsqueeze
v1.9.0,The dimensions affected by e'
v1.9.0,Project entities
v1.9.0,r_p (e_p.T e) + e'
v1.9.0,Enforce constraints
v1.9.0,TODO delete when deleting _normalize_dim (below)
v1.9.0,TODO delete when deleting convert_to_canonical_shape (below)
v1.9.0,TODO delete? See note in test_sim.py on its only usage
v1.9.0,upgrade to sequence
v1.9.0,broadcast
v1.9.0,"normalize ids: -> ids.shape: (batch_size, num_ids)"
v1.9.0,"normalize batch -> batch.shape: (batch_size, 1, 3)"
v1.9.0,allocate memory
v1.9.0,copy ids
v1.9.0,reshape
v1.9.0,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.9.0,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.9.0,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.9.0,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.9.0,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.9.0,ensure pathlib
v1.9.0,Enforce that sizes are strictly positive by passing through ELU
v1.9.0,Shape vector is normalized using the above helper function
v1.9.0,Size is learned separately and applied to normalized shape
v1.9.0,Compute potential boundaries by applying the shape in substraction
v1.9.0,and in addition
v1.9.0,Compute box upper bounds using min and max respectively
v1.9.0,compute width plus 1
v1.9.0,compute box midpoints
v1.9.0,"TODO: we already had this before, as `base`"
v1.9.0,inside box?
v1.9.0,yes: |p - c| / (w + 1)
v1.9.0,no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))
v1.9.0,Step 1: Apply the other entity bump
v1.9.0,Step 2: Apply tanh if tanh_map is set to True.
v1.9.0,Compute the distance function output element-wise
v1.9.0,"Finally, compute the norm"
v1.9.0,cf. https://stackoverflow.com/a/1176023
v1.9.0,check validity
v1.9.0,path compression
v1.9.0,get representatives
v1.9.0,already merged
v1.9.0,make x the smaller one
v1.9.0,merge
v1.9.0,extract partitions
v1.9.0,resolve path to make sure it is an absolute path
v1.9.0,ensure directory exists
v1.9.0,message passing: collect colors of neighbors
v1.9.0,"dense colors: shape: (n, c)"
v1.9.0,"adj:          shape: (n, n)"
v1.9.0,"values need to be float, since torch.sparse.mm does not support integer dtypes"
v1.9.0,size: will be correctly inferred
v1.9.0,concat with old colors
v1.9.0,hash
v1.9.0,create random indicator functions of low dimensionality
v1.9.0,collect neighbors' colors
v1.9.0,round to avoid numerical effects
v1.9.0,hash first
v1.9.0,concat with old colors
v1.9.0,re-hash
v1.9.0,"only keep connectivity, but remove multiplicity"
v1.9.0,"note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,"
v1.9.0,this is rather useless
v1.9.0,initial: degree
v1.9.0,"note: we calculate this separately, since we can use a more efficient implementation for the first step"
v1.9.0,hash
v1.9.0,determine small integer type for dense count array
v1.9.0,convergence check
v1.9.0,each node has a unique color
v1.9.0,the number of colors did not improve in the last iteration
v1.9.0,cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Base Class
v1.9.0,Child classes
v1.9.0,Utils
v1.9.0,: The overall regularization weight
v1.9.0,: The current regularization term (a scalar)
v1.9.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.9.0,: Has this regularizer been updated since last being reset?
v1.9.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.9.0,"If there are tracked parameters, update based on them"
v1.9.0,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.9.0,docstr-coverage: inherited
v1.9.0,no need to compute anything
v1.9.0,docstr-coverage: inherited
v1.9.0,always return zero
v1.9.0,: The dimension along which to compute the vector-based regularization terms.
v1.9.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.9.0,: This allows dimensionality-independent weight tuning.
v1.9.0,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.9.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.9.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.9.0,docstr-coverage: inherited
v1.9.0,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.9.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.9.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.9.0,docstr-coverage: inherited
v1.9.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.9.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.9.0,regularizer-specific parameters
v1.9.0,docstr-coverage: inherited
v1.9.0,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.9.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.9.0,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,orthogonality soft constraint: cosine similarity at most epsilon
v1.9.0,The normalization factor to balance individual regularizers' contribution.
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"""Closed-Form Expectation"","
v1.9.0,"""Closed-Form Variance"","
v1.9.0,"""✓"" if metric.closed_expectation else """","
v1.9.0,"""✓"" if metric.closed_variance else """","
v1.9.0,Add HPO command
v1.9.0,Add NodePiece tokenization command
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,General types
v1.9.0,Triples
v1.9.0,Others
v1.9.0,Tensor Functions
v1.9.0,Tensors
v1.9.0,Dataclasses
v1.9.0,prediction targets
v1.9.0,modes
v1.9.0,entity alignment sides
v1.9.0,: A function that mutates the input and returns a new object of the same type as output
v1.9.0,: A function that can be applied to a tensor to initialize it
v1.9.0,: A function that can be applied to a tensor to normalize it
v1.9.0,: A function that can be applied to a tensor to constrain it
v1.9.0,: A hint for a :class:`torch.device`
v1.9.0,: A hint for a :class:`torch.Generator`
v1.9.0,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.9.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.9.0,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.9.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.9.0,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.9.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.9.0,: the inductive prediction and training mode
v1.9.0,: the prediction target
v1.9.0,: the prediction target index
v1.9.0,: the rank types
v1.9.0,"RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8"
v1.9.0,entity alignment
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,infer shape
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,input normalization
v1.9.0,note: the base class does not have any parameters
v1.9.0,Heuristic for default value
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"note: the only parameters are inside the relation representation module, which has its own reset_parameters"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,normalize num blocks
v1.9.0,determine necessary padding
v1.9.0,determine block sizes
v1.9.0,"(R, nb, bsi, bso)"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,apply padding if necessary
v1.9.0,"(n, di) -> (n, nb, bsi)"
v1.9.0,"(n, nb, bsi), (R, nb, bsi, bso) -> (R, n, nb, bso)"
v1.9.0,"(R, n, nb, bso) -> (R * n, do)"
v1.9.0,TODO: can we change the dimension order to make this contiguous?
v1.9.0,"(n, R * n), (R * n, do) -> (n, do)"
v1.9.0,remove padding if necessary
v1.9.0,docstr-coverage: inherited
v1.9.0,apply padding if necessary
v1.9.0,"(R * n, n), (n, di) -> (R * n, di)"
v1.9.0,"(R * n, di) -> (R, n, nb, bsi)"
v1.9.0,"(R, nb, bsi, bso), (R, n, nb, bsi) -> (n, nb, bso)"
v1.9.0,"(n, nb, bso) -> (n, do)"
v1.9.0,remove padding if necessary
v1.9.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501
v1.9.0,there are separate decompositions for forward and backward relations.
v1.9.0,the self-loop weight is not decomposed.
v1.9.0,TODO: we could cache the stacked adjacency matrices
v1.9.0,self-loop
v1.9.0,forward messages
v1.9.0,backward messages
v1.9.0,activation
v1.9.0,input validation
v1.9.0,has to be imported now to avoid cyclic imports
v1.9.0,has to be assigned after call to nn.Module init
v1.9.0,Resolve edge weighting
v1.9.0,dropout
v1.9.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.9.0,no activation on last layer
v1.9.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501
v1.9.0,buffering of enriched representations
v1.9.0,docstr-coverage: inherited
v1.9.0,invalidate enriched embeddings
v1.9.0,docstr-coverage: inherited
v1.9.0,Bind fields
v1.9.0,"shape: (num_entities, embedding_dim)"
v1.9.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.9.0,Get random dropout mask
v1.9.0,Apply to edges
v1.9.0,fixed edges -> pre-compute weights
v1.9.0,Cache enriched representations
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Utils
v1.9.0,: the maximum ID (exclusively)
v1.9.0,: the shape of an individual representation
v1.9.0,: a normalizer for individual representations
v1.9.0,: a regularizer for individual representations
v1.9.0,: dropout
v1.9.0,heuristic
v1.9.0,normalize *before* repeating
v1.9.0,repeat if necessary
v1.9.0,regularize *after* repeating
v1.9.0,"dropout & regularizer will appear automatically, since it is a nn.Module"
v1.9.0,has to be imported here to avoid cyclic import
v1.9.0,docstr-coverage: inherited
v1.9.0,normalize num_embeddings vs. max_id
v1.9.0,normalize embedding_dim vs. shape
v1.9.0,work-around until full complex support (torch==1.10 still does not work)
v1.9.0,TODO: verify that this is our understanding of complex!
v1.9.0,"note: this seems to work, as finfo returns the datatype of the underlying floating"
v1.9.0,"point dtype, rather than the combined complex one"
v1.9.0,"use make for initializer since there's a default, and make_safe"
v1.9.0,for the others to pass through None values
v1.9.0,docstr-coverage: inherited
v1.9.0,initialize weights in-place
v1.9.0,docstr-coverage: inherited
v1.9.0,apply constraints in-place
v1.9.0,fixme: work-around until nn.Embedding supports complex
v1.9.0,docstr-coverage: inherited
v1.9.0,fixme: work-around until nn.Embedding supports complex
v1.9.0,verify that contiguity is preserved
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"get all base representations, shape: (num_bases, *shape)"
v1.9.0,"get base weights, shape: (*batch_dims, num_bases)"
v1.9.0,"weighted linear combination of bases, shape: (*batch_dims, *shape)"
v1.9.0,normalize output dimension
v1.9.0,entity-relation composition
v1.9.0,edge weighting
v1.9.0,message passing weights
v1.9.0,linear relation transformation
v1.9.0,layer-specific self-loop relation representation
v1.9.0,other components
v1.9.0,initialize
v1.9.0,split
v1.9.0,compose
v1.9.0,transform
v1.9.0,normalization
v1.9.0,aggregate by sum
v1.9.0,dropout
v1.9.0,prepare for inverse relations
v1.9.0,update entity representations: mean over self-loops / forward edges / backward edges
v1.9.0,Relation transformation
v1.9.0,has to be imported here to avoid cyclic imports
v1.9.0,kwargs
v1.9.0,Buffered enriched entity and relation representations
v1.9.0,TODO: Check
v1.9.0,TODO: might not be true for all compositions
v1.9.0,hidden dimension normalization
v1.9.0,Create message passing layers
v1.9.0,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.9.0,TODO: This always uses all training triples for message passing
v1.9.0,initialize buffer of enriched representations
v1.9.0,docstr-coverage: inherited
v1.9.0,invalidate enriched embeddings
v1.9.0,docstr-coverage: inherited
v1.9.0,"when changing from evaluation to training mode, the buffered representations have been computed without"
v1.9.0,"gradient tracking. hence, we need to invalidate them."
v1.9.0,note: this occurs in practice when continuing training after evaluation.
v1.9.0,enrich
v1.9.0,docstr-coverage: inherited
v1.9.0,check max_id
v1.9.0,infer shape
v1.9.0,"assign after super, since they should be properly registered as submodules"
v1.9.0,docstr-coverage: inherited
v1.9.0,: the base representations
v1.9.0,: the combination module
v1.9.0,input normalization
v1.9.0,has to be imported here to avoid cyclic import
v1.9.0,create base representations
v1.9.0,verify same ID range
v1.9.0,"note: we could also relax the requiremen, and set max_id = min(max_ids)"
v1.9.0,shape inference
v1.9.0,assign base representations *after* super init
v1.9.0,docstr-coverage: inherited
v1.9.0,set up cache
v1.9.0,get labels & descriptions
v1.9.0,compose labels
v1.9.0,delegate to super class
v1.9.0,": the assignment from global ID to (representation, local id), shape: (max_id, 2)"
v1.9.0,import here to avoid cyclic import
v1.9.0,instantiate base representations if necessary
v1.9.0,there needs to be at least one base
v1.9.0,"while possible, this might be unintended"
v1.9.0,extract shape
v1.9.0,check for invalid base ids
v1.9.0,check for invalid local indices
v1.9.0,assign modules / buffers *after* super init
v1.9.0,docstr-coverage: inherited
v1.9.0,flatten assignment to ease construction of inverse indices
v1.9.0,we group indices by the representation which provides them
v1.9.0,"thus, we need an inverse to restore the correct order"
v1.9.0,get representations
v1.9.0,update inverse indices
v1.9.0,invert flattening
v1.9.0,import here to avoid cyclic import
v1.9.0,comment: not all representations support passing a shape parameter
v1.9.0,create assignment
v1.9.0,base
v1.9.0,other
v1.9.0,import here to avoid cyclic import
v1.9.0,infer shape
v1.9.0,infer max_id
v1.9.0,docstr-coverage: inherited
v1.9.0,"TODO: can be a combined representations, with appropriate tensor-train combination"
v1.9.0,": shape: (max_id, num_cores)"
v1.9.0,": the bases, length: num_cores, with compatible shapes"
v1.9.0,check shape
v1.9.0,check value range
v1.9.0,"do not increase counter i, since the dimension is shared with the following term"
v1.9.0,i += 1
v1.9.0,ids //= m_i
v1.9.0,import here to avoid cyclic import
v1.9.0,normalize ranks
v1.9.0,"determine M_k, N_k"
v1.9.0,TODO: allow to pass them from outside?
v1.9.0,normalize assignment
v1.9.0,determine shapes and einsum equation
v1.9.0,create base representations
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,abstract
v1.9.0,concrete classes
v1.9.0,default flow
v1.9.0,: the message passing layers
v1.9.0,: the flow direction of messages across layers
v1.9.0,": the edge index, shape: (2, num_edges)"
v1.9.0,fail if dependencies are missing
v1.9.0,avoid cyclic import
v1.9.0,"the base representations, e.g., entity embeddings or features"
v1.9.0,verify max_id
v1.9.0,verify shape
v1.9.0,assign sub-module *after* super call
v1.9.0,initialize layers
v1.9.0,normalize activation
v1.9.0,check consistency
v1.9.0,buffer edge index for message passing
v1.9.0,TODO: inductiveness; we need to
v1.9.0,* replace edge_index
v1.9.0,* replace base representations
v1.9.0,* keep layers & activations
v1.9.0,docstr-coverage: inherited
v1.9.0,we can restrict the message passing to the k-hop neighborhood of the desired indices;
v1.9.0,this does only make sense if we do not request *all* indices
v1.9.0,k_hop_subgraph returns:
v1.9.0,(1) the nodes involved in the subgraph
v1.9.0,(2) the filtered edge_index connectivity
v1.9.0,"(3) the mapping from node indices in node_idx to their new location, and"
v1.9.0,(4) the edge mask indicating which edges were preserved
v1.9.0,we only need the base representations for the neighbor indices
v1.9.0,get *all* base representations
v1.9.0,use *all* edges
v1.9.0,perform message passing
v1.9.0,select desired indices
v1.9.0,docstr-coverage: inherited
v1.9.0,": the edge type, shape: (num_edges,)"
v1.9.0,register an additional buffer for the categorical edge type
v1.9.0,docstr-coverage: inherited
v1.9.0,: the relation representations used to obtain initial edge features
v1.9.0,avoid cyclic import
v1.9.0,docstr-coverage: inherited
v1.9.0,get initial relation representations
v1.9.0,select edge attributes from relation representations according to relation type
v1.9.0,perform message passing
v1.9.0,"apply relation transformation, if necessary"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Classes
v1.9.0,Resolver
v1.9.0,backwards compatibility
v1.9.0,scaling factor
v1.9.0,"modulus ~ Uniform[-s, s]"
v1.9.0,"phase ~ Uniform[0, 2*pi]"
v1.9.0,real part
v1.9.0,purely imaginary quaternions unitary
v1.9.0,this is usually loaded from somewhere else
v1.9.0,"the shape must match, as well as the entity-to-id mapping"
v1.9.0,must be cloned if we want to do backprop
v1.9.0,the color initializer
v1.9.0,variants for the edge index
v1.9.0,additional parameters for iter_weisfeiler_lehman
v1.9.0,normalize shape
v1.9.0,get coloring
v1.9.0,make color initializer
v1.9.0,initialize color representations
v1.9.0,note: this could be a representation?
v1.9.0,init entity representations according to the color
v1.9.0,create random walk matrix
v1.9.0,stack diagonal entries of powers of rw
v1.9.0,abstract
v1.9.0,concrete
v1.9.0,docstr-coverage: inherited
v1.9.0,tokenize
v1.9.0,pad
v1.9.0,get character embeddings
v1.9.0,pool
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: whether the edge weighting needs access to the message
v1.9.0,stub init to enable arbitrary arguments in subclasses
v1.9.0,"Calculate in-degree, i.e. number of incoming edges"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,backward compatibility with RGCN
v1.9.0,docstr-coverage: inherited
v1.9.0,view for heads
v1.9.0,"compute attention coefficients, shape: (num_edges, num_heads)"
v1.9.0,"TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"if the sparsity becomes too low, convert to a dense matrix"
v1.9.0,"note: this heuristic is based on the memory consumption,"
v1.9.0,"for a sparse matrix, we store 3 values per nnz (row index, column index, value)"
v1.9.0,"performance-wise, it likely makes sense to switch even earlier"
v1.9.0,`torch.sparse.mm` can also deal with dense 2nd argument
v1.9.0,note: torch.sparse.mm only works for COO matrices;
v1.9.0,@ only works for CSR matrices
v1.9.0,"convert to COO, if necessary"
v1.9.0,"we need to use indices here, since there may be zero diagonal entries"
v1.9.0,: Wikidata SPARQL endpoint. See https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service#Interfacing
v1.9.0,cf. https://meta.wikimedia.org/wiki/User-Agent_policy
v1.9.0,cf. https://wikitech.wikimedia.org/wiki/Robot_policy
v1.9.0,break into smaller requests
v1.9.0,try to load cached first
v1.9.0,determine missing entries
v1.9.0,retrieve information via SPARQL
v1.9.0,save entries
v1.9.0,fill missing descriptions
v1.9.0,for mypy
v1.9.0,we can have multiple images per entity -> collect image URLs per image
v1.9.0,entity ID
v1.9.0,relation ID
v1.9.0,image URL
v1.9.0,check whether images are still missing
v1.9.0,select on image url per image in a reproducible way
v1.9.0,traverse relations in order of preference
v1.9.0,now there is an image available -> select reproducible by URL sorting
v1.9.0,did not break -> no image
v1.9.0,darglint does not like
v1.9.0,"raise cls(shape=shape, reference=reference)"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO test
v1.9.0,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.9.0,: a = \mu^T\Sigma^{-1}\mu
v1.9.0,: b = \log \det \Sigma
v1.9.0,1. Component
v1.9.0,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.9.0,2. Component
v1.9.0,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.9.0,with mu = (mu_1 - mu_0)
v1.9.0,= mu * Sigma_1^-1 mu
v1.9.0,since Sigma_1 is diagonal
v1.9.0,= mu**2 / sigma_1
v1.9.0,3. Component
v1.9.0,4. Component
v1.9.0,ln (det(\Sigma_1) / det(\Sigma_0))
v1.9.0,= ln det Sigma_1 - ln det Sigma_0
v1.9.0,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.9.0,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.9.0,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.9.0,allocate result
v1.9.0,prepare distributions
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO benchmark
v1.9.0,TODO benchmark
v1.9.0,TODO benchmark
v1.9.0,TODO benchmark
v1.9.0,TODO benchmark
v1.9.0,TODO benchmark
v1.9.0,TODO benchmark
v1.9.0,TODO benchmark
v1.9.0,TODO benchmark
v1.9.0,"h = h_re, -h_im"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,REPRESENTATION
v1.9.0,base
v1.9.0,concrete
v1.9.0,INITIALIZER
v1.9.0,INTERACTIONS
v1.9.0,Adapter classes
v1.9.0,Concrete Classes
v1.9.0,combinations
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Base Classes
v1.9.0,Adapter classes
v1.9.0,Concrete Classes
v1.9.0,normalize input
v1.9.0,get number of head/relation/tail representations
v1.9.0,flatten list
v1.9.0,split tensors
v1.9.0,broadcasting
v1.9.0,yield batches
v1.9.0,complex typing
v1.9.0,: The symbolic shapes for entity representations
v1.9.0,": The symbolic shapes for entity representations for tail entities, if different."
v1.9.0,": Otherwise, the entity_shape is used for head & tail entities"
v1.9.0,: The symbolic shapes for relation representations
v1.9.0,if the interaction function's head parameter should only receive a subset of entity representations
v1.9.0,if the interaction function's tail parameter should only receive a subset of entity representations
v1.9.0,"TODO: cannot cover dynamic shapes, e.g., AutoSF"
v1.9.0,"TODO: we could change that to slicing along multiple dimensions, if necessary"
v1.9.0,: The functional interaction form
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Store initial input for error message
v1.9.0,All are None -> try and make closest to square
v1.9.0,Only input channels is None
v1.9.0,Only width is None
v1.9.0,Only height is none
v1.9.0,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.9.0,Width and input channels are None -> set input channels to 1 and calculate width
v1.9.0,vector & scalar offset
v1.9.0,": The head-relation encoder operating on 2D ""images"""
v1.9.0,: The head-relation encoder operating on the 1D flattened version
v1.9.0,: The interaction function
v1.9.0,Automatic calculation of remaining dimensions
v1.9.0,Parameter need to fulfil:
v1.9.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.9.0,normalize kernel height
v1.9.0,encoders
v1.9.0,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.9.0,"2: 1D encoder: FC, DO, BN?, Act"
v1.9.0,store reshaping dimensions
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,The interaction model
v1.9.0,docstr-coverage: inherited
v1.9.0,Use Xavier initialization for weight; bias to zero
v1.9.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.9.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.9.0,docstr-coverage: inherited
v1.9.0,normalize hidden_dim
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Initialize biases with zero
v1.9.0,"In the original formulation,"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Global entity projection
v1.9.0,Global relation projection
v1.9.0,Global combination bias
v1.9.0,Global combination bias
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,default core tensor initialization
v1.9.0,cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12
v1.9.0,normalize initializer
v1.9.0,normalize relation dimension
v1.9.0,Core tensor
v1.9.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.9.0,Dropout
v1.9.0,docstr-coverage: inherited
v1.9.0,instantiate here to make module easily serializable
v1.9.0,"batch norm gets reset automatically, since it defines reset_parameters"
v1.9.0,shapes
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,there are separate biases for entities in head and tail position
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,the base interaction
v1.9.0,forward entity/relation shapes
v1.9.0,The parameters of the affine transformation: bias
v1.9.0,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,head position and bump
v1.9.0,relation box: head
v1.9.0,relation box: tail
v1.9.0,tail position and bump
v1.9.0,docstr-coverage: inherited
v1.9.0,input normalization
v1.9.0,Core tensor
v1.9.0,docstr-coverage: inherited
v1.9.0,initialize core tensor
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"r_head, r_mid, r_tail"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"r_head, r_bias, r_tail"
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,docstr-coverage: excused `wrapped`
v1.9.0,TODO: switch to einsum ?
v1.9.0,"return torch.real(torch.einsum(""...d, ...d, ...d -> ..."", h, r, torch.conj(t)))"
v1.9.0,"repeat if necessary, and concat head and relation"
v1.9.0,"shape: -1, num_input_channels, 2*height, width"
v1.9.0,"shape: -1, num_input_channels, 2*height, width"
v1.9.0,"-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.9.0,"reshape: (-1, dim) -> (*batch_dims, dim)"
v1.9.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.9.0,output_shape: batch_dims
v1.9.0,add bias term
v1.9.0,decompose convolution for faster computation in 1-n case
v1.9.0,"while ConvKB uses a convolution operation to calculate scores, its use of convolution is unusual in the"
v1.9.0,following aspects
v1.9.0,"1. the ""height"" of the ""image"" is the dimension of the embedding vectors"
v1.9.0,"2. the ""width"" of the ""image"" is 3, i.e., the number of vectors; moreover, since the convolution kernel is of"
v1.9.0,"shape (1, 3), there is no sliding across the input, but only a single column position where the kernel is"
v1.9.0,applied
v1.9.0,3. we always have a single input channel
v1.9.0,we utilize these observations for the ConvKB specific convolution filter to simplify and accelerate the code
v1.9.0,"here, conv.weight.shape = (num_filters, 1, 1, 3)"
v1.9.0,"thus, we have for the output of the convolution operation: x[..., f, d] = conv.weight[f, :] * x[..., d] + b[d]"
v1.9.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.9.0,"Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)"
v1.9.0,shortcut for same shape
v1.9.0,split weight into head-/relation-/tail-specific sub-matrices
v1.9.0,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.9.0,"Predict t embedding, shape: (*batch_dims, d)"
v1.9.0,dot product
v1.9.0,"composite: (*batch_dims, d)"
v1.9.0,inner product with relation embedding
v1.9.0,Circular correlation of entity embeddings
v1.9.0,complex conjugate
v1.9.0,Hadamard product in frequency domain
v1.9.0,inverse real FFT
v1.9.0,global projections
v1.9.0,"combination, shape: (*batch_dims, d)"
v1.9.0,dot product with t
v1.9.0,r expresses a rotation in complex plane.
v1.9.0,rotate head by relation (=Hadamard product in complex space)
v1.9.0,rotate tail by inverse of relation
v1.9.0,The inverse rotation is expressed by the complex conjugate of r.
v1.9.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.9.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.9.0,|h * r - t| = |h - conj(r) * t|
v1.9.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.9.0,"That is not mentioned in the paper, so it is made optional here."
v1.9.0,Project entities
v1.9.0,h projection to hyperplane
v1.9.0,r
v1.9.0,-t projection to hyperplane
v1.9.0,project to relation specific subspace
v1.9.0,ensure constraints
v1.9.0,x_1 contraction
v1.9.0,x_2 contraction
v1.9.0,Rotate (=Hamilton product in quaternion space).
v1.9.0,Rotation in quaternion space
v1.9.0,head interaction
v1.9.0,relation interaction (notice that h has been updated)
v1.9.0,combination
v1.9.0,similarity
v1.9.0,head
v1.9.0,relation box: head
v1.9.0,relation box: tail
v1.9.0,tail
v1.9.0,power norm
v1.9.0,the relation-specific head box base shape (normalized to have a volume of 1):
v1.9.0,the relation-specific tail box base shape (normalized to have a volume of 1):
v1.9.0,head
v1.9.0,relation
v1.9.0,tail
v1.9.0,version 2: relation factor offset
v1.9.0,extension: negative (power) norm
v1.9.0,note: normalization should be done from the representations
v1.9.0,cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501
v1.9.0,version 2
v1.9.0,r_head = r_head + u * torch.ones_like(r_head)
v1.9.0,r_tail = r_tail + u * torch.ones_like(r_tail)
v1.9.0,"stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)"
v1.9.0,"remember shape for output, but reshape for transformer"
v1.9.0,"get position embeddings, shape: (seq_len, dim)"
v1.9.0,Now we are position-dependent w.r.t qualifier pairs.
v1.9.0,"seq_length, batch_size, dim"
v1.9.0,Pool output
v1.9.0,"output shape: (batch_size, dim)"
v1.9.0,reshape
v1.9.0,head
v1.9.0,relation
v1.9.0,tail
v1.9.0,extension: negative (power) norm
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Concrete classes
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,input normalization
v1.9.0,instantiate separate combinations
v1.9.0,docstr-coverage: inherited
v1.9.0,split complex; repeat real
v1.9.0,separately combine real and imaginary parts
v1.9.0,combine
v1.9.0,docstr-coverage: inherited
v1.9.0,symbolic output to avoid dtype issue
v1.9.0,we only need to consider real part here
v1.9.0,the gate
v1.9.0,the combination
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Resolver
v1.9.0,Base classes
v1.9.0,Concrete classes
v1.9.0,TODO: allow relative
v1.9.0,isin() preserves the sorted order
v1.9.0,docstr-coverage: inherited
v1.9.0,sort by decreasing degree
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,sort by decreasing page rank
v1.9.0,docstr-coverage: inherited
v1.9.0,input normalization
v1.9.0,determine absolute number of anchors for each strategy
v1.9.0,if pre-instantiated
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: the token ID of the padding token
v1.9.0,: the token representations
v1.9.0,: the assigned tokens for each entity
v1.9.0,needs to be lazily imported to avoid cyclic imports
v1.9.0,fill padding (nn.Embedding cannot deal with negative indices)
v1.9.0,"sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs"
v1.9.0,"contain a subset of training relations) - for that, the padding index is the last index of the Representation"
v1.9.0,resolve token representation
v1.9.0,input validation
v1.9.0,register as buffer
v1.9.0,assign sub-module
v1.9.0,apply tokenizer
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"get token IDs, shape: (*, num_chosen_tokens)"
v1.9.0,"lookup token representations, shape: (*, num_chosen_tokens, *shape)"
v1.9.0,": A list with ratios per representation in their creation order,"
v1.9.0,": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`"
v1.9.0,": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95"
v1.9.0,normalize triples
v1.9.0,inverse triples are created afterwards implicitly
v1.9.0,tokenize
v1.9.0,Create an MLP for string aggregation
v1.9.0,note: the token representations' shape includes the number of tokens as leading dim
v1.9.0,unique hashes per representation
v1.9.0,unique hashes if we concatenate all representations together
v1.9.0,TODO: vectorization?
v1.9.0,remove self-loops
v1.9.0,add inverse edges and remove duplicates
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Resolver
v1.9.0,Base classes
v1.9.0,Concrete classes
v1.9.0,docstr-coverage: inherited
v1.9.0,tokenize: represent entities by bag of relations
v1.9.0,collect candidates
v1.9.0,randomly sample without replacement num_tokens relations for each entity
v1.9.0,TODO: expose num_anchors?
v1.9.0,select anchors
v1.9.0,find closest anchors
v1.9.0,convert to torch
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"To prevent possible segfaults in the METIS C code, METIS expects a graph"
v1.9.0,(1) without self-loops; (2) with inverse edges added; (3) with unique edges only
v1.9.0,https://github.com/KarypisLab/METIS/blob/94c03a6e2d1860128c2d0675cbbb86ad4f261256/libmetis/checkgraph.c#L18
v1.9.0,select independently per partition
v1.9.0,select adjacency part;
v1.9.0,"note: the indices will automatically be in [0, ..., high - low), since they are *local* indices"
v1.9.0,offset
v1.9.0,the -1 comes from the shared padding token
v1.9.0,note: permutation will be later on reverted
v1.9.0,add back 1 for the shared padding token
v1.9.0,TODO: check if perm is used correctly
v1.9.0,verify pool
v1.9.0,docstr-coverage: inherited
v1.9.0,choose first num_tokens
v1.9.0,TODO: vectorization?
v1.9.0,heuristic
v1.9.0,heuristic
v1.9.0,calculate configuration digest
v1.9.0,create anchor selection instance
v1.9.0,select anchors
v1.9.0,anchor search (=anchor assignment?)
v1.9.0,assign anchors
v1.9.0,save
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Resolver
v1.9.0,Base classes
v1.9.0,Concrete classes
v1.9.0,docstr-coverage: inherited
v1.9.0,"contains: anchor_ids, entity_ids, mapping {entity_id -> {""ancs"": anchors, ""dists"": distances}}"
v1.9.0,normalize anchor_ids
v1.9.0,cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541
v1.9.0,TODO: keep distances?
v1.9.0,ensure parent directory exists
v1.9.0,save via torch.save
v1.9.0,docstr-coverage: inherited
v1.9.0,"TODO: since we save a contiguous array of (num_entities, num_anchors),"
v1.9.0,"it would be more efficient to not convert to a mapping, but directly select from the tensor"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Anchor Searchers
v1.9.0,Anchor Selection
v1.9.0,Tokenizers
v1.9.0,Token Loaders
v1.9.0,Representations
v1.9.0,Data containers
v1.9.0,"TODO: use graph library, such as igraph, graph-tool, or networkit"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Resolver
v1.9.0,Base classes
v1.9.0,Concrete classes
v1.9.0,docstr-coverage: inherited
v1.9.0,convert to adjacency matrix
v1.9.0,convert to scipy sparse csr
v1.9.0,"compute distances between anchors and all nodes, shape: (num_anchors, num_entities)"
v1.9.0,TODO: padding for unreachable?
v1.9.0,select anchor IDs with smallest distance
v1.9.0,docstr-coverage: inherited
v1.9.0,infer shape
v1.9.0,create adjacency matrix
v1.9.0,symmetric + self-loops
v1.9.0,"for each entity, determine anchor pool by BFS"
v1.9.0,an array storing whether node i is reachable by anchor j
v1.9.0,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.9.0,the output
v1.9.0,anchor nodes have themselves as a starting found anchor
v1.9.0,TODO: take all (q-1) hop neighbors before selecting from q-hop
v1.9.0,propagate one hop
v1.9.0,convergence check
v1.9.0,copy pool if we have seen enough anchors and have not yet stopped
v1.9.0,stop once we have enough
v1.9.0,TODO: can we replace this loop with something vectorized?
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,symmetric + self-loops
v1.9.0,"for each entity, determine anchor pool by BFS"
v1.9.0,an array storing whether node i is reachable by anchor j
v1.9.0,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.9.0,the output that track the distance to each found anchor
v1.9.0,"dtype is unsigned int 8 bit, so we initialize the maximum distance to 255 (or max default)"
v1.9.0,initial anchors are 0-hop away from themselves
v1.9.0,propagate one hop
v1.9.0,TODO the float() trick for GPU result stability until the torch_sparse issue is resolved
v1.9.0,https://github.com/rusty1s/pytorch_sparse/issues/243
v1.9.0,convergence check
v1.9.0,newly reached is a mask that points to newly discovered anchors at this particular step
v1.9.0,implemented as element-wise XOR (will only give True in 0 XOR 1 or 1 XOR 0)
v1.9.0,"in our case we enrich the set of found anchors, so we can only have values turning 0 to 1, eg 0 XOR 1"
v1.9.0,copy pool if we have seen enough anchors and have not yet stopped
v1.9.0,"update the value in the pool by the current hop value (we start from 0, so +1 be default)"
v1.9.0,stop once we have enough
v1.9.0,sort the pool by nearest to farthest anchors
v1.9.0,values with distance 255 (or max for unsigned int8 type) are padding tokens
v1.9.0,"since the output is sorted, no need for random sampling, we just take top-k nearest"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"select k anchors with largest ppr, shape: (batch_size, k)"
v1.9.0,prepare adjacency matrix only once
v1.9.0,prepare result
v1.9.0,progress bar?
v1.9.0,batch-wise computation of PPR
v1.9.0,"run page-rank calculation, shape: (batch_size, n)"
v1.9.0,"select PPR values for the anchors, shape: (batch_size, num_anchors)"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Base classes
v1.9.0,Concrete classes
v1.9.0,
v1.9.0,
v1.9.0,
v1.9.0,
v1.9.0,
v1.9.0,Misc
v1.9.0,
v1.9.0,rank based metrics do not need binarized scores
v1.9.0,: the supported rank types. Most of the time equal to all rank types
v1.9.0,: whether the metric requires the number of candidates for each ranking task
v1.9.0,normalize confidence level
v1.9.0,sample metric values
v1.9.0,"bootstrap estimator (i.e., compute on sample with replacement)"
v1.9.0,cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function
v1.9.0,: The rank-based metric class that this derived metric extends
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"since scale and offset are constant for a given number of candidates, we have"
v1.9.0,E[scale * M + offset] = scale * E[M] + offset
v1.9.0,docstr-coverage: inherited
v1.9.0,"since scale and offset are constant for a given number of candidates, we have"
v1.9.0,V[scale * M + offset] = scale^2 * V[M]
v1.9.0,: Z-adjusted metrics are formulated to be increasing
v1.9.0,: Z-adjusted metrics can only be applied to realistic ranks
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,should be exactly 0.0
v1.9.0,docstr-coverage: inherited
v1.9.0,should be exactly 1.0
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,: Expectation/maximum reindexed metrics are formulated to be increasing
v1.9.0,: Expectation/maximum reindexed metrics can only be applied to realistic ranks
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,should be exactly 0.0
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2
v1.9.0,use V[x] = E[x^2] - E[x]^2
v1.9.0,group by same weight -> compute H_w(n) for multiple n at once
v1.9.0,we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once
v1.9.0,now select from precomputed cumulative sums and aggregate
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective"
v1.9.0,cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,TODO: should we return the sum of weights?
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.9.0,docstr-coverage: inherited
v1.9.0,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: the lower bound
v1.9.0,: whether the lower bound is inclusive
v1.9.0,: the upper bound
v1.9.0,: whether the upper bound is inclusive
v1.9.0,: The name of the metric
v1.9.0,: a link to further information
v1.9.0,: whether the metric needs binarized scores
v1.9.0,": whether it is increasing, i.e., larger values are better"
v1.9.0,: the value range
v1.9.0,: synonyms for this metric
v1.9.0,: whether the metric supports weights
v1.9.0,: whether there is a closed-form solution of the expectation
v1.9.0,: whether there is a closed-form solution of the variance
v1.9.0,normalize weights
v1.9.0,calculate weighted harmonic mean
v1.9.0,calculate cdf
v1.9.0,determine value at p=0.5
v1.9.0,special case for exactly 0.5
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: A description of the metric
v1.9.0,: The function that runs the metric
v1.9.0,docstr-coverage: inherited
v1.9.0,: Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics
v1.9.0,: This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`
v1.9.0,"TODO there's something wrong with this, so add it later"
v1.9.0,classifier_annotator.higher(
v1.9.0,"rmc.pr_auc_score,"
v1.9.0,"name=""AUC-PR"","
v1.9.0,"description=""Area Under the Precision-Recall Curve"","
v1.9.0,"link=""https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score"","
v1.9.0,)
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,don't worry about functions because they can't be specified by JSON.
v1.9.0,Could make a better mo
v1.9.0,later could extend for other non-JSON valid types
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Score with original triples
v1.9.0,Score with inverse triples
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,noqa:DAR101
v1.9.0,noqa:DAR401
v1.9.0,Create directory in which all experimental artifacts are saved
v1.9.0,noqa:DAR101
v1.9.0,clip for node piece configurations
v1.9.0,"""pykeen experiments reproduce"" expects ""model reference dataset"""
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO: take care that triples aren't removed that are the only ones with any given entity
v1.9.0,distribute the deteriorated triples across the remaining factories
v1.9.0,"'kinships',"
v1.9.0,"'umls',"
v1.9.0,"'codexsmall',"
v1.9.0,"'wn18',"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: Functions for specifying exotic resources with a given prefix
v1.9.0,: Functions for specifying exotic resources based on their file extension
v1.9.0,Input validation
v1.9.0,convert to numpy
v1.9.0,Additional columns
v1.9.0,convert PyTorch tensors to numpy
v1.9.0,convert to dataframe
v1.9.0,Re-order columns
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.9.0,TODO vectorize code
v1.9.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,save literal-to-id mapping
v1.9.0,save numeric literals
v1.9.0,load literal-to-id
v1.9.0,load literals
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Split triples
v1.9.0,Sorting ensures consistent results when the triples are permuted
v1.9.0,Create mapping
v1.9.0,Sorting ensures consistent results when the triples are permuted
v1.9.0,Create mapping
v1.9.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.9.0,Filter all non-existent triples
v1.9.0,Note: Unique changes the order of the triples
v1.9.0,Note: Using unique means implicit balancing of training samples
v1.9.0,normalize input
v1.9.0,TODO: method is_inverse?
v1.9.0,TODO: inverse of inverse?
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.9.0,Id of inverse relation: relation + 1
v1.9.0,: The mapping from labels to IDs.
v1.9.0,: The inverse mapping for label_to_id; initialized automatically
v1.9.0,: A vectorized version of entity_label_to_id; initialized automatically
v1.9.0,: A vectorized version of entity_id_to_label; initialized automatically
v1.9.0,Normalize input
v1.9.0,label
v1.9.0,Filter for entities
v1.9.0,Filter for relations
v1.9.0,No filter
v1.9.0,: the number of unique entities
v1.9.0,": the number of relations (maybe including ""artificial"" inverse relations)"
v1.9.0,: whether to create inverse triples
v1.9.0,": the number of real relations, i.e., without artificial inverses"
v1.9.0,ensure torch.Tensor
v1.9.0,input validation
v1.9.0,"always store as torch.long, i.e., torch's default integer dtype"
v1.9.0,check new label to ID mappings
v1.9.0,Make new triples factories for each group
v1.9.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.9.0,prepare metadata
v1.9.0,Delegate to function
v1.9.0,"restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed"
v1.9.0,docstr-coverage: inherited
v1.9.0,load base
v1.9.0,load numeric triples
v1.9.0,store numeric triples
v1.9.0,store metadata
v1.9.0,note: num_relations will be doubled again when instantiating with create_inverse_triples=True
v1.9.0,Check if the triples are inverted already
v1.9.0,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.9.0,contained if and only if create_inverse_triples is True.
v1.9.0,Generate entity mapping if necessary
v1.9.0,Generate relation mapping if necessary
v1.9.0,Map triples of labels to triples of IDs.
v1.9.0,TODO: Check if lazy evaluation would make sense
v1.9.0,docstr-coverage: inherited
v1.9.0,store entity/relation to ID
v1.9.0,load entity/relation to ID
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,pre-filter to keep only topk
v1.9.0,if top is larger than the number of available options
v1.9.0,generate text
v1.9.0,docstr-coverage: inherited
v1.9.0,vectorized label lookup
v1.9.0,Re-order columns
v1.9.0,docstr-coverage: inherited
v1.9.0,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.9.0,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Split indices
v1.9.0,Split triples
v1.9.0,select one triple per relation
v1.9.0,maintain set of covered entities
v1.9.0,"Select one triple for each head/tail entity, which is not yet covered."
v1.9.0,create mask
v1.9.0,Prepare split index
v1.9.0,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.9.0,base cases
v1.9.0,IDs not in training
v1.9.0,triples with exclusive test IDs
v1.9.0,docstr-coverage: inherited
v1.9.0,While there are still triples that should be moved to the training set
v1.9.0,Pick a random triple to move over to the training triples
v1.9.0,add to training
v1.9.0,remove from testing
v1.9.0,Recalculate the move_id_mask
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Make sure that the first element has all the right stuff in it
v1.9.0,docstr-coverage: inherited
v1.9.0,backwards compatibility
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,constants
v1.9.0,constants
v1.9.0,unary
v1.9.0,binary
v1.9.0,ternary
v1.9.0,column names
v1.9.0,return candidates
v1.9.0,index triples
v1.9.0,incoming relations per entity
v1.9.0,outgoing relations per entity
v1.9.0,indexing triples for fast join r1 & r2
v1.9.0,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.9.0,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.9.0,actual evaluation of the pattern
v1.9.0,skip empty support
v1.9.0,TODO: Can this happen after pre-filtering?
v1.9.0,"sort first, for triple order invariance"
v1.9.0,TODO: what is the support?
v1.9.0,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.9.0,sort decreasingly. i dominates j for all j > i in x-dimension
v1.9.0,"if it is also dominated by any y, it is not part of the skyline"
v1.9.0,"group by (relation id, pattern type)"
v1.9.0,"for each group, yield from skyline"
v1.9.0,determine patterns from triples
v1.9.0,drop zero-confidence
v1.9.0,keep only skyline
v1.9.0,create data frame
v1.9.0,iterate relation types
v1.9.0,drop zero-confidence
v1.9.0,keep only skyline
v1.9.0,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.9.0,base = skyline(base)
v1.9.0,create data frame
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO: the same
v1.9.0,": the positive triples, shape: (batch_size, 3)"
v1.9.0,": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)"
v1.9.0,": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)"
v1.9.0,noqa:DAR202
v1.9.0,noqa:DAR401
v1.9.0,TODO: some negative samplers require batches
v1.9.0,"shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.9.0,"each shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset
v1.9.0,docstr-coverage: inherited
v1.9.0,indexing
v1.9.0,initialize
v1.9.0,sample iteratively
v1.9.0,determine weights
v1.9.0,randomly choose a vertex which has not been chosen yet
v1.9.0,normalize to probabilities
v1.9.0,sample a start node
v1.9.0,get list of neighbors
v1.9.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.9.0,visit target node
v1.9.0,decrease sample counts
v1.9.0,docstr-coverage: inherited
v1.9.0,convert to csr for fast row slicing
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,safe division for empty sets
v1.9.0,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.9.0,duplicates
v1.9.0,we are not interested in self-similarity
v1.9.0,compute similarities
v1.9.0,Calculate which relations are the inverse ones
v1.9.0,get existing IDs
v1.9.0,remove non-existing ID from label mapping
v1.9.0,create translation tensor
v1.9.0,get entities and relations occurring in triples
v1.9.0,generate ID translation and new label to Id mappings
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.9.0,seamless loading and saving of training checkpoints
v1.9.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.9.0,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.9.0,"If a checkpoint file is given, it must be loaded if it exists already"
v1.9.0,"If the stopper dict has any keys, those are written back to the stopper"
v1.9.0,The checkpoint frequency needs to be set to save checkpoints
v1.9.0,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.9.0,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.9.0,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.9.0,send model to device before going into the internal training loop
v1.9.0,Ensure the release of memory
v1.9.0,Clear optimizer
v1.9.0,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.9.0,due to the patience continue to train after the best epoch and thus alter the model
v1.9.0,Create a path
v1.9.0,Prepare all of the callbacks
v1.9.0,"Register a callback for the result tracker, if given"
v1.9.0,"Register a callback for the early stopper, if given"
v1.9.0,TODO should mode be passed here?
v1.9.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.9.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.9.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.9.0,return the relevant parameters slice_size and batch_size
v1.9.0,Force weight initialization if training continuation is not explicitly requested.
v1.9.0,Reset the weights
v1.9.0,"afterwards, some parameters may be on the wrong device"
v1.9.0,Create new optimizer
v1.9.0,Create a new lr scheduler and add the optimizer
v1.9.0,Ensure the model is on the correct device
v1.9.0,"When size probing, we don't want progress bars"
v1.9.0,Create progress bar
v1.9.0,Save the time to track when the saved point was available
v1.9.0,Training Loop
v1.9.0,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.9.0,Enforce training mode
v1.9.0,Accumulate loss over epoch
v1.9.0,Batching
v1.9.0,Only create a progress bar when not in size probing mode
v1.9.0,Flag to check when to quit the size probing
v1.9.0,Recall that torch *accumulates* gradients. Before passing in a
v1.9.0,"new instance, you need to zero out the gradients from the old instance"
v1.9.0,Get batch size of current batch (last batch may be incomplete)
v1.9.0,accumulate gradients for whole batch
v1.9.0,forward pass call
v1.9.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.9.0,update parameters according to optimizer
v1.9.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.9.0,constraints are no longer applied
v1.9.0,For testing purposes we're only interested in processing one batch
v1.9.0,When size probing we don't need the losses
v1.9.0,Update learning rate scheduler
v1.9.0,Track epoch loss
v1.9.0,"note: this epoch loss can be slightly biased towards the last batch, if this is smaller than the rest"
v1.9.0,"in practice, this should have a minor effect, since typically batch_size << num_instances"
v1.9.0,Print loss information to console
v1.9.0,Save the last successful finished epoch
v1.9.0,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.9.0,During automatic memory optimization only the error message is of interest
v1.9.0,When there wasn't a best epoch the checkpoint path should be None
v1.9.0,Delete temporary best epoch model
v1.9.0,Includes a call to result_tracker.log_metrics
v1.9.0,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.9.0,MyPy overrides are because you should
v1.9.0,When there wasn't a best epoch the checkpoint path should be None
v1.9.0,Delete temporary best epoch model
v1.9.0,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.9.0,at that state
v1.9.0,Delete temporary best epoch model
v1.9.0,forward pass
v1.9.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.9.0,correction for loss reduction
v1.9.0,backward pass
v1.9.0,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.9.0,Set upper bound
v1.9.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.9.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.9.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.9.0,"Only if a cuda device is available, the random state is accessed"
v1.9.0,This is an entire checkpoint for the optional best model when using early stopping
v1.9.0,Saving triples factory related states
v1.9.0,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.9.0,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.9.0,Check whether the triples factory mappings match those from the checkpoints
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Shuffle each epoch
v1.9.0,Lazy-splitting into batches
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,docstr-coverage: inherited
v1.9.0,disable automatic batching
v1.9.0,docstr-coverage: inherited
v1.9.0,Slicing is not possible in sLCWA training loops
v1.9.0,split batch
v1.9.0,send to device
v1.9.0,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.9.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.9.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.9.0,Compute negative and positive scores
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Slicing is not possible for sLCWA
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,lazy init
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,TODO how to pass inductive mode
v1.9.0,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.9.0,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.9.0,: A hint for constructing a :class:`MultiTrainingCallback`
v1.9.0,: A collection of callbacks
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,normalize target column
v1.9.0,The type inference is so confusing between the function switching
v1.9.0,and polymorphism introduced by slicability that these need to be ignored
v1.9.0,Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number
v1.9.0,of total entities from another inductive inference factory
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Split batch components
v1.9.0,Send batch to device
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.9.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,To make MyPy happy
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,now: smaller is better
v1.9.0,: the number of reported results with no improvement after which training will be stopped
v1.9.0,the minimum relative improvement necessary to consider it an improved result
v1.9.0,"whether a larger value is better, or a smaller."
v1.9.0,: The epoch at which the best result occurred
v1.9.0,: The best result so far
v1.9.0,: The remaining patience
v1.9.0,check for improvement
v1.9.0,stop if the result did not improve more than delta for patience evaluations
v1.9.0,: The model
v1.9.0,: The evaluator
v1.9.0,: The triples to use for training (to be used during filtered evaluation)
v1.9.0,: The triples to use for evaluation
v1.9.0,: Size of the evaluation batches
v1.9.0,: Slice size of the evaluation batches
v1.9.0,: The number of epochs after which the model is evaluated on validation set
v1.9.0,: The number of iterations (one iteration can correspond to various epochs)
v1.9.0,: with no improvement after which training will be stopped.
v1.9.0,: The name of the metric to use
v1.9.0,: The minimum relative improvement necessary to consider it an improved result
v1.9.0,: The metric results from all evaluations
v1.9.0,": Whether a larger value is better, or a smaller"
v1.9.0,: The result tracker
v1.9.0,: Callbacks when after results are calculated
v1.9.0,: Callbacks when training gets continued
v1.9.0,: Callbacks when training is stopped early
v1.9.0,: Did the stopper ever decide to stop?
v1.9.0,: the path to the weights of the best model
v1.9.0,: whether to delete the file with the best model weights after termination
v1.9.0,: note: the weights will be re-loaded into the model before
v1.9.0,TODO: Fix this
v1.9.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.9.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.9.0,for mypy
v1.9.0,Evaluate
v1.9.0,Only perform time consuming checks for the first call.
v1.9.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.9.0,Append to history
v1.9.0,TODO need a test that this all re-instantiates properly
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Utils
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,dataset
v1.9.0,model
v1.9.0,stored outside of the training loop / optimizer to give access to auto-tuning from Lightning
v1.9.0,optimizer
v1.9.0,"TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA"
v1.9.0,docstr-coverage: inherited
v1.9.0,call post_parameter_update
v1.9.0,docstr-coverage: inherited
v1.9.0,TODO: sub-batching / slicing
v1.9.0,docstr-coverage: inherited
v1.9.0,TODO:
v1.9.0,"shuffle=shuffle,"
v1.9.0,"drop_last=drop_last,"
v1.9.0,"sampler=sampler,"
v1.9.0,"shuffle=shuffle,"
v1.9.0,disable automatic batching in data loader
v1.9.0,docstr-coverage: inherited
v1.9.0,TODO: sub-batching / slicing
v1.9.0,docstr-coverage: inherited
v1.9.0,"note: since this file is executed via __main__, its module name is replaced by __name__"
v1.9.0,"hence, the two classes' fully qualified names start with ""_"" and are considered private"
v1.9.0,cf. https://github.com/cthoyt/class-resolver/issues/39
v1.9.0,automatically choose accelerator
v1.9.0,defaults to TensorBoard; explicitly disabled here
v1.9.0,disable checkpointing
v1.9.0,mixed precision training
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,parsing metrics
v1.9.0,metric pattern = side?.type?.metric.k?
v1.9.0,: The metric key
v1.9.0,": Side of the metric, or ""both"""
v1.9.0,: The rank type
v1.9.0,normalize metric name
v1.9.0,normalize side
v1.9.0,normalize rank type
v1.9.0,normalize keys
v1.9.0,TODO: this can only normalize rank-based metrics!
v1.9.0,TODO: find a better way to handle this
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO: fix this upstream / make metric.score comply to signature
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Transfer to cpu and convert to numpy
v1.9.0,Ensure that each key gets counted only once
v1.9.0,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.9.0,docstr-coverage: inherited
v1.9.0,"Because the order of the values of an dictionary is not guaranteed,"
v1.9.0,we need to retrieve scores and masks using the exact same key order.
v1.9.0,Clear buffers
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The optimistic rank is the rank when assuming all options with an equal score are placed
v1.9.0,: behind the current test triple.
v1.9.0,": shape: (batch_size,)"
v1.9.0,": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank"
v1.9.0,: over all permutations of the elements with the same score as the currently considered option.
v1.9.0,": shape: (batch_size,)"
v1.9.0,: The pessimistic rank is the rank when assuming all options with an equal score are placed
v1.9.0,: in front of current test triple.
v1.9.0,": shape: (batch_size,)"
v1.9.0,: The number of options is the number of items considered in the ranking. It may change for
v1.9.0,: filtered evaluation
v1.9.0,": shape: (batch_size,)"
v1.9.0,The optimistic rank is the rank when assuming all options with an
v1.9.0,"equal score are placed behind the currently considered. Hence, the"
v1.9.0,"rank is the number of options with better scores, plus one, as the"
v1.9.0,rank is one-based.
v1.9.0,The pessimistic rank is the rank when assuming all options with an
v1.9.0,"equal score are placed in front of the currently considered. Hence,"
v1.9.0,the rank is the number of options which have at least the same score
v1.9.0,minus one (as the currently considered option in included in all
v1.9.0,"options). As the rank is one-based, we have to add 1, which nullifies"
v1.9.0,"the ""minus 1"" from before."
v1.9.0,The realistic rank is the average of the optimistic and pessimistic
v1.9.0,"rank, and hence the expected rank over all permutations of the elements"
v1.9.0,with the same score as the currently considered option.
v1.9.0,"We set values which should be ignored to NaN, hence the number of options"
v1.9.0,which should be considered is given by
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"TODO remove this, it makes code much harder to reason about"
v1.9.0,add mode parameter
v1.9.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.9.0,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.9.0,Clear the ranks from the current evaluator
v1.9.0,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.9.0,do not display progress bar while searching
v1.9.0,start by searching for batch_size
v1.9.0,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.9.0,we do not need to repeat time-consuming checks
v1.9.0,infer start value
v1.9.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r), (h, t) or (r, t)) scored on all"
v1.9.0,"entities/relations, must have failed to start slice_size search, we start with trying half the"
v1.9.0,entities/relations.
v1.9.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.9.0,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.9.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.9.0,values_dict[key] will always be an int at this point
v1.9.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.9.0,"if inverse triples are used, we only do score_t (TODO: by default; can this be changed?)"
v1.9.0,"otherwise, i.e., without inverse triples, we also need score_h"
v1.9.0,"if relations are to be predicted, we need to slice score_r"
v1.9.0,"raise an error, if any of the required methods cannot slice"
v1.9.0,Split batch
v1.9.0,Bind shape
v1.9.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.9.0,Warn if all entities will be filtered
v1.9.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.9.0,TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place
v1.9.0,verify that the triples have been filtered
v1.9.0,Filter triples if necessary
v1.9.0,Send to device
v1.9.0,Ensure evaluation mode
v1.9.0,Prepare for result filtering
v1.9.0,Send tensors to device
v1.9.0,Prepare batches
v1.9.0,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.9.0,Show progressbar
v1.9.0,Flag to check when to quit the size probing
v1.9.0,Disable gradient tracking
v1.9.0,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.9.0,batch-wise processing
v1.9.0,If we only probe sizes we do not need more than one batch
v1.9.0,Finalize
v1.9.0,Create filter
v1.9.0,Select scores of true
v1.9.0,overwrite filtered scores
v1.9.0,The scores for the true triples have to be rewritten to the scores tensor
v1.9.0,the rank-based evaluators needs the true scores with trailing 1-dim
v1.9.0,Create a positive mask with the size of the scores from the positive filter
v1.9.0,Restrict to entities of interest
v1.9.0,process scores
v1.9.0,optionally restrict triples (nop if no restriction)
v1.9.0,evaluation triples as dataframe
v1.9.0,determine filter triples
v1.9.0,infer num_entities if not given
v1.9.0,"TODO: unique, or max ID + 1?"
v1.9.0,optionally restrict triples
v1.9.0,compute candidate set sizes for different targets
v1.9.0,TODO: extend to relations?
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: the MemoryUtilizationMaximizer instance for :func:`_evaluate`.
v1.9.0,batch
v1.9.0,tqdm
v1.9.0,data loader
v1.9.0,set upper limit of batch size for automatic memory optimization
v1.9.0,set model to evaluation mode
v1.9.0,delegate to AMO wrapper
v1.9.0,"The key-id for each triple, shape: (num_triples,)"
v1.9.0,": the number of targets for each key, shape: (num_unique_keys + 1,)"
v1.9.0,: the concatenation of unique targets for each key (use bounds to select appropriate sub-array)
v1.9.0,input verification
v1.9.0,group key = everything except the prediction target
v1.9.0,initialize data structure
v1.9.0,group by key
v1.9.0,convert lists to arrays
v1.9.0,instantiate
v1.9.0,return indices corresponding to the `item`-th triple
v1.9.0,input normalization
v1.9.0,prepare filter indices if required
v1.9.0,sorted by target -> most of the batches only have a single target
v1.9.0,group by target
v1.9.0,stack groups into a single tensor
v1.9.0,avoid cyclic imports
v1.9.0,TODO: it would be better to allow separate batch sizes for entity/relation prediction
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"note: most of the time, this loop will only make a single iteration, since the evaluation dataset typically is"
v1.9.0,"not shuffled, and contains evaluation ranking tasks sorted by target"
v1.9.0,"TODO: in theory, we could make a single score calculation for e.g.,"
v1.9.0,"{(h, r, t1), (h, r, t1), ..., (h, r, tk)}"
v1.9.0,predict scores for all candidates
v1.9.0,filter scores
v1.9.0,extract true scores
v1.9.0,replace by nan
v1.9.0,rewrite true scores
v1.9.0,create dense positive masks
v1.9.0,"TODO: afaik, dense positive masks are not used on GPU -> we do not need to move the masks around"
v1.9.0,delegate processing of scores to the evaluator
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,terminate early if there are no ranks
v1.9.0,flatten dictionaries
v1.9.0,individual side
v1.9.0,combined
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Clear buffers
v1.9.0,repeat
v1.9.0,default for inductive LP by [teru2020]
v1.9.0,verify input
v1.9.0,docstr-coverage: inherited
v1.9.0,TODO: do not require to compute all scores beforehand
v1.9.0,cf. Model.score_t(ts=...)
v1.9.0,super.evaluation assumes that the true scores are part of all_scores
v1.9.0,write back correct num_entities
v1.9.0,TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?
v1.9.0,compute macro weights
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,Clear buffers
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.9.0,to create the class
v1.9.0,TODO: update to hint + kwargs
v1.9.0,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default regularizer class
v1.9.0,: The default parameters for the default regularizer class
v1.9.0,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.9.0,Make sure that all modules with parameters do have a reset_parameters method.
v1.9.0,Recursively visit all sub-modules
v1.9.0,skip self
v1.9.0,Track parents for blaming
v1.9.0,call reset_parameters if possible
v1.9.0,initialize from bottom to top
v1.9.0,This ensures that specialized initializations will take priority over the default ones of its components.
v1.9.0,emit warning if there where parameters which were not initialised by reset_parameters.
v1.9.0,Additional debug information
v1.9.0,docstr-coverage: inherited
v1.9.0,TODO: allow max_id being present in representation_kwargs; if it matches max_id
v1.9.0,TODO: we could infer some shapes from the given interaction shape information
v1.9.0,check max-id
v1.9.0,check shapes
v1.9.0,: The entity representations
v1.9.0,: The relation representations
v1.9.0,: The weight regularizers
v1.9.0,: The interaction function
v1.9.0,"TODO: support ""broadcasting"" representation regularizers?"
v1.9.0,e.g. re-use the same regularizer for everything; or
v1.9.0,"pass a dictionary with keys ""entity""/""relation"";"
v1.9.0,values are either a regularizer hint (=the same regularizer for all repr); or a sequence of appropriate length
v1.9.0,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.9.0,"model.modules(). Thereby, we can collect them automatically."
v1.9.0,Explicitly call reset_parameters to trigger initialization
v1.9.0,instantiate regularizer
v1.9.0,normalize input
v1.9.0,Note: slicing cannot be used here: the indices for score_hrt only have a batch
v1.9.0,"dimension, and slicing along this dimension is already considered by sub-batching."
v1.9.0,Note: we do not delegate to the general method for performance reasons
v1.9.0,Note: repetition is not necessary here
v1.9.0,docstr-coverage: inherited
v1.9.0,add broadcast dimension
v1.9.0,unsqueeze if necessary
v1.9.0,docstr-coverage: inherited
v1.9.0,add broadcast dimension
v1.9.0,unsqueeze if necessary
v1.9.0,docstr-coverage: inherited
v1.9.0,add broadcast dimension
v1.9.0,unsqueeze if necessary
v1.9.0,normalization
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,train model
v1.9.0,"note: as this is an example, the model is only trained for a few epochs,"
v1.9.0,"but not until convergence. In practice, you would usually first verify that"
v1.9.0,"the model is sufficiently good in prediction, before looking at uncertainty scores"
v1.9.0,predict triple scores with uncertainty
v1.9.0,"use a larger number of samples, to increase quality of uncertainty estimate"
v1.9.0,get most and least uncertain prediction on training set
v1.9.0,: The scores
v1.9.0,": The uncertainty, in the same shape as scores"
v1.9.0,Enforce evaluation mode
v1.9.0,set dropout layers to training mode
v1.9.0,draw samples
v1.9.0,compute mean and std
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,deprecated
v1.9.0,exactly one of them is None
v1.9.0,create input batch
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,exactly one of them is None
v1.9.0,
v1.9.0,get input & target
v1.9.0,get label-to-id mapping and prediction targets
v1.9.0,get scores
v1.9.0,create raw dataframe
v1.9.0,postprocess prediction df
v1.9.0,Train a model (quickly)
v1.9.0,Get scores for *all* triples
v1.9.0,Get scores for top 15 triples
v1.9.0,initialize buffer on device
v1.9.0,docstr-coverage: inherited
v1.9.0,"reshape, shape: (batch_size * num_entities,)"
v1.9.0,get top scores within batch
v1.9.0,append to global top scores
v1.9.0,reduce size if necessary
v1.9.0,initialize buffer on cpu
v1.9.0,Explicitly create triples
v1.9.0,docstr-coverage: inherited
v1.9.0,"TODO: in the future, we may want to expose this method"
v1.9.0,set model to evaluation mode
v1.9.0,calculate batch scores
v1.9.0,base case: infer maximum batch size
v1.9.0,base case: single batch
v1.9.0,TODO: this could happen because of AMO
v1.9.0,TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043
v1.9.0,no OOM error.
v1.9.0,make sure triples are a numpy array
v1.9.0,make sure triples are 2d
v1.9.0,convert to ID-based
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"This empty 1-element tensor doesn't actually do anything,"
v1.9.0,but is necessary since models with no grad params blow
v1.9.0,up the optimizer
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default loss function class
v1.9.0,: The default parameters for the default loss function class
v1.9.0,: The instance of the loss
v1.9.0,Random seeds have to set before the embeddings are initialized
v1.9.0,Loss
v1.9.0,TODO: why do we need to empty the cache?
v1.9.0,"TODO: this currently compute (batch_size, num_relations) instead,"
v1.9.0,"i.e., scores for normal and inverse relations"
v1.9.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.9.0,send to device
v1.9.0,special handling of inverse relations
v1.9.0,"when trained on inverse relations, the internal relation ID is twice the original relation ID"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Base Models
v1.9.0,Concrete Models
v1.9.0,Inductive Models
v1.9.0,Evaluation-only models
v1.9.0,Meta Models
v1.9.0,Utils
v1.9.0,Abstract Models
v1.9.0,We might be able to relax this later
v1.9.0,baseline models behave differently
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,always create representations for normal and inverse relations and padding
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,default composition is DistMult-style
v1.9.0,Saving edge indices for all the supplied splits
v1.9.0,Extract all entity and relation representations
v1.9.0,Perform message passing and get updated states
v1.9.0,Use updated entity and relation states to extract requested IDs
v1.9.0,TODO I got lost in all the Representation Modules and shape casting and wrote this ;(
v1.9.0,normalization
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,": the indexed filter triples, i.e., sparse masks"
v1.9.0,avoid cyclic imports
v1.9.0,create base model
v1.9.0,assign *after* nn.Module.__init__
v1.9.0,save constants
v1.9.0,index triples
v1.9.0,initialize base model's parameters
v1.9.0,"get masks, shape: (batch_size, num_entities/num_relations)"
v1.9.0,combine masks
v1.9.0,"note: * is an elementwise and, and + and elementwise or"
v1.9.0,get non-zero entries
v1.9.0,set scores for fill value for every non-occuring entry
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,NodePiece
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO rethink after RGCN update
v1.9.0,TODO: other parameters?
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default loss function class
v1.9.0,: The default parameters for the default loss function class
v1.9.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.9.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.9.0,ConvE should be trained with inverse triples
v1.9.0,entity embedding
v1.9.0,ConvE uses one bias for each entity
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,head representation
v1.9.0,tail representation
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default loss function class
v1.9.0,: The default parameters for the default loss function class
v1.9.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.9.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.9.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.9.0,use torch's native complex data type
v1.9.0,use torch's native complex data type
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.9.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.9.0,: a normalized weight of 10 is used.
v1.9.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.9.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.9.0,In the code base only the weights of the output layer are used for regularization
v1.9.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,comment:
v1.9.0,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.9.0,uses float64
v1.9.0,entity bias for head
v1.9.0,entity bias for tail
v1.9.0,relation offset
v1.9.0,diagonal relation transformation matrix
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: the default loss function is the self-adversarial negative sampling loss
v1.9.0,: The default parameters for the default loss function class
v1.9.0,: The default entity normalizer parameters
v1.9.0,: The entity representations are normalized to L2 unit length
v1.9.0,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.9.0,"update initializer settings, cf."
v1.9.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.9.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.9.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.9.0,in the original implementation the embeddings are initialized in one parameter
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,"w: (k, d, d)"
v1.9.0,"vh: (k, d)"
v1.9.0,"vt: (k, d)"
v1.9.0,"b: (k,)"
v1.9.0,"u: (k,)"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The regularizer used by [yang2014]_ for DistMult
v1.9.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.9.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.9.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.9.0,: The LP settings used by [yang2014]_ for DistMult
v1.9.0,note: DistMult only regularizes the relation embeddings;
v1.9.0,entity embeddings are hard constrained instead
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default settings for the entity constrainer
v1.9.0,mean
v1.9.0,diagonal covariance
v1.9.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.9.0,mean
v1.9.0,diagonal covariance
v1.9.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,diagonal entries
v1.9.0,off-diagonal
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The custom regularizer used by [wang2014]_ for TransH
v1.9.0,: The settings used by [wang2014]_ for TransH
v1.9.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.9.0,"Therefore, apply_only_once is always set to True."
v1.9.0,: The custom regularizer used by [wang2014]_ for TransH
v1.9.0,: The settings used by [wang2014]_ for TransH
v1.9.0,translation vector in hyperplane
v1.9.0,normal vector of hyperplane
v1.9.0,normalise the normal vectors to unit l2 length
v1.9.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.9.0,which enforces the defined soft constraints.
v1.9.0,"thus, we need to use a weight regularizer instead of having an Embedding regularizer,"
v1.9.0,which only regularizes the weights used in a batch
v1.9.0,note: the following is already the default
v1.9.0,"default_regularizer=self.regularizer_default,"
v1.9.0,"default_regularizer_kwargs=self.regularizer_default_kwargs,"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,TODO: Initialize from TransE
v1.9.0,relation embedding
v1.9.0,relation projection
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,": The default strategy for optimizing the model""s hyper-parameters"
v1.9.0,TODO: Decomposition kwargs
v1.9.0,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.9.0,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.9.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.9.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.9.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,combined representation
v1.9.0,Resolve interaction function
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default loss function class
v1.9.0,: The default parameters for the default loss function class
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,entity bias for head
v1.9.0,relation position head
v1.9.0,relation shape head
v1.9.0,relation size head
v1.9.0,relation position tail
v1.9.0,relation shape tail
v1.9.0,relation size tail
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default loss function class
v1.9.0,: The default parameters for the default loss function class
v1.9.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.9.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.9.0,": regularization term by the number of elements, which is 200."
v1.9.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.9.0,(head) entity
v1.9.0,tail entity
v1.9.0,relations
v1.9.0,inverse relations
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,input normalization
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,Regular relation embeddings
v1.9.0,The relation-specific interaction vector
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,always create representations for normal and inverse relations and padding
v1.9.0,normalize embedding specification
v1.9.0,prepare token representations & kwargs
v1.9.0,"max_id=triples_factory.num_relations,  # will get added by ERModel"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default loss function class
v1.9.0,: The default parameters for the default loss function class
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Normalize relation embeddings
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default loss function class
v1.9.0,: The default parameters for the default loss function class
v1.9.0,: The LP settings used by [zhang2019]_ for QuatE.
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default settings for the entity constrainer
v1.9.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default loss function class
v1.9.0,: The default parameters for the default loss function class
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default parameters for the default loss function class
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default loss function class
v1.9.0,: The default parameters for the default loss function class
v1.9.0,the individual combination for real/complex parts
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the model's hyper-parameters
v1.9.0,: The default parameters for the default loss function class
v1.9.0,no activation
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: the interaction class (for generating the overview table)
v1.9.0,added by ERModel
v1.9.0,"max_id=triples_factory.num_entities,"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,create sparse matrix of absolute counts
v1.9.0,normalize to relative counts
v1.9.0,base case
v1.9.0,"note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could"
v1.9.0,"stay sparse here, with a potential of a huge memory benefit on large datasets!"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,These operations are deterministic and a random seed can be fixed
v1.9.0,just to avoid warnings
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,compute relation similarity matrix
v1.9.0,mapping from relations to head/tail entities
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"if we really need access to the path later, we can expose it as a property"
v1.9.0,via self.writer.log_dir
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The WANDB run
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The name of the run
v1.9.0,": The configuration dictionary, a mapping from name -> value"
v1.9.0,: Should metrics be stored when running ``log_metrics()``?
v1.9.0,": The metrics, a mapping from step -> (name -> value)"
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,: A hint for constructing a :class:`MultiResultTracker`
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Base classes
v1.9.0,Concrete classes
v1.9.0,Utilities
v1.9.0,always add a Python result tracker for storing the configuration
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The file extension for this writer (do not include dot)
v1.9.0,: The file where the results are written to.
v1.9.0,docstr-coverage: inherited
v1.9.0,: The column names
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,store set of triples
v1.9.0,docstr-coverage: inherited
v1.9.0,: some prime numbers for tuple hashing
v1.9.0,: The bit-array for the Bloom filter data structure
v1.9.0,Allocate bit array
v1.9.0,calculate number of hashing rounds
v1.9.0,index triples
v1.9.0,Store some meta-data
v1.9.0,pre-hash
v1.9.0,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,At least make sure to not replace the triples by the original value
v1.9.0,"To make sure we don't replace the {head, relation, tail} by the"
v1.9.0,original value we shift all values greater or equal than the original value by one up
v1.9.0,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.9.0,Set the indices
v1.9.0,docstr-coverage: inherited
v1.9.0,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.9.0,Bind the total number of negatives to sample in this batch
v1.9.0,Equally corrupt all sides
v1.9.0,"Do not detach, as no gradients should flow into the indices."
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.9.0,: A filterer for negative batches
v1.9.0,create unfiltered negative batch by corruption
v1.9.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Utils
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.9.0,create index structure
v1.9.0,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.9.0,: The concatenated sorted sets of head/tail entities
v1.9.0,docstr-coverage: inherited
v1.9.0,"shape: (batch_size, num_neg_per_pos, 3)"
v1.9.0,Uniformly sample from head/tail offsets
v1.9.0,get corresponding entity
v1.9.0,"and position within triple (0: head, 2: tail)"
v1.9.0,write into negative batch
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Preprocessing: Compute corruption probabilities
v1.9.0,"compute tph, i.e. the average number of tail entities per head"
v1.9.0,"compute hpt, i.e. the average number of head entities per tail"
v1.9.0,Set parameter for Bernoulli distribution
v1.9.0,docstr-coverage: inherited
v1.9.0,Decide whether to corrupt head or tail
v1.9.0,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.9.0,flatten mask
v1.9.0,Tails are corrupted if heads are not corrupted
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The random seed used at the beginning of the pipeline
v1.9.0,: The model trained by the pipeline
v1.9.0,: The training triples
v1.9.0,: The training loop used by the pipeline
v1.9.0,: The losses during training
v1.9.0,: The results evaluated by the pipeline
v1.9.0,: How long in seconds did training take?
v1.9.0,: How long in seconds did evaluation take?
v1.9.0,: An early stopper
v1.9.0,: The configuration
v1.9.0,: Any additional metadata as a dictionary
v1.9.0,: The version of PyKEEN used to create these results
v1.9.0,: The git hash of PyKEEN used to create these results
v1.9.0,file names for storing results
v1.9.0,TODO: rename param?
v1.9.0,always save results as json file
v1.9.0,"save other components only if requested (which they are, by default)"
v1.9.0,TODO use pathlib here
v1.9.0,"note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:"
v1.9.0,"when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless"
v1.9.0,note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()
v1.9.0,only one original value => assume this to be the mean
v1.9.0,multiple values => assume they correspond to individual trials
v1.9.0,metrics accumulates rows for a dataframe for comparison against the original reported results (if any)
v1.9.0,"TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)"
v1.9.0,summarize
v1.9.0,skip special parameters
v1.9.0,FIXME this should never happen.
v1.9.0,1. Dataset
v1.9.0,2. Model
v1.9.0,3. Loss
v1.9.0,4. Regularizer
v1.9.0,5. Optimizer
v1.9.0,5.1 Learning Rate Scheduler
v1.9.0,6. Training Loop
v1.9.0,7. Training (ronaldo style)
v1.9.0,8. Evaluation
v1.9.0,9. Tracking
v1.9.0,Misc
v1.9.0,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.9.0,used random_seed to ensure reproducible results
v1.9.0,We have to set clear optimizer to False since training should be continued
v1.9.0,Start tracking
v1.9.0,evaluation restriction to a subset of entities/relations
v1.9.0,TODO should training be reset?
v1.9.0,TODO should kwargs for loss and regularizer be checked and raised for?
v1.9.0,Log model parameters
v1.9.0,Log loss parameters
v1.9.0,the loss was already logged as part of the model kwargs
v1.9.0,"loss=loss_resolver.normalize_inst(model_instance.loss),"
v1.9.0,Log regularizer parameters
v1.9.0,Stopping
v1.9.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.9.0,Add logging for debugging
v1.9.0,Train like Cristiano Ronaldo
v1.9.0,Build up a list of triples if we want to be in the filtered setting
v1.9.0,"If the user gave custom ""additional_filter_triples"""
v1.9.0,Determine whether the validation triples should also be filtered while performing test evaluation
v1.9.0,TODO consider implications of duplicates
v1.9.0,Evaluate
v1.9.0,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.9.0,Add logging about evaluator for debugging
v1.9.0,"If the evaluation still fail using the CPU, the error is raised"
v1.9.0,"When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is"
v1.9.0,restarted with PyKEEN's automatic memory optimization
v1.9.0,"When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation"
v1.9.0,is restarted using the cpu
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Imported from PyTorch
v1.9.0,: A wrapper around the hidden scheduler base class
v1.9.0,: The default strategy for optimizing the lr_schedulers' hyper-parameters
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,TODO what happens if already exists?
v1.9.0,TODO incorporate setting of random seed
v1.9.0,pipeline_kwargs=dict(
v1.9.0,"random_seed=random_non_negative_int(),"
v1.9.0,"),"
v1.9.0,Add dataset to current_pipeline
v1.9.0,"Training, test, and validation paths are provided"
v1.9.0,Add loss function to current_pipeline
v1.9.0,Add regularizer to current_pipeline
v1.9.0,Add optimizer to current_pipeline
v1.9.0,Add training approach to current_pipeline
v1.9.0,Add evaluation
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.9.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.9.0,the columns are not ordered properly.
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,convert class to string to use caching
v1.9.0,Assume it's a file path
v1.9.0,note: we only need to set the create_inverse_triples in the training factory.
v1.9.0,normalize dataset kwargs
v1.9.0,enable passing force option via dataset_kwargs
v1.9.0,hash kwargs
v1.9.0,normalize dataset name
v1.9.0,get canonic path
v1.9.0,try to use cached dataset
v1.9.0,load dataset without cache
v1.9.0,store cache
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The name of the dataset to download
v1.9.0,"note: we do not use the built-in constants here, since those refer to OGB nomenclature"
v1.9.0,(which happens to coincide with ours)
v1.9.0,FIXME these are already identifiers
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,relation typing
v1.9.0,constants
v1.9.0,unique
v1.9.0,compute over all triples
v1.9.0,Determine group key
v1.9.0,Add labels if requested
v1.9.0,TODO: Merge with _common?
v1.9.0,include hash over triples into cache-file name
v1.9.0,include part hash into cache-file name
v1.9.0,re-use cached file if possible
v1.9.0,select triples
v1.9.0,save to file
v1.9.0,Prune by support and confidence
v1.9.0,TODO: Consider merging with other analysis methods
v1.9.0,TODO: Consider merging with other analysis methods
v1.9.0,TODO: Consider merging with other analysis methods
v1.9.0,"num_triples_validation: Optional[int],"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Raise matplotlib level
v1.9.0,expected metrics
v1.9.0,Needs simulation
v1.9.0,See https://zenodo.org/record/6331629
v1.9.0,TODO: maybe merge into analyze / make sub-command
v1.9.0,only save full data
v1.9.0,Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)
v1.9.0,Plot: difference between mean head and tail degree
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,don't call this function by itself. assumes called through the `validation`
v1.9.0,property and the _training factory has already been loaded
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Normalize path
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Base classes
v1.9.0,Utilities
v1.9.0,: A factory wrapping the training triples
v1.9.0,": A factory wrapping the testing triples, that share indices with the training triples"
v1.9.0,": A factory wrapping the validation triples, that share indices with the training triples"
v1.9.0,: the dataset's name
v1.9.0,TODO: Make a constant for the names
v1.9.0,docstr-coverage: inherited
v1.9.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.9.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.9.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.9.0,: The directory in which the cached data is stored
v1.9.0,TODO: use class-resolver normalize?
v1.9.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.9.0,don't call this function by itself. assumes called through the `validation`
v1.9.0,property and the _training factory has already been loaded
v1.9.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,"relative paths within zip file's always follow Posix path, even on Windows"
v1.9.0,tarfile does not like pathlib
v1.9.0,: URL to the data to download
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Utilities
v1.9.0,Base Classes
v1.9.0,Concrete Classes
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"ZENODO_URL = ""https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip"""
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.9.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Base class
v1.9.0,Mid-level classes
v1.9.0,: A factory wrapping the training triples
v1.9.0,: A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT
v1.9.0,share indices with the transductive training
v1.9.0,": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.9.0,": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.9.0,: All datasets should take care of inverse triple creation
v1.9.0,": The actual instance of the training factory, which is exposed to the user through `transductive_training`"
v1.9.0,": The actual instance of the inductive inference factory,"
v1.9.0,: which is exposed to the user through `inductive_inference`
v1.9.0,": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`"
v1.9.0,": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`"
v1.9.0,: The directory in which the cached data is stored
v1.9.0,generate subfolders 'training' and  'inference'
v1.9.0,TODO: use class-resolver normalize?
v1.9.0,add v1 / v2 / v3 / v4 for inductive splits if available
v1.9.0,important: inductive_inference shares the same RELATIONS with the transductive training graph
v1.9.0,inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph
v1.9.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.9.0,inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph
v1.9.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,Base class
v1.9.0,Mid-level classes
v1.9.0,Datasets
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,graph pairs
v1.9.0,graph sizes
v1.9.0,graph versions
v1.9.0,: The link to the zip file
v1.9.0,: The hex digest for the zip file
v1.9.0,Input validation.
v1.9.0,ensure zip file is present
v1.9.0,save relative paths beforehand so they are present for loading
v1.9.0,delegate to super class
v1.9.0,docstr-coverage: inherited
v1.9.0,"left side has files ending with 1, right side with 2"
v1.9.0,docstr-coverage: inherited
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,": The mapping from (graph-pair, side) to triple file name"
v1.9.0,: The internal dataset name
v1.9.0,: The hex digest for the zip file
v1.9.0,input validation
v1.9.0,store *before* calling super to have it available when loading the graphs
v1.9.0,ensure zip file is present
v1.9.0,shared directory for multiple datasets.
v1.9.0,docstr-coverage: inherited
v1.9.0,create triples factory
v1.9.0,docstr-coverage: inherited
v1.9.0,load mappings for both sides
v1.9.0,load triple alignments
v1.9.0,extract entity alignments
v1.9.0,"(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2"
v1.9.0,TODO: support ID-only graphs
v1.9.0,load both graphs
v1.9.0,load alignment
v1.9.0,drop duplicates
v1.9.0,combine
v1.9.0,store for repr
v1.9.0,split
v1.9.0,create inverse triples only for training
v1.9.0,docstr-coverage: inherited
v1.9.0,base
v1.9.0,concrete
v1.9.0,Abstract class
v1.9.0,Concrete classes
v1.9.0,Data Structures
v1.9.0,a buffer for the triples
v1.9.0,the offsets
v1.9.0,normalization
v1.9.0,append shifted mapped triples
v1.9.0,update offsets
v1.9.0,merge labels with same ID
v1.9.0,for mypy
v1.9.0,reconstruct label-to-id
v1.9.0,optional
v1.9.0,merge entity mapping
v1.9.0,merge relation mapping
v1.9.0,convert labels to IDs
v1.9.0,"map labels, using -1 as fill-value for invalid labels"
v1.9.0,"we cannot drop them here, since the two columns need to stay aligned"
v1.9.0,filter alignment
v1.9.0,map alignment from old IDs to new IDs
v1.9.0,determine swapping partner
v1.9.0,only keep triples where we have a swapping partner
v1.9.0,replace by swapping partner
v1.9.0,": the merged id-based triples, shape: (n, 3)"
v1.9.0,": the updated alignment, shape: (2, m)"
v1.9.0,: additional keyword-based parameters for adjusting label-to-id mappings
v1.9.0,concatenate triples
v1.9.0,filter alignment and translate to IDs
v1.9.0,process
v1.9.0,TODO: restrict to only using training alignments?
v1.9.0,merge mappings
v1.9.0,docstr-coverage: inherited
v1.9.0,docstr-coverage: inherited
v1.9.0,add swap triples
v1.9.0,"e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)"
v1.9.0,create dense entity remapping for swap
v1.9.0,add swapped triples
v1.9.0,swap head
v1.9.0,swap tail
v1.9.0,: the name of the additional alignment relation
v1.9.0,docstr-coverage: inherited
v1.9.0,add alignment triples with extra relation
v1.9.0,docstr-coverage: inherited
v1.9.0,"determine connected components regarding the same-as relation (i.e., applies transitivity)"
v1.9.0,apply id mapping
v1.9.0,ensure consecutive IDs
v1.9.0,only use training alignments?
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,1. Dataset
v1.9.0,2. Model
v1.9.0,3. Loss
v1.9.0,4. Regularizer
v1.9.0,5. Optimizer
v1.9.0,5.1 Learning Rate Scheduler
v1.9.0,6. Training Loop
v1.9.0,7. Training
v1.9.0,8. Evaluation
v1.9.0,9. Trackers
v1.9.0,Misc.
v1.9.0,log pruning
v1.9.0,"trial was successful, but has to be ended"
v1.9.0,also show info
v1.9.0,2. Model
v1.9.0,3. Loss
v1.9.0,4. Regularizer
v1.9.0,5. Optimizer
v1.9.0,5.1 Learning Rate Scheduler
v1.9.0,"TODO this fixes the issue for negative samplers, but does not generally address it."
v1.9.0,"For example, some of them obscure their arguments with **kwargs, so should we look"
v1.9.0,at the parent class? Sounds like something to put in class resolver by using the
v1.9.0,"inspect module. For now, this solution will rely on the fact that the sampler is a"
v1.9.0,direct descendent of a parent NegativeSampler
v1.9.0,create result tracker to allow to gracefully close failed trials
v1.9.0,1. Dataset
v1.9.0,2. Model
v1.9.0,3. Loss
v1.9.0,4. Regularizer
v1.9.0,5. Optimizer
v1.9.0,5.1 Learning Rate Scheduler
v1.9.0,6. Training Loop
v1.9.0,7. Training
v1.9.0,8. Evaluation
v1.9.0,9. Tracker
v1.9.0,Misc.
v1.9.0,close run in result tracker
v1.9.0,raise the error again (which will be catched in study.optimize)
v1.9.0,: The :mod:`optuna` study object
v1.9.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.9.0,Output study information
v1.9.0,Output all trials
v1.9.0,Output best trial as pipeline configuration file
v1.9.0,1. Dataset
v1.9.0,2. Model
v1.9.0,3. Loss
v1.9.0,4. Regularizer
v1.9.0,5. Optimizer
v1.9.0,5.1 Learning Rate Scheduler
v1.9.0,6. Training Loop
v1.9.0,7. Training
v1.9.0,8. Evaluation
v1.9.0,9. Tracking
v1.9.0,6. Misc
v1.9.0,Optuna Study Settings
v1.9.0,Optuna Optimization Settings
v1.9.0,TODO: use metric.increasing to determine default direction
v1.9.0,0. Metadata/Provenance
v1.9.0,1. Dataset
v1.9.0,2. Model
v1.9.0,3. Loss
v1.9.0,4. Regularizer
v1.9.0,5. Optimizer
v1.9.0,5.1 Learning Rate Scheduler
v1.9.0,6. Training Loop
v1.9.0,7. Training
v1.9.0,8. Evaluation
v1.9.0,9. Tracking
v1.9.0,1. Dataset
v1.9.0,2. Model
v1.9.0,3. Loss
v1.9.0,4. Regularizer
v1.9.0,5. Optimizer
v1.9.0,5.1 Learning Rate Scheduler
v1.9.0,6. Training Loop
v1.9.0,7. Training
v1.9.0,8. Evaluation
v1.9.0,9. Tracker
v1.9.0,Optuna Misc.
v1.9.0,Pipeline Misc.
v1.9.0,Invoke optimization of the objective function.
v1.9.0,TODO: make it even easier to specify categorical strategies just as lists
v1.9.0,"if isinstance(info, (tuple, list, set)):"
v1.9.0,"info = dict(type='categorical', choices=list(info))"
v1.9.0,get log from info - could either be a boolean or string
v1.9.0,"otherwise, dataset refers to a file that should be automatically split"
v1.9.0,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.9.0,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.9.0,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.9.0,if has_dataset(dataset_name):
v1.9.0,"study.set_user_attr('dataset', dataset_name)"
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,noqa: DAR101
v1.9.0,-*- coding: utf-8 -*-
v1.9.0,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,
v1.8.2,Configuration file for the Sphinx documentation builder.
v1.8.2,
v1.8.2,This file does only contain a selection of the most common options. For a
v1.8.2,full list see the documentation:
v1.8.2,http://www.sphinx-doc.org/en/master/config
v1.8.2,-- Path setup --------------------------------------------------------------
v1.8.2,"If extensions (or modules to document with autodoc) are in another directory,"
v1.8.2,add these directories to sys.path here. If the directory is relative to the
v1.8.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.8.2,
v1.8.2,"sys.path.insert(0, os.path.abspath('..'))"
v1.8.2,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.8.2,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.8.2,from unittest.mock import Mock
v1.8.2,sys.modules['numpy'] = Mock()
v1.8.2,sys.modules['numpy.linalg'] = Mock()
v1.8.2,sys.modules['scipy'] = Mock()
v1.8.2,sys.modules['scipy.optimize'] = Mock()
v1.8.2,sys.modules['scipy.interpolate'] = Mock()
v1.8.2,sys.modules['scipy.sparse'] = Mock()
v1.8.2,sys.modules['scipy.ndimage'] = Mock()
v1.8.2,sys.modules['scipy.ndimage.filters'] = Mock()
v1.8.2,sys.modules['tensorflow'] = Mock()
v1.8.2,sys.modules['theano'] = Mock()
v1.8.2,sys.modules['theano.tensor'] = Mock()
v1.8.2,sys.modules['torch'] = Mock()
v1.8.2,sys.modules['torch.optim'] = Mock()
v1.8.2,sys.modules['torch.nn'] = Mock()
v1.8.2,sys.modules['torch.nn.init'] = Mock()
v1.8.2,sys.modules['torch.autograd'] = Mock()
v1.8.2,sys.modules['sklearn'] = Mock()
v1.8.2,sys.modules['sklearn.model_selection'] = Mock()
v1.8.2,sys.modules['sklearn.utils'] = Mock()
v1.8.2,-- Project information -----------------------------------------------------
v1.8.2,"The full version, including alpha/beta/rc tags."
v1.8.2,The short X.Y version.
v1.8.2,-- General configuration ---------------------------------------------------
v1.8.2,"If your documentation needs a minimal Sphinx version, state it here."
v1.8.2,
v1.8.2,needs_sphinx = '1.0'
v1.8.2,"If true, the current module name will be prepended to all description"
v1.8.2,unit titles (such as .. function::).
v1.8.2,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.8.2,"Add any Sphinx extension module names here, as strings. They can be"
v1.8.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.8.2,ones.
v1.8.2,show todo's
v1.8.2,generate autosummary pages
v1.8.2,"Add any paths that contain templates here, relative to this directory."
v1.8.2,The suffix(es) of source filenames.
v1.8.2,You can specify multiple suffix as a list of string:
v1.8.2,
v1.8.2,"source_suffix = ['.rst', '.md']"
v1.8.2,The master toctree document.
v1.8.2,The language for content autogenerated by Sphinx. Refer to documentation
v1.8.2,for a list of supported languages.
v1.8.2,
v1.8.2,This is also used if you do content translation via gettext catalogs.
v1.8.2,"Usually you set ""language"" from the command line for these cases."
v1.8.2,"List of patterns, relative to source directory, that match files and"
v1.8.2,directories to ignore when looking for source files.
v1.8.2,This pattern also affects html_static_path and html_extra_path.
v1.8.2,The name of the Pygments (syntax highlighting) style to use.
v1.8.2,-- Options for HTML output -------------------------------------------------
v1.8.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.8.2,a list of builtin themes.
v1.8.2,
v1.8.2,Theme options are theme-specific and customize the look and feel of a theme
v1.8.2,"further.  For a list of options available for each theme, see the"
v1.8.2,documentation.
v1.8.2,
v1.8.2,html_theme_options = {}
v1.8.2,"Add any paths that contain custom static files (such as style sheets) here,"
v1.8.2,"relative to this directory. They are copied after the builtin static files,"
v1.8.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.8.2,html_static_path = ['_static']
v1.8.2,"Custom sidebar templates, must be a dictionary that maps document names"
v1.8.2,to template names.
v1.8.2,
v1.8.2,The default sidebars (for documents that don't match any pattern) are
v1.8.2,defined by theme itself.  Builtin themes are using these templates by
v1.8.2,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.8.2,'searchbox.html']``.
v1.8.2,
v1.8.2,html_sidebars = {}
v1.8.2,The name of an image file (relative to this directory) to place at the top
v1.8.2,of the sidebar.
v1.8.2,
v1.8.2,-- Options for HTMLHelp output ---------------------------------------------
v1.8.2,Output file base name for HTML help builder.
v1.8.2,-- Options for LaTeX output ------------------------------------------------
v1.8.2,latex_elements = {
v1.8.2,The paper size ('letterpaper' or 'a4paper').
v1.8.2,
v1.8.2,"'papersize': 'letterpaper',"
v1.8.2,
v1.8.2,"The font size ('10pt', '11pt' or '12pt')."
v1.8.2,
v1.8.2,"'pointsize': '10pt',"
v1.8.2,
v1.8.2,Additional stuff for the LaTeX preamble.
v1.8.2,
v1.8.2,"'preamble': '',"
v1.8.2,
v1.8.2,Latex figure (float) alignment
v1.8.2,
v1.8.2,"'figure_align': 'htbp',"
v1.8.2,}
v1.8.2,Grouping the document tree into LaTeX files. List of tuples
v1.8.2,"(source start file, target name, title,"
v1.8.2,"author, documentclass [howto, manual, or own class])."
v1.8.2,latex_documents = [
v1.8.2,(
v1.8.2,"master_doc,"
v1.8.2,"'pykeen.tex',"
v1.8.2,"'PyKEEN Documentation',"
v1.8.2,"author,"
v1.8.2,"'manual',"
v1.8.2,"),"
v1.8.2,]
v1.8.2,-- Options for manual page output ------------------------------------------
v1.8.2,One entry per manual page. List of tuples
v1.8.2,"(source start file, name, description, authors, manual section)."
v1.8.2,-- Options for Texinfo output ----------------------------------------------
v1.8.2,Grouping the document tree into Texinfo files. List of tuples
v1.8.2,"(source start file, target name, title, author,"
v1.8.2,"dir menu entry, description, category)"
v1.8.2,-- Options for Epub output -------------------------------------------------
v1.8.2,Bibliographic Dublin Core info.
v1.8.2,epub_title = project
v1.8.2,The unique identifier of the text. This can be a ISBN number
v1.8.2,or the project homepage.
v1.8.2,
v1.8.2,epub_identifier = ''
v1.8.2,A unique identification for the text.
v1.8.2,
v1.8.2,epub_uid = ''
v1.8.2,A list of files that should not be packed into the epub file.
v1.8.2,epub_exclude_files = ['search.html']
v1.8.2,-- Extension configuration -------------------------------------------------
v1.8.2,-- Options for intersphinx extension ---------------------------------------
v1.8.2,Example configuration for intersphinx: refer to the Python standard library.
v1.8.2,"'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),"
v1.8.2,autodoc_member_order = 'bysource'
v1.8.2,autodoc_typehints = 'both' # TODO turn on after 4.1 release
v1.8.2,autodoc_preserve_defaults = True
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,check probability distribution
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Check a model param is optimized
v1.8.2,Check a loss param is optimized
v1.8.2,Check a model param is NOT optimized
v1.8.2,Check a loss param is optimized
v1.8.2,Check a model param is optimized
v1.8.2,Check a loss param is NOT optimized
v1.8.2,Check a model param is NOT optimized
v1.8.2,Check a loss param is NOT optimized
v1.8.2,verify failure
v1.8.2,"Since custom data was passed, we can't store any of this"
v1.8.2,"currently, any custom data doesn't get stored."
v1.8.2,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.8.2,"Since there's no source path information, these shouldn't be"
v1.8.2,"added, even if it might be possible to infer path information"
v1.8.2,from the triples factories
v1.8.2,"Since paths were passed for training, testing, and validation,"
v1.8.2,they should be stored as study-level attributes
v1.8.2,Check a model param is optimized
v1.8.2,Check a loss param is optimized
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,check if within 0.5 std of observed
v1.8.2,test error is raised
v1.8.2,Tests that exception will be thrown when more than or less than three tensors are passed
v1.8.2,Test that regularization term is computed correctly
v1.8.2,Entity soft constraint
v1.8.2,Orthogonality soft constraint
v1.8.2,ensure regularizer is on correct device
v1.8.2,"After first update, should change the term"
v1.8.2,"After second update, no change should happen"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,create broadcastable shapes
v1.8.2,check correct value range
v1.8.2,check maximum norm constraint
v1.8.2,unchanged values for small norms
v1.8.2,random entity embeddings & projections
v1.8.2,random relation embeddings & projections
v1.8.2,project
v1.8.2,check shape:
v1.8.2,check normalization
v1.8.2,check equivalence of re-formulation
v1.8.2,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.8.2,= r_p (e_p^T e) + e'
v1.8.2,"create random array, estimate the costs of addition, and measure some execution times."
v1.8.2,"then, compute correlation between the estimated cost, and the measured time."
v1.8.2,check for strong correlation between estimated costs and measured execution time
v1.8.2,get optimal sequence
v1.8.2,check caching
v1.8.2,get optimal sequence
v1.8.2,check correct cost
v1.8.2,check optimality
v1.8.2,compare result to sequential addition
v1.8.2,compare result to sequential addition
v1.8.2,ensure each node participates in at least one edge
v1.8.2,check type and shape
v1.8.2,number of colors is monotonically increasing
v1.8.2,ensure each node participates in at least one edge
v1.8.2,normalize
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,equal value; larger is better
v1.8.2,equal value; smaller is better
v1.8.2,larger is better; improvement
v1.8.2,larger is better; improvement; but not significant
v1.8.2,: The window size used by the early stopper
v1.8.2,: The mock losses the mock evaluator will return
v1.8.2,: The (zeroed) index  - 1 at which stopping will occur
v1.8.2,: The minimum improvement
v1.8.2,: The best results
v1.8.2,Set automatic_memory_optimization to false for tests
v1.8.2,Step early stopper
v1.8.2,check storing of results
v1.8.2,not needed for test
v1.8.2,assert that reporting another metric for this epoch raises an error
v1.8.2,: The window size used by the early stopper
v1.8.2,: The (zeroed) index  - 1 at which stopping will occur
v1.8.2,: The minimum improvement
v1.8.2,: The random seed to use for reproducibility
v1.8.2,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.8.2,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.8.2,: The batch size to use.
v1.8.2,Fix seed for reproducibility
v1.8.2,Set automatic_memory_optimization to false during testing
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,See https://github.com/pykeen/pykeen/pull/883
v1.8.2,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.8.2,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.8.2,"first triple, and passing create_inverse_triples=True."
v1.8.2,check for warning
v1.8.2,check for filtered triples
v1.8.2,check for correct inverse triples flag
v1.8.2,check correct translation
v1.8.2,check column order
v1.8.2,apply restriction
v1.8.2,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.8.2,check that inverse_triples is correctly carried over
v1.8.2,verify that the label-to-ID mapping has not been changed
v1.8.2,verify that triples have been filtered
v1.8.2,Test different combinations of restrictions
v1.8.2,check compressed triples
v1.8.2,reconstruct triples from compressed form
v1.8.2,check data loader
v1.8.2,set create inverse triple to true
v1.8.2,split factory
v1.8.2,check that in *training* inverse triple are to be created
v1.8.2,check that in all other splits no inverse triples are to be created
v1.8.2,verify that all entities and relations are present in the training factory
v1.8.2,verify that no triple got lost
v1.8.2,verify that the label-to-id mappings match
v1.8.2,Slightly larger number of triples to guarantee split can find coverage of all entities and relations.
v1.8.2,serialize
v1.8.2,de-serialize
v1.8.2,check for equality
v1.8.2,TODO: this could be (Core)TriplesFactory.__equal__
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"DummyModel,"
v1.8.2,3x batch norm: bias + scale --> 6
v1.8.2,entity specific bias        --> 1
v1.8.2,==================================
v1.8.2,7
v1.8.2,"two bias terms, one conv-filter"
v1.8.2,check type
v1.8.2,check shape
v1.8.2,check ID ranges
v1.8.2,this is only done in one of the models
v1.8.2,this is only done in one of the models
v1.8.2,Two linear layer biases
v1.8.2,"Two BN layers, bias & scale"
v1.8.2,Test that the weight in the MLP is trainable (i.e. requires grad)
v1.8.2,quaternion have four components
v1.8.2,: one bias per layer
v1.8.2,: one bias per layer
v1.8.2,entity embeddings
v1.8.2,relation embeddings
v1.8.2,Compute Scores
v1.8.2,Use different dimension for relation embedding: relation_dim > entity_dim
v1.8.2,relation embeddings
v1.8.2,Compute Scores
v1.8.2,Use different dimension for relation embedding: relation_dim < entity_dim
v1.8.2,entity embeddings
v1.8.2,relation embeddings
v1.8.2,Compute Scores
v1.8.2,: 2xBN (bias & scale)
v1.8.2,the combination bias
v1.8.2,FIXME definitely a type mismatch going on here
v1.8.2,check shape
v1.8.2,check content
v1.8.2,create triples factory with inverse relations
v1.8.2,head prediction via inverse tail prediction
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,empty lists are falsy
v1.8.2,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.8.2,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.8.2,pipeline is checked.
v1.8.2,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.8.2,Resume the previous pipeline
v1.8.2,The MockModel gives the highest score to the highest entity id
v1.8.2,The test triples are created to yield the third highest score on both head and tail prediction
v1.8.2,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.8.2,These triples are created to yield the highest score on both head and tail prediction for the
v1.8.2,test triple at hand
v1.8.2,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.8.2,test triple at hand
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.8.2,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.8.2,Test looking up inverse triples
v1.8.2,test new label to ID
v1.8.2,type
v1.8.2,old labels
v1.8.2,"new, compact IDs"
v1.8.2,test vectorized lookup
v1.8.2,type
v1.8.2,shape
v1.8.2,value range
v1.8.2,only occurring Ids get mapped to non-negative numbers
v1.8.2,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.8.2,check type
v1.8.2,check shape
v1.8.2,check content
v1.8.2,check type
v1.8.2,check shape
v1.8.2,check 1-hot
v1.8.2,check type
v1.8.2,check shape
v1.8.2,check value range
v1.8.2,check self-similarity = 1
v1.8.2,base relation
v1.8.2,exact duplicate
v1.8.2,99% duplicate
v1.8.2,50% duplicate
v1.8.2,exact inverse
v1.8.2,99% inverse
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The expected number of entities
v1.8.2,: The expected number of relations
v1.8.2,: The expected number of triples
v1.8.2,": The tolerance on expected number of triples, for randomized situations"
v1.8.2,: The dataset to test
v1.8.2,: The instantiated dataset
v1.8.2,: Should the validation be assumed to have been loaded with train/test?
v1.8.2,Not loaded
v1.8.2,Load
v1.8.2,Test caching
v1.8.2,assert (end - start) < 1.0e-02
v1.8.2,Test consistency of training / validation / testing mapping
v1.8.2,": The directory, if there is caching"
v1.8.2,: The batch size
v1.8.2,: The number of negatives per positive for sLCWA training loop.
v1.8.2,: The number of entities LCWA training loop / label smoothing.
v1.8.2,test reduction
v1.8.2,test finite loss value
v1.8.2,Test backward
v1.8.2,negative scores decreased compared to positive ones
v1.8.2,negative scores decreased compared to positive ones
v1.8.2,: The number of entities.
v1.8.2,: The number of negative samples
v1.8.2,: The number of entities.
v1.8.2,: The equivalence for models with batch norm only holds in evaluation mode
v1.8.2,: The equivalence for models with batch norm only holds in evaluation mode
v1.8.2,: The equivalence for models with batch norm only holds in evaluation mode
v1.8.2,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.8.2,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.8.2,test multiple different initializations
v1.8.2,calculate by functional
v1.8.2,calculate manually
v1.8.2,simple
v1.8.2,nested
v1.8.2,nested
v1.8.2,prepare a temporary test directory
v1.8.2,check that file was created
v1.8.2,make sure to close file before trying to delete it
v1.8.2,delete intermediate files
v1.8.2,: The batch size
v1.8.2,: The triples factory
v1.8.2,: Class of regularizer to test
v1.8.2,: The constructor parameters to pass to the regularizer
v1.8.2,": The regularizer instance, initialized in setUp"
v1.8.2,: A positive batch
v1.8.2,: The device
v1.8.2,move test instance to device
v1.8.2,Use RESCAL as it regularizes multiple tensors of different shape.
v1.8.2,Check if regularizer is stored correctly.
v1.8.2,Forward pass (should update regularizer)
v1.8.2,Call post_parameter_update (should reset regularizer)
v1.8.2,Check if regularization term is reset
v1.8.2,Call method
v1.8.2,Generate random tensors
v1.8.2,Call update
v1.8.2,check shape
v1.8.2,compute expected term
v1.8.2,Generate random tensor
v1.8.2,calculate penalty
v1.8.2,check shape
v1.8.2,check value
v1.8.2,update term
v1.8.2,check that the expected term is returned
v1.8.2,FIXME isn't any finite number allowed now?
v1.8.2,: Additional arguments passed to the training loop's constructor method
v1.8.2,: The triples factory instance
v1.8.2,: The batch size for use for forward_* tests
v1.8.2,: The embedding dimensionality
v1.8.2,: Whether to create inverse triples (needed e.g. by ConvE)
v1.8.2,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.8.2,: The batch size for use when testing training procedures
v1.8.2,: The number of epochs to train the model
v1.8.2,: A random number generator from torch
v1.8.2,: The number of parameters which receive a constant (i.e. non-randomized)
v1.8.2,initialization
v1.8.2,: Static extras to append to the CLI
v1.8.2,: the model's device
v1.8.2,: the inductive mode
v1.8.2,for reproducible testing
v1.8.2,insert shared parameters
v1.8.2,move model to correct device
v1.8.2,Check that all the parameters actually require a gradient
v1.8.2,Try to initialize an optimizer
v1.8.2,get model parameters
v1.8.2,re-initialize
v1.8.2,check that the operation works in-place
v1.8.2,check that the parameters where modified
v1.8.2,check for finite values by default
v1.8.2,check whether a gradient can be back-propgated
v1.8.2,"assert batch comprises (head, relation) pairs"
v1.8.2,"assert batch comprises (head, tail) pairs"
v1.8.2,TODO: look into score_r for inverse relations
v1.8.2,"assert batch comprises (relation, tail) pairs"
v1.8.2,"For the high/low memory test cases of NTN, SE, etc."
v1.8.2,"else, leave to default"
v1.8.2,Make sure that inverse triples are created if create_inverse_triples=True
v1.8.2,triples factory is added by the pipeline
v1.8.2,TODO: Catch HolE MKL error?
v1.8.2,set regularizer term to something that isn't zero
v1.8.2,call post_parameter_update
v1.8.2,assert that the regularization term has been reset
v1.8.2,do one optimization step
v1.8.2,call post_parameter_update
v1.8.2,check model constraints
v1.8.2,"assert batch comprises (relation, tail) pairs"
v1.8.2,"assert batch comprises (relation, tail) pairs"
v1.8.2,"assert batch comprises (relation, tail) pairs"
v1.8.2,call some functions
v1.8.2,reset to old state
v1.8.2,Distance-based model
v1.8.2,dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)
v1.8.2,check type
v1.8.2,check shape
v1.8.2,create a new instance with guaranteed dropout
v1.8.2,set to training mode
v1.8.2,check for different output
v1.8.2,use more samples to make sure that enough values can be dropped
v1.8.2,select random indices
v1.8.2,forward pass with full graph
v1.8.2,forward pass with restricted graph
v1.8.2,verify the results are similar
v1.8.2,: The number of entities
v1.8.2,: The number of triples
v1.8.2,: the message dim
v1.8.2,TODO: separation message vs. entity dim?
v1.8.2,check shape
v1.8.2,check dtype
v1.8.2,check finite values (e.g. due to division by zero)
v1.8.2,check non-negativity
v1.8.2,: The input dimension
v1.8.2,: the number of entities
v1.8.2,: the shape of the tensor to initialize
v1.8.2,: to be initialized / set in subclass
v1.8.2,: the interaction to use for testing a model
v1.8.2,initializers *may* work in-place => clone
v1.8.2,actual number may be different...
v1.8.2,unfavourable split to ensure that cleanup is necessary
v1.8.2,check for unclean split
v1.8.2,check that no triple got lost
v1.8.2,check that triples where only moved from other to reference
v1.8.2,check that all entities occur in reference
v1.8.2,check that no triple got lost
v1.8.2,check that all entities are covered in first part
v1.8.2,the model
v1.8.2,Settings
v1.8.2,Use small model (untrained)
v1.8.2,Get batch
v1.8.2,Compute scores
v1.8.2,Compute mask only if required
v1.8.2,TODO: Re-use filtering code
v1.8.2,"shape: (batch_size, num_triples)"
v1.8.2,"shape: (batch_size, num_entities)"
v1.8.2,Process one batch
v1.8.2,shape
v1.8.2,value range
v1.8.2,no duplicates
v1.8.2,shape
v1.8.2,value range
v1.8.2,no duplicates
v1.8.2,shape
v1.8.2,value range
v1.8.2,"no repetition, except padding idx"
v1.8.2,: The batch size
v1.8.2,: the maximum number of candidates
v1.8.2,: the number of ranks
v1.8.2,: the number of samples to use for monte-carlo estimation
v1.8.2,: the number of candidates for each individual ranking task
v1.8.2,: the ranks for each individual ranking task
v1.8.2,data type
v1.8.2,value range
v1.8.2,original ranks
v1.8.2,better ranks
v1.8.2,variances are non-negative
v1.8.2,generate random weights such that sum = n
v1.8.2,for sanity checking: give the largest weight to best rank => should improve
v1.8.2,generate two versions
v1.8.2,1. repeat each rank/candidate pair a random number of times
v1.8.2,"2. do not repeat, but assign a corresponding weight"
v1.8.2,check flatness
v1.8.2,"TODO: does this suffice, or do we really need float as datatype?"
v1.8.2,generate random triples factories
v1.8.2,generate random alignment
v1.8.2,add label information if necessary
v1.8.2,prepare alignment data frame
v1.8.2,call
v1.8.2,check
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,TODO: this could be shared with the model tests
v1.8.2,"FixedModel: dict(embedding_dim=EMBEDDING_DIM),"
v1.8.2,test combinations of models with training loops
v1.8.2,some models require inverse relations
v1.8.2,some model require access to the training triples
v1.8.2,"inductive models require an inductive mode to be set, and an inference factory to be passed"
v1.8.2,fake an inference factory
v1.8.2,automatically choose accelerator
v1.8.2,defaults to TensorBoard; explicitly disabled here
v1.8.2,disable checkpointing
v1.8.2,fast run
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,check for finite values by default
v1.8.2,Set into training mode to check if it is correctly set to evaluation mode.
v1.8.2,Set into training mode to check if it is correctly set to evaluation mode.
v1.8.2,Set into training mode to check if it is correctly set to evaluation mode.
v1.8.2,"TODO: Remove, since it stems from old-style model"
v1.8.2,Get embeddings
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,≈ result of softmax
v1.8.2,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.8.2,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.8.2,"pos_distances = [0., 0., 0.5, 0.5]"
v1.8.2,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.8.2,≈ result of sigmoid
v1.8.2,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.8.2,expected_loss ≈ 0.34
v1.8.2,Create dummy dense labels
v1.8.2,Check if labels form a probability distribution
v1.8.2,Apply label smoothing
v1.8.2,Check if smooth labels form probability distribution
v1.8.2,Create dummy sLCWA labels
v1.8.2,Apply label smoothing
v1.8.2,generate random ratios
v1.8.2,check size
v1.8.2,check value range
v1.8.2,check total split
v1.8.2,check consistency with ratios
v1.8.2,the number of decimal digits equivalent to 1 / n_total
v1.8.2,check type
v1.8.2,check values
v1.8.2,compare against expected
v1.8.2,generated_triples = generate_triples()
v1.8.2,check type
v1.8.2,check format
v1.8.2,check coverage
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"naive implementation, O(n2)"
v1.8.2,check correct output type
v1.8.2,check value range subset
v1.8.2,check value range side
v1.8.2,check columns
v1.8.2,check value range and type
v1.8.2,check value range entity IDs
v1.8.2,check value range entity labels
v1.8.2,check correct type
v1.8.2,check relation_id value range
v1.8.2,check pattern value range
v1.8.2,check confidence value range
v1.8.2,check support value range
v1.8.2,check correct type
v1.8.2,check relation_id value range
v1.8.2,check pattern value range
v1.8.2,check correct type
v1.8.2,check relation_id value range
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,clear
v1.8.2,docstr-coverage: inherited
v1.8.2,assumes deterministic entity to id mapping
v1.8.2,from left_tf
v1.8.2,from right_tf with offset
v1.8.2,docstr-coverage: inherited
v1.8.2,assumes deterministic entity to id mapping
v1.8.2,from left_tf
v1.8.2,from right_tf with offset
v1.8.2,extra-relation
v1.8.2,docstr-coverage: inherited
v1.8.2,assumes deterministic entity to id mapping
v1.8.2,docstr-coverage: inherited
v1.8.2,assumes deterministic entity to id mapping
v1.8.2,from left_tf
v1.8.2,from right_tf with offset
v1.8.2,additional
v1.8.2,verify shape
v1.8.2,verify dtype
v1.8.2,verify number of entities/relations
v1.8.2,verify offsets
v1.8.2,"create old, new pairs"
v1.8.2,simulate merging ids
v1.8.2,only a single pair
v1.8.2,apply
v1.8.2,every key is contained
v1.8.2,value range
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Check minimal statistics
v1.8.2,Check either a github link or author/publication information is given
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.8.2,"prepare conv input (N, C, H, W)"
v1.8.2,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.8.2,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.8.2,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.8.2,"f(h, r, t) = h @ r @ t"
v1.8.2,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.8.2,normalize rotations to unit modulus
v1.8.2,check for unit modulus
v1.8.2,entity embeddings
v1.8.2,relation embeddings
v1.8.2,Compute Scores
v1.8.2,entity embeddings
v1.8.2,relation embeddings
v1.8.2,Compute Scores
v1.8.2,Compute Scores
v1.8.2,-\|R_h h - R_t t\|
v1.8.2,-\|h - t\|
v1.8.2,"Since MuRE has offsets, the scores do not need to negative"
v1.8.2,"We do not need this, since we do not check for functional consistency anyway"
v1.8.2,intra-interaction comparison
v1.8.2,dimension needs to be divisible by num_heads
v1.8.2,FIXME
v1.8.2,multiple
v1.8.2,single
v1.8.2,head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,message_dim must be divisible by num_heads
v1.8.2,determine pool using anchor searcher
v1.8.2,determine expected pool using shortest path distances via scipy.sparse.csgraph
v1.8.2,generate random pool
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,complex tensor
v1.8.2,check value range
v1.8.2,check modulus == 1
v1.8.2,quaternion needs dim divisible by 4
v1.8.2,"check value range (actually [-s, +s] with s = 1/sqrt(2*n))"
v1.8.2,value range
v1.8.2,highest degree node has largest value
v1.8.2,Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf
v1.8.2,create triples with a dummy relation type 0
v1.8.2,"0: green: 2, 3, 7, 8"
v1.8.2,"1: red: 1, 4, 6, 9"
v1.8.2,"2: blue: 0, 5"
v1.8.2,the example includes the first power
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"typically, the model takes care of adjusting the dimension size for ""complex"""
v1.8.2,"tensors, but we have to do it manually here for testing purposes"
v1.8.2,hotfix for mixed dtypes
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,TODO this is the only place this function is used.
v1.8.2,Is there an alternative so we can remove it?
v1.8.2,ensure positivity
v1.8.2,compute using pytorch
v1.8.2,prepare distributions
v1.8.2,compute using pykeen
v1.8.2,"e: (batch_size, num_heads, num_tails, d)"
v1.8.2,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.8.2,divergence = 0 => similarity = -divergence = 0
v1.8.2,"(h - t), r"
v1.8.2,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.8.2,divergence >= 0 => similarity = -divergence <= 0
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.8.2,filter vs. no filter thing.
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,check for empty batches
v1.8.2,: The window size used by the early stopper
v1.8.2,: The mock losses the mock evaluator will return
v1.8.2,: The (zeroed) index  - 1 at which stopping will occur
v1.8.2,: The minimum improvement
v1.8.2,: The best results
v1.8.2,Set automatic_memory_optimization to false for tests
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Train a model in one shot
v1.8.2,Train a model for the first half
v1.8.2,Continue training of the first part
v1.8.2,check non-empty metrics
v1.8.2,: Should negative samples be filtered?
v1.8.2,expectation = (1 + n) / 2
v1.8.2,variance = (n**2 - 1) / 12
v1.8.2,"x_i ~ N(mu_i, 1)"
v1.8.2,closed-form solution
v1.8.2,sampled confidence interval
v1.8.2,check that closed-form is in confidence interval of sampled
v1.8.2,positive values only
v1.8.2,positive and negative values
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Check for correct class
v1.8.2,check correct num_entities
v1.8.2,Check for correct class
v1.8.2,check value
v1.8.2,filtering
v1.8.2,"true_score: (2, 3, 3)"
v1.8.2,head based filter
v1.8.2,preprocessing for faster lookup
v1.8.2,check that all found positives are positive
v1.8.2,check in-place
v1.8.2,Test head scores
v1.8.2,Assert in-place modification
v1.8.2,Assert correct filtering
v1.8.2,Test tail scores
v1.8.2,Assert in-place modification
v1.8.2,Assert correct filtering
v1.8.2,The MockModel gives the highest score to the highest entity id
v1.8.2,The test triples are created to yield the third highest score on both head and tail prediction
v1.8.2,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.8.2,These triples are created to yield the highest score on both head and tail prediction for the
v1.8.2,test triple at hand
v1.8.2,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.8.2,test triple at hand
v1.8.2,check true negatives
v1.8.2,TODO: check no repetitions (if possible)
v1.8.2,return type
v1.8.2,columns
v1.8.2,value range
v1.8.2,relation restriction
v1.8.2,with explicit num_entities
v1.8.2,with inferred num_entities
v1.8.2,test different shapes
v1.8.2,test different shapes
v1.8.2,value range
v1.8.2,value range
v1.8.2,check unique
v1.8.2,"strips off the ""k"" at the end"
v1.8.2,Populate with real results.
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"(-1, 1),"
v1.8.2,"(-1, -1),"
v1.8.2,"(-5, -3),"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Check whether filtering works correctly
v1.8.2,First giving an example where all triples have to be filtered
v1.8.2,The filter should remove all triples
v1.8.2,Create an example where no triples will be filtered
v1.8.2,The filter should not remove any triple
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,same relation
v1.8.2,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.8.2,Test that half of the subjects and half of the objects are corrupted
v1.8.2,check that corrupted entities co-occur with the relation in training data
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The batch size
v1.8.2,: The random seed
v1.8.2,: The triples factory
v1.8.2,: The instances
v1.8.2,: A positive batch
v1.8.2,: Kwargs
v1.8.2,Generate negative sample
v1.8.2,check filter shape if necessary
v1.8.2,check shape
v1.8.2,check bounds: heads
v1.8.2,check bounds: relations
v1.8.2,check bounds: tails
v1.8.2,test that the negative triple is not the original positive triple
v1.8.2,"shape: (batch_size, 1, num_neg)"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Base Classes
v1.8.2,Concrete Classes
v1.8.2,Utils
v1.8.2,: synonyms of this loss
v1.8.2,: The default strategy for optimizing the loss's hyper-parameters
v1.8.2,flatten and stack
v1.8.2,apply label smoothing if necessary.
v1.8.2,TODO: Do label smoothing only once
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Sanity check
v1.8.2,"prepare for broadcasting, shape: (batch_size, 1, 3)"
v1.8.2,negative_scores have already been filtered in the sampler!
v1.8.2,"shape: (nnz,)"
v1.8.2,docstr-coverage: inherited
v1.8.2,Sanity check
v1.8.2,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.8.2,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.8.2,positive entries in the labels tensor.
v1.8.2,"This shows how often one row has to be repeated,"
v1.8.2,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.8.2,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.8.2,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.8.2,This tells us how often each true label should be repeated
v1.8.2,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.8.2,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.8.2,within the ablation pipeline.
v1.8.2,1. positive & negative margin
v1.8.2,2. negative margin & offset
v1.8.2,3. positive margin & offset
v1.8.2,docstr-coverage: inherited
v1.8.2,Sanity check
v1.8.2,positive term
v1.8.2,implicitly repeat positive scores
v1.8.2,"shape: (nnz,)"
v1.8.2,negative term
v1.8.2,negative_scores have already been filtered in the sampler!
v1.8.2,docstr-coverage: inherited
v1.8.2,Sanity check
v1.8.2,"scale labels from [0, 1] to [-1, 1]"
v1.8.2,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.8.2,within the ablation pipeline.
v1.8.2,docstr-coverage: inherited
v1.8.2,negative_scores have already been filtered in the sampler!
v1.8.2,(dense) softmax requires unfiltered scores / masking
v1.8.2,we need to fill the scores with -inf for all filtered negative examples
v1.8.2,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.8.2,use filled negatives scores
v1.8.2,docstr-coverage: inherited
v1.8.2,we need dense negative scores => unfilter if necessary
v1.8.2,"we may have inf rows, since there will be one additional finite positive score per row"
v1.8.2,"combine scores: shape: (batch_size, num_negatives + 1)"
v1.8.2,use sparse version of cross entropy
v1.8.2,calculate cross entropy loss
v1.8.2,docstr-coverage: inherited
v1.8.2,make sure labels form a proper probability distribution
v1.8.2,calculate cross entropy loss
v1.8.2,docstr-coverage: inherited
v1.8.2,determine positive; do not check with == since the labels are floats
v1.8.2,subtract margin from positive scores
v1.8.2,divide by temperature
v1.8.2,docstr-coverage: inherited
v1.8.2,subtract margin from positive scores
v1.8.2,normalize positive score shape
v1.8.2,divide by temperature
v1.8.2,docstr-coverage: inherited
v1.8.2,Sanity check
v1.8.2,determine positive; do not check with == since the labels are floats
v1.8.2,compute negative weights (without gradient tracking)
v1.8.2,clone is necessary since we modify in-place
v1.8.2,Split positive and negative scores
v1.8.2,docstr-coverage: inherited
v1.8.2,Sanity check
v1.8.2,"we do not allow full -inf rows, since we compute the softmax over this tensor"
v1.8.2,compute weights (without gradient tracking)
v1.8.2,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.8.2,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.8.2,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.8.2,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.8.2,": For more information, see https://github.com/cthoyt/pystow"
v1.8.2,: A path representing the PyKEEN data folder
v1.8.2,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.8.2,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.8.2,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.8.2,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.8.2,: A subdirectory for PyKEEN logs
v1.8.2,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.8.2,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.8.2,"TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728"
v1.8.2,"SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.8.2,get datatype specific epsilon
v1.8.2,clamp minimum value
v1.8.2,try to resolve ambiguous device; there has to be at least one cuda device
v1.8.2,lower bound
v1.8.2,upper bound
v1.8.2,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.8.2,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.8.2,Determine optimal order and cost
v1.8.2,translate back to original order
v1.8.2,determine optimal processing order
v1.8.2,heuristic
v1.8.2,TODO: check if einsum is still very slow.
v1.8.2,TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation
v1.8.2,unsqueeze
v1.8.2,The dimensions affected by e'
v1.8.2,Project entities
v1.8.2,r_p (e_p.T e) + e'
v1.8.2,Enforce constraints
v1.8.2,TODO delete when deleting _normalize_dim (below)
v1.8.2,TODO delete when deleting convert_to_canonical_shape (below)
v1.8.2,TODO delete? See note in test_sim.py on its only usage
v1.8.2,upgrade to sequence
v1.8.2,broadcast
v1.8.2,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.8.2,Create a tensor of all IDs
v1.8.2,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.8.2,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.8.2,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.8.2,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.8.2,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.8.2,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.8.2,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.8.2,ensure pathlib
v1.8.2,Enforce that sizes are strictly positive by passing through ELU
v1.8.2,Shape vector is normalized using the above helper function
v1.8.2,Size is learned separately and applied to normalized shape
v1.8.2,Compute potential boundaries by applying the shape in substraction
v1.8.2,and in addition
v1.8.2,Compute box upper bounds using min and max respectively
v1.8.2,compute width plus 1
v1.8.2,compute box midpoints
v1.8.2,"TODO: we already had this before, as `base`"
v1.8.2,inside box?
v1.8.2,yes: |p - c| / (w + 1)
v1.8.2,no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))
v1.8.2,Step 1: Apply the other entity bump
v1.8.2,Step 2: Apply tanh if tanh_map is set to True.
v1.8.2,Compute the distance function output element-wise
v1.8.2,"Finally, compute the norm"
v1.8.2,cf. https://stackoverflow.com/a/1176023
v1.8.2,check validity
v1.8.2,path compression
v1.8.2,get representatives
v1.8.2,already merged
v1.8.2,make x the smaller one
v1.8.2,merge
v1.8.2,extract partitions
v1.8.2,resolve path to make sure it is an absolute path
v1.8.2,ensure directory exists
v1.8.2,message passing: collect colors of neighbors
v1.8.2,"dense colors: shape: (n, c)"
v1.8.2,"adj:          shape: (n, n)"
v1.8.2,"values need to be float, since torch.sparse.mm does not support integer dtypes"
v1.8.2,size: will be correctly inferred
v1.8.2,concat with old colors
v1.8.2,hash
v1.8.2,create random indicator functions of low dimensionality
v1.8.2,collect neighbors' colors
v1.8.2,round to avoid numerical effects
v1.8.2,hash first
v1.8.2,concat with old colors
v1.8.2,re-hash
v1.8.2,"only keep connectivity, but remove multiplicity"
v1.8.2,"note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,"
v1.8.2,this is rather useless
v1.8.2,initial: degree
v1.8.2,"note: we calculate this separately, since we can use a more efficient implementation for the first step"
v1.8.2,hash
v1.8.2,determine small integer type for dense count array
v1.8.2,convergence check
v1.8.2,each node has a unique color
v1.8.2,the number of colors did not improve in the last iteration
v1.8.2,cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Base Class
v1.8.2,Child classes
v1.8.2,Utils
v1.8.2,: The overall regularization weight
v1.8.2,: The current regularization term (a scalar)
v1.8.2,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.8.2,: Has this regularizer been updated since last being reset?
v1.8.2,: The default strategy for optimizing the regularizer's hyper-parameters
v1.8.2,"If there are tracked parameters, update based on them"
v1.8.2,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.8.2,docstr-coverage: inherited
v1.8.2,no need to compute anything
v1.8.2,docstr-coverage: inherited
v1.8.2,always return zero
v1.8.2,: The dimension along which to compute the vector-based regularization terms.
v1.8.2,: Whether to normalize the regularization term by the dimension of the vectors.
v1.8.2,: This allows dimensionality-independent weight tuning.
v1.8.2,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.8.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.8.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.8.2,docstr-coverage: inherited
v1.8.2,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.8.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.8.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.8.2,docstr-coverage: inherited
v1.8.2,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.8.2,"could be moved into kwargs, but needs to stay for experiment integrity check"
v1.8.2,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.8.2,"Therefore, apply_only_once is always set to True."
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Entity soft constraint
v1.8.2,Orthogonality soft constraint
v1.8.2,The normalization factor to balance individual regularizers' contribution.
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"""Closed-Form Expectation"","
v1.8.2,"""Closed-Form Variance"","
v1.8.2,"""✓"" if metric.closed_expectation else """","
v1.8.2,"""✓"" if metric.closed_variance else """","
v1.8.2,Add HPO command
v1.8.2,Add NodePiece tokenization command
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,General types
v1.8.2,Triples
v1.8.2,Others
v1.8.2,Tensor Functions
v1.8.2,Tensors
v1.8.2,Dataclasses
v1.8.2,prediction targets
v1.8.2,modes
v1.8.2,entity alignment sides
v1.8.2,: A function that mutates the input and returns a new object of the same type as output
v1.8.2,: A function that can be applied to a tensor to initialize it
v1.8.2,: A function that can be applied to a tensor to normalize it
v1.8.2,: A function that can be applied to a tensor to constrain it
v1.8.2,: A hint for a :class:`torch.device`
v1.8.2,: A hint for a :class:`torch.Generator`
v1.8.2,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.8.2,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.8.2,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.8.2,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.8.2,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.8.2,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.8.2,: the inductive prediction and training mode
v1.8.2,: the prediction target
v1.8.2,: the prediction target index
v1.8.2,: the rank types
v1.8.2,"RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8"
v1.8.2,entity alignment
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,pad with zeros
v1.8.2,trim
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"mask, shape: (num_edges,)"
v1.8.2,bi-directional message passing
v1.8.2,Heuristic for default value
v1.8.2,docstr-coverage: inherited
v1.8.2,other relations
v1.8.2,other relations
v1.8.2,Select source and target indices as well as edge weights for the
v1.8.2,currently considered relation
v1.8.2,skip relations without edges
v1.8.2,"compute message, shape: (num_edges_of_type, output_dim)"
v1.8.2,since we may have one node ID appearing multiple times as source
v1.8.2,"ID, we can save some computation by first reducing to the unique"
v1.8.2,"source IDs, compute transformed representations and afterwards"
v1.8.2,select these representations for the correct edges.
v1.8.2,select unique source node representations
v1.8.2,transform representations by relation specific weight
v1.8.2,select the uniquely transformed representations for each edge
v1.8.2,optional message weighting
v1.8.2,message aggregation
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Xavier Glorot initialization of each block
v1.8.2,docstr-coverage: inherited
v1.8.2,accumulator
v1.8.2,view as blocks
v1.8.2,other relations
v1.8.2,skip relations without edges
v1.8.2,"compute message, shape: (num_edges_of_type, num_blocks, block_size)"
v1.8.2,optional message weighting
v1.8.2,message aggregation
v1.8.2,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501
v1.8.2,there are separate decompositions for forward and backward relations.
v1.8.2,the self-loop weight is not decomposed.
v1.8.2,docstr-coverage: inherited
v1.8.2,self-loop
v1.8.2,forward messages
v1.8.2,backward messages
v1.8.2,activation
v1.8.2,has to be imported now to avoid cyclic imports
v1.8.2,Resolve edge weighting
v1.8.2,dropout
v1.8.2,"Save graph using buffers, such that the tensors are moved together with the model"
v1.8.2,no activation on last layer
v1.8.2,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501
v1.8.2,buffering of enriched representations
v1.8.2,docstr-coverage: inherited
v1.8.2,invalidate enriched embeddings
v1.8.2,docstr-coverage: inherited
v1.8.2,Bind fields
v1.8.2,"shape: (num_entities, embedding_dim)"
v1.8.2,Edge dropout: drop the same edges on all layers (only in training mode)
v1.8.2,Get random dropout mask
v1.8.2,Apply to edges
v1.8.2,fixed edges -> pre-compute weights
v1.8.2,Cache enriched representations
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Utils
v1.8.2,: the maximum ID (exclusively)
v1.8.2,: the shape of an individual representation
v1.8.2,: a normalizer for individual representations
v1.8.2,: a regularizer for individual representations
v1.8.2,: dropout
v1.8.2,normalize *before* repeating
v1.8.2,regularize *after* repeating
v1.8.2,TODO: Remove this property and update code to use shape instead
v1.8.2,has to be imported here to avoid cyclic import
v1.8.2,docstr-coverage: inherited
v1.8.2,normalize num_embeddings vs. max_id
v1.8.2,normalize embedding_dim vs. shape
v1.8.2,work-around until full complex support (torch==1.10 still does not work)
v1.8.2,TODO: verify that this is our understanding of complex!
v1.8.2,"note: this seems to work, as finfo returns the datatype of the underlying floating"
v1.8.2,"point dtype, rather than the combined complex one"
v1.8.2,"use make for initializer since there's a default, and make_safe"
v1.8.2,for the others to pass through None values
v1.8.2,"wrapper around max_id, for backward compatibility"
v1.8.2,docstr-coverage: inherited
v1.8.2,initialize weights in-place
v1.8.2,docstr-coverage: inherited
v1.8.2,apply constraints in-place
v1.8.2,fixme: work-around until nn.Embedding supports complex
v1.8.2,docstr-coverage: inherited
v1.8.2,fixme: work-around until nn.Embedding supports complex
v1.8.2,verify that contiguity is preserved
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,"get all base representations, shape: (num_bases, *shape)"
v1.8.2,"get base weights, shape: (*batch_dims, num_bases)"
v1.8.2,"weighted linear combination of bases, shape: (*batch_dims, *shape)"
v1.8.2,normalize output dimension
v1.8.2,entity-relation composition
v1.8.2,edge weighting
v1.8.2,message passing weights
v1.8.2,linear relation transformation
v1.8.2,layer-specific self-loop relation representation
v1.8.2,other components
v1.8.2,initialize
v1.8.2,split
v1.8.2,compose
v1.8.2,transform
v1.8.2,normalization
v1.8.2,aggregate by sum
v1.8.2,dropout
v1.8.2,prepare for inverse relations
v1.8.2,update entity representations: mean over self-loops / forward edges / backward edges
v1.8.2,Relation transformation
v1.8.2,has to be imported here to avoid cyclic imports
v1.8.2,kwargs
v1.8.2,Buffered enriched entity and relation representations
v1.8.2,TODO: Check
v1.8.2,hidden dimension normalization
v1.8.2,Create message passing layers
v1.8.2,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.8.2,TODO: This always uses all training triples for message passing
v1.8.2,initialize buffer of enriched representations
v1.8.2,docstr-coverage: inherited
v1.8.2,invalidate enriched embeddings
v1.8.2,docstr-coverage: inherited
v1.8.2,"when changing from evaluation to training mode, the buffered representations have been computed without"
v1.8.2,"gradient tracking. hence, we need to invalidate them."
v1.8.2,note: this occurs in practice when continuing training after evaluation.
v1.8.2,enrich
v1.8.2,docstr-coverage: inherited
v1.8.2,infer shape
v1.8.2,"assign after super, since they should be properly registered as submodules"
v1.8.2,docstr-coverage: inherited
v1.8.2,abstract
v1.8.2,concrete classes
v1.8.2,default flow
v1.8.2,: the message passing layers
v1.8.2,: the flow direction of messages across layers
v1.8.2,": the edge index, shape: (2, num_edges)"
v1.8.2,fail if dependencies are missing
v1.8.2,avoid cyclic import
v1.8.2,"the base representations, e.g., entity embeddings or features"
v1.8.2,assign sub-module *after* super call
v1.8.2,initialize layers
v1.8.2,normalize activation
v1.8.2,check consistency
v1.8.2,buffer edge index for message passing
v1.8.2,TODO: inductiveness; we need to
v1.8.2,* replace edge_index
v1.8.2,* replace base representations
v1.8.2,* keep layers & activations
v1.8.2,docstr-coverage: inherited
v1.8.2,we can restrict the message passing to the k-hop neighborhood of the desired indices;
v1.8.2,this does only make sense if we do not request *all* indices
v1.8.2,k_hop_subgraph returns:
v1.8.2,(1) the nodes involved in the subgraph
v1.8.2,(2) the filtered edge_index connectivity
v1.8.2,"(3) the mapping from node indices in node_idx to their new location, and"
v1.8.2,(4) the edge mask indicating which edges were preserved
v1.8.2,we only need the base representations for the neighbor indices
v1.8.2,get *all* base representations
v1.8.2,use *all* edges
v1.8.2,perform message passing
v1.8.2,select desired indices
v1.8.2,docstr-coverage: inherited
v1.8.2,": the edge type, shape: (num_edges,)"
v1.8.2,register an additional buffer for the categorical edge type
v1.8.2,docstr-coverage: inherited
v1.8.2,: the relation representations used to obtain initial edge features
v1.8.2,avoid cyclic import
v1.8.2,docstr-coverage: inherited
v1.8.2,get initial relation representations
v1.8.2,select edge attributes from relation representations according to relation type
v1.8.2,perform message passing
v1.8.2,"apply relation transformation, if necessary"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,backwards compatibility
v1.8.2,scaling factor
v1.8.2,"modulus ~ Uniform[-s, s]"
v1.8.2,"phase ~ Uniform[0, 2*pi]"
v1.8.2,real part
v1.8.2,purely imaginary quaternions unitary
v1.8.2,this is usually loaded from somewhere else
v1.8.2,"the shape must match, as well as the entity-to-id mapping"
v1.8.2,must be cloned if we want to do backprop
v1.8.2,the color initializer
v1.8.2,variants for the edge index
v1.8.2,additional parameters for iter_weisfeiler_lehman
v1.8.2,get coloring
v1.8.2,make color initializer
v1.8.2,initialize color representations
v1.8.2,init entity representations according to the color
v1.8.2,create random walk matrix
v1.8.2,stack diagonal entries of powers of rw
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: whether the edge weighting needs access to the message
v1.8.2,stub init to enable arbitrary arguments in subclasses
v1.8.2,"Calculate in-degree, i.e. number of incoming edges"
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,backward compatibility with RGCN
v1.8.2,docstr-coverage: inherited
v1.8.2,view for heads
v1.8.2,"compute attention coefficients, shape: (num_edges, num_heads)"
v1.8.2,"TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"if the sparsity becomes too low, convert to a dense matrix"
v1.8.2,"note: this heuristic is based on the memory consumption,"
v1.8.2,"for a sparse matrix, we store 3 values per nnz (row index, column index, value)"
v1.8.2,"performance-wise, it likely makes sense to switch even earlier"
v1.8.2,`torch.sparse.mm` can also deal with dense 2nd argument
v1.8.2,note: torch.sparse.mm only works for COO matrices;
v1.8.2,@ only works for CSR matrices
v1.8.2,"convert to COO, if necessary"
v1.8.2,"we need to use indices here, since there may be zero diagonal entries"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,TODO test
v1.8.2,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.8.2,: a = \mu^T\Sigma^{-1}\mu
v1.8.2,: b = \log \det \Sigma
v1.8.2,1. Component
v1.8.2,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.8.2,2. Component
v1.8.2,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.8.2,with mu = (mu_1 - mu_0)
v1.8.2,= mu * Sigma_1^-1 mu
v1.8.2,since Sigma_1 is diagonal
v1.8.2,= mu**2 / sigma_1
v1.8.2,3. Component
v1.8.2,4. Component
v1.8.2,ln (det(\Sigma_1) / det(\Sigma_0))
v1.8.2,= ln det Sigma_1 - ln det Sigma_0
v1.8.2,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.8.2,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.8.2,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.8.2,allocate result
v1.8.2,prepare distributions
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,TODO benchmark
v1.8.2,TODO benchmark
v1.8.2,TODO benchmark
v1.8.2,TODO benchmark
v1.8.2,TODO benchmark
v1.8.2,TODO benchmark
v1.8.2,TODO benchmark
v1.8.2,TODO benchmark
v1.8.2,TODO benchmark
v1.8.2,"h = h_re, -h_im"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Adapter classes
v1.8.2,Concrete Classes
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Base Classes
v1.8.2,Adapter classes
v1.8.2,Concrete Classes
v1.8.2,normalize input
v1.8.2,get number of head/relation/tail representations
v1.8.2,flatten list
v1.8.2,split tensors
v1.8.2,broadcasting
v1.8.2,yield batches
v1.8.2,complex typing
v1.8.2,: The symbolic shapes for entity representations
v1.8.2,": The symbolic shapes for entity representations for tail entities, if different."
v1.8.2,": Otherwise, the entity_shape is used for head & tail entities"
v1.8.2,: The symbolic shapes for relation representations
v1.8.2,if the interaction function's head parameter should only receive a subset of entity representations
v1.8.2,if the interaction function's tail parameter should only receive a subset of entity representations
v1.8.2,"TODO: cannot cover dynamic shapes, e.g., AutoSF"
v1.8.2,"TODO: we could change that to slicing along multiple dimensions, if necessary"
v1.8.2,"The appended ""e"" represents the literals that get concatenated"
v1.8.2,on the entity representations. It does not necessarily have the
v1.8.2,"same dimension ""d"" as the entity representations."
v1.8.2,alternate way of combining entity embeddings + literals
v1.8.2,"h = torch.cat(h, dim=-1)"
v1.8.2,"h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore"
v1.8.2,"t = torch.cat(t, dim=-1)"
v1.8.2,"t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore"
v1.8.2,: The functional interaction form
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Store initial input for error message
v1.8.2,All are None -> try and make closest to square
v1.8.2,Only input channels is None
v1.8.2,Only width is None
v1.8.2,Only height is none
v1.8.2,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.8.2,Width and input channels are None -> set input channels to 1 and calculate width
v1.8.2,vector & scalar offset
v1.8.2,": The head-relation encoder operating on 2D ""images"""
v1.8.2,: The head-relation encoder operating on the 1D flattened version
v1.8.2,: The interaction function
v1.8.2,Automatic calculation of remaining dimensions
v1.8.2,Parameter need to fulfil:
v1.8.2,input_channels * embedding_height * embedding_width = embedding_dim
v1.8.2,normalize kernel height
v1.8.2,encoders
v1.8.2,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.8.2,"2: 1D encoder: FC, DO, BN?, Act"
v1.8.2,store reshaping dimensions
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,The interaction model
v1.8.2,docstr-coverage: inherited
v1.8.2,Use Xavier initialization for weight; bias to zero
v1.8.2,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.8.2,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.8.2,docstr-coverage: inherited
v1.8.2,normalize hidden_dim
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Initialize biases with zero
v1.8.2,"In the original formulation,"
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Global entity projection
v1.8.2,Global relation projection
v1.8.2,Global combination bias
v1.8.2,Global combination bias
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,default core tensor initialization
v1.8.2,cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12
v1.8.2,normalize initializer
v1.8.2,normalize relation dimension
v1.8.2,Core tensor
v1.8.2,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.8.2,Dropout
v1.8.2,docstr-coverage: inherited
v1.8.2,instantiate here to make module easily serializable
v1.8.2,"batch norm gets reset automatically, since it defines reset_parameters"
v1.8.2,shapes
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,there are separate biases for entities in head and tail position
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,the base interaction
v1.8.2,forward entity/relation shapes
v1.8.2,The parameters of the affine transformation: bias
v1.8.2,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,head position and bump
v1.8.2,relation box: head
v1.8.2,relation box: tail
v1.8.2,tail position and bump
v1.8.2,docstr-coverage: inherited
v1.8.2,input normalization
v1.8.2,Core tensor
v1.8.2,docstr-coverage: inherited
v1.8.2,initialize core tensor
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,"r_head, r_mid, r_tail"
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,docstr-coverage: excused `wrapped`
v1.8.2,TODO: switch to einsum ?
v1.8.2,"return torch.real(torch.einsum(""...d, ...d, ...d -> ..."", h, r, torch.conj(t)))"
v1.8.2,"repeat if necessary, and concat head and relation"
v1.8.2,"shape: -1, num_input_channels, 2*height, width"
v1.8.2,"shape: -1, num_input_channels, 2*height, width"
v1.8.2,"-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.8.2,"reshape: (-1, dim) -> (*batch_dims, dim)"
v1.8.2,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.8.2,output_shape: batch_dims
v1.8.2,add bias term
v1.8.2,decompose convolution for faster computation in 1-n case
v1.8.2,"compute conv(stack(h, r, t))"
v1.8.2,prepare input shapes for broadcasting
v1.8.2,"(*batch_dims, 1, d)"
v1.8.2,"conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])"
v1.8.2,"here, kernel_size = (1, 3), C_in = 1, C_out = num_filters"
v1.8.2,"-> conv_head, conv_rel, conv_tail shapes: (num_filters,)"
v1.8.2,"reshape to (..., f, 1)"
v1.8.2,"convolve -> output.shape: (*, embedding_dim, num_filters)"
v1.8.2,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.8.2,"Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)"
v1.8.2,same shape
v1.8.2,"split, shape: (embedding_dim, hidden_dim)"
v1.8.2,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.8.2,"Predict t embedding, shape: (*batch_dims, d)"
v1.8.2,dot product
v1.8.2,"composite: (*batch_dims, d)"
v1.8.2,inner product with relation embedding
v1.8.2,Circular correlation of entity embeddings
v1.8.2,complex conjugate
v1.8.2,Hadamard product in frequency domain
v1.8.2,inverse real FFT
v1.8.2,global projections
v1.8.2,"combination, shape: (*batch_dims, d)"
v1.8.2,dot product with t
v1.8.2,r expresses a rotation in complex plane.
v1.8.2,rotate head by relation (=Hadamard product in complex space)
v1.8.2,rotate tail by inverse of relation
v1.8.2,The inverse rotation is expressed by the complex conjugate of r.
v1.8.2,The score is computed as the distance of the relation-rotated head to the tail.
v1.8.2,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.8.2,|h * r - t| = |h - conj(r) * t|
v1.8.2,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.8.2,"That is not mentioned in the paper, so it is made optional here."
v1.8.2,Project entities
v1.8.2,h projection to hyperplane
v1.8.2,r
v1.8.2,-t projection to hyperplane
v1.8.2,project to relation specific subspace
v1.8.2,ensure constraints
v1.8.2,x_1 contraction
v1.8.2,x_2 contraction
v1.8.2,Rotate (=Hamilton product in quaternion space).
v1.8.2,Rotation in quaternion space
v1.8.2,head interaction
v1.8.2,relation interaction (notice that h has been updated)
v1.8.2,combination
v1.8.2,similarity
v1.8.2,head
v1.8.2,relation box: head
v1.8.2,relation box: tail
v1.8.2,tail
v1.8.2,power norm
v1.8.2,the relation-specific head box base shape (normalized to have a volume of 1):
v1.8.2,the relation-specific tail box base shape (normalized to have a volume of 1):
v1.8.2,head
v1.8.2,relation
v1.8.2,tail
v1.8.2,version 2: relation factor offset
v1.8.2,extension: negative (power) norm
v1.8.2,note: normalization should be done from the representations
v1.8.2,cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501
v1.8.2,version 2
v1.8.2,r_head = r_head + u * torch.ones_like(r_head)
v1.8.2,r_tail = r_tail + u * torch.ones_like(r_tail)
v1.8.2,"stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)"
v1.8.2,"remember shape for output, but reshape for transformer"
v1.8.2,"get position embeddings, shape: (seq_len, dim)"
v1.8.2,Now we are position-dependent w.r.t qualifier pairs.
v1.8.2,"seq_length, batch_size, dim"
v1.8.2,Pool output
v1.8.2,"output shape: (batch_size, dim)"
v1.8.2,reshape
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Concrete classes
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: the token ID of the padding token
v1.8.2,: the token representations
v1.8.2,: the assigned tokens for each entity
v1.8.2,needs to be lazily imported to avoid cyclic imports
v1.8.2,fill padding (nn.Embedding cannot deal with negative indices)
v1.8.2,"sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs"
v1.8.2,"contain a subset of training relations) - for that, the padding index is the last index of the Representation"
v1.8.2,resolve token representation
v1.8.2,input validation
v1.8.2,register as buffer
v1.8.2,assign sub-module
v1.8.2,apply tokenizer
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,"get token IDs, shape: (*, num_chosen_tokens)"
v1.8.2,"lookup token representations, shape: (*, num_chosen_tokens, *shape)"
v1.8.2,": A list with ratios per representation in their creation order,"
v1.8.2,": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`"
v1.8.2,": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95"
v1.8.2,: the token representations
v1.8.2,normalize triples
v1.8.2,inverse triples are created afterwards implicitly
v1.8.2,tokenize
v1.8.2,determine shape
v1.8.2,super init; has to happen *before* any parameter or buffer is assigned
v1.8.2,assign module
v1.8.2,Assign default aggregation
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,unique hashes per representation
v1.8.2,unique hashes if we concatenate all representations together
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Resolver
v1.8.2,Base classes
v1.8.2,Concrete classes
v1.8.2,TODO: allow relative
v1.8.2,isin() preserves the sorted order
v1.8.2,docstr-coverage: inherited
v1.8.2,sort by decreasing degree
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,sort by decreasing page rank
v1.8.2,docstr-coverage: inherited
v1.8.2,input normalization
v1.8.2,determine absolute number of anchors for each strategy
v1.8.2,if pre-instantiated
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,input normalization
v1.8.2,power iteration
v1.8.2,"convert to sparse matrix, shape: (n, n)"
v1.8.2,symmetrize
v1.8.2,TODO: should we add self-links
v1.8.2,"adj = adj + scipy.sparse.eye(m=adj.shape[0], format=""coo"")"
v1.8.2,convert to CSR
v1.8.2,adjacency normalization
v1.8.2,TODO: vectorization?
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Resolver
v1.8.2,Base classes
v1.8.2,Concrete classes
v1.8.2,docstr-coverage: inherited
v1.8.2,tokenize: represent entities by bag of relations
v1.8.2,collect candidates
v1.8.2,randomly sample without replacement num_tokens relations for each entity
v1.8.2,docstr-coverage: inherited
v1.8.2,select anchors
v1.8.2,find closest anchors
v1.8.2,convert to torch
v1.8.2,verify pool
v1.8.2,docstr-coverage: inherited
v1.8.2,choose first num_tokens
v1.8.2,TODO: vectorization?
v1.8.2,heuristic
v1.8.2,heuristic
v1.8.2,calculate configuration digest
v1.8.2,create anchor selection instance
v1.8.2,select anchors
v1.8.2,anchor search (=anchor assignment?)
v1.8.2,assign anchors
v1.8.2,save
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Resolver
v1.8.2,Base classes
v1.8.2,Concrete classes
v1.8.2,docstr-coverage: inherited
v1.8.2,"contains: anchor_ids, entity_ids, mapping {entity_id -> {""ancs"": anchors, ""dists"": distances}}"
v1.8.2,normalize anchor_ids
v1.8.2,cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541
v1.8.2,TODO: keep distances?
v1.8.2,ensure parent directory exists
v1.8.2,save via torch.save
v1.8.2,docstr-coverage: inherited
v1.8.2,"TODO: since we save a contiguous array of (num_entities, num_anchors),"
v1.8.2,"it would be more efficient to not convert to a mapping, but directly select from the tensor"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Anchor Searchers
v1.8.2,Anchor Selection
v1.8.2,Tokenizers
v1.8.2,Token Loaders
v1.8.2,Representations
v1.8.2,Data containers
v1.8.2,"TODO: use graph library, such as igraph, graph-tool, or networkit"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Resolver
v1.8.2,Base classes
v1.8.2,Concrete classes
v1.8.2,docstr-coverage: inherited
v1.8.2,convert to adjacency matrix
v1.8.2,"compute distances between anchors and all nodes, shape: (num_anchors, num_entities)"
v1.8.2,select anchor IDs with smallest distance
v1.8.2,docstr-coverage: inherited
v1.8.2,infer shape
v1.8.2,create adjacency matrix
v1.8.2,symmetric + self-loops
v1.8.2,"for each entity, determine anchor pool by BFS"
v1.8.2,an array storing whether node i is reachable by anchor j
v1.8.2,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.8.2,the output
v1.8.2,TODO: take all (q-1) hop neighbors before selecting from q-hop
v1.8.2,propagate one hop
v1.8.2,convergence check
v1.8.2,copy pool if we have seen enough anchors and have not yet stopped
v1.8.2,stop once we have enough
v1.8.2,TODO: can we replace this loop with something vectorized?
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,"select k anchors with largest ppr, shape: (batch_size, k)"
v1.8.2,prepare adjacency matrix only once
v1.8.2,prepare result
v1.8.2,progress bar?
v1.8.2,batch-wise computation of PPR
v1.8.2,"create a batch of starting vectors, shape: (n, batch_size)"
v1.8.2,"run page-rank calculation, shape: (batch_size, n)"
v1.8.2,"select PPR values for the anchors, shape: (num_anchors, batch_size)"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Base classes
v1.8.2,Concrete classes
v1.8.2,
v1.8.2,
v1.8.2,
v1.8.2,
v1.8.2,
v1.8.2,Misc
v1.8.2,
v1.8.2,rank based metrics do not need binarized scores
v1.8.2,: the supported rank types. Most of the time equal to all rank types
v1.8.2,: whether the metric requires the number of candidates for each ranking task
v1.8.2,normalize confidence level
v1.8.2,sample metric values
v1.8.2,"bootstrap estimator (i.e., compute on sample with replacement)"
v1.8.2,cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function
v1.8.2,: The rank-based metric class that this derived metric extends
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,"since scale and offset are constant for a given number of candidates, we have"
v1.8.2,E[scale * M + offset] = scale * E[M] + offset
v1.8.2,docstr-coverage: inherited
v1.8.2,"since scale and offset are constant for a given number of candidates, we have"
v1.8.2,V[scale * M + offset] = scale^2 * V[M]
v1.8.2,: Z-adjusted metrics are formulated to be increasing
v1.8.2,: Z-adjusted metrics can only be applied to realistic ranks
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,should be exactly 0.0
v1.8.2,docstr-coverage: inherited
v1.8.2,should be exactly 1.0
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,: Expectation/maximum reindexed metrics are formulated to be increasing
v1.8.2,: Expectation/maximum reindexed metrics can only be applied to realistic ranks
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,should be exactly 0.0
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2
v1.8.2,use V[x] = E[x^2] - E[x]^2
v1.8.2,group by same weight -> compute H_w(n) for multiple n at once
v1.8.2,we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once
v1.8.2,now select from precomputed cumulative sums and aggregate
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,"ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective"
v1.8.2,cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,TODO: should we return the sum of weights?
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.8.2,docstr-coverage: inherited
v1.8.2,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: the lower bound
v1.8.2,: whether the lower bound is inclusive
v1.8.2,: the upper bound
v1.8.2,: whether the upper bound is inclusive
v1.8.2,: The name of the metric
v1.8.2,: a link to further information
v1.8.2,: whether the metric needs binarized scores
v1.8.2,": whether it is increasing, i.e., larger values are better"
v1.8.2,: the value range
v1.8.2,: synonyms for this metric
v1.8.2,: whether the metric supports weights
v1.8.2,: whether there is a closed-form solution of the expectation
v1.8.2,: whether there is a closed-form solution of the variance
v1.8.2,normalize weights
v1.8.2,calculate weighted harmonic mean
v1.8.2,calculate cdf
v1.8.2,determine value at p=0.5
v1.8.2,special case for exactly 0.5
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: A description of the metric
v1.8.2,: The function that runs the metric
v1.8.2,docstr-coverage: inherited
v1.8.2,: Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics
v1.8.2,: This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`
v1.8.2,"TODO there's something wrong with this, so add it later"
v1.8.2,classifier_annotator.higher(
v1.8.2,"rmc.pr_auc_score,"
v1.8.2,"name=""AUC-PR"","
v1.8.2,"description=""Area Under the Precision-Recall Curve"","
v1.8.2,"link=""https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score"","
v1.8.2,)
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,don't worry about functions because they can't be specified by JSON.
v1.8.2,Could make a better mo
v1.8.2,later could extend for other non-JSON valid types
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Score with original triples
v1.8.2,Score with inverse triples
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Create directory in which all experimental artifacts are saved
v1.8.2,clip for node piece configurations
v1.8.2,"""pykeen experiments reproduce"" expects ""model reference dataset"""
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,distribute the deteriorated triples across the remaining factories
v1.8.2,"'kinships',"
v1.8.2,"'umls',"
v1.8.2,"'codexsmall',"
v1.8.2,"'wn18',"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: Functions for specifying exotic resources with a given prefix
v1.8.2,: Functions for specifying exotic resources based on their file extension
v1.8.2,Input validation
v1.8.2,convert to numpy
v1.8.2,Additional columns
v1.8.2,convert PyTorch tensors to numpy
v1.8.2,convert to dataframe
v1.8.2,Re-order columns
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.8.2,TODO vectorize code
v1.8.2,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,save literal-to-id mapping
v1.8.2,save numeric literals
v1.8.2,load literal-to-id
v1.8.2,load literals
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Split triples
v1.8.2,Sorting ensures consistent results when the triples are permuted
v1.8.2,Create mapping
v1.8.2,Sorting ensures consistent results when the triples are permuted
v1.8.2,Create mapping
v1.8.2,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.8.2,Filter all non-existent triples
v1.8.2,Note: Unique changes the order of the triples
v1.8.2,Note: Using unique means implicit balancing of training samples
v1.8.2,normalize input
v1.8.2,TODO: method is_inverse?
v1.8.2,TODO: inverse of inverse?
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,The number of relations stored in the triples factory includes the number of inverse relations
v1.8.2,Id of inverse relation: relation + 1
v1.8.2,: The mapping from labels to IDs.
v1.8.2,: The inverse mapping for label_to_id; initialized automatically
v1.8.2,: A vectorized version of entity_label_to_id; initialized automatically
v1.8.2,: A vectorized version of entity_id_to_label; initialized automatically
v1.8.2,Normalize input
v1.8.2,label
v1.8.2,Filter for entities
v1.8.2,Filter for relations
v1.8.2,No filter
v1.8.2,: the number of unique entities
v1.8.2,": the number of relations (maybe including ""artificial"" inverse relations)"
v1.8.2,: whether to create inverse triples
v1.8.2,": the number of real relations, i.e., without artificial inverses"
v1.8.2,check new label to ID mappings
v1.8.2,Make new triples factories for each group
v1.8.2,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.2,prepare metadata
v1.8.2,Delegate to function
v1.8.2,"restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed"
v1.8.2,docstr-coverage: inherited
v1.8.2,load base
v1.8.2,load numeric triples
v1.8.2,store numeric triples
v1.8.2,store metadata
v1.8.2,Check if the triples are inverted already
v1.8.2,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.8.2,contained if and only if create_inverse_triples is True.
v1.8.2,Generate entity mapping if necessary
v1.8.2,Generate relation mapping if necessary
v1.8.2,Map triples of labels to triples of IDs.
v1.8.2,TODO: Check if lazy evaluation would make sense
v1.8.2,docstr-coverage: inherited
v1.8.2,store entity/relation to ID
v1.8.2,load entity/relation to ID
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,pre-filter to keep only topk
v1.8.2,if top is larger than the number of available options
v1.8.2,generate text
v1.8.2,docstr-coverage: inherited
v1.8.2,vectorized label lookup
v1.8.2,Re-order columns
v1.8.2,docstr-coverage: inherited
v1.8.2,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.8.2,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Split indices
v1.8.2,Split triples
v1.8.2,select one triple per relation
v1.8.2,maintain set of covered entities
v1.8.2,"Select one triple for each head/tail entity, which is not yet covered."
v1.8.2,create mask
v1.8.2,Prepare split index
v1.8.2,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.8.2,base cases
v1.8.2,IDs not in training
v1.8.2,triples with exclusive test IDs
v1.8.2,docstr-coverage: inherited
v1.8.2,While there are still triples that should be moved to the training set
v1.8.2,Pick a random triple to move over to the training triples
v1.8.2,add to training
v1.8.2,remove from testing
v1.8.2,Recalculate the move_id_mask
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Make sure that the first element has all the right stuff in it
v1.8.2,docstr-coverage: inherited
v1.8.2,backwards compatibility
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,constants
v1.8.2,constants
v1.8.2,unary
v1.8.2,binary
v1.8.2,ternary
v1.8.2,column names
v1.8.2,return candidates
v1.8.2,index triples
v1.8.2,incoming relations per entity
v1.8.2,outgoing relations per entity
v1.8.2,indexing triples for fast join r1 & r2
v1.8.2,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.8.2,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.8.2,actual evaluation of the pattern
v1.8.2,skip empty support
v1.8.2,TODO: Can this happen after pre-filtering?
v1.8.2,"sort first, for triple order invariance"
v1.8.2,TODO: what is the support?
v1.8.2,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.8.2,sort decreasingly. i dominates j for all j > i in x-dimension
v1.8.2,"if it is also dominated by any y, it is not part of the skyline"
v1.8.2,"group by (relation id, pattern type)"
v1.8.2,"for each group, yield from skyline"
v1.8.2,determine patterns from triples
v1.8.2,drop zero-confidence
v1.8.2,keep only skyline
v1.8.2,create data frame
v1.8.2,iterate relation types
v1.8.2,drop zero-confidence
v1.8.2,keep only skyline
v1.8.2,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.8.2,base = skyline(base)
v1.8.2,create data frame
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,TODO: the same
v1.8.2,": the positive triples, shape: (batch_size, 3)"
v1.8.2,": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)"
v1.8.2,": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)"
v1.8.2,TODO: some negative samplers require batches
v1.8.2,"shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.8.2,"each shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset
v1.8.2,docstr-coverage: inherited
v1.8.2,indexing
v1.8.2,initialize
v1.8.2,sample iteratively
v1.8.2,determine weights
v1.8.2,randomly choose a vertex which has not been chosen yet
v1.8.2,normalize to probabilities
v1.8.2,sample a start node
v1.8.2,get list of neighbors
v1.8.2,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.8.2,visit target node
v1.8.2,decrease sample counts
v1.8.2,docstr-coverage: inherited
v1.8.2,convert to csr for fast row slicing
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,safe division for empty sets
v1.8.2,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.8.2,duplicates
v1.8.2,we are not interested in self-similarity
v1.8.2,compute similarities
v1.8.2,Calculate which relations are the inverse ones
v1.8.2,get existing IDs
v1.8.2,remove non-existing ID from label mapping
v1.8.2,create translation tensor
v1.8.2,get entities and relations occurring in triples
v1.8.2,generate ID translation and new label to Id mappings
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.8.2,seamless loading and saving of training checkpoints
v1.8.2,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.8.2,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.8.2,"If a checkpoint file is given, it must be loaded if it exists already"
v1.8.2,"If the stopper dict has any keys, those are written back to the stopper"
v1.8.2,The checkpoint frequency needs to be set to save checkpoints
v1.8.2,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.8.2,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.8.2,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.8.2,send model to device before going into the internal training loop
v1.8.2,Ensure the release of memory
v1.8.2,Clear optimizer
v1.8.2,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.8.2,due to the patience continue to train after the best epoch and thus alter the model
v1.8.2,Create a path
v1.8.2,Prepare all of the callbacks
v1.8.2,"Register a callback for the result tracker, if given"
v1.8.2,"Register a callback for the early stopper, if given"
v1.8.2,TODO should mode be passed here?
v1.8.2,"Take the biggest possible training batch_size, if batch_size not set"
v1.8.2,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.8.2,This will find necessary parameters to optimize the use of the hardware at hand
v1.8.2,return the relevant parameters slice_size and batch_size
v1.8.2,Force weight initialization if training continuation is not explicitly requested.
v1.8.2,Reset the weights
v1.8.2,"afterwards, some parameters may be on the wrong device"
v1.8.2,Create new optimizer
v1.8.2,Create a new lr scheduler and add the optimizer
v1.8.2,Ensure the model is on the correct device
v1.8.2,"When size probing, we don't want progress bars"
v1.8.2,Create progress bar
v1.8.2,Save the time to track when the saved point was available
v1.8.2,Training Loop
v1.8.2,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.8.2,Enforce training mode
v1.8.2,Accumulate loss over epoch
v1.8.2,Batching
v1.8.2,Only create a progress bar when not in size probing mode
v1.8.2,Flag to check when to quit the size probing
v1.8.2,Recall that torch *accumulates* gradients. Before passing in a
v1.8.2,"new instance, you need to zero out the gradients from the old instance"
v1.8.2,Get batch size of current batch (last batch may be incomplete)
v1.8.2,accumulate gradients for whole batch
v1.8.2,forward pass call
v1.8.2,"when called by batch_size_search(), the parameter update should not be applied."
v1.8.2,update parameters according to optimizer
v1.8.2,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.8.2,constraints are no longer applied
v1.8.2,For testing purposes we're only interested in processing one batch
v1.8.2,When size probing we don't need the losses
v1.8.2,Update learning rate scheduler
v1.8.2,Track epoch loss
v1.8.2,Print loss information to console
v1.8.2,Save the last successful finished epoch
v1.8.2,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.8.2,During automatic memory optimization only the error message is of interest
v1.8.2,When there wasn't a best epoch the checkpoint path should be None
v1.8.2,Delete temporary best epoch model
v1.8.2,Includes a call to result_tracker.log_metrics
v1.8.2,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.8.2,MyPy overrides are because you should
v1.8.2,When there wasn't a best epoch the checkpoint path should be None
v1.8.2,Delete temporary best epoch model
v1.8.2,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.8.2,at that state
v1.8.2,Delete temporary best epoch model
v1.8.2,forward pass
v1.8.2,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.8.2,correction for loss reduction
v1.8.2,backward pass
v1.8.2,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.8.2,Set upper bound
v1.8.2,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.8.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.2,"Only if a cuda device is available, the random state is accessed"
v1.8.2,This is an entire checkpoint for the optional best model when using early stopping
v1.8.2,Saving triples factory related states
v1.8.2,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.8.2,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.8.2,Check whether the triples factory mappings match those from the checkpoints
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Shuffle each epoch
v1.8.2,Lazy-splitting into batches
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,docstr-coverage: inherited
v1.8.2,disable automatic batching
v1.8.2,docstr-coverage: inherited
v1.8.2,Slicing is not possible in sLCWA training loops
v1.8.2,split batch
v1.8.2,send to device
v1.8.2,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.8.2,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.8.2,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.8.2,Compute negative and positive scores
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Slicing is not possible for sLCWA
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,TODO how to pass inductive mode
v1.8.2,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.8.2,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.8.2,: A hint for constructing a :class:`MultiTrainingCallback`
v1.8.2,: A collection of callbacks
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,normalize target column
v1.8.2,The type inference is so confusing between the function switching
v1.8.2,and polymorphism introduced by slicability that these need to be ignored
v1.8.2,Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number
v1.8.2,of total entities from another inductive inference factory
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Split batch components
v1.8.2,Send batch to device
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.8.2,"must have failed to start slice_size search, we start with trying half the entities."
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,To make MyPy happy
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,now: smaller is better
v1.8.2,: the number of reported results with no improvement after which training will be stopped
v1.8.2,the minimum relative improvement necessary to consider it an improved result
v1.8.2,"whether a larger value is better, or a smaller."
v1.8.2,: The epoch at which the best result occurred
v1.8.2,: The best result so far
v1.8.2,: The remaining patience
v1.8.2,check for improvement
v1.8.2,stop if the result did not improve more than delta for patience evaluations
v1.8.2,: The model
v1.8.2,: The evaluator
v1.8.2,: The triples to use for training (to be used during filtered evaluation)
v1.8.2,: The triples to use for evaluation
v1.8.2,: Size of the evaluation batches
v1.8.2,: Slice size of the evaluation batches
v1.8.2,: The number of epochs after which the model is evaluated on validation set
v1.8.2,: The number of iterations (one iteration can correspond to various epochs)
v1.8.2,: with no improvement after which training will be stopped.
v1.8.2,: The name of the metric to use
v1.8.2,: The minimum relative improvement necessary to consider it an improved result
v1.8.2,: The metric results from all evaluations
v1.8.2,": Whether a larger value is better, or a smaller"
v1.8.2,: The result tracker
v1.8.2,: Callbacks when after results are calculated
v1.8.2,: Callbacks when training gets continued
v1.8.2,: Callbacks when training is stopped early
v1.8.2,: Did the stopper ever decide to stop?
v1.8.2,TODO: Fix this
v1.8.2,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.8.2,raise ValueError(f'Invalid metric name: {self.metric}')
v1.8.2,Evaluate
v1.8.2,Only perform time consuming checks for the first call.
v1.8.2,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.8.2,Append to history
v1.8.2,TODO need a test that this all re-instantiates properly
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Utils
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,dataset
v1.8.2,model
v1.8.2,stored outside of the training loop / optimizer to give access to auto-tuning from Lightning
v1.8.2,optimizer
v1.8.2,"TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA"
v1.8.2,docstr-coverage: inherited
v1.8.2,call post_parameter_update
v1.8.2,docstr-coverage: inherited
v1.8.2,TODO: sub-batching / slicing
v1.8.2,docstr-coverage: inherited
v1.8.2,TODO:
v1.8.2,"shuffle=shuffle,"
v1.8.2,"drop_last=drop_last,"
v1.8.2,"sampler=sampler,"
v1.8.2,"shuffle=shuffle,"
v1.8.2,disable automatic batching in data loader
v1.8.2,docstr-coverage: inherited
v1.8.2,TODO: sub-batching / slicing
v1.8.2,docstr-coverage: inherited
v1.8.2,"note: since this file is executed via __main__, its module name is replaced by __name__"
v1.8.2,"hence, the two classes' fully qualified names start with ""_"" and are considered private"
v1.8.2,cf. https://github.com/cthoyt/class-resolver/issues/39
v1.8.2,automatically choose accelerator
v1.8.2,defaults to TensorBoard; explicitly disabled here
v1.8.2,disable checkpointing
v1.8.2,mixed precision training
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,parsing metrics
v1.8.2,metric pattern = side?.type?.metric.k?
v1.8.2,: The metric key
v1.8.2,": Side of the metric, or ""both"""
v1.8.2,: The rank type
v1.8.2,normalize metric name
v1.8.2,normalize side
v1.8.2,normalize rank type
v1.8.2,normalize keys
v1.8.2,TODO: this can only normalize rank-based metrics!
v1.8.2,TODO: find a better way to handle this
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,TODO: fix this upstream / make metric.score comply to signature
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Transfer to cpu and convert to numpy
v1.8.2,Ensure that each key gets counted only once
v1.8.2,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.8.2,docstr-coverage: inherited
v1.8.2,"Because the order of the values of an dictionary is not guaranteed,"
v1.8.2,we need to retrieve scores and masks using the exact same key order.
v1.8.2,Clear buffers
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The optimistic rank is the rank when assuming all options with an equal score are placed
v1.8.2,: behind the current test triple.
v1.8.2,": shape: (batch_size,)"
v1.8.2,": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank"
v1.8.2,: over all permutations of the elements with the same score as the currently considered option.
v1.8.2,": shape: (batch_size,)"
v1.8.2,: The pessimistic rank is the rank when assuming all options with an equal score are placed
v1.8.2,: in front of current test triple.
v1.8.2,": shape: (batch_size,)"
v1.8.2,: The number of options is the number of items considered in the ranking. It may change for
v1.8.2,: filtered evaluation
v1.8.2,": shape: (batch_size,)"
v1.8.2,The optimistic rank is the rank when assuming all options with an
v1.8.2,"equal score are placed behind the currently considered. Hence, the"
v1.8.2,"rank is the number of options with better scores, plus one, as the"
v1.8.2,rank is one-based.
v1.8.2,The pessimistic rank is the rank when assuming all options with an
v1.8.2,"equal score are placed in front of the currently considered. Hence,"
v1.8.2,the rank is the number of options which have at least the same score
v1.8.2,minus one (as the currently considered option in included in all
v1.8.2,"options). As the rank is one-based, we have to add 1, which nullifies"
v1.8.2,"the ""minus 1"" from before."
v1.8.2,The realistic rank is the average of the optimistic and pessimistic
v1.8.2,"rank, and hence the expected rank over all permutations of the elements"
v1.8.2,with the same score as the currently considered option.
v1.8.2,"We set values which should be ignored to NaN, hence the number of options"
v1.8.2,which should be considered is given by
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"TODO remove this, it makes code much harder to reason about"
v1.8.2,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.8.2,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.8.2,Clear the ranks from the current evaluator
v1.8.2,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.8.2,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.8.2,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.8.2,"must have failed to start slice_size search, we start with trying half the entities."
v1.8.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.2,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.8.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.2,values_dict[key] will always be an int at this point
v1.8.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.2,Test if slicing is implemented for the required functions of this model
v1.8.2,Split batch
v1.8.2,Bind shape
v1.8.2,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.8.2,Warn if all entities will be filtered
v1.8.2,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.8.2,TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place
v1.8.2,verify that the triples have been filtered
v1.8.2,Filter triples if necessary
v1.8.2,Send to device
v1.8.2,Ensure evaluation mode
v1.8.2,Prepare for result filtering
v1.8.2,Send tensors to device
v1.8.2,Prepare batches
v1.8.2,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.8.2,Show progressbar
v1.8.2,Flag to check when to quit the size probing
v1.8.2,Disable gradient tracking
v1.8.2,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.8.2,batch-wise processing
v1.8.2,If we only probe sizes we do not need more than one batch
v1.8.2,Finalize
v1.8.2,Create filter
v1.8.2,Select scores of true
v1.8.2,overwrite filtered scores
v1.8.2,The scores for the true triples have to be rewritten to the scores tensor
v1.8.2,the rank-based evaluators needs the true scores with trailing 1-dim
v1.8.2,Create a positive mask with the size of the scores from the positive filter
v1.8.2,Restrict to entities of interest
v1.8.2,process scores
v1.8.2,optionally restrict triples (nop if no restriction)
v1.8.2,evaluation triples as dataframe
v1.8.2,determine filter triples
v1.8.2,infer num_entities if not given
v1.8.2,"TODO: unique, or max ID + 1?"
v1.8.2,optionally restrict triples
v1.8.2,compute candidate set sizes for different targets
v1.8.2,TODO: extend to relations?
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,terminate early if there are no ranks
v1.8.2,flatten dictionaries
v1.8.2,individual side
v1.8.2,combined
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Clear buffers
v1.8.2,repeat
v1.8.2,default for inductive LP by [teru2020]
v1.8.2,verify input
v1.8.2,docstr-coverage: inherited
v1.8.2,TODO: do not require to compute all scores beforehand
v1.8.2,super.evaluation assumes that the true scores are part of all_scores
v1.8.2,write back correct num_entities
v1.8.2,TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?
v1.8.2,compute macro weights
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Clear buffers
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.8.2,to create the class
v1.8.2,TODO: update to hint + kwargs
v1.8.2,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default regularizer class
v1.8.2,: The default parameters for the default regularizer class
v1.8.2,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.8.2,Make sure that all modules with parameters do have a reset_parameters method.
v1.8.2,Recursively visit all sub-modules
v1.8.2,skip self
v1.8.2,Track parents for blaming
v1.8.2,call reset_parameters if possible
v1.8.2,initialize from bottom to top
v1.8.2,This ensures that specialized initializations will take priority over the default ones of its components.
v1.8.2,emit warning if there where parameters which were not initialised by reset_parameters.
v1.8.2,Additional debug information
v1.8.2,docstr-coverage: inherited
v1.8.2,TODO: allow max_id being present in representation_kwargs; if it matches max_id
v1.8.2,TODO: we could infer some shapes from the given interaction shape information
v1.8.2,check max-id
v1.8.2,check shapes
v1.8.2,: The entity representations
v1.8.2,: The relation representations
v1.8.2,: The weight regularizers
v1.8.2,: The interaction function
v1.8.2,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.8.2,"model.modules(). Thereby, we can collect them automatically."
v1.8.2,Explicitly call reset_parameters to trigger initialization
v1.8.2,normalize input
v1.8.2,Note: slicing cannot be used here: the indices for score_hrt only have a batch
v1.8.2,"dimension, and slicing along this dimension is already considered by sub-batching."
v1.8.2,Note: we do not delegate to the general method for performance reasons
v1.8.2,Note: repetition is not necessary here
v1.8.2,normalization
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,train model
v1.8.2,"note: as this is an example, the model is only trained for a few epochs,"
v1.8.2,"but not until convergence. In practice, you would usually first verify that"
v1.8.2,"the model is sufficiently good in prediction, before looking at uncertainty scores"
v1.8.2,predict triple scores with uncertainty
v1.8.2,"use a larger number of samples, to increase quality of uncertainty estimate"
v1.8.2,get most and least uncertain prediction on training set
v1.8.2,: The scores
v1.8.2,": The uncertainty, in the same shape as scores"
v1.8.2,Enforce evaluation mode
v1.8.2,set dropout layers to training mode
v1.8.2,draw samples
v1.8.2,compute mean and std
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Train a model (quickly)
v1.8.2,Get scores for *all* triples
v1.8.2,Get scores for top 15 triples
v1.8.2,initialize buffer on device
v1.8.2,docstr-coverage: inherited
v1.8.2,"reshape, shape: (batch_size * num_entities,)"
v1.8.2,get top scores within batch
v1.8.2,append to global top scores
v1.8.2,reduce size if necessary
v1.8.2,initialize buffer on cpu
v1.8.2,Explicitly create triples
v1.8.2,docstr-coverage: inherited
v1.8.2,"TODO: in the future, we may want to expose this method"
v1.8.2,set model to evaluation mode
v1.8.2,calculate batch scores
v1.8.2,base case: infer maximum batch size
v1.8.2,base case: single batch
v1.8.2,TODO: this could happen because of AMO
v1.8.2,TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043
v1.8.2,no OOM error.
v1.8.2,make sure triples are a numpy array
v1.8.2,make sure triples are 2d
v1.8.2,convert to ID-based
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"This empty 1-element tensor doesn't actually do anything,"
v1.8.2,but is necessary since models with no grad params blow
v1.8.2,up the optimizer
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default loss function class
v1.8.2,: The default parameters for the default loss function class
v1.8.2,: The instance of the loss
v1.8.2,Random seeds have to set before the embeddings are initialized
v1.8.2,Loss
v1.8.2,TODO: why do we need to empty the cache?
v1.8.2,"TODO: this currently compute (batch_size, num_relations) instead,"
v1.8.2,"i.e., scores for normal and inverse relations"
v1.8.2,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.8.2,send to device
v1.8.2,special handling of inverse relations
v1.8.2,"when trained on inverse relations, the internal relation ID is twice the original relation ID"
v1.8.2,: The default regularizer class
v1.8.2,: The default parameters for the default regularizer class
v1.8.2,: The instance of the regularizer
v1.8.2,Regularizer
v1.8.2,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.8.2,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.8.2,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.8.2,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.8.2,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.8.2,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.8.2,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.8.2,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.8.2,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.8.2,docstr-coverage: inherited
v1.8.2,: Primary embeddings for entities
v1.8.2,: Primary embeddings for relations
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,"make sure to call this first, to reset regularizer state!"
v1.8.2,The following lines add in a post-init hook to all subclasses
v1.8.2,such that the reset_parameters_() function is run
v1.8.2,"sorry mypy, but this kind of evil must be permitted."
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Base Models
v1.8.2,Concrete Models
v1.8.2,Inductive Models
v1.8.2,Evaluation-only models
v1.8.2,Utils
v1.8.2,Abstract Models
v1.8.2,We might be able to relax this later
v1.8.2,baseline models behave differently
v1.8.2,Old style models should never be looked up
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Create an MLP for string aggregation
v1.8.2,always create representations for normal and inverse relations and padding
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,default composition is DistMult-style
v1.8.2,Saving edge indices for all the supplied splits
v1.8.2,Extract all entity and relation representations
v1.8.2,Perform message passing and get updated states
v1.8.2,Use updated entity and relation states to extract requested IDs
v1.8.2,TODO I got lost in all the Representation Modules and shape casting and wrote this ;(
v1.8.2,normalization
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,NodePiece
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,TODO rethink after RGCN update
v1.8.2,TODO: other parameters?
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default loss function class
v1.8.2,: The default parameters for the default loss function class
v1.8.2,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.8.2,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.8.2,ConvE should be trained with inverse triples
v1.8.2,entity embedding
v1.8.2,ConvE uses one bias for each entity
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,head representation
v1.8.2,tail representation
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default loss function class
v1.8.2,: The default parameters for the default loss function class
v1.8.2,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.8.2,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.8.2,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.8.2,use torch's native complex data type
v1.8.2,use torch's native complex data type
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The regularizer used by [nickel2011]_ for for RESCAL
v1.8.2,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.8.2,: a normalized weight of 10 is used.
v1.8.2,: The LP settings used by [nickel2011]_ for for RESCAL
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,"shape: (b, d)"
v1.8.2,"shape: (b, d, d)"
v1.8.2,"shape: (b, d)"
v1.8.2,Compute scores
v1.8.2,Regularization
v1.8.2,docstr-coverage: inherited
v1.8.2,Compute scores
v1.8.2,Regularization
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,Compute scores
v1.8.2,Regularization
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.8.2,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.8.2,In the code base only the weights of the output layer are used for regularization
v1.8.2,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,comment:
v1.8.2,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.8.2,uses float64
v1.8.2,entity bias for head
v1.8.2,entity bias for tail
v1.8.2,relation offset
v1.8.2,diagonal relation transformation matrix
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: the default loss function is the self-adversarial negative sampling loss
v1.8.2,: The default parameters for the default loss function class
v1.8.2,: The default entity normalizer parameters
v1.8.2,: The entity representations are normalized to L2 unit length
v1.8.2,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.8.2,"update initializer settings, cf."
v1.8.2,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.8.2,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.8.2,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.8.2,in the original implementation the embeddings are initialized in one parameter
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,"w: (k, d, d)"
v1.8.2,"vh: (k, d)"
v1.8.2,"vt: (k, d)"
v1.8.2,"b: (k,)"
v1.8.2,"u: (k,)"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The regularizer used by [yang2014]_ for DistMult
v1.8.2,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.8.2,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.8.2,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.8.2,: The LP settings used by [yang2014]_ for DistMult
v1.8.2,note: DistMult only regularizes the relation embeddings;
v1.8.2,entity embeddings are hard constrained instead
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default settings for the entity constrainer
v1.8.2,mean
v1.8.2,diagonal covariance
v1.8.2,Ensure positive definite covariances matrices and appropriate size by clamping
v1.8.2,mean
v1.8.2,diagonal covariance
v1.8.2,Ensure positive definite covariances matrices and appropriate size by clamping
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,diagonal entries
v1.8.2,off-diagonal
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The custom regularizer used by [wang2014]_ for TransH
v1.8.2,: The settings used by [wang2014]_ for TransH
v1.8.2,embeddings
v1.8.2,Normalise the normal vectors by their l2 norms
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,TODO: Add initialization
v1.8.2,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.8.2,which enforces the defined soft constraints.
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,Project to hyperplane
v1.8.2,Regularization term
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,Project to hyperplane
v1.8.2,Regularization term
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,Project to hyperplane
v1.8.2,Regularization term
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,TODO: Initialize from TransE
v1.8.2,relation embedding
v1.8.2,relation projection
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,": The default strategy for optimizing the model""s hyper-parameters"
v1.8.2,TODO: Decomposition kwargs
v1.8.2,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.8.2,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.8.2,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.8.2,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.8.2,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,combined representation
v1.8.2,Resolve interaction function
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default loss function class
v1.8.2,: The default parameters for the default loss function class
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,TODO: Use torch.cdist
v1.8.2,"There were some performance/memory issues with cdist, cf."
v1.8.2,"https://github.com/pytorch/pytorch/issues?q=cdist however, @mberr thinks"
v1.8.2,they are mostly resolved by now. A Benefit would be that we can harness the
v1.8.2,"future (performance) improvements made by the core torch developers. However,"
v1.8.2,this will require some benchmarking.
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,TODO: Use torch.cdist (see note above in score_hrt())
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,TODO: Use torch.cdist (see note above in score_hrt())
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,entity bias for head
v1.8.2,relation position head
v1.8.2,relation shape head
v1.8.2,relation size head
v1.8.2,relation position tail
v1.8.2,relation shape tail
v1.8.2,relation size tail
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default loss function class
v1.8.2,: The default parameters for the default loss function class
v1.8.2,: The regularizer used by [trouillon2016]_ for SimplE
v1.8.2,": In the paper, they use weight of 0.1, and do not normalize the"
v1.8.2,": regularization term by the number of elements, which is 200."
v1.8.2,: The power sum settings used by [trouillon2016]_ for SimplE
v1.8.2,(head) entity
v1.8.2,tail entity
v1.8.2,relations
v1.8.2,inverse relations
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,docstr-coverage: inherited
v1.8.2,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.8.2,weight initialization
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,Embedding Regularization
v1.8.2,Concatenate them
v1.8.2,Compute scores
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,Embedding Regularization
v1.8.2,First layer can be unrolled
v1.8.2,Send scores through rest of the network
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,Embedding Regularization
v1.8.2,First layer can be unrolled
v1.8.2,Send scores through rest of the network
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,Regular relation embeddings
v1.8.2,The relation-specific interaction vector
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Create an MLP for string aggregation
v1.8.2,always create representations for normal and inverse relations and padding
v1.8.2,normalize embedding specification
v1.8.2,prepare token representations & kwargs
v1.8.2,"max_id=triples_factory.num_relations,  # will get added by ERModel"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default loss function class
v1.8.2,: The default parameters for the default loss function class
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Normalize relation embeddings
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default loss function class
v1.8.2,: The default parameters for the default loss function class
v1.8.2,: The LP settings used by [zhang2019]_ for QuatE.
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default settings for the entity constrainer
v1.8.2,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.8.2,Circular correlation of entity embeddings
v1.8.2,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.8.2,compatibility: new style fft returns complex tensor
v1.8.2,Hadamard product in frequency domain
v1.8.2,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.8.2,inner product with relation embedding
v1.8.2,docstr-coverage: inherited
v1.8.2,Embedding Regularization
v1.8.2,docstr-coverage: inherited
v1.8.2,Embedding Regularization
v1.8.2,docstr-coverage: inherited
v1.8.2,Embedding Regularization
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default loss function class
v1.8.2,: The default parameters for the default loss function class
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,Get embeddings
v1.8.2,Embedding Regularization
v1.8.2,Concatenate them
v1.8.2,Predict t embedding
v1.8.2,compare with all t's
v1.8.2,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.8.2,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.2,docstr-coverage: inherited
v1.8.2,Embedding Regularization
v1.8.2,Concatenate them
v1.8.2,Predict t embedding
v1.8.2,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.2,docstr-coverage: inherited
v1.8.2,Embedding Regularization
v1.8.2,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.8.2,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.8.2,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.8.2,Extend t
v1.8.2,Concatenate them
v1.8.2,Predict t embedding
v1.8.2,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.8.2,The results have to be realigned with the expected output of the score_h function
v1.8.2,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default parameters for the default loss function class
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default loss function class
v1.8.2,: The default parameters for the default loss function class
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the model's hyper-parameters
v1.8.2,: The default parameters for the default loss function class
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"max_id=max_id,  # will be added by ERModel"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,create sparse matrix of absolute counts
v1.8.2,normalize to relative counts
v1.8.2,base case
v1.8.2,"note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could"
v1.8.2,"stay sparse here, with a potential of a huge memory benefit on large datasets!"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,These operations are deterministic and a random seed can be fixed
v1.8.2,just to avoid warnings
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,compute relation similarity matrix
v1.8.2,mapping from relations to head/tail entities
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"if we really need access to the path later, we can expose it as a property"
v1.8.2,via self.writer.log_dir
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The WANDB run
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The name of the run
v1.8.2,": The configuration dictionary, a mapping from name -> value"
v1.8.2,: Should metrics be stored when running ``log_metrics()``?
v1.8.2,": The metrics, a mapping from step -> (name -> value)"
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,: A hint for constructing a :class:`MultiResultTracker`
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Base classes
v1.8.2,Concrete classes
v1.8.2,Utilities
v1.8.2,always add a Python result tracker for storing the configuration
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The file extension for this writer (do not include dot)
v1.8.2,: The file where the results are written to.
v1.8.2,docstr-coverage: inherited
v1.8.2,: The column names
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,store set of triples
v1.8.2,docstr-coverage: inherited
v1.8.2,: some prime numbers for tuple hashing
v1.8.2,: The bit-array for the Bloom filter data structure
v1.8.2,Allocate bit array
v1.8.2,calculate number of hashing rounds
v1.8.2,index triples
v1.8.2,Store some meta-data
v1.8.2,pre-hash
v1.8.2,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,At least make sure to not replace the triples by the original value
v1.8.2,"To make sure we don't replace the {head, relation, tail} by the"
v1.8.2,original value we shift all values greater or equal than the original value by one up
v1.8.2,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.8.2,Set the indices
v1.8.2,docstr-coverage: inherited
v1.8.2,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.8.2,Bind the total number of negatives to sample in this batch
v1.8.2,Equally corrupt all sides
v1.8.2,"Do not detach, as no gradients should flow into the indices."
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.8.2,: A filterer for negative batches
v1.8.2,create unfiltered negative batch by corruption
v1.8.2,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Utils
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.8.2,create index structure
v1.8.2,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.8.2,: The concatenated sorted sets of head/tail entities
v1.8.2,docstr-coverage: inherited
v1.8.2,"shape: (batch_size, num_neg_per_pos, 3)"
v1.8.2,Uniformly sample from head/tail offsets
v1.8.2,get corresponding entity
v1.8.2,"and position within triple (0: head, 2: tail)"
v1.8.2,write into negative batch
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Preprocessing: Compute corruption probabilities
v1.8.2,"compute tph, i.e. the average number of tail entities per head"
v1.8.2,"compute hpt, i.e. the average number of head entities per tail"
v1.8.2,Set parameter for Bernoulli distribution
v1.8.2,docstr-coverage: inherited
v1.8.2,Decide whether to corrupt head or tail
v1.8.2,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.8.2,flatten mask
v1.8.2,Tails are corrupted if heads are not corrupted
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The random seed used at the beginning of the pipeline
v1.8.2,: The model trained by the pipeline
v1.8.2,: The training triples
v1.8.2,: The training loop used by the pipeline
v1.8.2,: The losses during training
v1.8.2,: The results evaluated by the pipeline
v1.8.2,: How long in seconds did training take?
v1.8.2,: How long in seconds did evaluation take?
v1.8.2,: An early stopper
v1.8.2,: The configuration
v1.8.2,: Any additional metadata as a dictionary
v1.8.2,: The version of PyKEEN used to create these results
v1.8.2,: The git hash of PyKEEN used to create these results
v1.8.2,file names for storing results
v1.8.2,TODO: rename param?
v1.8.2,always save results as json file
v1.8.2,"save other components only if requested (which they are, by default)"
v1.8.2,TODO use pathlib here
v1.8.2,"note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:"
v1.8.2,"when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless"
v1.8.2,note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()
v1.8.2,only one original value => assume this to be the mean
v1.8.2,multiple values => assume they correspond to individual trials
v1.8.2,metrics accumulates rows for a dataframe for comparison against the original reported results (if any)
v1.8.2,"TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)"
v1.8.2,summarize
v1.8.2,skip special parameters
v1.8.2,FIXME this should never happen.
v1.8.2,1. Dataset
v1.8.2,2. Model
v1.8.2,3. Loss
v1.8.2,4. Regularizer
v1.8.2,5. Optimizer
v1.8.2,5.1 Learning Rate Scheduler
v1.8.2,6. Training Loop
v1.8.2,7. Training (ronaldo style)
v1.8.2,8. Evaluation
v1.8.2,9. Tracking
v1.8.2,Misc
v1.8.2,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.8.2,used random_seed to ensure reproducible results
v1.8.2,We have to set clear optimizer to False since training should be continued
v1.8.2,Start tracking
v1.8.2,evaluation restriction to a subset of entities/relations
v1.8.2,TODO should training be reset?
v1.8.2,TODO should kwargs for loss and regularizer be checked and raised for?
v1.8.2,Log model parameters
v1.8.2,Log loss parameters
v1.8.2,the loss was already logged as part of the model kwargs
v1.8.2,"loss=loss_resolver.normalize_inst(model_instance.loss),"
v1.8.2,Log regularizer parameters
v1.8.2,Stopping
v1.8.2,"Load the evaluation batch size for the stopper, if it has been set"
v1.8.2,Add logging for debugging
v1.8.2,Train like Cristiano Ronaldo
v1.8.2,Build up a list of triples if we want to be in the filtered setting
v1.8.2,"If the user gave custom ""additional_filter_triples"""
v1.8.2,Determine whether the validation triples should also be filtered while performing test evaluation
v1.8.2,TODO consider implications of duplicates
v1.8.2,Evaluate
v1.8.2,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.8.2,Add logging about evaluator for debugging
v1.8.2,"If the evaluation still fail using the CPU, the error is raised"
v1.8.2,"When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is"
v1.8.2,restarted with PyKEEN's automatic memory optimization
v1.8.2,"When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation"
v1.8.2,is restarted using the cpu
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Imported from PyTorch
v1.8.2,: A wrapper around the hidden scheduler base class
v1.8.2,: The default strategy for optimizing the lr_schedulers' hyper-parameters
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,TODO what happens if already exists?
v1.8.2,TODO incorporate setting of random seed
v1.8.2,pipeline_kwargs=dict(
v1.8.2,"random_seed=random_non_negative_int(),"
v1.8.2,"),"
v1.8.2,Add dataset to current_pipeline
v1.8.2,"Training, test, and validation paths are provided"
v1.8.2,Add loss function to current_pipeline
v1.8.2,Add regularizer to current_pipeline
v1.8.2,Add optimizer to current_pipeline
v1.8.2,Add training approach to current_pipeline
v1.8.2,Add evaluation
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.8.2,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.8.2,the columns are not ordered properly.
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,convert class to string to use caching
v1.8.2,Assume it's a file path
v1.8.2,note: we only need to set the create_inverse_triples in the training factory.
v1.8.2,hash kwargs
v1.8.2,normalize dataset name
v1.8.2,get canonic path
v1.8.2,try to use cached dataset
v1.8.2,load dataset without cache
v1.8.2,store cache
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The name of the dataset to download
v1.8.2,"note: we do not use the built-in constants here, since those refer to OGB nomenclature"
v1.8.2,(which happens to coincide with ours)
v1.8.2,FIXME these are already identifiers
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,relation typing
v1.8.2,constants
v1.8.2,unique
v1.8.2,compute over all triples
v1.8.2,Determine group key
v1.8.2,Add labels if requested
v1.8.2,TODO: Merge with _common?
v1.8.2,include hash over triples into cache-file name
v1.8.2,include part hash into cache-file name
v1.8.2,re-use cached file if possible
v1.8.2,select triples
v1.8.2,save to file
v1.8.2,Prune by support and confidence
v1.8.2,TODO: Consider merging with other analysis methods
v1.8.2,TODO: Consider merging with other analysis methods
v1.8.2,TODO: Consider merging with other analysis methods
v1.8.2,"num_triples_validation: Optional[int],"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Raise matplotlib level
v1.8.2,expected metrics
v1.8.2,Needs simulation
v1.8.2,See https://zenodo.org/record/6331629
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,don't call this function by itself. assumes called through the `validation`
v1.8.2,property and the _training factory has already been loaded
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Normalize path
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Base classes
v1.8.2,Utilities
v1.8.2,: A factory wrapping the training triples
v1.8.2,": A factory wrapping the testing triples, that share indices with the training triples"
v1.8.2,": A factory wrapping the validation triples, that share indices with the training triples"
v1.8.2,: the dataset's name
v1.8.2,TODO: Make a constant for the names
v1.8.2,docstr-coverage: inherited
v1.8.2,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.8.2,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.8.2,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.8.2,: The directory in which the cached data is stored
v1.8.2,TODO: use class-resolver normalize?
v1.8.2,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.2,don't call this function by itself. assumes called through the `validation`
v1.8.2,property and the _training factory has already been loaded
v1.8.2,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,"relative paths within zip file's always follow Posix path, even on Windows"
v1.8.2,tarfile does not like pathlib
v1.8.2,: URL to the data to download
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Utilities
v1.8.2,Base Classes
v1.8.2,Concrete Classes
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"ZENODO_URL = ""https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip"""
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.8.2,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Base class
v1.8.2,Mid-level classes
v1.8.2,: A factory wrapping the training triples
v1.8.2,: A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT
v1.8.2,share indices with the transductive training
v1.8.2,": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.8.2,": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.8.2,: All datasets should take care of inverse triple creation
v1.8.2,": The actual instance of the training factory, which is exposed to the user through `transductive_training`"
v1.8.2,": The actual instance of the inductive inference factory,"
v1.8.2,: which is exposed to the user through `inductive_inference`
v1.8.2,": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`"
v1.8.2,": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`"
v1.8.2,: The directory in which the cached data is stored
v1.8.2,generate subfolders 'training' and  'inference'
v1.8.2,TODO: use class-resolver normalize?
v1.8.2,add v1 / v2 / v3 / v4 for inductive splits if available
v1.8.2,important: inductive_inference shares the same RELATIONS with the transductive training graph
v1.8.2,inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph
v1.8.2,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.2,inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph
v1.8.2,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,Base class
v1.8.2,Mid-level classes
v1.8.2,Datasets
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,graph pairs
v1.8.2,graph sizes
v1.8.2,graph versions
v1.8.2,: The link to the zip file
v1.8.2,: The hex digest for the zip file
v1.8.2,Input validation.
v1.8.2,ensure zip file is present
v1.8.2,save relative paths beforehand so they are present for loading
v1.8.2,delegate to super class
v1.8.2,docstr-coverage: inherited
v1.8.2,"left side has files ending with 1, right side with 2"
v1.8.2,docstr-coverage: inherited
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,": The mapping from (graph-pair, side) to triple file name"
v1.8.2,: The internal dataset name
v1.8.2,: The hex digest for the zip file
v1.8.2,input validation
v1.8.2,store *before* calling super to have it available when loading the graphs
v1.8.2,ensure zip file is present
v1.8.2,shared directory for multiple datasets.
v1.8.2,docstr-coverage: inherited
v1.8.2,create triples factory
v1.8.2,docstr-coverage: inherited
v1.8.2,load mappings for both sides
v1.8.2,load triple alignments
v1.8.2,extract entity alignments
v1.8.2,"(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2"
v1.8.2,TODO: support ID-only graphs
v1.8.2,load both graphs
v1.8.2,load alignment
v1.8.2,drop duplicates
v1.8.2,combine
v1.8.2,store for repr
v1.8.2,split
v1.8.2,create inverse triples only for training
v1.8.2,base
v1.8.2,concrete
v1.8.2,Abstract class
v1.8.2,Concrete classes
v1.8.2,Data Structures
v1.8.2,a buffer for the triples
v1.8.2,the offsets
v1.8.2,normalization
v1.8.2,append shifted mapped triples
v1.8.2,update offsets
v1.8.2,merge labels with same ID
v1.8.2,for mypy
v1.8.2,reconstruct label-to-id
v1.8.2,optional
v1.8.2,merge entity mapping
v1.8.2,merge relation mapping
v1.8.2,convert labels to IDs
v1.8.2,"map labels, using -1 as fill-value for invalid labels"
v1.8.2,"we cannot drop them here, since the two columns need to stay aligned"
v1.8.2,filter alignment
v1.8.2,map alignment from old IDs to new IDs
v1.8.2,determine swapping partner
v1.8.2,only keep triples where we have a swapping partner
v1.8.2,replace by swapping partner
v1.8.2,": the merged id-based triples, shape: (n, 3)"
v1.8.2,": the updated alignment, shape: (2, m)"
v1.8.2,: additional keyword-based parameters for adjusting label-to-id mappings
v1.8.2,concatenate triples
v1.8.2,filter alignment and translate to IDs
v1.8.2,process
v1.8.2,TODO: restrict to only using training alignments?
v1.8.2,merge mappings
v1.8.2,docstr-coverage: inherited
v1.8.2,docstr-coverage: inherited
v1.8.2,add swap triples
v1.8.2,"e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)"
v1.8.2,create dense entity remapping for swap
v1.8.2,add swapped triples
v1.8.2,swap head
v1.8.2,swap tail
v1.8.2,: the name of the additional alignment relation
v1.8.2,docstr-coverage: inherited
v1.8.2,add alignment triples with extra relation
v1.8.2,docstr-coverage: inherited
v1.8.2,"determine connected components regarding the same-as relation (i.e., applies transitivity)"
v1.8.2,apply id mapping
v1.8.2,ensure consecutive IDs
v1.8.2,only use training alignments?
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,1. Dataset
v1.8.2,2. Model
v1.8.2,3. Loss
v1.8.2,4. Regularizer
v1.8.2,5. Optimizer
v1.8.2,5.1 Learning Rate Scheduler
v1.8.2,6. Training Loop
v1.8.2,7. Training
v1.8.2,8. Evaluation
v1.8.2,9. Trackers
v1.8.2,Misc.
v1.8.2,log pruning
v1.8.2,"trial was successful, but has to be ended"
v1.8.2,also show info
v1.8.2,2. Model
v1.8.2,3. Loss
v1.8.2,4. Regularizer
v1.8.2,5. Optimizer
v1.8.2,5.1 Learning Rate Scheduler
v1.8.2,"TODO this fixes the issue for negative samplers, but does not generally address it."
v1.8.2,"For example, some of them obscure their arguments with **kwargs, so should we look"
v1.8.2,at the parent class? Sounds like something to put in class resolver by using the
v1.8.2,"inspect module. For now, this solution will rely on the fact that the sampler is a"
v1.8.2,direct descendent of a parent NegativeSampler
v1.8.2,create result tracker to allow to gracefully close failed trials
v1.8.2,1. Dataset
v1.8.2,2. Model
v1.8.2,3. Loss
v1.8.2,4. Regularizer
v1.8.2,5. Optimizer
v1.8.2,5.1 Learning Rate Scheduler
v1.8.2,6. Training Loop
v1.8.2,7. Training
v1.8.2,8. Evaluation
v1.8.2,9. Tracker
v1.8.2,Misc.
v1.8.2,close run in result tracker
v1.8.2,raise the error again (which will be catched in study.optimize)
v1.8.2,: The :mod:`optuna` study object
v1.8.2,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.8.2,Output study information
v1.8.2,Output all trials
v1.8.2,Output best trial as pipeline configuration file
v1.8.2,1. Dataset
v1.8.2,2. Model
v1.8.2,3. Loss
v1.8.2,4. Regularizer
v1.8.2,5. Optimizer
v1.8.2,5.1 Learning Rate Scheduler
v1.8.2,6. Training Loop
v1.8.2,7. Training
v1.8.2,8. Evaluation
v1.8.2,9. Tracking
v1.8.2,6. Misc
v1.8.2,Optuna Study Settings
v1.8.2,Optuna Optimization Settings
v1.8.2,TODO: use metric.increasing to determine default direction
v1.8.2,0. Metadata/Provenance
v1.8.2,1. Dataset
v1.8.2,2. Model
v1.8.2,3. Loss
v1.8.2,4. Regularizer
v1.8.2,5. Optimizer
v1.8.2,5.1 Learning Rate Scheduler
v1.8.2,6. Training Loop
v1.8.2,7. Training
v1.8.2,8. Evaluation
v1.8.2,9. Tracking
v1.8.2,1. Dataset
v1.8.2,2. Model
v1.8.2,3. Loss
v1.8.2,4. Regularizer
v1.8.2,5. Optimizer
v1.8.2,5.1 Learning Rate Scheduler
v1.8.2,6. Training Loop
v1.8.2,7. Training
v1.8.2,8. Evaluation
v1.8.2,9. Tracker
v1.8.2,Optuna Misc.
v1.8.2,Pipeline Misc.
v1.8.2,Invoke optimization of the objective function.
v1.8.2,TODO: make it even easier to specify categorical strategies just as lists
v1.8.2,"if isinstance(info, (tuple, list, set)):"
v1.8.2,"info = dict(type='categorical', choices=list(info))"
v1.8.2,get log from info - could either be a boolean or string
v1.8.2,"otherwise, dataset refers to a file that should be automatically split"
v1.8.2,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.8.2,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.8.2,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.8.2,if has_dataset(dataset_name):
v1.8.2,"study.set_user_attr('dataset', dataset_name)"
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,noqa: DAR101
v1.8.2,-*- coding: utf-8 -*-
v1.8.2,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,
v1.8.1,Configuration file for the Sphinx documentation builder.
v1.8.1,
v1.8.1,This file does only contain a selection of the most common options. For a
v1.8.1,full list see the documentation:
v1.8.1,http://www.sphinx-doc.org/en/master/config
v1.8.1,-- Path setup --------------------------------------------------------------
v1.8.1,"If extensions (or modules to document with autodoc) are in another directory,"
v1.8.1,add these directories to sys.path here. If the directory is relative to the
v1.8.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.8.1,
v1.8.1,"sys.path.insert(0, os.path.abspath('..'))"
v1.8.1,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.8.1,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.8.1,from unittest.mock import Mock
v1.8.1,sys.modules['numpy'] = Mock()
v1.8.1,sys.modules['numpy.linalg'] = Mock()
v1.8.1,sys.modules['scipy'] = Mock()
v1.8.1,sys.modules['scipy.optimize'] = Mock()
v1.8.1,sys.modules['scipy.interpolate'] = Mock()
v1.8.1,sys.modules['scipy.sparse'] = Mock()
v1.8.1,sys.modules['scipy.ndimage'] = Mock()
v1.8.1,sys.modules['scipy.ndimage.filters'] = Mock()
v1.8.1,sys.modules['tensorflow'] = Mock()
v1.8.1,sys.modules['theano'] = Mock()
v1.8.1,sys.modules['theano.tensor'] = Mock()
v1.8.1,sys.modules['torch'] = Mock()
v1.8.1,sys.modules['torch.optim'] = Mock()
v1.8.1,sys.modules['torch.nn'] = Mock()
v1.8.1,sys.modules['torch.nn.init'] = Mock()
v1.8.1,sys.modules['torch.autograd'] = Mock()
v1.8.1,sys.modules['sklearn'] = Mock()
v1.8.1,sys.modules['sklearn.model_selection'] = Mock()
v1.8.1,sys.modules['sklearn.utils'] = Mock()
v1.8.1,-- Project information -----------------------------------------------------
v1.8.1,"The full version, including alpha/beta/rc tags."
v1.8.1,The short X.Y version.
v1.8.1,-- General configuration ---------------------------------------------------
v1.8.1,"If your documentation needs a minimal Sphinx version, state it here."
v1.8.1,
v1.8.1,needs_sphinx = '1.0'
v1.8.1,"If true, the current module name will be prepended to all description"
v1.8.1,unit titles (such as .. function::).
v1.8.1,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.8.1,"Add any Sphinx extension module names here, as strings. They can be"
v1.8.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.8.1,ones.
v1.8.1,show todo's
v1.8.1,generate autosummary pages
v1.8.1,"Add any paths that contain templates here, relative to this directory."
v1.8.1,The suffix(es) of source filenames.
v1.8.1,You can specify multiple suffix as a list of string:
v1.8.1,
v1.8.1,"source_suffix = ['.rst', '.md']"
v1.8.1,The master toctree document.
v1.8.1,The language for content autogenerated by Sphinx. Refer to documentation
v1.8.1,for a list of supported languages.
v1.8.1,
v1.8.1,This is also used if you do content translation via gettext catalogs.
v1.8.1,"Usually you set ""language"" from the command line for these cases."
v1.8.1,"List of patterns, relative to source directory, that match files and"
v1.8.1,directories to ignore when looking for source files.
v1.8.1,This pattern also affects html_static_path and html_extra_path.
v1.8.1,The name of the Pygments (syntax highlighting) style to use.
v1.8.1,-- Options for HTML output -------------------------------------------------
v1.8.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.8.1,a list of builtin themes.
v1.8.1,
v1.8.1,Theme options are theme-specific and customize the look and feel of a theme
v1.8.1,"further.  For a list of options available for each theme, see the"
v1.8.1,documentation.
v1.8.1,
v1.8.1,html_theme_options = {}
v1.8.1,"Add any paths that contain custom static files (such as style sheets) here,"
v1.8.1,"relative to this directory. They are copied after the builtin static files,"
v1.8.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.8.1,html_static_path = ['_static']
v1.8.1,"Custom sidebar templates, must be a dictionary that maps document names"
v1.8.1,to template names.
v1.8.1,
v1.8.1,The default sidebars (for documents that don't match any pattern) are
v1.8.1,defined by theme itself.  Builtin themes are using these templates by
v1.8.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.8.1,'searchbox.html']``.
v1.8.1,
v1.8.1,html_sidebars = {}
v1.8.1,The name of an image file (relative to this directory) to place at the top
v1.8.1,of the sidebar.
v1.8.1,
v1.8.1,-- Options for HTMLHelp output ---------------------------------------------
v1.8.1,Output file base name for HTML help builder.
v1.8.1,-- Options for LaTeX output ------------------------------------------------
v1.8.1,latex_elements = {
v1.8.1,The paper size ('letterpaper' or 'a4paper').
v1.8.1,
v1.8.1,"'papersize': 'letterpaper',"
v1.8.1,
v1.8.1,"The font size ('10pt', '11pt' or '12pt')."
v1.8.1,
v1.8.1,"'pointsize': '10pt',"
v1.8.1,
v1.8.1,Additional stuff for the LaTeX preamble.
v1.8.1,
v1.8.1,"'preamble': '',"
v1.8.1,
v1.8.1,Latex figure (float) alignment
v1.8.1,
v1.8.1,"'figure_align': 'htbp',"
v1.8.1,}
v1.8.1,Grouping the document tree into LaTeX files. List of tuples
v1.8.1,"(source start file, target name, title,"
v1.8.1,"author, documentclass [howto, manual, or own class])."
v1.8.1,latex_documents = [
v1.8.1,(
v1.8.1,"master_doc,"
v1.8.1,"'pykeen.tex',"
v1.8.1,"'PyKEEN Documentation',"
v1.8.1,"author,"
v1.8.1,"'manual',"
v1.8.1,"),"
v1.8.1,]
v1.8.1,-- Options for manual page output ------------------------------------------
v1.8.1,One entry per manual page. List of tuples
v1.8.1,"(source start file, name, description, authors, manual section)."
v1.8.1,-- Options for Texinfo output ----------------------------------------------
v1.8.1,Grouping the document tree into Texinfo files. List of tuples
v1.8.1,"(source start file, target name, title, author,"
v1.8.1,"dir menu entry, description, category)"
v1.8.1,-- Options for Epub output -------------------------------------------------
v1.8.1,Bibliographic Dublin Core info.
v1.8.1,epub_title = project
v1.8.1,The unique identifier of the text. This can be a ISBN number
v1.8.1,or the project homepage.
v1.8.1,
v1.8.1,epub_identifier = ''
v1.8.1,A unique identification for the text.
v1.8.1,
v1.8.1,epub_uid = ''
v1.8.1,A list of files that should not be packed into the epub file.
v1.8.1,epub_exclude_files = ['search.html']
v1.8.1,-- Extension configuration -------------------------------------------------
v1.8.1,-- Options for intersphinx extension ---------------------------------------
v1.8.1,Example configuration for intersphinx: refer to the Python standard library.
v1.8.1,"'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),"
v1.8.1,autodoc_member_order = 'bysource'
v1.8.1,autodoc_typehints = 'both' # TODO turn on after 4.1 release
v1.8.1,autodoc_preserve_defaults = True
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,check probability distribution
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Check a model param is optimized
v1.8.1,Check a loss param is optimized
v1.8.1,Check a model param is NOT optimized
v1.8.1,Check a loss param is optimized
v1.8.1,Check a model param is optimized
v1.8.1,Check a loss param is NOT optimized
v1.8.1,Check a model param is NOT optimized
v1.8.1,Check a loss param is NOT optimized
v1.8.1,verify failure
v1.8.1,"Since custom data was passed, we can't store any of this"
v1.8.1,"currently, any custom data doesn't get stored."
v1.8.1,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.8.1,"Since there's no source path information, these shouldn't be"
v1.8.1,"added, even if it might be possible to infer path information"
v1.8.1,from the triples factories
v1.8.1,"Since paths were passed for training, testing, and validation,"
v1.8.1,they should be stored as study-level attributes
v1.8.1,Check a model param is optimized
v1.8.1,Check a loss param is optimized
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,check if within 0.5 std of observed
v1.8.1,test error is raised
v1.8.1,Tests that exception will be thrown when more than or less than three tensors are passed
v1.8.1,Test that regularization term is computed correctly
v1.8.1,Entity soft constraint
v1.8.1,Orthogonality soft constraint
v1.8.1,ensure regularizer is on correct device
v1.8.1,"After first update, should change the term"
v1.8.1,"After second update, no change should happen"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,create broadcastable shapes
v1.8.1,check correct value range
v1.8.1,check maximum norm constraint
v1.8.1,unchanged values for small norms
v1.8.1,random entity embeddings & projections
v1.8.1,random relation embeddings & projections
v1.8.1,project
v1.8.1,check shape:
v1.8.1,check normalization
v1.8.1,check equivalence of re-formulation
v1.8.1,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.8.1,= r_p (e_p^T e) + e'
v1.8.1,"create random array, estimate the costs of addition, and measure some execution times."
v1.8.1,"then, compute correlation between the estimated cost, and the measured time."
v1.8.1,check for strong correlation between estimated costs and measured execution time
v1.8.1,get optimal sequence
v1.8.1,check caching
v1.8.1,get optimal sequence
v1.8.1,check correct cost
v1.8.1,check optimality
v1.8.1,compare result to sequential addition
v1.8.1,compare result to sequential addition
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,equal value; larger is better
v1.8.1,equal value; smaller is better
v1.8.1,larger is better; improvement
v1.8.1,larger is better; improvement; but not significant
v1.8.1,: The window size used by the early stopper
v1.8.1,: The mock losses the mock evaluator will return
v1.8.1,: The (zeroed) index  - 1 at which stopping will occur
v1.8.1,: The minimum improvement
v1.8.1,: The best results
v1.8.1,Set automatic_memory_optimization to false for tests
v1.8.1,Step early stopper
v1.8.1,check storing of results
v1.8.1,not needed for test
v1.8.1,assert that reporting another metric for this epoch raises an error
v1.8.1,: The window size used by the early stopper
v1.8.1,: The (zeroed) index  - 1 at which stopping will occur
v1.8.1,: The minimum improvement
v1.8.1,: The random seed to use for reproducibility
v1.8.1,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.8.1,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.8.1,: The batch size to use.
v1.8.1,Fix seed for reproducibility
v1.8.1,Set automatic_memory_optimization to false during testing
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.8.1,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.8.1,"first triple, and passing create_inverse_triples=True."
v1.8.1,check for warning
v1.8.1,check for filtered triples
v1.8.1,check for correct inverse triples flag
v1.8.1,check correct translation
v1.8.1,check column order
v1.8.1,apply restriction
v1.8.1,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.8.1,check that inverse_triples is correctly carried over
v1.8.1,verify that the label-to-ID mapping has not been changed
v1.8.1,verify that triples have been filtered
v1.8.1,Test different combinations of restrictions
v1.8.1,check compressed triples
v1.8.1,reconstruct triples from compressed form
v1.8.1,check data loader
v1.8.1,set create inverse triple to true
v1.8.1,split factory
v1.8.1,check that in *training* inverse triple are to be created
v1.8.1,check that in all other splits no inverse triples are to be created
v1.8.1,verify that all entities and relations are present in the training factory
v1.8.1,verify that no triple got lost
v1.8.1,verify that the label-to-id mappings match
v1.8.1,Slightly larger number of triples to guarantee split can find coverage of all entities and relations.
v1.8.1,serialize
v1.8.1,de-serialize
v1.8.1,check for equality
v1.8.1,TODO: this could be (Core)TriplesFactory.__equal__
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"DummyModel,"
v1.8.1,3x batch norm: bias + scale --> 6
v1.8.1,entity specific bias        --> 1
v1.8.1,==================================
v1.8.1,7
v1.8.1,"two bias terms, one conv-filter"
v1.8.1,check type
v1.8.1,check shape
v1.8.1,check ID ranges
v1.8.1,this is only done in one of the models
v1.8.1,this is only done in one of the models
v1.8.1,Two linear layer biases
v1.8.1,"Two BN layers, bias & scale"
v1.8.1,Test that the weight in the MLP is trainable (i.e. requires grad)
v1.8.1,quaternion have four components
v1.8.1,: one bias per layer
v1.8.1,: one bias per layer
v1.8.1,entity embeddings
v1.8.1,relation embeddings
v1.8.1,Compute Scores
v1.8.1,Use different dimension for relation embedding: relation_dim > entity_dim
v1.8.1,relation embeddings
v1.8.1,Compute Scores
v1.8.1,Use different dimension for relation embedding: relation_dim < entity_dim
v1.8.1,entity embeddings
v1.8.1,relation embeddings
v1.8.1,Compute Scores
v1.8.1,: 2xBN (bias & scale)
v1.8.1,the combination bias
v1.8.1,FIXME definitely a type mismatch going on here
v1.8.1,check shape
v1.8.1,check content
v1.8.1,create triples factory with inverse relations
v1.8.1,head prediction via inverse tail prediction
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,empty lists are falsy
v1.8.1,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.8.1,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.8.1,pipeline is checked.
v1.8.1,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.8.1,Resume the previous pipeline
v1.8.1,The MockModel gives the highest score to the highest entity id
v1.8.1,The test triples are created to yield the third highest score on both head and tail prediction
v1.8.1,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.8.1,These triples are created to yield the highest score on both head and tail prediction for the
v1.8.1,test triple at hand
v1.8.1,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.8.1,test triple at hand
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.8.1,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.8.1,Test looking up inverse triples
v1.8.1,test new label to ID
v1.8.1,type
v1.8.1,old labels
v1.8.1,"new, compact IDs"
v1.8.1,test vectorized lookup
v1.8.1,type
v1.8.1,shape
v1.8.1,value range
v1.8.1,only occurring Ids get mapped to non-negative numbers
v1.8.1,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.8.1,check type
v1.8.1,check shape
v1.8.1,check content
v1.8.1,check type
v1.8.1,check shape
v1.8.1,check 1-hot
v1.8.1,check type
v1.8.1,check shape
v1.8.1,check value range
v1.8.1,check self-similarity = 1
v1.8.1,base relation
v1.8.1,exact duplicate
v1.8.1,99% duplicate
v1.8.1,50% duplicate
v1.8.1,exact inverse
v1.8.1,99% inverse
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The expected number of entities
v1.8.1,: The expected number of relations
v1.8.1,: The expected number of triples
v1.8.1,": The tolerance on expected number of triples, for randomized situations"
v1.8.1,: The dataset to test
v1.8.1,: The instantiated dataset
v1.8.1,: Should the validation be assumed to have been loaded with train/test?
v1.8.1,Not loaded
v1.8.1,Load
v1.8.1,Test caching
v1.8.1,assert (end - start) < 1.0e-02
v1.8.1,Test consistency of training / validation / testing mapping
v1.8.1,": The directory, if there is caching"
v1.8.1,: The batch size
v1.8.1,: The number of negatives per positive for sLCWA training loop.
v1.8.1,: The number of entities LCWA training loop / label smoothing.
v1.8.1,test reduction
v1.8.1,test finite loss value
v1.8.1,Test backward
v1.8.1,negative scores decreased compared to positive ones
v1.8.1,negative scores decreased compared to positive ones
v1.8.1,: The number of entities.
v1.8.1,: The number of negative samples
v1.8.1,: The number of entities.
v1.8.1,: The equivalence for models with batch norm only holds in evaluation mode
v1.8.1,: The equivalence for models with batch norm only holds in evaluation mode
v1.8.1,: The equivalence for models with batch norm only holds in evaluation mode
v1.8.1,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.8.1,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.8.1,test multiple different initializations
v1.8.1,calculate by functional
v1.8.1,calculate manually
v1.8.1,simple
v1.8.1,nested
v1.8.1,nested
v1.8.1,prepare a temporary test directory
v1.8.1,check that file was created
v1.8.1,make sure to close file before trying to delete it
v1.8.1,delete intermediate files
v1.8.1,: The batch size
v1.8.1,: The triples factory
v1.8.1,: Class of regularizer to test
v1.8.1,: The constructor parameters to pass to the regularizer
v1.8.1,": The regularizer instance, initialized in setUp"
v1.8.1,: A positive batch
v1.8.1,: The device
v1.8.1,move test instance to device
v1.8.1,Use RESCAL as it regularizes multiple tensors of different shape.
v1.8.1,Check if regularizer is stored correctly.
v1.8.1,Forward pass (should update regularizer)
v1.8.1,Call post_parameter_update (should reset regularizer)
v1.8.1,Check if regularization term is reset
v1.8.1,Call method
v1.8.1,Generate random tensors
v1.8.1,Call update
v1.8.1,check shape
v1.8.1,compute expected term
v1.8.1,Generate random tensor
v1.8.1,calculate penalty
v1.8.1,check shape
v1.8.1,check value
v1.8.1,update term
v1.8.1,check that the expected term is returned
v1.8.1,FIXME isn't any finite number allowed now?
v1.8.1,: Additional arguments passed to the training loop's constructor method
v1.8.1,: The triples factory instance
v1.8.1,: The batch size for use for forward_* tests
v1.8.1,: The embedding dimensionality
v1.8.1,: Whether to create inverse triples (needed e.g. by ConvE)
v1.8.1,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.8.1,: The batch size for use when testing training procedures
v1.8.1,: The number of epochs to train the model
v1.8.1,: A random number generator from torch
v1.8.1,: The number of parameters which receive a constant (i.e. non-randomized)
v1.8.1,initialization
v1.8.1,: Static extras to append to the CLI
v1.8.1,: the model's device
v1.8.1,: the inductive mode
v1.8.1,for reproducible testing
v1.8.1,insert shared parameters
v1.8.1,move model to correct device
v1.8.1,Check that all the parameters actually require a gradient
v1.8.1,Try to initialize an optimizer
v1.8.1,get model parameters
v1.8.1,re-initialize
v1.8.1,check that the operation works in-place
v1.8.1,check that the parameters where modified
v1.8.1,check for finite values by default
v1.8.1,check whether a gradient can be back-propgated
v1.8.1,"assert batch comprises (head, relation) pairs"
v1.8.1,"assert batch comprises (head, tail) pairs"
v1.8.1,TODO: look into score_r for inverse relations
v1.8.1,"assert batch comprises (relation, tail) pairs"
v1.8.1,"For the high/low memory test cases of NTN, SE, etc."
v1.8.1,"else, leave to default"
v1.8.1,Make sure that inverse triples are created if create_inverse_triples=True
v1.8.1,triples factory is added by the pipeline
v1.8.1,TODO: Catch HolE MKL error?
v1.8.1,set regularizer term to something that isn't zero
v1.8.1,call post_parameter_update
v1.8.1,assert that the regularization term has been reset
v1.8.1,do one optimization step
v1.8.1,call post_parameter_update
v1.8.1,check model constraints
v1.8.1,"assert batch comprises (relation, tail) pairs"
v1.8.1,"assert batch comprises (relation, tail) pairs"
v1.8.1,"assert batch comprises (relation, tail) pairs"
v1.8.1,call some functions
v1.8.1,reset to old state
v1.8.1,Distance-based model
v1.8.1,dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)
v1.8.1,check type
v1.8.1,check shape
v1.8.1,create a new instance with guaranteed dropout
v1.8.1,set to training mode
v1.8.1,check for different output
v1.8.1,use more samples to make sure that enough values can be dropped
v1.8.1,: The number of entities
v1.8.1,: The number of triples
v1.8.1,: the message dim
v1.8.1,TODO: separation message vs. entity dim?
v1.8.1,check shape
v1.8.1,check dtype
v1.8.1,check finite values (e.g. due to division by zero)
v1.8.1,check non-negativity
v1.8.1,: The input dimension
v1.8.1,: the shape of the tensor to initialize
v1.8.1,: to be initialized / set in subclass
v1.8.1,initializers *may* work in-place => clone
v1.8.1,unfavourable split to ensure that cleanup is necessary
v1.8.1,check for unclean split
v1.8.1,check that no triple got lost
v1.8.1,check that triples where only moved from other to reference
v1.8.1,check that all entities occur in reference
v1.8.1,check that no triple got lost
v1.8.1,check that all entities are covered in first part
v1.8.1,the model
v1.8.1,Settings
v1.8.1,Use small model (untrained)
v1.8.1,Get batch
v1.8.1,Compute scores
v1.8.1,Compute mask only if required
v1.8.1,TODO: Re-use filtering code
v1.8.1,"shape: (batch_size, num_triples)"
v1.8.1,"shape: (batch_size, num_entities)"
v1.8.1,Process one batch
v1.8.1,shape
v1.8.1,value range
v1.8.1,no duplicates
v1.8.1,shape
v1.8.1,value range
v1.8.1,no duplicates
v1.8.1,shape
v1.8.1,value range
v1.8.1,"no repetition, except padding idx"
v1.8.1,: The batch size
v1.8.1,: the maximum number of candidates
v1.8.1,: the number of ranks
v1.8.1,: the number of samples to use for monte-carlo estimation
v1.8.1,: the number of candidates for each individual ranking task
v1.8.1,: the ranks for each individual ranking task
v1.8.1,data type
v1.8.1,value range
v1.8.1,original ranks
v1.8.1,better ranks
v1.8.1,variances are non-negative
v1.8.1,generate random weights such that sum = n
v1.8.1,for sanity checking: give the largest weight to best rank => should improve
v1.8.1,generate two versions
v1.8.1,1. repeat each rank/candidate pair a random number of times
v1.8.1,"2. do not repeat, but assign a corresponding weight"
v1.8.1,check flatness
v1.8.1,"TODO: does this suffice, or do we really need float as datatype?"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,check for finite values by default
v1.8.1,Set into training mode to check if it is correctly set to evaluation mode.
v1.8.1,Set into training mode to check if it is correctly set to evaluation mode.
v1.8.1,Set into training mode to check if it is correctly set to evaluation mode.
v1.8.1,"TODO: Remove, since it stems from old-style model"
v1.8.1,Get embeddings
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,≈ result of softmax
v1.8.1,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.8.1,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.8.1,"pos_distances = [0., 0., 0.5, 0.5]"
v1.8.1,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.8.1,≈ result of sigmoid
v1.8.1,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.8.1,expected_loss ≈ 0.34
v1.8.1,Create dummy dense labels
v1.8.1,Check if labels form a probability distribution
v1.8.1,Apply label smoothing
v1.8.1,Check if smooth labels form probability distribution
v1.8.1,Create dummy sLCWA labels
v1.8.1,Apply label smoothing
v1.8.1,generate random ratios
v1.8.1,check size
v1.8.1,check value range
v1.8.1,check total split
v1.8.1,check consistency with ratios
v1.8.1,the number of decimal digits equivalent to 1 / n_total
v1.8.1,check type
v1.8.1,check values
v1.8.1,compare against expected
v1.8.1,generated_triples = generate_triples()
v1.8.1,check type
v1.8.1,check format
v1.8.1,check coverage
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"naive implementation, O(n2)"
v1.8.1,check correct output type
v1.8.1,check value range subset
v1.8.1,check value range side
v1.8.1,check columns
v1.8.1,check value range and type
v1.8.1,check value range entity IDs
v1.8.1,check value range entity labels
v1.8.1,check correct type
v1.8.1,check relation_id value range
v1.8.1,check pattern value range
v1.8.1,check confidence value range
v1.8.1,check support value range
v1.8.1,check correct type
v1.8.1,check relation_id value range
v1.8.1,check pattern value range
v1.8.1,check correct type
v1.8.1,check relation_id value range
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,clear
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Check minimal statistics
v1.8.1,Check either a github link or author/publication information is given
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.8.1,"prepare conv input (N, C, H, W)"
v1.8.1,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.8.1,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.8.1,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.8.1,"f(h, r, t) = h @ r @ t"
v1.8.1,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.8.1,normalize rotations to unit modulus
v1.8.1,check for unit modulus
v1.8.1,entity embeddings
v1.8.1,relation embeddings
v1.8.1,Compute Scores
v1.8.1,entity embeddings
v1.8.1,relation embeddings
v1.8.1,Compute Scores
v1.8.1,Compute Scores
v1.8.1,-\|R_h h - R_t t\|
v1.8.1,-\|h - t\|
v1.8.1,"Since MuRE has offsets, the scores do not need to negative"
v1.8.1,"We do not need this, since we do not check for functional consistency anyway"
v1.8.1,intra-interaction comparison
v1.8.1,dimension needs to be divisible by num_heads
v1.8.1,FIXME
v1.8.1,multiple
v1.8.1,single
v1.8.1,head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,message_dim must be divisible by num_heads
v1.8.1,determine pool using anchor searcher
v1.8.1,determine expected pool using shortest path distances via scipy.sparse.csgraph
v1.8.1,generate random pool
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,check value range
v1.8.1,check sin**2 + cos**2 == 1
v1.8.1,"check value range (actually [-s, +s] with s = 1/sqrt(2*n))"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"typically, the model takes care of adjusting the dimension size for ""complex"""
v1.8.1,"tensors, but we have to do it manually here for testing purposes"
v1.8.1,hotfix for mixed dtypes
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO consider making subclass of cases.RepresentationTestCase
v1.8.1,"that has num_entities, num_relations, num_triples, and"
v1.8.1,create_inverse_triples as well as a generate_triples_factory()
v1.8.1,wrapper
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO this is the only place this function is used.
v1.8.1,Is there an alternative so we can remove it?
v1.8.1,ensure positivity
v1.8.1,compute using pytorch
v1.8.1,prepare distributions
v1.8.1,compute using pykeen
v1.8.1,"e: (batch_size, num_heads, num_tails, d)"
v1.8.1,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.8.1,divergence = 0 => similarity = -divergence = 0
v1.8.1,"(h - t), r"
v1.8.1,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.8.1,divergence >= 0 => similarity = -divergence <= 0
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.8.1,filter vs. no filter thing.
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,check for empty batches
v1.8.1,: The window size used by the early stopper
v1.8.1,: The mock losses the mock evaluator will return
v1.8.1,: The (zeroed) index  - 1 at which stopping will occur
v1.8.1,: The minimum improvement
v1.8.1,: The best results
v1.8.1,Set automatic_memory_optimization to false for tests
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Train a model in one shot
v1.8.1,Train a model for the first half
v1.8.1,Continue training of the first part
v1.8.1,check non-empty metrics
v1.8.1,: Should negative samples be filtered?
v1.8.1,expectation = (1 + n) / 2
v1.8.1,variance = (n**2 - 1) / 12
v1.8.1,"x_i ~ N(mu_i, 1)"
v1.8.1,closed-form solution
v1.8.1,sampled confidence interval
v1.8.1,check that closed-form is in confidence interval of sampled
v1.8.1,positive values only
v1.8.1,positive and negative values
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Check for correct class
v1.8.1,check correct num_entities
v1.8.1,Check for correct class
v1.8.1,check value
v1.8.1,filtering
v1.8.1,"true_score: (2, 3, 3)"
v1.8.1,head based filter
v1.8.1,preprocessing for faster lookup
v1.8.1,check that all found positives are positive
v1.8.1,check in-place
v1.8.1,Test head scores
v1.8.1,Assert in-place modification
v1.8.1,Assert correct filtering
v1.8.1,Test tail scores
v1.8.1,Assert in-place modification
v1.8.1,Assert correct filtering
v1.8.1,The MockModel gives the highest score to the highest entity id
v1.8.1,The test triples are created to yield the third highest score on both head and tail prediction
v1.8.1,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.8.1,These triples are created to yield the highest score on both head and tail prediction for the
v1.8.1,test triple at hand
v1.8.1,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.8.1,test triple at hand
v1.8.1,check true negatives
v1.8.1,TODO: check no repetitions (if possible)
v1.8.1,return type
v1.8.1,columns
v1.8.1,value range
v1.8.1,relation restriction
v1.8.1,with explicit num_entities
v1.8.1,with inferred num_entities
v1.8.1,test different shapes
v1.8.1,test different shapes
v1.8.1,value range
v1.8.1,value range
v1.8.1,check unique
v1.8.1,"strips off the ""k"" at the end"
v1.8.1,Populate with real results.
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"(-1, 1),"
v1.8.1,"(-1, -1),"
v1.8.1,"(-5, -3),"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Check whether filtering works correctly
v1.8.1,First giving an example where all triples have to be filtered
v1.8.1,The filter should remove all triples
v1.8.1,Create an example where no triples will be filtered
v1.8.1,The filter should not remove any triple
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,same relation
v1.8.1,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.8.1,Test that half of the subjects and half of the objects are corrupted
v1.8.1,check that corrupted entities co-occur with the relation in training data
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The batch size
v1.8.1,: The random seed
v1.8.1,: The triples factory
v1.8.1,: The instances
v1.8.1,: A positive batch
v1.8.1,: Kwargs
v1.8.1,Generate negative sample
v1.8.1,check filter shape if necessary
v1.8.1,check shape
v1.8.1,check bounds: heads
v1.8.1,check bounds: relations
v1.8.1,check bounds: tails
v1.8.1,test that the negative triple is not the original positive triple
v1.8.1,"shape: (batch_size, 1, num_neg)"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Base Classes
v1.8.1,Concrete Classes
v1.8.1,Utils
v1.8.1,: The default strategy for optimizing the loss's hyper-parameters
v1.8.1,flatten and stack
v1.8.1,apply label smoothing if necessary.
v1.8.1,TODO: Do label smoothing only once
v1.8.1,Sanity check
v1.8.1,"prepare for broadcasting, shape: (batch_size, 1, 3)"
v1.8.1,negative_scores have already been filtered in the sampler!
v1.8.1,"shape: (nnz,)"
v1.8.1,Sanity check
v1.8.1,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.8.1,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.8.1,positive entries in the labels tensor.
v1.8.1,"This shows how often one row has to be repeated,"
v1.8.1,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.8.1,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.8.1,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.8.1,This tells us how often each true label should be repeated
v1.8.1,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.8.1,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.8.1,within the ablation pipeline.
v1.8.1,1. positive & negative margin
v1.8.1,2. negative margin & offset
v1.8.1,3. positive margin & offset
v1.8.1,Sanity check
v1.8.1,positive term
v1.8.1,implicitly repeat positive scores
v1.8.1,"shape: (nnz,)"
v1.8.1,negative term
v1.8.1,negative_scores have already been filtered in the sampler!
v1.8.1,Sanity check
v1.8.1,"scale labels from [0, 1] to [-1, 1]"
v1.8.1,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.8.1,within the ablation pipeline.
v1.8.1,negative_scores have already been filtered in the sampler!
v1.8.1,(dense) softmax requires unfiltered scores / masking
v1.8.1,we need to fill the scores with -inf for all filtered negative examples
v1.8.1,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.8.1,use filled negatives scores
v1.8.1,we need dense negative scores => unfilter if necessary
v1.8.1,"we may have inf rows, since there will be one additional finite positive score per row"
v1.8.1,"combine scores: shape: (batch_size, num_negatives + 1)"
v1.8.1,use sparse version of cross entropy
v1.8.1,Sanity check
v1.8.1,compute negative weights (without gradient tracking)
v1.8.1,clone is necessary since we modify in-place
v1.8.1,Split positive and negative scores
v1.8.1,Sanity check
v1.8.1,"we do not allow full -inf rows, since we compute the softmax over this tensor"
v1.8.1,compute weights (without gradient tracking)
v1.8.1,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.8.1,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.8.1,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.8.1,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.8.1,": For more information, see https://github.com/cthoyt/pystow"
v1.8.1,: A path representing the PyKEEN data folder
v1.8.1,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.8.1,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.8.1,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.8.1,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.8.1,: A subdirectory for PyKEEN logs
v1.8.1,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.8.1,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.8.1,"TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728"
v1.8.1,"SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.8.1,get datatype specific epsilon
v1.8.1,clamp minimum value
v1.8.1,try to resolve ambiguous device; there has to be at least one cuda device
v1.8.1,lower bound
v1.8.1,upper bound
v1.8.1,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.8.1,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.8.1,Determine optimal order and cost
v1.8.1,translate back to original order
v1.8.1,determine optimal processing order
v1.8.1,heuristic
v1.8.1,TODO: check if einsum is still very slow.
v1.8.1,TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation
v1.8.1,unsqueeze
v1.8.1,The dimensions affected by e'
v1.8.1,Project entities
v1.8.1,r_p (e_p.T e) + e'
v1.8.1,Enforce constraints
v1.8.1,TODO delete when deleting _normalize_dim (below)
v1.8.1,TODO delete when deleting convert_to_canonical_shape (below)
v1.8.1,TODO delete? See note in test_sim.py on its only usage
v1.8.1,upgrade to sequence
v1.8.1,broadcast
v1.8.1,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.8.1,Create a tensor of all IDs
v1.8.1,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.8.1,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.8.1,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.8.1,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.8.1,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.8.1,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.8.1,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.8.1,ensure pathlib
v1.8.1,Enforce that sizes are strictly positive by passing through ELU
v1.8.1,Shape vector is normalized using the above helper function
v1.8.1,Size is learned separately and applied to normalized shape
v1.8.1,Compute potential boundaries by applying the shape in substraction
v1.8.1,and in addition
v1.8.1,Compute box upper bounds using min and max respectively
v1.8.1,compute width plus 1
v1.8.1,compute box midpoints
v1.8.1,"TODO: we already had this before, as `base`"
v1.8.1,inside box?
v1.8.1,yes: |p - c| / (w + 1)
v1.8.1,no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))
v1.8.1,Step 1: Apply the other entity bump
v1.8.1,Step 2: Apply tanh if tanh_map is set to True.
v1.8.1,Compute the distance function output element-wise
v1.8.1,"Finally, compute the norm"
v1.8.1,cf. https://stackoverflow.com/a/1176023
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Base Class
v1.8.1,Child classes
v1.8.1,Utils
v1.8.1,: The overall regularization weight
v1.8.1,: The current regularization term (a scalar)
v1.8.1,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.8.1,: Has this regularizer been updated since last being reset?
v1.8.1,: The default strategy for optimizing the regularizer's hyper-parameters
v1.8.1,"If there are tracked parameters, update based on them"
v1.8.1,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.8.1,no need to compute anything
v1.8.1,always return zero
v1.8.1,: The dimension along which to compute the vector-based regularization terms.
v1.8.1,: Whether to normalize the regularization term by the dimension of the vectors.
v1.8.1,: This allows dimensionality-independent weight tuning.
v1.8.1,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.8.1,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.8.1,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.8.1,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.8.1,"Therefore, apply_only_once is always set to True."
v1.8.1,Entity soft constraint
v1.8.1,Orthogonality soft constraint
v1.8.1,The normalization factor to balance individual regularizers' contribution.
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"""Closed-Form Expectation"","
v1.8.1,"""Closed-Form Variance"","
v1.8.1,"""✓"" if metric.closed_expectation else """","
v1.8.1,"""✓"" if metric.closed_variance else """","
v1.8.1,Add HPO command
v1.8.1,Add NodePiece tokenization command
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,General types
v1.8.1,Triples
v1.8.1,Others
v1.8.1,Tensor Functions
v1.8.1,Tensors
v1.8.1,Dataclasses
v1.8.1,prediction targets
v1.8.1,modes
v1.8.1,: A function that mutates the input and returns a new object of the same type as output
v1.8.1,: A function that can be applied to a tensor to initialize it
v1.8.1,: A function that can be applied to a tensor to normalize it
v1.8.1,: A function that can be applied to a tensor to constrain it
v1.8.1,: A hint for a :class:`torch.device`
v1.8.1,: A hint for a :class:`torch.Generator`
v1.8.1,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.8.1,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.8.1,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.8.1,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.8.1,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.8.1,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.8.1,: the inductive prediction and training mode
v1.8.1,: the prediction target
v1.8.1,: the prediction target index
v1.8.1,: the rank types
v1.8.1,"RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,pad with zeros
v1.8.1,trim
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"mask, shape: (num_edges,)"
v1.8.1,bi-directional message passing
v1.8.1,Heuristic for default value
v1.8.1,other relations
v1.8.1,other relations
v1.8.1,Select source and target indices as well as edge weights for the
v1.8.1,currently considered relation
v1.8.1,skip relations without edges
v1.8.1,"compute message, shape: (num_edges_of_type, output_dim)"
v1.8.1,since we may have one node ID appearing multiple times as source
v1.8.1,"ID, we can save some computation by first reducing to the unique"
v1.8.1,"source IDs, compute transformed representations and afterwards"
v1.8.1,select these representations for the correct edges.
v1.8.1,select unique source node representations
v1.8.1,transform representations by relation specific weight
v1.8.1,select the uniquely transformed representations for each edge
v1.8.1,optional message weighting
v1.8.1,message aggregation
v1.8.1,Xavier Glorot initialization of each block
v1.8.1,accumulator
v1.8.1,view as blocks
v1.8.1,other relations
v1.8.1,skip relations without edges
v1.8.1,"compute message, shape: (num_edges_of_type, num_blocks, block_size)"
v1.8.1,optional message weighting
v1.8.1,message aggregation
v1.8.1,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501
v1.8.1,there are separate decompositions for forward and backward relations.
v1.8.1,the self-loop weight is not decomposed.
v1.8.1,self-loop
v1.8.1,forward messages
v1.8.1,backward messages
v1.8.1,activation
v1.8.1,has to be imported now to avoid cyclic imports
v1.8.1,Resolve edge weighting
v1.8.1,dropout
v1.8.1,"Save graph using buffers, such that the tensors are moved together with the model"
v1.8.1,no activation on last layer
v1.8.1,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501
v1.8.1,buffering of enriched representations
v1.8.1,invalidate enriched embeddings
v1.8.1,Bind fields
v1.8.1,"shape: (num_entities, embedding_dim)"
v1.8.1,Edge dropout: drop the same edges on all layers (only in training mode)
v1.8.1,Get random dropout mask
v1.8.1,Apply to edges
v1.8.1,fixed edges -> pre-compute weights
v1.8.1,Cache enriched representations
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Utils
v1.8.1,: the maximum ID (exclusively)
v1.8.1,: the shape of an individual representation
v1.8.1,: a normalizer for individual representations
v1.8.1,: a regularizer for individual representations
v1.8.1,: dropout
v1.8.1,normalize *before* repeating
v1.8.1,regularize *after* repeating
v1.8.1,TODO: Remove this property and update code to use shape instead
v1.8.1,has to be imported here to avoid cyclic import
v1.8.1,normalize num_embeddings vs. max_id
v1.8.1,normalize embedding_dim vs. shape
v1.8.1,work-around until full complex support (torch==1.10 still does not work)
v1.8.1,TODO: verify that this is our understanding of complex!
v1.8.1,"note: this seems to work, as finfo returns the datatype of the underlying floating"
v1.8.1,"point dtype, rather than the combined complex one"
v1.8.1,"use make for initializer since there's a default, and make_safe"
v1.8.1,for the others to pass through None values
v1.8.1,"wrapper around max_id, for backward compatibility"
v1.8.1,initialize weights in-place
v1.8.1,apply constraints in-place
v1.8.1,fixme: work-around until nn.Embedding supports complex
v1.8.1,fixme: work-around until nn.Embedding supports complex
v1.8.1,verify that contiguity is preserved
v1.8.1,"get all base representations, shape: (num_bases, *shape)"
v1.8.1,"get base weights, shape: (*batch_dims, num_bases)"
v1.8.1,"weighted linear combination of bases, shape: (*batch_dims, *shape)"
v1.8.1,normalize output dimension
v1.8.1,entity-relation composition
v1.8.1,edge weighting
v1.8.1,message passing weights
v1.8.1,linear relation transformation
v1.8.1,layer-specific self-loop relation representation
v1.8.1,other components
v1.8.1,initialize
v1.8.1,split
v1.8.1,compose
v1.8.1,transform
v1.8.1,normalization
v1.8.1,aggregate by sum
v1.8.1,dropout
v1.8.1,prepare for inverse relations
v1.8.1,update entity representations: mean over self-loops / forward edges / backward edges
v1.8.1,Relation transformation
v1.8.1,has to be imported here to avoid cyclic imports
v1.8.1,kwargs
v1.8.1,Buffered enriched entity and relation representations
v1.8.1,TODO: Check
v1.8.1,hidden dimension normalization
v1.8.1,Create message passing layers
v1.8.1,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.8.1,TODO: This always uses all training triples for message passing
v1.8.1,initialize buffer of enriched representations
v1.8.1,invalidate enriched embeddings
v1.8.1,"when changing from evaluation to training mode, the buffered representations have been computed without"
v1.8.1,"gradient tracking. hence, we need to invalidate them."
v1.8.1,note: this occurs in practice when continuing training after evaluation.
v1.8.1,enrich
v1.8.1,infer shape
v1.8.1,"assign after super, since they should be properly registered as submodules"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,scaling factor
v1.8.1,"modulus ~ Uniform[-s, s]"
v1.8.1,"phase ~ Uniform[0, 2*pi]"
v1.8.1,real part
v1.8.1,purely imaginary quaternions unitary
v1.8.1,this is usually loaded from somewhere else
v1.8.1,"the shape must match, as well as the entity-to-id mapping"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: whether the edge weighting needs access to the message
v1.8.1,stub init to enable arbitrary arguments in subclasses
v1.8.1,"Calculate in-degree, i.e. number of incoming edges"
v1.8.1,backward compatibility with RGCN
v1.8.1,view for heads
v1.8.1,"compute attention coefficients, shape: (num_edges, num_heads)"
v1.8.1,"TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO test
v1.8.1,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.8.1,: a = \mu^T\Sigma^{-1}\mu
v1.8.1,: b = \log \det \Sigma
v1.8.1,1. Component
v1.8.1,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.8.1,2. Component
v1.8.1,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.8.1,with mu = (mu_1 - mu_0)
v1.8.1,= mu * Sigma_1^-1 mu
v1.8.1,since Sigma_1 is diagonal
v1.8.1,= mu**2 / sigma_1
v1.8.1,3. Component
v1.8.1,4. Component
v1.8.1,ln (det(\Sigma_1) / det(\Sigma_0))
v1.8.1,= ln det Sigma_1 - ln det Sigma_0
v1.8.1,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.8.1,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.8.1,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.8.1,allocate result
v1.8.1,prepare distributions
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO benchmark
v1.8.1,TODO benchmark
v1.8.1,TODO benchmark
v1.8.1,TODO benchmark
v1.8.1,TODO benchmark
v1.8.1,TODO benchmark
v1.8.1,TODO benchmark
v1.8.1,TODO benchmark
v1.8.1,TODO benchmark
v1.8.1,"h = h_re, -h_im"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Adapter classes
v1.8.1,Concrete Classes
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Base Classes
v1.8.1,Adapter classes
v1.8.1,Concrete Classes
v1.8.1,normalize input
v1.8.1,get number of head/relation/tail representations
v1.8.1,flatten list
v1.8.1,split tensors
v1.8.1,broadcasting
v1.8.1,yield batches
v1.8.1,complex typing
v1.8.1,: The symbolic shapes for entity representations
v1.8.1,": The symbolic shapes for entity representations for tail entities, if different."
v1.8.1,": Otherwise, the entity_shape is used for head & tail entities"
v1.8.1,: The symbolic shapes for relation representations
v1.8.1,if the interaction function's head parameter should only receive a subset of entity representations
v1.8.1,if the interaction function's tail parameter should only receive a subset of entity representations
v1.8.1,"TODO: cannot cover dynamic shapes, e.g., AutoSF"
v1.8.1,"TODO: we could change that to slicing along multiple dimensions, if necessary"
v1.8.1,"The appended ""e"" represents the literals that get concatenated"
v1.8.1,on the entity representations. It does not necessarily have the
v1.8.1,"same dimension ""d"" as the entity representations."
v1.8.1,alternate way of combining entity embeddings + literals
v1.8.1,"h = torch.cat(h, dim=-1)"
v1.8.1,"h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore"
v1.8.1,"t = torch.cat(t, dim=-1)"
v1.8.1,"t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore"
v1.8.1,: The functional interaction form
v1.8.1,Store initial input for error message
v1.8.1,All are None -> try and make closest to square
v1.8.1,Only input channels is None
v1.8.1,Only width is None
v1.8.1,Only height is none
v1.8.1,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.8.1,Width and input channels are None -> set input channels to 1 and calculate width
v1.8.1,vector & scalar offset
v1.8.1,": The head-relation encoder operating on 2D ""images"""
v1.8.1,: The head-relation encoder operating on the 1D flattened version
v1.8.1,: The interaction function
v1.8.1,Automatic calculation of remaining dimensions
v1.8.1,Parameter need to fulfil:
v1.8.1,input_channels * embedding_height * embedding_width = embedding_dim
v1.8.1,encoders
v1.8.1,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.8.1,"2: 1D encoder: FC, DO, BN?, Act"
v1.8.1,store reshaping dimensions
v1.8.1,The interaction model
v1.8.1,Use Xavier initialization for weight; bias to zero
v1.8.1,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.8.1,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.8.1,Initialize biases with zero
v1.8.1,"In the original formulation,"
v1.8.1,Global entity projection
v1.8.1,Global relation projection
v1.8.1,Global combination bias
v1.8.1,Global combination bias
v1.8.1,default core tensor initialization
v1.8.1,cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12
v1.8.1,normalize initializer
v1.8.1,normalize relation dimension
v1.8.1,Core tensor
v1.8.1,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.8.1,Dropout
v1.8.1,instantiate here to make module easily serializable
v1.8.1,"batch norm gets reset automatically, since it defines reset_parameters"
v1.8.1,shapes
v1.8.1,there are separate biases for entities in head and tail position
v1.8.1,the base interaction
v1.8.1,forward entity/relation shapes
v1.8.1,The parameters of the affine transformation: bias
v1.8.1,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.8.1,head position and bump
v1.8.1,relation box: head
v1.8.1,relation box: tail
v1.8.1,tail position and bump
v1.8.1,input normalization
v1.8.1,Core tensor
v1.8.1,initialize core tensor
v1.8.1,"r_head, r_mid, r_tail"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO: switch to einsum ?
v1.8.1,"return torch.real(torch.einsum(""...d, ...d, ...d -> ..."", h, r, torch.conj(t)))"
v1.8.1,"repeat if necessary, and concat head and relation"
v1.8.1,"shape: -1, num_input_channels, 2*height, width"
v1.8.1,"shape: -1, num_input_channels, 2*height, width"
v1.8.1,"-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.8.1,"reshape: (-1, dim) -> (*batch_dims, dim)"
v1.8.1,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.8.1,output_shape: batch_dims
v1.8.1,add bias term
v1.8.1,decompose convolution for faster computation in 1-n case
v1.8.1,"compute conv(stack(h, r, t))"
v1.8.1,prepare input shapes for broadcasting
v1.8.1,"(*batch_dims, 1, d)"
v1.8.1,"conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])"
v1.8.1,"here, kernel_size = (1, 3), C_in = 1, C_out = num_filters"
v1.8.1,"-> conv_head, conv_rel, conv_tail shapes: (num_filters,)"
v1.8.1,"reshape to (..., f, 1)"
v1.8.1,"convolve -> output.shape: (*, embedding_dim, num_filters)"
v1.8.1,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.8.1,"Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)"
v1.8.1,same shape
v1.8.1,"split, shape: (embedding_dim, hidden_dim)"
v1.8.1,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.8.1,"Predict t embedding, shape: (*batch_dims, d)"
v1.8.1,dot product
v1.8.1,"composite: (*batch_dims, d)"
v1.8.1,inner product with relation embedding
v1.8.1,Circular correlation of entity embeddings
v1.8.1,complex conjugate
v1.8.1,Hadamard product in frequency domain
v1.8.1,inverse real FFT
v1.8.1,global projections
v1.8.1,"combination, shape: (*batch_dims, d)"
v1.8.1,dot product with t
v1.8.1,r expresses a rotation in complex plane.
v1.8.1,rotate head by relation (=Hadamard product in complex space)
v1.8.1,rotate tail by inverse of relation
v1.8.1,The inverse rotation is expressed by the complex conjugate of r.
v1.8.1,The score is computed as the distance of the relation-rotated head to the tail.
v1.8.1,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.8.1,|h * r - t| = |h - conj(r) * t|
v1.8.1,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.8.1,"That is not mentioned in the paper, so it is made optional here."
v1.8.1,Project entities
v1.8.1,h projection to hyperplane
v1.8.1,r
v1.8.1,-t projection to hyperplane
v1.8.1,project to relation specific subspace
v1.8.1,ensure constraints
v1.8.1,x_1 contraction
v1.8.1,x_2 contraction
v1.8.1,Rotate (=Hamilton product in quaternion space).
v1.8.1,Rotation in quaternion space
v1.8.1,head interaction
v1.8.1,relation interaction (notice that h has been updated)
v1.8.1,combination
v1.8.1,similarity
v1.8.1,head
v1.8.1,relation box: head
v1.8.1,relation box: tail
v1.8.1,tail
v1.8.1,power norm
v1.8.1,the relation-specific head box base shape (normalized to have a volume of 1):
v1.8.1,the relation-specific tail box base shape (normalized to have a volume of 1):
v1.8.1,head
v1.8.1,relation
v1.8.1,tail
v1.8.1,version 2: relation factor offset
v1.8.1,extension: negative (power) norm
v1.8.1,note: normalization should be done from the representations
v1.8.1,cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501
v1.8.1,version 2
v1.8.1,r_head = r_head + u * torch.ones_like(r_head)
v1.8.1,r_tail = r_tail + u * torch.ones_like(r_tail)
v1.8.1,"stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)"
v1.8.1,"remember shape for output, but reshape for transformer"
v1.8.1,"get position embeddings, shape: (seq_len, dim)"
v1.8.1,Now we are position-dependent w.r.t qualifier pairs.
v1.8.1,"seq_length, batch_size, dim"
v1.8.1,Pool output
v1.8.1,"output shape: (batch_size, dim)"
v1.8.1,reshape
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Concrete classes
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: the token ID of the padding token
v1.8.1,: the token representations
v1.8.1,: the assigned tokens for each entity
v1.8.1,needs to be lazily imported to avoid cyclic imports
v1.8.1,fill padding (nn.Embedding cannot deal with negative indices)
v1.8.1,"sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs"
v1.8.1,"contain a subset of training relations) - for that, the padding index is the last index of the Representation"
v1.8.1,resolve token representation
v1.8.1,input validation
v1.8.1,register as buffer
v1.8.1,assign sub-module
v1.8.1,apply tokenizer
v1.8.1,"get token IDs, shape: (*, num_chosen_tokens)"
v1.8.1,"lookup token representations, shape: (*, num_chosen_tokens, *shape)"
v1.8.1,: the token representations
v1.8.1,normalize triples
v1.8.1,inverse triples are created afterwards implicitly
v1.8.1,tokenize
v1.8.1,determine shape
v1.8.1,super init; has to happen *before* any parameter or buffer is assigned
v1.8.1,assign module
v1.8.1,Assign default aggregation
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Resolver
v1.8.1,Base classes
v1.8.1,Concrete classes
v1.8.1,TODO: allow relative
v1.8.1,isin() preserves the sorted order
v1.8.1,sort by decreasing degree
v1.8.1,sort by decreasing page rank
v1.8.1,input normalization
v1.8.1,determine absolute number of anchors for each strategy
v1.8.1,if pre-instantiated
v1.8.1,input normalization
v1.8.1,power iteration
v1.8.1,"convert to sparse matrix, shape: (n, n)"
v1.8.1,symmetrize
v1.8.1,TODO: should we add self-links
v1.8.1,"adj = adj + scipy.sparse.eye(m=adj.shape[0], format=""coo"")"
v1.8.1,convert to CSR
v1.8.1,adjacency normalization
v1.8.1,TODO: vectorization?
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Resolver
v1.8.1,Base classes
v1.8.1,Concrete classes
v1.8.1,tokenize: represent entities by bag of relations
v1.8.1,collect candidates
v1.8.1,randomly sample without replacement num_tokens relations for each entity
v1.8.1,select anchors
v1.8.1,find closest anchors
v1.8.1,convert to torch
v1.8.1,verify pool
v1.8.1,choose first num_tokens
v1.8.1,TODO: vectorization?
v1.8.1,heuristic
v1.8.1,heuristic
v1.8.1,calculate configuration digest
v1.8.1,create anchor selection instance
v1.8.1,select anchors
v1.8.1,anchor search (=anchor assignment?)
v1.8.1,assign anchors
v1.8.1,save
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Resolver
v1.8.1,Base classes
v1.8.1,Concrete classes
v1.8.1,"contains: anchor_ids, entity_ids, mapping {entity_id -> {""ancs"": anchors, ""dists"": distances}}"
v1.8.1,normalize anchor_ids
v1.8.1,cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541
v1.8.1,TODO: keep distances?
v1.8.1,ensure parent directory exists
v1.8.1,save via torch.save
v1.8.1,"TODO: since we save a contiguous array of (num_entities, num_anchors),"
v1.8.1,"it would be more efficient to not convert to a mapping, but directly select from the tensor"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Anchor Searchers
v1.8.1,Anchor Selection
v1.8.1,Tokenizers
v1.8.1,Token Loaders
v1.8.1,Representations
v1.8.1,"TODO: use graph library, such as igraph, graph-tool, or networkit"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Resolver
v1.8.1,Base classes
v1.8.1,Concrete classes
v1.8.1,convert to adjacency matrix
v1.8.1,"compute distances between anchors and all nodes, shape: (num_anchors, num_entities)"
v1.8.1,select anchor IDs with smallest distance
v1.8.1,infer shape
v1.8.1,create adjacency matrix
v1.8.1,symmetric + self-loops
v1.8.1,"for each entity, determine anchor pool by BFS"
v1.8.1,an array storing whether node i is reachable by anchor j
v1.8.1,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.8.1,the output
v1.8.1,TODO: take all (q-1) hop neighbors before selecting from q-hop
v1.8.1,propagate one hop
v1.8.1,convergence check
v1.8.1,copy pool if we have seen enough anchors and have not yet stopped
v1.8.1,stop once we have enough
v1.8.1,TODO: can we replace this loop with something vectorized?
v1.8.1,"select k anchors with largest ppr, shape: (batch_size, k)"
v1.8.1,prepare adjacency matrix only once
v1.8.1,prepare result
v1.8.1,progress bar?
v1.8.1,batch-wise computation of PPR
v1.8.1,"create a batch of starting vectors, shape: (n, batch_size)"
v1.8.1,"run page-rank calculation, shape: (batch_size, n)"
v1.8.1,"select PPR values for the anchors, shape: (num_anchors, batch_size)"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Base classes
v1.8.1,Concrete classes
v1.8.1,
v1.8.1,
v1.8.1,
v1.8.1,
v1.8.1,
v1.8.1,Misc
v1.8.1,
v1.8.1,rank based metrics do not need binarized scores
v1.8.1,: the supported rank types. Most of the time equal to all rank types
v1.8.1,: whether the metric requires the number of candidates for each ranking task
v1.8.1,normalize confidence level
v1.8.1,sample metric values
v1.8.1,"bootstrap estimator (i.e., compute on sample with replacement)"
v1.8.1,cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function
v1.8.1,: The rank-based metric class that this derived metric extends
v1.8.1,"since scale and offset are constant for a given number of candidates, we have"
v1.8.1,E[scale * M + offset] = scale * E[M] + offset
v1.8.1,"since scale and offset are constant for a given number of candidates, we have"
v1.8.1,V[scale * M + offset] = scale^2 * V[M]
v1.8.1,: Z-adjusted metrics are formulated to be increasing
v1.8.1,: Z-adjusted metrics can only be applied to realistic ranks
v1.8.1,should be exactly 0.0
v1.8.1,should be exactly 1.0
v1.8.1,: Expectation/maximum reindexed metrics are formulated to be increasing
v1.8.1,: Expectation/maximum reindexed metrics can only be applied to realistic ranks
v1.8.1,should be exactly 0.0
v1.8.1,V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2
v1.8.1,use V[x] = E[x^2] - E[x]^2
v1.8.1,group by same weight -> compute H_w(n) for multiple n at once
v1.8.1,we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once
v1.8.1,now select from precomputed cumulative sums and aggregate
v1.8.1,"ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective"
v1.8.1,cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction
v1.8.1,TODO: should we return the sum of weights?
v1.8.1,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.8.1,"for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: the lower bound
v1.8.1,: whether the lower bound is inclusive
v1.8.1,: the upper bound
v1.8.1,: whether the upper bound is inclusive
v1.8.1,: The name of the metric
v1.8.1,: a link to further information
v1.8.1,: whether the metric needs binarized scores
v1.8.1,": whether it is increasing, i.e., larger values are better"
v1.8.1,: the value range
v1.8.1,: synonyms for this metric
v1.8.1,: whether the metric supports weights
v1.8.1,: whether there is a closed-form solution of the expectation
v1.8.1,: whether there is a closed-form solution of the variance
v1.8.1,normalize weights
v1.8.1,calculate weighted harmonic mean
v1.8.1,calculate cdf
v1.8.1,determine value at p=0.5
v1.8.1,special case for exactly 0.5
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: A description of the metric
v1.8.1,: The function that runs the metric
v1.8.1,: Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics
v1.8.1,: This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`
v1.8.1,"TODO there's something wrong with this, so add it later"
v1.8.1,classifier_annotator.higher(
v1.8.1,"rmc.pr_auc_score,"
v1.8.1,"name=""AUC-PR"","
v1.8.1,"description=""Area Under the Precision-Recall Curve"","
v1.8.1,"link=""https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score"","
v1.8.1,)
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,don't worry about functions because they can't be specified by JSON.
v1.8.1,Could make a better mo
v1.8.1,later could extend for other non-JSON valid types
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Score with original triples
v1.8.1,Score with inverse triples
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Create directory in which all experimental artifacts are saved
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,distribute the deteriorated triples across the remaining factories
v1.8.1,"'kinships',"
v1.8.1,"'umls',"
v1.8.1,"'codexsmall',"
v1.8.1,"'wn18',"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: Functions for specifying exotic resources with a given prefix
v1.8.1,: Functions for specifying exotic resources based on their file extension
v1.8.1,Input validation
v1.8.1,convert to numpy
v1.8.1,Additional columns
v1.8.1,convert PyTorch tensors to numpy
v1.8.1,convert to dataframe
v1.8.1,Re-order columns
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.8.1,TODO vectorize code
v1.8.1,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.8.1,save literal-to-id mapping
v1.8.1,save numeric literals
v1.8.1,load literal-to-id
v1.8.1,load literals
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Split triples
v1.8.1,Sorting ensures consistent results when the triples are permuted
v1.8.1,Create mapping
v1.8.1,Sorting ensures consistent results when the triples are permuted
v1.8.1,Create mapping
v1.8.1,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.8.1,Filter all non-existent triples
v1.8.1,Note: Unique changes the order of the triples
v1.8.1,Note: Using unique means implicit balancing of training samples
v1.8.1,normalize input
v1.8.1,TODO: method is_inverse?
v1.8.1,TODO: inverse of inverse?
v1.8.1,The number of relations stored in the triples factory includes the number of inverse relations
v1.8.1,Id of inverse relation: relation + 1
v1.8.1,: The mapping from labels to IDs.
v1.8.1,: The inverse mapping for label_to_id; initialized automatically
v1.8.1,: A vectorized version of entity_label_to_id; initialized automatically
v1.8.1,: A vectorized version of entity_id_to_label; initialized automatically
v1.8.1,Normalize input
v1.8.1,label
v1.8.1,Filter for entities
v1.8.1,Filter for relations
v1.8.1,No filter
v1.8.1,check new label to ID mappings
v1.8.1,Make new triples factories for each group
v1.8.1,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.1,prepare metadata
v1.8.1,Delegate to function
v1.8.1,"restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed"
v1.8.1,load base
v1.8.1,load numeric triples
v1.8.1,store numeric triples
v1.8.1,store metadata
v1.8.1,Check if the triples are inverted already
v1.8.1,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.8.1,contained if and only if create_inverse_triples is True.
v1.8.1,Generate entity mapping if necessary
v1.8.1,Generate relation mapping if necessary
v1.8.1,Map triples of labels to triples of IDs.
v1.8.1,TODO: Check if lazy evaluation would make sense
v1.8.1,store entity/relation to ID
v1.8.1,load entity/relation to ID
v1.8.1,pre-filter to keep only topk
v1.8.1,if top is larger than the number of available options
v1.8.1,generate text
v1.8.1,vectorized label lookup
v1.8.1,Re-order columns
v1.8.1,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.8.1,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Split indices
v1.8.1,Split triples
v1.8.1,select one triple per relation
v1.8.1,maintain set of covered entities
v1.8.1,"Select one triple for each head/tail entity, which is not yet covered."
v1.8.1,create mask
v1.8.1,Prepare split index
v1.8.1,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.8.1,[...] is necessary for Python 3.7 compatibility
v1.8.1,base cases
v1.8.1,IDs not in training
v1.8.1,triples with exclusive test IDs
v1.8.1,While there are still triples that should be moved to the training set
v1.8.1,Pick a random triple to move over to the training triples
v1.8.1,add to training
v1.8.1,remove from testing
v1.8.1,Recalculate the move_id_mask
v1.8.1,Make sure that the first element has all the right stuff in it
v1.8.1,backwards compatibility
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,constants
v1.8.1,constants
v1.8.1,unary
v1.8.1,binary
v1.8.1,ternary
v1.8.1,column names
v1.8.1,return candidates
v1.8.1,index triples
v1.8.1,incoming relations per entity
v1.8.1,outgoing relations per entity
v1.8.1,indexing triples for fast join r1 & r2
v1.8.1,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.8.1,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.8.1,actual evaluation of the pattern
v1.8.1,skip empty support
v1.8.1,TODO: Can this happen after pre-filtering?
v1.8.1,"sort first, for triple order invariance"
v1.8.1,TODO: what is the support?
v1.8.1,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.8.1,sort decreasingly. i dominates j for all j > i in x-dimension
v1.8.1,"if it is also dominated by any y, it is not part of the skyline"
v1.8.1,"group by (relation id, pattern type)"
v1.8.1,"for each group, yield from skyline"
v1.8.1,determine patterns from triples
v1.8.1,drop zero-confidence
v1.8.1,keep only skyline
v1.8.1,create data frame
v1.8.1,iterate relation types
v1.8.1,drop zero-confidence
v1.8.1,keep only skyline
v1.8.1,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.8.1,base = skyline(base)
v1.8.1,create data frame
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO: the same
v1.8.1,": the positive triples, shape: (batch_size, 3)"
v1.8.1,": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)"
v1.8.1,": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)"
v1.8.1,TODO: some negative samplers require batches
v1.8.1,"shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.8.1,"each shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.8.1,cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset
v1.8.1,indexing
v1.8.1,initialize
v1.8.1,sample iteratively
v1.8.1,determine weights
v1.8.1,randomly choose a vertex which has not been chosen yet
v1.8.1,normalize to probabilities
v1.8.1,sample a start node
v1.8.1,get list of neighbors
v1.8.1,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.8.1,visit target node
v1.8.1,decrease sample counts
v1.8.1,convert to csr for fast row slicing
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,check validity
v1.8.1,path compression
v1.8.1,collect connected components using union find with path compression
v1.8.1,get representatives
v1.8.1,already merged
v1.8.1,make x the smaller one
v1.8.1,merge
v1.8.1,extract partitions
v1.8.1,safe division for empty sets
v1.8.1,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.8.1,duplicates
v1.8.1,we are not interested in self-similarity
v1.8.1,compute similarities
v1.8.1,Calculate which relations are the inverse ones
v1.8.1,get existing IDs
v1.8.1,remove non-existing ID from label mapping
v1.8.1,create translation tensor
v1.8.1,get entities and relations occurring in triples
v1.8.1,generate ID translation and new label to Id mappings
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.8.1,seamless loading and saving of training checkpoints
v1.8.1,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.8.1,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.8.1,"If a checkpoint file is given, it must be loaded if it exists already"
v1.8.1,"If the stopper dict has any keys, those are written back to the stopper"
v1.8.1,The checkpoint frequency needs to be set to save checkpoints
v1.8.1,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.8.1,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.8.1,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.8.1,send model to device before going into the internal training loop
v1.8.1,Ensure the release of memory
v1.8.1,Clear optimizer
v1.8.1,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.8.1,due to the patience continue to train after the best epoch and thus alter the model
v1.8.1,Create a path
v1.8.1,Prepare all of the callbacks
v1.8.1,"Register a callback for the result tracker, if given"
v1.8.1,"Register a callback for the early stopper, if given"
v1.8.1,TODO should mode be passed here?
v1.8.1,"Take the biggest possible training batch_size, if batch_size not set"
v1.8.1,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.8.1,This will find necessary parameters to optimize the use of the hardware at hand
v1.8.1,return the relevant parameters slice_size and batch_size
v1.8.1,Force weight initialization if training continuation is not explicitly requested.
v1.8.1,Reset the weights
v1.8.1,"afterwards, some parameters may be on the wrong device"
v1.8.1,Create new optimizer
v1.8.1,Create a new lr scheduler and add the optimizer
v1.8.1,Ensure the model is on the correct device
v1.8.1,"When size probing, we don't want progress bars"
v1.8.1,Create progress bar
v1.8.1,Save the time to track when the saved point was available
v1.8.1,Training Loop
v1.8.1,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.8.1,Enforce training mode
v1.8.1,Accumulate loss over epoch
v1.8.1,Batching
v1.8.1,Only create a progress bar when not in size probing mode
v1.8.1,Flag to check when to quit the size probing
v1.8.1,Recall that torch *accumulates* gradients. Before passing in a
v1.8.1,"new instance, you need to zero out the gradients from the old instance"
v1.8.1,Get batch size of current batch (last batch may be incomplete)
v1.8.1,accumulate gradients for whole batch
v1.8.1,forward pass call
v1.8.1,"when called by batch_size_search(), the parameter update should not be applied."
v1.8.1,update parameters according to optimizer
v1.8.1,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.8.1,constraints are no longer applied
v1.8.1,For testing purposes we're only interested in processing one batch
v1.8.1,When size probing we don't need the losses
v1.8.1,Update learning rate scheduler
v1.8.1,Track epoch loss
v1.8.1,Print loss information to console
v1.8.1,Save the last successful finished epoch
v1.8.1,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.8.1,During automatic memory optimization only the error message is of interest
v1.8.1,When there wasn't a best epoch the checkpoint path should be None
v1.8.1,Delete temporary best epoch model
v1.8.1,Includes a call to result_tracker.log_metrics
v1.8.1,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.8.1,MyPy overrides are because you should
v1.8.1,When there wasn't a best epoch the checkpoint path should be None
v1.8.1,Delete temporary best epoch model
v1.8.1,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.8.1,at that state
v1.8.1,Delete temporary best epoch model
v1.8.1,forward pass
v1.8.1,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.8.1,correction for loss reduction
v1.8.1,backward pass
v1.8.1,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.8.1,Set upper bound
v1.8.1,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.8.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.1,"Only if a cuda device is available, the random state is accessed"
v1.8.1,This is an entire checkpoint for the optional best model when using early stopping
v1.8.1,Saving triples factory related states
v1.8.1,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.8.1,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.8.1,Check whether the triples factory mappings match those from the checkpoints
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Shuffle each epoch
v1.8.1,Lazy-splitting into batches
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,disable automatic batching
v1.8.1,Slicing is not possible in sLCWA training loops
v1.8.1,split batch
v1.8.1,send to device
v1.8.1,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.8.1,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.8.1,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.8.1,Compute negative and positive scores
v1.8.1,Slicing is not possible for sLCWA
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO how to pass inductive mode
v1.8.1,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.8.1,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.8.1,: A hint for constructing a :class:`MultiTrainingCallback`
v1.8.1,: A collection of callbacks
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,normalize target column
v1.8.1,The type inference is so confusing between the function switching
v1.8.1,and polymorphism introduced by slicability that these need to be ignored
v1.8.1,Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number
v1.8.1,of total entities from another inductive inference factory
v1.8.1,Split batch components
v1.8.1,Send batch to device
v1.8.1,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.8.1,"must have failed to start slice_size search, we start with trying half the entities."
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,To make MyPy happy
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,now: smaller is better
v1.8.1,: the number of reported results with no improvement after which training will be stopped
v1.8.1,the minimum relative improvement necessary to consider it an improved result
v1.8.1,"whether a larger value is better, or a smaller."
v1.8.1,: The epoch at which the best result occurred
v1.8.1,: The best result so far
v1.8.1,: The remaining patience
v1.8.1,check for improvement
v1.8.1,stop if the result did not improve more than delta for patience evaluations
v1.8.1,: The model
v1.8.1,: The evaluator
v1.8.1,: The triples to use for training (to be used during filtered evaluation)
v1.8.1,: The triples to use for evaluation
v1.8.1,: Size of the evaluation batches
v1.8.1,: Slice size of the evaluation batches
v1.8.1,: The number of epochs after which the model is evaluated on validation set
v1.8.1,: The number of iterations (one iteration can correspond to various epochs)
v1.8.1,: with no improvement after which training will be stopped.
v1.8.1,: The name of the metric to use
v1.8.1,: The minimum relative improvement necessary to consider it an improved result
v1.8.1,: The metric results from all evaluations
v1.8.1,": Whether a larger value is better, or a smaller"
v1.8.1,: The result tracker
v1.8.1,: Callbacks when after results are calculated
v1.8.1,: Callbacks when training gets continued
v1.8.1,: Callbacks when training is stopped early
v1.8.1,: Did the stopper ever decide to stop?
v1.8.1,TODO: Fix this
v1.8.1,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.8.1,raise ValueError(f'Invalid metric name: {self.metric}')
v1.8.1,Evaluate
v1.8.1,Only perform time consuming checks for the first call.
v1.8.1,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.8.1,Append to history
v1.8.1,TODO need a test that this all re-instantiates properly
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Utils
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,parsing metrics
v1.8.1,metric pattern = side?.type?.metric.k?
v1.8.1,: The metric key
v1.8.1,": Side of the metric, or ""both"""
v1.8.1,: The rank type
v1.8.1,normalize metric name
v1.8.1,normalize side
v1.8.1,normalize rank type
v1.8.1,normalize keys
v1.8.1,TODO: this can only normalize rank-based metrics!
v1.8.1,TODO: find a better way to handle this
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO: fix this upstream / make metric.score comply to signature
v1.8.1,docstr-coverage:inherited
v1.8.1,docstr-coverage:inherited
v1.8.1,Transfer to cpu and convert to numpy
v1.8.1,Ensure that each key gets counted only once
v1.8.1,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.8.1,docstr-coverage:inherited
v1.8.1,"Because the order of the values of an dictionary is not guaranteed,"
v1.8.1,we need to retrieve scores and masks using the exact same key order.
v1.8.1,Clear buffers
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The optimistic rank is the rank when assuming all options with an equal score are placed
v1.8.1,: behind the current test triple.
v1.8.1,": shape: (batch_size,)"
v1.8.1,": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank"
v1.8.1,: over all permutations of the elements with the same score as the currently considered option.
v1.8.1,": shape: (batch_size,)"
v1.8.1,: The pessimistic rank is the rank when assuming all options with an equal score are placed
v1.8.1,: in front of current test triple.
v1.8.1,": shape: (batch_size,)"
v1.8.1,: The number of options is the number of items considered in the ranking. It may change for
v1.8.1,: filtered evaluation
v1.8.1,": shape: (batch_size,)"
v1.8.1,The optimistic rank is the rank when assuming all options with an
v1.8.1,"equal score are placed behind the currently considered. Hence, the"
v1.8.1,"rank is the number of options with better scores, plus one, as the"
v1.8.1,rank is one-based.
v1.8.1,The pessimistic rank is the rank when assuming all options with an
v1.8.1,"equal score are placed in front of the currently considered. Hence,"
v1.8.1,the rank is the number of options which have at least the same score
v1.8.1,minus one (as the currently considered option in included in all
v1.8.1,"options). As the rank is one-based, we have to add 1, which nullifies"
v1.8.1,"the ""minus 1"" from before."
v1.8.1,The realistic rank is the average of the optimistic and pessimistic
v1.8.1,"rank, and hence the expected rank over all permutations of the elements"
v1.8.1,with the same score as the currently considered option.
v1.8.1,"We set values which should be ignored to NaN, hence the number of options"
v1.8.1,which should be considered is given by
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"TODO remove this, it makes code much harder to reason about"
v1.8.1,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.8.1,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.8.1,Clear the ranks from the current evaluator
v1.8.1,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.8.1,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.8.1,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.8.1,"must have failed to start slice_size search, we start with trying half the entities."
v1.8.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.1,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.8.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.1,values_dict[key] will always be an int at this point
v1.8.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.1,Test if slicing is implemented for the required functions of this model
v1.8.1,Split batch
v1.8.1,Bind shape
v1.8.1,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.8.1,Warn if all entities will be filtered
v1.8.1,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.8.1,TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place
v1.8.1,verify that the triples have been filtered
v1.8.1,Filter triples if necessary
v1.8.1,Send to device
v1.8.1,Ensure evaluation mode
v1.8.1,Prepare for result filtering
v1.8.1,Send tensors to device
v1.8.1,Prepare batches
v1.8.1,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.8.1,Show progressbar
v1.8.1,Flag to check when to quit the size probing
v1.8.1,Disable gradient tracking
v1.8.1,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.8.1,batch-wise processing
v1.8.1,If we only probe sizes we do not need more than one batch
v1.8.1,Finalize
v1.8.1,Create filter
v1.8.1,Select scores of true
v1.8.1,overwrite filtered scores
v1.8.1,The scores for the true triples have to be rewritten to the scores tensor
v1.8.1,the rank-based evaluators needs the true scores with trailing 1-dim
v1.8.1,Create a positive mask with the size of the scores from the positive filter
v1.8.1,Restrict to entities of interest
v1.8.1,process scores
v1.8.1,optionally restrict triples (nop if no restriction)
v1.8.1,evaluation triples as dataframe
v1.8.1,determine filter triples
v1.8.1,infer num_entities if not given
v1.8.1,"TODO: unique, or max ID + 1?"
v1.8.1,optionally restrict triples
v1.8.1,compute candidate set sizes for different targets
v1.8.1,TODO: extend to relations?
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,terminate early if there are no ranks
v1.8.1,flatten dictionaries
v1.8.1,individual side
v1.8.1,combined
v1.8.1,docstr-coverage:inherited
v1.8.1,docstr-coverage:inherited
v1.8.1,docstr-coverage:inherited
v1.8.1,docstr-coverage:inherited
v1.8.1,Clear buffers
v1.8.1,repeat
v1.8.1,default for inductive LP by [teru2020]
v1.8.1,verify input
v1.8.1,docstr-coverage:inherited
v1.8.1,TODO: do not require to compute all scores beforehand
v1.8.1,super.evaluation assumes that the true scores are part of all_scores
v1.8.1,write back correct num_entities
v1.8.1,TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?
v1.8.1,compute macro weights
v1.8.1,docstr-coverage:inherited
v1.8.1,docstr-coverage:inherited
v1.8.1,Clear buffers
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.8.1,to create the class
v1.8.1,TODO: update to hint + kwargs
v1.8.1,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default regularizer class
v1.8.1,: The default parameters for the default regularizer class
v1.8.1,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.8.1,Make sure that all modules with parameters do have a reset_parameters method.
v1.8.1,Recursively visit all sub-modules
v1.8.1,skip self
v1.8.1,Track parents for blaming
v1.8.1,call reset_parameters if possible
v1.8.1,initialize from bottom to top
v1.8.1,This ensures that specialized initializations will take priority over the default ones of its components.
v1.8.1,emit warning if there where parameters which were not initialised by reset_parameters.
v1.8.1,Additional debug information
v1.8.1,TODO: allow max_id being present in representation_kwargs; if it matches max_id
v1.8.1,TODO: we could infer some shapes from the given interaction shape information
v1.8.1,check max-id
v1.8.1,check shapes
v1.8.1,: The entity representations
v1.8.1,: The relation representations
v1.8.1,: The weight regularizers
v1.8.1,: The interaction function
v1.8.1,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.8.1,"model.modules(). Thereby, we can collect them automatically."
v1.8.1,Explicitly call reset_parameters to trigger initialization
v1.8.1,normalize input
v1.8.1,Note: slicing cannot be used here: the indices for score_hrt only have a batch
v1.8.1,"dimension, and slicing along this dimension is already considered by sub-batching."
v1.8.1,Note: we do not delegate to the general method for performance reasons
v1.8.1,Note: repetition is not necessary here
v1.8.1,normalization
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,train model
v1.8.1,"note: as this is an example, the model is only trained for a few epochs,"
v1.8.1,"but not until convergence. In practice, you would usually first verify that"
v1.8.1,"the model is sufficiently good in prediction, before looking at uncertainty scores"
v1.8.1,predict triple scores with uncertainty
v1.8.1,"use a larger number of samples, to increase quality of uncertainty estimate"
v1.8.1,get most and least uncertain prediction on training set
v1.8.1,: The scores
v1.8.1,": The uncertainty, in the same shape as scores"
v1.8.1,Enforce evaluation mode
v1.8.1,set dropout layers to training mode
v1.8.1,draw samples
v1.8.1,compute mean and std
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Train a model (quickly)
v1.8.1,Get scores for *all* triples
v1.8.1,Get scores for top 15 triples
v1.8.1,initialize buffer on device
v1.8.1,"reshape, shape: (batch_size * num_entities,)"
v1.8.1,get top scores within batch
v1.8.1,append to global top scores
v1.8.1,reduce size if necessary
v1.8.1,initialize buffer on cpu
v1.8.1,Explicitly create triples
v1.8.1,"TODO: in the future, we may want to expose this method"
v1.8.1,set model to evaluation mode
v1.8.1,calculate batch scores
v1.8.1,base case: infer maximum batch size
v1.8.1,base case: single batch
v1.8.1,TODO: this could happen because of AMO
v1.8.1,TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043
v1.8.1,no OOM error.
v1.8.1,make sure triples are a numpy array
v1.8.1,make sure triples are 2d
v1.8.1,convert to ID-based
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"This empty 1-element tensor doesn't actually do anything,"
v1.8.1,but is necessary since models with no grad params blow
v1.8.1,up the optimizer
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default loss function class
v1.8.1,: The default parameters for the default loss function class
v1.8.1,: The instance of the loss
v1.8.1,Random seeds have to set before the embeddings are initialized
v1.8.1,Loss
v1.8.1,TODO: why do we need to empty the cache?
v1.8.1,"TODO: this currently compute (batch_size, num_relations) instead,"
v1.8.1,"i.e., scores for normal and inverse relations"
v1.8.1,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.8.1,send to device
v1.8.1,special handling of inverse relations
v1.8.1,"when trained on inverse relations, the internal relation ID is twice the original relation ID"
v1.8.1,: The default regularizer class
v1.8.1,: The default parameters for the default regularizer class
v1.8.1,: The instance of the regularizer
v1.8.1,Regularizer
v1.8.1,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.8.1,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.8.1,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.8.1,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.8.1,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.8.1,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.8.1,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.8.1,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.8.1,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.8.1,: Primary embeddings for entities
v1.8.1,: Primary embeddings for relations
v1.8.1,"make sure to call this first, to reset regularizer state!"
v1.8.1,The following lines add in a post-init hook to all subclasses
v1.8.1,such that the reset_parameters_() function is run
v1.8.1,"sorry mypy, but this kind of evil must be permitted."
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Base Models
v1.8.1,Concrete Models
v1.8.1,Inductive Models
v1.8.1,Evaluation-only models
v1.8.1,Utils
v1.8.1,Abstract Models
v1.8.1,We might be able to relax this later
v1.8.1,baseline models behave differently
v1.8.1,Old style models should never be looked up
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Create an MLP for string aggregation
v1.8.1,always create representations for normal and inverse relations and padding
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,default composition is DistMult-style
v1.8.1,Saving edge indices for all the supplied splits
v1.8.1,Extract all entity and relation representations
v1.8.1,Perform message passing and get updated states
v1.8.1,Use updated entity and relation states to extract requested IDs
v1.8.1,TODO I got lost in all the Representation Modules and shape casting and wrote this ;(
v1.8.1,normalization
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,NodePiece
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO rethink after RGCN update
v1.8.1,TODO: other parameters?
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default loss function class
v1.8.1,: The default parameters for the default loss function class
v1.8.1,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.8.1,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.8.1,ConvE should be trained with inverse triples
v1.8.1,entity embedding
v1.8.1,ConvE uses one bias for each entity
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,head representation
v1.8.1,tail representation
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default loss function class
v1.8.1,: The default parameters for the default loss function class
v1.8.1,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.8.1,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.8.1,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.8.1,use torch's native complex data type
v1.8.1,use torch's native complex data type
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The regularizer used by [nickel2011]_ for for RESCAL
v1.8.1,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.8.1,: a normalized weight of 10 is used.
v1.8.1,: The LP settings used by [nickel2011]_ for for RESCAL
v1.8.1,Get embeddings
v1.8.1,"shape: (b, d)"
v1.8.1,"shape: (b, d, d)"
v1.8.1,"shape: (b, d)"
v1.8.1,Compute scores
v1.8.1,Regularization
v1.8.1,Compute scores
v1.8.1,Regularization
v1.8.1,Get embeddings
v1.8.1,Compute scores
v1.8.1,Regularization
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.8.1,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.8.1,In the code base only the weights of the output layer are used for regularization
v1.8.1,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,comment:
v1.8.1,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.8.1,uses float64
v1.8.1,entity bias for head
v1.8.1,entity bias for tail
v1.8.1,relation offset
v1.8.1,diagonal relation transformation matrix
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default entity normalizer parameters
v1.8.1,: The entity representations are normalized to L2 unit length
v1.8.1,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.8.1,"update initializer settings, cf."
v1.8.1,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.8.1,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.8.1,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.8.1,in the original implementation the embeddings are initialized in one parameter
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,"w: (k, d, d)"
v1.8.1,"vh: (k, d)"
v1.8.1,"vt: (k, d)"
v1.8.1,"b: (k,)"
v1.8.1,"u: (k,)"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The regularizer used by [yang2014]_ for DistMult
v1.8.1,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.8.1,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.8.1,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.8.1,: The LP settings used by [yang2014]_ for DistMult
v1.8.1,note: DistMult only regularizes the relation embeddings;
v1.8.1,entity embeddings are hard constrained instead
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default settings for the entity constrainer
v1.8.1,mean
v1.8.1,diagonal covariance
v1.8.1,Ensure positive definite covariances matrices and appropriate size by clamping
v1.8.1,mean
v1.8.1,diagonal covariance
v1.8.1,Ensure positive definite covariances matrices and appropriate size by clamping
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,diagonal entries
v1.8.1,off-diagonal
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The custom regularizer used by [wang2014]_ for TransH
v1.8.1,: The settings used by [wang2014]_ for TransH
v1.8.1,embeddings
v1.8.1,Normalise the normal vectors by their l2 norms
v1.8.1,TODO: Add initialization
v1.8.1,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.8.1,which enforces the defined soft constraints.
v1.8.1,Get embeddings
v1.8.1,Project to hyperplane
v1.8.1,Regularization term
v1.8.1,Get embeddings
v1.8.1,Project to hyperplane
v1.8.1,Regularization term
v1.8.1,Get embeddings
v1.8.1,Project to hyperplane
v1.8.1,Regularization term
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,TODO: Initialize from TransE
v1.8.1,relation embedding
v1.8.1,relation projection
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,": The default strategy for optimizing the model""s hyper-parameters"
v1.8.1,TODO: Decomposition kwargs
v1.8.1,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.8.1,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.8.1,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.8.1,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.8.1,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,combined representation
v1.8.1,Resolve interaction function
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default loss function class
v1.8.1,: The default parameters for the default loss function class
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,Get embeddings
v1.8.1,TODO: Use torch.cdist
v1.8.1,"There were some performance/memory issues with cdist, cf."
v1.8.1,"https://github.com/pytorch/pytorch/issues?q=cdist however, @mberr thinks"
v1.8.1,they are mostly resolved by now. A Benefit would be that we can harness the
v1.8.1,"future (performance) improvements made by the core torch developers. However,"
v1.8.1,this will require some benchmarking.
v1.8.1,Get embeddings
v1.8.1,TODO: Use torch.cdist (see note above in score_hrt())
v1.8.1,Get embeddings
v1.8.1,TODO: Use torch.cdist (see note above in score_hrt())
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,entity bias for head
v1.8.1,relation position head
v1.8.1,relation shape head
v1.8.1,relation size head
v1.8.1,relation position tail
v1.8.1,relation shape tail
v1.8.1,relation size tail
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default loss function class
v1.8.1,: The default parameters for the default loss function class
v1.8.1,: The regularizer used by [trouillon2016]_ for SimplE
v1.8.1,": In the paper, they use weight of 0.1, and do not normalize the"
v1.8.1,": regularization term by the number of elements, which is 200."
v1.8.1,: The power sum settings used by [trouillon2016]_ for SimplE
v1.8.1,(head) entity
v1.8.1,tail entity
v1.8.1,relations
v1.8.1,inverse relations
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.8.1,weight initialization
v1.8.1,Get embeddings
v1.8.1,Embedding Regularization
v1.8.1,Concatenate them
v1.8.1,Compute scores
v1.8.1,Get embeddings
v1.8.1,Embedding Regularization
v1.8.1,First layer can be unrolled
v1.8.1,Send scores through rest of the network
v1.8.1,Get embeddings
v1.8.1,Embedding Regularization
v1.8.1,First layer can be unrolled
v1.8.1,Send scores through rest of the network
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,Regular relation embeddings
v1.8.1,The relation-specific interaction vector
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Create an MLP for string aggregation
v1.8.1,always create representations for normal and inverse relations and padding
v1.8.1,normalize embedding specification
v1.8.1,prepare token representations & kwargs
v1.8.1,"max_id=triples_factory.num_relations,  # will get added by ERModel"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default loss function class
v1.8.1,: The default parameters for the default loss function class
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Normalize relation embeddings
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default loss function class
v1.8.1,: The default parameters for the default loss function class
v1.8.1,: The LP settings used by [zhang2019]_ for QuatE.
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default settings for the entity constrainer
v1.8.1,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.8.1,Circular correlation of entity embeddings
v1.8.1,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.8.1,compatibility: new style fft returns complex tensor
v1.8.1,Hadamard product in frequency domain
v1.8.1,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.8.1,inner product with relation embedding
v1.8.1,Embedding Regularization
v1.8.1,Embedding Regularization
v1.8.1,Embedding Regularization
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default loss function class
v1.8.1,: The default parameters for the default loss function class
v1.8.1,Get embeddings
v1.8.1,Embedding Regularization
v1.8.1,Concatenate them
v1.8.1,Predict t embedding
v1.8.1,compare with all t's
v1.8.1,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.8.1,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.1,Embedding Regularization
v1.8.1,Concatenate them
v1.8.1,Predict t embedding
v1.8.1,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.1,Embedding Regularization
v1.8.1,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.8.1,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.8.1,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.8.1,Extend t
v1.8.1,Concatenate them
v1.8.1,Predict t embedding
v1.8.1,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.8.1,The results have to be realigned with the expected output of the score_h function
v1.8.1,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default parameters for the default loss function class
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default loss function class
v1.8.1,: The default parameters for the default loss function class
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the model's hyper-parameters
v1.8.1,: The default parameters for the default loss function class
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"max_id=max_id,  # will be added by ERModel"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,create sparse matrix of absolute counts
v1.8.1,normalize to relative counts
v1.8.1,base case
v1.8.1,"note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could"
v1.8.1,"stay sparse here, with a potential of a huge memory benefit on large datasets!"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,These operations are deterministic and a random seed can be fixed
v1.8.1,just to avoid warnings
v1.8.1,compute relation similarity matrix
v1.8.1,mapping from relations to head/tail entities
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"if we really need access to the path later, we can expose it as a property"
v1.8.1,via self.writer.log_dir
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The WANDB run
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The name of the run
v1.8.1,": The configuration dictionary, a mapping from name -> value"
v1.8.1,: Should metrics be stored when running ``log_metrics()``?
v1.8.1,": The metrics, a mapping from step -> (name -> value)"
v1.8.1,: A hint for constructing a :class:`MultiResultTracker`
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Base classes
v1.8.1,Concrete classes
v1.8.1,Utilities
v1.8.1,always add a Python result tracker for storing the configuration
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The file extension for this writer (do not include dot)
v1.8.1,: The file where the results are written to.
v1.8.1,as_uri() requires the path to be absolute. resolve additionally also normalizes the path
v1.8.1,: The column names
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,store set of triples
v1.8.1,: some prime numbers for tuple hashing
v1.8.1,: The bit-array for the Bloom filter data structure
v1.8.1,Allocate bit array
v1.8.1,calculate number of hashing rounds
v1.8.1,index triples
v1.8.1,Store some meta-data
v1.8.1,pre-hash
v1.8.1,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,At least make sure to not replace the triples by the original value
v1.8.1,"To make sure we don't replace the {head, relation, tail} by the"
v1.8.1,original value we shift all values greater or equal than the original value by one up
v1.8.1,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.8.1,Set the indices
v1.8.1,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.8.1,Bind the total number of negatives to sample in this batch
v1.8.1,Equally corrupt all sides
v1.8.1,"Do not detach, as no gradients should flow into the indices."
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.8.1,: A filterer for negative batches
v1.8.1,create unfiltered negative batch by corruption
v1.8.1,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Utils
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.8.1,create index structure
v1.8.1,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.8.1,: The concatenated sorted sets of head/tail entities
v1.8.1,"shape: (batch_size, num_neg_per_pos, 3)"
v1.8.1,Uniformly sample from head/tail offsets
v1.8.1,get corresponding entity
v1.8.1,"and position within triple (0: head, 2: tail)"
v1.8.1,write into negative batch
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Preprocessing: Compute corruption probabilities
v1.8.1,"compute tph, i.e. the average number of tail entities per head"
v1.8.1,"compute hpt, i.e. the average number of head entities per tail"
v1.8.1,Set parameter for Bernoulli distribution
v1.8.1,Decide whether to corrupt head or tail
v1.8.1,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.8.1,flatten mask
v1.8.1,Tails are corrupted if heads are not corrupted
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The random seed used at the beginning of the pipeline
v1.8.1,: The model trained by the pipeline
v1.8.1,: The training triples
v1.8.1,: The training loop used by the pipeline
v1.8.1,: The losses during training
v1.8.1,: The results evaluated by the pipeline
v1.8.1,: How long in seconds did training take?
v1.8.1,: How long in seconds did evaluation take?
v1.8.1,: An early stopper
v1.8.1,: The configuration
v1.8.1,: Any additional metadata as a dictionary
v1.8.1,: The version of PyKEEN used to create these results
v1.8.1,: The git hash of PyKEEN used to create these results
v1.8.1,TODO use pathlib here
v1.8.1,"note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:"
v1.8.1,"when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless"
v1.8.1,note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()
v1.8.1,only one original value => assume this to be the mean
v1.8.1,multiple values => assume they correspond to individual trials
v1.8.1,metrics accumulates rows for a dataframe for comparison against the original reported results (if any)
v1.8.1,"TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)"
v1.8.1,summarize
v1.8.1,skip special parameters
v1.8.1,FIXME this should never happen.
v1.8.1,1. Dataset
v1.8.1,2. Model
v1.8.1,3. Loss
v1.8.1,4. Regularizer
v1.8.1,5. Optimizer
v1.8.1,5.1 Learning Rate Scheduler
v1.8.1,6. Training Loop
v1.8.1,7. Training (ronaldo style)
v1.8.1,8. Evaluation
v1.8.1,9. Tracking
v1.8.1,Misc
v1.8.1,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.8.1,used random_seed to ensure reproducible results
v1.8.1,We have to set clear optimizer to False since training should be continued
v1.8.1,Start tracking
v1.8.1,evaluation restriction to a subset of entities/relations
v1.8.1,TODO should training be reset?
v1.8.1,TODO should kwargs for loss and regularizer be checked and raised for?
v1.8.1,Log model parameters
v1.8.1,Log loss parameters
v1.8.1,the loss was already logged as part of the model kwargs
v1.8.1,"loss=loss_resolver.normalize_inst(model_instance.loss),"
v1.8.1,Log regularizer parameters
v1.8.1,Stopping
v1.8.1,"Load the evaluation batch size for the stopper, if it has been set"
v1.8.1,Add logging for debugging
v1.8.1,Train like Cristiano Ronaldo
v1.8.1,Build up a list of triples if we want to be in the filtered setting
v1.8.1,"If the user gave custom ""additional_filter_triples"""
v1.8.1,Determine whether the validation triples should also be filtered while performing test evaluation
v1.8.1,TODO consider implications of duplicates
v1.8.1,Evaluate
v1.8.1,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.8.1,Add logging about evaluator for debugging
v1.8.1,"If the evaluation still fail using the CPU, the error is raised"
v1.8.1,"When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is"
v1.8.1,restarted with PyKEEN's automatic memory optimization
v1.8.1,"When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation"
v1.8.1,is restarted using the cpu
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Imported from PyTorch
v1.8.1,: A wrapper around the hidden scheduler base class
v1.8.1,: The default strategy for optimizing the lr_schedulers' hyper-parameters
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,TODO what happens if already exists?
v1.8.1,TODO incorporate setting of random seed
v1.8.1,pipeline_kwargs=dict(
v1.8.1,"random_seed=random_non_negative_int(),"
v1.8.1,"),"
v1.8.1,Add dataset to current_pipeline
v1.8.1,"Training, test, and validation paths are provided"
v1.8.1,Add loss function to current_pipeline
v1.8.1,Add regularizer to current_pipeline
v1.8.1,Add optimizer to current_pipeline
v1.8.1,Add training approach to current_pipeline
v1.8.1,Add evaluation
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The link to the zip file
v1.8.1,: The hex digest for the zip file
v1.8.1,Input validation.
v1.8.1,"left side has files ending with 1, right side with 2"
v1.8.1,For downloading
v1.8.1,For splitting
v1.8.1,Whether to create inverse triples
v1.8.1,ensure file is present
v1.8.1,create triples factory
v1.8.1,split
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,": The mapping from (graph-pair, side) to triple file name"
v1.8.1,: The internal dataset name
v1.8.1,: The hex digest for the zip file
v1.8.1,Input validation.
v1.8.1,For downloading
v1.8.1,For splitting
v1.8.1,Whether to create inverse triples
v1.8.1,shared directory for multiple datasets.
v1.8.1,ensure file is present
v1.8.1,TODO: Re-use ensure_from_google?
v1.8.1,read all triples from file
v1.8.1,"some ""entities"" have numeric labels"
v1.8.1,"pandas.read_csv(..., dtype=str) does not work properly."
v1.8.1,create triples factory
v1.8.1,split
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.8.1,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.8.1,the columns are not ordered properly.
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,convert class to string to use caching
v1.8.1,Assume it's a file path
v1.8.1,hash kwargs
v1.8.1,normalize dataset name
v1.8.1,get canonic path
v1.8.1,try to use cached dataset
v1.8.1,load dataset without cache
v1.8.1,store cache
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The name of the dataset to download
v1.8.1,"note: we do not use the built-in constants here, since those refer to OGB nomenclature"
v1.8.1,(which happens to coincide with ours)
v1.8.1,FIXME these are already identifiers
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,relation typing
v1.8.1,constants
v1.8.1,unique
v1.8.1,compute over all triples
v1.8.1,Determine group key
v1.8.1,Add labels if requested
v1.8.1,TODO: Merge with _common?
v1.8.1,include hash over triples into cache-file name
v1.8.1,include part hash into cache-file name
v1.8.1,re-use cached file if possible
v1.8.1,select triples
v1.8.1,save to file
v1.8.1,Prune by support and confidence
v1.8.1,TODO: Consider merging with other analysis methods
v1.8.1,TODO: Consider merging with other analysis methods
v1.8.1,TODO: Consider merging with other analysis methods
v1.8.1,"num_triples_validation: Optional[int],"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Raise matplotlib level
v1.8.1,expected metrics
v1.8.1,Needs simulation
v1.8.1,See https://zenodo.org/record/6331629
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,don't call this function by itself. assumes called through the `validation`
v1.8.1,property and the _training factory has already been loaded
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Normalize path
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Base classes
v1.8.1,Utilities
v1.8.1,: A factory wrapping the training triples
v1.8.1,": A factory wrapping the testing triples, that share indices with the training triples"
v1.8.1,": A factory wrapping the validation triples, that share indices with the training triples"
v1.8.1,: All datasets should take care of inverse triple creation
v1.8.1,: the dataset's name
v1.8.1,TODO: Make a constant for the names
v1.8.1,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.8.1,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.8.1,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.8.1,: The directory in which the cached data is stored
v1.8.1,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.1,don't call this function by itself. assumes called through the `validation`
v1.8.1,property and the _training factory has already been loaded
v1.8.1,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.1,"relative paths within zip file's always follow Posix path, even on Windows"
v1.8.1,tarfile does not like pathlib
v1.8.1,: URL to the data to download
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Utilities
v1.8.1,Base Classes
v1.8.1,Concrete Classes
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"ZENODO_URL = ""https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip"""
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.8.1,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Base class
v1.8.1,Mid-level classes
v1.8.1,: A factory wrapping the training triples
v1.8.1,: A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT
v1.8.1,share indices with the transductive training
v1.8.1,": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.8.1,": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.8.1,: All datasets should take care of inverse triple creation
v1.8.1,": The actual instance of the training factory, which is exposed to the user through `transductive_training`"
v1.8.1,": The actual instance of the inductive inference factory,"
v1.8.1,: which is exposed to the user through `inductive_inference`
v1.8.1,": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`"
v1.8.1,": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`"
v1.8.1,: The directory in which the cached data is stored
v1.8.1,add v1 / v2 / v3 / v4 for inductive splits if available
v1.8.1,generate subfolders 'training' and  'inference'
v1.8.1,important: inductive_inference shares the same RELATIONS with the transductive training graph
v1.8.1,inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph
v1.8.1,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.1,inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph
v1.8.1,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,Base class
v1.8.1,Mid-level classes
v1.8.1,Datasets
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,1. Dataset
v1.8.1,2. Model
v1.8.1,3. Loss
v1.8.1,4. Regularizer
v1.8.1,5. Optimizer
v1.8.1,5.1 Learning Rate Scheduler
v1.8.1,6. Training Loop
v1.8.1,7. Training
v1.8.1,8. Evaluation
v1.8.1,9. Trackers
v1.8.1,Misc.
v1.8.1,log pruning
v1.8.1,"trial was successful, but has to be ended"
v1.8.1,also show info
v1.8.1,2. Model
v1.8.1,3. Loss
v1.8.1,4. Regularizer
v1.8.1,5. Optimizer
v1.8.1,5.1 Learning Rate Scheduler
v1.8.1,"TODO this fixes the issue for negative samplers, but does not generally address it."
v1.8.1,"For example, some of them obscure their arguments with **kwargs, so should we look"
v1.8.1,at the parent class? Sounds like something to put in class resolver by using the
v1.8.1,"inspect module. For now, this solution will rely on the fact that the sampler is a"
v1.8.1,direct descendent of a parent NegativeSampler
v1.8.1,create result tracker to allow to gracefully close failed trials
v1.8.1,1. Dataset
v1.8.1,2. Model
v1.8.1,3. Loss
v1.8.1,4. Regularizer
v1.8.1,5. Optimizer
v1.8.1,5.1 Learning Rate Scheduler
v1.8.1,6. Training Loop
v1.8.1,7. Training
v1.8.1,8. Evaluation
v1.8.1,9. Tracker
v1.8.1,Misc.
v1.8.1,close run in result tracker
v1.8.1,raise the error again (which will be catched in study.optimize)
v1.8.1,: The :mod:`optuna` study object
v1.8.1,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.8.1,Output study information
v1.8.1,Output all trials
v1.8.1,Output best trial as pipeline configuration file
v1.8.1,1. Dataset
v1.8.1,2. Model
v1.8.1,3. Loss
v1.8.1,4. Regularizer
v1.8.1,5. Optimizer
v1.8.1,5.1 Learning Rate Scheduler
v1.8.1,6. Training Loop
v1.8.1,7. Training
v1.8.1,8. Evaluation
v1.8.1,9. Tracking
v1.8.1,6. Misc
v1.8.1,Optuna Study Settings
v1.8.1,Optuna Optimization Settings
v1.8.1,TODO: use metric.increasing to determine default direction
v1.8.1,0. Metadata/Provenance
v1.8.1,1. Dataset
v1.8.1,2. Model
v1.8.1,3. Loss
v1.8.1,4. Regularizer
v1.8.1,5. Optimizer
v1.8.1,5.1 Learning Rate Scheduler
v1.8.1,6. Training Loop
v1.8.1,7. Training
v1.8.1,8. Evaluation
v1.8.1,9. Tracking
v1.8.1,1. Dataset
v1.8.1,2. Model
v1.8.1,3. Loss
v1.8.1,4. Regularizer
v1.8.1,5. Optimizer
v1.8.1,5.1 Learning Rate Scheduler
v1.8.1,6. Training Loop
v1.8.1,7. Training
v1.8.1,8. Evaluation
v1.8.1,9. Tracker
v1.8.1,Optuna Misc.
v1.8.1,Pipeline Misc.
v1.8.1,Invoke optimization of the objective function.
v1.8.1,TODO: make it even easier to specify categorical strategies just as lists
v1.8.1,"if isinstance(info, (tuple, list, set)):"
v1.8.1,"info = dict(type='categorical', choices=list(info))"
v1.8.1,get log from info - could either be a boolean or string
v1.8.1,"otherwise, dataset refers to a file that should be automatically split"
v1.8.1,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.8.1,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.8.1,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.8.1,if has_dataset(dataset_name):
v1.8.1,"study.set_user_attr('dataset', dataset_name)"
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,noqa: DAR101
v1.8.1,-*- coding: utf-8 -*-
v1.8.1,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,
v1.8.0,Configuration file for the Sphinx documentation builder.
v1.8.0,
v1.8.0,This file does only contain a selection of the most common options. For a
v1.8.0,full list see the documentation:
v1.8.0,http://www.sphinx-doc.org/en/master/config
v1.8.0,-- Path setup --------------------------------------------------------------
v1.8.0,"If extensions (or modules to document with autodoc) are in another directory,"
v1.8.0,add these directories to sys.path here. If the directory is relative to the
v1.8.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.8.0,
v1.8.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.8.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.8.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.8.0,from unittest.mock import Mock
v1.8.0,sys.modules['numpy'] = Mock()
v1.8.0,sys.modules['numpy.linalg'] = Mock()
v1.8.0,sys.modules['scipy'] = Mock()
v1.8.0,sys.modules['scipy.optimize'] = Mock()
v1.8.0,sys.modules['scipy.interpolate'] = Mock()
v1.8.0,sys.modules['scipy.sparse'] = Mock()
v1.8.0,sys.modules['scipy.ndimage'] = Mock()
v1.8.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.8.0,sys.modules['tensorflow'] = Mock()
v1.8.0,sys.modules['theano'] = Mock()
v1.8.0,sys.modules['theano.tensor'] = Mock()
v1.8.0,sys.modules['torch'] = Mock()
v1.8.0,sys.modules['torch.optim'] = Mock()
v1.8.0,sys.modules['torch.nn'] = Mock()
v1.8.0,sys.modules['torch.nn.init'] = Mock()
v1.8.0,sys.modules['torch.autograd'] = Mock()
v1.8.0,sys.modules['sklearn'] = Mock()
v1.8.0,sys.modules['sklearn.model_selection'] = Mock()
v1.8.0,sys.modules['sklearn.utils'] = Mock()
v1.8.0,-- Project information -----------------------------------------------------
v1.8.0,"The full version, including alpha/beta/rc tags."
v1.8.0,The short X.Y version.
v1.8.0,-- General configuration ---------------------------------------------------
v1.8.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.8.0,
v1.8.0,needs_sphinx = '1.0'
v1.8.0,"If true, the current module name will be prepended to all description"
v1.8.0,unit titles (such as .. function::).
v1.8.0,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.8.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.8.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.8.0,ones.
v1.8.0,show todo's
v1.8.0,generate autosummary pages
v1.8.0,"Add any paths that contain templates here, relative to this directory."
v1.8.0,The suffix(es) of source filenames.
v1.8.0,You can specify multiple suffix as a list of string:
v1.8.0,
v1.8.0,"source_suffix = ['.rst', '.md']"
v1.8.0,The master toctree document.
v1.8.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.8.0,for a list of supported languages.
v1.8.0,
v1.8.0,This is also used if you do content translation via gettext catalogs.
v1.8.0,"Usually you set ""language"" from the command line for these cases."
v1.8.0,"List of patterns, relative to source directory, that match files and"
v1.8.0,directories to ignore when looking for source files.
v1.8.0,This pattern also affects html_static_path and html_extra_path.
v1.8.0,The name of the Pygments (syntax highlighting) style to use.
v1.8.0,-- Options for HTML output -------------------------------------------------
v1.8.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.8.0,a list of builtin themes.
v1.8.0,
v1.8.0,Theme options are theme-specific and customize the look and feel of a theme
v1.8.0,"further.  For a list of options available for each theme, see the"
v1.8.0,documentation.
v1.8.0,
v1.8.0,html_theme_options = {}
v1.8.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.8.0,"relative to this directory. They are copied after the builtin static files,"
v1.8.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.8.0,html_static_path = ['_static']
v1.8.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.8.0,to template names.
v1.8.0,
v1.8.0,The default sidebars (for documents that don't match any pattern) are
v1.8.0,defined by theme itself.  Builtin themes are using these templates by
v1.8.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.8.0,'searchbox.html']``.
v1.8.0,
v1.8.0,html_sidebars = {}
v1.8.0,The name of an image file (relative to this directory) to place at the top
v1.8.0,of the sidebar.
v1.8.0,
v1.8.0,-- Options for HTMLHelp output ---------------------------------------------
v1.8.0,Output file base name for HTML help builder.
v1.8.0,-- Options for LaTeX output ------------------------------------------------
v1.8.0,latex_elements = {
v1.8.0,The paper size ('letterpaper' or 'a4paper').
v1.8.0,
v1.8.0,"'papersize': 'letterpaper',"
v1.8.0,
v1.8.0,"The font size ('10pt', '11pt' or '12pt')."
v1.8.0,
v1.8.0,"'pointsize': '10pt',"
v1.8.0,
v1.8.0,Additional stuff for the LaTeX preamble.
v1.8.0,
v1.8.0,"'preamble': '',"
v1.8.0,
v1.8.0,Latex figure (float) alignment
v1.8.0,
v1.8.0,"'figure_align': 'htbp',"
v1.8.0,}
v1.8.0,Grouping the document tree into LaTeX files. List of tuples
v1.8.0,"(source start file, target name, title,"
v1.8.0,"author, documentclass [howto, manual, or own class])."
v1.8.0,latex_documents = [
v1.8.0,(
v1.8.0,"master_doc,"
v1.8.0,"'pykeen.tex',"
v1.8.0,"'PyKEEN Documentation',"
v1.8.0,"author,"
v1.8.0,"'manual',"
v1.8.0,"),"
v1.8.0,]
v1.8.0,-- Options for manual page output ------------------------------------------
v1.8.0,One entry per manual page. List of tuples
v1.8.0,"(source start file, name, description, authors, manual section)."
v1.8.0,-- Options for Texinfo output ----------------------------------------------
v1.8.0,Grouping the document tree into Texinfo files. List of tuples
v1.8.0,"(source start file, target name, title, author,"
v1.8.0,"dir menu entry, description, category)"
v1.8.0,-- Options for Epub output -------------------------------------------------
v1.8.0,Bibliographic Dublin Core info.
v1.8.0,epub_title = project
v1.8.0,The unique identifier of the text. This can be a ISBN number
v1.8.0,or the project homepage.
v1.8.0,
v1.8.0,epub_identifier = ''
v1.8.0,A unique identification for the text.
v1.8.0,
v1.8.0,epub_uid = ''
v1.8.0,A list of files that should not be packed into the epub file.
v1.8.0,epub_exclude_files = ['search.html']
v1.8.0,-- Extension configuration -------------------------------------------------
v1.8.0,-- Options for intersphinx extension ---------------------------------------
v1.8.0,Example configuration for intersphinx: refer to the Python standard library.
v1.8.0,"'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),"
v1.8.0,autodoc_member_order = 'bysource'
v1.8.0,autodoc_typehints = 'both' # TODO turn on after 4.1 release
v1.8.0,autodoc_preserve_defaults = True
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,check probability distribution
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Check a model param is optimized
v1.8.0,Check a loss param is optimized
v1.8.0,Check a model param is NOT optimized
v1.8.0,Check a loss param is optimized
v1.8.0,Check a model param is optimized
v1.8.0,Check a loss param is NOT optimized
v1.8.0,Check a model param is NOT optimized
v1.8.0,Check a loss param is NOT optimized
v1.8.0,verify failure
v1.8.0,"Since custom data was passed, we can't store any of this"
v1.8.0,"currently, any custom data doesn't get stored."
v1.8.0,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.8.0,"Since there's no source path information, these shouldn't be"
v1.8.0,"added, even if it might be possible to infer path information"
v1.8.0,from the triples factories
v1.8.0,"Since paths were passed for training, testing, and validation,"
v1.8.0,they should be stored as study-level attributes
v1.8.0,Check a model param is optimized
v1.8.0,Check a loss param is optimized
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,check if within 0.5 std of observed
v1.8.0,test error is raised
v1.8.0,Tests that exception will be thrown when more than or less than three tensors are passed
v1.8.0,Test that regularization term is computed correctly
v1.8.0,Entity soft constraint
v1.8.0,Orthogonality soft constraint
v1.8.0,ensure regularizer is on correct device
v1.8.0,"After first update, should change the term"
v1.8.0,"After second update, no change should happen"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,create broadcastable shapes
v1.8.0,check correct value range
v1.8.0,check maximum norm constraint
v1.8.0,unchanged values for small norms
v1.8.0,random entity embeddings & projections
v1.8.0,random relation embeddings & projections
v1.8.0,project
v1.8.0,check shape:
v1.8.0,check normalization
v1.8.0,check equivalence of re-formulation
v1.8.0,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.8.0,= r_p (e_p^T e) + e'
v1.8.0,"create random array, estimate the costs of addition, and measure some execution times."
v1.8.0,"then, compute correlation between the estimated cost, and the measured time."
v1.8.0,check for strong correlation between estimated costs and measured execution time
v1.8.0,get optimal sequence
v1.8.0,check caching
v1.8.0,get optimal sequence
v1.8.0,check correct cost
v1.8.0,check optimality
v1.8.0,compare result to sequential addition
v1.8.0,compare result to sequential addition
v1.8.0,check result shape
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,equal value; larger is better
v1.8.0,equal value; smaller is better
v1.8.0,larger is better; improvement
v1.8.0,larger is better; improvement; but not significant
v1.8.0,: The window size used by the early stopper
v1.8.0,: The mock losses the mock evaluator will return
v1.8.0,: The (zeroed) index  - 1 at which stopping will occur
v1.8.0,: The minimum improvement
v1.8.0,: The best results
v1.8.0,Set automatic_memory_optimization to false for tests
v1.8.0,Step early stopper
v1.8.0,check storing of results
v1.8.0,not needed for test
v1.8.0,assert that reporting another metric for this epoch raises an error
v1.8.0,: The window size used by the early stopper
v1.8.0,: The (zeroed) index  - 1 at which stopping will occur
v1.8.0,: The minimum improvement
v1.8.0,: The random seed to use for reproducibility
v1.8.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.8.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.8.0,: The batch size to use.
v1.8.0,Fix seed for reproducibility
v1.8.0,Set automatic_memory_optimization to false during testing
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.8.0,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.8.0,"first triple, and passing create_inverse_triples=True."
v1.8.0,check for warning
v1.8.0,check for filtered triples
v1.8.0,check for correct inverse triples flag
v1.8.0,check correct translation
v1.8.0,check column order
v1.8.0,apply restriction
v1.8.0,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.8.0,check that inverse_triples is correctly carried over
v1.8.0,verify that the label-to-ID mapping has not been changed
v1.8.0,verify that triples have been filtered
v1.8.0,Test different combinations of restrictions
v1.8.0,check compressed triples
v1.8.0,reconstruct triples from compressed form
v1.8.0,check data loader
v1.8.0,set create inverse triple to true
v1.8.0,split factory
v1.8.0,check that in *training* inverse triple are to be created
v1.8.0,check that in all other splits no inverse triples are to be created
v1.8.0,verify that all entities and relations are present in the training factory
v1.8.0,verify that no triple got lost
v1.8.0,verify that the label-to-id mappings match
v1.8.0,Slightly larger number of triples to guarantee split can find coverage of all entities and relations.
v1.8.0,serialize
v1.8.0,de-serialize
v1.8.0,check for equality
v1.8.0,TODO: this could be (Core)TriplesFactory.__equal__
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"DummyModel,"
v1.8.0,3x batch norm: bias + scale --> 6
v1.8.0,entity specific bias        --> 1
v1.8.0,==================================
v1.8.0,7
v1.8.0,"two bias terms, one conv-filter"
v1.8.0,check type
v1.8.0,check shape
v1.8.0,check ID ranges
v1.8.0,this is only done in one of the models
v1.8.0,this is only done in one of the models
v1.8.0,Two linear layer biases
v1.8.0,"Two BN layers, bias & scale"
v1.8.0,Test that the weight in the MLP is trainable (i.e. requires grad)
v1.8.0,quaternion have four components
v1.8.0,: one bias per layer
v1.8.0,: one bias per layer
v1.8.0,entity embeddings
v1.8.0,relation embeddings
v1.8.0,Compute Scores
v1.8.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.8.0,relation embeddings
v1.8.0,Compute Scores
v1.8.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.8.0,entity embeddings
v1.8.0,relation embeddings
v1.8.0,Compute Scores
v1.8.0,entity embeddings
v1.8.0,relation embeddings
v1.8.0,Compute Scores
v1.8.0,second_score = scores[1].item()
v1.8.0,: 2xBN (bias & scale)
v1.8.0,the combination bias
v1.8.0,FIXME definitely a type mismatch going on here
v1.8.0,check shape
v1.8.0,check content
v1.8.0,create triples factory with inverse relations
v1.8.0,head prediction via inverse tail prediction
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,empty lists are falsy
v1.8.0,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.8.0,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.8.0,pipeline is checked.
v1.8.0,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.8.0,Resume the previous pipeline
v1.8.0,The MockModel gives the highest score to the highest entity id
v1.8.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.8.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.8.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.8.0,test triple at hand
v1.8.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.8.0,test triple at hand
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.8.0,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.8.0,Test looking up inverse triples
v1.8.0,test new label to ID
v1.8.0,type
v1.8.0,old labels
v1.8.0,"new, compact IDs"
v1.8.0,test vectorized lookup
v1.8.0,type
v1.8.0,shape
v1.8.0,value range
v1.8.0,only occurring Ids get mapped to non-negative numbers
v1.8.0,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.8.0,check type
v1.8.0,check shape
v1.8.0,check content
v1.8.0,check type
v1.8.0,check shape
v1.8.0,check 1-hot
v1.8.0,check type
v1.8.0,check shape
v1.8.0,check value range
v1.8.0,check self-similarity = 1
v1.8.0,base relation
v1.8.0,exact duplicate
v1.8.0,99% duplicate
v1.8.0,50% duplicate
v1.8.0,exact inverse
v1.8.0,99% inverse
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The expected number of entities
v1.8.0,: The expected number of relations
v1.8.0,: The expected number of triples
v1.8.0,": The tolerance on expected number of triples, for randomized situations"
v1.8.0,: The dataset to test
v1.8.0,: The instantiated dataset
v1.8.0,: Should the validation be assumed to have been loaded with train/test?
v1.8.0,Not loaded
v1.8.0,Load
v1.8.0,Test caching
v1.8.0,assert (end - start) < 1.0e-02
v1.8.0,Test consistency of training / validation / testing mapping
v1.8.0,": The directory, if there is caching"
v1.8.0,: The batch size
v1.8.0,: The number of negatives per positive for sLCWA training loop.
v1.8.0,: The number of entities LCWA training loop / label smoothing.
v1.8.0,test reduction
v1.8.0,test finite loss value
v1.8.0,Test backward
v1.8.0,negative scores decreased compared to positive ones
v1.8.0,negative scores decreased compared to positive ones
v1.8.0,: The number of entities.
v1.8.0,: The number of negative samples
v1.8.0,: The number of entities.
v1.8.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.8.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.8.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.8.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.8.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.8.0,test multiple different initializations
v1.8.0,calculate by functional
v1.8.0,calculate manually
v1.8.0,simple
v1.8.0,nested
v1.8.0,nested
v1.8.0,prepare a temporary test directory
v1.8.0,check that file was created
v1.8.0,make sure to close file before trying to delete it
v1.8.0,delete intermediate files
v1.8.0,: The batch size
v1.8.0,: The triples factory
v1.8.0,: Class of regularizer to test
v1.8.0,: The constructor parameters to pass to the regularizer
v1.8.0,": The regularizer instance, initialized in setUp"
v1.8.0,: A positive batch
v1.8.0,: The device
v1.8.0,move test instance to device
v1.8.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.8.0,Check if regularizer is stored correctly.
v1.8.0,Forward pass (should update regularizer)
v1.8.0,Call post_parameter_update (should reset regularizer)
v1.8.0,Check if regularization term is reset
v1.8.0,Call method
v1.8.0,Generate random tensors
v1.8.0,Call update
v1.8.0,check shape
v1.8.0,compute expected term
v1.8.0,Generate random tensor
v1.8.0,calculate penalty
v1.8.0,check shape
v1.8.0,check value
v1.8.0,FIXME isn't any finite number allowed now?
v1.8.0,: Additional arguments passed to the training loop's constructor method
v1.8.0,: The triples factory instance
v1.8.0,: The batch size for use for forward_* tests
v1.8.0,: The embedding dimensionality
v1.8.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.8.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.8.0,: The batch size for use when testing training procedures
v1.8.0,: The number of epochs to train the model
v1.8.0,: A random number generator from torch
v1.8.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.8.0,initialization
v1.8.0,: Static extras to append to the CLI
v1.8.0,: the model's device
v1.8.0,: the inductive mode
v1.8.0,for reproducible testing
v1.8.0,insert shared parameters
v1.8.0,move model to correct device
v1.8.0,Check that all the parameters actually require a gradient
v1.8.0,Try to initialize an optimizer
v1.8.0,get model parameters
v1.8.0,re-initialize
v1.8.0,check that the operation works in-place
v1.8.0,check that the parameters where modified
v1.8.0,check for finite values by default
v1.8.0,check whether a gradient can be back-propgated
v1.8.0,"assert batch comprises (head, relation) pairs"
v1.8.0,"assert batch comprises (head, tail) pairs"
v1.8.0,TODO: look into score_r for inverse relations
v1.8.0,"assert batch comprises (relation, tail) pairs"
v1.8.0,"For the high/low memory test cases of NTN, SE, etc."
v1.8.0,"else, leave to default"
v1.8.0,Make sure that inverse triples are created if create_inverse_triples=True
v1.8.0,triples factory is added by the pipeline
v1.8.0,TODO: Catch HolE MKL error?
v1.8.0,set regularizer term to something that isn't zero
v1.8.0,call post_parameter_update
v1.8.0,assert that the regularization term has been reset
v1.8.0,do one optimization step
v1.8.0,call post_parameter_update
v1.8.0,check model constraints
v1.8.0,"assert batch comprises (relation, tail) pairs"
v1.8.0,"assert batch comprises (relation, tail) pairs"
v1.8.0,"assert batch comprises (relation, tail) pairs"
v1.8.0,call some functions
v1.8.0,reset to old state
v1.8.0,Distance-based model
v1.8.0,dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)
v1.8.0,check type
v1.8.0,check shape
v1.8.0,create a new instance with guaranteed dropout
v1.8.0,set to training mode
v1.8.0,check for different output
v1.8.0,use more samples to make sure that enough values can be dropped
v1.8.0,: The number of entities
v1.8.0,: The number of triples
v1.8.0,: the message dim
v1.8.0,TODO: separation message vs. entity dim?
v1.8.0,check shape
v1.8.0,check dtype
v1.8.0,check finite values (e.g. due to division by zero)
v1.8.0,check non-negativity
v1.8.0,: The input dimension
v1.8.0,: the shape of the tensor to initialize
v1.8.0,: to be initialized / set in subclass
v1.8.0,initializers *may* work in-place => clone
v1.8.0,unfavourable split to ensure that cleanup is necessary
v1.8.0,check for unclean split
v1.8.0,check that no triple got lost
v1.8.0,check that triples where only moved from other to reference
v1.8.0,check that all entities occur in reference
v1.8.0,check that no triple got lost
v1.8.0,check that all entities are covered in first part
v1.8.0,the model
v1.8.0,Settings
v1.8.0,Use small model (untrained)
v1.8.0,Get batch
v1.8.0,Compute scores
v1.8.0,Compute mask only if required
v1.8.0,TODO: Re-use filtering code
v1.8.0,"shape: (batch_size, num_triples)"
v1.8.0,"shape: (batch_size, num_entities)"
v1.8.0,Process one batch
v1.8.0,shape
v1.8.0,value range
v1.8.0,no duplicates
v1.8.0,shape
v1.8.0,value range
v1.8.0,no duplicates
v1.8.0,shape
v1.8.0,value range
v1.8.0,"no repetition, except padding idx"
v1.8.0,: The batch size
v1.8.0,: the maximum number of candidates
v1.8.0,: the number of ranks
v1.8.0,: the number of samples to use for monte-carlo estimation
v1.8.0,: the number of candidates for each individual ranking task
v1.8.0,: the ranks for each individual ranking task
v1.8.0,data type
v1.8.0,value range
v1.8.0,original ranks
v1.8.0,better ranks
v1.8.0,variances are non-negative
v1.8.0,check flatness
v1.8.0,"TODO: does this suffice, or do we really need float as datatype?"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,check for finite values by default
v1.8.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.8.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.8.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.8.0,"TODO: Remove, since it stems from old-style model"
v1.8.0,Get embeddings
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,≈ result of softmax
v1.8.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.8.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.8.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.8.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.8.0,≈ result of sigmoid
v1.8.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.8.0,expected_loss ≈ 0.34
v1.8.0,Create dummy dense labels
v1.8.0,Check if labels form a probability distribution
v1.8.0,Apply label smoothing
v1.8.0,Check if smooth labels form probability distribution
v1.8.0,Create dummy sLCWA labels
v1.8.0,Apply label smoothing
v1.8.0,generate random ratios
v1.8.0,check size
v1.8.0,check value range
v1.8.0,check total split
v1.8.0,check consistency with ratios
v1.8.0,the number of decimal digits equivalent to 1 / n_total
v1.8.0,check type
v1.8.0,check values
v1.8.0,compare against expected
v1.8.0,generated_triples = generate_triples()
v1.8.0,check type
v1.8.0,check format
v1.8.0,check coverage
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"naive implementation, O(n2)"
v1.8.0,check correct output type
v1.8.0,check value range subset
v1.8.0,check value range side
v1.8.0,check columns
v1.8.0,check value range and type
v1.8.0,check value range entity IDs
v1.8.0,check value range entity labels
v1.8.0,check correct type
v1.8.0,check relation_id value range
v1.8.0,check pattern value range
v1.8.0,check confidence value range
v1.8.0,check support value range
v1.8.0,check correct type
v1.8.0,check relation_id value range
v1.8.0,check pattern value range
v1.8.0,check correct type
v1.8.0,check relation_id value range
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,clear
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Check minimal statistics
v1.8.0,Check either a github link or author/publication information is given
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.8.0,"prepare conv input (N, C, H, W)"
v1.8.0,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.8.0,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.8.0,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.8.0,"f(h, r, t) = h @ r @ t"
v1.8.0,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.8.0,normalize length of r
v1.8.0,check for unit length
v1.8.0,entity embeddings
v1.8.0,relation embeddings
v1.8.0,Compute Scores
v1.8.0,entity embeddings
v1.8.0,relation embeddings
v1.8.0,Compute Scores
v1.8.0,Compute Scores
v1.8.0,-\|R_h h - R_t t\|
v1.8.0,-\|h - t\|
v1.8.0,"Since MuRE has offsets, the scores do not need to negative"
v1.8.0,"We do not need this, since we do not check for functional consistency anyway"
v1.8.0,intra-interaction comparison
v1.8.0,dimension needs to be divisible by num_heads
v1.8.0,FIXME
v1.8.0,multiple
v1.8.0,single
v1.8.0,head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,message_dim must be divisible by num_heads
v1.8.0,determine pool using anchor searcher
v1.8.0,determine expected pool using shortest path distances via scipy.sparse.csgraph
v1.8.0,generate random pool
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,check value range
v1.8.0,check sin**2 + cos**2 == 1
v1.8.0,"check value range (actually [-s, +s] with s = 1/sqrt(2*n))"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"typically, the model takes care of adjusting the dimension size for ""complex"""
v1.8.0,"tensors, but we have to do it manually here for testing purposes"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO consider making subclass of cases.RepresentationTestCase
v1.8.0,"that has num_entities, num_relations, num_triples, and"
v1.8.0,create_inverse_triples as well as a generate_triples_factory()
v1.8.0,wrapper
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO this is the only place this function is used.
v1.8.0,Is there an alternative so we can remove it?
v1.8.0,ensure positivity
v1.8.0,compute using pytorch
v1.8.0,prepare distributions
v1.8.0,compute using pykeen
v1.8.0,"e: (batch_size, num_heads, num_tails, d)"
v1.8.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.8.0,divergence = 0 => similarity = -divergence = 0
v1.8.0,"(h - t), r"
v1.8.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.8.0,divergence >= 0 => similarity = -divergence <= 0
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.8.0,filter vs. no filter thing.
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,check for empty batches
v1.8.0,: The window size used by the early stopper
v1.8.0,: The mock losses the mock evaluator will return
v1.8.0,: The (zeroed) index  - 1 at which stopping will occur
v1.8.0,: The minimum improvement
v1.8.0,: The best results
v1.8.0,Set automatic_memory_optimization to false for tests
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Train a model in one shot
v1.8.0,Train a model for the first half
v1.8.0,Continue training of the first part
v1.8.0,check non-empty metrics
v1.8.0,: Should negative samples be filtered?
v1.8.0,expectation = (1 + n) / 2
v1.8.0,variance = (n**2 - 1) / 12
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Check for correct class
v1.8.0,check correct num_entities
v1.8.0,Check for correct class
v1.8.0,check value
v1.8.0,filtering
v1.8.0,"true_score: (2, 3, 3)"
v1.8.0,head based filter
v1.8.0,preprocessing for faster lookup
v1.8.0,check that all found positives are positive
v1.8.0,check in-place
v1.8.0,Test head scores
v1.8.0,Assert in-place modification
v1.8.0,Assert correct filtering
v1.8.0,Test tail scores
v1.8.0,Assert in-place modification
v1.8.0,Assert correct filtering
v1.8.0,The MockModel gives the highest score to the highest entity id
v1.8.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.8.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.8.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.8.0,test triple at hand
v1.8.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.8.0,test triple at hand
v1.8.0,check true negatives
v1.8.0,TODO: check no repetitions (if possible)
v1.8.0,return type
v1.8.0,columns
v1.8.0,value range
v1.8.0,relation restriction
v1.8.0,with explicit num_entities
v1.8.0,with inferred num_entities
v1.8.0,test different shapes
v1.8.0,test different shapes
v1.8.0,value range
v1.8.0,value range
v1.8.0,check unique
v1.8.0,"strips off the ""k"" at the end"
v1.8.0,Populate with real results.
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"(-1, 1),"
v1.8.0,"(-1, -1),"
v1.8.0,"(-5, -3),"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Check whether filtering works correctly
v1.8.0,First giving an example where all triples have to be filtered
v1.8.0,The filter should remove all triples
v1.8.0,Create an example where no triples will be filtered
v1.8.0,The filter should not remove any triple
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,same relation
v1.8.0,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.8.0,Test that half of the subjects and half of the objects are corrupted
v1.8.0,check that corrupted entities co-occur with the relation in training data
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The batch size
v1.8.0,: The random seed
v1.8.0,: The triples factory
v1.8.0,: The instances
v1.8.0,: A positive batch
v1.8.0,: Kwargs
v1.8.0,Generate negative sample
v1.8.0,check filter shape if necessary
v1.8.0,check shape
v1.8.0,check bounds: heads
v1.8.0,check bounds: relations
v1.8.0,check bounds: tails
v1.8.0,test that the negative triple is not the original positive triple
v1.8.0,"shape: (batch_size, 1, num_neg)"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Base Classes
v1.8.0,Concrete Classes
v1.8.0,Utils
v1.8.0,: The default strategy for optimizing the loss's hyper-parameters
v1.8.0,flatten and stack
v1.8.0,apply label smoothing if necessary.
v1.8.0,TODO: Do label smoothing only once
v1.8.0,Sanity check
v1.8.0,"prepare for broadcasting, shape: (batch_size, 1, 3)"
v1.8.0,negative_scores have already been filtered in the sampler!
v1.8.0,"shape: (nnz,)"
v1.8.0,Sanity check
v1.8.0,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.8.0,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.8.0,positive entries in the labels tensor.
v1.8.0,"This shows how often one row has to be repeated,"
v1.8.0,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.8.0,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.8.0,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.8.0,This tells us how often each true label should be repeated
v1.8.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.8.0,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.8.0,within the ablation pipeline.
v1.8.0,1. positive & negative margin
v1.8.0,2. negative margin & offset
v1.8.0,3. positive margin & offset
v1.8.0,Sanity check
v1.8.0,positive term
v1.8.0,implicitly repeat positive scores
v1.8.0,"shape: (nnz,)"
v1.8.0,negative term
v1.8.0,negative_scores have already been filtered in the sampler!
v1.8.0,Sanity check
v1.8.0,"scale labels from [0, 1] to [-1, 1]"
v1.8.0,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.8.0,within the ablation pipeline.
v1.8.0,negative_scores have already been filtered in the sampler!
v1.8.0,(dense) softmax requires unfiltered scores / masking
v1.8.0,we need to fill the scores with -inf for all filtered negative examples
v1.8.0,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.8.0,use filled negatives scores
v1.8.0,we need dense negative scores => unfilter if necessary
v1.8.0,"we may have inf rows, since there will be one additional finite positive score per row"
v1.8.0,"combine scores: shape: (batch_size, num_negatives + 1)"
v1.8.0,use sparse version of cross entropy
v1.8.0,Sanity check
v1.8.0,compute negative weights (without gradient tracking)
v1.8.0,clone is necessary since we modify in-place
v1.8.0,Split positive and negative scores
v1.8.0,Sanity check
v1.8.0,"we do not allow full -inf rows, since we compute the softmax over this tensor"
v1.8.0,compute weights (without gradient tracking)
v1.8.0,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.8.0,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.8.0,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.8.0,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.8.0,": For more information, see https://github.com/cthoyt/pystow"
v1.8.0,: A path representing the PyKEEN data folder
v1.8.0,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.8.0,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.8.0,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.8.0,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.8.0,: A subdirectory for PyKEEN logs
v1.8.0,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.8.0,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.8.0,"TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728"
v1.8.0,"SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.8.0,get datatype specific epsilon
v1.8.0,clamp minimum value
v1.8.0,try to resolve ambiguous device; there has to be at least one cuda device
v1.8.0,lower bound
v1.8.0,upper bound
v1.8.0,input validation
v1.8.0,base case
v1.8.0,normalize dim
v1.8.0,calculate repeats for each tensor
v1.8.0,dimensions along concatenation axis do not need to match
v1.8.0,get desired extent along dimension
v1.8.0,repeat tensors along axes if necessary
v1.8.0,concatenate
v1.8.0,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.8.0,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.8.0,Determine optimal order and cost
v1.8.0,translate back to original order
v1.8.0,determine optimal processing order
v1.8.0,heuristic
v1.8.0,workaround for complex numbers: manually compute norm
v1.8.0,TODO: check if einsum is still very slow.
v1.8.0,TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation
v1.8.0,unsqueeze
v1.8.0,The dimensions affected by e'
v1.8.0,Project entities
v1.8.0,r_p (e_p.T e) + e'
v1.8.0,Enforce constraints
v1.8.0,TODO delete when deleting _normalize_dim (below)
v1.8.0,TODO delete when deleting convert_to_canonical_shape (below)
v1.8.0,TODO delete? See note in test_sim.py on its only usage
v1.8.0,upgrade to sequence
v1.8.0,broadcast
v1.8.0,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.8.0,Create a tensor of all IDs
v1.8.0,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.8.0,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.8.0,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.8.0,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.8.0,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.8.0,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.8.0,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.8.0,ensure pathlib
v1.8.0,Enforce that sizes are strictly positive by passing through ELU
v1.8.0,Shape vector is normalized using the above helper function
v1.8.0,Size is learned separately and applied to normalized shape
v1.8.0,Compute potential boundaries by applying the shape in substraction
v1.8.0,and in addition
v1.8.0,Compute box upper bounds using min and max respectively
v1.8.0,compute width plus 1
v1.8.0,compute box midpoints
v1.8.0,"TODO: we already had this before, as `base`"
v1.8.0,inside box?
v1.8.0,yes: |p - c| / (w + 1)
v1.8.0,no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))
v1.8.0,Step 1: Apply the other entity bump
v1.8.0,Step 2: Apply tanh if tanh_map is set to True.
v1.8.0,Compute the distance function output element-wise
v1.8.0,"Finally, compute the norm"
v1.8.0,cf. https://stackoverflow.com/a/1176023
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Base Class
v1.8.0,Child classes
v1.8.0,Utils
v1.8.0,: The overall regularization weight
v1.8.0,: The current regularization term (a scalar)
v1.8.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.8.0,: Has this regularizer been updated since last being reset?
v1.8.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.8.0,"If there are tracked parameters, update based on them"
v1.8.0,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.8.0,no need to compute anything
v1.8.0,always return zero
v1.8.0,: The dimension along which to compute the vector-based regularization terms.
v1.8.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.8.0,: This allows dimensionality-independent weight tuning.
v1.8.0,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.8.0,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.8.0,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.8.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.8.0,"Therefore, apply_only_once is always set to True."
v1.8.0,Entity soft constraint
v1.8.0,Orthogonality soft constraint
v1.8.0,The normalization factor to balance individual regularizers' contribution.
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Add HPO command
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,General types
v1.8.0,Triples
v1.8.0,Others
v1.8.0,Tensor Functions
v1.8.0,Tensors
v1.8.0,Dataclasses
v1.8.0,prediction targets
v1.8.0,modes
v1.8.0,: A function that mutates the input and returns a new object of the same type as output
v1.8.0,: A function that can be applied to a tensor to initialize it
v1.8.0,: A function that can be applied to a tensor to normalize it
v1.8.0,: A function that can be applied to a tensor to constrain it
v1.8.0,: A hint for a :class:`torch.device`
v1.8.0,: A hint for a :class:`torch.Generator`
v1.8.0,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.8.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.8.0,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.8.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.8.0,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.8.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.8.0,: the inductive prediction and training mode
v1.8.0,: the prediction target
v1.8.0,: the prediction target index
v1.8.0,: the rank types
v1.8.0,"RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,pad with zeros
v1.8.0,trim
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"mask, shape: (num_edges,)"
v1.8.0,bi-directional message passing
v1.8.0,Heuristic for default value
v1.8.0,other relations
v1.8.0,other relations
v1.8.0,Select source and target indices as well as edge weights for the
v1.8.0,currently considered relation
v1.8.0,skip relations without edges
v1.8.0,"compute message, shape: (num_edges_of_type, output_dim)"
v1.8.0,since we may have one node ID appearing multiple times as source
v1.8.0,"ID, we can save some computation by first reducing to the unique"
v1.8.0,"source IDs, compute transformed representations and afterwards"
v1.8.0,select these representations for the correct edges.
v1.8.0,select unique source node representations
v1.8.0,transform representations by relation specific weight
v1.8.0,select the uniquely transformed representations for each edge
v1.8.0,optional message weighting
v1.8.0,message aggregation
v1.8.0,Xavier Glorot initialization of each block
v1.8.0,accumulator
v1.8.0,view as blocks
v1.8.0,other relations
v1.8.0,skip relations without edges
v1.8.0,"compute message, shape: (num_edges_of_type, num_blocks, block_size)"
v1.8.0,optional message weighting
v1.8.0,message aggregation
v1.8.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501
v1.8.0,there are separate decompositions for forward and backward relations.
v1.8.0,the self-loop weight is not decomposed.
v1.8.0,self-loop
v1.8.0,forward messages
v1.8.0,backward messages
v1.8.0,activation
v1.8.0,has to be imported now to avoid cyclic imports
v1.8.0,Resolve edge weighting
v1.8.0,dropout
v1.8.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.8.0,no activation on last layer
v1.8.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501
v1.8.0,buffering of enriched representations
v1.8.0,invalidate enriched embeddings
v1.8.0,Bind fields
v1.8.0,"shape: (num_entities, embedding_dim)"
v1.8.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.8.0,Get random dropout mask
v1.8.0,Apply to edges
v1.8.0,fixed edges -> pre-compute weights
v1.8.0,Cache enriched representations
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Utils
v1.8.0,: the maximum ID (exclusively)
v1.8.0,: the shape of an individual representation
v1.8.0,: a normalizer for individual representations
v1.8.0,: a regularizer for individual representations
v1.8.0,: dropout
v1.8.0,normalize *before* repeating
v1.8.0,regularize *after* repeating
v1.8.0,TODO: Remove this property and update code to use shape instead
v1.8.0,has to be imported here to avoid cyclic import
v1.8.0,normalize num_embeddings vs. max_id
v1.8.0,normalize embedding_dim vs. shape
v1.8.0,work-around until full complex support (torch==1.10 still does not work)
v1.8.0,TODO: verify that this is our understanding of complex!
v1.8.0,"note: this seems to work, as finfo returns the datatype of the underlying floating"
v1.8.0,"point dtype, rather than the combined complex one"
v1.8.0,"use make for initializer since there's a default, and make_safe"
v1.8.0,for the others to pass through None values
v1.8.0,"wrapper around max_id, for backward compatibility"
v1.8.0,initialize weights in-place
v1.8.0,apply constraints in-place
v1.8.0,verify that contiguity is preserved
v1.8.0,"get all base representations, shape: (num_bases, *shape)"
v1.8.0,"get base weights, shape: (*batch_dims, num_bases)"
v1.8.0,"weighted linear combination of bases, shape: (*batch_dims, *shape)"
v1.8.0,normalize output dimension
v1.8.0,entity-relation composition
v1.8.0,edge weighting
v1.8.0,message passing weights
v1.8.0,linear relation transformation
v1.8.0,layer-specific self-loop relation representation
v1.8.0,other components
v1.8.0,initialize
v1.8.0,split
v1.8.0,compose
v1.8.0,transform
v1.8.0,normalization
v1.8.0,aggregate by sum
v1.8.0,dropout
v1.8.0,prepare for inverse relations
v1.8.0,update entity representations: mean over self-loops / forward edges / backward edges
v1.8.0,Relation transformation
v1.8.0,has to be imported here to avoid cyclic imports
v1.8.0,kwargs
v1.8.0,Buffered enriched entity and relation representations
v1.8.0,TODO: Check
v1.8.0,hidden dimension normalization
v1.8.0,Create message passing layers
v1.8.0,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.8.0,TODO: This always uses all training triples for message passing
v1.8.0,initialize buffer of enriched representations
v1.8.0,invalidate enriched embeddings
v1.8.0,"when changing from evaluation to training mode, the buffered representations have been computed without"
v1.8.0,"gradient tracking. hence, we need to invalidate them."
v1.8.0,note: this occurs in practice when continuing training after evaluation.
v1.8.0,enrich
v1.8.0,infer shape
v1.8.0,"assign after super, since they should be properly registered as submodules"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,scaling factor
v1.8.0,"modulus ~ Uniform[-s, s]"
v1.8.0,"phase ~ Uniform[0, 2*pi]"
v1.8.0,real part
v1.8.0,purely imaginary quaternions unitary
v1.8.0,this is usually loaded from somewhere else
v1.8.0,"the shape must match, as well as the entity-to-id mapping"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: whether the edge weighting needs access to the message
v1.8.0,stub init to enable arbitrary arguments in subclasses
v1.8.0,"Calculate in-degree, i.e. number of incoming edges"
v1.8.0,backward compatibility with RGCN
v1.8.0,view for heads
v1.8.0,"compute attention coefficients, shape: (num_edges, num_heads)"
v1.8.0,"TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO test
v1.8.0,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.8.0,: a = \mu^T\Sigma^{-1}\mu
v1.8.0,: b = \log \det \Sigma
v1.8.0,1. Component
v1.8.0,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.8.0,2. Component
v1.8.0,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.8.0,with mu = (mu_1 - mu_0)
v1.8.0,= mu * Sigma_1^-1 mu
v1.8.0,since Sigma_1 is diagonal
v1.8.0,= mu**2 / sigma_1
v1.8.0,3. Component
v1.8.0,4. Component
v1.8.0,ln (det(\Sigma_1) / det(\Sigma_0))
v1.8.0,= ln det Sigma_1 - ln det Sigma_0
v1.8.0,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.8.0,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.8.0,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.8.0,allocate result
v1.8.0,prepare distributions
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO benchmark
v1.8.0,TODO benchmark
v1.8.0,TODO benchmark
v1.8.0,TODO benchmark
v1.8.0,TODO benchmark
v1.8.0,TODO benchmark
v1.8.0,TODO benchmark
v1.8.0,TODO benchmark
v1.8.0,TODO benchmark
v1.8.0,"h = h_re, -h_im"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Adapter classes
v1.8.0,Concrete Classes
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Base Classes
v1.8.0,Adapter classes
v1.8.0,Concrete Classes
v1.8.0,normalize input
v1.8.0,get number of head/relation/tail representations
v1.8.0,flatten list
v1.8.0,split tensors
v1.8.0,broadcasting
v1.8.0,yield batches
v1.8.0,complex typing
v1.8.0,: The symbolic shapes for entity representations
v1.8.0,": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE."
v1.8.0,: The symbolic shapes for relation representations
v1.8.0,"TODO: we could change that to slicing along multiple dimensions, if necessary"
v1.8.0,"The appended ""e"" represents the literals that get concatenated"
v1.8.0,on the entity representations. It does not necessarily have the
v1.8.0,"same dimension ""d"" as the entity representations."
v1.8.0,alternate way of combining entity embeddings + literals
v1.8.0,"h = torch.cat(h, dim=-1)"
v1.8.0,"h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore"
v1.8.0,"t = torch.cat(t, dim=-1)"
v1.8.0,"t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore"
v1.8.0,: The functional interaction form
v1.8.0,Store initial input for error message
v1.8.0,All are None -> try and make closest to square
v1.8.0,Only input channels is None
v1.8.0,Only width is None
v1.8.0,Only height is none
v1.8.0,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.8.0,Width and input channels are None -> set input channels to 1 and calculate width
v1.8.0,": The head-relation encoder operating on 2D ""images"""
v1.8.0,: The head-relation encoder operating on the 1D flattened version
v1.8.0,: The interaction function
v1.8.0,Automatic calculation of remaining dimensions
v1.8.0,Parameter need to fulfil:
v1.8.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.8.0,encoders
v1.8.0,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.8.0,"2: 1D encoder: FC, DO, BN?, Act"
v1.8.0,store reshaping dimensions
v1.8.0,The interaction model
v1.8.0,Use Xavier initialization for weight; bias to zero
v1.8.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.8.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.8.0,Initialize biases with zero
v1.8.0,"In the original formulation,"
v1.8.0,Global entity projection
v1.8.0,Global relation projection
v1.8.0,Global combination bias
v1.8.0,Global combination bias
v1.8.0,Core tensor
v1.8.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.8.0,Dropout
v1.8.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.8.0,"batch norm gets reset automatically, since it defines reset_parameters"
v1.8.0,shapes
v1.8.0,there are separate biases for entities in head and tail position
v1.8.0,the base interaction
v1.8.0,forward entity/relation shapes
v1.8.0,The parameters of the affine transformation: bias
v1.8.0,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.8.0,head position and bump
v1.8.0,relation box: head
v1.8.0,relation box: tail
v1.8.0,tail position and bump
v1.8.0,input normalization
v1.8.0,Core tensor
v1.8.0,initialize core tensor
v1.8.0,"r_head, r_mid, r_tail"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"repeat if necessary, and concat head and relation"
v1.8.0,"shape: -1, num_input_channels, 2*height, width"
v1.8.0,"shape: -1, num_input_channels, 2*height, width"
v1.8.0,"-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.8.0,"reshape: (-1, dim) -> (*batch_dims, dim)"
v1.8.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.8.0,output_shape: batch_dims
v1.8.0,add bias term
v1.8.0,decompose convolution for faster computation in 1-n case
v1.8.0,"compute conv(stack(h, r, t))"
v1.8.0,prepare input shapes for broadcasting
v1.8.0,"(*batch_dims, 1, d)"
v1.8.0,"conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])"
v1.8.0,"here, kernel_size = (1, 3), C_in = 1, C_out = num_filters"
v1.8.0,"-> conv_head, conv_rel, conv_tail shapes: (num_filters,)"
v1.8.0,"reshape to (..., f, 1)"
v1.8.0,"convolve -> output.shape: (*, embedding_dim, num_filters)"
v1.8.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.8.0,"Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)"
v1.8.0,same shape
v1.8.0,"split, shape: (embedding_dim, hidden_dim)"
v1.8.0,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.8.0,"Predict t embedding, shape: (*batch_dims, d)"
v1.8.0,dot product
v1.8.0,"composite: (*batch_dims, d)"
v1.8.0,inner product with relation embedding
v1.8.0,Circular correlation of entity embeddings
v1.8.0,complex conjugate
v1.8.0,Hadamard product in frequency domain
v1.8.0,inverse real FFT
v1.8.0,global projections
v1.8.0,"combination, shape: (*batch_dims, d)"
v1.8.0,dot product with t
v1.8.0,r expresses a rotation in complex plane.
v1.8.0,rotate head by relation (=Hadamard product in complex space)
v1.8.0,rotate tail by inverse of relation
v1.8.0,The inverse rotation is expressed by the complex conjugate of r.
v1.8.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.8.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.8.0,|h * r - t| = |h - conj(r) * t|
v1.8.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.8.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.8.0,"That is not mentioned in the paper, so it is made optional here."
v1.8.0,Project entities
v1.8.0,h projection to hyperplane
v1.8.0,r
v1.8.0,-t projection to hyperplane
v1.8.0,project to relation specific subspace and ensure constraints
v1.8.0,x_1 contraction
v1.8.0,x_2 contraction
v1.8.0,Rotate (=Hamilton product in quaternion space).
v1.8.0,Rotation in quaternion space
v1.8.0,head interaction
v1.8.0,relation interaction (notice that h has been updated)
v1.8.0,combination
v1.8.0,similarity
v1.8.0,head
v1.8.0,relation box: head
v1.8.0,relation box: tail
v1.8.0,tail
v1.8.0,power norm
v1.8.0,the relation-specific head box base shape (normalized to have a volume of 1):
v1.8.0,the relation-specific tail box base shape (normalized to have a volume of 1):
v1.8.0,head
v1.8.0,relation
v1.8.0,tail
v1.8.0,version 2: relation factor offset
v1.8.0,extension: negative (power) norm
v1.8.0,note: normalization should be done from the representations
v1.8.0,cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501
v1.8.0,version 2
v1.8.0,r_head = r_head + u * torch.ones_like(r_head)
v1.8.0,r_tail = r_tail + u * torch.ones_like(r_tail)
v1.8.0,"stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)"
v1.8.0,"remember shape for output, but reshape for transformer"
v1.8.0,"get position embeddings, shape: (seq_len, dim)"
v1.8.0,Now we are position-dependent w.r.t qualifier pairs.
v1.8.0,"seq_length, batch_size, dim"
v1.8.0,Pool output
v1.8.0,"output shape: (batch_size, dim)"
v1.8.0,reshape
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Concrete classes
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: the token ID of the padding token
v1.8.0,: the token representations
v1.8.0,: the assigned tokens for each entity
v1.8.0,needs to be lazily imported to avoid cyclic imports
v1.8.0,fill padding (nn.Embedding cannot deal with negative indices)
v1.8.0,"sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs"
v1.8.0,"contain a subset of training relations) - for that, the padding index is the last index of the Representation"
v1.8.0,resolve token representation
v1.8.0,input validation
v1.8.0,register as buffer
v1.8.0,assign sub-module
v1.8.0,apply tokenizer
v1.8.0,"get token IDs, shape: (*, num_chosen_tokens)"
v1.8.0,"lookup token representations, shape: (*, num_chosen_tokens, *shape)"
v1.8.0,: the token representations
v1.8.0,normalize triples
v1.8.0,inverse triples are created afterwards implicitly
v1.8.0,tokenize
v1.8.0,determine shape
v1.8.0,super init; has to happen *before* any parameter or buffer is assigned
v1.8.0,assign module
v1.8.0,Assign default aggregation
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Resolver
v1.8.0,Base classes
v1.8.0,Concrete classes
v1.8.0,TODO: allow relative
v1.8.0,isin() preserves the sorted order
v1.8.0,sort by decreasing degree
v1.8.0,sort by decreasing page rank
v1.8.0,input normalization
v1.8.0,determine absolute number of anchors for each strategy
v1.8.0,if pre-instantiated
v1.8.0,convert to sparse matrix
v1.8.0,symmetrize
v1.8.0,TODO: should we add self-links
v1.8.0,"adj = (adj + adj.transpose() + scipy.sparse.eye(m=adj.shape[0], format=""coo"")).tocsr()"
v1.8.0,degree for adjacency normalization
v1.8.0,power iteration
v1.8.0,TODO: vectorization?
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Resolver
v1.8.0,Base classes
v1.8.0,Concrete classes
v1.8.0,tokenize: represent entities by bag of relations
v1.8.0,collect candidates
v1.8.0,randomly sample without replacement num_tokens relations for each entity
v1.8.0,select anchors
v1.8.0,find closest anchors
v1.8.0,convert to torch
v1.8.0,verify pool
v1.8.0,choose first num_tokens
v1.8.0,TODO: vectorization?
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Resolver
v1.8.0,Base classes
v1.8.0,Concrete classes
v1.8.0,"contains: anchor_ids, entity_ids, mapping {entity_id -> {""ancs"": anchors, ""dists"": distances}}"
v1.8.0,normalize anchor_ids
v1.8.0,cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541
v1.8.0,TODO: keep distances?
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Anchor Searchers
v1.8.0,Anchor Selection
v1.8.0,Tokenizers
v1.8.0,Token Loaders
v1.8.0,Representations
v1.8.0,"TODO: use graph library, such as igraph, graph-tool, or networkit"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Resolver
v1.8.0,Base classes
v1.8.0,Concrete classes
v1.8.0,convert to adjacency matrix
v1.8.0,"compute distances between anchors and all nodes, shape: (num_anchors, num_entities)"
v1.8.0,select anchor IDs with smallest distance
v1.8.0,infer shape
v1.8.0,create adjacency matrix
v1.8.0,symmetric + self-loops
v1.8.0,"for each entity, determine anchor pool by BFS"
v1.8.0,an array storing whether node i is reachable by anchor j
v1.8.0,"an array indicating whether a node is closed, i.e., has found at least $k$ anchors"
v1.8.0,the output
v1.8.0,TODO: take all (q-1) hop neighbors before selecting from q-hop
v1.8.0,propagate one hop
v1.8.0,convergence check
v1.8.0,copy pool if we have seen enough anchors and have not yet stopped
v1.8.0,stop once we have enough
v1.8.0,TODO: can we replace this loop with something vectorized?
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Base classes
v1.8.0,Concrete classes
v1.8.0,
v1.8.0,
v1.8.0,
v1.8.0,
v1.8.0,
v1.8.0,Misc
v1.8.0,
v1.8.0,rank based metrics do not need binarized scores
v1.8.0,: the supported rank types. Most of the time equal to all rank types
v1.8.0,: whether the metric requires the number of candidates for each ranking task
v1.8.0,normalize confidence level
v1.8.0,sample metric values
v1.8.0,"bootstrap estimator (i.e., compute on sample with replacement)"
v1.8.0,cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function
v1.8.0,: The rank-based metric class that this derived metric extends
v1.8.0,"since scale and offset are constant for a given number of candidates, we have"
v1.8.0,E[scale * M + offset] = scale * E[M] + offset
v1.8.0,"since scale and offset are constant for a given number of candidates, we have"
v1.8.0,V[scale * M + offset] = scale^2 * V[M]
v1.8.0,: Z-adjusted metrics are formulated to be increasing
v1.8.0,: Z-adjusted metrics can only be applied to realistic ranks
v1.8.0,should be exactly 0.0
v1.8.0,should be exactly 1.0
v1.8.0,: Expectation/maximum reindexed metrics are formulated to be increasing
v1.8.0,: Expectation/maximum reindexed metrics can only be applied to realistic ranks
v1.8.0,should be exactly 0.0
v1.8.0,we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once
v1.8.0,now select from precomputed cumulative sums and aggregate
v1.8.0,individual ranks' expectation
v1.8.0,individual inverse ranks' variance
v1.8.0,rank aggregation
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: the lower bound
v1.8.0,: whether the lower bound is inclusive
v1.8.0,: the upper bound
v1.8.0,: whether the upper bound is inclusive
v1.8.0,: The name of the metric
v1.8.0,: a link to further information
v1.8.0,: whether the metric needs binarized scores
v1.8.0,": whether it is increasing, i.e., larger values are better"
v1.8.0,: the value range
v1.8.0,: synonyms for this metric
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: A description of the metric
v1.8.0,: The function that runs the metric
v1.8.0,: Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics
v1.8.0,: This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`
v1.8.0,"TODO there's something wrong with this, so add it later"
v1.8.0,classifier_annotator.higher(
v1.8.0,"rmc.pr_auc_score,"
v1.8.0,"name=""AUC-PR"","
v1.8.0,"description=""Area Under the Precision-Recall Curve"","
v1.8.0,"link=""https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score"","
v1.8.0,)
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,don't worry about functions because they can't be specified by JSON.
v1.8.0,Could make a better mo
v1.8.0,later could extend for other non-JSON valid types
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Score with original triples
v1.8.0,Score with inverse triples
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Create directory in which all experimental artifacts are saved
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,distribute the deteriorated triples across the remaining factories
v1.8.0,"'kinships',"
v1.8.0,"'umls',"
v1.8.0,"'codexsmall',"
v1.8.0,"'wn18',"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: Functions for specifying exotic resources with a given prefix
v1.8.0,: Functions for specifying exotic resources based on their file extension
v1.8.0,Input validation
v1.8.0,convert to numpy
v1.8.0,Additional columns
v1.8.0,convert PyTorch tensors to numpy
v1.8.0,convert to dataframe
v1.8.0,Re-order columns
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.8.0,TODO vectorize code
v1.8.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Split triples
v1.8.0,Sorting ensures consistent results when the triples are permuted
v1.8.0,Create mapping
v1.8.0,Sorting ensures consistent results when the triples are permuted
v1.8.0,Create mapping
v1.8.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.8.0,Filter all non-existent triples
v1.8.0,Note: Unique changes the order of the triples
v1.8.0,Note: Using unique means implicit balancing of training samples
v1.8.0,normalize input
v1.8.0,TODO: method is_inverse?
v1.8.0,TODO: inverse of inverse?
v1.8.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.8.0,Id of inverse relation: relation + 1
v1.8.0,: The mapping from labels to IDs.
v1.8.0,: The inverse mapping for label_to_id; initialized automatically
v1.8.0,: A vectorized version of entity_label_to_id; initialized automatically
v1.8.0,: A vectorized version of entity_id_to_label; initialized automatically
v1.8.0,Normalize input
v1.8.0,label
v1.8.0,Filter for entities
v1.8.0,Filter for relations
v1.8.0,No filter
v1.8.0,check new label to ID mappings
v1.8.0,Make new triples factories for each group
v1.8.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.0,prepare metadata
v1.8.0,Delegate to function
v1.8.0,"restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed"
v1.8.0,load base
v1.8.0,load numeric triples
v1.8.0,store numeric triples
v1.8.0,store metadata
v1.8.0,Check if the triples are inverted already
v1.8.0,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.8.0,contained if and only if create_inverse_triples is True.
v1.8.0,Generate entity mapping if necessary
v1.8.0,Generate relation mapping if necessary
v1.8.0,Map triples of labels to triples of IDs.
v1.8.0,TODO: Check if lazy evaluation would make sense
v1.8.0,store entity/relation to ID
v1.8.0,load entity/relation to ID
v1.8.0,pre-filter to keep only topk
v1.8.0,if top is larger than the number of available options
v1.8.0,generate text
v1.8.0,vectorized label lookup
v1.8.0,Re-order columns
v1.8.0,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.8.0,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Split indices
v1.8.0,Split triples
v1.8.0,select one triple per relation
v1.8.0,maintain set of covered entities
v1.8.0,"Select one triple for each head/tail entity, which is not yet covered."
v1.8.0,create mask
v1.8.0,Prepare split index
v1.8.0,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.8.0,[...] is necessary for Python 3.7 compatibility
v1.8.0,base cases
v1.8.0,IDs not in training
v1.8.0,triples with exclusive test IDs
v1.8.0,While there are still triples that should be moved to the training set
v1.8.0,Pick a random triple to move over to the training triples
v1.8.0,add to training
v1.8.0,remove from testing
v1.8.0,Recalculate the move_id_mask
v1.8.0,Make sure that the first element has all the right stuff in it
v1.8.0,backwards compatibility
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,constants
v1.8.0,constants
v1.8.0,unary
v1.8.0,binary
v1.8.0,ternary
v1.8.0,column names
v1.8.0,return candidates
v1.8.0,index triples
v1.8.0,incoming relations per entity
v1.8.0,outgoing relations per entity
v1.8.0,indexing triples for fast join r1 & r2
v1.8.0,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.8.0,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.8.0,actual evaluation of the pattern
v1.8.0,skip empty support
v1.8.0,TODO: Can this happen after pre-filtering?
v1.8.0,"sort first, for triple order invariance"
v1.8.0,TODO: what is the support?
v1.8.0,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.8.0,sort decreasingly. i dominates j for all j > i in x-dimension
v1.8.0,"if it is also dominated by any y, it is not part of the skyline"
v1.8.0,"group by (relation id, pattern type)"
v1.8.0,"for each group, yield from skyline"
v1.8.0,determine patterns from triples
v1.8.0,drop zero-confidence
v1.8.0,keep only skyline
v1.8.0,create data frame
v1.8.0,iterate relation types
v1.8.0,drop zero-confidence
v1.8.0,keep only skyline
v1.8.0,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.8.0,base = skyline(base)
v1.8.0,create data frame
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO: the same
v1.8.0,": the positive triples, shape: (batch_size, 3)"
v1.8.0,": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)"
v1.8.0,": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)"
v1.8.0,TODO: some negative samplers require batches
v1.8.0,"shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.8.0,"each shape: (1, 3), (1, k, 3), (1, k, 3)?"
v1.8.0,cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset
v1.8.0,indexing
v1.8.0,initialize
v1.8.0,sample iteratively
v1.8.0,determine weights
v1.8.0,randomly choose a vertex which has not been chosen yet
v1.8.0,normalize to probabilities
v1.8.0,sample a start node
v1.8.0,get list of neighbors
v1.8.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.8.0,visit target node
v1.8.0,decrease sample counts
v1.8.0,convert to csr for fast row slicing
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,check validity
v1.8.0,path compression
v1.8.0,collect connected components using union find with path compression
v1.8.0,get representatives
v1.8.0,already merged
v1.8.0,make x the smaller one
v1.8.0,merge
v1.8.0,extract partitions
v1.8.0,safe division for empty sets
v1.8.0,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.8.0,duplicates
v1.8.0,we are not interested in self-similarity
v1.8.0,compute similarities
v1.8.0,Calculate which relations are the inverse ones
v1.8.0,get existing IDs
v1.8.0,remove non-existing ID from label mapping
v1.8.0,create translation tensor
v1.8.0,get entities and relations occurring in triples
v1.8.0,generate ID translation and new label to Id mappings
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.8.0,seamless loading and saving of training checkpoints
v1.8.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.8.0,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.8.0,"If a checkpoint file is given, it must be loaded if it exists already"
v1.8.0,"If the stopper dict has any keys, those are written back to the stopper"
v1.8.0,The checkpoint frequency needs to be set to save checkpoints
v1.8.0,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.8.0,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.8.0,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.8.0,send model to device before going into the internal training loop
v1.8.0,Ensure the release of memory
v1.8.0,Clear optimizer
v1.8.0,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.8.0,due to the patience continue to train after the best epoch and thus alter the model
v1.8.0,Create a path
v1.8.0,Prepare all of the callbacks
v1.8.0,"Register a callback for the result tracker, if given"
v1.8.0,"Register a callback for the early stopper, if given"
v1.8.0,TODO should mode be passed here?
v1.8.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.8.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.8.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.8.0,return the relevant parameters slice_size and batch_size
v1.8.0,Force weight initialization if training continuation is not explicitly requested.
v1.8.0,Reset the weights
v1.8.0,"afterwards, some parameters may be on the wrong device"
v1.8.0,Create new optimizer
v1.8.0,Create a new lr scheduler and add the optimizer
v1.8.0,Ensure the model is on the correct device
v1.8.0,"When size probing, we don't want progress bars"
v1.8.0,Create progress bar
v1.8.0,Save the time to track when the saved point was available
v1.8.0,Training Loop
v1.8.0,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.8.0,Enforce training mode
v1.8.0,Accumulate loss over epoch
v1.8.0,Batching
v1.8.0,Only create a progress bar when not in size probing mode
v1.8.0,Flag to check when to quit the size probing
v1.8.0,Recall that torch *accumulates* gradients. Before passing in a
v1.8.0,"new instance, you need to zero out the gradients from the old instance"
v1.8.0,Get batch size of current batch (last batch may be incomplete)
v1.8.0,accumulate gradients for whole batch
v1.8.0,forward pass call
v1.8.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.8.0,update parameters according to optimizer
v1.8.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.8.0,constraints are no longer applied
v1.8.0,For testing purposes we're only interested in processing one batch
v1.8.0,When size probing we don't need the losses
v1.8.0,Update learning rate scheduler
v1.8.0,Track epoch loss
v1.8.0,Print loss information to console
v1.8.0,Save the last successful finished epoch
v1.8.0,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.8.0,During automatic memory optimization only the error message is of interest
v1.8.0,When there wasn't a best epoch the checkpoint path should be None
v1.8.0,Delete temporary best epoch model
v1.8.0,Includes a call to result_tracker.log_metrics
v1.8.0,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.8.0,MyPy overrides are because you should
v1.8.0,When there wasn't a best epoch the checkpoint path should be None
v1.8.0,Delete temporary best epoch model
v1.8.0,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.8.0,at that state
v1.8.0,Delete temporary best epoch model
v1.8.0,forward pass
v1.8.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.8.0,correction for loss reduction
v1.8.0,backward pass
v1.8.0,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.8.0,Set upper bound
v1.8.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.8.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.0,"Only if a cuda device is available, the random state is accessed"
v1.8.0,This is an entire checkpoint for the optional best model when using early stopping
v1.8.0,Saving triples factory related states
v1.8.0,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.8.0,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.8.0,Check whether the triples factory mappings match those from the checkpoints
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Shuffle each epoch
v1.8.0,Lazy-splitting into batches
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,disable automatic batching
v1.8.0,Slicing is not possible in sLCWA training loops
v1.8.0,split batch
v1.8.0,send to device
v1.8.0,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.8.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.8.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.8.0,Compute negative and positive scores
v1.8.0,Slicing is not possible for sLCWA
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO how to pass inductive mode
v1.8.0,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.8.0,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.8.0,: A hint for constructing a :class:`MultiTrainingCallback`
v1.8.0,: A collection of callbacks
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,normalize target column
v1.8.0,The type inference is so confusing between the function switching
v1.8.0,and polymorphism introduced by slicability that these need to be ignored
v1.8.0,Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number
v1.8.0,of total entities from another inductive inference factory
v1.8.0,Split batch components
v1.8.0,Send batch to device
v1.8.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.8.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,To make MyPy happy
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,now: smaller is better
v1.8.0,: the number of reported results with no improvement after which training will be stopped
v1.8.0,the minimum relative improvement necessary to consider it an improved result
v1.8.0,"whether a larger value is better, or a smaller."
v1.8.0,: The epoch at which the best result occurred
v1.8.0,: The best result so far
v1.8.0,: The remaining patience
v1.8.0,check for improvement
v1.8.0,stop if the result did not improve more than delta for patience evaluations
v1.8.0,: The model
v1.8.0,: The evaluator
v1.8.0,: The triples to use for training (to be used during filtered evaluation)
v1.8.0,: The triples to use for evaluation
v1.8.0,: Size of the evaluation batches
v1.8.0,: Slice size of the evaluation batches
v1.8.0,: The number of epochs after which the model is evaluated on validation set
v1.8.0,: The number of iterations (one iteration can correspond to various epochs)
v1.8.0,: with no improvement after which training will be stopped.
v1.8.0,: The name of the metric to use
v1.8.0,: The minimum relative improvement necessary to consider it an improved result
v1.8.0,: The metric results from all evaluations
v1.8.0,": Whether a larger value is better, or a smaller"
v1.8.0,: The result tracker
v1.8.0,: Callbacks when after results are calculated
v1.8.0,: Callbacks when training gets continued
v1.8.0,: Callbacks when training is stopped early
v1.8.0,: Did the stopper ever decide to stop?
v1.8.0,TODO: Fix this
v1.8.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.8.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.8.0,Evaluate
v1.8.0,Only perform time consuming checks for the first call.
v1.8.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.8.0,Append to history
v1.8.0,TODO need a test that this all re-instantiates properly
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Utils
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,parsing metrics
v1.8.0,metric pattern = side?.type?.metric.k?
v1.8.0,: The metric key
v1.8.0,": Side of the metric, or ""both"""
v1.8.0,: The rank type
v1.8.0,normalize metric name
v1.8.0,normalize side
v1.8.0,normalize rank type
v1.8.0,normalize keys
v1.8.0,TODO: this can only normalize rank-based metrics!
v1.8.0,TODO: find a better way to handle this
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO: fix this upstream / make metric.score comply to signature
v1.8.0,Transfer to cpu and convert to numpy
v1.8.0,Ensure that each key gets counted only once
v1.8.0,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.8.0,"Because the order of the values of an dictionary is not guaranteed,"
v1.8.0,we need to retrieve scores and masks using the exact same key order.
v1.8.0,Clear buffers
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The optimistic rank is the rank when assuming all options with an equal score are placed
v1.8.0,: behind the current test triple.
v1.8.0,": shape: (batch_size,)"
v1.8.0,": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank"
v1.8.0,: over all permutations of the elements with the same score as the currently considered option.
v1.8.0,": shape: (batch_size,)"
v1.8.0,: The pessimistic rank is the rank when assuming all options with an equal score are placed
v1.8.0,: in front of current test triple.
v1.8.0,": shape: (batch_size,)"
v1.8.0,: The number of options is the number of items considered in the ranking. It may change for
v1.8.0,: filtered evaluation
v1.8.0,": shape: (batch_size,)"
v1.8.0,The optimistic rank is the rank when assuming all options with an
v1.8.0,"equal score are placed behind the currently considered. Hence, the"
v1.8.0,"rank is the number of options with better scores, plus one, as the"
v1.8.0,rank is one-based.
v1.8.0,The pessimistic rank is the rank when assuming all options with an
v1.8.0,"equal score are placed in front of the currently considered. Hence,"
v1.8.0,the rank is the number of options which have at least the same score
v1.8.0,minus one (as the currently considered option in included in all
v1.8.0,"options). As the rank is one-based, we have to add 1, which nullifies"
v1.8.0,"the ""minus 1"" from before."
v1.8.0,The realistic rank is the average of the optimistic and pessimistic
v1.8.0,"rank, and hence the expected rank over all permutations of the elements"
v1.8.0,with the same score as the currently considered option.
v1.8.0,"We set values which should be ignored to NaN, hence the number of options"
v1.8.0,which should be considered is given by
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"TODO remove this, it makes code much harder to reason about"
v1.8.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.8.0,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.8.0,Clear the ranks from the current evaluator
v1.8.0,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.8.0,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.8.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.8.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.8.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.0,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.8.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.0,values_dict[key] will always be an int at this point
v1.8.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.8.0,Test if slicing is implemented for the required functions of this model
v1.8.0,Split batch
v1.8.0,Bind shape
v1.8.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.8.0,Warn if all entities will be filtered
v1.8.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.8.0,TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place
v1.8.0,verify that the triples have been filtered
v1.8.0,Filter triples if necessary
v1.8.0,Send to device
v1.8.0,Ensure evaluation mode
v1.8.0,Prepare for result filtering
v1.8.0,Send tensors to device
v1.8.0,Prepare batches
v1.8.0,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.8.0,Show progressbar
v1.8.0,Flag to check when to quit the size probing
v1.8.0,Disable gradient tracking
v1.8.0,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.8.0,batch-wise processing
v1.8.0,If we only probe sizes we do not need more than one batch
v1.8.0,Finalize
v1.8.0,Create filter
v1.8.0,Select scores of true
v1.8.0,overwrite filtered scores
v1.8.0,The scores for the true triples have to be rewritten to the scores tensor
v1.8.0,the rank-based evaluators needs the true scores with trailing 1-dim
v1.8.0,Create a positive mask with the size of the scores from the positive filter
v1.8.0,Restrict to entities of interest
v1.8.0,process scores
v1.8.0,optinally restrict triples (nop if no restriction)
v1.8.0,evaluation triples as dataframe
v1.8.0,determine filter triples
v1.8.0,infer num_entities if not given
v1.8.0,"TODO: unique, or max ID + 1?"
v1.8.0,optionally restrict triples
v1.8.0,compute candidate set sizes for different targets
v1.8.0,TODO: extend to relations?
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,terminate early if there are no ranks
v1.8.0,flatten dictionaries
v1.8.0,individual side
v1.8.0,combined
v1.8.0,Clear buffers
v1.8.0,repeat
v1.8.0,default for inductive LP by [teru2020]
v1.8.0,verify input
v1.8.0,TODO: do not require to compute all scores beforehand
v1.8.0,super.evaluation assumes that the true scores are part of all_scores
v1.8.0,write back correct num_entities
v1.8.0,TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.8.0,to create the class
v1.8.0,TODO: update to hint + kwargs
v1.8.0,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default regularizer class
v1.8.0,: The default parameters for the default regularizer class
v1.8.0,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.8.0,Make sure that all modules with parameters do have a reset_parameters method.
v1.8.0,Recursively visit all sub-modules
v1.8.0,skip self
v1.8.0,Track parents for blaming
v1.8.0,call reset_parameters if possible
v1.8.0,initialize from bottom to top
v1.8.0,This ensures that specialized initializations will take priority over the default ones of its components.
v1.8.0,emit warning if there where parameters which were not initialised by reset_parameters.
v1.8.0,Additional debug information
v1.8.0,TODO: allow max_id being present in representation_kwargs; if it matches max_id
v1.8.0,TODO: we could infer some shapes from the given interaction shape information
v1.8.0,check max-id
v1.8.0,check shapes
v1.8.0,: The entity representations
v1.8.0,: The relation representations
v1.8.0,: The weight regularizers
v1.8.0,: The interaction function
v1.8.0,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.8.0,"model.modules(). Thereby, we can collect them automatically."
v1.8.0,Explicitly call reset_parameters to trigger initialization
v1.8.0,normalize input
v1.8.0,Note: slicing cannot be used here: the indices for score_hrt only have a batch
v1.8.0,"dimension, and slicing along this dimension is already considered by sub-batching."
v1.8.0,Note: we do not delegate to the general method for performance reasons
v1.8.0,Note: repetition is not necessary here
v1.8.0,normalization
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,train model
v1.8.0,"note: as this is an example, the model is only trained for a few epochs,"
v1.8.0,"but not until convergence. In practice, you would usually first verify that"
v1.8.0,"the model is sufficiently good in prediction, before looking at uncertainty scores"
v1.8.0,predict triple scores with uncertainty
v1.8.0,"use a larger number of samples, to increase quality of uncertainty estimate"
v1.8.0,get most and least uncertain prediction on training set
v1.8.0,: The scores
v1.8.0,": The uncertainty, in the same shape as scores"
v1.8.0,Enforce evaluation mode
v1.8.0,set dropout layers to training mode
v1.8.0,draw samples
v1.8.0,compute mean and std
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Train a model (quickly)
v1.8.0,Get scores for *all* triples
v1.8.0,Get scores for top 15 triples
v1.8.0,initialize buffer on device
v1.8.0,"reshape, shape: (batch_size * num_entities,)"
v1.8.0,get top scores within batch
v1.8.0,append to global top scores
v1.8.0,reduce size if necessary
v1.8.0,initialize buffer on cpu
v1.8.0,Explicitly create triples
v1.8.0,"TODO: in the future, we may want to expose this method"
v1.8.0,set model to evaluation mode
v1.8.0,calculate batch scores
v1.8.0,base case: infer maximum batch size
v1.8.0,base case: single batch
v1.8.0,TODO: this could happen because of AMO
v1.8.0,TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043
v1.8.0,no OOM error.
v1.8.0,make sure triples are a numpy array
v1.8.0,make sure triples are 2d
v1.8.0,convert to ID-based
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"This empty 1-element tensor doesn't actually do anything,"
v1.8.0,but is necessary since models with no grad params blow
v1.8.0,up the optimizer
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default loss function class
v1.8.0,: The default parameters for the default loss function class
v1.8.0,: The instance of the loss
v1.8.0,Random seeds have to set before the embeddings are initialized
v1.8.0,Loss
v1.8.0,TODO: why do we need to empty the cache?
v1.8.0,"TODO: this currently compute (batch_size, num_relations) instead,"
v1.8.0,"i.e., scores for normal and inverse relations"
v1.8.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.8.0,send to device
v1.8.0,special handling of inverse relations
v1.8.0,"when trained on inverse relations, the internal relation ID is twice the original relation ID"
v1.8.0,: The default regularizer class
v1.8.0,: The default parameters for the default regularizer class
v1.8.0,: The instance of the regularizer
v1.8.0,Regularizer
v1.8.0,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.8.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.8.0,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.8.0,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.8.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.8.0,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.8.0,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.8.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.8.0,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.8.0,: Primary embeddings for entities
v1.8.0,: Primary embeddings for relations
v1.8.0,"make sure to call this first, to reset regularizer state!"
v1.8.0,The following lines add in a post-init hook to all subclasses
v1.8.0,such that the reset_parameters_() function is run
v1.8.0,"sorry mypy, but this kind of evil must be permitted."
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Base Models
v1.8.0,Concrete Models
v1.8.0,Inductive Models
v1.8.0,Evaluation-only models
v1.8.0,Utils
v1.8.0,Abstract Models
v1.8.0,We might be able to relax this later
v1.8.0,baseline models behave differently
v1.8.0,Old style models should never be looked up
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Create an MLP for string aggregation
v1.8.0,always create representations for normal and inverse relations and padding
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,default composition is DistMult-style
v1.8.0,Saving edge indices for all the supplied splits
v1.8.0,Extract all entity and relation representations
v1.8.0,Perform message passing and get updated states
v1.8.0,Use updated entity and relation states to extract requested IDs
v1.8.0,TODO I got lost in all the Representation Modules and shape casting and wrote this ;(
v1.8.0,normalization
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,NodePiece
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO rethink after RGCN update
v1.8.0,TODO: other parameters?
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default loss function class
v1.8.0,: The default parameters for the default loss function class
v1.8.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.8.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.8.0,ConvE should be trained with inverse triples
v1.8.0,ConvE uses one bias for each entity
v1.8.0,Automatic calculation of remaining dimensions
v1.8.0,Parameter need to fulfil:
v1.8.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.8.0,weights
v1.8.0,"batch_size, num_input_channels, 2*height, width"
v1.8.0,"batch_size, num_input_channels, 2*height, width"
v1.8.0,"batch_size, num_input_channels, 2*height, width"
v1.8.0,"(N,C_out,H_out,W_out)"
v1.8.0,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.8.0,Embedding Regularization
v1.8.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.8.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.0,Embedding Regularization
v1.8.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.0,Embedding Regularization
v1.8.0,Code to repeat each item successively instead of the entire tensor
v1.8.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,head representation
v1.8.0,tail representation
v1.8.0,"Since CP uses different representations for entities in head / tail role,"
v1.8.0,"the current solution is a bit hacky, and may be improved. See discussion"
v1.8.0,on https://github.com/pykeen/pykeen/pull/663.
v1.8.0,Override to allow different head and tail entity representations
v1.8.0,normalization
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default loss function class
v1.8.0,: The default parameters for the default loss function class
v1.8.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.8.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.8.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.8.0,use torch's native complex data type
v1.8.0,use torch's native complex data type
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.8.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.8.0,: a normalized weight of 10 is used.
v1.8.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.8.0,Get embeddings
v1.8.0,"shape: (b, d)"
v1.8.0,"shape: (b, d, d)"
v1.8.0,"shape: (b, d)"
v1.8.0,Compute scores
v1.8.0,Regularization
v1.8.0,Compute scores
v1.8.0,Regularization
v1.8.0,Get embeddings
v1.8.0,Compute scores
v1.8.0,Regularization
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.8.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.8.0,In the code base only the weights of the output layer are used for regularization
v1.8.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,comment:
v1.8.0,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.8.0,uses float64
v1.8.0,entity bias for head
v1.8.0,entity bias for tail
v1.8.0,relation offset
v1.8.0,diagonal relation transformation matrix
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default entity normalizer parameters
v1.8.0,: The entity representations are normalized to L2 unit length
v1.8.0,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.8.0,"update initializer settings, cf."
v1.8.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.8.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.8.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.8.0,in the original implementation the embeddings are initialized in one parameter
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,"w: (k, d, d)"
v1.8.0,"vh: (k, d)"
v1.8.0,"vt: (k, d)"
v1.8.0,"b: (k,)"
v1.8.0,"u: (k,)"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The regularizer used by [yang2014]_ for DistMult
v1.8.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.8.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.8.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.8.0,: The LP settings used by [yang2014]_ for DistMult
v1.8.0,Bilinear product
v1.8.0,*: Elementwise multiplication
v1.8.0,Get embeddings
v1.8.0,Compute score
v1.8.0,Only regularize relation embeddings
v1.8.0,Get embeddings
v1.8.0,Rank against all entities
v1.8.0,Only regularize relation embeddings
v1.8.0,Get embeddings
v1.8.0,Rank against all entities
v1.8.0,Only regularize relation embeddings
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default settings for the entity constrainer
v1.8.0,mean
v1.8.0,diagonal covariance
v1.8.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.8.0,mean
v1.8.0,diagonal covariance
v1.8.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,diagonal entries
v1.8.0,off-diagonal
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The custom regularizer used by [wang2014]_ for TransH
v1.8.0,: The settings used by [wang2014]_ for TransH
v1.8.0,embeddings
v1.8.0,Normalise the normal vectors by their l2 norms
v1.8.0,TODO: Add initialization
v1.8.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.8.0,which enforces the defined soft constraints.
v1.8.0,Get embeddings
v1.8.0,Project to hyperplane
v1.8.0,Regularization term
v1.8.0,Get embeddings
v1.8.0,Project to hyperplane
v1.8.0,Regularization term
v1.8.0,Get embeddings
v1.8.0,Project to hyperplane
v1.8.0,Regularization term
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,TODO: Initialize from TransE
v1.8.0,embeddings
v1.8.0,"project to relation specific subspace, shape: (b, e, d_r)"
v1.8.0,ensure constraints
v1.8.0,"evaluate score function, shape: (b, e)"
v1.8.0,Get embeddings
v1.8.0,Get embeddings
v1.8.0,Get embeddings
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,": The default strategy for optimizing the model""s hyper-parameters"
v1.8.0,TODO: Decomposition kwargs
v1.8.0,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.8.0,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.8.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.8.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.8.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,combined representation
v1.8.0,Resolve interaction function
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default loss function class
v1.8.0,: The default parameters for the default loss function class
v1.8.0,Core tensor
v1.8.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.8.0,Dropout
v1.8.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.8.0,Abbreviation
v1.8.0,Compute h_n = DO(BN(h))
v1.8.0,Compute wr = DO(W x_2 r)
v1.8.0,compute whr = DO(BN(h_n x_1 wr))
v1.8.0,Compute whr x_3 t
v1.8.0,Get embeddings
v1.8.0,Compute scores
v1.8.0,Get embeddings
v1.8.0,Compute scores
v1.8.0,Get embeddings
v1.8.0,Compute scores
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,Get embeddings
v1.8.0,TODO: Use torch.cdist
v1.8.0,"There were some performance/memory issues with cdist, cf."
v1.8.0,"https://github.com/pytorch/pytorch/issues?q=cdist however, @mberr thinks"
v1.8.0,they are mostly resolved by now. A Benefit would be that we can harness the
v1.8.0,"future (performance) improvements made by the core torch developers. However,"
v1.8.0,this will require some benchmarking.
v1.8.0,Get embeddings
v1.8.0,TODO: Use torch.cdist (see note above in score_hrt())
v1.8.0,Get embeddings
v1.8.0,TODO: Use torch.cdist (see note above in score_hrt())
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,entity bias for head
v1.8.0,relation position head
v1.8.0,relation shape head
v1.8.0,relation size head
v1.8.0,relation position tail
v1.8.0,relation shape tail
v1.8.0,relation size tail
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default loss function class
v1.8.0,: The default parameters for the default loss function class
v1.8.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.8.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.8.0,": regularization term by the number of elements, which is 200."
v1.8.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.8.0,(head) entity
v1.8.0,tail entity
v1.8.0,relations
v1.8.0,inverse relations
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.8.0,weight initialization
v1.8.0,Get embeddings
v1.8.0,Embedding Regularization
v1.8.0,Concatenate them
v1.8.0,Compute scores
v1.8.0,Get embeddings
v1.8.0,Embedding Regularization
v1.8.0,First layer can be unrolled
v1.8.0,Send scores through rest of the network
v1.8.0,Get embeddings
v1.8.0,Embedding Regularization
v1.8.0,First layer can be unrolled
v1.8.0,Send scores through rest of the network
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,Regular relation embeddings
v1.8.0,The relation-specific interaction vector
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Create an MLP for string aggregation
v1.8.0,always create representations for normal and inverse relations and padding
v1.8.0,normalize embedding specification
v1.8.0,prepare token representations & kwargs
v1.8.0,"max_id=triples_factory.num_relations,  # will get added by ERModel"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,Decompose into real and imaginary part
v1.8.0,Rotate (=Hadamard product in complex space).
v1.8.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.8.0,Get embeddings
v1.8.0,Compute scores
v1.8.0,Embedding Regularization
v1.8.0,Get embeddings
v1.8.0,Rank against all entities
v1.8.0,Compute scores
v1.8.0,Embedding Regularization
v1.8.0,Get embeddings
v1.8.0,r expresses a rotation in complex plane.
v1.8.0,The inverse rotation is expressed by the complex conjugate of r.
v1.8.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.8.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.8.0,|h * r - t| = |h - conj(r) * t|
v1.8.0,Rank against all entities
v1.8.0,Compute scores
v1.8.0,Embedding Regularization
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default loss function class
v1.8.0,: The default parameters for the default loss function class
v1.8.0,Global entity projection
v1.8.0,Global relation projection
v1.8.0,Global combination bias
v1.8.0,Global combination bias
v1.8.0,Get embeddings
v1.8.0,Compute score
v1.8.0,Get embeddings
v1.8.0,Rank against all entities
v1.8.0,Get embeddings
v1.8.0,Rank against all entities
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Normalize relation embeddings
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default loss function class
v1.8.0,: The default parameters for the default loss function class
v1.8.0,: The LP settings used by [zhang2019]_ for QuatE.
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default settings for the entity constrainer
v1.8.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.8.0,Circular correlation of entity embeddings
v1.8.0,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.8.0,compatibility: new style fft returns complex tensor
v1.8.0,Hadamard product in frequency domain
v1.8.0,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.8.0,inner product with relation embedding
v1.8.0,Embedding Regularization
v1.8.0,Embedding Regularization
v1.8.0,Embedding Regularization
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default loss function class
v1.8.0,: The default parameters for the default loss function class
v1.8.0,Get embeddings
v1.8.0,Embedding Regularization
v1.8.0,Concatenate them
v1.8.0,Predict t embedding
v1.8.0,compare with all t's
v1.8.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.8.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.0,Embedding Regularization
v1.8.0,Concatenate them
v1.8.0,Predict t embedding
v1.8.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.0,Embedding Regularization
v1.8.0,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.8.0,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.8.0,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.8.0,Extend t
v1.8.0,Concatenate them
v1.8.0,Predict t embedding
v1.8.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.8.0,The results have to be realigned with the expected output of the score_h function
v1.8.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default parameters for the default loss function class
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default loss function class
v1.8.0,: The default parameters for the default loss function class
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the model's hyper-parameters
v1.8.0,: The default parameters for the default loss function class
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"max_id=max_id,  # will be added by ERModel"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,create sparse matrix of absolute counts
v1.8.0,normalize to relative counts
v1.8.0,base case
v1.8.0,"note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could"
v1.8.0,"stay sparse here, with a potential of a huge memory benefit on large datasets!"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,These operations are deterministic and a random seed can be fixed
v1.8.0,just to avoid warnings
v1.8.0,compute relation similarity matrix
v1.8.0,mapping from relations to head/tail entities
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"if we really need access to the path later, we can expose it as a property"
v1.8.0,via self.writer.log_dir
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The WANDB run
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The name of the run
v1.8.0,": The configuration dictionary, a mapping from name -> value"
v1.8.0,: Should metrics be stored when running ``log_metrics()``?
v1.8.0,": The metrics, a mapping from step -> (name -> value)"
v1.8.0,: A hint for constructing a :class:`MultiResultTracker`
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Base classes
v1.8.0,Concrete classes
v1.8.0,Utilities
v1.8.0,always add a Python result tracker for storing the configuration
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The file extension for this writer (do not include dot)
v1.8.0,: The file where the results are written to.
v1.8.0,as_uri() requires the path to be absolute. resolve additionally also normalizes the path
v1.8.0,: The column names
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,store set of triples
v1.8.0,: some prime numbers for tuple hashing
v1.8.0,: The bit-array for the Bloom filter data structure
v1.8.0,Allocate bit array
v1.8.0,calculate number of hashing rounds
v1.8.0,index triples
v1.8.0,Store some meta-data
v1.8.0,pre-hash
v1.8.0,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,At least make sure to not replace the triples by the original value
v1.8.0,"To make sure we don't replace the {head, relation, tail} by the"
v1.8.0,original value we shift all values greater or equal than the original value by one up
v1.8.0,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.8.0,Set the indices
v1.8.0,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.8.0,Bind the total number of negatives to sample in this batch
v1.8.0,Equally corrupt all sides
v1.8.0,"Do not detach, as no gradients should flow into the indices."
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.8.0,: A filterer for negative batches
v1.8.0,create unfiltered negative batch by corruption
v1.8.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Utils
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.8.0,create index structure
v1.8.0,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.8.0,: The concatenated sorted sets of head/tail entities
v1.8.0,"shape: (batch_size, num_neg_per_pos, 3)"
v1.8.0,Uniformly sample from head/tail offsets
v1.8.0,get corresponding entity
v1.8.0,"and position within triple (0: head, 2: tail)"
v1.8.0,write into negative batch
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Preprocessing: Compute corruption probabilities
v1.8.0,"compute tph, i.e. the average number of tail entities per head"
v1.8.0,"compute hpt, i.e. the average number of head entities per tail"
v1.8.0,Set parameter for Bernoulli distribution
v1.8.0,Decide whether to corrupt head or tail
v1.8.0,clone positive batch for corruption (.repeat_interleave creates a copy)
v1.8.0,flatten mask
v1.8.0,Tails are corrupted if heads are not corrupted
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The random seed used at the beginning of the pipeline
v1.8.0,: The model trained by the pipeline
v1.8.0,: The training triples
v1.8.0,: The training loop used by the pipeline
v1.8.0,: The losses during training
v1.8.0,: The results evaluated by the pipeline
v1.8.0,: How long in seconds did training take?
v1.8.0,: How long in seconds did evaluation take?
v1.8.0,: An early stopper
v1.8.0,: The configuration
v1.8.0,: Any additional metadata as a dictionary
v1.8.0,: The version of PyKEEN used to create these results
v1.8.0,: The git hash of PyKEEN used to create these results
v1.8.0,TODO use pathlib here
v1.8.0,"note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:"
v1.8.0,"when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless"
v1.8.0,note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()
v1.8.0,only one original value => assume this to be the mean
v1.8.0,multiple values => assume they correspond to individual trials
v1.8.0,metrics accumulates rows for a dataframe for comparison against the original reported results (if any)
v1.8.0,"TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)"
v1.8.0,summarize
v1.8.0,skip special parameters
v1.8.0,FIXME this should never happen.
v1.8.0,1. Dataset
v1.8.0,2. Model
v1.8.0,3. Loss
v1.8.0,4. Regularizer
v1.8.0,5. Optimizer
v1.8.0,5.1 Learning Rate Scheduler
v1.8.0,6. Training Loop
v1.8.0,7. Training (ronaldo style)
v1.8.0,8. Evaluation
v1.8.0,9. Tracking
v1.8.0,Misc
v1.8.0,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.8.0,used random_seed to ensure reproducible results
v1.8.0,We have to set clear optimizer to False since training should be continued
v1.8.0,Start tracking
v1.8.0,evaluation restriction to a subset of entities/relations
v1.8.0,TODO should training be reset?
v1.8.0,TODO should kwargs for loss and regularizer be checked and raised for?
v1.8.0,Log model parameters
v1.8.0,Log loss parameters
v1.8.0,the loss was already logged as part of the model kwargs
v1.8.0,"loss=loss_resolver.normalize_inst(model_instance.loss),"
v1.8.0,Log regularizer parameters
v1.8.0,Stopping
v1.8.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.8.0,Add logging for debugging
v1.8.0,Train like Cristiano Ronaldo
v1.8.0,Build up a list of triples if we want to be in the filtered setting
v1.8.0,"If the user gave custom ""additional_filter_triples"""
v1.8.0,Determine whether the validation triples should also be filtered while performing test evaluation
v1.8.0,TODO consider implications of duplicates
v1.8.0,Evaluate
v1.8.0,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.8.0,Add logging about evaluator for debugging
v1.8.0,"If the evaluation still fail using the CPU, the error is raised"
v1.8.0,"When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is"
v1.8.0,restarted with PyKEEN's automatic memory optimization
v1.8.0,"When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation"
v1.8.0,is restarted using the cpu
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Imported from PyTorch
v1.8.0,: A wrapper around the hidden scheduler base class
v1.8.0,: The default strategy for optimizing the lr_schedulers' hyper-parameters
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,TODO what happens if already exists?
v1.8.0,TODO incorporate setting of random seed
v1.8.0,pipeline_kwargs=dict(
v1.8.0,"random_seed=random_non_negative_int(),"
v1.8.0,"),"
v1.8.0,Add dataset to current_pipeline
v1.8.0,"Training, test, and validation paths are provided"
v1.8.0,Add loss function to current_pipeline
v1.8.0,Add regularizer to current_pipeline
v1.8.0,Add optimizer to current_pipeline
v1.8.0,Add training approach to current_pipeline
v1.8.0,Add evaluation
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The link to the zip file
v1.8.0,: The hex digest for the zip file
v1.8.0,Input validation.
v1.8.0,"left side has files ending with 1, right side with 2"
v1.8.0,For downloading
v1.8.0,For splitting
v1.8.0,Whether to create inverse triples
v1.8.0,ensure file is present
v1.8.0,create triples factory
v1.8.0,split
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,": The mapping from (graph-pair, side) to triple file name"
v1.8.0,: The internal dataset name
v1.8.0,: The hex digest for the zip file
v1.8.0,Input validation.
v1.8.0,For downloading
v1.8.0,For splitting
v1.8.0,Whether to create inverse triples
v1.8.0,shared directory for multiple datasets.
v1.8.0,ensure file is present
v1.8.0,TODO: Re-use ensure_from_google?
v1.8.0,read all triples from file
v1.8.0,"some ""entities"" have numeric labels"
v1.8.0,"pandas.read_csv(..., dtype=str) does not work properly."
v1.8.0,create triples factory
v1.8.0,split
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.8.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.8.0,the columns are not ordered properly.
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Assume it's a file path
v1.8.0,hash kwargs
v1.8.0,normalize dataset name
v1.8.0,get canonic path
v1.8.0,try to use cached dataset
v1.8.0,load dataset without cache
v1.8.0,store cache
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The name of the dataset to download
v1.8.0,"note: we do not use the built-in constants here, since those refer to OGB nomenclature"
v1.8.0,(which happens to coincide with ours)
v1.8.0,FIXME these are already identifiers
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,relation typing
v1.8.0,constants
v1.8.0,unique
v1.8.0,compute over all triples
v1.8.0,Determine group key
v1.8.0,Add labels if requested
v1.8.0,TODO: Merge with _common?
v1.8.0,include hash over triples into cache-file name
v1.8.0,include part hash into cache-file name
v1.8.0,re-use cached file if possible
v1.8.0,select triples
v1.8.0,save to file
v1.8.0,Prune by support and confidence
v1.8.0,TODO: Consider merging with other analysis methods
v1.8.0,TODO: Consider merging with other analysis methods
v1.8.0,TODO: Consider merging with other analysis methods
v1.8.0,"num_triples_validation: Optional[int],"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Raise matplotlib level
v1.8.0,expected metrics
v1.8.0,Needs simulation
v1.8.0,See https://zenodo.org/record/6331629
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,don't call this function by itself. assumes called through the `validation`
v1.8.0,property and the _training factory has already been loaded
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Normalize path
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Base classes
v1.8.0,Utilities
v1.8.0,: A factory wrapping the training triples
v1.8.0,": A factory wrapping the testing triples, that share indices with the training triples"
v1.8.0,": A factory wrapping the validation triples, that share indices with the training triples"
v1.8.0,: All datasets should take care of inverse triple creation
v1.8.0,: the dataset's name
v1.8.0,TODO: Make a constant for the names
v1.8.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.8.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.8.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.8.0,: The directory in which the cached data is stored
v1.8.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.0,don't call this function by itself. assumes called through the `validation`
v1.8.0,property and the _training factory has already been loaded
v1.8.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.0,"relative paths within zip file's always follow Posix path, even on Windows"
v1.8.0,tarfile does not like pathlib
v1.8.0,: URL to the data to download
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,Utilities
v1.8.0,Base Classes
v1.8.0,Concrete Classes
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.8.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: A factory wrapping the training triples
v1.8.0,: A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT
v1.8.0,share indices with the transductive training
v1.8.0,": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.8.0,": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples"
v1.8.0,: All datasets should take care of inverse triple creation
v1.8.0,": The actual instance of the training factory, which is exposed to the user through `transductive_training`"
v1.8.0,": The actual instance of the inductive inference factory,"
v1.8.0,: which is exposed to the user through `inductive_inference`
v1.8.0,": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`"
v1.8.0,": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`"
v1.8.0,: The directory in which the cached data is stored
v1.8.0,add v1 / v2 / v3 / v4 for inductive splits if available
v1.8.0,generate subfolders 'training' and  'inference'
v1.8.0,important: inductive_inference shares the same RELATIONS with the transductive training graph
v1.8.0,inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph
v1.8.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.0,inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph
v1.8.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,1. Dataset
v1.8.0,2. Model
v1.8.0,3. Loss
v1.8.0,4. Regularizer
v1.8.0,5. Optimizer
v1.8.0,5.1 Learning Rate Scheduler
v1.8.0,6. Training Loop
v1.8.0,7. Training
v1.8.0,8. Evaluation
v1.8.0,9. Trackers
v1.8.0,Misc.
v1.8.0,log pruning
v1.8.0,"trial was successful, but has to be ended"
v1.8.0,also show info
v1.8.0,2. Model
v1.8.0,3. Loss
v1.8.0,4. Regularizer
v1.8.0,5. Optimizer
v1.8.0,5.1 Learning Rate Scheduler
v1.8.0,"TODO this fixes the issue for negative samplers, but does not generally address it."
v1.8.0,"For example, some of them obscure their arguments with **kwargs, so should we look"
v1.8.0,at the parent class? Sounds like something to put in class resolver by using the
v1.8.0,"inspect module. For now, this solution will rely on the fact that the sampler is a"
v1.8.0,direct descendent of a parent NegativeSampler
v1.8.0,create result tracker to allow to gracefully close failed trials
v1.8.0,1. Dataset
v1.8.0,2. Model
v1.8.0,3. Loss
v1.8.0,4. Regularizer
v1.8.0,5. Optimizer
v1.8.0,5.1 Learning Rate Scheduler
v1.8.0,6. Training Loop
v1.8.0,7. Training
v1.8.0,8. Evaluation
v1.8.0,9. Tracker
v1.8.0,Misc.
v1.8.0,close run in result tracker
v1.8.0,raise the error again (which will be catched in study.optimize)
v1.8.0,: The :mod:`optuna` study object
v1.8.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.8.0,Output study information
v1.8.0,Output all trials
v1.8.0,Output best trial as pipeline configuration file
v1.8.0,1. Dataset
v1.8.0,2. Model
v1.8.0,3. Loss
v1.8.0,4. Regularizer
v1.8.0,5. Optimizer
v1.8.0,5.1 Learning Rate Scheduler
v1.8.0,6. Training Loop
v1.8.0,7. Training
v1.8.0,8. Evaluation
v1.8.0,9. Tracking
v1.8.0,6. Misc
v1.8.0,Optuna Study Settings
v1.8.0,Optuna Optimization Settings
v1.8.0,TODO: use metric.increasing to determine default direction
v1.8.0,0. Metadata/Provenance
v1.8.0,1. Dataset
v1.8.0,2. Model
v1.8.0,3. Loss
v1.8.0,4. Regularizer
v1.8.0,5. Optimizer
v1.8.0,5.1 Learning Rate Scheduler
v1.8.0,6. Training Loop
v1.8.0,7. Training
v1.8.0,8. Evaluation
v1.8.0,9. Tracking
v1.8.0,1. Dataset
v1.8.0,2. Model
v1.8.0,3. Loss
v1.8.0,4. Regularizer
v1.8.0,5. Optimizer
v1.8.0,5.1 Learning Rate Scheduler
v1.8.0,6. Training Loop
v1.8.0,7. Training
v1.8.0,8. Evaluation
v1.8.0,9. Tracker
v1.8.0,Optuna Misc.
v1.8.0,Pipeline Misc.
v1.8.0,Invoke optimization of the objective function.
v1.8.0,TODO: make it even easier to specify categorical strategies just as lists
v1.8.0,"if isinstance(info, (tuple, list, set)):"
v1.8.0,"info = dict(type='categorical', choices=list(info))"
v1.8.0,get log from info - could either be a boolean or string
v1.8.0,"otherwise, dataset refers to a file that should be automatically split"
v1.8.0,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.8.0,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.8.0,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.8.0,if has_dataset(dataset_name):
v1.8.0,"study.set_user_attr('dataset', dataset_name)"
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,noqa: DAR101
v1.8.0,-*- coding: utf-8 -*-
v1.8.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,
v1.7.0,Configuration file for the Sphinx documentation builder.
v1.7.0,
v1.7.0,This file does only contain a selection of the most common options. For a
v1.7.0,full list see the documentation:
v1.7.0,http://www.sphinx-doc.org/en/master/config
v1.7.0,-- Path setup --------------------------------------------------------------
v1.7.0,"If extensions (or modules to document with autodoc) are in another directory,"
v1.7.0,add these directories to sys.path here. If the directory is relative to the
v1.7.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.7.0,
v1.7.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.7.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.7.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.7.0,from unittest.mock import Mock
v1.7.0,sys.modules['numpy'] = Mock()
v1.7.0,sys.modules['numpy.linalg'] = Mock()
v1.7.0,sys.modules['scipy'] = Mock()
v1.7.0,sys.modules['scipy.optimize'] = Mock()
v1.7.0,sys.modules['scipy.interpolate'] = Mock()
v1.7.0,sys.modules['scipy.sparse'] = Mock()
v1.7.0,sys.modules['scipy.ndimage'] = Mock()
v1.7.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.7.0,sys.modules['tensorflow'] = Mock()
v1.7.0,sys.modules['theano'] = Mock()
v1.7.0,sys.modules['theano.tensor'] = Mock()
v1.7.0,sys.modules['torch'] = Mock()
v1.7.0,sys.modules['torch.optim'] = Mock()
v1.7.0,sys.modules['torch.nn'] = Mock()
v1.7.0,sys.modules['torch.nn.init'] = Mock()
v1.7.0,sys.modules['torch.autograd'] = Mock()
v1.7.0,sys.modules['sklearn'] = Mock()
v1.7.0,sys.modules['sklearn.model_selection'] = Mock()
v1.7.0,sys.modules['sklearn.utils'] = Mock()
v1.7.0,-- Project information -----------------------------------------------------
v1.7.0,"The full version, including alpha/beta/rc tags."
v1.7.0,The short X.Y version.
v1.7.0,-- General configuration ---------------------------------------------------
v1.7.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.7.0,
v1.7.0,needs_sphinx = '1.0'
v1.7.0,"If true, the current module name will be prepended to all description"
v1.7.0,unit titles (such as .. function::).
v1.7.0,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.7.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.7.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.7.0,ones.
v1.7.0,show todo's
v1.7.0,generate autosummary pages
v1.7.0,"Add any paths that contain templates here, relative to this directory."
v1.7.0,The suffix(es) of source filenames.
v1.7.0,You can specify multiple suffix as a list of string:
v1.7.0,
v1.7.0,"source_suffix = ['.rst', '.md']"
v1.7.0,The master toctree document.
v1.7.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.7.0,for a list of supported languages.
v1.7.0,
v1.7.0,This is also used if you do content translation via gettext catalogs.
v1.7.0,"Usually you set ""language"" from the command line for these cases."
v1.7.0,"List of patterns, relative to source directory, that match files and"
v1.7.0,directories to ignore when looking for source files.
v1.7.0,This pattern also affects html_static_path and html_extra_path.
v1.7.0,The name of the Pygments (syntax highlighting) style to use.
v1.7.0,-- Options for HTML output -------------------------------------------------
v1.7.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.7.0,a list of builtin themes.
v1.7.0,
v1.7.0,Theme options are theme-specific and customize the look and feel of a theme
v1.7.0,"further.  For a list of options available for each theme, see the"
v1.7.0,documentation.
v1.7.0,
v1.7.0,html_theme_options = {}
v1.7.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.7.0,"relative to this directory. They are copied after the builtin static files,"
v1.7.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.7.0,html_static_path = ['_static']
v1.7.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.7.0,to template names.
v1.7.0,
v1.7.0,The default sidebars (for documents that don't match any pattern) are
v1.7.0,defined by theme itself.  Builtin themes are using these templates by
v1.7.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.7.0,'searchbox.html']``.
v1.7.0,
v1.7.0,html_sidebars = {}
v1.7.0,The name of an image file (relative to this directory) to place at the top
v1.7.0,of the sidebar.
v1.7.0,
v1.7.0,-- Options for HTMLHelp output ---------------------------------------------
v1.7.0,Output file base name for HTML help builder.
v1.7.0,-- Options for LaTeX output ------------------------------------------------
v1.7.0,latex_elements = {
v1.7.0,The paper size ('letterpaper' or 'a4paper').
v1.7.0,
v1.7.0,"'papersize': 'letterpaper',"
v1.7.0,
v1.7.0,"The font size ('10pt', '11pt' or '12pt')."
v1.7.0,
v1.7.0,"'pointsize': '10pt',"
v1.7.0,
v1.7.0,Additional stuff for the LaTeX preamble.
v1.7.0,
v1.7.0,"'preamble': '',"
v1.7.0,
v1.7.0,Latex figure (float) alignment
v1.7.0,
v1.7.0,"'figure_align': 'htbp',"
v1.7.0,}
v1.7.0,Grouping the document tree into LaTeX files. List of tuples
v1.7.0,"(source start file, target name, title,"
v1.7.0,"author, documentclass [howto, manual, or own class])."
v1.7.0,latex_documents = [
v1.7.0,(
v1.7.0,"master_doc,"
v1.7.0,"'pykeen.tex',"
v1.7.0,"'PyKEEN Documentation',"
v1.7.0,"author,"
v1.7.0,"'manual',"
v1.7.0,"),"
v1.7.0,]
v1.7.0,-- Options for manual page output ------------------------------------------
v1.7.0,One entry per manual page. List of tuples
v1.7.0,"(source start file, name, description, authors, manual section)."
v1.7.0,-- Options for Texinfo output ----------------------------------------------
v1.7.0,Grouping the document tree into Texinfo files. List of tuples
v1.7.0,"(source start file, target name, title, author,"
v1.7.0,"dir menu entry, description, category)"
v1.7.0,-- Options for Epub output -------------------------------------------------
v1.7.0,Bibliographic Dublin Core info.
v1.7.0,epub_title = project
v1.7.0,The unique identifier of the text. This can be a ISBN number
v1.7.0,or the project homepage.
v1.7.0,
v1.7.0,epub_identifier = ''
v1.7.0,A unique identification for the text.
v1.7.0,
v1.7.0,epub_uid = ''
v1.7.0,A list of files that should not be packed into the epub file.
v1.7.0,epub_exclude_files = ['search.html']
v1.7.0,-- Extension configuration -------------------------------------------------
v1.7.0,-- Options for intersphinx extension ---------------------------------------
v1.7.0,Example configuration for intersphinx: refer to the Python standard library.
v1.7.0,autodoc_member_order = 'bysource'
v1.7.0,autodoc_typehints = 'both' # TODO turn on after 4.1 release
v1.7.0,autodoc_preserve_defaults = True
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The batch size
v1.7.0,: The tested class
v1.7.0,check probability distribution
v1.7.0,check probability distribution
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Check a model param is optimized
v1.7.0,Check a loss param is optimized
v1.7.0,Check a model param is NOT optimized
v1.7.0,Check a loss param is optimized
v1.7.0,Check a model param is optimized
v1.7.0,Check a loss param is NOT optimized
v1.7.0,Check a model param is NOT optimized
v1.7.0,Check a loss param is NOT optimized
v1.7.0,verify failure
v1.7.0,"Since custom data was passed, we can't store any of this"
v1.7.0,"currently, any custom data doesn't get stored."
v1.7.0,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.7.0,"Since there's no source path information, these shouldn't be"
v1.7.0,"added, even if it might be possible to infer path information"
v1.7.0,from the triples factories
v1.7.0,"Since paths were passed for training, testing, and validation,"
v1.7.0,they should be stored as study-level attributes
v1.7.0,Check a model param is optimized
v1.7.0,Check a loss param is optimized
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Check for correct class
v1.7.0,Check value ranges
v1.7.0,check mean rank (MR)
v1.7.0,check mean reciprocal rank (MRR)
v1.7.0,check hits at k (H@k)
v1.7.0,check adjusted mean rank (AMR)
v1.7.0,check adjusted mean rank index (AMRI)
v1.7.0,the test only considered a single batch
v1.7.0,all rank types have the same count
v1.7.0,TODO: Validate with data?
v1.7.0,Check for correct class
v1.7.0,check value
v1.7.0,filtering
v1.7.0,"true_score: (2, 3, 3)"
v1.7.0,head based filter
v1.7.0,preprocessing for faster lookup
v1.7.0,check that all found positives are positive
v1.7.0,check in-place
v1.7.0,Test head scores
v1.7.0,Assert in-place modification
v1.7.0,Assert correct filtering
v1.7.0,Test tail scores
v1.7.0,Assert in-place modification
v1.7.0,Assert correct filtering
v1.7.0,The MockModel gives the highest score to the highest entity id
v1.7.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.7.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.7.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.7.0,test triple at hand
v1.7.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.7.0,test triple at hand
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,check if within 0.5 std of observed
v1.7.0,test error is raised
v1.7.0,Tests that exception will be thrown when more than or less than three tensors are passed
v1.7.0,Test that regularization term is computed correctly
v1.7.0,Entity soft constraint
v1.7.0,Orthogonality soft constraint
v1.7.0,ensure regularizer is on correct device
v1.7.0,"After first update, should change the term"
v1.7.0,"After second update, no change should happen"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,create broadcastable shapes
v1.7.0,check correct value range
v1.7.0,check maximum norm constraint
v1.7.0,unchanged values for small norms
v1.7.0,random entity embeddings & projections
v1.7.0,random relation embeddings & projections
v1.7.0,project
v1.7.0,check shape:
v1.7.0,check normalization
v1.7.0,check equivalence of re-formulation
v1.7.0,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.7.0,= r_p (e_p^T e) + e'
v1.7.0,"create random array, estimate the costs of addition, and measure some execution times."
v1.7.0,"then, compute correlation between the estimated cost, and the measured time."
v1.7.0,check for strong correlation between estimated costs and measured execution time
v1.7.0,get optimal sequence
v1.7.0,check caching
v1.7.0,get optimal sequence
v1.7.0,check correct cost
v1.7.0,check optimality
v1.7.0,compare result to sequential addition
v1.7.0,compare result to sequential addition
v1.7.0,check result shape
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,equal value; larger is better
v1.7.0,equal value; smaller is better
v1.7.0,larger is better; improvement
v1.7.0,larger is better; improvement; but not significant
v1.7.0,: The window size used by the early stopper
v1.7.0,: The mock losses the mock evaluator will return
v1.7.0,: The (zeroed) index  - 1 at which stopping will occur
v1.7.0,: The minimum improvement
v1.7.0,: The best results
v1.7.0,Set automatic_memory_optimization to false for tests
v1.7.0,Step early stopper
v1.7.0,check storing of results
v1.7.0,check ring buffer
v1.7.0,: The window size used by the early stopper
v1.7.0,: The (zeroed) index  - 1 at which stopping will occur
v1.7.0,: The minimum improvement
v1.7.0,: The random seed to use for reproducibility
v1.7.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.7.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.7.0,: The batch size to use.
v1.7.0,Fix seed for reproducibility
v1.7.0,Set automatic_memory_optimization to false during testing
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.7.0,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.7.0,"first triple, and passing create_inverse_triples=True."
v1.7.0,check for warning
v1.7.0,check for filtered triples
v1.7.0,check for correct inverse triples flag
v1.7.0,check correct translation
v1.7.0,check column order
v1.7.0,apply restriction
v1.7.0,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.7.0,check that inverse_triples is correctly carried over
v1.7.0,verify that the label-to-ID mapping has not been changed
v1.7.0,verify that triples have been filtered
v1.7.0,Test different combinations of restrictions
v1.7.0,check compressed triples
v1.7.0,reconstruct triples from compressed form
v1.7.0,check data loader
v1.7.0,set create inverse triple to true
v1.7.0,split factory
v1.7.0,check that in *training* inverse triple are to be created
v1.7.0,check that in all other splits no inverse triples are to be created
v1.7.0,verify that all entities and relations are present in the training factory
v1.7.0,verify that no triple got lost
v1.7.0,verify that the label-to-id mappings match
v1.7.0,Check if multilabels are working correctly
v1.7.0,Slightly larger number of triples to guarantee split can find coverage of all entities and relations.
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"DummyModel,"
v1.7.0,3x batch norm: bias + scale --> 6
v1.7.0,entity specific bias        --> 1
v1.7.0,==================================
v1.7.0,7
v1.7.0,"two bias terms, one conv-filter"
v1.7.0,check type
v1.7.0,check shape
v1.7.0,check ID ranges
v1.7.0,this is only done in one of the models
v1.7.0,this is only done in one of the models
v1.7.0,Two linear layer biases
v1.7.0,"Two BN layers, bias & scale"
v1.7.0,Test that the weight in the MLP is trainable (i.e. requires grad)
v1.7.0,quaternion have four components
v1.7.0,: one bias per layer
v1.7.0,: one bias per layer
v1.7.0,entity embeddings
v1.7.0,relation embeddings
v1.7.0,Compute Scores
v1.7.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.7.0,relation embeddings
v1.7.0,Compute Scores
v1.7.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.7.0,entity embeddings
v1.7.0,relation embeddings
v1.7.0,Compute Scores
v1.7.0,random entity embeddings & projections
v1.7.0,random relation embeddings & projections
v1.7.0,project
v1.7.0,check shape:
v1.7.0,check normalization
v1.7.0,entity embeddings
v1.7.0,relation embeddings
v1.7.0,Compute Scores
v1.7.0,second_score = scores[1].item()
v1.7.0,: 2xBN (bias & scale)
v1.7.0,the combination bias
v1.7.0,check shape
v1.7.0,check content
v1.7.0,create triples factory with inverse relations
v1.7.0,head prediction via inverse tail prediction
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,empty lists are falsy
v1.7.0,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.7.0,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.7.0,pipeline is checked.
v1.7.0,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.7.0,Resume the previous pipeline
v1.7.0,The MockModel gives the highest score to the highest entity id
v1.7.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.7.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.7.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.7.0,test triple at hand
v1.7.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.7.0,test triple at hand
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.7.0,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.7.0,Test looking up inverse triples
v1.7.0,test new label to ID
v1.7.0,type
v1.7.0,old labels
v1.7.0,"new, compact IDs"
v1.7.0,test vectorized lookup
v1.7.0,type
v1.7.0,shape
v1.7.0,value range
v1.7.0,only occurring Ids get mapped to non-negative numbers
v1.7.0,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.7.0,check type
v1.7.0,check shape
v1.7.0,check content
v1.7.0,check type
v1.7.0,check shape
v1.7.0,check 1-hot
v1.7.0,check type
v1.7.0,check shape
v1.7.0,check value range
v1.7.0,check self-similarity = 1
v1.7.0,base relation
v1.7.0,exact duplicate
v1.7.0,99% duplicate
v1.7.0,50% duplicate
v1.7.0,exact inverse
v1.7.0,99% inverse
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The expected number of entities
v1.7.0,: The expected number of relations
v1.7.0,: The expected number of triples
v1.7.0,": The tolerance on expected number of triples, for randomized situations"
v1.7.0,: The dataset to test
v1.7.0,: The instantiated dataset
v1.7.0,: Should the validation be assumed to have been loaded with train/test?
v1.7.0,Not loaded
v1.7.0,Load
v1.7.0,Test caching
v1.7.0,assert (end - start) < 1.0e-02
v1.7.0,Test consistency of training / validation / testing mapping
v1.7.0,": The directory, if there is caching"
v1.7.0,: The batch size
v1.7.0,: The number of negatives per positive for sLCWA training loop.
v1.7.0,: The number of entities LCWA training loop / label smoothing.
v1.7.0,test reduction
v1.7.0,test finite loss value
v1.7.0,Test backward
v1.7.0,negative scores decreased compared to positive ones
v1.7.0,negative scores decreased compared to positive ones
v1.7.0,: The number of entities.
v1.7.0,: The number of negative samples
v1.7.0,: The number of entities.
v1.7.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.7.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.7.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.7.0,check whether the error originates from batch norm for single element batches
v1.7.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.7.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.7.0,test multiple different initializations
v1.7.0,calculate by functional
v1.7.0,calculate manually
v1.7.0,simple
v1.7.0,nested
v1.7.0,nested
v1.7.0,prepare a temporary test directory
v1.7.0,check that file was created
v1.7.0,make sure to close file before trying to delete it
v1.7.0,delete intermediate files
v1.7.0,: The batch size
v1.7.0,: The triples factory
v1.7.0,: Class of regularizer to test
v1.7.0,: The constructor parameters to pass to the regularizer
v1.7.0,": The regularizer instance, initialized in setUp"
v1.7.0,: A positive batch
v1.7.0,: The device
v1.7.0,move test instance to device
v1.7.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.7.0,Check if regularizer is stored correctly.
v1.7.0,Forward pass (should update regularizer)
v1.7.0,Call post_parameter_update (should reset regularizer)
v1.7.0,Check if regularization term is reset
v1.7.0,Call method
v1.7.0,Generate random tensors
v1.7.0,Call update
v1.7.0,check shape
v1.7.0,compute expected term
v1.7.0,Generate random tensor
v1.7.0,calculate penalty
v1.7.0,check shape
v1.7.0,check value
v1.7.0,FIXME isn't any finite number allowed now?
v1.7.0,: Additional arguments passed to the training loop's constructor method
v1.7.0,: The triples factory instance
v1.7.0,: The batch size for use for forward_* tests
v1.7.0,: The embedding dimensionality
v1.7.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.7.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.7.0,: The batch size for use when testing training procedures
v1.7.0,: The number of epochs to train the model
v1.7.0,: A random number generator from torch
v1.7.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.7.0,initialization
v1.7.0,: Static extras to append to the CLI
v1.7.0,for reproducible testing
v1.7.0,insert shared parameters
v1.7.0,move model to correct device
v1.7.0,Check that all the parameters actually require a gradient
v1.7.0,Try to initialize an optimizer
v1.7.0,get model parameters
v1.7.0,re-initialize
v1.7.0,check that the operation works in-place
v1.7.0,check that the parameters where modified
v1.7.0,check for finite values by default
v1.7.0,check whether a gradient can be back-propgated
v1.7.0,"assert batch comprises (head, relation) pairs"
v1.7.0,"assert batch comprises (head, tail) pairs"
v1.7.0,TODO: look into score_r for inverse relations
v1.7.0,"assert batch comprises (relation, tail) pairs"
v1.7.0,"For the high/low memory test cases of NTN, SE, etc."
v1.7.0,"else, leave to default"
v1.7.0,Make sure that inverse triples are created if create_inverse_triples=True
v1.7.0,triples factory is added by the pipeline
v1.7.0,TODO: Catch HolE MKL error?
v1.7.0,set regularizer term to something that isn't zero
v1.7.0,call post_parameter_update
v1.7.0,assert that the regularization term has been reset
v1.7.0,do one optimization step
v1.7.0,call post_parameter_update
v1.7.0,check model constraints
v1.7.0,"assert batch comprises (relation, tail) pairs"
v1.7.0,"assert batch comprises (relation, tail) pairs"
v1.7.0,"assert batch comprises (relation, tail) pairs"
v1.7.0,call some functions
v1.7.0,reset to old state
v1.7.0,Distance-based model
v1.7.0,check type
v1.7.0,check shape
v1.7.0,: The number of entities
v1.7.0,: The number of triples
v1.7.0,check shape
v1.7.0,check dtype
v1.7.0,check finite values (e.g. due to division by zero)
v1.7.0,check non-negativity
v1.7.0,: The input dimension
v1.7.0,: the shape of the tensor to initialize
v1.7.0,: to be initialized / set in subclass
v1.7.0,initializers *may* work in-place => clone
v1.7.0,unfavourable split to ensure that cleanup is necessary
v1.7.0,check for unclean split
v1.7.0,check that no triple got lost
v1.7.0,check that triples where only moved from other to reference
v1.7.0,check that all entities occur in reference
v1.7.0,check that no triple got lost
v1.7.0,check that all entities are covered in first part
v1.7.0,The triples factory and model
v1.7.0,: The evaluator to be tested
v1.7.0,Settings
v1.7.0,: The evaluator instantiation
v1.7.0,Settings
v1.7.0,Initialize evaluator
v1.7.0,Use small test dataset
v1.7.0,Use small model (untrained)
v1.7.0,Get batch
v1.7.0,Compute scores
v1.7.0,Compute mask only if required
v1.7.0,TODO: Re-use filtering code
v1.7.0,"shape: (batch_size, num_triples)"
v1.7.0,"shape: (batch_size, num_entities)"
v1.7.0,Process one batch
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,check for finite values by default
v1.7.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.7.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.7.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.7.0,Get embeddings
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,≈ result of softmax
v1.7.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.7.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.7.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.7.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.7.0,≈ result of sigmoid
v1.7.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.7.0,expected_loss ≈ 0.34
v1.7.0,Create dummy dense labels
v1.7.0,Check if labels form a probability distribution
v1.7.0,Apply label smoothing
v1.7.0,Check if smooth labels form probability distribution
v1.7.0,Create dummy sLCWA labels
v1.7.0,Apply label smoothing
v1.7.0,generate random ratios
v1.7.0,check size
v1.7.0,check value range
v1.7.0,check total split
v1.7.0,check consistency with ratios
v1.7.0,the number of decimal digits equivalent to 1 / n_total
v1.7.0,check type
v1.7.0,check values
v1.7.0,compare against expected
v1.7.0,generated_triples = generate_triples()
v1.7.0,check type
v1.7.0,check format
v1.7.0,check coverage
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"naive implementation, O(n2)"
v1.7.0,check correct output type
v1.7.0,check value range subset
v1.7.0,check value range side
v1.7.0,check columns
v1.7.0,check value range and type
v1.7.0,check value range entity IDs
v1.7.0,check value range entity labels
v1.7.0,check correct type
v1.7.0,check relation_id value range
v1.7.0,check pattern value range
v1.7.0,check confidence value range
v1.7.0,check support value range
v1.7.0,check correct type
v1.7.0,check relation_id value range
v1.7.0,check pattern value range
v1.7.0,check correct type
v1.7.0,check relation_id value range
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Check minimal statistics
v1.7.0,Check statistics for pre-stratified datasets
v1.7.0,Check either a github link or author/publication information is given
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.7.0,"prepare conv input (N, C, H, W)"
v1.7.0,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.7.0,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.7.0,remove batch/num dimension
v1.7.0,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.7.0,"f(h, r, t) = h @ r @ t"
v1.7.0,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.7.0,normalize length of r
v1.7.0,check for unit length
v1.7.0,entity embeddings
v1.7.0,relation embeddings
v1.7.0,Compute Scores
v1.7.0,entity embeddings
v1.7.0,relation embeddings
v1.7.0,Compute Scores
v1.7.0,Compute Scores
v1.7.0,-\|R_h h - R_t t\|
v1.7.0,-\|h - t\|
v1.7.0,"Since MuRE has offsets, the scores do not need to negative"
v1.7.0,"We do not need this, since we do not check for functional consistency anyway"
v1.7.0,intra-interaction comparison
v1.7.0,dimension needs to be divisible by num_heads
v1.7.0,FIXME
v1.7.0,head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,check value range
v1.7.0,check sin**2 + cos**2 == 1
v1.7.0,"check value range (actually [-s, +s] with s = 1/sqrt(2*n))"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,create a new instance with guaranteed dropout
v1.7.0,set to training mode
v1.7.0,check for different output
v1.7.0,TODO consider making subclass of cases.RepresentationTestCase
v1.7.0,"that has num_entities, num_relations, num_triples, and"
v1.7.0,create_inverse_triples as well as a generate_triples_factory()
v1.7.0,wrapper
v1.7.0,: The number of embeddings
v1.7.0,check shape
v1.7.0,check attributes
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"typically, the model takes care of adjusting the dimension size for ""complex"""
v1.7.0,"tensors, but we have to do it manually here for testing purposes"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,ensure positivity
v1.7.0,compute using pytorch
v1.7.0,prepare distributions
v1.7.0,compute using pykeen
v1.7.0,"e: (batch_size, num_heads, num_tails, d)"
v1.7.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.7.0,divergence = 0 => similarity = -divergence = 0
v1.7.0,"(h - t), r"
v1.7.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.7.0,divergence >= 0 => similarity = -divergence <= 0
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.7.0,filter vs. no filter thing.
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,check for empty batches
v1.7.0,: The window size used by the early stopper
v1.7.0,: The mock losses the mock evaluator will return
v1.7.0,: The (zeroed) index  - 1 at which stopping will occur
v1.7.0,: The minimum improvement
v1.7.0,: The best results
v1.7.0,Set automatic_memory_optimization to false for tests
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Train a model in one shot
v1.7.0,Train a model for the first half
v1.7.0,Continue training of the first part
v1.7.0,: Should negative samples be filtered?
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Check whether filtering works correctly
v1.7.0,First giving an example where all triples have to be filtered
v1.7.0,The filter should remove all triples
v1.7.0,Create an example where no triples will be filtered
v1.7.0,The filter should not remove any triple
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,check shape
v1.7.0,get triples
v1.7.0,check connected components
v1.7.0,super inefficient
v1.7.0,join
v1.7.0,already joined
v1.7.0,check that there is only a single component
v1.7.0,check content of comp_adj_lists
v1.7.0,check edge ids
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Test that half of the subjects and half of the objects are corrupted
v1.7.0,same relation
v1.7.0,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.7.0,check that corrupted entities co-occur with the relation in training data
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The batch size
v1.7.0,: The random seed
v1.7.0,: The triples factory
v1.7.0,: The instances
v1.7.0,: A positive batch
v1.7.0,: Kwargs
v1.7.0,Generate negative sample
v1.7.0,check filter shape if necessary
v1.7.0,check shape
v1.7.0,check bounds: heads
v1.7.0,check bounds: relations
v1.7.0,check bounds: tails
v1.7.0,test that the negative triple is not the original positive triple
v1.7.0,"shape: (batch_size, 1, num_neg)"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Base Classes
v1.7.0,Concrete Classes
v1.7.0,Utils
v1.7.0,: The default strategy for optimizing the loss's hyper-parameters
v1.7.0,flatten and stack
v1.7.0,apply label smoothing if necessary.
v1.7.0,TODO: Do label smoothing only once
v1.7.0,Sanity check
v1.7.0,"prepare for broadcasting, shape: (batch_size, 1, 3)"
v1.7.0,negative_scores have already been filtered in the sampler!
v1.7.0,"shape: (nnz,)"
v1.7.0,Sanity check
v1.7.0,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.7.0,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.7.0,positive entries in the labels tensor.
v1.7.0,"This shows how often one row has to be repeated,"
v1.7.0,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.7.0,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.7.0,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.7.0,This tells us how often each true label should be repeated
v1.7.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.7.0,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.7.0,within the ablation pipeline.
v1.7.0,1. positive & negative margin
v1.7.0,2. negative margin & offset
v1.7.0,3. positive margin & offset
v1.7.0,Sanity check
v1.7.0,positive term
v1.7.0,implicitly repeat positive scores
v1.7.0,"shape: (nnz,)"
v1.7.0,negative term
v1.7.0,negative_scores have already been filtered in the sampler!
v1.7.0,Sanity check
v1.7.0,"scale labels from [0, 1] to [-1, 1]"
v1.7.0,"Ensures that for this class incompatible hyper-parameter ""margin"" of superclass is not used"
v1.7.0,within the ablation pipeline.
v1.7.0,negative_scores have already been filtered in the sampler!
v1.7.0,(dense) softmax requires unfiltered scores / masking
v1.7.0,we need to fill the scores with -inf for all filtered negative examples
v1.7.0,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.7.0,use filled negatives scores
v1.7.0,we need dense negative scores => unfilter if necessary
v1.7.0,"we may have inf rows, since there will be one additional finite positive score per row"
v1.7.0,"combine scores: shape: (batch_size, num_negatives + 1)"
v1.7.0,use sparse version of cross entropy
v1.7.0,Sanity check
v1.7.0,compute negative weights (without gradient tracking)
v1.7.0,clone is necessary since we modify in-place
v1.7.0,Split positive and negative scores
v1.7.0,Sanity check
v1.7.0,"we do not allow full -inf rows, since we compute the softmax over this tensor"
v1.7.0,compute weights (without gradient tracking)
v1.7.0,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.7.0,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.7.0,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.7.0,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.7.0,": For more information, see https://github.com/cthoyt/pystow"
v1.7.0,: A path representing the PyKEEN data folder
v1.7.0,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.7.0,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.7.0,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.7.0,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.7.0,: A subdirectory for PyKEEN logs
v1.7.0,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.7.0,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.7.0,get datatype specific epsilon
v1.7.0,clamp minimum value
v1.7.0,lower bound
v1.7.0,upper bound
v1.7.0,input validation
v1.7.0,base case
v1.7.0,normalize dim
v1.7.0,calculate repeats for each tensor
v1.7.0,dimensions along concatenation axis do not need to match
v1.7.0,get desired extent along dimension
v1.7.0,repeat tensors along axes if necessary
v1.7.0,concatenate
v1.7.0,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.7.0,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.7.0,Determine optimal order and cost
v1.7.0,translate back to original order
v1.7.0,determine optimal processing order
v1.7.0,heuristic
v1.7.0,workaround for complex numbers: manually compute norm
v1.7.0,TODO: check if einsum is still very slow.
v1.7.0,TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation
v1.7.0,unsqueeze
v1.7.0,The dimensions affected by e'
v1.7.0,Project entities
v1.7.0,r_p (e_p.T e) + e'
v1.7.0,Enforce constraints
v1.7.0,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.7.0,Create a tensor of all IDs
v1.7.0,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.7.0,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.7.0,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.7.0,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.7.0,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.7.0,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.7.0,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.7.0,ensure pathlib
v1.7.0,Enforce that sizes are strictly positive by passing through ELU
v1.7.0,Shape vector is normalized using the above helper function
v1.7.0,Size is learned separately and applied to normalized shape
v1.7.0,Compute potential boundaries by applying the shape in substraction
v1.7.0,and in addition
v1.7.0,Compute box upper bounds using min and max respectively
v1.7.0,compute width plus 1
v1.7.0,compute box midpoints
v1.7.0,"TODO: we already had this before, as `base`"
v1.7.0,inside box?
v1.7.0,yes: |p - c| / (w + 1)
v1.7.0,no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))
v1.7.0,Step 1: Apply the other entity bump
v1.7.0,Step 2: Apply tanh if tanh_map is set to True.
v1.7.0,Compute the distance function output element-wise
v1.7.0,"Finally, compute the norm"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Base Class
v1.7.0,Child classes
v1.7.0,Utils
v1.7.0,: The overall regularization weight
v1.7.0,: The current regularization term (a scalar)
v1.7.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.7.0,: Has this regularizer been updated since last being reset?
v1.7.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.7.0,"If there are tracked parameters, update based on them"
v1.7.0,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.7.0,no need to compute anything
v1.7.0,always return zero
v1.7.0,: The dimension along which to compute the vector-based regularization terms.
v1.7.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.7.0,: This allows dimensionality-independent weight tuning.
v1.7.0,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.7.0,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.7.0,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.7.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.7.0,"Therefore, apply_only_once is always set to True."
v1.7.0,Entity soft constraint
v1.7.0,Orthogonality soft constraint
v1.7.0,The normalization factor to balance individual regularizers' contribution.
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Add HPO command
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,General types
v1.7.0,Triples
v1.7.0,Others
v1.7.0,Tensor Functions
v1.7.0,Tensors
v1.7.0,Dataclasses
v1.7.0,: A function that mutates the input and returns a new object of the same type as output
v1.7.0,: A function that can be applied to a tensor to initialize it
v1.7.0,: A function that can be applied to a tensor to normalize it
v1.7.0,: A function that can be applied to a tensor to constrain it
v1.7.0,: A hint for a :class:`torch.device`
v1.7.0,: A hint for a :class:`torch.Generator`
v1.7.0,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.7.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.7.0,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.7.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.7.0,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.7.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,pad with zeros
v1.7.0,trim
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"mask, shape: (num_edges,)"
v1.7.0,bi-directional message passing
v1.7.0,Heuristic for default value
v1.7.0,other relations
v1.7.0,other relations
v1.7.0,Select source and target indices as well as edge weights for the
v1.7.0,currently considered relation
v1.7.0,skip relations without edges
v1.7.0,"compute message, shape: (num_edges_of_type, output_dim)"
v1.7.0,since we may have one node ID appearing multiple times as source
v1.7.0,"ID, we can save some computation by first reducing to the unique"
v1.7.0,"source IDs, compute transformed representations and afterwards"
v1.7.0,select these representations for the correct edges.
v1.7.0,select unique source node representations
v1.7.0,transform representations by relation specific weight
v1.7.0,select the uniquely transformed representations for each edge
v1.7.0,optional message weighting
v1.7.0,message aggregation
v1.7.0,Xavier Glorot initialization of each block
v1.7.0,accumulator
v1.7.0,view as blocks
v1.7.0,other relations
v1.7.0,skip relations without edges
v1.7.0,"compute message, shape: (num_edges_of_type, num_blocks, block_size)"
v1.7.0,optional message weighting
v1.7.0,message aggregation
v1.7.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501
v1.7.0,there are separate decompositions for forward and backward relations.
v1.7.0,the self-loop weight is not decomposed.
v1.7.0,self-loop
v1.7.0,forward messages
v1.7.0,backward messages
v1.7.0,activation
v1.7.0,Resolve edge weighting
v1.7.0,dropout
v1.7.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.7.0,no activation on last layer
v1.7.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501
v1.7.0,buffering of enriched representations
v1.7.0,invalidate enriched embeddings
v1.7.0,Bind fields
v1.7.0,"shape: (num_entities, embedding_dim)"
v1.7.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.7.0,Get random dropout mask
v1.7.0,Apply to edges
v1.7.0,fixed edges -> pre-compute weights
v1.7.0,Cache enriched representations
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Utils
v1.7.0,: the maximum ID (exclusively)
v1.7.0,: the shape of an individual representation
v1.7.0,TODO: Remove this property and update code to use shape instead
v1.7.0,normalize embedding_dim vs. shape
v1.7.0,work-around until full complex support (torch==1.10 still does not work)
v1.7.0,TODO: verify that this is our understanding of complex!
v1.7.0,"wrapper around max_id, for backward compatibility"
v1.7.0,initialize weights in-place
v1.7.0,apply constraints in-place
v1.7.0,verify that contiguity is preserved
v1.7.0,TODO: move normalizer / regularizer to base class?
v1.7.0,"get all base representations, shape: (num_bases, *shape)"
v1.7.0,"get base weights, shape: (*batch_dims, num_bases)"
v1.7.0,"weighted linear combination of bases, shape: (*batch_dims, *shape)"
v1.7.0,: Initializers
v1.7.0,: Constrainers
v1.7.0,TODO add normalization functions
v1.7.0,normalize output dimension
v1.7.0,entity-relation composition
v1.7.0,edge weighting
v1.7.0,message passing weights
v1.7.0,linear relation transformation
v1.7.0,layer-specific self-loop relation representation
v1.7.0,other components
v1.7.0,initialize
v1.7.0,split
v1.7.0,compose
v1.7.0,transform
v1.7.0,normalization
v1.7.0,aggregate by sum
v1.7.0,dropout
v1.7.0,prepare for inverse relations
v1.7.0,update entity representations: mean over self-loops / forward edges / backward edges
v1.7.0,Relation transformation
v1.7.0,Buffered enriched entity and relation representations
v1.7.0,TODO: Check
v1.7.0,hidden dimension normalization
v1.7.0,Create message passing layers
v1.7.0,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.7.0,TODO: This always uses all training triples for message passing
v1.7.0,initialize buffer of enriched representations
v1.7.0,invalidate enriched embeddings
v1.7.0,"when changing from evaluation to training mode, the buffered representations have been computed without"
v1.7.0,"gradient tracking. hence, we need to invalidate them."
v1.7.0,note: this occurs in practice when continuing training after evaluation.
v1.7.0,enrich
v1.7.0,inverse triples are created afterwards implicitly
v1.7.0,tokenize: represent entities by bag of relations
v1.7.0,collect candidates
v1.7.0,randomly sample without replacement num_tokens relations for each entity
v1.7.0,: the token representations
v1.7.0,: the entity-to-token mapping
v1.7.0,create token representations
v1.7.0,normal relations + inverse relations + padding
v1.7.0,super init; has to happen *before* any parameter or buffer is assigned
v1.7.0,Assign default aggregation
v1.7.0,assign module
v1.7.0,"get token IDs, shape: (*, k)"
v1.7.0,"lookup token representations, shape: (*, k, d)"
v1.7.0,aggregate
v1.7.0,infer shape
v1.7.0,"assign after super, since they should be properly registered as submodules"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,scaling factor
v1.7.0,"modulus ~ Uniform[-s, s]"
v1.7.0,"phase ~ Uniform[0, 2*pi]"
v1.7.0,real part
v1.7.0,purely imaginary quaternions unitary
v1.7.0,this is usually loaded from somewhere else
v1.7.0,"the shape must match, as well as the entity-to-id mapping"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"Calculate in-degree, i.e. number of incoming edges"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,TODO test
v1.7.0,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.7.0,: a = \mu^T\Sigma^{-1}\mu
v1.7.0,: b = \log \det \Sigma
v1.7.0,1. Component
v1.7.0,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.7.0,2. Component
v1.7.0,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.7.0,with mu = (mu_1 - mu_0)
v1.7.0,= mu * Sigma_1^-1 mu
v1.7.0,since Sigma_1 is diagonal
v1.7.0,= mu**2 / sigma_1
v1.7.0,3. Component
v1.7.0,4. Component
v1.7.0,ln (det(\Sigma_1) / det(\Sigma_0))
v1.7.0,= ln det Sigma_1 - ln det Sigma_0
v1.7.0,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.7.0,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.7.0,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.7.0,allocate result
v1.7.0,prepare distributions
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,TODO benchmark
v1.7.0,TODO benchmark
v1.7.0,TODO benchmark
v1.7.0,TODO benchmark
v1.7.0,TODO benchmark
v1.7.0,TODO benchmark
v1.7.0,TODO benchmark
v1.7.0,TODO benchmark
v1.7.0,TODO benchmark
v1.7.0,"h = h_re, -h_im"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Adapter classes
v1.7.0,Concrete Classes
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Base Classes
v1.7.0,Adapter classes
v1.7.0,Concrete Classes
v1.7.0,input normalization -> Tuple[Tensor]
v1.7.0,"shape: (num_representations,)"
v1.7.0,split each representation along the given dimension
v1.7.0,"shape: (num_representations,)"
v1.7.0,"create batches, shape: (num_slices,), each being a tuple of shape (num_representations,)"
v1.7.0,unpack_singletons
v1.7.0,: The symbolic shapes for entity representations
v1.7.0,": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE."
v1.7.0,: The symbolic shapes for relation representations
v1.7.0,"bring to (b, n, *)"
v1.7.0,"bring to (b, h, r, t, *)"
v1.7.0,unpack singleton
v1.7.0,"The appended ""e"" represents the literals that get concatenated"
v1.7.0,on the entity representations. It does not necessarily have the
v1.7.0,"same dimension ""d"" as the entity representations."
v1.7.0,alternate way of combining entity embeddings + literals
v1.7.0,"h = torch.cat(h, dim=-1)"
v1.7.0,"h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore"
v1.7.0,"t = torch.cat(t, dim=-1)"
v1.7.0,"t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore"
v1.7.0,: The functional interaction form
v1.7.0,Store initial input for error message
v1.7.0,All are None -> try and make closest to square
v1.7.0,Only input channels is None
v1.7.0,Only width is None
v1.7.0,Only height is none
v1.7.0,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.7.0,Width and input channels are None -> set input channels to 1 and calculate width
v1.7.0,": The head-relation encoder operating on 2D ""images"""
v1.7.0,: The head-relation encoder operating on the 1D flattened version
v1.7.0,: The interaction function
v1.7.0,Automatic calculation of remaining dimensions
v1.7.0,Parameter need to fulfil:
v1.7.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.7.0,encoders
v1.7.0,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.7.0,"2: 1D encoder: FC, DO, BN?, Act"
v1.7.0,store reshaping dimensions
v1.7.0,The interaction model
v1.7.0,Use Xavier initialization for weight; bias to zero
v1.7.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.7.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.7.0,Initialize biases with zero
v1.7.0,"In the original formulation,"
v1.7.0,Global entity projection
v1.7.0,Global relation projection
v1.7.0,Global combination bias
v1.7.0,Global combination bias
v1.7.0,Core tensor
v1.7.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.7.0,Dropout
v1.7.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.7.0,"batch norm gets reset automatically, since it defines reset_parameters"
v1.7.0,shapes
v1.7.0,there are separate biases for entities in head and tail position
v1.7.0,the base interaction
v1.7.0,forward entity/relation shapes
v1.7.0,The parameters of the affine transformation: bias
v1.7.0,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.7.0,head position and bump
v1.7.0,relation box: head
v1.7.0,relation box: tail
v1.7.0,tail position and bump
v1.7.0,"r_head, r_mid, r_tail"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"repeat if necessary, and concat head and relation, batch_size', num_input_channels, 2*height, width"
v1.7.0,with batch_size' = batch_size * num_heads * num_relations
v1.7.0,"batch_size', num_input_channels, 2*height, width"
v1.7.0,"batch_size', num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.7.0,"reshape: (batch_size', embedding_dim) -> (b, h, r, 1, d)"
v1.7.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.7.0,"output_shape: (batch_size, num_heads, num_relations, num_tails)"
v1.7.0,add bias term
v1.7.0,decompose convolution for faster computation in 1-n case
v1.7.0,"compute conv(stack(h, r, t))"
v1.7.0,prepare input shapes for broadcasting
v1.7.0,"(b, h, r, t, 1, d)"
v1.7.0,"conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])"
v1.7.0,"here, kernel_size = (1, 3), C_in = 1, C_out = num_filters"
v1.7.0,"-> conv_head, conv_rel, conv_tail shapes: (num_filters,)"
v1.7.0,"reshape to (1, 1, 1, 1, f, 1)"
v1.7.0,"convolve -> output.shape: (*, embedding_dim, num_filters)"
v1.7.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.7.0,"Linear layer for final scores; use flattened representations, shape: (b, h, r, t, d * f)"
v1.7.0,same shape
v1.7.0,"split, shape: (embedding_dim, hidden_dim)"
v1.7.0,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.7.0,"Predict t embedding, shape: (b, h, r, 1, d)"
v1.7.0,"transpose t, (b, 1, 1, d, t)"
v1.7.0,"dot product, (b, h, r, 1, t)"
v1.7.0,"composite: (b, h, 1, t, d)"
v1.7.0,"transpose composite: (b, h, 1, d, t)"
v1.7.0,inner product with relation embedding
v1.7.0,Circular correlation of entity embeddings
v1.7.0,complex conjugate
v1.7.0,Hadamard product in frequency domain
v1.7.0,inverse real FFT
v1.7.0,global projections
v1.7.0,"combination, shape: (b, h, r, 1, d)"
v1.7.0,"dot product with t, shape: (b, h, r, t)"
v1.7.0,r expresses a rotation in complex plane.
v1.7.0,rotate head by relation (=Hadamard product in complex space)
v1.7.0,rotate tail by inverse of relation
v1.7.0,The inverse rotation is expressed by the complex conjugate of r.
v1.7.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.7.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.7.0,|h * r - t| = |h - conj(r) * t|
v1.7.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.7.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.7.0,"That is not mentioned in the paper, so it is made optional here."
v1.7.0,Project entities
v1.7.0,h projection to hyperplane
v1.7.0,r
v1.7.0,-t projection to hyperplane
v1.7.0,project to relation specific subspace and ensure constraints
v1.7.0,x_3 contraction
v1.7.0,x_1 contraction
v1.7.0,x_2 contraction
v1.7.0,Rotate (=Hamilton product in quaternion space).
v1.7.0,Rotation in quaternion space
v1.7.0,head interaction
v1.7.0,relation interaction (notice that h has been updated)
v1.7.0,combination
v1.7.0,similarity
v1.7.0,head
v1.7.0,relation box: head
v1.7.0,relation box: tail
v1.7.0,tail
v1.7.0,power norm
v1.7.0,the relation-specific head box base shape (normalized to have a volume of 1):
v1.7.0,the relation-specific tail box base shape (normalized to have a volume of 1):
v1.7.0,head
v1.7.0,relation
v1.7.0,tail
v1.7.0,version 2: relation factor offset
v1.7.0,extension: negative (power) norm
v1.7.0,note: normalization should be done from the representations
v1.7.0,cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501
v1.7.0,version 2
v1.7.0,r_head = r_head + u * torch.ones_like(r_head)
v1.7.0,r_tail = r_tail + u * torch.ones_like(r_tail)
v1.7.0,"stack h & r (+ broadcast) => shape: (2, batch_size', num_heads, num_relations, 1, *dims)"
v1.7.0,"remember shape for output, but reshape for transformer"
v1.7.0,"get position embeddings, shape: (seq_len, dim)"
v1.7.0,Now we are position-dependent w.r.t qualifier pairs.
v1.7.0,"seq_length, batch_size, dim"
v1.7.0,Pool output
v1.7.0,"output shape: (batch_size, dim)"
v1.7.0,reshape
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Concrete classes
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,don't worry about functions because they can't be specified by JSON.
v1.7.0,Could make a better mo
v1.7.0,later could extend for other non-JSON valid types
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Score with original triples
v1.7.0,Score with inverse triples
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Create directory in which all experimental artifacts are saved
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,distribute the deteriorated triples across the remaining factories
v1.7.0,"'kinships',"
v1.7.0,"'umls',"
v1.7.0,"'codexsmall',"
v1.7.0,"'wn18',"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: Functions for specifying exotic resources with a given prefix
v1.7.0,: Functions for specifying exotic resources based on their file extension
v1.7.0,Input validation
v1.7.0,convert to numpy
v1.7.0,Additional columns
v1.7.0,convert PyTorch tensors to numpy
v1.7.0,convert to dataframe
v1.7.0,Re-order columns
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.7.0,TODO vectorize code
v1.7.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Split triples
v1.7.0,Sorting ensures consistent results when the triples are permuted
v1.7.0,Create mapping
v1.7.0,Sorting ensures consistent results when the triples are permuted
v1.7.0,Create mapping
v1.7.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.7.0,Filter all non-existent triples
v1.7.0,Note: Unique changes the order of the triples
v1.7.0,Note: Using unique means implicit balancing of training samples
v1.7.0,normalize input
v1.7.0,TODO: method is_inverse?
v1.7.0,TODO: inverse of inverse?
v1.7.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.7.0,Id of inverse relation: relation + 1
v1.7.0,: The mapping from labels to IDs.
v1.7.0,: The inverse mapping for label_to_id; initialized automatically
v1.7.0,: A vectorized version of entity_label_to_id; initialized automatically
v1.7.0,: A vectorized version of entity_id_to_label; initialized automatically
v1.7.0,Normalize input
v1.7.0,label
v1.7.0,Filter for entities
v1.7.0,Filter for relations
v1.7.0,No filter
v1.7.0,check new label to ID mappings
v1.7.0,Make new triples factories for each group
v1.7.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.7.0,prepare metadata
v1.7.0,Delegate to function
v1.7.0,"restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed"
v1.7.0,Check if the triples are inverted already
v1.7.0,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.7.0,contained if and only if create_inverse_triples is True.
v1.7.0,Generate entity mapping if necessary
v1.7.0,Generate relation mapping if necessary
v1.7.0,Map triples of labels to triples of IDs.
v1.7.0,TODO: Check if lazy evaluation would make sense
v1.7.0,pre-filter to keep only topk
v1.7.0,if top is larger than the number of available options
v1.7.0,generate text
v1.7.0,vectorized label lookup
v1.7.0,Re-order columns
v1.7.0,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.7.0,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Split indices
v1.7.0,Split triples
v1.7.0,select one triple per relation
v1.7.0,maintain set of covered entities
v1.7.0,"Select one triple for each head/tail entity, which is not yet covered."
v1.7.0,create mask
v1.7.0,Prepare split index
v1.7.0,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.7.0,[...] is necessary for Python 3.7 compatibility
v1.7.0,base cases
v1.7.0,IDs not in training
v1.7.0,triples with exclusive test IDs
v1.7.0,While there are still triples that should be moved to the training set
v1.7.0,Pick a random triple to move over to the training triples
v1.7.0,add to training
v1.7.0,remove from testing
v1.7.0,Recalculate the move_id_mask
v1.7.0,Make sure that the first element has all the right stuff in it
v1.7.0,backwards compatibility
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,constants
v1.7.0,constants
v1.7.0,unary
v1.7.0,binary
v1.7.0,ternary
v1.7.0,column names
v1.7.0,return candidates
v1.7.0,index triples
v1.7.0,incoming relations per entity
v1.7.0,outgoing relations per entity
v1.7.0,indexing triples for fast join r1 & r2
v1.7.0,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.7.0,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.7.0,actual evaluation of the pattern
v1.7.0,skip empty support
v1.7.0,TODO: Can this happen after pre-filtering?
v1.7.0,"sort first, for triple order invariance"
v1.7.0,TODO: what is the support?
v1.7.0,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.7.0,sort decreasingly. i dominates j for all j > i in x-dimension
v1.7.0,"if it is also dominated by any y, it is not part of the skyline"
v1.7.0,"group by (relation id, pattern type)"
v1.7.0,"for each group, yield from skyline"
v1.7.0,determine patterns from triples
v1.7.0,drop zero-confidence
v1.7.0,keep only skyline
v1.7.0,create data frame
v1.7.0,iterate relation types
v1.7.0,drop zero-confidence
v1.7.0,keep only skyline
v1.7.0,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.7.0,base = skyline(base)
v1.7.0,create data frame
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,convert to csr for fast row slicing
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,check validity
v1.7.0,path compression
v1.7.0,collect connected components using union find with path compression
v1.7.0,get representatives
v1.7.0,already merged
v1.7.0,make x the smaller one
v1.7.0,merge
v1.7.0,extract partitions
v1.7.0,safe division for empty sets
v1.7.0,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.7.0,duplicates
v1.7.0,we are not interested in self-similarity
v1.7.0,compute similarities
v1.7.0,Calculate which relations are the inverse ones
v1.7.0,get existing IDs
v1.7.0,remove non-existing ID from label mapping
v1.7.0,create translation tensor
v1.7.0,get entities and relations occurring in triples
v1.7.0,generate ID translation and new label to Id mappings
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,preprocessing
v1.7.0,initialize
v1.7.0,sample iteratively
v1.7.0,determine weights
v1.7.0,randomly choose a vertex which has not been chosen yet
v1.7.0,normalize to probabilities
v1.7.0,sample a start node
v1.7.0,get list of neighbors
v1.7.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.7.0,visit target node
v1.7.0,decrease sample counts
v1.7.0,is already batched!
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.7.0,seamless loading and saving of training checkpoints
v1.7.0,Create training instances. Use the _create_instances function to allow subclasses
v1.7.0,to modify this behavior
v1.7.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.7.0,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.7.0,"If a checkpoint file is given, it must be loaded if it exists already"
v1.7.0,"If the stopper dict has any keys, those are written back to the stopper"
v1.7.0,The checkpoint frequency needs to be set to save checkpoints
v1.7.0,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.7.0,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.7.0,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.7.0,Ensure the release of memory
v1.7.0,Clear optimizer
v1.7.0,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.7.0,due to the patience continue to train after the best epoch and thus alter the model
v1.7.0,Create a path
v1.7.0,Prepare all of the callbacks
v1.7.0,"Register a callback for the result tracker, if given"
v1.7.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.7.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.7.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.7.0,return the relevant parameters slice_size and batch_size
v1.7.0,Force weight initialization if training continuation is not explicitly requested.
v1.7.0,Reset the weights
v1.7.0,Create new optimizer
v1.7.0,Create a new lr scheduler and add the optimizer
v1.7.0,Ensure the model is on the correct device
v1.7.0,Create Sampler
v1.7.0,wrap training instances
v1.7.0,disable automatic batching
v1.7.0,no support for sub-batching
v1.7.0,this is already done
v1.7.0,Bind
v1.7.0,"When size probing, we don't want progress bars"
v1.7.0,Create progress bar
v1.7.0,Save the time to track when the saved point was available
v1.7.0,Training Loop
v1.7.0,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.7.0,Enforce training mode
v1.7.0,Accumulate loss over epoch
v1.7.0,Batching
v1.7.0,Only create a progress bar when not in size probing mode
v1.7.0,Flag to check when to quit the size probing
v1.7.0,Recall that torch *accumulates* gradients. Before passing in a
v1.7.0,"new instance, you need to zero out the gradients from the old instance"
v1.7.0,Get batch size of current batch (last batch may be incomplete)
v1.7.0,accumulate gradients for whole batch
v1.7.0,forward pass call
v1.7.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.7.0,update parameters according to optimizer
v1.7.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.7.0,constraints are no longer applied
v1.7.0,For testing purposes we're only interested in processing one batch
v1.7.0,When size probing we don't need the losses
v1.7.0,Update learning rate scheduler
v1.7.0,Track epoch loss
v1.7.0,Print loss information to console
v1.7.0,Save the last successful finished epoch
v1.7.0,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.7.0,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.7.0,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.7.0,During automatic memory optimization only the error message is of interest
v1.7.0,When there wasn't a best epoch the checkpoint path should be None
v1.7.0,Delete temporary best epoch model
v1.7.0,Includes a call to result_tracker.log_metrics
v1.7.0,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.7.0,MyPy overrides are because you should
v1.7.0,When there wasn't a best epoch the checkpoint path should be None
v1.7.0,Delete temporary best epoch model
v1.7.0,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.7.0,at that state
v1.7.0,Delete temporary best epoch model
v1.7.0,forward pass
v1.7.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.7.0,correction for loss reduction
v1.7.0,backward pass
v1.7.0,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.7.0,Set upper bound
v1.7.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.7.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.7.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.7.0,"Only if a cuda device is available, the random state is accessed"
v1.7.0,This is an entire checkpoint for the optional best model when using early stopping
v1.7.0,Saving triples factory related states
v1.7.0,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.7.0,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.7.0,Check whether the triples factory mappings match those from the checkpoints
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Shuffle each epoch
v1.7.0,Lazy-splitting into batches
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Slicing is not possible in sLCWA training loops
v1.7.0,Send positive batch to device
v1.7.0,"Create negative samples, shape: (batch_size, num_neg_per_pos, 3)"
v1.7.0,apply filter mask
v1.7.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.7.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.7.0,Compute negative and positive scores
v1.7.0,Slicing is not possible for sLCWA
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: A hint for constructing a :class:`MultiTrainingCallback`
v1.7.0,: A collection of callbacks
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,normalize target column
v1.7.0,The type inference is so confusing between the function switching
v1.7.0,and polymorphism introduced by slicability that these need to be ignored
v1.7.0,Split batch components
v1.7.0,Send batch to device
v1.7.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.7.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,To make MyPy happy
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,now: smaller is better
v1.7.0,: The model
v1.7.0,: The evaluator
v1.7.0,: The triples to use for training (to be used during filtered evaluation)
v1.7.0,: The triples to use for evaluation
v1.7.0,: Size of the evaluation batches
v1.7.0,: Slice size of the evaluation batches
v1.7.0,: The number of epochs after which the model is evaluated on validation set
v1.7.0,: The number of iterations (one iteration can correspond to various epochs)
v1.7.0,: with no improvement after which training will be stopped.
v1.7.0,: The name of the metric to use
v1.7.0,: The minimum relative improvement necessary to consider it an improved result
v1.7.0,: The best result so far
v1.7.0,: The epoch at which the best result occurred
v1.7.0,: The remaining patience
v1.7.0,: The metric results from all evaluations
v1.7.0,": Whether a larger value is better, or a smaller"
v1.7.0,: The result tracker
v1.7.0,: Callbacks when after results are calculated
v1.7.0,: Callbacks when training gets continued
v1.7.0,: Callbacks when training is stopped early
v1.7.0,: Did the stopper ever decide to stop?
v1.7.0,TODO: Fix this
v1.7.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.7.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.7.0,Evaluate
v1.7.0,Only perform time consuming checks for the first call.
v1.7.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.7.0,Append to history
v1.7.0,check for improvement
v1.7.0,Stop if the result did not improve more than delta for patience evaluations
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Utils
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Transfer to cpu and convert to numpy
v1.7.0,Ensure that each key gets counted only once
v1.7.0,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.7.0,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.7.0,masks using the exact same key order.
v1.7.0,TODO how to define a cutoff on y_scores to make binary?
v1.7.0,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.7.0,Clear buffers
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.7.0,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.7.0,Clear the ranks from the current evaluator
v1.7.0,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.7.0,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.7.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.7.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.7.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.7.0,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.7.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.7.0,values_dict[key] will always be an int at this point
v1.7.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.7.0,Test if slicing is implemented for the required functions of this model
v1.7.0,Split batch
v1.7.0,Bind shape
v1.7.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.7.0,Warn if all entities will be filtered
v1.7.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.7.0,verify that the triples have been filtered
v1.7.0,Filter triples if necessary
v1.7.0,Send to device
v1.7.0,Ensure evaluation mode
v1.7.0,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.7.0,Check whether we need to be prepared for filtering
v1.7.0,Check whether an evaluator needs access to the masks
v1.7.0,This can only be an unfiltered evaluator.
v1.7.0,Prepare for result filtering
v1.7.0,Send tensors to device
v1.7.0,Prepare batches
v1.7.0,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.7.0,Show progressbar
v1.7.0,Flag to check when to quit the size probing
v1.7.0,Disable gradient tracking
v1.7.0,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.7.0,batch-wise processing
v1.7.0,If we only probe sizes we do not need more than one batch
v1.7.0,Finalize
v1.7.0,Predict scores once
v1.7.0,Select scores of true
v1.7.0,Create positive filter for all corrupted
v1.7.0,Needs all positive triples
v1.7.0,Create filter
v1.7.0,Create a positive mask with the size of the scores from the positive filter
v1.7.0,Restrict to entities of interest
v1.7.0,Evaluate metrics on these *unfiltered* scores
v1.7.0,Filter
v1.7.0,The scores for the true triples have to be rewritten to the scores tensor
v1.7.0,Restrict to entities of interest
v1.7.0,Evaluate metrics on these *filtered* scores
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics
v1.7.0,: This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`
v1.7.0,"TODO there's something wrong with this, so add it later"
v1.7.0,classifier_annotator.higher(
v1.7.0,"rmc.pr_auc_score,"
v1.7.0,"name=""AUC-PR"","
v1.7.0,"description=""Area Under the Precision-Recall Curve"","
v1.7.0,"link=""https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score"","
v1.7.0,)
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Extra stats stuff
v1.7.0,The optimistic rank is the rank when assuming all options with an equal score are placed behind the currently
v1.7.0,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.7.0,The pessimistic rank is the rank when assuming all options with an equal score are placed in front of the
v1.7.0,"currently considered. Hence, the rank is the number of options which have at least the same score minus one"
v1.7.0,"(as the currently considered option in included in all options). As the rank is one-based, we have to add 1,"
v1.7.0,"which nullifies the ""minus 1"" from before."
v1.7.0,"The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank over"
v1.7.0,all permutations of the elements with the same score as the currently considered option.
v1.7.0,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.7.0,The expected rank of a random scoring
v1.7.0,normalize metric name
v1.7.0,handle spaces and case
v1.7.0,special case for hits_at_k
v1.7.0,TODO: Fractional?
v1.7.0,synonym normalization
v1.7.0,normalize side
v1.7.0,normalize rank type
v1.7.0,Adjusted mean rank calculation
v1.7.0,Clear buffers
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.7.0,to create the class
v1.7.0,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default regularizer class
v1.7.0,: The default parameters for the default regularizer class
v1.7.0,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.7.0,Make sure that all modules with parameters do have a reset_parameters method.
v1.7.0,Recursively visit all sub-modules
v1.7.0,skip self
v1.7.0,Track parents for blaming
v1.7.0,call reset_parameters if possible
v1.7.0,initialize from bottom to top
v1.7.0,This ensures that specialized initializations will take priority over the default ones of its components.
v1.7.0,emit warning if there where parameters which were not initialised by reset_parameters.
v1.7.0,Additional debug information
v1.7.0,Important: use ModuleList to ensure that Pytorch correctly handles their devices and parameters
v1.7.0,: The entity representations
v1.7.0,: The relation representations
v1.7.0,: The weight regularizers
v1.7.0,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.7.0,"model.modules(). Thereby, we can collect them automatically."
v1.7.0,Explicitly call reset_parameters to trigger initialization
v1.7.0,normalize input
v1.7.0,normalization
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,train model
v1.7.0,"note: as this is an example, the model is only trained for a few epochs,"
v1.7.0,"but not until convergence. In practice, you would usually first verify that"
v1.7.0,"the model is sufficiently good in prediction, before looking at uncertainty scores"
v1.7.0,predict triple scores with uncertainty
v1.7.0,"use a larger number of samples, to increase quality of uncertainty estimate"
v1.7.0,get most and least uncertain prediction on training set
v1.7.0,: The scores
v1.7.0,": The uncertainty, in the same shape as scores"
v1.7.0,Enforce evaluation mode
v1.7.0,set dropout layers to training mode
v1.7.0,draw samples
v1.7.0,compute mean and std
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Train a model (quickly)
v1.7.0,Get scores for *all* triples
v1.7.0,Get scores for top 15 triples
v1.7.0,initialize buffer on device
v1.7.0,"reshape, shape: (batch_size * num_entities,)"
v1.7.0,get top scores within batch
v1.7.0,append to global top scores
v1.7.0,reduce size if necessary
v1.7.0,initialize buffer on cpu
v1.7.0,Explicitly create triples
v1.7.0,"TODO: in the future, we may want to expose this method"
v1.7.0,set model to evaluation mode
v1.7.0,calculate batch scores
v1.7.0,base case: infer maximum batch size
v1.7.0,base case: single batch
v1.7.0,TODO: this could happen because of AMO
v1.7.0,TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043
v1.7.0,no OOM error.
v1.7.0,make sure triples are a numpy array
v1.7.0,make sure triples are 2d
v1.7.0,convert to ID-based
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"This empty 1-element tensor doesn't actually do anything,"
v1.7.0,but is necessary since models with no grad params blow
v1.7.0,up the optimizer
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The device on which this model and its submodules are stored
v1.7.0,: The default loss function class
v1.7.0,: The default parameters for the default loss function class
v1.7.0,: The instance of the loss
v1.7.0,Initialize the device
v1.7.0,Random seeds have to set before the embeddings are initialized
v1.7.0,Loss
v1.7.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.7.0,send to device
v1.7.0,special handling of inverse relations
v1.7.0,"when trained on inverse relations, the internal relation ID is twice the original relation ID"
v1.7.0,: The default regularizer class
v1.7.0,: The default parameters for the default regularizer class
v1.7.0,: The instance of the regularizer
v1.7.0,Regularizer
v1.7.0,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.7.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.7.0,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.7.0,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.7.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.7.0,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.7.0,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.7.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.7.0,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.7.0,: Primary embeddings for entities
v1.7.0,: Primary embeddings for relations
v1.7.0,"make sure to call this first, to reset regularizer state!"
v1.7.0,The following lines add in a post-init hook to all subclasses
v1.7.0,such that the reset_parameters_() function is run
v1.7.0,"sorry mypy, but this kind of evil must be permitted."
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Base Models
v1.7.0,Concrete Models
v1.7.0,Evaluation-only models
v1.7.0,Utils
v1.7.0,Abstract Models
v1.7.0,We might be able to relax this later
v1.7.0,baseline models behave differently
v1.7.0,Old style models should never be looked up
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,TODO rethink after RGCN update
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default loss function class
v1.7.0,: The default parameters for the default loss function class
v1.7.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.7.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.7.0,ConvE should be trained with inverse triples
v1.7.0,ConvE uses one bias for each entity
v1.7.0,Automatic calculation of remaining dimensions
v1.7.0,Parameter need to fulfil:
v1.7.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.7.0,weights
v1.7.0,"batch_size, num_input_channels, 2*height, width"
v1.7.0,"batch_size, num_input_channels, 2*height, width"
v1.7.0,"batch_size, num_input_channels, 2*height, width"
v1.7.0,"(N,C_out,H_out,W_out)"
v1.7.0,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.7.0,Embedding Regularization
v1.7.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.7.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.7.0,Embedding Regularization
v1.7.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.7.0,Embedding Regularization
v1.7.0,Code to repeat each item successively instead of the entire tensor
v1.7.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,head representation
v1.7.0,tail representation
v1.7.0,"Since CP uses different representations for entities in head / tail role,"
v1.7.0,"the current solution is a bit hacky, and may be improved. See discussion"
v1.7.0,on https://github.com/pykeen/pykeen/pull/663.
v1.7.0,Override to allow different head and tail entity representations
v1.7.0,normalization
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default loss function class
v1.7.0,: The default parameters for the default loss function class
v1.7.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.7.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.7.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.7.0,use torch's native complex data type
v1.7.0,use torch's native complex data type
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.7.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.7.0,: a normalized weight of 10 is used.
v1.7.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.7.0,Get embeddings
v1.7.0,"shape: (b, d)"
v1.7.0,"shape: (b, d, d)"
v1.7.0,"shape: (b, d)"
v1.7.0,Compute scores
v1.7.0,Regularization
v1.7.0,Compute scores
v1.7.0,Regularization
v1.7.0,Get embeddings
v1.7.0,Compute scores
v1.7.0,Regularization
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.7.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.7.0,In the code base only the weights of the output layer are used for regularization
v1.7.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,comment:
v1.7.0,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.7.0,uses float64
v1.7.0,entity bias for head
v1.7.0,entity bias for tail
v1.7.0,relation offset
v1.7.0,diagonal relation transformation matrix
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default entity normalizer parameters
v1.7.0,: The entity representations are normalized to L2 unit length
v1.7.0,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.7.0,"update initializer settings, cf."
v1.7.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.7.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.7.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.7.0,in the original implementation the embeddings are initialized in one parameter
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,"w: (k, d, d)"
v1.7.0,"vh: (k, d)"
v1.7.0,"vt: (k, d)"
v1.7.0,"b: (k,)"
v1.7.0,"u: (k,)"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The regularizer used by [yang2014]_ for DistMult
v1.7.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.7.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.7.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.7.0,: The LP settings used by [yang2014]_ for DistMult
v1.7.0,Bilinear product
v1.7.0,*: Elementwise multiplication
v1.7.0,Get embeddings
v1.7.0,Compute score
v1.7.0,Only regularize relation embeddings
v1.7.0,Get embeddings
v1.7.0,Rank against all entities
v1.7.0,Only regularize relation embeddings
v1.7.0,Get embeddings
v1.7.0,Rank against all entities
v1.7.0,Only regularize relation embeddings
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default settings for the entity constrainer
v1.7.0,Similarity function used for distributions
v1.7.0,element-wise covariance bounds
v1.7.0,Additional covariance embeddings
v1.7.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.7.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.7.0,Constraints are applied through post_parameter_update
v1.7.0,Get embeddings
v1.7.0,Compute entity distribution
v1.7.0,: a = \mu^T\Sigma^{-1}\mu
v1.7.0,: b = \log \det \Sigma
v1.7.0,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.7.0,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.7.0,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.7.0,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,diagonal entries
v1.7.0,off-diagonal
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The custom regularizer used by [wang2014]_ for TransH
v1.7.0,: The settings used by [wang2014]_ for TransH
v1.7.0,embeddings
v1.7.0,Normalise the normal vectors by their l2 norms
v1.7.0,TODO: Add initialization
v1.7.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.7.0,which enforces the defined soft constraints.
v1.7.0,Get embeddings
v1.7.0,Project to hyperplane
v1.7.0,Regularization term
v1.7.0,Get embeddings
v1.7.0,Project to hyperplane
v1.7.0,Regularization term
v1.7.0,Get embeddings
v1.7.0,Project to hyperplane
v1.7.0,Regularization term
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,TODO: Initialize from TransE
v1.7.0,embeddings
v1.7.0,"project to relation specific subspace, shape: (b, e, d_r)"
v1.7.0,ensure constraints
v1.7.0,"evaluate score function, shape: (b, e)"
v1.7.0,Get embeddings
v1.7.0,Get embeddings
v1.7.0,Get embeddings
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,": The default strategy for optimizing the model""s hyper-parameters"
v1.7.0,TODO: Decomposition kwargs
v1.7.0,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.7.0,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.7.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.7.0,create enriched entity representations
v1.7.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.7.0,Resolve interaction function
v1.7.0,set default relation representation
v1.7.0,cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,combined representation
v1.7.0,Resolve interaction function
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default loss function class
v1.7.0,: The default parameters for the default loss function class
v1.7.0,Core tensor
v1.7.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.7.0,Dropout
v1.7.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.7.0,Abbreviation
v1.7.0,Compute h_n = DO(BN(h))
v1.7.0,Compute wr = DO(W x_2 r)
v1.7.0,compute whr = DO(BN(h_n x_1 wr))
v1.7.0,Compute whr x_3 t
v1.7.0,Get embeddings
v1.7.0,Compute scores
v1.7.0,Get embeddings
v1.7.0,Compute scores
v1.7.0,Get embeddings
v1.7.0,Compute scores
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,Get embeddings
v1.7.0,TODO: Use torch.cdist
v1.7.0,"There were some performance/memory issues with cdist, cf."
v1.7.0,"https://github.com/pytorch/pytorch/issues?q=cdist however, @mberr thinks"
v1.7.0,they are mostly resolved by now. A Benefit would be that we can harness the
v1.7.0,"future (performance) improvements made by the core torch developers. However,"
v1.7.0,this will require some benchmarking.
v1.7.0,Get embeddings
v1.7.0,TODO: Use torch.cdist (see note above in score_hrt())
v1.7.0,Get embeddings
v1.7.0,TODO: Use torch.cdist (see note above in score_hrt())
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,entity bias for head
v1.7.0,relation position head
v1.7.0,relation shape head
v1.7.0,relation shape tail
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default loss function class
v1.7.0,: The default parameters for the default loss function class
v1.7.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.7.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.7.0,": regularization term by the number of elements, which is 200."
v1.7.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.7.0,extra embeddings
v1.7.0,forward model
v1.7.0,Regularization
v1.7.0,backward model
v1.7.0,Regularization
v1.7.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.7.0,"That is not mentioned in the paper, so it is omitted here."
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.7.0,weight initialization
v1.7.0,Get embeddings
v1.7.0,Embedding Regularization
v1.7.0,Concatenate them
v1.7.0,Compute scores
v1.7.0,Get embeddings
v1.7.0,Embedding Regularization
v1.7.0,First layer can be unrolled
v1.7.0,Send scores through rest of the network
v1.7.0,Get embeddings
v1.7.0,Embedding Regularization
v1.7.0,First layer can be unrolled
v1.7.0,Send scores through rest of the network
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,The dimensions affected by e'
v1.7.0,Project entities
v1.7.0,r_p (e_p.T e) + e'
v1.7.0,Enforce constraints
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: Secondary embeddings for entities
v1.7.0,: Secondary embeddings for relations
v1.7.0,Project entities
v1.7.0,score = -||h_bot + r - t_bot||_2^2
v1.7.0,Head
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,Regular relation embeddings
v1.7.0,The relation-specific interaction vector
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,dim is only a parameter to match the signature of torch.mean / torch.sum
v1.7.0,this class is not thought to be usable from outside
v1.7.0,Create an MLP for string aggregation
v1.7.0,always create representations for normal and inverse relations and padding
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,Decompose into real and imaginary part
v1.7.0,Rotate (=Hadamard product in complex space).
v1.7.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.7.0,Get embeddings
v1.7.0,Compute scores
v1.7.0,Embedding Regularization
v1.7.0,Get embeddings
v1.7.0,Rank against all entities
v1.7.0,Compute scores
v1.7.0,Embedding Regularization
v1.7.0,Get embeddings
v1.7.0,r expresses a rotation in complex plane.
v1.7.0,The inverse rotation is expressed by the complex conjugate of r.
v1.7.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.7.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.7.0,|h * r - t| = |h - conj(r) * t|
v1.7.0,Rank against all entities
v1.7.0,Compute scores
v1.7.0,Embedding Regularization
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default loss function class
v1.7.0,: The default parameters for the default loss function class
v1.7.0,Global entity projection
v1.7.0,Global relation projection
v1.7.0,Global combination bias
v1.7.0,Global combination bias
v1.7.0,Get embeddings
v1.7.0,Compute score
v1.7.0,Get embeddings
v1.7.0,Rank against all entities
v1.7.0,Get embeddings
v1.7.0,Rank against all entities
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Normalize relation embeddings
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default loss function class
v1.7.0,: The default parameters for the default loss function class
v1.7.0,: The LP settings used by [zhang2019]_ for QuatE.
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default settings for the entity constrainer
v1.7.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.7.0,Circular correlation of entity embeddings
v1.7.0,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.7.0,compatibility: new style fft returns complex tensor
v1.7.0,Hadamard product in frequency domain
v1.7.0,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.7.0,inner product with relation embedding
v1.7.0,Embedding Regularization
v1.7.0,Embedding Regularization
v1.7.0,Embedding Regularization
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default loss function class
v1.7.0,: The default parameters for the default loss function class
v1.7.0,Get embeddings
v1.7.0,Embedding Regularization
v1.7.0,Concatenate them
v1.7.0,Predict t embedding
v1.7.0,compare with all t's
v1.7.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.7.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.7.0,Embedding Regularization
v1.7.0,Concatenate them
v1.7.0,Predict t embedding
v1.7.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.7.0,Embedding Regularization
v1.7.0,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.7.0,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.7.0,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.7.0,Extend t
v1.7.0,Concatenate them
v1.7.0,Predict t embedding
v1.7.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.7.0,The results have to be realigned with the expected output of the score_h function
v1.7.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default parameters for the default loss function class
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default loss function class
v1.7.0,: The default parameters for the default loss function class
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the model's hyper-parameters
v1.7.0,: The default parameters for the default loss function class
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,create sparse matrix of absolute counts
v1.7.0,normalize to relative counts
v1.7.0,base case
v1.7.0,"note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could"
v1.7.0,"stay sparse here, with a potential of a huge memory benefit on large datasets!"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,These operations are deterministic and a random seed can be fixed
v1.7.0,just to avoid warnings
v1.7.0,These operations do not need to be performed on a GPU
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"if we really need access to the path later, we can expose it as a property"
v1.7.0,via self.writer.log_dir
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The WANDB run
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The name of the run
v1.7.0,": The configuration dictionary, a mapping from name -> value"
v1.7.0,: Should metrics be stored when running ``log_metrics()``?
v1.7.0,": The metrics, a mapping from step -> (name -> value)"
v1.7.0,: A hint for constructing a :class:`MultiResultTracker`
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Base classes
v1.7.0,Concrete classes
v1.7.0,Utilities
v1.7.0,always add a Python result tracker for storing the configuration
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The file extension for this writer (do not include dot)
v1.7.0,: The file where the results are written to.
v1.7.0,as_uri() requires the path to be absolute. resolve additionally also normalizes the path
v1.7.0,: The column names
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,store set of triples
v1.7.0,: some prime numbers for tuple hashing
v1.7.0,: The bit-array for the Bloom filter data structure
v1.7.0,Allocate bit array
v1.7.0,calculate number of hashing rounds
v1.7.0,index triples
v1.7.0,Store some meta-data
v1.7.0,pre-hash
v1.7.0,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Set the indices
v1.7.0,Bind number of negatives to sample
v1.7.0,Equally corrupt all sides
v1.7.0,Copy positive batch for corruption.
v1.7.0,"Do not detach, as no gradients should flow into the indices."
v1.7.0,Relations have a different index maximum than entities
v1.7.0,At least make sure to not replace the triples by the original value
v1.7.0,"To make sure we don't replace the {head, relation, tail} by the"
v1.7.0,original value we shift all values greater or equal than the original value by one up
v1.7.0,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.7.0,: A filterer for negative batches
v1.7.0,create unfiltered negative batch by corruption
v1.7.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Utils
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.7.0,create index structure
v1.7.0,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.7.0,: The concatenated sorted sets of head/tail entities
v1.7.0,"shape: (batch_size, num_neg_per_pos, 3)"
v1.7.0,Uniformly sample from head/tail offsets
v1.7.0,get corresponding entity
v1.7.0,"and position within triple (0: head, 2: tail)"
v1.7.0,write into negative batch
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Preprocessing: Compute corruption probabilities
v1.7.0,"compute tph, i.e. the average number of tail entities per head"
v1.7.0,"compute hpt, i.e. the average number of head entities per tail"
v1.7.0,Set parameter for Bernoulli distribution
v1.7.0,Bind number of negatives to sample
v1.7.0,Copy positive batch for corruption.
v1.7.0,"Do not detach, as no gradients should flow into the indices."
v1.7.0,Decide whether to corrupt head or tail
v1.7.0,Tails are corrupted if heads are not corrupted
v1.7.0,We at least make sure to not replace the triples by the original value
v1.7.0,"See below for explanation of why this is on a range of [0, num_entities - 1]"
v1.7.0,Randomly sample corruption.
v1.7.0,Replace heads
v1.7.0,Replace tails
v1.7.0,To make sure we don't replace the head by the original value
v1.7.0,we shift all values greater or equal than the original value by one up
v1.7.0,"for that reason we choose the random value from [0, num_entities -1]"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The random seed used at the beginning of the pipeline
v1.7.0,: The model trained by the pipeline
v1.7.0,: The training triples
v1.7.0,: The training loop used by the pipeline
v1.7.0,: The losses during training
v1.7.0,: The results evaluated by the pipeline
v1.7.0,: How long in seconds did training take?
v1.7.0,: How long in seconds did evaluation take?
v1.7.0,: An early stopper
v1.7.0,: The configuration
v1.7.0,: Any additional metadata as a dictionary
v1.7.0,: The version of PyKEEN used to create these results
v1.7.0,: The git hash of PyKEEN used to create these results
v1.7.0,TODO use pathlib here
v1.7.0,normalize keys
v1.7.0,TODO: this can only normalize rank-based metrics!
v1.7.0,only one original value => assume this to be the mean
v1.7.0,multiple values => assume they correspond to individual trials
v1.7.0,metrics accumulates rows for a dataframe for comparison against the original reported results (if any)
v1.7.0,"TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)"
v1.7.0,summarize
v1.7.0,skip special parameters
v1.7.0,FIXME this should never happen.
v1.7.0,1. Dataset
v1.7.0,2. Model
v1.7.0,3. Loss
v1.7.0,4. Regularizer
v1.7.0,5. Optimizer
v1.7.0,5.1 Learning Rate Scheduler
v1.7.0,6. Training Loop
v1.7.0,7. Training (ronaldo style)
v1.7.0,8. Evaluation
v1.7.0,9. Tracking
v1.7.0,Misc
v1.7.0,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.7.0,used random_seed to ensure reproducible results
v1.7.0,We have to set clear optimizer to False since training should be continued
v1.7.0,Start tracking
v1.7.0,evaluation restriction to a subset of entities/relations
v1.7.0,TODO should training be reset?
v1.7.0,TODO should kwargs for loss and regularizer be checked and raised for?
v1.7.0,Log model parameters
v1.7.0,Stopping
v1.7.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.7.0,Add logging for debugging
v1.7.0,Train like Cristiano Ronaldo
v1.7.0,Build up a list of triples if we want to be in the filtered setting
v1.7.0,"If the user gave custom ""additional_filter_triples"""
v1.7.0,Determine whether the validation triples should also be filtered while performing test evaluation
v1.7.0,TODO consider implications of duplicates
v1.7.0,Evaluate
v1.7.0,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.7.0,Add logging about evaluator for debugging
v1.7.0,"If the evaluation still fail using the CPU, the error is raised"
v1.7.0,"When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is"
v1.7.0,restarted with PyKEEN's automatic memory optimization
v1.7.0,"When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation"
v1.7.0,is restarted using the cpu
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Imported from PyTorch
v1.7.0,: A wrapper around the hidden scheduler base class
v1.7.0,: The default strategy for optimizing the lr_schedulers' hyper-parameters
v1.7.0,: A resolver for learning rate schedulers
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,TODO what happens if already exists?
v1.7.0,TODO incorporate setting of random seed
v1.7.0,pipeline_kwargs=dict(
v1.7.0,"random_seed=random_non_negative_int(),"
v1.7.0,"),"
v1.7.0,Add dataset to current_pipeline
v1.7.0,"Training, test, and validation paths are provided"
v1.7.0,Add loss function to current_pipeline
v1.7.0,Add regularizer to current_pipeline
v1.7.0,Add optimizer to current_pipeline
v1.7.0,Add training approach to current_pipeline
v1.7.0,Add evaluation
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,": The mapping from (graph-pair, side) to triple file name"
v1.7.0,: The internal dataset name
v1.7.0,: The hex digest for the zip file
v1.7.0,Input validation.
v1.7.0,For downloading
v1.7.0,For splitting
v1.7.0,Whether to create inverse triples
v1.7.0,shared directory for multiple datasets.
v1.7.0,ensure file is present
v1.7.0,TODO: Re-use ensure_from_google?
v1.7.0,read all triples from file
v1.7.0,"some ""entities"" have numeric labels"
v1.7.0,"pandas.read_csv(..., dtype=str) does not work properly."
v1.7.0,create triples factory
v1.7.0,split
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.7.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.7.0,the columns are not ordered properly.
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The name of the dataset to download
v1.7.0,FIXME these are already identifiers
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,relation typing
v1.7.0,constants
v1.7.0,unique
v1.7.0,compute over all triples
v1.7.0,Determine group key
v1.7.0,Add labels if requested
v1.7.0,TODO: Merge with _common?
v1.7.0,include hash over triples into cache-file name
v1.7.0,include part hash into cache-file name
v1.7.0,re-use cached file if possible
v1.7.0,select triples
v1.7.0,save to file
v1.7.0,Prune by support and confidence
v1.7.0,TODO: Consider merging with other analysis methods
v1.7.0,TODO: Consider merging with other analysis methods
v1.7.0,TODO: Consider merging with other analysis methods
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Raise matplotlib level
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,don't call this function by itself. assumes called through the `validation`
v1.7.0,property and the _training factory has already been loaded
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Normalize path
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: A factory wrapping the training triples
v1.7.0,": A factory wrapping the testing triples, that share indices with the training triples"
v1.7.0,": A factory wrapping the validation triples, that share indices with the training triples"
v1.7.0,: All datasets should take care of inverse triple creation
v1.7.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.7.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.7.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.7.0,: The directory in which the cached data is stored
v1.7.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.7.0,don't call this function by itself. assumes called through the `validation`
v1.7.0,property and the _training factory has already been loaded
v1.7.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.7.0,"relative paths within zip file's always follow Posix path, even on Windows"
v1.7.0,tarfile does not like pathlib
v1.7.0,: URL to the data to download
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,Concrete Classes
v1.7.0,Utilities
v1.7.0,Assume it's a file path
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,TODO update docs with table and CLI wtih generator
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,1. Dataset
v1.7.0,2. Model
v1.7.0,3. Loss
v1.7.0,4. Regularizer
v1.7.0,5. Optimizer
v1.7.0,5.1 Learning Rate Scheduler
v1.7.0,6. Training Loop
v1.7.0,7. Training
v1.7.0,8. Evaluation
v1.7.0,9. Trackers
v1.7.0,Misc.
v1.7.0,log pruning
v1.7.0,"trial was successful, but has to be ended"
v1.7.0,also show info
v1.7.0,2. Model
v1.7.0,3. Loss
v1.7.0,4. Regularizer
v1.7.0,5. Optimizer
v1.7.0,5.1 Learning Rate Scheduler
v1.7.0,"TODO this fixes the issue for negative samplers, but does not generally address it."
v1.7.0,"For example, some of them obscure their arguments with **kwargs, so should we look"
v1.7.0,at the parent class? Sounds like something to put in class resolver by using the
v1.7.0,"inspect module. For now, this solution will rely on the fact that the sampler is a"
v1.7.0,direct descendent of a parent NegativeSampler
v1.7.0,create result tracker to allow to gracefully close failed trials
v1.7.0,1. Dataset
v1.7.0,2. Model
v1.7.0,3. Loss
v1.7.0,4. Regularizer
v1.7.0,5. Optimizer
v1.7.0,5.1 Learning Rate Scheduler
v1.7.0,6. Training Loop
v1.7.0,7. Training
v1.7.0,8. Evaluation
v1.7.0,9. Tracker
v1.7.0,Misc.
v1.7.0,close run in result tracker
v1.7.0,raise the error again (which will be catched in study.optimize)
v1.7.0,: The :mod:`optuna` study object
v1.7.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.7.0,Output study information
v1.7.0,Output all trials
v1.7.0,Output best trial as pipeline configuration file
v1.7.0,1. Dataset
v1.7.0,2. Model
v1.7.0,3. Loss
v1.7.0,4. Regularizer
v1.7.0,5. Optimizer
v1.7.0,5.1 Learning Rate Scheduler
v1.7.0,6. Training Loop
v1.7.0,7. Training
v1.7.0,8. Evaluation
v1.7.0,9. Tracking
v1.7.0,6. Misc
v1.7.0,Optuna Study Settings
v1.7.0,Optuna Optimization Settings
v1.7.0,0. Metadata/Provenance
v1.7.0,1. Dataset
v1.7.0,2. Model
v1.7.0,3. Loss
v1.7.0,4. Regularizer
v1.7.0,5. Optimizer
v1.7.0,5.1 Learning Rate Scheduler
v1.7.0,6. Training Loop
v1.7.0,7. Training
v1.7.0,8. Evaluation
v1.7.0,9. Tracking
v1.7.0,1. Dataset
v1.7.0,2. Model
v1.7.0,3. Loss
v1.7.0,4. Regularizer
v1.7.0,5. Optimizer
v1.7.0,5.1 Learning Rate Scheduler
v1.7.0,6. Training Loop
v1.7.0,7. Training
v1.7.0,8. Evaluation
v1.7.0,9. Tracker
v1.7.0,Optuna Misc.
v1.7.0,Pipeline Misc.
v1.7.0,Invoke optimization of the objective function.
v1.7.0,TODO: make it even easier to specify categorical strategies just as lists
v1.7.0,"if isinstance(info, (tuple, list, set)):"
v1.7.0,"info = dict(type='categorical', choices=list(info))"
v1.7.0,get log from info - could either be a boolean or string
v1.7.0,"otherwise, dataset refers to a file that should be automatically split"
v1.7.0,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.7.0,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.7.0,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.7.0,if has_dataset(dataset_name):
v1.7.0,"study.set_user_attr('dataset', dataset_name)"
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.7.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,
v1.6.0,Configuration file for the Sphinx documentation builder.
v1.6.0,
v1.6.0,This file does only contain a selection of the most common options. For a
v1.6.0,full list see the documentation:
v1.6.0,http://www.sphinx-doc.org/en/master/config
v1.6.0,-- Path setup --------------------------------------------------------------
v1.6.0,"If extensions (or modules to document with autodoc) are in another directory,"
v1.6.0,add these directories to sys.path here. If the directory is relative to the
v1.6.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.6.0,
v1.6.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.6.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.6.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.6.0,from unittest.mock import Mock
v1.6.0,sys.modules['numpy'] = Mock()
v1.6.0,sys.modules['numpy.linalg'] = Mock()
v1.6.0,sys.modules['scipy'] = Mock()
v1.6.0,sys.modules['scipy.optimize'] = Mock()
v1.6.0,sys.modules['scipy.interpolate'] = Mock()
v1.6.0,sys.modules['scipy.sparse'] = Mock()
v1.6.0,sys.modules['scipy.ndimage'] = Mock()
v1.6.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.6.0,sys.modules['tensorflow'] = Mock()
v1.6.0,sys.modules['theano'] = Mock()
v1.6.0,sys.modules['theano.tensor'] = Mock()
v1.6.0,sys.modules['torch'] = Mock()
v1.6.0,sys.modules['torch.optim'] = Mock()
v1.6.0,sys.modules['torch.nn'] = Mock()
v1.6.0,sys.modules['torch.nn.init'] = Mock()
v1.6.0,sys.modules['torch.autograd'] = Mock()
v1.6.0,sys.modules['sklearn'] = Mock()
v1.6.0,sys.modules['sklearn.model_selection'] = Mock()
v1.6.0,sys.modules['sklearn.utils'] = Mock()
v1.6.0,-- Project information -----------------------------------------------------
v1.6.0,"The full version, including alpha/beta/rc tags."
v1.6.0,The short X.Y version.
v1.6.0,-- General configuration ---------------------------------------------------
v1.6.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.6.0,
v1.6.0,needs_sphinx = '1.0'
v1.6.0,"If true, the current module name will be prepended to all description"
v1.6.0,unit titles (such as .. function::).
v1.6.0,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.6.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.6.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.6.0,ones.
v1.6.0,show todo's
v1.6.0,generate autosummary pages
v1.6.0,"Add any paths that contain templates here, relative to this directory."
v1.6.0,The suffix(es) of source filenames.
v1.6.0,You can specify multiple suffix as a list of string:
v1.6.0,
v1.6.0,"source_suffix = ['.rst', '.md']"
v1.6.0,The master toctree document.
v1.6.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.6.0,for a list of supported languages.
v1.6.0,
v1.6.0,This is also used if you do content translation via gettext catalogs.
v1.6.0,"Usually you set ""language"" from the command line for these cases."
v1.6.0,"List of patterns, relative to source directory, that match files and"
v1.6.0,directories to ignore when looking for source files.
v1.6.0,This pattern also affects html_static_path and html_extra_path.
v1.6.0,The name of the Pygments (syntax highlighting) style to use.
v1.6.0,-- Options for HTML output -------------------------------------------------
v1.6.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.6.0,a list of builtin themes.
v1.6.0,
v1.6.0,Theme options are theme-specific and customize the look and feel of a theme
v1.6.0,"further.  For a list of options available for each theme, see the"
v1.6.0,documentation.
v1.6.0,
v1.6.0,html_theme_options = {}
v1.6.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.6.0,"relative to this directory. They are copied after the builtin static files,"
v1.6.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.6.0,html_static_path = ['_static']
v1.6.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.6.0,to template names.
v1.6.0,
v1.6.0,The default sidebars (for documents that don't match any pattern) are
v1.6.0,defined by theme itself.  Builtin themes are using these templates by
v1.6.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.6.0,'searchbox.html']``.
v1.6.0,
v1.6.0,html_sidebars = {}
v1.6.0,The name of an image file (relative to this directory) to place at the top
v1.6.0,of the sidebar.
v1.6.0,
v1.6.0,-- Options for HTMLHelp output ---------------------------------------------
v1.6.0,Output file base name for HTML help builder.
v1.6.0,-- Options for LaTeX output ------------------------------------------------
v1.6.0,latex_elements = {
v1.6.0,The paper size ('letterpaper' or 'a4paper').
v1.6.0,
v1.6.0,"'papersize': 'letterpaper',"
v1.6.0,
v1.6.0,"The font size ('10pt', '11pt' or '12pt')."
v1.6.0,
v1.6.0,"'pointsize': '10pt',"
v1.6.0,
v1.6.0,Additional stuff for the LaTeX preamble.
v1.6.0,
v1.6.0,"'preamble': '',"
v1.6.0,
v1.6.0,Latex figure (float) alignment
v1.6.0,
v1.6.0,"'figure_align': 'htbp',"
v1.6.0,}
v1.6.0,Grouping the document tree into LaTeX files. List of tuples
v1.6.0,"(source start file, target name, title,"
v1.6.0,"author, documentclass [howto, manual, or own class])."
v1.6.0,latex_documents = [
v1.6.0,(
v1.6.0,"master_doc,"
v1.6.0,"'pykeen.tex',"
v1.6.0,"'PyKEEN Documentation',"
v1.6.0,"author,"
v1.6.0,"'manual',"
v1.6.0,"),"
v1.6.0,]
v1.6.0,-- Options for manual page output ------------------------------------------
v1.6.0,One entry per manual page. List of tuples
v1.6.0,"(source start file, name, description, authors, manual section)."
v1.6.0,-- Options for Texinfo output ----------------------------------------------
v1.6.0,Grouping the document tree into Texinfo files. List of tuples
v1.6.0,"(source start file, target name, title, author,"
v1.6.0,"dir menu entry, description, category)"
v1.6.0,-- Options for Epub output -------------------------------------------------
v1.6.0,Bibliographic Dublin Core info.
v1.6.0,epub_title = project
v1.6.0,The unique identifier of the text. This can be a ISBN number
v1.6.0,or the project homepage.
v1.6.0,
v1.6.0,epub_identifier = ''
v1.6.0,A unique identification for the text.
v1.6.0,
v1.6.0,epub_uid = ''
v1.6.0,A list of files that should not be packed into the epub file.
v1.6.0,epub_exclude_files = ['search.html']
v1.6.0,-- Extension configuration -------------------------------------------------
v1.6.0,-- Options for intersphinx extension ---------------------------------------
v1.6.0,Example configuration for intersphinx: refer to the Python standard library.
v1.6.0,autodoc_member_order = 'bysource'
v1.6.0,autodoc_typehints = 'both' # TODO turn on after 4.1 release
v1.6.0,autodoc_preserve_defaults = True
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The batch size
v1.6.0,: The tested class
v1.6.0,check probability distribution
v1.6.0,check probability distribution
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Check a model param is optimized
v1.6.0,Check a loss param is optimized
v1.6.0,Check a model param is NOT optimized
v1.6.0,Check a loss param is optimized
v1.6.0,Check a model param is optimized
v1.6.0,Check a loss param is NOT optimized
v1.6.0,Check a model param is NOT optimized
v1.6.0,Check a loss param is NOT optimized
v1.6.0,verify failure
v1.6.0,"Since custom data was passed, we can't store any of this"
v1.6.0,"currently, any custom data doesn't get stored."
v1.6.0,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.6.0,"Since there's no source path information, these shouldn't be"
v1.6.0,"added, even if it might be possible to infer path information"
v1.6.0,from the triples factories
v1.6.0,"Since paths were passed for training, testing, and validation,"
v1.6.0,they should be stored as study-level attributes
v1.6.0,Check a model param is optimized
v1.6.0,Check a loss param is optimized
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,The triples factory and model
v1.6.0,: The evaluator to be tested
v1.6.0,Settings
v1.6.0,: The evaluator instantiation
v1.6.0,Settings
v1.6.0,Initialize evaluator
v1.6.0,Use small test dataset
v1.6.0,Use small model (untrained)
v1.6.0,Get batch
v1.6.0,Compute scores
v1.6.0,Compute mask only if required
v1.6.0,TODO: Re-use filtering code
v1.6.0,"shape: (batch_size, num_triples)"
v1.6.0,"shape: (batch_size, num_entities)"
v1.6.0,Process one batch
v1.6.0,Check for correct class
v1.6.0,Check value ranges
v1.6.0,check mean rank (MR)
v1.6.0,check mean reciprocal rank (MRR)
v1.6.0,check hits at k (H@k)
v1.6.0,check adjusted mean rank (AMR)
v1.6.0,check adjusted mean rank index (AMRI)
v1.6.0,TODO: Validate with data?
v1.6.0,Check for correct class
v1.6.0,check value
v1.6.0,filtering
v1.6.0,"true_score: (2, 3, 3)"
v1.6.0,head based filter
v1.6.0,preprocessing for faster lookup
v1.6.0,check that all found positives are positive
v1.6.0,check in-place
v1.6.0,Test head scores
v1.6.0,Assert in-place modification
v1.6.0,Assert correct filtering
v1.6.0,Test tail scores
v1.6.0,Assert in-place modification
v1.6.0,Assert correct filtering
v1.6.0,The MockModel gives the highest score to the highest entity id
v1.6.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.6.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.6.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.6.0,test triple at hand
v1.6.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.6.0,test triple at hand
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,check if within 0.5 std of observed
v1.6.0,test error is raised
v1.6.0,Tests that exception will be thrown when more than or less than three tensors are passed
v1.6.0,Test that regularization term is computed correctly
v1.6.0,Entity soft constraint
v1.6.0,Orthogonality soft constraint
v1.6.0,ensure regularizer is on correct device
v1.6.0,"After first update, should change the term"
v1.6.0,"After second update, no change should happen"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,create broadcastable shapes
v1.6.0,check correct value range
v1.6.0,check maximum norm constraint
v1.6.0,unchanged values for small norms
v1.6.0,generate random query tensor
v1.6.0,random entity embeddings & projections
v1.6.0,random relation embeddings & projections
v1.6.0,project
v1.6.0,check shape:
v1.6.0,check normalization
v1.6.0,check equivalence of re-formulation
v1.6.0,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.6.0,= r_p (e_p^T e) + e'
v1.6.0,"create random array, estimate the costs of addition, and measure some execution times."
v1.6.0,"then, compute correlation between the estimated cost, and the measured time."
v1.6.0,check for strong correlation between estimated costs and measured execution time
v1.6.0,get optimal sequence
v1.6.0,check caching
v1.6.0,get optimal sequence
v1.6.0,check correct cost
v1.6.0,check optimality
v1.6.0,compare result to sequential addition
v1.6.0,compare result to sequential addition
v1.6.0,check result shape
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,equal value; larger is better
v1.6.0,equal value; smaller is better
v1.6.0,larger is better; improvement
v1.6.0,larger is better; improvement; but not significant
v1.6.0,: The window size used by the early stopper
v1.6.0,: The mock losses the mock evaluator will return
v1.6.0,: The (zeroed) index  - 1 at which stopping will occur
v1.6.0,: The minimum improvement
v1.6.0,: The best results
v1.6.0,Set automatic_memory_optimization to false for tests
v1.6.0,Step early stopper
v1.6.0,check storing of results
v1.6.0,check ring buffer
v1.6.0,: The window size used by the early stopper
v1.6.0,: The (zeroed) index  - 1 at which stopping will occur
v1.6.0,: The minimum improvement
v1.6.0,: The random seed to use for reproducibility
v1.6.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.6.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.6.0,: The batch size to use.
v1.6.0,Fix seed for reproducibility
v1.6.0,Set automatic_memory_optimization to false during testing
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.6.0,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.6.0,"first triple, and passing create_inverse_triples=True."
v1.6.0,check for warning
v1.6.0,check for filtered triples
v1.6.0,check for correct inverse triples flag
v1.6.0,check correct translation
v1.6.0,check column order
v1.6.0,apply restriction
v1.6.0,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.6.0,check that inverse_triples is correctly carried over
v1.6.0,verify that the label-to-ID mapping has not been changed
v1.6.0,verify that triples have been filtered
v1.6.0,check compressed triples
v1.6.0,reconstruct triples from compressed form
v1.6.0,check data loader
v1.6.0,set create inverse triple to true
v1.6.0,split factory
v1.6.0,check that in *training* inverse triple are to be created
v1.6.0,check that in all other splits no inverse triples are to be created
v1.6.0,verify that all entities and relations are present in the training factory
v1.6.0,verify that no triple got lost
v1.6.0,verify that the label-to-id mappings match
v1.6.0,check type
v1.6.0,check format
v1.6.0,check coverage
v1.6.0,Check if multilabels are working correctly
v1.6.0,Slightly larger number of triples to guarantee split can find coverage of all entities and relations.
v1.6.0,generate random ratios
v1.6.0,check size
v1.6.0,check value range
v1.6.0,check total split
v1.6.0,check consistency with ratios
v1.6.0,the number of decimal digits equivalent to 1 / n_total
v1.6.0,check type
v1.6.0,check values
v1.6.0,compare against expected
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,"DummyModel,"
v1.6.0,3x batch norm: bias + scale --> 6
v1.6.0,entity specific bias        --> 1
v1.6.0,==================================
v1.6.0,7
v1.6.0,"two bias terms, one conv-filter"
v1.6.0,check type
v1.6.0,check shape
v1.6.0,check ID ranges
v1.6.0,this is only done in one of the models
v1.6.0,this is only done in one of the models
v1.6.0,Two linear layer biases
v1.6.0,"Two BN layers, bias & scale"
v1.6.0,quaternion have four components
v1.6.0,: one bias per layer
v1.6.0,: (scale & bias for BN) * layers
v1.6.0,entity embeddings
v1.6.0,relation embeddings
v1.6.0,Compute Scores
v1.6.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.6.0,relation embeddings
v1.6.0,Compute Scores
v1.6.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.6.0,entity embeddings
v1.6.0,relation embeddings
v1.6.0,Compute Scores
v1.6.0,random entity embeddings & projections
v1.6.0,random relation embeddings & projections
v1.6.0,project
v1.6.0,check shape:
v1.6.0,check normalization
v1.6.0,entity embeddings
v1.6.0,relation embeddings
v1.6.0,Compute Scores
v1.6.0,second_score = scores[1].item()
v1.6.0,: 2xBN (bias & scale)
v1.6.0,the combination bias
v1.6.0,check shape
v1.6.0,check content
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,empty lists are falsy
v1.6.0,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.6.0,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.6.0,pipeline is checked.
v1.6.0,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.6.0,Resume the previous pipeline
v1.6.0,The MockModel gives the highest score to the highest entity id
v1.6.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.6.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.6.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.6.0,test triple at hand
v1.6.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.6.0,test triple at hand
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.6.0,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.6.0,Test looking up inverse triples
v1.6.0,test new label to ID
v1.6.0,type
v1.6.0,old labels
v1.6.0,"new, compact IDs"
v1.6.0,test vectorized lookup
v1.6.0,type
v1.6.0,shape
v1.6.0,value range
v1.6.0,only occurring Ids get mapped to non-negative numbers
v1.6.0,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.6.0,check type
v1.6.0,check shape
v1.6.0,check content
v1.6.0,check type
v1.6.0,check shape
v1.6.0,check 1-hot
v1.6.0,check type
v1.6.0,check shape
v1.6.0,check value range
v1.6.0,check self-similarity = 1
v1.6.0,base relation
v1.6.0,exact duplicate
v1.6.0,99% duplicate
v1.6.0,50% duplicate
v1.6.0,exact inverse
v1.6.0,99% inverse
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The expected number of entities
v1.6.0,: The expected number of relations
v1.6.0,: The expected number of triples
v1.6.0,": The tolerance on expected number of triples, for randomized situations"
v1.6.0,: The dataset to test
v1.6.0,: The instantiated dataset
v1.6.0,: Should the validation be assumed to have been loaded with train/test?
v1.6.0,Not loaded
v1.6.0,Load
v1.6.0,Test caching
v1.6.0,assert (end - start) < 1.0e-02
v1.6.0,Test consistency of training / validation / testing mapping
v1.6.0,": The directory, if there is caching"
v1.6.0,: The batch size
v1.6.0,: The number of negatives per positive for sLCWA training loop.
v1.6.0,: The number of entities LCWA training loop / label smoothing.
v1.6.0,test reduction
v1.6.0,test finite loss value
v1.6.0,Test backward
v1.6.0,negative scores decreased compared to positive ones
v1.6.0,negative scores decreased compared to positive ones
v1.6.0,: The number of entities.
v1.6.0,: The number of negative samples
v1.6.0,: The number of entities.
v1.6.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.6.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.6.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.6.0,check whether the error originates from batch norm for single element batches
v1.6.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.6.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.6.0,test multiple different initializations
v1.6.0,calculate by functional
v1.6.0,calculate manually
v1.6.0,simple
v1.6.0,nested
v1.6.0,nested
v1.6.0,prepare a temporary test directory
v1.6.0,check that file was created
v1.6.0,make sure to close file before trying to delete it
v1.6.0,delete intermediate files
v1.6.0,: The batch size
v1.6.0,: The triples factory
v1.6.0,: Class of regularizer to test
v1.6.0,: The constructor parameters to pass to the regularizer
v1.6.0,": The regularizer instance, initialized in setUp"
v1.6.0,: A positive batch
v1.6.0,: The device
v1.6.0,move test instance to device
v1.6.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.6.0,Check if regularizer is stored correctly.
v1.6.0,Forward pass (should update regularizer)
v1.6.0,Call post_parameter_update (should reset regularizer)
v1.6.0,Check if regularization term is reset
v1.6.0,Call method
v1.6.0,Generate random tensors
v1.6.0,Call update
v1.6.0,check shape
v1.6.0,compute expected term
v1.6.0,Generate random tensor
v1.6.0,calculate penalty
v1.6.0,check shape
v1.6.0,check value
v1.6.0,FIXME isn't any finite number allowed now?
v1.6.0,: Additional arguments passed to the training loop's constructor method
v1.6.0,: The triples factory instance
v1.6.0,: The batch size for use for forward_* tests
v1.6.0,: The embedding dimensionality
v1.6.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.6.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.6.0,: The batch size for use when testing training procedures
v1.6.0,: The number of epochs to train the model
v1.6.0,: A random number generator from torch
v1.6.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.6.0,initialization
v1.6.0,: Static extras to append to the CLI
v1.6.0,for reproducible testing
v1.6.0,insert shared parameters
v1.6.0,move model to correct device
v1.6.0,assert there is at least one trainable parameter
v1.6.0,Check that all the parameters actually require a gradient
v1.6.0,Try to initialize an optimizer
v1.6.0,get model parameters
v1.6.0,re-initialize
v1.6.0,check that the operation works in-place
v1.6.0,check that the parameters where modified
v1.6.0,check for finite values by default
v1.6.0,check whether a gradient can be back-propgated
v1.6.0,"assert batch comprises (head, relation) pairs"
v1.6.0,"assert batch comprises (relation, tail) pairs"
v1.6.0,"For the high/low memory test cases of NTN, SE, etc."
v1.6.0,"else, leave to default"
v1.6.0,TODO: Make sure that inverse triples are created if create_inverse_triples=True
v1.6.0,triples factory is added by the pipeline
v1.6.0,TODO: Catch HolE MKL error?
v1.6.0,set regularizer term to something that isn't zero
v1.6.0,call post_parameter_update
v1.6.0,assert that the regularization term has been reset
v1.6.0,do one optimization step
v1.6.0,call post_parameter_update
v1.6.0,check model constraints
v1.6.0,"assert batch comprises (relation, tail) pairs"
v1.6.0,"assert batch comprises (relation, tail) pairs"
v1.6.0,"assert batch comprises (relation, tail) pairs"
v1.6.0,call some functions
v1.6.0,reset to old state
v1.6.0,Distance-based model
v1.6.0,check type
v1.6.0,check shape
v1.6.0,: The number of entities
v1.6.0,: The number of triples
v1.6.0,check shape
v1.6.0,check dtype
v1.6.0,check finite values (e.g. due to division by zero)
v1.6.0,check non-negativity
v1.6.0,: The input dimension
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,check for finite values by default
v1.6.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.6.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.6.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.6.0,Get embeddings
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,≈ result of softmax
v1.6.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.6.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.6.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.6.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.6.0,≈ result of sigmoid
v1.6.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.6.0,expected_loss ≈ 0.34
v1.6.0,Create dummy dense labels
v1.6.0,Check if labels form a probability distribution
v1.6.0,Apply label smoothing
v1.6.0,Check if smooth labels form probability distribution
v1.6.0,Create dummy sLCWA labels
v1.6.0,Apply label smoothing
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,"naive implementation, O(n2)"
v1.6.0,check correct output type
v1.6.0,check value range subset
v1.6.0,check value range side
v1.6.0,check columns
v1.6.0,check value range and type
v1.6.0,check value range entity IDs
v1.6.0,check value range entity labels
v1.6.0,check correct type
v1.6.0,check relation_id value range
v1.6.0,check pattern value range
v1.6.0,check confidence value range
v1.6.0,check support value range
v1.6.0,check correct type
v1.6.0,check relation_id value range
v1.6.0,check pattern value range
v1.6.0,check correct type
v1.6.0,check relation_id value range
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Check minimal statistics
v1.6.0,Check statistics for pre-stratified datasets
v1.6.0,Check either a github link or author/publication information is given
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.6.0,"prepare conv input (N, C, H, W)"
v1.6.0,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.6.0,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.6.0,remove batch/num dimension
v1.6.0,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.6.0,"f(h, r, t) = h @ r @ t"
v1.6.0,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.6.0,normalize length of r
v1.6.0,check for unit length
v1.6.0,entity embeddings
v1.6.0,relation embeddings
v1.6.0,Compute Scores
v1.6.0,entity embeddings
v1.6.0,relation embeddings
v1.6.0,Compute Scores
v1.6.0,Compute Scores
v1.6.0,-\|R_h h - R_t t\|
v1.6.0,-\|h - t\|
v1.6.0,"Since MuRE has offsets, the scores do not need to negative"
v1.6.0,"We do not need this, since we do not check for functional consistency anyway"
v1.6.0,intra-interaction comparison
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,create a new instance with guaranteed dropout
v1.6.0,set to training mode
v1.6.0,check for different output
v1.6.0,: The number of embeddings
v1.6.0,check shape
v1.6.0,check attributes
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,"typically, the model takes care of adjusting the dimension size for ""complex"""
v1.6.0,"tensors, but we have to do it manually here for testing purposes"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,ensure positivity
v1.6.0,compute using pytorch
v1.6.0,prepare distributions
v1.6.0,compute using pykeen
v1.6.0,"e: (batch_size, num_heads, num_tails, d)"
v1.6.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.6.0,divergence = 0 => similarity = -divergence = 0
v1.6.0,"(h - t), r"
v1.6.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.6.0,divergence >= 0 => similarity = -divergence <= 0
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.6.0,filter vs. no filter thing.
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,check for empty batches
v1.6.0,: The window size used by the early stopper
v1.6.0,: The mock losses the mock evaluator will return
v1.6.0,: The (zeroed) index  - 1 at which stopping will occur
v1.6.0,: The minimum improvement
v1.6.0,: The best results
v1.6.0,Set automatic_memory_optimization to false for tests
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Train a model in one shot
v1.6.0,Train a model for the first half
v1.6.0,Continue training of the first part
v1.6.0,: Should negative samples be filtered?
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Check whether filtering works correctly
v1.6.0,First giving an example where all triples have to be filtered
v1.6.0,The filter should remove all triples
v1.6.0,Create an example where no triples will be filtered
v1.6.0,The filter should not remove any triple
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,sample a batch
v1.6.0,check shape
v1.6.0,get triples
v1.6.0,check connected components
v1.6.0,super inefficient
v1.6.0,join
v1.6.0,already joined
v1.6.0,check that there is only a single component
v1.6.0,check content of comp_adj_lists
v1.6.0,check edge ids
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Test that half of the subjects and half of the objects are corrupted
v1.6.0,same relation
v1.6.0,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.6.0,check that corrupted entities co-occur with the relation in training data
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The batch size
v1.6.0,: The random seed
v1.6.0,: The triples factory
v1.6.0,: The instances
v1.6.0,: A positive batch
v1.6.0,: Kwargs
v1.6.0,Generate negative sample
v1.6.0,check filter shape if necessary
v1.6.0,check shape
v1.6.0,check bounds: heads
v1.6.0,check bounds: relations
v1.6.0,check bounds: tails
v1.6.0,test that the negative triple is not the original positive triple
v1.6.0,"shape: (batch_size, 1, num_neg)"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Base Classes
v1.6.0,Concrete Classes
v1.6.0,Utils
v1.6.0,: The default strategy for optimizing the loss's hyper-parameters
v1.6.0,flatten and stack
v1.6.0,apply label smoothing if necessary.
v1.6.0,TODO: Do label smoothing only once
v1.6.0,Sanity check
v1.6.0,"prepare for broadcasting, shape: (batch_size, 1, 3)"
v1.6.0,negative_scores have already been filtered in the sampler!
v1.6.0,"shape: (nnz,)"
v1.6.0,Sanity check
v1.6.0,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.6.0,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.6.0,positive entries in the labels tensor.
v1.6.0,"This shows how often one row has to be repeated,"
v1.6.0,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.6.0,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.6.0,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.6.0,This tells us how often each true label should be repeated
v1.6.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.6.0,1. positive & negative margin
v1.6.0,2. negative margin & offset
v1.6.0,3. positive margin & offset
v1.6.0,Sanity check
v1.6.0,positive term
v1.6.0,implicitly repeat positive scores
v1.6.0,"shape: (nnz,)"
v1.6.0,negative term
v1.6.0,negative_scores have already been filtered in the sampler!
v1.6.0,Sanity check
v1.6.0,"scale labels from [0, 1] to [-1, 1]"
v1.6.0,cross entropy expects a proper probability distribution -> normalize labels
v1.6.0,Use numerically stable variant to compute log(softmax)
v1.6.0,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.6.0,Sanity check
v1.6.0,compute negative weights (without gradient tracking)
v1.6.0,clone is necessary since we modify in-place
v1.6.0,Split positive and negative scores
v1.6.0,Sanity check
v1.6.0,negative_scores have already been filtered in the sampler!
v1.6.0,(dense) softmax requires unfiltered scores / masking
v1.6.0,we need to fill the scores with -inf for all filtered negative examples
v1.6.0,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.6.0,use filled negatives scores
v1.6.0,compute weights (without gradient tracking)
v1.6.0,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.6.0,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.6.0,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.6.0,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.6.0,": For more information, see https://github.com/cthoyt/pystow"
v1.6.0,: A path representing the PyKEEN data folder
v1.6.0,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.6.0,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.6.0,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.6.0,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.6.0,: A subdirectory for PyKEEN logs
v1.6.0,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.6.0,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.6.0,lower bound
v1.6.0,upper bound
v1.6.0,normalize input
v1.6.0,input validation
v1.6.0,base case
v1.6.0,normalize dim
v1.6.0,calculate repeats for each tensor
v1.6.0,dimensions along concatenation axis do not need to match
v1.6.0,get desired extent along dimension
v1.6.0,repeat tensors along axes if necessary
v1.6.0,concatenate
v1.6.0,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.6.0,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.6.0,Determine optimal order and cost
v1.6.0,translate back to original order
v1.6.0,determine optimal processing order
v1.6.0,heuristic
v1.6.0,workaround for complex numbers: manually compute norm
v1.6.0,TODO: check if einsum is still very slow.
v1.6.0,TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation
v1.6.0,unsqueeze
v1.6.0,The dimensions affected by e'
v1.6.0,Project entities
v1.6.0,r_p (e_p.T e) + e'
v1.6.0,Enforce constraints
v1.6.0,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.6.0,Create a tensor of all IDs
v1.6.0,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.6.0,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.6.0,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.6.0,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.6.0,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.6.0,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.6.0,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Base Class
v1.6.0,Child classes
v1.6.0,Utils
v1.6.0,: The overall regularization weight
v1.6.0,: The current regularization term (a scalar)
v1.6.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.6.0,: Has this regularizer been updated since last being reset?
v1.6.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.6.0,"If there are tracked parameters, update based on them"
v1.6.0,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.6.0,no need to compute anything
v1.6.0,always return zero
v1.6.0,: The dimension along which to compute the vector-based regularization terms.
v1.6.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.6.0,: This allows dimensionality-independent weight tuning.
v1.6.0,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.6.0,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.6.0,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.6.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.6.0,"Therefore, apply_only_once is always set to True."
v1.6.0,Entity soft constraint
v1.6.0,Orthogonality soft constraint
v1.6.0,The normalization factor to balance individual regularizers' contribution.
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Add HPO command
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,General types
v1.6.0,Triples
v1.6.0,Others
v1.6.0,Tensor Functions
v1.6.0,Tensors
v1.6.0,Dataclasses
v1.6.0,: A function that mutates the input and returns a new object of the same type as output
v1.6.0,: A function that can be applied to a tensor to initialize it
v1.6.0,: A function that can be applied to a tensor to normalize it
v1.6.0,: A function that can be applied to a tensor to constrain it
v1.6.0,: A hint for a :class:`torch.device`
v1.6.0,: A hint for a :class:`torch.Generator`
v1.6.0,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.6.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.6.0,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.6.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.6.0,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.6.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,pad with zeros
v1.6.0,trim
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,"mask, shape: (num_edges,)"
v1.6.0,bi-directional message passing
v1.6.0,Heuristic for default value
v1.6.0,weights
v1.6.0,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.6.0,initialized properly)
v1.6.0,We have one additional relation for self-loops
v1.6.0,other relations
v1.6.0,other relations
v1.6.0,Select source and target indices as well as edge weights for the
v1.6.0,currently considered relation
v1.6.0,skip relations without edges
v1.6.0,"compute message, shape: (num_edges_of_type, output_dim)"
v1.6.0,since we may have one node ID appearing multiple times as source
v1.6.0,"ID, we can save some computation by first reducing to the unique"
v1.6.0,"source IDs, compute transformed representations and afterwards"
v1.6.0,select these representations for the correct edges.
v1.6.0,select unique source node representations
v1.6.0,transform representations by relation specific weight
v1.6.0,select the uniquely transformed representations for each edge
v1.6.0,optional message weighting
v1.6.0,message aggregation
v1.6.0,self-loops first
v1.6.0,the last relation_id refers to the self-loop
v1.6.0,Xavier Glorot initialization of each block
v1.6.0,view as blocks
v1.6.0,self-loop first
v1.6.0,other relations
v1.6.0,skip relations without edges
v1.6.0,"compute message, shape: (num_edges_of_type, num_blocks, block_size)"
v1.6.0,optional message weighting
v1.6.0,message aggregation
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: the maximum ID (exclusively)
v1.6.0,: the shape of an individual representation
v1.6.0,TODO: Remove this property and update code to use shape instead
v1.6.0,normalize embedding_dim vs. shape
v1.6.0,work-around until full complex support
v1.6.0,TODO: verify that this is our understanding of complex!
v1.6.0,"wrapper around max_id, for backward compatibility"
v1.6.0,initialize weights in-place
v1.6.0,apply constraints in-place
v1.6.0,verify that contiguity is preserved
v1.6.0,TODO: move normalizer / regularizer to base class?
v1.6.0,freeze
v1.6.0,use this instead of a lambda to make sure that it can be pickled
v1.6.0,TODO add normalization functions
v1.6.0,Resolve edge weighting
v1.6.0,dropout
v1.6.0,batch norm and bias
v1.6.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.6.0,buffering of enriched representations
v1.6.0,invalidate enriched embeddings
v1.6.0,Bind fields
v1.6.0,"shape: (num_entities, embedding_dim)"
v1.6.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.6.0,Get random dropout mask
v1.6.0,Apply to edges
v1.6.0,Different dropout for self-loops (only in training mode)
v1.6.0,fixed edges -> pre-compute weights
v1.6.0,Cache enriched representations
v1.6.0,normalize output dimension
v1.6.0,entity-relation composition
v1.6.0,edge weighting
v1.6.0,message passing weights
v1.6.0,linear relation transformation
v1.6.0,layer-specific self-loop relation representation
v1.6.0,other components
v1.6.0,initialize
v1.6.0,split
v1.6.0,compose
v1.6.0,transform
v1.6.0,normalization
v1.6.0,aggregate by sum
v1.6.0,dropout
v1.6.0,prepare for inverse relations
v1.6.0,update entity representations: mean over self-loops / forward edges / backward edges
v1.6.0,Relation transformation
v1.6.0,Buffered enriched entity and relation representations
v1.6.0,TODO: Check
v1.6.0,hidden dimension normalization
v1.6.0,Create message passing layers
v1.6.0,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.6.0,TODO: This always uses all training triples for message passing
v1.6.0,initialize buffer of enriched representations
v1.6.0,invalidate enriched embeddings
v1.6.0,"when changing from evaluation to training mode, the buffered representations have been computed without"
v1.6.0,"gradient tracking. hence, we need to invalidate them."
v1.6.0,note: this occurs in practice when continuing training after evaluation.
v1.6.0,enrich
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,scaling factor
v1.6.0,"modulus ~ Uniform[-s, s]"
v1.6.0,"phase ~ Uniform[0, 2*pi]"
v1.6.0,real part
v1.6.0,purely imaginary quaternions unitary
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,"Calculate in-degree, i.e. number of incoming edges"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,TODO test
v1.6.0,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.6.0,: a = \mu^T\Sigma^{-1}\mu
v1.6.0,: b = \log \det \Sigma
v1.6.0,1. Component
v1.6.0,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.6.0,2. Component
v1.6.0,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.6.0,with mu = (mu_1 - mu_0)
v1.6.0,= mu * Sigma_1^-1 mu
v1.6.0,since Sigma_1 is diagonal
v1.6.0,= mu**2 / sigma_1
v1.6.0,3. Component
v1.6.0,4. Component
v1.6.0,ln (det(\Sigma_1) / det(\Sigma_0))
v1.6.0,= ln det Sigma_1 - ln det Sigma_0
v1.6.0,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.6.0,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.6.0,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.6.0,allocate result
v1.6.0,prepare distributions
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,TODO benchmark
v1.6.0,TODO benchmark
v1.6.0,TODO benchmark
v1.6.0,TODO benchmark
v1.6.0,TODO benchmark
v1.6.0,TODO benchmark
v1.6.0,TODO benchmark
v1.6.0,TODO benchmark
v1.6.0,TODO benchmark
v1.6.0,"h = h_re, -h_im"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Base Classes
v1.6.0,Adapter classes
v1.6.0,Concrete Classes
v1.6.0,: The symbolic shapes for entity representations
v1.6.0,": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE."
v1.6.0,: The symbolic shapes for relation representations
v1.6.0,"bring to (b, n, *)"
v1.6.0,"bring to (b, h, r, t, *)"
v1.6.0,unpack singleton
v1.6.0,"The appended ""e"" represents the literals that get concatenated"
v1.6.0,on the entity representations. It does not necessarily have the
v1.6.0,"same dimension ""d"" as the entity representations."
v1.6.0,alternate way of combining entity embeddings + literals
v1.6.0,"h = torch.cat(h, dim=-1)"
v1.6.0,"h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore"
v1.6.0,"t = torch.cat(t, dim=-1)"
v1.6.0,"t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore"
v1.6.0,: The functional interaction form
v1.6.0,Store initial input for error message
v1.6.0,All are None -> try and make closest to square
v1.6.0,Only input channels is None
v1.6.0,Only width is None
v1.6.0,Only height is none
v1.6.0,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.6.0,Width and input channels are None -> set input channels to 1 and calculate width
v1.6.0,": The head-relation encoder operating on 2D ""images"""
v1.6.0,: The head-relation encoder operating on the 1D flattened version
v1.6.0,: The interaction function
v1.6.0,Automatic calculation of remaining dimensions
v1.6.0,Parameter need to fulfil:
v1.6.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.6.0,encoders
v1.6.0,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.6.0,"2: 1D encoder: FC, DO, BN?, Act"
v1.6.0,store reshaping dimensions
v1.6.0,The interaction model
v1.6.0,Use Xavier initialization for weight; bias to zero
v1.6.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.6.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.6.0,Initialize biases with zero
v1.6.0,"In the original formulation,"
v1.6.0,Global entity projection
v1.6.0,Global relation projection
v1.6.0,Global combination bias
v1.6.0,Global combination bias
v1.6.0,Core tensor
v1.6.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.6.0,Dropout
v1.6.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.6.0,"batch norm gets reset automatically, since it defines reset_parameters"
v1.6.0,shapes
v1.6.0,there are separate biases for entities in head and tail position
v1.6.0,the base interaction
v1.6.0,forward entity/relation shapes
v1.6.0,The parameters of the affine transformation: bias
v1.6.0,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,"repeat if necessary, and concat head and relation, batch_size', num_input_channels, 2*height, width"
v1.6.0,with batch_size' = batch_size * num_heads * num_relations
v1.6.0,"batch_size', num_input_channels, 2*height, width"
v1.6.0,"batch_size', num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.6.0,"reshape: (batch_size', embedding_dim) -> (b, h, r, 1, d)"
v1.6.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.6.0,"output_shape: (batch_size, num_heads, num_relations, num_tails)"
v1.6.0,add bias term
v1.6.0,decompose convolution for faster computation in 1-n case
v1.6.0,"compute conv(stack(h, r, t))"
v1.6.0,prepare input shapes for broadcasting
v1.6.0,"(b, h, r, t, 1, d)"
v1.6.0,"conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])"
v1.6.0,"here, kernel_size = (1, 3), C_in = 1, C_out = num_filters"
v1.6.0,"-> conv_head, conv_rel, conv_tail shapes: (num_filters,)"
v1.6.0,"reshape to (1, 1, 1, 1, f, 1)"
v1.6.0,"convolve -> output.shape: (*, embedding_dim, num_filters)"
v1.6.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.6.0,"Linear layer for final scores; use flattened representations, shape: (b, h, r, t, d * f)"
v1.6.0,same shape
v1.6.0,"split, shape: (embedding_dim, hidden_dim)"
v1.6.0,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.6.0,"Predict t embedding, shape: (b, h, r, 1, d)"
v1.6.0,"transpose t, (b, 1, 1, d, t)"
v1.6.0,"dot product, (b, h, r, 1, t)"
v1.6.0,"composite: (b, h, 1, t, d)"
v1.6.0,"transpose composite: (b, h, 1, d, t)"
v1.6.0,inner product with relation embedding
v1.6.0,Circular correlation of entity embeddings
v1.6.0,complex conjugate
v1.6.0,Hadamard product in frequency domain
v1.6.0,inverse real FFT
v1.6.0,global projections
v1.6.0,"combination, shape: (b, h, r, 1, d)"
v1.6.0,"dot product with t, shape: (b, h, r, t)"
v1.6.0,r expresses a rotation in complex plane.
v1.6.0,rotate head by relation (=Hadamard product in complex space)
v1.6.0,rotate tail by inverse of relation
v1.6.0,The inverse rotation is expressed by the complex conjugate of r.
v1.6.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.6.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.6.0,|h * r - t| = |h - conj(r) * t|
v1.6.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.6.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.6.0,"That is not mentioned in the paper, so it is made optional here."
v1.6.0,Project entities
v1.6.0,h projection to hyperplane
v1.6.0,r
v1.6.0,-t projection to hyperplane
v1.6.0,project to relation specific subspace and ensure constraints
v1.6.0,x_3 contraction
v1.6.0,x_1 contraction
v1.6.0,x_2 contraction
v1.6.0,Rotate (=Hamilton product in quaternion space).
v1.6.0,Rotation in quaternion space
v1.6.0,head interaction
v1.6.0,relation interaction (notice that h has been updated)
v1.6.0,combination
v1.6.0,similarity
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Concrete classes
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,don't worry about functions because they can't be specified by JSON.
v1.6.0,Could make a better mo
v1.6.0,later could extend for other non-JSON valid types
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Score with original triples
v1.6.0,Score with inverse triples
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Create directory in which all experimental artifacts are saved
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,distribute the deteriorated triples across the remaining factories
v1.6.0,"'kinships',"
v1.6.0,"'umls',"
v1.6.0,"'codexsmall',"
v1.6.0,"'wn18',"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: Functions for specifying exotic resources with a given prefix
v1.6.0,: Functions for specifying exotic resources based on their file extension
v1.6.0,Input validation
v1.6.0,convert to numpy
v1.6.0,Additional columns
v1.6.0,convert PyTorch tensors to numpy
v1.6.0,convert to dataframe
v1.6.0,Re-order columns
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.6.0,TODO vectorize code
v1.6.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Split triples
v1.6.0,Sorting ensures consistent results when the triples are permuted
v1.6.0,Create mapping
v1.6.0,Sorting ensures consistent results when the triples are permuted
v1.6.0,Create mapping
v1.6.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.6.0,Filter all non-existent triples
v1.6.0,Note: Unique changes the order of the triples
v1.6.0,Note: Using unique means implicit balancing of training samples
v1.6.0,normalize input
v1.6.0,: The mapping from labels to IDs.
v1.6.0,: The inverse mapping for label_to_id; initialized automatically
v1.6.0,: A vectorized version of entity_label_to_id; initialized automatically
v1.6.0,: A vectorized version of entity_id_to_label; initialized automatically
v1.6.0,Normalize input
v1.6.0,label
v1.6.0,check new label to ID mappings
v1.6.0,Make new triples factories for each group
v1.6.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.6.0,Filter for entities
v1.6.0,Filter for relations
v1.6.0,No filtering happened
v1.6.0,Check if the triples are inverted already
v1.6.0,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.6.0,contained if and only if create_inverse_triples is True.
v1.6.0,Generate entity mapping if necessary
v1.6.0,Generate relation mapping if necessary
v1.6.0,Map triples of labels to triples of IDs.
v1.6.0,TODO: Check if lazy evaluation would make sense
v1.6.0,pre-filter to keep only topk
v1.6.0,if top is larger than the number of available options
v1.6.0,generate text
v1.6.0,vectorized label lookup
v1.6.0,Re-order columns
v1.6.0,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.6.0,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Split indices
v1.6.0,Split triples
v1.6.0,select one triple per relation
v1.6.0,maintain set of covered entities
v1.6.0,"Select one triple for each head/tail entity, which is not yet covered."
v1.6.0,create mask
v1.6.0,Prepare split index
v1.6.0,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.6.0,[...] is necessary for Python 3.7 compatibility
v1.6.0,While there are still triples that should be moved to the training set
v1.6.0,Pick a random triple to move over to the training triples
v1.6.0,add to training
v1.6.0,remove from testing
v1.6.0,Recalculate the move_id_mask
v1.6.0,base cases
v1.6.0,IDs not in training
v1.6.0,triples with exclusive test IDs
v1.6.0,Make sure that the first element has all the right stuff in it
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,constants
v1.6.0,constants
v1.6.0,unary
v1.6.0,binary
v1.6.0,ternary
v1.6.0,column names
v1.6.0,return candidates
v1.6.0,index triples
v1.6.0,incoming relations per entity
v1.6.0,outgoing relations per entity
v1.6.0,indexing triples for fast join r1 & r2
v1.6.0,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.6.0,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.6.0,actual evaluation of the pattern
v1.6.0,skip empty support
v1.6.0,TODO: Can this happen after pre-filtering?
v1.6.0,"sort first, for triple order invariance"
v1.6.0,TODO: what is the support?
v1.6.0,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.6.0,sort decreasingly. i dominates j for all j > i in x-dimension
v1.6.0,"if it is also dominated by any y, it is not part of the skyline"
v1.6.0,"group by (relation id, pattern type)"
v1.6.0,"for each group, yield from skyline"
v1.6.0,determine patterns from triples
v1.6.0,drop zero-confidence
v1.6.0,keep only skyline
v1.6.0,create data frame
v1.6.0,iterate relation types
v1.6.0,drop zero-confidence
v1.6.0,keep only skyline
v1.6.0,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.6.0,base = skyline(base)
v1.6.0,create data frame
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,convert to csr for fast row slicing
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,check validity
v1.6.0,path compression
v1.6.0,collect connected components using union find with path compression
v1.6.0,get representatives
v1.6.0,already merged
v1.6.0,make x the smaller one
v1.6.0,merge
v1.6.0,extract partitions
v1.6.0,safe division for empty sets
v1.6.0,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.6.0,duplicates
v1.6.0,we are not interested in self-similarity
v1.6.0,compute similarities
v1.6.0,Calculate which relations are the inverse ones
v1.6.0,get existing IDs
v1.6.0,remove non-existing ID from label mapping
v1.6.0,create translation tensor
v1.6.0,get entities and relations occurring in triples
v1.6.0,generate ID translation and new label to Id mappings
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,preprocessing
v1.6.0,initialize
v1.6.0,sample iteratively
v1.6.0,determine weights
v1.6.0,only happens at first iteration
v1.6.0,normalize to probabilities
v1.6.0,sample a start node
v1.6.0,get list of neighbors
v1.6.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.6.0,visit target node
v1.6.0,decrease sample counts
v1.6.0,return chosen edges
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.6.0,seamless loading and saving of training checkpoints
v1.6.0,Create training instances. Use the _create_instances function to allow subclasses
v1.6.0,to modify this behavior
v1.6.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.6.0,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.6.0,"If a checkpoint file is given, it must be loaded if it exists already"
v1.6.0,"If the stopper dict has any keys, those are written back to the stopper"
v1.6.0,The checkpoint frequency needs to be set to save checkpoints
v1.6.0,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.6.0,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.6.0,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.6.0,Ensure the release of memory
v1.6.0,Clear optimizer
v1.6.0,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.6.0,due to the patience continue to train after the best epoch and thus alter the model
v1.6.0,Create a path
v1.6.0,Prepare all of the callbacks
v1.6.0,"Register a callback for the result tracker, if given"
v1.6.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.6.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.6.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.6.0,return the relevant parameters slice_size and batch_size
v1.6.0,Force weight initialization if training continuation is not explicitly requested.
v1.6.0,Reset the weights
v1.6.0,Create new optimizer
v1.6.0,Create a new lr scheduler and add the optimizer
v1.6.0,Ensure the model is on the correct device
v1.6.0,Create Sampler
v1.6.0,Bind
v1.6.0,"When size probing, we don't want progress bars"
v1.6.0,Create progress bar
v1.6.0,Save the time to track when the saved point was available
v1.6.0,Training Loop
v1.6.0,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.6.0,Enforce training mode
v1.6.0,Accumulate loss over epoch
v1.6.0,Batching
v1.6.0,Only create a progress bar when not in size probing mode
v1.6.0,Flag to check when to quit the size probing
v1.6.0,Recall that torch *accumulates* gradients. Before passing in a
v1.6.0,"new instance, you need to zero out the gradients from the old instance"
v1.6.0,Get batch size of current batch (last batch may be incomplete)
v1.6.0,accumulate gradients for whole batch
v1.6.0,forward pass call
v1.6.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.6.0,update parameters according to optimizer
v1.6.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.6.0,constraints are no longer applied
v1.6.0,For testing purposes we're only interested in processing one batch
v1.6.0,When size probing we don't need the losses
v1.6.0,Update learning rate scheduler
v1.6.0,Track epoch loss
v1.6.0,Print loss information to console
v1.6.0,Save the last successful finished epoch
v1.6.0,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.6.0,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.6.0,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.6.0,During automatic memory optimization only the error message is of interest
v1.6.0,When there wasn't a best epoch the checkpoint path should be None
v1.6.0,Delete temporary best epoch model
v1.6.0,Includes a call to result_tracker.log_metrics
v1.6.0,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.6.0,MyPy overrides are because you should
v1.6.0,When there wasn't a best epoch the checkpoint path should be None
v1.6.0,Delete temporary best epoch model
v1.6.0,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.6.0,at that state
v1.6.0,Delete temporary best epoch model
v1.6.0,forward pass
v1.6.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.6.0,correction for loss reduction
v1.6.0,backward pass
v1.6.0,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.6.0,Set upper bound
v1.6.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.6.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.6.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.6.0,"Only if a cuda device is available, the random state is accessed"
v1.6.0,This is an entire checkpoint for the optional best model when using early stopping
v1.6.0,Saving triples factory related states
v1.6.0,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.6.0,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.6.0,Check whether the triples factory mappings match those from the checkpoints
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Shuffle each epoch
v1.6.0,Lazy-splitting into batches
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Slicing is not possible in sLCWA training loops
v1.6.0,Send positive batch to device
v1.6.0,"Create negative samples, shape: (batch_size, num_neg_per_pos, 3)"
v1.6.0,apply filter mask
v1.6.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.6.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.6.0,Compute negative and positive scores
v1.6.0,Slicing is not possible for sLCWA
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: A hint for constructing a :class:`MultiTrainingCallback`
v1.6.0,: A collection of callbacks
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,normalize target column
v1.6.0,The type inference is so confusing between the function switching
v1.6.0,and polymorphism introduced by slicability that these need to be ignored
v1.6.0,Split batch components
v1.6.0,Send batch to device
v1.6.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.6.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,To make MyPy happy
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,now: smaller is better
v1.6.0,: The model
v1.6.0,: The evaluator
v1.6.0,: The triples to use for training (to be used during filtered evaluation)
v1.6.0,: The triples to use for evaluation
v1.6.0,: Size of the evaluation batches
v1.6.0,: Slice size of the evaluation batches
v1.6.0,: The number of epochs after which the model is evaluated on validation set
v1.6.0,: The number of iterations (one iteration can correspond to various epochs)
v1.6.0,: with no improvement after which training will be stopped.
v1.6.0,: The name of the metric to use
v1.6.0,: The minimum relative improvement necessary to consider it an improved result
v1.6.0,: The best result so far
v1.6.0,: The epoch at which the best result occurred
v1.6.0,: The remaining patience
v1.6.0,: The metric results from all evaluations
v1.6.0,": Whether a larger value is better, or a smaller"
v1.6.0,: The result tracker
v1.6.0,: Callbacks when after results are calculated
v1.6.0,: Callbacks when training gets continued
v1.6.0,: Callbacks when training is stopped early
v1.6.0,: Did the stopper ever decide to stop?
v1.6.0,TODO: Fix this
v1.6.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.6.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.6.0,Evaluate
v1.6.0,Only perform time consuming checks for the first call.
v1.6.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.6.0,Append to history
v1.6.0,check for improvement
v1.6.0,Stop if the result did not improve more than delta for patience evaluations
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Utils
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.6.0,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.6.0,Clear the ranks from the current evaluator
v1.6.0,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.6.0,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.6.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.6.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.6.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.6.0,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.6.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.6.0,values_dict[key] will always be an int at this point
v1.6.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.6.0,Test if slicing is implemented for the required functions of this model
v1.6.0,Split batch
v1.6.0,Bind shape
v1.6.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.6.0,Warn if all entities will be filtered
v1.6.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.6.0,verify that the triples have been filtered
v1.6.0,Send to device
v1.6.0,Ensure evaluation mode
v1.6.0,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.6.0,Check whether we need to be prepared for filtering
v1.6.0,Check whether an evaluator needs access to the masks
v1.6.0,This can only be an unfiltered evaluator.
v1.6.0,Prepare for result filtering
v1.6.0,Send tensors to device
v1.6.0,Prepare batches
v1.6.0,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.6.0,Show progressbar
v1.6.0,Flag to check when to quit the size probing
v1.6.0,Disable gradient tracking
v1.6.0,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.6.0,batch-wise processing
v1.6.0,If we only probe sizes we do not need more than one batch
v1.6.0,Finalize
v1.6.0,Predict scores once
v1.6.0,Select scores of true
v1.6.0,Create positive filter for all corrupted
v1.6.0,Needs all positive triples
v1.6.0,Create filter
v1.6.0,Create a positive mask with the size of the scores from the positive filter
v1.6.0,Restrict to entities of interest
v1.6.0,Evaluate metrics on these *unfiltered* scores
v1.6.0,Filter
v1.6.0,The scores for the true triples have to be rewritten to the scores tensor
v1.6.0,Restrict to entities of interest
v1.6.0,Evaluate metrics on these *filtered* scores
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The area under the ROC curve
v1.6.0,: The area under the precision-recall curve
v1.6.0,: The coverage error
v1.6.0,coverage_error: float = field(metadata=dict(
v1.6.0,"doc='The coverage error',"
v1.6.0,"f=metrics.coverage_error,"
v1.6.0,))
v1.6.0,: The label ranking loss (APS)
v1.6.0,label_ranking_average_precision_score: float = field(metadata=dict(
v1.6.0,"doc='The label ranking loss (APS)',"
v1.6.0,"f=metrics.label_ranking_average_precision_score,"
v1.6.0,))
v1.6.0,#: The label ranking loss
v1.6.0,label_ranking_loss: float = field(metadata=dict(
v1.6.0,"doc='The label ranking loss',"
v1.6.0,"f=metrics.label_ranking_loss,"
v1.6.0,))
v1.6.0,Transfer to cpu and convert to numpy
v1.6.0,Ensure that each key gets counted only once
v1.6.0,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.6.0,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.6.0,masks using the exact same key order.
v1.6.0,TODO how to define a cutoff on y_scores to make binary?
v1.6.0,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.6.0,Clear buffers
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Extra stats stuff
v1.6.0,The optimistic rank is the rank when assuming all options with an equal score are placed behind the currently
v1.6.0,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.6.0,The pessimistic rank is the rank when assuming all options with an equal score are placed in front of the
v1.6.0,"currently considered. Hence, the rank is the number of options which have at least the same score minus one"
v1.6.0,"(as the currently considered option in included in all options). As the rank is one-based, we have to add 1,"
v1.6.0,"which nullifies the ""minus 1"" from before."
v1.6.0,"The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank over"
v1.6.0,all permutations of the elements with the same score as the currently considered option.
v1.6.0,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.6.0,The expected rank of a random scoring
v1.6.0,Check if it a side or rank type
v1.6.0,update old names for metrics and handle spaces
v1.6.0,"otherwise, assume is hits@k, which is handled differently"
v1.6.0,Adjusted mean rank calculation
v1.6.0,Clear buffers
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.6.0,to create the class
v1.6.0,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default regularizer class
v1.6.0,: The default parameters for the default regularizer class
v1.6.0,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.6.0,Make sure that all modules with parameters do have a reset_parameters method.
v1.6.0,Recursively visit all sub-modules
v1.6.0,skip self
v1.6.0,Track parents for blaming
v1.6.0,call reset_parameters if possible
v1.6.0,initialize from bottom to top
v1.6.0,This ensures that specialized initializations will take priority over the default ones of its components.
v1.6.0,emit warning if there where parameters which were not initialised by reset_parameters.
v1.6.0,Additional debug information
v1.6.0,Important: use ModuleList to ensure that Pytorch correctly handles their devices and parameters
v1.6.0,: The entity representations
v1.6.0,: The relation representations
v1.6.0,: The weight regularizers
v1.6.0,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.6.0,"model.modules(). Thereby, we can collect them automatically."
v1.6.0,Explicitly call reset_parameters to trigger initialization
v1.6.0,normalize input
v1.6.0,normalization
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Train a model (quickly)
v1.6.0,Get scores for *all* triples
v1.6.0,Get scores for top 15 triples
v1.6.0,initialize buffer on cpu
v1.6.0,calculate batch scores
v1.6.0,Explicitly create triples
v1.6.0,initialize buffer on device
v1.6.0,calculate batch scores
v1.6.0,get top scores within batch
v1.6.0,append to global top scores
v1.6.0,reduce size if necessary
v1.6.0,base case: infer maximum batch size
v1.6.0,base case: single batch
v1.6.0,TODO: this could happen because of AMO
v1.6.0,TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043
v1.6.0,no OOM error.
v1.6.0,make sure triples are a numpy array
v1.6.0,make sure triples are 2d
v1.6.0,convert to ID-based
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The device on which this model and its submodules are stored
v1.6.0,: The default loss function class
v1.6.0,: The default parameters for the default loss function class
v1.6.0,: The instance of the loss
v1.6.0,Initialize the device
v1.6.0,Random seeds have to set before the embeddings are initialized
v1.6.0,Loss
v1.6.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.6.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.6.0,Id of inverse relation: relation + 1
v1.6.0,: The default regularizer class
v1.6.0,: The default parameters for the default regularizer class
v1.6.0,: The instance of the regularizer
v1.6.0,Regularizer
v1.6.0,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.6.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.6.0,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.6.0,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.6.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.6.0,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.6.0,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.6.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.6.0,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.6.0,: Primary embeddings for entities
v1.6.0,: Primary embeddings for relations
v1.6.0,"make sure to call this first, to reset regularizer state!"
v1.6.0,The following lines add in a post-init hook to all subclasses
v1.6.0,such that the reset_parameters_() function is run
v1.6.0,"sorry mypy, but this kind of evil must be permitted."
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Base Models
v1.6.0,Concrete Models
v1.6.0,Evaluation-only models
v1.6.0,Utils
v1.6.0,Abstract Models
v1.6.0,We might be able to relax this later
v1.6.0,baseline models behave differently
v1.6.0,Old style models should never be looked up
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,TODO rethink after RGCN update
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default loss function class
v1.6.0,: The default parameters for the default loss function class
v1.6.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.6.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.6.0,ConvE should be trained with inverse triples
v1.6.0,ConvE uses one bias for each entity
v1.6.0,Automatic calculation of remaining dimensions
v1.6.0,Parameter need to fulfil:
v1.6.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.6.0,weights
v1.6.0,"batch_size, num_input_channels, 2*height, width"
v1.6.0,"batch_size, num_input_channels, 2*height, width"
v1.6.0,"batch_size, num_input_channels, 2*height, width"
v1.6.0,"(N,C_out,H_out,W_out)"
v1.6.0,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.6.0,Embedding Regularization
v1.6.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.6.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.6.0,Embedding Regularization
v1.6.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.6.0,Embedding Regularization
v1.6.0,Code to repeat each item successively instead of the entire tensor
v1.6.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default loss function class
v1.6.0,: The default parameters for the default loss function class
v1.6.0,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.6.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.6.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.6.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.6.0,split into real and imaginary part
v1.6.0,ComplEx space bilinear product
v1.6.0,*: Elementwise multiplication
v1.6.0,get embeddings
v1.6.0,Regularization
v1.6.0,Compute scores
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.6.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.6.0,: a normalized weight of 10 is used.
v1.6.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.6.0,Get embeddings
v1.6.0,"shape: (b, d)"
v1.6.0,"shape: (b, d, d)"
v1.6.0,"shape: (b, d)"
v1.6.0,Compute scores
v1.6.0,Regularization
v1.6.0,Compute scores
v1.6.0,Regularization
v1.6.0,Get embeddings
v1.6.0,Compute scores
v1.6.0,Regularization
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.6.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.6.0,The interaction model
v1.6.0,embeddings
v1.6.0,Use Xavier initialization for weight; bias to zero
v1.6.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.6.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.6.0,Output layer regularization
v1.6.0,In the code base only the weights of the output layer are used for regularization
v1.6.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.6.0,Stack to convolution input
v1.6.0,Convolution
v1.6.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.6.0,Linear layer for final scores
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,comment:
v1.6.0,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.6.0,uses float64
v1.6.0,entity bias for head
v1.6.0,entity bias for tail
v1.6.0,relation offset
v1.6.0,diagonal relation transformation matrix
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default entity normalizer parameters
v1.6.0,: The entity representations are normalized to L2 unit length
v1.6.0,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.6.0,"update initializer settings, cf."
v1.6.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.6.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.6.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.6.0,in the original implementation the embeddings are initialized in one parameter
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,"w: (k, d, d)"
v1.6.0,"vh: (k, d)"
v1.6.0,"vt: (k, d)"
v1.6.0,"b: (k,)"
v1.6.0,"u: (k,)"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The regularizer used by [yang2014]_ for DistMult
v1.6.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.6.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.6.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.6.0,: The LP settings used by [yang2014]_ for DistMult
v1.6.0,Bilinear product
v1.6.0,*: Elementwise multiplication
v1.6.0,Get embeddings
v1.6.0,Compute score
v1.6.0,Only regularize relation embeddings
v1.6.0,Get embeddings
v1.6.0,Rank against all entities
v1.6.0,Only regularize relation embeddings
v1.6.0,Get embeddings
v1.6.0,Rank against all entities
v1.6.0,Only regularize relation embeddings
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default settings for the entity constrainer
v1.6.0,Similarity function used for distributions
v1.6.0,element-wise covariance bounds
v1.6.0,Additional covariance embeddings
v1.6.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.6.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.6.0,Constraints are applied through post_parameter_update
v1.6.0,Get embeddings
v1.6.0,Compute entity distribution
v1.6.0,: a = \mu^T\Sigma^{-1}\mu
v1.6.0,: b = \log \det \Sigma
v1.6.0,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.6.0,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.6.0,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.6.0,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The custom regularizer used by [wang2014]_ for TransH
v1.6.0,: The settings used by [wang2014]_ for TransH
v1.6.0,embeddings
v1.6.0,Normalise the normal vectors by their l2 norms
v1.6.0,TODO: Add initialization
v1.6.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.6.0,which enforces the defined soft constraints.
v1.6.0,Get embeddings
v1.6.0,Project to hyperplane
v1.6.0,Regularization term
v1.6.0,Get embeddings
v1.6.0,Project to hyperplane
v1.6.0,Regularization term
v1.6.0,Get embeddings
v1.6.0,Project to hyperplane
v1.6.0,Regularization term
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,TODO: Initialize from TransE
v1.6.0,embeddings
v1.6.0,"project to relation specific subspace, shape: (b, e, d_r)"
v1.6.0,ensure constraints
v1.6.0,"evaluate score function, shape: (b, e)"
v1.6.0,Get embeddings
v1.6.0,Get embeddings
v1.6.0,Get embeddings
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,": The default strategy for optimizing the model""s hyper-parameters"
v1.6.0,TODO: Decomposition kwargs
v1.6.0,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.6.0,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.6.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.6.0,create enriched entity representations
v1.6.0,Resolve interaction function
v1.6.0,set default relation representation
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,combined representation
v1.6.0,Resolve interaction function
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default loss function class
v1.6.0,: The default parameters for the default loss function class
v1.6.0,Core tensor
v1.6.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.6.0,Dropout
v1.6.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.6.0,Abbreviation
v1.6.0,Compute h_n = DO(BN(h))
v1.6.0,Compute wr = DO(W x_2 r)
v1.6.0,compute whr = DO(BN(h_n x_1 wr))
v1.6.0,Compute whr x_3 t
v1.6.0,Get embeddings
v1.6.0,Compute scores
v1.6.0,Get embeddings
v1.6.0,Compute scores
v1.6.0,Get embeddings
v1.6.0,Compute scores
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,Get embeddings
v1.6.0,TODO: Use torch.cdist
v1.6.0,"There were some performance/memory issues with cdist, cf."
v1.6.0,"https://github.com/pytorch/pytorch/issues?q=cdist however, @mberr thinks"
v1.6.0,they are mostly resolved by now. A Benefit would be that we can harness the
v1.6.0,"future (performance) improvements made by the core torch developers. However,"
v1.6.0,this will require some benchmarking.
v1.6.0,Get embeddings
v1.6.0,TODO: Use torch.cdist (see note above in score_hrt())
v1.6.0,Get embeddings
v1.6.0,TODO: Use torch.cdist (see note above in score_hrt())
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default loss function class
v1.6.0,: The default parameters for the default loss function class
v1.6.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.6.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.6.0,": regularization term by the number of elements, which is 200."
v1.6.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.6.0,extra embeddings
v1.6.0,forward model
v1.6.0,Regularization
v1.6.0,backward model
v1.6.0,Regularization
v1.6.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.6.0,"That is not mentioned in the paper, so it is omitted here."
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.6.0,weight initialization
v1.6.0,Get embeddings
v1.6.0,Embedding Regularization
v1.6.0,Concatenate them
v1.6.0,Compute scores
v1.6.0,Get embeddings
v1.6.0,Embedding Regularization
v1.6.0,First layer can be unrolled
v1.6.0,Send scores through rest of the network
v1.6.0,Get embeddings
v1.6.0,Embedding Regularization
v1.6.0,First layer can be unrolled
v1.6.0,Send scores through rest of the network
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,The dimensions affected by e'
v1.6.0,Project entities
v1.6.0,r_p (e_p.T e) + e'
v1.6.0,Enforce constraints
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: Secondary embeddings for entities
v1.6.0,: Secondary embeddings for relations
v1.6.0,Project entities
v1.6.0,score = -||h_bot + r - t_bot||_2^2
v1.6.0,Head
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,Regular relation embeddings
v1.6.0,The relation-specific interaction vector
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,Decompose into real and imaginary part
v1.6.0,Rotate (=Hadamard product in complex space).
v1.6.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.6.0,Get embeddings
v1.6.0,Compute scores
v1.6.0,Embedding Regularization
v1.6.0,Get embeddings
v1.6.0,Rank against all entities
v1.6.0,Compute scores
v1.6.0,Embedding Regularization
v1.6.0,Get embeddings
v1.6.0,r expresses a rotation in complex plane.
v1.6.0,The inverse rotation is expressed by the complex conjugate of r.
v1.6.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.6.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.6.0,|h * r - t| = |h - conj(r) * t|
v1.6.0,Rank against all entities
v1.6.0,Compute scores
v1.6.0,Embedding Regularization
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default loss function class
v1.6.0,: The default parameters for the default loss function class
v1.6.0,Global entity projection
v1.6.0,Global relation projection
v1.6.0,Global combination bias
v1.6.0,Global combination bias
v1.6.0,Get embeddings
v1.6.0,Compute score
v1.6.0,Get embeddings
v1.6.0,Rank against all entities
v1.6.0,Get embeddings
v1.6.0,Rank against all entities
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Normalize relation embeddings
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default loss function class
v1.6.0,: The default parameters for the default loss function class
v1.6.0,: The LP settings used by [zhang2019]_ for QuatE.
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default settings for the entity constrainer
v1.6.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.6.0,Circular correlation of entity embeddings
v1.6.0,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.6.0,compatibility: new style fft returns complex tensor
v1.6.0,Hadamard product in frequency domain
v1.6.0,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.6.0,inner product with relation embedding
v1.6.0,Embedding Regularization
v1.6.0,Embedding Regularization
v1.6.0,Embedding Regularization
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default loss function class
v1.6.0,: The default parameters for the default loss function class
v1.6.0,Get embeddings
v1.6.0,Embedding Regularization
v1.6.0,Concatenate them
v1.6.0,Predict t embedding
v1.6.0,compare with all t's
v1.6.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.6.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.6.0,Embedding Regularization
v1.6.0,Concatenate them
v1.6.0,Predict t embedding
v1.6.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.6.0,Embedding Regularization
v1.6.0,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.6.0,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.6.0,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.6.0,Extend t
v1.6.0,Concatenate them
v1.6.0,Predict t embedding
v1.6.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.6.0,The results have to be realigned with the expected output of the score_h function
v1.6.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default parameters for the default loss function class
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default loss function class
v1.6.0,: The default parameters for the default loss function class
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the model's hyper-parameters
v1.6.0,: The default parameters for the default loss function class
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,create sparse matrix of absolute counts
v1.6.0,normalize to relative counts
v1.6.0,base case
v1.6.0,"note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could"
v1.6.0,"stay sparse here, with a potential of a huge memory benefit on large datasets!"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,These operations are deterministic and a random seed can be fixed
v1.6.0,just to avoid warnings
v1.6.0,These operations do not need to be performed on a GPU
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,"if we really need access to the path later, we can expose it as a property"
v1.6.0,via self.writer.log_dir
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The WANDB run
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Base classes
v1.6.0,Concrete classes
v1.6.0,Utilities
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The file extension for this writer (do not include dot)
v1.6.0,: The file where the results are written to.
v1.6.0,as_uri() requires the path to be absolute. resolve additionally also normalizes the path
v1.6.0,: The column names
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,store set of triples
v1.6.0,: some prime numbers for tuple hashing
v1.6.0,: The bit-array for the Bloom filter data structure
v1.6.0,Allocate bit array
v1.6.0,calculate number of hashing rounds
v1.6.0,index triples
v1.6.0,Store some meta-data
v1.6.0,pre-hash
v1.6.0,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Set the indices
v1.6.0,Bind number of negatives to sample
v1.6.0,Equally corrupt all sides
v1.6.0,Copy positive batch for corruption.
v1.6.0,"Do not detach, as no gradients should flow into the indices."
v1.6.0,Relations have a different index maximum than entities
v1.6.0,At least make sure to not replace the triples by the original value
v1.6.0,"To make sure we don't replace the {head, relation, tail} by the"
v1.6.0,original value we shift all values greater or equal than the original value by one up
v1.6.0,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.6.0,: A filterer for negative batches
v1.6.0,create unfiltered negative batch by corruption
v1.6.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Utils
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.6.0,create index structure
v1.6.0,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.6.0,: The concatenated sorted sets of head/tail entities
v1.6.0,"shape: (batch_size, num_neg_per_pos, 3)"
v1.6.0,Uniformly sample from head/tail offsets
v1.6.0,get corresponding entity
v1.6.0,"and position within triple (0: head, 2: tail)"
v1.6.0,write into negative batch
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Preprocessing: Compute corruption probabilities
v1.6.0,"compute tph, i.e. the average number of tail entities per head"
v1.6.0,"compute hpt, i.e. the average number of head entities per tail"
v1.6.0,Set parameter for Bernoulli distribution
v1.6.0,Bind number of negatives to sample
v1.6.0,Copy positive batch for corruption.
v1.6.0,"Do not detach, as no gradients should flow into the indices."
v1.6.0,Decide whether to corrupt head or tail
v1.6.0,Tails are corrupted if heads are not corrupted
v1.6.0,We at least make sure to not replace the triples by the original value
v1.6.0,"See below for explanation of why this is on a range of [0, num_entities - 1]"
v1.6.0,Randomly sample corruption.
v1.6.0,Replace heads
v1.6.0,Replace tails
v1.6.0,To make sure we don't replace the head by the original value
v1.6.0,we shift all values greater or equal than the original value by one up
v1.6.0,"for that reason we choose the random value from [0, num_entities -1]"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The random seed used at the beginning of the pipeline
v1.6.0,: The model trained by the pipeline
v1.6.0,: The training triples
v1.6.0,: The training loop used by the pipeline
v1.6.0,: The losses during training
v1.6.0,: The results evaluated by the pipeline
v1.6.0,: How long in seconds did training take?
v1.6.0,: How long in seconds did evaluation take?
v1.6.0,: An early stopper
v1.6.0,: Any additional metadata as a dictionary
v1.6.0,: The version of PyKEEN used to create these results
v1.6.0,: The git hash of PyKEEN used to create these results
v1.6.0,TODO use pathlib here
v1.6.0,FIXME this should never happen.
v1.6.0,1. Dataset
v1.6.0,2. Model
v1.6.0,3. Loss
v1.6.0,4. Regularizer
v1.6.0,5. Optimizer
v1.6.0,5.1 Learning Rate Scheduler
v1.6.0,6. Training Loop
v1.6.0,7. Training (ronaldo style)
v1.6.0,8. Evaluation
v1.6.0,9. Tracking
v1.6.0,Misc
v1.6.0,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.6.0,used random_seed to ensure reproducible results
v1.6.0,We have to set clear optimizer to False since training should be continued
v1.6.0,Start tracking
v1.6.0,evaluation restriction to a subset of entities/relations
v1.6.0,TODO should training be reset?
v1.6.0,TODO should kwargs for loss and regularizer be checked and raised for?
v1.6.0,Log model parameters
v1.6.0,Stopping
v1.6.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.6.0,Add logging for debugging
v1.6.0,Train like Cristiano Ronaldo
v1.6.0,Build up a list of triples if we want to be in the filtered setting
v1.6.0,"If the user gave custom ""additional_filter_triples"""
v1.6.0,Determine whether the validation triples should also be filtered while performing test evaluation
v1.6.0,TODO consider implications of duplicates
v1.6.0,Evaluate
v1.6.0,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.6.0,Add logging about evaluator for debugging
v1.6.0,"If the evaluation still fail using the CPU, the error is raised"
v1.6.0,"When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is"
v1.6.0,restarted with PyKEEN's automatic memory optimization
v1.6.0,"When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation"
v1.6.0,is restarted using the cpu
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: A wrapper around the hidden scheduler base class
v1.6.0,: The default strategy for optimizing the lr_schedulers' hyper-parameters
v1.6.0,: A resolver for learning rate schedulers
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,TODO what happens if already exists?
v1.6.0,TODO incorporate setting of random seed
v1.6.0,pipeline_kwargs=dict(
v1.6.0,"random_seed=random_non_negative_int(),"
v1.6.0,"),"
v1.6.0,Add dataset to current_pipeline
v1.6.0,"Training, test, and validation paths are provided"
v1.6.0,Add loss function to current_pipeline
v1.6.0,Add regularizer to current_pipeline
v1.6.0,Add optimizer to current_pipeline
v1.6.0,Add training approach to current_pipeline
v1.6.0,Add evaluation
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,": The mapping from (graph-pair, side) to triple file name"
v1.6.0,: The internal dataset name
v1.6.0,: The hex digest for the zip file
v1.6.0,Input validation.
v1.6.0,For downloading
v1.6.0,For splitting
v1.6.0,Whether to create inverse triples
v1.6.0,shared directory for multiple datasets.
v1.6.0,ensure file is present
v1.6.0,TODO: Re-use ensure_from_google?
v1.6.0,read all triples from file
v1.6.0,"some ""entities"" have numeric labels"
v1.6.0,"pandas.read_csv(..., dtype=str) does not work properly."
v1.6.0,create triples factory
v1.6.0,split
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.6.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.6.0,the columns are not ordered properly.
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The name of the dataset to download
v1.6.0,FIXME these are already identifiers
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,relation typing
v1.6.0,constants
v1.6.0,unique
v1.6.0,compute over all triples
v1.6.0,Determine group key
v1.6.0,Add labels if requested
v1.6.0,TODO: Merge with _common?
v1.6.0,include hash over triples into cache-file name
v1.6.0,include part hash into cache-file name
v1.6.0,re-use cached file if possible
v1.6.0,select triples
v1.6.0,save to file
v1.6.0,Prune by support and confidence
v1.6.0,TODO: Consider merging with other analysis methods
v1.6.0,TODO: Consider merging with other analysis methods
v1.6.0,TODO: Consider merging with other analysis methods
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Raise matplotlib level
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,don't call this function by itself. assumes called through the `validation`
v1.6.0,property and the _training factory has already been loaded
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Normalize path
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: A factory wrapping the training triples
v1.6.0,": A factory wrapping the testing triples, that share indices with the training triples"
v1.6.0,": A factory wrapping the validation triples, that share indices with the training triples"
v1.6.0,: All datasets should take care of inverse triple creation
v1.6.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.6.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.6.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.6.0,: The directory in which the cached data is stored
v1.6.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.6.0,don't call this function by itself. assumes called through the `validation`
v1.6.0,property and the _training factory has already been loaded
v1.6.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.6.0,"relative paths within zip file's always follow Posix path, even on Windows"
v1.6.0,tarfile does not like pathlib
v1.6.0,: URL to the data to download
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,Concrete Classes
v1.6.0,Utilities
v1.6.0,Assume it's a file path
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,TODO update docs with table and CLI wtih generator
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,1. Dataset
v1.6.0,2. Model
v1.6.0,3. Loss
v1.6.0,4. Regularizer
v1.6.0,5. Optimizer
v1.6.0,5.1 Learning Rate Scheduler
v1.6.0,6. Training Loop
v1.6.0,7. Training
v1.6.0,8. Evaluation
v1.6.0,9. Trackers
v1.6.0,Misc.
v1.6.0,2. Model
v1.6.0,3. Loss
v1.6.0,4. Regularizer
v1.6.0,5. Optimizer
v1.6.0,5.1 Learning Rate Scheduler
v1.6.0,"TODO this fixes the issue for negative samplers, but does not generally address it."
v1.6.0,"For example, some of them obscure their arguments with **kwargs, so should we look"
v1.6.0,at the parent class? Sounds like something to put in class resolver by using the
v1.6.0,"inspect module. For now, this solution will rely on the fact that the sampler is a"
v1.6.0,direct descendent of a parent NegativeSampler
v1.6.0,create result tracker to allow to gracefully close failed trials
v1.6.0,1. Dataset
v1.6.0,2. Model
v1.6.0,3. Loss
v1.6.0,4. Regularizer
v1.6.0,5. Optimizer
v1.6.0,5.1 Learning Rate Scheduler
v1.6.0,6. Training Loop
v1.6.0,7. Training
v1.6.0,8. Evaluation
v1.6.0,9. Tracker
v1.6.0,Misc.
v1.6.0,close run in result tracker
v1.6.0,Will trigger Optuna to set the state of the trial as failed
v1.6.0,: The :mod:`optuna` study object
v1.6.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.6.0,Output study information
v1.6.0,Output all trials
v1.6.0,Output best trial as pipeline configuration file
v1.6.0,1. Dataset
v1.6.0,2. Model
v1.6.0,3. Loss
v1.6.0,4. Regularizer
v1.6.0,5. Optimizer
v1.6.0,5.1 Learning Rate Scheduler
v1.6.0,6. Training Loop
v1.6.0,7. Training
v1.6.0,8. Evaluation
v1.6.0,9. Tracking
v1.6.0,6. Misc
v1.6.0,Optuna Study Settings
v1.6.0,Optuna Optimization Settings
v1.6.0,0. Metadata/Provenance
v1.6.0,1. Dataset
v1.6.0,2. Model
v1.6.0,3. Loss
v1.6.0,4. Regularizer
v1.6.0,5. Optimizer
v1.6.0,5.1 Learning Rate Scheduler
v1.6.0,6. Training Loop
v1.6.0,7. Training
v1.6.0,8. Evaluation
v1.6.0,9. Tracking
v1.6.0,1. Dataset
v1.6.0,2. Model
v1.6.0,3. Loss
v1.6.0,4. Regularizer
v1.6.0,5. Optimizer
v1.6.0,5.1 Learning Rate Scheduler
v1.6.0,6. Training Loop
v1.6.0,7. Training
v1.6.0,8. Evaluation
v1.6.0,9. Tracker
v1.6.0,Optuna Misc.
v1.6.0,Pipeline Misc.
v1.6.0,Invoke optimization of the objective function.
v1.6.0,TODO: make it even easier to specify categorical strategies just as lists
v1.6.0,"if isinstance(info, (tuple, list, set)):"
v1.6.0,"info = dict(type='categorical', choices=list(info))"
v1.6.0,get log from info - could either be a boolean or string
v1.6.0,"otherwise, dataset refers to a file that should be automatically split"
v1.6.0,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.6.0,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.6.0,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.6.0,if has_dataset(dataset_name):
v1.6.0,"study.set_user_attr('dataset', dataset_name)"
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.6.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,
v1.5.0,Configuration file for the Sphinx documentation builder.
v1.5.0,
v1.5.0,This file does only contain a selection of the most common options. For a
v1.5.0,full list see the documentation:
v1.5.0,http://www.sphinx-doc.org/en/master/config
v1.5.0,-- Path setup --------------------------------------------------------------
v1.5.0,"If extensions (or modules to document with autodoc) are in another directory,"
v1.5.0,add these directories to sys.path here. If the directory is relative to the
v1.5.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.5.0,
v1.5.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.5.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.5.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.5.0,from unittest.mock import Mock
v1.5.0,sys.modules['numpy'] = Mock()
v1.5.0,sys.modules['numpy.linalg'] = Mock()
v1.5.0,sys.modules['scipy'] = Mock()
v1.5.0,sys.modules['scipy.optimize'] = Mock()
v1.5.0,sys.modules['scipy.interpolate'] = Mock()
v1.5.0,sys.modules['scipy.sparse'] = Mock()
v1.5.0,sys.modules['scipy.ndimage'] = Mock()
v1.5.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.5.0,sys.modules['tensorflow'] = Mock()
v1.5.0,sys.modules['theano'] = Mock()
v1.5.0,sys.modules['theano.tensor'] = Mock()
v1.5.0,sys.modules['torch'] = Mock()
v1.5.0,sys.modules['torch.optim'] = Mock()
v1.5.0,sys.modules['torch.nn'] = Mock()
v1.5.0,sys.modules['torch.nn.init'] = Mock()
v1.5.0,sys.modules['torch.autograd'] = Mock()
v1.5.0,sys.modules['sklearn'] = Mock()
v1.5.0,sys.modules['sklearn.model_selection'] = Mock()
v1.5.0,sys.modules['sklearn.utils'] = Mock()
v1.5.0,-- Project information -----------------------------------------------------
v1.5.0,"The full version, including alpha/beta/rc tags."
v1.5.0,The short X.Y version.
v1.5.0,-- General configuration ---------------------------------------------------
v1.5.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.5.0,
v1.5.0,needs_sphinx = '1.0'
v1.5.0,"If true, the current module name will be prepended to all description"
v1.5.0,unit titles (such as .. function::).
v1.5.0,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.5.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.5.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.5.0,ones.
v1.5.0,show todo's
v1.5.0,generate autosummary pages
v1.5.0,"Add any paths that contain templates here, relative to this directory."
v1.5.0,The suffix(es) of source filenames.
v1.5.0,You can specify multiple suffix as a list of string:
v1.5.0,
v1.5.0,"source_suffix = ['.rst', '.md']"
v1.5.0,The master toctree document.
v1.5.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.5.0,for a list of supported languages.
v1.5.0,
v1.5.0,This is also used if you do content translation via gettext catalogs.
v1.5.0,"Usually you set ""language"" from the command line for these cases."
v1.5.0,"List of patterns, relative to source directory, that match files and"
v1.5.0,directories to ignore when looking for source files.
v1.5.0,This pattern also affects html_static_path and html_extra_path.
v1.5.0,The name of the Pygments (syntax highlighting) style to use.
v1.5.0,-- Options for HTML output -------------------------------------------------
v1.5.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.5.0,a list of builtin themes.
v1.5.0,
v1.5.0,Theme options are theme-specific and customize the look and feel of a theme
v1.5.0,"further.  For a list of options available for each theme, see the"
v1.5.0,documentation.
v1.5.0,
v1.5.0,html_theme_options = {}
v1.5.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.5.0,"relative to this directory. They are copied after the builtin static files,"
v1.5.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.5.0,html_static_path = ['_static']
v1.5.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.5.0,to template names.
v1.5.0,
v1.5.0,The default sidebars (for documents that don't match any pattern) are
v1.5.0,defined by theme itself.  Builtin themes are using these templates by
v1.5.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.5.0,'searchbox.html']``.
v1.5.0,
v1.5.0,html_sidebars = {}
v1.5.0,The name of an image file (relative to this directory) to place at the top
v1.5.0,of the sidebar.
v1.5.0,
v1.5.0,-- Options for HTMLHelp output ---------------------------------------------
v1.5.0,Output file base name for HTML help builder.
v1.5.0,-- Options for LaTeX output ------------------------------------------------
v1.5.0,latex_elements = {
v1.5.0,The paper size ('letterpaper' or 'a4paper').
v1.5.0,
v1.5.0,"'papersize': 'letterpaper',"
v1.5.0,
v1.5.0,"The font size ('10pt', '11pt' or '12pt')."
v1.5.0,
v1.5.0,"'pointsize': '10pt',"
v1.5.0,
v1.5.0,Additional stuff for the LaTeX preamble.
v1.5.0,
v1.5.0,"'preamble': '',"
v1.5.0,
v1.5.0,Latex figure (float) alignment
v1.5.0,
v1.5.0,"'figure_align': 'htbp',"
v1.5.0,}
v1.5.0,Grouping the document tree into LaTeX files. List of tuples
v1.5.0,"(source start file, target name, title,"
v1.5.0,"author, documentclass [howto, manual, or own class])."
v1.5.0,latex_documents = [
v1.5.0,(
v1.5.0,"master_doc,"
v1.5.0,"'pykeen.tex',"
v1.5.0,"'PyKEEN Documentation',"
v1.5.0,"author,"
v1.5.0,"'manual',"
v1.5.0,"),"
v1.5.0,]
v1.5.0,-- Options for manual page output ------------------------------------------
v1.5.0,One entry per manual page. List of tuples
v1.5.0,"(source start file, name, description, authors, manual section)."
v1.5.0,-- Options for Texinfo output ----------------------------------------------
v1.5.0,Grouping the document tree into Texinfo files. List of tuples
v1.5.0,"(source start file, target name, title, author,"
v1.5.0,"dir menu entry, description, category)"
v1.5.0,-- Options for Epub output -------------------------------------------------
v1.5.0,Bibliographic Dublin Core info.
v1.5.0,epub_title = project
v1.5.0,The unique identifier of the text. This can be a ISBN number
v1.5.0,or the project homepage.
v1.5.0,
v1.5.0,epub_identifier = ''
v1.5.0,A unique identification for the text.
v1.5.0,
v1.5.0,epub_uid = ''
v1.5.0,A list of files that should not be packed into the epub file.
v1.5.0,epub_exclude_files = ['search.html']
v1.5.0,-- Extension configuration -------------------------------------------------
v1.5.0,-- Options for intersphinx extension ---------------------------------------
v1.5.0,Example configuration for intersphinx: refer to the Python standard library.
v1.5.0,autodoc_member_order = 'bysource'
v1.5.0,autodoc_typehints = 'both' # TODO turn on after 4.1 release
v1.5.0,autodoc_preserve_defaults = True
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Check a model param is optimized
v1.5.0,Check a loss param is optimized
v1.5.0,Check a model param is NOT optimized
v1.5.0,Check a loss param is optimized
v1.5.0,Check a model param is optimized
v1.5.0,Check a loss param is NOT optimized
v1.5.0,Check a model param is NOT optimized
v1.5.0,Check a loss param is NOT optimized
v1.5.0,"Since custom data was passed, we can't store any of this"
v1.5.0,"currently, any custom data doesn't get stored."
v1.5.0,"self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])"
v1.5.0,"Since there's no source path information, these shouldn't be"
v1.5.0,"added, even if it might be possible to infer path information"
v1.5.0,from the triples factories
v1.5.0,"Since paths were passed for training, testing, and validation,"
v1.5.0,they should be stored as study-level attributes
v1.5.0,Check a model param is optimized
v1.5.0,Check a loss param is optimized
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,The triples factory and model
v1.5.0,: The evaluator to be tested
v1.5.0,Settings
v1.5.0,: The evaluator instantiation
v1.5.0,Settings
v1.5.0,Initialize evaluator
v1.5.0,Use small test dataset
v1.5.0,Use small model (untrained)
v1.5.0,Get batch
v1.5.0,Compute scores
v1.5.0,Compute mask only if required
v1.5.0,TODO: Re-use filtering code
v1.5.0,"shape: (batch_size, num_triples)"
v1.5.0,"shape: (batch_size, num_entities)"
v1.5.0,Process one batch
v1.5.0,Check for correct class
v1.5.0,Check value ranges
v1.5.0,check mean rank (MR)
v1.5.0,check mean reciprocal rank (MRR)
v1.5.0,check hits at k (H@k)
v1.5.0,check adjusted mean rank (AMR)
v1.5.0,check adjusted mean rank index (AMRI)
v1.5.0,TODO: Validate with data?
v1.5.0,Check for correct class
v1.5.0,check value
v1.5.0,filtering
v1.5.0,"true_score: (2, 3, 3)"
v1.5.0,head based filter
v1.5.0,preprocessing for faster lookup
v1.5.0,check that all found positives are positive
v1.5.0,check in-place
v1.5.0,Test head scores
v1.5.0,Assert in-place modification
v1.5.0,Assert correct filtering
v1.5.0,Test tail scores
v1.5.0,Assert in-place modification
v1.5.0,Assert correct filtering
v1.5.0,The MockModel gives the highest score to the highest entity id
v1.5.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.5.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.5.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.5.0,test triple at hand
v1.5.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.5.0,test triple at hand
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,check if within 0.5 std of observed
v1.5.0,test error is raised
v1.5.0,Tests that exception will be thrown when more than or less than three tensors are passed
v1.5.0,Test that regularization term is computed correctly
v1.5.0,Entity soft constraint
v1.5.0,Orthogonality soft constraint
v1.5.0,ensure regularizer is on correct device
v1.5.0,"After first update, should change the term"
v1.5.0,"After second update, no change should happen"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,create broadcastable shapes
v1.5.0,check correct value range
v1.5.0,check maximum norm constraint
v1.5.0,unchanged values for small norms
v1.5.0,generate random query tensor
v1.5.0,random entity embeddings & projections
v1.5.0,random relation embeddings & projections
v1.5.0,project
v1.5.0,check shape:
v1.5.0,check normalization
v1.5.0,check equivalence of re-formulation
v1.5.0,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.5.0,= r_p (e_p^T e) + e'
v1.5.0,"create random array, estimate the costs of addition, and measure some execution times."
v1.5.0,"then, compute correlation between the estimated cost, and the measured time."
v1.5.0,check for strong correlation between estimated costs and measured execution time
v1.5.0,get optimal sequence
v1.5.0,check caching
v1.5.0,get optimal sequence
v1.5.0,check correct cost
v1.5.0,check optimality
v1.5.0,compare result to sequential addition
v1.5.0,compare result to sequential addition
v1.5.0,check result shape
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,equal value; larger is better
v1.5.0,equal value; smaller is better
v1.5.0,larger is better; improvement
v1.5.0,larger is better; improvement; but not significant
v1.5.0,: The window size used by the early stopper
v1.5.0,: The mock losses the mock evaluator will return
v1.5.0,: The (zeroed) index  - 1 at which stopping will occur
v1.5.0,: The minimum improvement
v1.5.0,: The best results
v1.5.0,Set automatic_memory_optimization to false for tests
v1.5.0,Step early stopper
v1.5.0,check storing of results
v1.5.0,check ring buffer
v1.5.0,: The window size used by the early stopper
v1.5.0,: The (zeroed) index  - 1 at which stopping will occur
v1.5.0,: The minimum improvement
v1.5.0,: The random seed to use for reproducibility
v1.5.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.5.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.5.0,: The batch size to use.
v1.5.0,Fix seed for reproducibility
v1.5.0,Set automatic_memory_optimization to false during testing
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.5.0,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.5.0,"first triple, and passing create_inverse_triples=True."
v1.5.0,check for warning
v1.5.0,check for filtered triples
v1.5.0,check for correct inverse triples flag
v1.5.0,check correct translation
v1.5.0,check column order
v1.5.0,apply restriction
v1.5.0,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.5.0,check that inverse_triples is correctly carried over
v1.5.0,verify that the label-to-ID mapping has not been changed
v1.5.0,verify that triples have been filtered
v1.5.0,check compressed triples
v1.5.0,reconstruct triples from compressed form
v1.5.0,check data loader
v1.5.0,set create inverse triple to true
v1.5.0,split factory
v1.5.0,check that in *training* inverse triple are to be created
v1.5.0,check that in all other splits no inverse triples are to be created
v1.5.0,verify that all entities and relations are present in the training factory
v1.5.0,verify that no triple got lost
v1.5.0,verify that the label-to-id mappings match
v1.5.0,check type
v1.5.0,check format
v1.5.0,check coverage
v1.5.0,Check if multilabels are working correctly
v1.5.0,generate random ratios
v1.5.0,check size
v1.5.0,check value range
v1.5.0,check total split
v1.5.0,check consistency with ratios
v1.5.0,the number of decimal digits equivalent to 1 / n_total
v1.5.0,check type
v1.5.0,check values
v1.5.0,compare against expected
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,"DummyModel,"
v1.5.0,3x batch norm: bias + scale --> 6
v1.5.0,entity specific bias        --> 1
v1.5.0,==================================
v1.5.0,7
v1.5.0,"two bias terms, one conv-filter"
v1.5.0,check type
v1.5.0,check shape
v1.5.0,check ID ranges
v1.5.0,this is only done in one of the models
v1.5.0,this is only done in one of the models
v1.5.0,Two linear layer biases
v1.5.0,"Two BN layers, bias & scale"
v1.5.0,quaternion have four components
v1.5.0,: one bias per layer
v1.5.0,: (scale & bias for BN) * layers
v1.5.0,entity embeddings
v1.5.0,relation embeddings
v1.5.0,Compute Scores
v1.5.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.5.0,relation embeddings
v1.5.0,Compute Scores
v1.5.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.5.0,entity embeddings
v1.5.0,relation embeddings
v1.5.0,Compute Scores
v1.5.0,random entity embeddings & projections
v1.5.0,random relation embeddings & projections
v1.5.0,project
v1.5.0,check shape:
v1.5.0,check normalization
v1.5.0,entity embeddings
v1.5.0,relation embeddings
v1.5.0,Compute Scores
v1.5.0,second_score = scores[1].item()
v1.5.0,: 2xBN (bias & scale)
v1.5.0,the combination bias
v1.5.0,check shape
v1.5.0,check content
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,empty lists are falsy
v1.5.0,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.5.0,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.5.0,pipeline is checked.
v1.5.0,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.5.0,Resume the previous pipeline
v1.5.0,The MockModel gives the highest score to the highest entity id
v1.5.0,The test triples are created to yield the third highest score on both head and tail prediction
v1.5.0,"Write new mapped triples to the model, since the model's triples will be used to filter"
v1.5.0,These triples are created to yield the highest score on both head and tail prediction for the
v1.5.0,test triple at hand
v1.5.0,The validation triples are created to yield the second highest score on both head and tail prediction for the
v1.5.0,test triple at hand
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.5.0,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.5.0,Test looking up inverse triples
v1.5.0,test new label to ID
v1.5.0,type
v1.5.0,old labels
v1.5.0,"new, compact IDs"
v1.5.0,test vectorized lookup
v1.5.0,type
v1.5.0,shape
v1.5.0,value range
v1.5.0,only occurring Ids get mapped to non-negative numbers
v1.5.0,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.5.0,check type
v1.5.0,check shape
v1.5.0,check content
v1.5.0,check type
v1.5.0,check shape
v1.5.0,check 1-hot
v1.5.0,check type
v1.5.0,check shape
v1.5.0,check value range
v1.5.0,check self-similarity = 1
v1.5.0,base relation
v1.5.0,exact duplicate
v1.5.0,99% duplicate
v1.5.0,50% duplicate
v1.5.0,exact inverse
v1.5.0,99% inverse
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The expected number of entities
v1.5.0,: The expected number of relations
v1.5.0,: The expected number of triples
v1.5.0,": The tolerance on expected number of triples, for randomized situations"
v1.5.0,: The dataset to test
v1.5.0,: The instantiated dataset
v1.5.0,: Should the validation be assumed to have been loaded with train/test?
v1.5.0,Not loaded
v1.5.0,Load
v1.5.0,Test caching
v1.5.0,assert (end - start) < 1.0e-02
v1.5.0,Test consistency of training / validation / testing mapping
v1.5.0,": The directory, if there is caching"
v1.5.0,: The batch size
v1.5.0,: The number of negatives per positive for sLCWA training loop.
v1.5.0,: The number of entities LCWA training loop / label smoothing.
v1.5.0,test reduction
v1.5.0,test finite loss value
v1.5.0,Test backward
v1.5.0,negative scores decreased compared to positive ones
v1.5.0,negative scores decreased compared to positive ones
v1.5.0,: The number of entities.
v1.5.0,: The number of negative samples
v1.5.0,: The number of entities.
v1.5.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.5.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.5.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.5.0,check whether the error originates from batch norm for single element batches
v1.5.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.5.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.5.0,test multiple different initializations
v1.5.0,calculate by functional
v1.5.0,calculate manually
v1.5.0,simple
v1.5.0,nested
v1.5.0,nested
v1.5.0,prepare a temporary test directory
v1.5.0,check that file was created
v1.5.0,make sure to close file before trying to delete it
v1.5.0,delete intermediate files
v1.5.0,: The batch size
v1.5.0,: The triples factory
v1.5.0,: Class of regularizer to test
v1.5.0,: The constructor parameters to pass to the regularizer
v1.5.0,": The regularizer instance, initialized in setUp"
v1.5.0,: A positive batch
v1.5.0,: The device
v1.5.0,move test instance to device
v1.5.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.5.0,Check if regularizer is stored correctly.
v1.5.0,Forward pass (should update regularizer)
v1.5.0,Call post_parameter_update (should reset regularizer)
v1.5.0,Check if regularization term is reset
v1.5.0,Call method
v1.5.0,Generate random tensors
v1.5.0,Call update
v1.5.0,check shape
v1.5.0,compute expected term
v1.5.0,Generate random tensor
v1.5.0,calculate penalty
v1.5.0,check shape
v1.5.0,check value
v1.5.0,FIXME isn't any finite number allowed now?
v1.5.0,: Additional arguments passed to the training loop's constructor method
v1.5.0,: The triples factory instance
v1.5.0,: The batch size for use for forward_* tests
v1.5.0,: The embedding dimensionality
v1.5.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.5.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.5.0,: The batch size for use when testing training procedures
v1.5.0,: The number of epochs to train the model
v1.5.0,: A random number generator from torch
v1.5.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.5.0,initialization
v1.5.0,: Static extras to append to the CLI
v1.5.0,for reproducible testing
v1.5.0,insert shared parameters
v1.5.0,move model to correct device
v1.5.0,assert there is at least one trainable parameter
v1.5.0,Check that all the parameters actually require a gradient
v1.5.0,Try to initialize an optimizer
v1.5.0,get model parameters
v1.5.0,re-initialize
v1.5.0,check that the operation works in-place
v1.5.0,check that the parameters where modified
v1.5.0,check for finite values by default
v1.5.0,check whether a gradient can be back-propgated
v1.5.0,"assert batch comprises (head, relation) pairs"
v1.5.0,"assert batch comprises (relation, tail) pairs"
v1.5.0,"For the high/low memory test cases of NTN, SE, etc."
v1.5.0,"else, leave to default"
v1.5.0,TODO: Make sure that inverse triples are created if create_inverse_triples=True
v1.5.0,TODO: Catch HolE MKL error?
v1.5.0,set regularizer term to something that isn't zero
v1.5.0,call post_parameter_update
v1.5.0,assert that the regularization term has been reset
v1.5.0,do one optimization step
v1.5.0,call post_parameter_update
v1.5.0,check model constraints
v1.5.0,"assert batch comprises (relation, tail) pairs"
v1.5.0,"assert batch comprises (relation, tail) pairs"
v1.5.0,"assert batch comprises (relation, tail) pairs"
v1.5.0,call some functions
v1.5.0,reset to old state
v1.5.0,call some functions
v1.5.0,reset to old state
v1.5.0,Distance-based model
v1.5.0,check type
v1.5.0,check shape
v1.5.0,: The number of entities
v1.5.0,: The number of triples
v1.5.0,check shape
v1.5.0,check dtype
v1.5.0,check finite values (e.g. due to division by zero)
v1.5.0,check non-negativity
v1.5.0,: The input dimension
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,check for finite values by default
v1.5.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.5.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.5.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.5.0,Get embeddings
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,≈ result of softmax
v1.5.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.5.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.5.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.5.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.5.0,≈ result of sigmoid
v1.5.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.5.0,expected_loss ≈ 0.34
v1.5.0,Create dummy dense labels
v1.5.0,Check if labels form a probability distribution
v1.5.0,Apply label smoothing
v1.5.0,Check if smooth labels form probability distribution
v1.5.0,Create dummy sLCWA labels
v1.5.0,Apply label smoothing
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,"naive implementation, O(n2)"
v1.5.0,check correct output type
v1.5.0,check value range subset
v1.5.0,check value range side
v1.5.0,check columns
v1.5.0,check value range and type
v1.5.0,check value range entity IDs
v1.5.0,check value range entity labels
v1.5.0,check correct type
v1.5.0,check relation_id value range
v1.5.0,check pattern value range
v1.5.0,check confidence value range
v1.5.0,check support value range
v1.5.0,check correct type
v1.5.0,check relation_id value range
v1.5.0,check pattern value range
v1.5.0,check correct type
v1.5.0,check relation_id value range
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Check minimal statistics
v1.5.0,Check statistics for pre-stratified datasets
v1.5.0,Check either a github link or author/publication information is given
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.5.0,"prepare conv input (N, C, H, W)"
v1.5.0,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.5.0,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.5.0,remove batch/num dimension
v1.5.0,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.5.0,"f(h, r, t) = h @ r @ t"
v1.5.0,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.5.0,normalize length of r
v1.5.0,check for unit length
v1.5.0,entity embeddings
v1.5.0,relation embeddings
v1.5.0,Compute Scores
v1.5.0,entity embeddings
v1.5.0,relation embeddings
v1.5.0,Compute Scores
v1.5.0,Compute Scores
v1.5.0,-\|R_h h - R_t t\|
v1.5.0,-\|h - t\|
v1.5.0,"Since MuRE has offsets, the scores do not need to negative"
v1.5.0,"We do not need this, since we do not check for functional consistency anyway"
v1.5.0,intra-interaction comparison
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,create a new instance with guaranteed dropout
v1.5.0,set to training mode
v1.5.0,check for different output
v1.5.0,: The number of embeddings
v1.5.0,check shape
v1.5.0,check attributes
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,"typically, the model takes care of adjusting the dimension size for ""complex"""
v1.5.0,"tensors, but we have to do it manually here for testing purposes"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,ensure positivity
v1.5.0,compute using pytorch
v1.5.0,prepare distributions
v1.5.0,compute using pykeen
v1.5.0,"e: (batch_size, num_heads, num_tails, d)"
v1.5.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.5.0,divergence = 0 => similarity = -divergence = 0
v1.5.0,"(h - t), r"
v1.5.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.5.0,divergence >= 0 => similarity = -divergence <= 0
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Multiple permutations of loss not necessary for bloom filter since it's more of a
v1.5.0,filter vs. no filter thing.
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,check for empty batches
v1.5.0,: The window size used by the early stopper
v1.5.0,: The mock losses the mock evaluator will return
v1.5.0,: The (zeroed) index  - 1 at which stopping will occur
v1.5.0,: The minimum improvement
v1.5.0,: The best results
v1.5.0,Set automatic_memory_optimization to false for tests
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Train a model in one shot
v1.5.0,Train a model for the first half
v1.5.0,Continue training of the first part
v1.5.0,: Should negative samples be filtered?
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Check whether filtering works correctly
v1.5.0,First giving an example where all triples have to be filtered
v1.5.0,The filter should remove all triples
v1.5.0,Create an example where no triples will be filtered
v1.5.0,The filter should not remove any triple
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,sample a batch
v1.5.0,check shape
v1.5.0,get triples
v1.5.0,check connected components
v1.5.0,super inefficient
v1.5.0,join
v1.5.0,already joined
v1.5.0,check that there is only a single component
v1.5.0,check content of comp_adj_lists
v1.5.0,check edge ids
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Test that half of the subjects and half of the objects are corrupted
v1.5.0,same relation
v1.5.0,"only corruption of a single entity (note: we do not check for exactly 2, since we do not filter)."
v1.5.0,check that corrupted entities co-occur with the relation in training data
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The batch size
v1.5.0,: The random seed
v1.5.0,: The triples factory
v1.5.0,: The instances
v1.5.0,: A positive batch
v1.5.0,: Kwargs
v1.5.0,Generate negative sample
v1.5.0,check filter shape if necessary
v1.5.0,check shape
v1.5.0,check bounds: heads
v1.5.0,check bounds: relations
v1.5.0,check bounds: tails
v1.5.0,test that the negative triple is not the original positive triple
v1.5.0,"shape: (batch_size, 1, num_neg)"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Base Classes
v1.5.0,Concrete Classes
v1.5.0,Utils
v1.5.0,: The default strategy for optimizing the loss's hyper-parameters
v1.5.0,flatten and stack
v1.5.0,apply label smoothing if necessary.
v1.5.0,TODO: Do label smoothing only once
v1.5.0,Sanity check
v1.5.0,"prepare for broadcasting, shape: (batch_size, 1, 3)"
v1.5.0,negative_scores have already been filtered in the sampler!
v1.5.0,"shape: (nnz,)"
v1.5.0,Sanity check
v1.5.0,"for LCWA scores, we consider all pairs of positive and negative scores for a single batch element."
v1.5.0,"note: this leads to non-uniform memory requirements for different batches, depending on the total number of"
v1.5.0,positive entries in the labels tensor.
v1.5.0,"This shows how often one row has to be repeated,"
v1.5.0,"shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i"
v1.5.0,"Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)"
v1.5.0,"Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)"
v1.5.0,This tells us how often each true label should be repeated
v1.5.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.5.0,"scale labels from [0, 1] to [-1, 1]"
v1.5.0,cross entropy expects a proper probability distribution -> normalize labels
v1.5.0,Use numerically stable variant to compute log(softmax)
v1.5.0,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.5.0,Sanity check
v1.5.0,compute negative weights (without gradient tracking)
v1.5.0,clone is necessary since we modify in-place
v1.5.0,Split positive and negative scores
v1.5.0,Sanity check
v1.5.0,negative_scores have already been filtered in the sampler!
v1.5.0,(dense) softmax requires unfiltered scores / masking
v1.5.0,we need to fill the scores with -inf for all filtered negative examples
v1.5.0,EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)
v1.5.0,use filled negatives scores
v1.5.0,compute weights (without gradient tracking)
v1.5.0,-w * log sigma(-(m + n)) - log sigma (m + p)
v1.5.0,p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0
v1.5.0,p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.5.0,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.5.0,": For more information, see https://github.com/cthoyt/pystow"
v1.5.0,: A path representing the PyKEEN data folder
v1.5.0,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.5.0,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.5.0,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.5.0,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.5.0,: A subdirectory for PyKEEN logs
v1.5.0,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.5.0,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.5.0,lower bound
v1.5.0,upper bound
v1.5.0,normalize input
v1.5.0,input validation
v1.5.0,base case
v1.5.0,normalize dim
v1.5.0,calculate repeats for each tensor
v1.5.0,dimensions along concatenation axis do not need to match
v1.5.0,get desired extent along dimension
v1.5.0,repeat tensors along axes if necessary
v1.5.0,concatenate
v1.5.0,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.5.0,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.5.0,Determine optimal order and cost
v1.5.0,translate back to original order
v1.5.0,determine optimal processing order
v1.5.0,heuristic
v1.5.0,workaround for complex numbers: manually compute norm
v1.5.0,TODO: check if einsum is still very slow.
v1.5.0,TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation
v1.5.0,unsqueeze
v1.5.0,The dimensions affected by e'
v1.5.0,Project entities
v1.5.0,r_p (e_p.T e) + e'
v1.5.0,Enforce constraints
v1.5.0,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.5.0,Create a tensor of all IDs
v1.5.0,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.5.0,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.5.0,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.5.0,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.5.0,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.5.0,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.5.0,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Base Class
v1.5.0,Child classes
v1.5.0,Utils
v1.5.0,: The overall regularization weight
v1.5.0,: The current regularization term (a scalar)
v1.5.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.5.0,: Has this regularizer been updated since last being reset?
v1.5.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.5.0,"If there are tracked parameters, update based on them"
v1.5.0,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.5.0,no need to compute anything
v1.5.0,always return zero
v1.5.0,: The dimension along which to compute the vector-based regularization terms.
v1.5.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.5.0,: This allows dimensionality-independent weight tuning.
v1.5.0,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.5.0,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.5.0,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.5.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.5.0,"Therefore, apply_only_once is always set to True."
v1.5.0,Entity soft constraint
v1.5.0,Orthogonality soft constraint
v1.5.0,The normalization factor to balance individual regularizers' contribution.
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Add HPO command
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,General types
v1.5.0,Triples
v1.5.0,Others
v1.5.0,Tensor Functions
v1.5.0,Tensors
v1.5.0,Dataclasses
v1.5.0,: A function that mutates the input and returns a new object of the same type as output
v1.5.0,: A function that can be applied to a tensor to initialize it
v1.5.0,: A function that can be applied to a tensor to normalize it
v1.5.0,: A function that can be applied to a tensor to constrain it
v1.5.0,: A hint for a :class:`torch.device`
v1.5.0,: A hint for a :class:`torch.Generator`
v1.5.0,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.5.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.5.0,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.5.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.5.0,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.5.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,pad with zeros
v1.5.0,trim
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,"mask, shape: (num_edges,)"
v1.5.0,bi-directional message passing
v1.5.0,Heuristic for default value
v1.5.0,weights
v1.5.0,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.5.0,initialized properly)
v1.5.0,We have one additional relation for self-loops
v1.5.0,other relations
v1.5.0,other relations
v1.5.0,Select source and target indices as well as edge weights for the
v1.5.0,currently considered relation
v1.5.0,skip relations without edges
v1.5.0,"compute message, shape: (num_edges_of_type, output_dim)"
v1.5.0,since we may have one node ID appearing multiple times as source
v1.5.0,"ID, we can save some computation by first reducing to the unique"
v1.5.0,"source IDs, compute transformed representations and afterwards"
v1.5.0,select these representations for the correct edges.
v1.5.0,select unique source node representations
v1.5.0,transform representations by relation specific weight
v1.5.0,select the uniquely transformed representations for each edge
v1.5.0,optional message weighting
v1.5.0,message aggregation
v1.5.0,self-loops first
v1.5.0,the last relation_id refers to the self-loop
v1.5.0,Xavier Glorot initialization of each block
v1.5.0,view as blocks
v1.5.0,self-loop first
v1.5.0,other relations
v1.5.0,skip relations without edges
v1.5.0,"compute message, shape: (num_edges_of_type, num_blocks, block_size)"
v1.5.0,optional message weighting
v1.5.0,message aggregation
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: the maximum ID (exclusively)
v1.5.0,: the shape of an individual representation
v1.5.0,TODO: Remove this property and update code to use shape instead
v1.5.0,normalize embedding_dim vs. shape
v1.5.0,work-around until full complex support
v1.5.0,TODO: verify that this is our understanding of complex!
v1.5.0,"wrapper around max_id, for backward compatibility"
v1.5.0,initialize weights in-place
v1.5.0,apply constraints in-place
v1.5.0,verify that contiguity is preserved
v1.5.0,TODO: move normalizer / regularizer to base class?
v1.5.0,freeze
v1.5.0,use this instead of a lambda to make sure that it can be pickled
v1.5.0,TODO add normalization functions
v1.5.0,Resolve edge weighting
v1.5.0,dropout
v1.5.0,batch norm and bias
v1.5.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.5.0,buffering of enriched representations
v1.5.0,invalidate enriched embeddings
v1.5.0,Bind fields
v1.5.0,"shape: (num_entities, embedding_dim)"
v1.5.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.5.0,Get random dropout mask
v1.5.0,Apply to edges
v1.5.0,Different dropout for self-loops (only in training mode)
v1.5.0,fixed edges -> pre-compute weights
v1.5.0,Cache enriched representations
v1.5.0,normalize output dimension
v1.5.0,entity-relation composition
v1.5.0,edge weighting
v1.5.0,message passing weights
v1.5.0,linear relation transformation
v1.5.0,layer-specific self-loop relation representation
v1.5.0,other components
v1.5.0,initialize
v1.5.0,split
v1.5.0,compose
v1.5.0,transform
v1.5.0,normalization
v1.5.0,aggregate by sum
v1.5.0,dropout
v1.5.0,prepare for inverse relations
v1.5.0,update entity representations: mean over self-loops / forward edges / backward edges
v1.5.0,Relation transformation
v1.5.0,Buffered enriched entity and relation representations
v1.5.0,TODO: Check
v1.5.0,hidden dimension normalization
v1.5.0,Create message passing layers
v1.5.0,register buffers for adjacency matrix; we use the same format as PyTorch Geometric
v1.5.0,TODO: This always uses all training triples for message passing
v1.5.0,initialize buffer of enriched representations
v1.5.0,invalidate enriched embeddings
v1.5.0,enrich
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,scaling factor
v1.5.0,"modulus ~ Uniform[-s, s]"
v1.5.0,"phase ~ Uniform[0, 2*pi]"
v1.5.0,real part
v1.5.0,purely imaginary quaternions unitary
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,"Calculate in-degree, i.e. number of incoming edges"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,TODO test
v1.5.0,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.5.0,: a = \mu^T\Sigma^{-1}\mu
v1.5.0,: b = \log \det \Sigma
v1.5.0,1. Component
v1.5.0,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.5.0,2. Component
v1.5.0,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.5.0,with mu = (mu_1 - mu_0)
v1.5.0,= mu * Sigma_1^-1 mu
v1.5.0,since Sigma_1 is diagonal
v1.5.0,= mu**2 / sigma_1
v1.5.0,3. Component
v1.5.0,4. Component
v1.5.0,ln (det(\Sigma_1) / det(\Sigma_0))
v1.5.0,= ln det Sigma_1 - ln det Sigma_0
v1.5.0,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.5.0,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.5.0,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.5.0,allocate result
v1.5.0,prepare distributions
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,TODO benchmark
v1.5.0,TODO benchmark
v1.5.0,TODO benchmark
v1.5.0,TODO benchmark
v1.5.0,TODO benchmark
v1.5.0,TODO benchmark
v1.5.0,TODO benchmark
v1.5.0,TODO benchmark
v1.5.0,TODO benchmark
v1.5.0,"h = h_re, -h_im"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Base Classes
v1.5.0,Adapter classes
v1.5.0,Concrete Classes
v1.5.0,: The symbolic shapes for entity representations
v1.5.0,": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE."
v1.5.0,: The symbolic shapes for relation representations
v1.5.0,"bring to (b, n, *)"
v1.5.0,"bring to (b, h, r, t, *)"
v1.5.0,unpack singleton
v1.5.0,"The appended ""e"" represents the literals that get concatenated"
v1.5.0,on the entity representations. It does not necessarily have the
v1.5.0,"same dimension ""d"" as the entity representations."
v1.5.0,alternate way of combining entity embeddings + literals
v1.5.0,"h = torch.cat(h, dim=-1)"
v1.5.0,"h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore"
v1.5.0,"t = torch.cat(t, dim=-1)"
v1.5.0,"t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore"
v1.5.0,: The functional interaction form
v1.5.0,Store initial input for error message
v1.5.0,All are None -> try and make closest to square
v1.5.0,Only input channels is None
v1.5.0,Only width is None
v1.5.0,Only height is none
v1.5.0,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.5.0,Width and input channels are None -> set input channels to 1 and calculate width
v1.5.0,": The head-relation encoder operating on 2D ""images"""
v1.5.0,: The head-relation encoder operating on the 1D flattened version
v1.5.0,: The interaction function
v1.5.0,Automatic calculation of remaining dimensions
v1.5.0,Parameter need to fulfil:
v1.5.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.5.0,encoders
v1.5.0,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.5.0,"2: 1D encoder: FC, DO, BN?, Act"
v1.5.0,store reshaping dimensions
v1.5.0,The interaction model
v1.5.0,Use Xavier initialization for weight; bias to zero
v1.5.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.5.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.5.0,Initialize biases with zero
v1.5.0,"In the original formulation,"
v1.5.0,Global entity projection
v1.5.0,Global relation projection
v1.5.0,Global combination bias
v1.5.0,Global combination bias
v1.5.0,Core tensor
v1.5.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.5.0,Dropout
v1.5.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.5.0,"batch norm gets reset automatically, since it defines reset_parameters"
v1.5.0,shapes
v1.5.0,there are separate biases for entities in head and tail position
v1.5.0,the base interaction
v1.5.0,forward entity/relation shapes
v1.5.0,The parameters of the affine transformation: bias
v1.5.0,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,"repeat if necessary, and concat head and relation, batch_size', num_input_channels, 2*height, width"
v1.5.0,with batch_size' = batch_size * num_heads * num_relations
v1.5.0,"batch_size', num_input_channels, 2*height, width"
v1.5.0,"batch_size', num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.5.0,"reshape: (batch_size', embedding_dim) -> (b, h, r, 1, d)"
v1.5.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.5.0,"output_shape: (batch_size, num_heads, num_relations, num_tails)"
v1.5.0,add bias term
v1.5.0,decompose convolution for faster computation in 1-n case
v1.5.0,"compute conv(stack(h, r, t))"
v1.5.0,prepare input shapes for broadcasting
v1.5.0,"(b, h, r, t, 1, d)"
v1.5.0,"conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])"
v1.5.0,"here, kernel_size = (1, 3), C_in = 1, C_out = num_filters"
v1.5.0,"-> conv_head, conv_rel, conv_tail shapes: (num_filters,)"
v1.5.0,"reshape to (1, 1, 1, 1, f, 1)"
v1.5.0,"convolve -> output.shape: (*, embedding_dim, num_filters)"
v1.5.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.5.0,"Linear layer for final scores; use flattened representations, shape: (b, h, r, t, d * f)"
v1.5.0,same shape
v1.5.0,"split, shape: (embedding_dim, hidden_dim)"
v1.5.0,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.5.0,"Predict t embedding, shape: (b, h, r, 1, d)"
v1.5.0,"transpose t, (b, 1, 1, d, t)"
v1.5.0,"dot product, (b, h, r, 1, t)"
v1.5.0,"composite: (b, h, 1, t, d)"
v1.5.0,"transpose composite: (b, h, 1, d, t)"
v1.5.0,inner product with relation embedding
v1.5.0,Circular correlation of entity embeddings
v1.5.0,complex conjugate
v1.5.0,Hadamard product in frequency domain
v1.5.0,inverse real FFT
v1.5.0,global projections
v1.5.0,"combination, shape: (b, h, r, 1, d)"
v1.5.0,"dot product with t, shape: (b, h, r, t)"
v1.5.0,r expresses a rotation in complex plane.
v1.5.0,rotate head by relation (=Hadamard product in complex space)
v1.5.0,rotate tail by inverse of relation
v1.5.0,The inverse rotation is expressed by the complex conjugate of r.
v1.5.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.5.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.5.0,|h * r - t| = |h - conj(r) * t|
v1.5.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.5.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.5.0,"That is not mentioned in the paper, so it is made optional here."
v1.5.0,Project entities
v1.5.0,h projection to hyperplane
v1.5.0,r
v1.5.0,-t projection to hyperplane
v1.5.0,project to relation specific subspace and ensure constraints
v1.5.0,x_3 contraction
v1.5.0,x_1 contraction
v1.5.0,x_2 contraction
v1.5.0,Rotate (=Hamilton product in quaternion space).
v1.5.0,Rotation in quaternion space
v1.5.0,head interaction
v1.5.0,relation interaction (notice that h has been updated)
v1.5.0,combination
v1.5.0,similarity
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Concrete classes
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,don't worry about functions because they can't be specified by JSON.
v1.5.0,Could make a better mo
v1.5.0,later could extend for other non-JSON valid types
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Score with original triples
v1.5.0,Score with inverse triples
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Create directory in which all experimental artifacts are saved
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,distribute the deteriorated triples across the remaining factories
v1.5.0,"'kinships',"
v1.5.0,"'umls',"
v1.5.0,"'codexsmall',"
v1.5.0,"'wn18',"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: Functions for specifying exotic resources with a given prefix
v1.5.0,: Functions for specifying exotic resources based on their file extension
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.5.0,TODO vectorize code
v1.5.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Split triples
v1.5.0,Sorting ensures consistent results when the triples are permuted
v1.5.0,Create mapping
v1.5.0,Sorting ensures consistent results when the triples are permuted
v1.5.0,Create mapping
v1.5.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.5.0,Filter all non-existent triples
v1.5.0,Note: Unique changes the order of the triples
v1.5.0,Note: Using unique means implicit balancing of training samples
v1.5.0,normalize input
v1.5.0,: The mapping from labels to IDs.
v1.5.0,: The inverse mapping for label_to_id; initialized automatically
v1.5.0,: A vectorized version of entity_label_to_id; initialized automatically
v1.5.0,: A vectorized version of entity_id_to_label; initialized automatically
v1.5.0,Normalize input
v1.5.0,label
v1.5.0,check new label to ID mappings
v1.5.0,Make new triples factories for each group
v1.5.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.5.0,Input validation
v1.5.0,convert to numpy
v1.5.0,Additional columns
v1.5.0,convert PyTorch tensors to numpy
v1.5.0,convert to dataframe
v1.5.0,Re-order columns
v1.5.0,Filter for entities
v1.5.0,Filter for relations
v1.5.0,No filtering happened
v1.5.0,Check if the triples are inverted already
v1.5.0,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.5.0,contained if and only if create_inverse_triples is True.
v1.5.0,Generate entity mapping if necessary
v1.5.0,Generate relation mapping if necessary
v1.5.0,Map triples of labels to triples of IDs.
v1.5.0,TODO: Check if lazy evaluation would make sense
v1.5.0,pre-filter to keep only topk
v1.5.0,if top is larger than the number of available options
v1.5.0,generate text
v1.5.0,vectorized label lookup
v1.5.0,Re-order columns
v1.5.0,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.5.0,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Split indices
v1.5.0,Split triples
v1.5.0,index
v1.5.0,select
v1.5.0,Prepare split index
v1.5.0,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.5.0,[...] is necessary for Python 3.7 compatibility
v1.5.0,While there are still triples that should be moved to the training set
v1.5.0,Pick a random triple to move over to the training triples
v1.5.0,add to training
v1.5.0,remove from testing
v1.5.0,Recalculate the move_id_mask
v1.5.0,base cases
v1.5.0,IDs not in training
v1.5.0,triples with exclusive test IDs
v1.5.0,Make sure that the first element has all the right stuff in it
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,constants
v1.5.0,constants
v1.5.0,unary
v1.5.0,binary
v1.5.0,ternary
v1.5.0,column names
v1.5.0,return candidates
v1.5.0,index triples
v1.5.0,incoming relations per entity
v1.5.0,outgoing relations per entity
v1.5.0,indexing triples for fast join r1 & r2
v1.5.0,confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support
v1.5.0,"composition r1(x, y) & r2(y, z) => r(x, z)"
v1.5.0,actual evaluation of the pattern
v1.5.0,skip empty support
v1.5.0,TODO: Can this happen after pre-filtering?
v1.5.0,"sort first, for triple order invariance"
v1.5.0,TODO: what is the support?
v1.5.0,cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on
v1.5.0,sort decreasingly. i dominates j for all j > i in x-dimension
v1.5.0,"if it is also dominated by any y, it is not part of the skyline"
v1.5.0,"group by (relation id, pattern type)"
v1.5.0,"for each group, yield from skyline"
v1.5.0,determine patterns from triples
v1.5.0,drop zero-confidence
v1.5.0,keep only skyline
v1.5.0,create data frame
v1.5.0,iterate relation types
v1.5.0,drop zero-confidence
v1.5.0,keep only skyline
v1.5.0,"does not make much sense, since there is always exactly one entry per (relation, pattern) pair"
v1.5.0,base = skyline(base)
v1.5.0,create data frame
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,": The mapped triples, shape: (num_triples, 3)"
v1.5.0,: The unique pairs
v1.5.0,: The compressed triples in CSR format
v1.5.0,convert to csr for fast row slicing
v1.5.0,: TODO: do we need these?
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,check validity
v1.5.0,path compression
v1.5.0,collect connected components using union find with path compression
v1.5.0,get representatives
v1.5.0,already merged
v1.5.0,make x the smaller one
v1.5.0,merge
v1.5.0,extract partitions
v1.5.0,safe division for empty sets
v1.5.0,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.5.0,duplicates
v1.5.0,we are not interested in self-similarity
v1.5.0,compute similarities
v1.5.0,Calculate which relations are the inverse ones
v1.5.0,get existing IDs
v1.5.0,remove non-existing ID from label mapping
v1.5.0,create translation tensor
v1.5.0,get entities and relations occurring in triples
v1.5.0,generate ID translation and new label to Id mappings
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,preprocessing
v1.5.0,initialize
v1.5.0,sample iteratively
v1.5.0,determine weights
v1.5.0,only happens at first iteration
v1.5.0,normalize to probabilities
v1.5.0,sample a start node
v1.5.0,get list of neighbors
v1.5.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.5.0,visit target node
v1.5.0,decrease sample counts
v1.5.0,return chosen edges
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.5.0,seamless loading and saving of training checkpoints
v1.5.0,Create training instances. Use the _create_instances function to allow subclasses
v1.5.0,to modify this behavior
v1.5.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.5.0,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.5.0,"If a checkpoint file is given, it must be loaded if it exists already"
v1.5.0,"If the stopper dict has any keys, those are written back to the stopper"
v1.5.0,The checkpoint frequency needs to be set to save checkpoints
v1.5.0,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.5.0,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.5.0,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.5.0,Ensure the release of memory
v1.5.0,Clear optimizer
v1.5.0,"When using early stopping models have to be saved separately at the best epoch, since the training loop will"
v1.5.0,due to the patience continue to train after the best epoch and thus alter the model
v1.5.0,Create a path
v1.5.0,Prepare all of the callbacks
v1.5.0,"Register a callback for the result tracker, if given"
v1.5.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.5.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.5.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.5.0,return the relevant parameters slice_size and batch_size
v1.5.0,Force weight initialization if training continuation is not explicitly requested.
v1.5.0,Reset the weights
v1.5.0,Create new optimizer
v1.5.0,Ensure the model is on the correct device
v1.5.0,Create Sampler
v1.5.0,Bind
v1.5.0,"When size probing, we don't want progress bars"
v1.5.0,Create progress bar
v1.5.0,Save the time to track when the saved point was available
v1.5.0,Training Loop
v1.5.0,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.5.0,Enforce training mode
v1.5.0,Accumulate loss over epoch
v1.5.0,Batching
v1.5.0,Only create a progress bar when not in size probing mode
v1.5.0,Flag to check when to quit the size probing
v1.5.0,Recall that torch *accumulates* gradients. Before passing in a
v1.5.0,"new instance, you need to zero out the gradients from the old instance"
v1.5.0,Get batch size of current batch (last batch may be incomplete)
v1.5.0,accumulate gradients for whole batch
v1.5.0,forward pass call
v1.5.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.5.0,update parameters according to optimizer
v1.5.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.5.0,constraints are no longer applied
v1.5.0,For testing purposes we're only interested in processing one batch
v1.5.0,When size probing we don't need the losses
v1.5.0,Track epoch loss
v1.5.0,Print loss information to console
v1.5.0,Save the last successful finished epoch
v1.5.0,"Since the model is also used within the stopper, its graph and cache have to be cleared"
v1.5.0,"When the stopper obtained a new best epoch, this model has to be saved for reconstruction"
v1.5.0,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.5.0,During automatic memory optimization only the error message is of interest
v1.5.0,When there wasn't a best epoch the checkpoint path should be None
v1.5.0,Delete temporary best epoch model
v1.5.0,Includes a call to result_tracker.log_metrics
v1.5.0,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.5.0,MyPy overrides are because you should
v1.5.0,When there wasn't a best epoch the checkpoint path should be None
v1.5.0,Delete temporary best epoch model
v1.5.0,"If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed"
v1.5.0,at that state
v1.5.0,Delete temporary best epoch model
v1.5.0,forward pass
v1.5.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.5.0,correction for loss reduction
v1.5.0,backward pass
v1.5.0,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.5.0,Set upper bound
v1.5.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.5.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.5.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.5.0,"Only if a cuda device is available, the random state is accessed"
v1.5.0,This is an entire checkpoint for the optional best model when using early stopping
v1.5.0,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.5.0,"If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Shuffle each epoch
v1.5.0,Lazy-splitting into batches
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Slicing is not possible in sLCWA training loops
v1.5.0,Send positive batch to device
v1.5.0,"Create negative samples, shape: (batch_size, num_neg_per_pos, 3)"
v1.5.0,apply filter mask
v1.5.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.5.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.5.0,Compute negative and positive scores
v1.5.0,Slicing is not possible for sLCWA
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: A hint for constructing a :class:`MultiTrainingCallback`
v1.5.0,: A collection of callbacks
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Split batch components
v1.5.0,Send batch to device
v1.5.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.5.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,To make MyPy happy
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,now: smaller is better
v1.5.0,: The model
v1.5.0,: The evaluator
v1.5.0,: The triples to use for training (to be used during filtered evaluation)
v1.5.0,: The triples to use for evaluation
v1.5.0,: Size of the evaluation batches
v1.5.0,: Slice size of the evaluation batches
v1.5.0,: The number of epochs after which the model is evaluated on validation set
v1.5.0,: The number of iterations (one iteration can correspond to various epochs)
v1.5.0,: with no improvement after which training will be stopped.
v1.5.0,: The name of the metric to use
v1.5.0,: The minimum relative improvement necessary to consider it an improved result
v1.5.0,: The best result so far
v1.5.0,: The epoch at which the best result occurred
v1.5.0,: The remaining patience
v1.5.0,: The metric results from all evaluations
v1.5.0,": Whether a larger value is better, or a smaller"
v1.5.0,: The result tracker
v1.5.0,: Callbacks when after results are calculated
v1.5.0,: Callbacks when training gets continued
v1.5.0,: Callbacks when training is stopped early
v1.5.0,: Did the stopper ever decide to stop?
v1.5.0,TODO: Fix this
v1.5.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.5.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.5.0,Evaluate
v1.5.0,Only perform time consuming checks for the first call.
v1.5.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.5.0,Append to history
v1.5.0,check for improvement
v1.5.0,Stop if the result did not improve more than delta for patience evaluations
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Utils
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.5.0,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.5.0,Clear the ranks from the current evaluator
v1.5.0,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.5.0,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.5.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.5.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.5.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.5.0,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.5.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.5.0,values_dict[key] will always be an int at this point
v1.5.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.5.0,Test if slicing is implemented for the required functions of this model
v1.5.0,Split batch
v1.5.0,Bind shape
v1.5.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.5.0,Warn if all entities will be filtered
v1.5.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.5.0,verify that the triples have been filtered
v1.5.0,Send to device
v1.5.0,Ensure evaluation mode
v1.5.0,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.5.0,Check whether we need to be prepared for filtering
v1.5.0,Check whether an evaluator needs access to the masks
v1.5.0,This can only be an unfiltered evaluator.
v1.5.0,Prepare for result filtering
v1.5.0,Send tensors to device
v1.5.0,Prepare batches
v1.5.0,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.5.0,Show progressbar
v1.5.0,Flag to check when to quit the size probing
v1.5.0,Disable gradient tracking
v1.5.0,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.5.0,batch-wise processing
v1.5.0,If we only probe sizes we do not need more than one batch
v1.5.0,Finalize
v1.5.0,Predict scores once
v1.5.0,Select scores of true
v1.5.0,Create positive filter for all corrupted
v1.5.0,Needs all positive triples
v1.5.0,Create filter
v1.5.0,Create a positive mask with the size of the scores from the positive filter
v1.5.0,Restrict to entities of interest
v1.5.0,Evaluate metrics on these *unfiltered* scores
v1.5.0,Filter
v1.5.0,The scores for the true triples have to be rewritten to the scores tensor
v1.5.0,Restrict to entities of interest
v1.5.0,Evaluate metrics on these *filtered* scores
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The area under the ROC curve
v1.5.0,: The area under the precision-recall curve
v1.5.0,: The coverage error
v1.5.0,coverage_error: float = field(metadata=dict(
v1.5.0,"doc='The coverage error',"
v1.5.0,"f=metrics.coverage_error,"
v1.5.0,))
v1.5.0,: The label ranking loss (APS)
v1.5.0,label_ranking_average_precision_score: float = field(metadata=dict(
v1.5.0,"doc='The label ranking loss (APS)',"
v1.5.0,"f=metrics.label_ranking_average_precision_score,"
v1.5.0,))
v1.5.0,#: The label ranking loss
v1.5.0,label_ranking_loss: float = field(metadata=dict(
v1.5.0,"doc='The label ranking loss',"
v1.5.0,"f=metrics.label_ranking_loss,"
v1.5.0,))
v1.5.0,Transfer to cpu and convert to numpy
v1.5.0,Ensure that each key gets counted only once
v1.5.0,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.5.0,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.5.0,masks using the exact same key order.
v1.5.0,TODO how to define a cutoff on y_scores to make binary?
v1.5.0,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.5.0,Clear buffers
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Extra stats stuff
v1.5.0,The optimistic rank is the rank when assuming all options with an equal score are placed behind the currently
v1.5.0,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.5.0,The pessimistic rank is the rank when assuming all options with an equal score are placed in front of the
v1.5.0,"currently considered. Hence, the rank is the number of options which have at least the same score minus one"
v1.5.0,"(as the currently considered option in included in all options). As the rank is one-based, we have to add 1,"
v1.5.0,"which nullifies the ""minus 1"" from before."
v1.5.0,"The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank over"
v1.5.0,all permutations of the elements with the same score as the currently considered option.
v1.5.0,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.5.0,The expected rank of a random scoring
v1.5.0,Check if it a side or rank type
v1.5.0,update old names for metrics and handle spaces
v1.5.0,"otherwise, assume is hits@k, which is handled differently"
v1.5.0,Adjusted mean rank calculation
v1.5.0,Clear buffers
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.5.0,to create the class
v1.5.0,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default regularizer class
v1.5.0,: The default parameters for the default regularizer class
v1.5.0,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.5.0,Make sure that all modules with parameters do have a reset_parameters method.
v1.5.0,Recursively visit all sub-modules
v1.5.0,skip self
v1.5.0,Track parents for blaming
v1.5.0,call reset_parameters if possible
v1.5.0,initialize from bottom to top
v1.5.0,This ensures that specialized initializations will take priority over the default ones of its components.
v1.5.0,emit warning if there where parameters which were not initialised by reset_parameters.
v1.5.0,Additional debug information
v1.5.0,Important: use ModuleList to ensure that Pytorch correctly handles their devices and parameters
v1.5.0,: The entity representations
v1.5.0,: The relation representations
v1.5.0,: The weight regularizers
v1.5.0,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.5.0,"model.modules(). Thereby, we can collect them automatically."
v1.5.0,Explicitly call reset_parameters to trigger initialization
v1.5.0,normalize input
v1.5.0,normalization
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Train a model (quickly)
v1.5.0,Get scores for *all* triples
v1.5.0,Get scores for top 15 triples
v1.5.0,initialize buffer on cpu
v1.5.0,calculate batch scores
v1.5.0,Explicitly create triples
v1.5.0,initialize buffer on device
v1.5.0,calculate batch scores
v1.5.0,get top scores within batch
v1.5.0,append to global top scores
v1.5.0,reduce size if necessary
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The device on which this model and its submodules are stored
v1.5.0,: The default loss function class
v1.5.0,: The default parameters for the default loss function class
v1.5.0,: The instance of the loss
v1.5.0,Initialize the device
v1.5.0,Random seeds have to set before the embeddings are initialized
v1.5.0,Loss
v1.5.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.5.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.5.0,Id of inverse relation: relation + 1
v1.5.0,: The default regularizer class
v1.5.0,: The default parameters for the default regularizer class
v1.5.0,: The instance of the regularizer
v1.5.0,Regularizer
v1.5.0,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.5.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.5.0,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.5.0,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.5.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.5.0,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.5.0,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.5.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.5.0,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.5.0,"make sure to call this first, to reset regularizer state!"
v1.5.0,: Primary embeddings for entities
v1.5.0,: Primary embeddings for relations
v1.5.0,"make sure to call this first, to reset regularizer state!"
v1.5.0,The following lines add in a post-init hook to all subclasses
v1.5.0,such that the reset_parameters_() function is run
v1.5.0,"sorry mypy, but this kind of evil must be permitted."
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Base Models
v1.5.0,Concrete Models
v1.5.0,Utils
v1.5.0,We might be able to relax this later
v1.5.0,Old style models should never be looked up
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,TODO rethink after RGCN update
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default loss function class
v1.5.0,: The default parameters for the default loss function class
v1.5.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.5.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.5.0,ConvE should be trained with inverse triples
v1.5.0,ConvE uses one bias for each entity
v1.5.0,Automatic calculation of remaining dimensions
v1.5.0,Parameter need to fulfil:
v1.5.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.5.0,weights
v1.5.0,"batch_size, num_input_channels, 2*height, width"
v1.5.0,"batch_size, num_input_channels, 2*height, width"
v1.5.0,"batch_size, num_input_channels, 2*height, width"
v1.5.0,"(N,C_out,H_out,W_out)"
v1.5.0,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.5.0,Embedding Regularization
v1.5.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.5.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.5.0,Embedding Regularization
v1.5.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.5.0,Embedding Regularization
v1.5.0,Code to repeat each item successively instead of the entire tensor
v1.5.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,Embeddings
v1.5.0,Initialise relation embeddings to unit length
v1.5.0,Get embeddings
v1.5.0,Project entities
v1.5.0,Get embeddings
v1.5.0,Project entities
v1.5.0,Project entities
v1.5.0,Project entities
v1.5.0,Get embeddings
v1.5.0,Project entities
v1.5.0,Project entities
v1.5.0,Project entities
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default loss function class
v1.5.0,: The default parameters for the default loss function class
v1.5.0,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.5.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.5.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.5.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.5.0,split into real and imaginary part
v1.5.0,ComplEx space bilinear product
v1.5.0,*: Elementwise multiplication
v1.5.0,get embeddings
v1.5.0,Regularization
v1.5.0,Compute scores
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.5.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.5.0,: a normalized weight of 10 is used.
v1.5.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.5.0,Get embeddings
v1.5.0,"shape: (b, d)"
v1.5.0,"shape: (b, d, d)"
v1.5.0,"shape: (b, d)"
v1.5.0,Compute scores
v1.5.0,Regularization
v1.5.0,Compute scores
v1.5.0,Regularization
v1.5.0,Get embeddings
v1.5.0,Compute scores
v1.5.0,Regularization
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.5.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.5.0,The interaction model
v1.5.0,embeddings
v1.5.0,Use Xavier initialization for weight; bias to zero
v1.5.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.5.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.5.0,Output layer regularization
v1.5.0,In the code base only the weights of the output layer are used for regularization
v1.5.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.5.0,Stack to convolution input
v1.5.0,Convolution
v1.5.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.5.0,Linear layer for final scores
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,comment:
v1.5.0,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.5.0,uses float64
v1.5.0,entity bias for head
v1.5.0,entity bias for tail
v1.5.0,relation offset
v1.5.0,diagonal relation transformation matrix
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default entity normalizer parameters
v1.5.0,: The entity representations are normalized to L2 unit length
v1.5.0,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.5.0,"update initializer settings, cf."
v1.5.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.5.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.5.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.5.0,in the original implementation the embeddings are initialized in one parameter
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,": shape: (batch_size, num_entities, d)"
v1.5.0,": Prepare h: (b, e, d) -> (b, e, 1, 1, d)"
v1.5.0,": Prepare t: (b, e, d) -> (b, e, 1, d, 1)"
v1.5.0,": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)"
v1.5.0,"h.T @ W @ t, shape: (b, e, k, 1, 1)"
v1.5.0,": reduce (b, e, k, 1, 1) -> (b, e, k)"
v1.5.0,": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.5.0,": Prepare h: (b, e, d) -> (b, e, d, 1)"
v1.5.0,"V_h @ h, shape: (b, e, k, 1)"
v1.5.0,": reduce (b, e, k, 1) -> (b, e, k)"
v1.5.0,": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.5.0,": Prepare t: (b, e, d) -> (b, e, d, 1)"
v1.5.0,"V_t @ t, shape: (b, e, k, 1)"
v1.5.0,": reduce (b, e, k, 1) -> (b, e, k)"
v1.5.0,": Prepare b: (R, k) -> (b, k) -> (b, 1, k)"
v1.5.0,"a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)"
v1.5.0,"prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)"
v1.5.0,"prepare act: (b, e, k) -> (b, e, 1, k)"
v1.5.0,"compute score, shape: (b, e, 1, 1)"
v1.5.0,reduce
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The regularizer used by [yang2014]_ for DistMult
v1.5.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.5.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.5.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.5.0,: The LP settings used by [yang2014]_ for DistMult
v1.5.0,Bilinear product
v1.5.0,*: Elementwise multiplication
v1.5.0,Get embeddings
v1.5.0,Compute score
v1.5.0,Only regularize relation embeddings
v1.5.0,Get embeddings
v1.5.0,Rank against all entities
v1.5.0,Only regularize relation embeddings
v1.5.0,Get embeddings
v1.5.0,Rank against all entities
v1.5.0,Only regularize relation embeddings
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default settings for the entity constrainer
v1.5.0,Similarity function used for distributions
v1.5.0,element-wise covariance bounds
v1.5.0,Additional covariance embeddings
v1.5.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.5.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.5.0,Constraints are applied through post_parameter_update
v1.5.0,Get embeddings
v1.5.0,Compute entity distribution
v1.5.0,: a = \mu^T\Sigma^{-1}\mu
v1.5.0,: b = \log \det \Sigma
v1.5.0,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.5.0,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.5.0,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.5.0,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The custom regularizer used by [wang2014]_ for TransH
v1.5.0,: The settings used by [wang2014]_ for TransH
v1.5.0,embeddings
v1.5.0,Normalise the normal vectors by their l2 norms
v1.5.0,TODO: Add initialization
v1.5.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.5.0,which enforces the defined soft constraints.
v1.5.0,Get embeddings
v1.5.0,Project to hyperplane
v1.5.0,Regularization term
v1.5.0,Get embeddings
v1.5.0,Project to hyperplane
v1.5.0,Regularization term
v1.5.0,Get embeddings
v1.5.0,Project to hyperplane
v1.5.0,Regularization term
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,TODO: Initialize from TransE
v1.5.0,embeddings
v1.5.0,"project to relation specific subspace, shape: (b, e, d_r)"
v1.5.0,ensure constraints
v1.5.0,"evaluate score function, shape: (b, e)"
v1.5.0,Get embeddings
v1.5.0,Get embeddings
v1.5.0,Get embeddings
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,": The default strategy for optimizing the model""s hyper-parameters"
v1.5.0,TODO: Decomposition kwargs
v1.5.0,"num_bases=dict(type=int, low=2, high=100, q=1),"
v1.5.0,"num_blocks=dict(type=int, low=2, high=20, q=1),"
v1.5.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.5.0,create enriched entity representations
v1.5.0,Resolve interaction function
v1.5.0,set default relation representation
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,combined representation
v1.5.0,Resolve interaction function
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default loss function class
v1.5.0,: The default parameters for the default loss function class
v1.5.0,Core tensor
v1.5.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.5.0,Dropout
v1.5.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.5.0,Abbreviation
v1.5.0,Compute h_n = DO(BN(h))
v1.5.0,Compute wr = DO(W x_2 r)
v1.5.0,compute whr = DO(BN(h_n x_1 wr))
v1.5.0,Compute whr x_3 t
v1.5.0,Get embeddings
v1.5.0,Compute scores
v1.5.0,Get embeddings
v1.5.0,Compute scores
v1.5.0,Get embeddings
v1.5.0,Compute scores
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,Get embeddings
v1.5.0,TODO: Use torch.dist
v1.5.0,Get embeddings
v1.5.0,TODO: Use torch.cdist
v1.5.0,Get embeddings
v1.5.0,TODO: Use torch.cdist
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default loss function class
v1.5.0,: The default parameters for the default loss function class
v1.5.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.5.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.5.0,": regularization term by the number of elements, which is 200."
v1.5.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.5.0,extra embeddings
v1.5.0,forward model
v1.5.0,Regularization
v1.5.0,backward model
v1.5.0,Regularization
v1.5.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.5.0,"That is not mentioned in the paper, so it is omitted here."
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.5.0,weight initialization
v1.5.0,Get embeddings
v1.5.0,Embedding Regularization
v1.5.0,Concatenate them
v1.5.0,Compute scores
v1.5.0,Get embeddings
v1.5.0,Embedding Regularization
v1.5.0,First layer can be unrolled
v1.5.0,Send scores through rest of the network
v1.5.0,Get embeddings
v1.5.0,Embedding Regularization
v1.5.0,First layer can be unrolled
v1.5.0,Send scores through rest of the network
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,The dimensions affected by e'
v1.5.0,Project entities
v1.5.0,r_p (e_p.T e) + e'
v1.5.0,Enforce constraints
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: Secondary embeddings for entities
v1.5.0,: Secondary embeddings for relations
v1.5.0,Project entities
v1.5.0,score = -||h_bot + r - t_bot||_2^2
v1.5.0,Head
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,Regular relation embeddings
v1.5.0,The relation-specific interaction vector
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,Decompose into real and imaginary part
v1.5.0,Rotate (=Hadamard product in complex space).
v1.5.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.5.0,Get embeddings
v1.5.0,Compute scores
v1.5.0,Embedding Regularization
v1.5.0,Get embeddings
v1.5.0,Rank against all entities
v1.5.0,Compute scores
v1.5.0,Embedding Regularization
v1.5.0,Get embeddings
v1.5.0,r expresses a rotation in complex plane.
v1.5.0,The inverse rotation is expressed by the complex conjugate of r.
v1.5.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.5.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.5.0,|h * r - t| = |h - conj(r) * t|
v1.5.0,Rank against all entities
v1.5.0,Compute scores
v1.5.0,Embedding Regularization
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default loss function class
v1.5.0,: The default parameters for the default loss function class
v1.5.0,Global entity projection
v1.5.0,Global relation projection
v1.5.0,Global combination bias
v1.5.0,Global combination bias
v1.5.0,Get embeddings
v1.5.0,Compute score
v1.5.0,Get embeddings
v1.5.0,Rank against all entities
v1.5.0,Get embeddings
v1.5.0,Rank against all entities
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Normalize relation embeddings
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default loss function class
v1.5.0,: The default parameters for the default loss function class
v1.5.0,: The LP settings used by [zhang2019]_ for QuatE.
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default settings for the entity constrainer
v1.5.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.5.0,Circular correlation of entity embeddings
v1.5.0,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.5.0,compatibility: new style fft returns complex tensor
v1.5.0,Hadamard product in frequency domain
v1.5.0,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.5.0,inner product with relation embedding
v1.5.0,Embedding Regularization
v1.5.0,Embedding Regularization
v1.5.0,Embedding Regularization
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default loss function class
v1.5.0,: The default parameters for the default loss function class
v1.5.0,Get embeddings
v1.5.0,Embedding Regularization
v1.5.0,Concatenate them
v1.5.0,Predict t embedding
v1.5.0,compare with all t's
v1.5.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.5.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.5.0,Embedding Regularization
v1.5.0,Concatenate them
v1.5.0,Predict t embedding
v1.5.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.5.0,Embedding Regularization
v1.5.0,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.5.0,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.5.0,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.5.0,Extend t
v1.5.0,Concatenate them
v1.5.0,Predict t embedding
v1.5.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.5.0,The results have to be realigned with the expected output of the score_h function
v1.5.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default loss function class
v1.5.0,: The default parameters for the default loss function class
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the model's hyper-parameters
v1.5.0,: The default parameters for the default loss function class
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,"if we really need access to the path later, we can expose it as a property"
v1.5.0,via self.writer.log_dir
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The WANDB run
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Base classes
v1.5.0,Concrete classes
v1.5.0,Utilities
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The file extension for this writer (do not include dot)
v1.5.0,: The file where the results are written to.
v1.5.0,as_uri() requires the path to be absolute. resolve additionally also normalizes the path
v1.5.0,: The column names
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,store set of triples
v1.5.0,: some prime numbers for tuple hashing
v1.5.0,: The bit-array for the Bloom filter data structure
v1.5.0,Allocate bit array
v1.5.0,calculate number of hashing rounds
v1.5.0,index triples
v1.5.0,Store some meta-data
v1.5.0,pre-hash
v1.5.0,cf. https://github.com/skeeto/hash-prospector#two-round-functions
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Set the indices
v1.5.0,Bind number of negatives to sample
v1.5.0,Equally corrupt all sides
v1.5.0,Copy positive batch for corruption.
v1.5.0,"Do not detach, as no gradients should flow into the indices."
v1.5.0,Relations have a different index maximum than entities
v1.5.0,At least make sure to not replace the triples by the original value
v1.5.0,"To make sure we don't replace the {head, relation, tail} by the"
v1.5.0,original value we shift all values greater or equal than the original value by one up
v1.5.0,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.5.0,: A filterer for negative batches
v1.5.0,create unfiltered negative batch by corruption
v1.5.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Utils
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,TODO: move this warning to PseudoTypeNegativeSampler's constructor?
v1.5.0,create index structure
v1.5.0,": The array of offsets within the data array, shape: (2 * num_relations + 1,)"
v1.5.0,: The concatenated sorted sets of head/tail entities
v1.5.0,"shape: (batch_size, num_neg_per_pos, 3)"
v1.5.0,Uniformly sample from head/tail offsets
v1.5.0,get corresponding entity
v1.5.0,"and position within triple (0: head, 2: tail)"
v1.5.0,write into negative batch
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Preprocessing: Compute corruption probabilities
v1.5.0,"compute tph, i.e. the average number of tail entities per head"
v1.5.0,"compute hpt, i.e. the average number of head entities per tail"
v1.5.0,Set parameter for Bernoulli distribution
v1.5.0,Bind number of negatives to sample
v1.5.0,Copy positive batch for corruption.
v1.5.0,"Do not detach, as no gradients should flow into the indices."
v1.5.0,Decide whether to corrupt head or tail
v1.5.0,Tails are corrupted if heads are not corrupted
v1.5.0,We at least make sure to not replace the triples by the original value
v1.5.0,"See below for explanation of why this is on a range of [0, num_entities - 1]"
v1.5.0,Randomly sample corruption.
v1.5.0,Replace heads
v1.5.0,Replace tails
v1.5.0,To make sure we don't replace the head by the original value
v1.5.0,we shift all values greater or equal than the original value by one up
v1.5.0,"for that reason we choose the random value from [0, num_entities -1]"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The random seed used at the beginning of the pipeline
v1.5.0,: The model trained by the pipeline
v1.5.0,: The training triples
v1.5.0,: The training loop used by the pipeline
v1.5.0,: The losses during training
v1.5.0,: The results evaluated by the pipeline
v1.5.0,: How long in seconds did training take?
v1.5.0,: How long in seconds did evaluation take?
v1.5.0,: An early stopper
v1.5.0,: Any additional metadata as a dictionary
v1.5.0,: The version of PyKEEN used to create these results
v1.5.0,: The git hash of PyKEEN used to create these results
v1.5.0,TODO use pathlib here
v1.5.0,FIXME this should never happen.
v1.5.0,1. Dataset
v1.5.0,2. Model
v1.5.0,3. Loss
v1.5.0,4. Regularizer
v1.5.0,5. Optimizer
v1.5.0,6. Training Loop
v1.5.0,7. Training (ronaldo style)
v1.5.0,8. Evaluation
v1.5.0,9. Tracking
v1.5.0,Misc
v1.5.0,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.5.0,used random_seed to ensure reproducible results
v1.5.0,We have to set clear optimizer to False since training should be continued
v1.5.0,Start tracking
v1.5.0,evaluation restriction to a subset of entities/relations
v1.5.0,TODO should training be reset?
v1.5.0,TODO should kwargs for loss and regularizer be checked and raised for?
v1.5.0,Log model parameters
v1.5.0,Stopping
v1.5.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.5.0,Add logging for debugging
v1.5.0,Train like Cristiano Ronaldo
v1.5.0,Build up a list of triples if we want to be in the filtered setting
v1.5.0,"If the user gave custom ""additional_filter_triples"""
v1.5.0,Determine whether the validation triples should also be filtered while performing test evaluation
v1.5.0,TODO consider implications of duplicates
v1.5.0,Evaluate
v1.5.0,"Reuse optimal evaluation parameters from training if available, only if the validation triples are used again"
v1.5.0,Add logging about evaluator for debugging
v1.5.0,"If the evaluation still fail using the CPU, the error is raised"
v1.5.0,"When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is"
v1.5.0,restarted with PyKEEN's automatic memory optimization
v1.5.0,"When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation"
v1.5.0,is restarted using the cpu
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,TODO what happens if already exists?
v1.5.0,TODO incorporate setting of random seed
v1.5.0,pipeline_kwargs=dict(
v1.5.0,"random_seed=random_non_negative_int(),"
v1.5.0,"),"
v1.5.0,Add dataset to current_pipeline
v1.5.0,"Training, test, and validation paths are provided"
v1.5.0,Add loss function to current_pipeline
v1.5.0,Add regularizer to current_pipeline
v1.5.0,Add optimizer to current_pipeline
v1.5.0,Add training approach to current_pipeline
v1.5.0,Add evaluation
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,": The mapping from (graph-pair, side) to triple file name"
v1.5.0,: The internal dataset name
v1.5.0,: The hex digest for the zip file
v1.5.0,Input validation.
v1.5.0,For downloading
v1.5.0,For splitting
v1.5.0,Whether to create inverse triples
v1.5.0,shared directory for multiple datasets.
v1.5.0,ensure file is present
v1.5.0,TODO: Re-use ensure_from_google?
v1.5.0,read all triples from file
v1.5.0,"some ""entities"" have numeric labels"
v1.5.0,"pandas.read_csv(..., dtype=str) does not work properly."
v1.5.0,create triples factory
v1.5.0,split
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.5.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.5.0,the columns are not ordered properly.
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The name of the dataset to download
v1.5.0,FIXME these are already identifiers
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,relation typing
v1.5.0,constants
v1.5.0,unique
v1.5.0,compute over all triples
v1.5.0,Determine group key
v1.5.0,Add labels if requested
v1.5.0,TODO: Merge with _common?
v1.5.0,include hash over triples into cache-file name
v1.5.0,include part hash into cache-file name
v1.5.0,re-use cached file if possible
v1.5.0,select triples
v1.5.0,save to file
v1.5.0,Prune by support and confidence
v1.5.0,TODO: Consider merging with other analysis methods
v1.5.0,TODO: Consider merging with other analysis methods
v1.5.0,TODO: Consider merging with other analysis methods
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Raise matplotlib level
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,don't call this function by itself. assumes called through the `validation`
v1.5.0,property and the _training factory has already been loaded
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Normalize path
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: A factory wrapping the training triples
v1.5.0,": A factory wrapping the testing triples, that share indices with the training triples"
v1.5.0,": A factory wrapping the validation triples, that share indices with the training triples"
v1.5.0,: All datasets should take care of inverse triple creation
v1.5.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.5.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.5.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.5.0,: The directory in which the cached data is stored
v1.5.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.5.0,don't call this function by itself. assumes called through the `validation`
v1.5.0,property and the _training factory has already been loaded
v1.5.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.5.0,"relative paths within zip file's always follow Posix path, even on Windows"
v1.5.0,tarfile does not like pathlib
v1.5.0,: URL to the data to download
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,Concrete Classes
v1.5.0,Utilities
v1.5.0,Assume it's a file path
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,TODO update docs with table and CLI wtih generator
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,1. Dataset
v1.5.0,2. Model
v1.5.0,3. Loss
v1.5.0,4. Regularizer
v1.5.0,5. Optimizer
v1.5.0,6. Training Loop
v1.5.0,7. Training
v1.5.0,8. Evaluation
v1.5.0,9. Trackers
v1.5.0,Misc.
v1.5.0,2. Model
v1.5.0,3. Loss
v1.5.0,4. Regularizer
v1.5.0,5. Optimizer
v1.5.0,1. Dataset
v1.5.0,2. Model
v1.5.0,3. Loss
v1.5.0,4. Regularizer
v1.5.0,5. Optimizer
v1.5.0,6. Training Loop
v1.5.0,7. Training
v1.5.0,8. Evaluation
v1.5.0,9. Tracker
v1.5.0,Misc.
v1.5.0,Will trigger Optuna to set the state of the trial as failed
v1.5.0,: The :mod:`optuna` study object
v1.5.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.5.0,Output study information
v1.5.0,Output all trials
v1.5.0,Output best trial as pipeline configuration file
v1.5.0,1. Dataset
v1.5.0,2. Model
v1.5.0,3. Loss
v1.5.0,4. Regularizer
v1.5.0,5. Optimizer
v1.5.0,6. Training Loop
v1.5.0,7. Training
v1.5.0,8. Evaluation
v1.5.0,9. Tracking
v1.5.0,6. Misc
v1.5.0,Optuna Study Settings
v1.5.0,Optuna Optimization Settings
v1.5.0,0. Metadata/Provenance
v1.5.0,1. Dataset
v1.5.0,2. Model
v1.5.0,3. Loss
v1.5.0,4. Regularizer
v1.5.0,5. Optimizer
v1.5.0,6. Training Loop
v1.5.0,7. Training
v1.5.0,8. Evaluation
v1.5.0,9. Tracking
v1.5.0,1. Dataset
v1.5.0,2. Model
v1.5.0,3. Loss
v1.5.0,4. Regularizer
v1.5.0,5. Optimizer
v1.5.0,6. Training Loop
v1.5.0,7. Training
v1.5.0,8. Evaluation
v1.5.0,9. Tracker
v1.5.0,Optuna Misc.
v1.5.0,Pipeline Misc.
v1.5.0,Invoke optimization of the objective function.
v1.5.0,TODO: make it even easier to specify categorical strategies just as lists
v1.5.0,"if isinstance(info, (tuple, list, set)):"
v1.5.0,"info = dict(type='categorical', choices=list(info))"
v1.5.0,get log from info - could either be a boolean or string
v1.5.0,"otherwise, dataset refers to a file that should be automatically split"
v1.5.0,"this could be custom data, so don't store anything. However, it's possible to check if this"
v1.5.0,"was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:"
v1.5.0,dataset_name = dataset.get_normalized_name()  # this works both on instances and classes
v1.5.0,if has_dataset(dataset_name):
v1.5.0,"study.set_user_attr('dataset', dataset_name)"
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.5.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,
v1.4.0,Configuration file for the Sphinx documentation builder.
v1.4.0,
v1.4.0,This file does only contain a selection of the most common options. For a
v1.4.0,full list see the documentation:
v1.4.0,http://www.sphinx-doc.org/en/master/config
v1.4.0,-- Path setup --------------------------------------------------------------
v1.4.0,"If extensions (or modules to document with autodoc) are in another directory,"
v1.4.0,add these directories to sys.path here. If the directory is relative to the
v1.4.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.4.0,
v1.4.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.4.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.4.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.4.0,from unittest.mock import Mock
v1.4.0,sys.modules['numpy'] = Mock()
v1.4.0,sys.modules['numpy.linalg'] = Mock()
v1.4.0,sys.modules['scipy'] = Mock()
v1.4.0,sys.modules['scipy.optimize'] = Mock()
v1.4.0,sys.modules['scipy.interpolate'] = Mock()
v1.4.0,sys.modules['scipy.sparse'] = Mock()
v1.4.0,sys.modules['scipy.ndimage'] = Mock()
v1.4.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.4.0,sys.modules['tensorflow'] = Mock()
v1.4.0,sys.modules['theano'] = Mock()
v1.4.0,sys.modules['theano.tensor'] = Mock()
v1.4.0,sys.modules['torch'] = Mock()
v1.4.0,sys.modules['torch.optim'] = Mock()
v1.4.0,sys.modules['torch.nn'] = Mock()
v1.4.0,sys.modules['torch.nn.init'] = Mock()
v1.4.0,sys.modules['torch.autograd'] = Mock()
v1.4.0,sys.modules['sklearn'] = Mock()
v1.4.0,sys.modules['sklearn.model_selection'] = Mock()
v1.4.0,sys.modules['sklearn.utils'] = Mock()
v1.4.0,-- Project information -----------------------------------------------------
v1.4.0,"The full version, including alpha/beta/rc tags."
v1.4.0,The short X.Y version.
v1.4.0,-- General configuration ---------------------------------------------------
v1.4.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.4.0,
v1.4.0,needs_sphinx = '1.0'
v1.4.0,"If true, the current module name will be prepended to all description"
v1.4.0,unit titles (such as .. function::).
v1.4.0,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.4.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.4.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.4.0,ones.
v1.4.0,show todo's
v1.4.0,generate autosummary pages
v1.4.0,"Add any paths that contain templates here, relative to this directory."
v1.4.0,The suffix(es) of source filenames.
v1.4.0,You can specify multiple suffix as a list of string:
v1.4.0,
v1.4.0,"source_suffix = ['.rst', '.md']"
v1.4.0,The master toctree document.
v1.4.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.4.0,for a list of supported languages.
v1.4.0,
v1.4.0,This is also used if you do content translation via gettext catalogs.
v1.4.0,"Usually you set ""language"" from the command line for these cases."
v1.4.0,"List of patterns, relative to source directory, that match files and"
v1.4.0,directories to ignore when looking for source files.
v1.4.0,This pattern also affects html_static_path and html_extra_path.
v1.4.0,The name of the Pygments (syntax highlighting) style to use.
v1.4.0,-- Options for HTML output -------------------------------------------------
v1.4.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.4.0,a list of builtin themes.
v1.4.0,
v1.4.0,Theme options are theme-specific and customize the look and feel of a theme
v1.4.0,"further.  For a list of options available for each theme, see the"
v1.4.0,documentation.
v1.4.0,
v1.4.0,html_theme_options = {}
v1.4.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.4.0,"relative to this directory. They are copied after the builtin static files,"
v1.4.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.4.0,html_static_path = ['_static']
v1.4.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.4.0,to template names.
v1.4.0,
v1.4.0,The default sidebars (for documents that don't match any pattern) are
v1.4.0,defined by theme itself.  Builtin themes are using these templates by
v1.4.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.4.0,'searchbox.html']``.
v1.4.0,
v1.4.0,html_sidebars = {}
v1.4.0,The name of an image file (relative to this directory) to place at the top
v1.4.0,of the sidebar.
v1.4.0,
v1.4.0,-- Options for HTMLHelp output ---------------------------------------------
v1.4.0,Output file base name for HTML help builder.
v1.4.0,-- Options for LaTeX output ------------------------------------------------
v1.4.0,latex_elements = {
v1.4.0,The paper size ('letterpaper' or 'a4paper').
v1.4.0,
v1.4.0,"'papersize': 'letterpaper',"
v1.4.0,
v1.4.0,"The font size ('10pt', '11pt' or '12pt')."
v1.4.0,
v1.4.0,"'pointsize': '10pt',"
v1.4.0,
v1.4.0,Additional stuff for the LaTeX preamble.
v1.4.0,
v1.4.0,"'preamble': '',"
v1.4.0,
v1.4.0,Latex figure (float) alignment
v1.4.0,
v1.4.0,"'figure_align': 'htbp',"
v1.4.0,}
v1.4.0,Grouping the document tree into LaTeX files. List of tuples
v1.4.0,"(source start file, target name, title,"
v1.4.0,"author, documentclass [howto, manual, or own class])."
v1.4.0,latex_documents = [
v1.4.0,(
v1.4.0,"master_doc,"
v1.4.0,"'pykeen.tex',"
v1.4.0,"'PyKEEN Documentation',"
v1.4.0,"author,"
v1.4.0,"'manual',"
v1.4.0,"),"
v1.4.0,]
v1.4.0,-- Options for manual page output ------------------------------------------
v1.4.0,One entry per manual page. List of tuples
v1.4.0,"(source start file, name, description, authors, manual section)."
v1.4.0,-- Options for Texinfo output ----------------------------------------------
v1.4.0,Grouping the document tree into Texinfo files. List of tuples
v1.4.0,"(source start file, target name, title, author,"
v1.4.0,"dir menu entry, description, category)"
v1.4.0,-- Options for Epub output -------------------------------------------------
v1.4.0,Bibliographic Dublin Core info.
v1.4.0,epub_title = project
v1.4.0,The unique identifier of the text. This can be a ISBN number
v1.4.0,or the project homepage.
v1.4.0,
v1.4.0,epub_identifier = ''
v1.4.0,A unique identification for the text.
v1.4.0,
v1.4.0,epub_uid = ''
v1.4.0,A list of files that should not be packed into the epub file.
v1.4.0,epub_exclude_files = ['search.html']
v1.4.0,-- Extension configuration -------------------------------------------------
v1.4.0,-- Options for intersphinx extension ---------------------------------------
v1.4.0,Example configuration for intersphinx: refer to the Python standard library.
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Check a model param is optimized
v1.4.0,Check a loss param is optimized
v1.4.0,Check a model param is NOT optimized
v1.4.0,Check a loss param is optimized
v1.4.0,Check a model param is optimized
v1.4.0,Check a loss param is NOT optimized
v1.4.0,Check a model param is NOT optimized
v1.4.0,Check a loss param is NOT optimized
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,check for empty batches
v1.4.0,Train a model in one shot
v1.4.0,Train a model for the first half
v1.4.0,Continue training of the first part
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,The triples factory and model
v1.4.0,: The evaluator to be tested
v1.4.0,Settings
v1.4.0,: The evaluator instantiation
v1.4.0,Settings
v1.4.0,Initialize evaluator
v1.4.0,Use small test dataset
v1.4.0,Use small model (untrained)
v1.4.0,Get batch
v1.4.0,Compute scores
v1.4.0,Compute mask only if required
v1.4.0,TODO: Re-use filtering code
v1.4.0,"shape: (batch_size, num_triples)"
v1.4.0,"shape: (batch_size, num_entities)"
v1.4.0,Process one batch
v1.4.0,Check for correct class
v1.4.0,Check value ranges
v1.4.0,TODO: Validate with data?
v1.4.0,Check for correct class
v1.4.0,check value
v1.4.0,filtering
v1.4.0,"true_score: (2, 3, 3)"
v1.4.0,head based filter
v1.4.0,preprocessing for faster lookup
v1.4.0,check that all found positives are positive
v1.4.0,check in-place
v1.4.0,Test head scores
v1.4.0,Assert in-place modification
v1.4.0,Assert correct filtering
v1.4.0,Test tail scores
v1.4.0,Assert in-place modification
v1.4.0,Assert correct filtering
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,check if within 0.5 std of observed
v1.4.0,test error is raised
v1.4.0,Tests that exception will be thrown when more than or less than three tensors are passed
v1.4.0,Test that regularization term is computed correctly
v1.4.0,Entity soft constraint
v1.4.0,Orthogonality soft constraint
v1.4.0,ensure regularizer is on correct device
v1.4.0,"After first update, should change the term"
v1.4.0,"After second update, no change should happen"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,create broadcastable shapes
v1.4.0,check correct value range
v1.4.0,check maximum norm constraint
v1.4.0,unchanged values for small norms
v1.4.0,generate random query tensor
v1.4.0,random entity embeddings & projections
v1.4.0,random relation embeddings & projections
v1.4.0,project
v1.4.0,check shape:
v1.4.0,check normalization
v1.4.0,check equivalence of re-formulation
v1.4.0,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.4.0,= r_p (e_p^T e) + e'
v1.4.0,"create random array, estimate the costs of addition, and measure some execution times."
v1.4.0,"then, compute correlation between the estimated cost, and the measured time."
v1.4.0,check for strong correlation between estimated costs and measured execution time
v1.4.0,get optimal sequence
v1.4.0,check caching
v1.4.0,get optimal sequence
v1.4.0,check correct cost
v1.4.0,check optimality
v1.4.0,compare result to sequential addition
v1.4.0,compare result to sequential addition
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,equal value; larger is better
v1.4.0,equal value; smaller is better
v1.4.0,larger is better; improvement
v1.4.0,larger is better; improvement; but not significant
v1.4.0,: The window size used by the early stopper
v1.4.0,: The mock losses the mock evaluator will return
v1.4.0,: The (zeroed) index  - 1 at which stopping will occur
v1.4.0,: The minimum improvement
v1.4.0,: The best results
v1.4.0,Set automatic_memory_optimization to false for tests
v1.4.0,Step early stopper
v1.4.0,check storing of results
v1.4.0,check ring buffer
v1.4.0,: The window size used by the early stopper
v1.4.0,: The (zeroed) index  - 1 at which stopping will occur
v1.4.0,: The minimum improvement
v1.4.0,: The random seed to use for reproducibility
v1.4.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.4.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.4.0,: The batch size to use.
v1.4.0,Fix seed for reproducibility
v1.4.0,Set automatic_memory_optimization to false during testing
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.4.0,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.4.0,"first triple, and passing create_inverse_triples=True."
v1.4.0,check for warning
v1.4.0,check for filtered triples
v1.4.0,check for correct inverse triples flag
v1.4.0,check correct translation
v1.4.0,check column order
v1.4.0,apply restriction
v1.4.0,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.4.0,check that inverse_triples is correctly carried over
v1.4.0,verify that the label-to-ID mapping has not been changed
v1.4.0,verify that triples have been filtered
v1.4.0,check compressed triples
v1.4.0,reconstruct triples from compressed form
v1.4.0,check data loader
v1.4.0,set create inverse triple to true
v1.4.0,split factory
v1.4.0,check that in *training* inverse triple are to be created
v1.4.0,check that in all other splits no inverse triples are to be created
v1.4.0,verify that all entities and relations are present in the training factory
v1.4.0,verify that no triple got lost
v1.4.0,verify that the label-to-id mappings match
v1.4.0,check type
v1.4.0,check format
v1.4.0,check coverage
v1.4.0,Check if multilabels are working correctly
v1.4.0,generate random ratios
v1.4.0,check size
v1.4.0,check value range
v1.4.0,check total split
v1.4.0,check consistency with ratios
v1.4.0,the number of decimal digits equivalent to 1 / n_total
v1.4.0,check type
v1.4.0,check values
v1.4.0,compare against expected
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The batch size
v1.4.0,: The random seed
v1.4.0,: The triples factory
v1.4.0,: The sLCWA instances
v1.4.0,: Class of negative sampling to test
v1.4.0,": The negative sampler instance, initialized in setUp"
v1.4.0,: A positive batch
v1.4.0,Generate negative sample
v1.4.0,check shape
v1.4.0,check bounds: heads
v1.4.0,check bounds: relations
v1.4.0,check bounds: tails
v1.4.0,Check that all elements got corrupted
v1.4.0,Check whether filtering works correctly
v1.4.0,First giving an example where all triples have to be filtered
v1.4.0,The filter should remove all triples
v1.4.0,Create an example where no triples will be filtered
v1.4.0,The filter should not remove any triple
v1.4.0,Generate scaled negative sample
v1.4.0,Generate negative samples
v1.4.0,test that the relations were not changed
v1.4.0,Test that half of the subjects and half of the objects are corrupted
v1.4.0,Generate negative sample for additional tests
v1.4.0,test that the relations were not changed
v1.4.0,sample a batch
v1.4.0,check shape
v1.4.0,get triples
v1.4.0,check connected components
v1.4.0,super inefficient
v1.4.0,join
v1.4.0,already joined
v1.4.0,check that there is only a single component
v1.4.0,check content of comp_adj_lists
v1.4.0,check edge ids
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,3x batch norm: bias + scale --> 6
v1.4.0,entity specific bias        --> 1
v1.4.0,==================================
v1.4.0,7
v1.4.0,"two bias terms, one conv-filter"
v1.4.0,check type
v1.4.0,check shape
v1.4.0,check ID ranges
v1.4.0,this is only done in one of the models
v1.4.0,this is only done in one of the models
v1.4.0,Two linear layer biases
v1.4.0,"Two BN layers, bias & scale"
v1.4.0,: one bias per layer
v1.4.0,: (scale & bias for BN) * layers
v1.4.0,entity embeddings
v1.4.0,relation embeddings
v1.4.0,Compute Scores
v1.4.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.4.0,relation embeddings
v1.4.0,Compute Scores
v1.4.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.4.0,entity embeddings
v1.4.0,relation embeddings
v1.4.0,Compute Scores
v1.4.0,random entity embeddings & projections
v1.4.0,random relation embeddings & projections
v1.4.0,project
v1.4.0,check shape:
v1.4.0,check normalization
v1.4.0,entity embeddings
v1.4.0,relation embeddings
v1.4.0,Compute Scores
v1.4.0,second_score = scores[1].item()
v1.4.0,: 2xBN (bias & scale)
v1.4.0,: The number of entities
v1.4.0,: The number of triples
v1.4.0,check shape
v1.4.0,check dtype
v1.4.0,check finite values (e.g. due to division by zero)
v1.4.0,check non-negativity
v1.4.0,check shape
v1.4.0,check content
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.4.0,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.4.0,pipeline is checked.
v1.4.0,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.4.0,Resume the previous pipeline
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.4.0,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.4.0,Test looking up inverse triples
v1.4.0,test new label to ID
v1.4.0,type
v1.4.0,old labels
v1.4.0,"new, compact IDs"
v1.4.0,test vectorized lookup
v1.4.0,type
v1.4.0,shape
v1.4.0,value range
v1.4.0,only occurring Ids get mapped to non-negative numbers
v1.4.0,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.4.0,check type
v1.4.0,check shape
v1.4.0,check content
v1.4.0,check type
v1.4.0,check shape
v1.4.0,check 1-hot
v1.4.0,check type
v1.4.0,check shape
v1.4.0,check value range
v1.4.0,check self-similarity = 1
v1.4.0,base relation
v1.4.0,exact duplicate
v1.4.0,99% duplicate
v1.4.0,50% duplicate
v1.4.0,exact inverse
v1.4.0,99% inverse
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,fix seeds for reproducibility
v1.4.0,: The expected number of entities
v1.4.0,: The expected number of relations
v1.4.0,: The expected number of triples
v1.4.0,": The tolerance on expected number of triples, for randomized situations"
v1.4.0,: The dataset to test
v1.4.0,: The instantiated dataset
v1.4.0,: Should the validation be assumed to have been loaded with train/test?
v1.4.0,Not loaded
v1.4.0,Load
v1.4.0,Test caching
v1.4.0,assert (end - start) < 1.0e-02
v1.4.0,": The directory, if there is caching"
v1.4.0,TODO update
v1.4.0,: The batch size
v1.4.0,test reduction
v1.4.0,test finite loss value
v1.4.0,Test backward
v1.4.0,TODO update
v1.4.0,: The number of entities.
v1.4.0,TODO update
v1.4.0,: The number of negative samples
v1.4.0,TODO update
v1.4.0,: The number of entities.
v1.4.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.4.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.4.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.4.0,check whether the error originates from batch norm for single element batches
v1.4.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.4.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.4.0,test multiple different initializations
v1.4.0,calculate by functional
v1.4.0,calculate manually
v1.4.0,simple
v1.4.0,nested
v1.4.0,nested
v1.4.0,prepare a temporary test directory
v1.4.0,check that file was created
v1.4.0,make sure to close file before trying to delete it
v1.4.0,delete intermediate files
v1.4.0,: The batch size
v1.4.0,: The triples factory
v1.4.0,: Class of regularizer to test
v1.4.0,: The constructor parameters to pass to the regularizer
v1.4.0,": The regularizer instance, initialized in setUp"
v1.4.0,: A positive batch
v1.4.0,: The device
v1.4.0,move test instance to device
v1.4.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.4.0,Check if regularizer is stored correctly.
v1.4.0,Forward pass (should update regularizer)
v1.4.0,Call post_parameter_update (should reset regularizer)
v1.4.0,Check if regularization term is reset
v1.4.0,Call method
v1.4.0,Generate random tensors
v1.4.0,Call update
v1.4.0,check shape
v1.4.0,compute expected term
v1.4.0,Generate random tensor
v1.4.0,calculate penalty
v1.4.0,check shape
v1.4.0,check value
v1.4.0,FIXME isn't any finite number allowed now?
v1.4.0,: The class of the model to test
v1.4.0,: Additional arguments passed to the model's constructor method
v1.4.0,: Additional arguments passed to the training loop's constructor method
v1.4.0,: The triples factory instance
v1.4.0,: The model instance
v1.4.0,: The batch size for use for forward_* tests
v1.4.0,: The embedding dimensionality
v1.4.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.4.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.4.0,: The batch size for use when testing training procedures
v1.4.0,: The number of epochs to train the model
v1.4.0,: A random number generator from torch
v1.4.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.4.0,initialization
v1.4.0,assert there is at least one trainable parameter
v1.4.0,Check that all the parameters actually require a gradient
v1.4.0,Try to initialize an optimizer
v1.4.0,get model parameters
v1.4.0,re-initialize
v1.4.0,check that the operation works in-place
v1.4.0,check that the parameters where modified
v1.4.0,check for finite values by default
v1.4.0,check whether a gradient can be back-propgated
v1.4.0,"assert batch comprises (head, relation) pairs"
v1.4.0,"assert batch comprises (relation, tail) pairs"
v1.4.0,"For the high/low memory test cases of NTN, SE, etc."
v1.4.0,"else, leave to default"
v1.4.0,TODO: Catch HolE MKL error?
v1.4.0,set regularizer term to something that isn't zero
v1.4.0,call post_parameter_update
v1.4.0,assert that the regularization term has been reset
v1.4.0,do one optimization step
v1.4.0,call post_parameter_update
v1.4.0,check model constraints
v1.4.0,"assert batch comprises (relation, tail) pairs"
v1.4.0,"assert batch comprises (relation, tail) pairs"
v1.4.0,"assert batch comprises (relation, tail) pairs"
v1.4.0,call some functions
v1.4.0,reset to old state
v1.4.0,call some functions
v1.4.0,reset to old state
v1.4.0,Distance-based model
v1.4.0,check type
v1.4.0,check shape
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,check for finite values by default
v1.4.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.4.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.4.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.4.0,Get embeddings
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,≈ result of softmax
v1.4.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.4.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.4.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.4.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.4.0,≈ result of sigmoid
v1.4.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.4.0,expected_loss ≈ 0.34
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Check minimal statistics
v1.4.0,Check statistics for pre-stratified datasets
v1.4.0,Check either a github link or author/publication information is given
v1.4.0,TestFB15K237 is a stand-in to test the ZipFileRemoteDataset
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Create dummy dense labels
v1.4.0,Check if labels form a probability distribution
v1.4.0,Apply label smoothing
v1.4.0,Check if smooth labels form probability distribution
v1.4.0,Create dummy sLCWA labels
v1.4.0,Apply label smoothing
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.4.0,"prepare conv input (N, C, H, W)"
v1.4.0,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.4.0,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.4.0,remove batch/num dimension
v1.4.0,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.4.0,"f(h, r, t) = h @ r @ t"
v1.4.0,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.4.0,normalize length of r
v1.4.0,check for unit length
v1.4.0,entity embeddings
v1.4.0,relation embeddings
v1.4.0,Compute Scores
v1.4.0,entity embeddings
v1.4.0,relation embeddings
v1.4.0,Compute Scores
v1.4.0,Compute Scores
v1.4.0,-\|R_h h - R_t t\|
v1.4.0,-\|h - t\|
v1.4.0,"Since MuRE has offsets, the scores do not need to negative"
v1.4.0,"We do not need this, since we do not check for functional consistency anyway"
v1.4.0,intra-interaction comparison
v1.4.0,"LiteralInteraction,"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,TODO: use triple generation
v1.4.0,generate random triples
v1.4.0,: The number of embeddings
v1.4.0,check shape
v1.4.0,check attributes
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,ensure positivity
v1.4.0,compute using pytorch
v1.4.0,prepare distributions
v1.4.0,compute using pykeen
v1.4.0,"e: (batch_size, num_heads, num_tails, d)"
v1.4.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.4.0,divergence = 0 => similarity = -divergence = 0
v1.4.0,"(h - t), r"
v1.4.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.4.0,divergence >= 0 => similarity = -divergence <= 0
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Base Classes
v1.4.0,Concrete Classes
v1.4.0,Utils
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,"scale labels from [0, 1] to [-1, 1]"
v1.4.0,cross entropy expects a proper probability distribution -> normalize labels
v1.4.0,Use numerically stable variant to compute log(softmax)
v1.4.0,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.4.0,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.4.0,": For more information, see https://github.com/cthoyt/pystow"
v1.4.0,: A path representing the PyKEEN data folder
v1.4.0,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.4.0,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.4.0,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.4.0,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.4.0,: A subdirectory for PyKEEN logs
v1.4.0,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.4.0,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.4.0,lower bound
v1.4.0,upper bound
v1.4.0,normalize input
v1.4.0,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.4.0,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.4.0,Determine optimal order and cost
v1.4.0,translate back to original order
v1.4.0,determine optimal processing order
v1.4.0,heuristic
v1.4.0,workaround for complex numbers: manually compute norm
v1.4.0,TODO: check if einsum is still very slow.
v1.4.0,TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation
v1.4.0,unsqueeze
v1.4.0,The dimensions affected by e'
v1.4.0,Project entities
v1.4.0,r_p (e_p.T e) + e'
v1.4.0,Enforce constraints
v1.4.0,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.4.0,Create a tensor of all IDs
v1.4.0,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.4.0,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.4.0,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.4.0,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.4.0,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.4.0,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.4.0,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Base Class
v1.4.0,Child classes
v1.4.0,Utils
v1.4.0,: The overall regularization weight
v1.4.0,: The current regularization term (a scalar)
v1.4.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.4.0,: Has this regularizer been updated since last being reset?
v1.4.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.4.0,"If there are tracked parameters, update based on them"
v1.4.0,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.4.0,no need to compute anything
v1.4.0,always return zero
v1.4.0,: The dimension along which to compute the vector-based regularization terms.
v1.4.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.4.0,: This allows dimensionality-independent weight tuning.
v1.4.0,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.4.0,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.4.0,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.4.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.4.0,"Therefore, apply_only_once is always set to True."
v1.4.0,Entity soft constraint
v1.4.0,Orthogonality soft constraint
v1.4.0,The normalization factor to balance individual regularizers' contribution.
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Add HPO command
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,General types
v1.4.0,Triples
v1.4.0,Others
v1.4.0,Tensor Functions
v1.4.0,Tensors
v1.4.0,Dataclasses
v1.4.0,: A function that mutates the input and returns a new object of the same type as output
v1.4.0,: A function that can be applied to a tensor to initialize it
v1.4.0,: A function that can be applied to a tensor to normalize it
v1.4.0,: A function that can be applied to a tensor to constrain it
v1.4.0,: A hint for a :class:`torch.device`
v1.4.0,: A hint for a :class:`torch.Generator`
v1.4.0,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.4.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.4.0,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.4.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.4.0,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.4.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: the maximum ID (exclusively)
v1.4.0,: the shape of an individual representation
v1.4.0,TODO: Remove this property and update code to use shape instead
v1.4.0,normalize embedding_dim vs. shape
v1.4.0,work-around until full complex support
v1.4.0,TODO: verify that this is our understanding of complex!
v1.4.0,"wrapper around max_id, for backward compatibility"
v1.4.0,initialize weights in-place
v1.4.0,apply constraints in-place
v1.4.0,verify that contiguity is preserved
v1.4.0,TODO: move normalizer / regularizer to base class?
v1.4.0,TODO add normalization functions
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,TODO test
v1.4.0,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.4.0,: a = \mu^T\Sigma^{-1}\mu
v1.4.0,: b = \log \det \Sigma
v1.4.0,1. Component
v1.4.0,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.4.0,2. Component
v1.4.0,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.4.0,with mu = (mu_1 - mu_0)
v1.4.0,= mu * Sigma_1^-1 mu
v1.4.0,since Sigma_1 is diagonal
v1.4.0,= mu**2 / sigma_1
v1.4.0,3. Component
v1.4.0,4. Component
v1.4.0,ln (det(\Sigma_1) / det(\Sigma_0))
v1.4.0,= ln det Sigma_1 - ln det Sigma_0
v1.4.0,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.4.0,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.4.0,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.4.0,allocate result
v1.4.0,prepare distributions
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,TODO benchmark
v1.4.0,TODO benchmark
v1.4.0,TODO benchmark
v1.4.0,TODO benchmark
v1.4.0,TODO benchmark
v1.4.0,TODO benchmark
v1.4.0,TODO benchmark
v1.4.0,TODO benchmark
v1.4.0,TODO benchmark
v1.4.0,"h = h_re, -h_im"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Base Classes
v1.4.0,Adapter classes
v1.4.0,Concrete Classes
v1.4.0,: The symbolic shapes for entity representations
v1.4.0,": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE."
v1.4.0,: The symbolic shapes for relation representations
v1.4.0,"bring to (b, n, *)"
v1.4.0,"bring to (b, h, r, t, *)"
v1.4.0,unpack singleton
v1.4.0,: The functional interaction form
v1.4.0,Store initial input for error message
v1.4.0,All are None -> try and make closest to square
v1.4.0,Only input channels is None
v1.4.0,Only width is None
v1.4.0,Only height is none
v1.4.0,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.4.0,Width and input channels are None -> set input channels to 1 and calculate width
v1.4.0,": The head-relation encoder operating on 2D ""images"""
v1.4.0,: The head-relation encoder operating on the 1D flattened version
v1.4.0,: The interaction function
v1.4.0,Automatic calculation of remaining dimensions
v1.4.0,Parameter need to fulfil:
v1.4.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.4.0,encoders
v1.4.0,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.4.0,"2: 1D encoder: FC, DO, BN?, Act"
v1.4.0,store reshaping dimensions
v1.4.0,The interaction model
v1.4.0,Use Xavier initialization for weight; bias to zero
v1.4.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.4.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.4.0,Initialize biases with zero
v1.4.0,"In the original formulation,"
v1.4.0,Global entity projection
v1.4.0,Global relation projection
v1.4.0,Global combination bias
v1.4.0,Global combination bias
v1.4.0,Core tensor
v1.4.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.4.0,Dropout
v1.4.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.4.0,"batch norm gets reset automatically, since it defines reset_parameters"
v1.4.0,shapes
v1.4.0,there are separate biases for entities in head and tail position
v1.4.0,the base interaction
v1.4.0,forward entity/relation shapes
v1.4.0,The parameters of the affine transformation: bias
v1.4.0,"scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The batch size of the head representations.
v1.4.0,: The number of head representations per batch
v1.4.0,: The batch size of the relation representations.
v1.4.0,: The number of relation representations per batch
v1.4.0,: The batch size of the tail representations.
v1.4.0,: The number of tail representations per batch
v1.4.0,"repeat if necessary, and concat head and relation, batch_size', num_input_channels, 2*height, width"
v1.4.0,with batch_size' = batch_size * num_heads * num_relations
v1.4.0,"batch_size', num_input_channels, 2*height, width"
v1.4.0,"batch_size', num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.4.0,"reshape: (batch_size', embedding_dim) -> (b, h, r, 1, d)"
v1.4.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.4.0,"output_shape: (batch_size, num_heads, num_relations, num_tails)"
v1.4.0,add bias term
v1.4.0,decompose convolution for faster computation in 1-n case
v1.4.0,"compute conv(stack(h, r, t))"
v1.4.0,prepare input shapes for broadcasting
v1.4.0,"(b, h, r, t, 1, d)"
v1.4.0,"conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])"
v1.4.0,"here, kernel_size = (1, 3), C_in = 1, C_out = num_filters"
v1.4.0,"-> conv_head, conv_rel, conv_tail shapes: (num_filters,)"
v1.4.0,"reshape to (1, 1, 1, 1, f, 1)"
v1.4.0,"convolve -> output.shape: (*, embedding_dim, num_filters)"
v1.4.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.4.0,"Linear layer for final scores; use flattened representations, shape: (b, h, r, t, d * f)"
v1.4.0,same shape
v1.4.0,"split, shape: (embedding_dim, hidden_dim)"
v1.4.0,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.4.0,"Predict t embedding, shape: (b, h, r, 1, d)"
v1.4.0,"transpose t, (b, 1, 1, d, t)"
v1.4.0,"dot product, (b, h, r, 1, t)"
v1.4.0,Circular correlation of entity embeddings
v1.4.0,complex conjugate
v1.4.0,Hadamard product in frequency domain
v1.4.0,"inverse real FFT, shape: (b, h, 1, t, d)"
v1.4.0,"transpose composite: (b, h, 1, d, t)"
v1.4.0,inner product with relation embedding
v1.4.0,global projections
v1.4.0,"combination, shape: (b, h, r, 1, d)"
v1.4.0,"dot product with t, shape: (b, h, r, t)"
v1.4.0,r expresses a rotation in complex plane.
v1.4.0,rotate head by relation (=Hadamard product in complex space)
v1.4.0,rotate tail by inverse of relation
v1.4.0,The inverse rotation is expressed by the complex conjugate of r.
v1.4.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.4.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.4.0,|h * r - t| = |h - conj(r) * t|
v1.4.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.4.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.4.0,"That is not mentioned in the paper, so it is made optional here."
v1.4.0,Project entities
v1.4.0,h projection to hyperplane
v1.4.0,r
v1.4.0,-t projection to hyperplane
v1.4.0,project to relation specific subspace and ensure constraints
v1.4.0,x_3 contraction
v1.4.0,x_1 contraction
v1.4.0,x_2 contraction
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,don't worry about functions because they can't be specified by JSON.
v1.4.0,Could make a better mo
v1.4.0,later could extend for other non-JSON valid types
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Score with original triples
v1.4.0,Score with inverse triples
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Create directory in which all experimental artifacts are saved
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,distribute the deteriorated triples across the remaining factories
v1.4.0,"'kinships',"
v1.4.0,"'umls',"
v1.4.0,"'codexsmall',"
v1.4.0,"'wn18',"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: Functions for specifying exotic resources with a given prefix
v1.4.0,: Functions for specifying exotic resources based on their file extension
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.4.0,TODO vectorize code
v1.4.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Split triples
v1.4.0,Sorting ensures consistent results when the triples are permuted
v1.4.0,Create mapping
v1.4.0,Sorting ensures consistent results when the triples are permuted
v1.4.0,Create mapping
v1.4.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.4.0,Filter all non-existent triples
v1.4.0,Note: Unique changes the order of the triples
v1.4.0,Note: Using unique means implicit balancing of training samples
v1.4.0,normalize input
v1.4.0,: The mapping from labels to IDs.
v1.4.0,: The inverse mapping for label_to_id; initialized automatically
v1.4.0,: A vectorized version of entity_label_to_id; initialized automatically
v1.4.0,: A vectorized version of entity_id_to_label; initialized automatically
v1.4.0,Normalize input
v1.4.0,label
v1.4.0,check new label to ID mappings
v1.4.0,Make new triples factories for each group
v1.4.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.4.0,Input validation
v1.4.0,convert to numpy
v1.4.0,Additional columns
v1.4.0,convert PyTorch tensors to numpy
v1.4.0,convert to dataframe
v1.4.0,Re-order columns
v1.4.0,Filter for entities
v1.4.0,Filter for relations
v1.4.0,No filtering happened
v1.4.0,Check if the triples are inverted already
v1.4.0,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.4.0,contained if and only if create_inverse_triples is True.
v1.4.0,Generate entity mapping if necessary
v1.4.0,Generate relation mapping if necessary
v1.4.0,Map triples of labels to triples of IDs.
v1.4.0,TODO: Check if lazy evaluation would make sense
v1.4.0,pre-filter to keep only topk
v1.4.0,generate text
v1.4.0,vectorized label lookup
v1.4.0,Re-order columns
v1.4.0,"FIXME currently the implementation does not consider the non-training (i.e., second-last entries)"
v1.4.0,for the number of steps. Consider more interesting way to discuss splits w/ valid
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Split indices
v1.4.0,Split triples
v1.4.0,index
v1.4.0,select
v1.4.0,Prepare split index
v1.4.0,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.4.0,[...] is necessary for Python 3.7 compatibility
v1.4.0,While there are still triples that should be moved to the training set
v1.4.0,Pick a random triple to move over to the training triples
v1.4.0,add to training
v1.4.0,remove from testing
v1.4.0,Recalculate the move_id_mask
v1.4.0,base cases
v1.4.0,IDs not in training
v1.4.0,triples with exclusive test IDs
v1.4.0,Make sure that the first element has all the right stuff in it
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,": The mapped triples, shape: (num_triples, 3)"
v1.4.0,: The unique pairs
v1.4.0,: The compressed triples in CSR format
v1.4.0,convert to csr for fast row slicing
v1.4.0,: TODO: do we need these?
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,check validity
v1.4.0,path compression
v1.4.0,collect connected components using union find with path compression
v1.4.0,get representatives
v1.4.0,already merged
v1.4.0,make x the smaller one
v1.4.0,merge
v1.4.0,extract partitions
v1.4.0,safe division for empty sets
v1.4.0,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.4.0,duplicates
v1.4.0,we are not interested in self-similarity
v1.4.0,compute similarities
v1.4.0,Calculate which relations are the inverse ones
v1.4.0,get existing IDs
v1.4.0,remove non-existing ID from label mapping
v1.4.0,create translation tensor
v1.4.0,get entities and relations occurring in triples
v1.4.0,generate ID translation and new label to Id mappings
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,preprocessing
v1.4.0,initialize
v1.4.0,sample iteratively
v1.4.0,determine weights
v1.4.0,only happens at first iteration
v1.4.0,normalize to probabilities
v1.4.0,sample a start node
v1.4.0,get list of neighbors
v1.4.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.4.0,visit target node
v1.4.0,decrease sample counts
v1.4.0,return chosen edges
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.4.0,seamless loading and saving of training checkpoints
v1.4.0,Create training instances
v1.4.0,During size probing the training instances should not show the tqdm progress bar
v1.4.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.4.0,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.4.0,"If a checkpoint file is given, it must be loaded if it exists already"
v1.4.0,"If the stopper dict has any keys, those are written back to the stopper"
v1.4.0,The checkpoint frequency needs to be set to save checkpoints
v1.4.0,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.4.0,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.4.0,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.4.0,Ensure the release of memory
v1.4.0,Clear optimizer
v1.4.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.4.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.4.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.4.0,return the relevant parameters slice_size and batch_size
v1.4.0,Create dummy result tracker
v1.4.0,Sanity check
v1.4.0,Force weight initialization if training continuation is not explicitly requested.
v1.4.0,Reset the weights
v1.4.0,Create new optimizer
v1.4.0,Ensure the model is on the correct device
v1.4.0,Create Sampler
v1.4.0,Bind
v1.4.0,"When size probing, we don't want progress bars"
v1.4.0,Create progress bar
v1.4.0,Save the time to track when the saved point was available
v1.4.0,Training Loop
v1.4.0,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.4.0,Enforce training mode
v1.4.0,Accumulate loss over epoch
v1.4.0,Batching
v1.4.0,Only create a progress bar when not in size probing mode
v1.4.0,Flag to check when to quit the size probing
v1.4.0,Recall that torch *accumulates* gradients. Before passing in a
v1.4.0,"new instance, you need to zero out the gradients from the old instance"
v1.4.0,Get batch size of current batch (last batch may be incomplete)
v1.4.0,accumulate gradients for whole batch
v1.4.0,forward pass call
v1.4.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.4.0,update parameters according to optimizer
v1.4.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.4.0,constraints are no longer applied
v1.4.0,For testing purposes we're only interested in processing one batch
v1.4.0,When size probing we don't need the losses
v1.4.0,Track epoch loss
v1.4.0,Print loss information to console
v1.4.0,Save the last successful finished epoch
v1.4.0,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.4.0,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.4.0,MyPy overrides are because you should
v1.4.0,forward pass
v1.4.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.4.0,correction for loss reduction
v1.4.0,backward pass
v1.4.0,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.4.0,Set upper bound
v1.4.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.4.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.4.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.4.0,"Only if a cuda device is available, the random state is accessed"
v1.4.0,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Shuffle each epoch
v1.4.0,Lazy-splitting into batches
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Slicing is not possible in sLCWA training loops
v1.4.0,Send positive batch to device
v1.4.0,Create negative samples
v1.4.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.4.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.4.0,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.4.0,Compute negative and positive scores
v1.4.0,Repeat positives scores (necessary for more than one negative per positive)
v1.4.0,Stack predictions
v1.4.0,Create target
v1.4.0,Normalize the loss to have the average loss per positive triple
v1.4.0,This allows comparability of sLCWA and LCWA losses
v1.4.0,Slicing is not possible for sLCWA
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Split batch components
v1.4.0,Send batch to device
v1.4.0,Apply label smoothing
v1.4.0,This shows how often one row has to be repeated
v1.4.0,Create boolean indices for negative labels in the repeated rows
v1.4.0,Repeat the predictions and filter for negative labels
v1.4.0,This tells us how often each true label should be repeated
v1.4.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.4.0,Split positive and negative scores
v1.4.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.4.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,now: smaller is better
v1.4.0,: The model
v1.4.0,: The evaluator
v1.4.0,: The triples to use for evaluation
v1.4.0,: Size of the evaluation batches
v1.4.0,: Slice size of the evaluation batches
v1.4.0,: The number of epochs after which the model is evaluated on validation set
v1.4.0,: The number of iterations (one iteration can correspond to various epochs)
v1.4.0,: with no improvement after which training will be stopped.
v1.4.0,: The name of the metric to use
v1.4.0,: The minimum relative improvement necessary to consider it an improved result
v1.4.0,: The best result so far
v1.4.0,: The epoch at which the best result occurred
v1.4.0,: The remaining patience
v1.4.0,: The metric results from all evaluations
v1.4.0,": Whether a larger value is better, or a smaller"
v1.4.0,: The result tracker
v1.4.0,: Callbacks when after results are calculated
v1.4.0,: Callbacks when training gets continued
v1.4.0,: Callbacks when training is stopped early
v1.4.0,: Did the stopper ever decide to stop?
v1.4.0,TODO: Fix this
v1.4.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.4.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.4.0,Evaluate
v1.4.0,Only perform time consuming checks for the first call.
v1.4.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.4.0,Append to history
v1.4.0,check for improvement
v1.4.0,Stop if the result did not improve more than delta for patience evaluations
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Utils
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.4.0,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.4.0,Clear the ranks from the current evaluator
v1.4.0,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.4.0,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.4.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.4.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.4.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.4.0,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.4.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.4.0,values_dict[key] will always be an int at this point
v1.4.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.4.0,Test if slicing is implemented for the required functions of this model
v1.4.0,Split batch
v1.4.0,Bind shape
v1.4.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.4.0,Warn if all entities will be filtered
v1.4.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.4.0,verify that the triples have been filtered
v1.4.0,Send to device
v1.4.0,Ensure evaluation mode
v1.4.0,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.4.0,Check whether we need to be prepared for filtering
v1.4.0,Check whether an evaluator needs access to the masks
v1.4.0,This can only be an unfiltered evaluator.
v1.4.0,Prepare for result filtering
v1.4.0,Send tensors to device
v1.4.0,Prepare batches
v1.4.0,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.4.0,Show progressbar
v1.4.0,Flag to check when to quit the size probing
v1.4.0,Disable gradient tracking
v1.4.0,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.4.0,batch-wise processing
v1.4.0,If we only probe sizes we do not need more than one batch
v1.4.0,Finalize
v1.4.0,Predict scores once
v1.4.0,Select scores of true
v1.4.0,Create positive filter for all corrupted
v1.4.0,Needs all positive triples
v1.4.0,Create filter
v1.4.0,Create a positive mask with the size of the scores from the positive filter
v1.4.0,Restrict to entities of interest
v1.4.0,Evaluate metrics on these *unfiltered* scores
v1.4.0,Filter
v1.4.0,The scores for the true triples have to be rewritten to the scores tensor
v1.4.0,Restrict to entities of interest
v1.4.0,Evaluate metrics on these *filtered* scores
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The area under the ROC curve
v1.4.0,: The area under the precision-recall curve
v1.4.0,: The coverage error
v1.4.0,coverage_error: float = field(metadata=dict(
v1.4.0,"doc='The coverage error',"
v1.4.0,"f=metrics.coverage_error,"
v1.4.0,))
v1.4.0,: The label ranking loss (APS)
v1.4.0,label_ranking_average_precision_score: float = field(metadata=dict(
v1.4.0,"doc='The label ranking loss (APS)',"
v1.4.0,"f=metrics.label_ranking_average_precision_score,"
v1.4.0,))
v1.4.0,#: The label ranking loss
v1.4.0,label_ranking_loss: float = field(metadata=dict(
v1.4.0,"doc='The label ranking loss',"
v1.4.0,"f=metrics.label_ranking_loss,"
v1.4.0,))
v1.4.0,Transfer to cpu and convert to numpy
v1.4.0,Ensure that each key gets counted only once
v1.4.0,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.4.0,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.4.0,masks using the exact same key order.
v1.4.0,TODO how to define a cutoff on y_scores to make binary?
v1.4.0,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.4.0,Clear buffers
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,The best rank is the rank when assuming all options with an equal score are placed behind the currently
v1.4.0,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.4.0,The worst rank is the rank when assuming all options with an equal score are placed in front of the currently
v1.4.0,"considered. Hence, the rank is the number of options which have at least the same score minus one (as the"
v1.4.0,"currently considered option in included in all options). As the rank is one-based, we have to add 1, which"
v1.4.0,"nullifies the ""minus 1"" from before."
v1.4.0,"The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of"
v1.4.0,the elements with the same score as the currently considered option.
v1.4.0,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.4.0,The expected rank of a random scoring
v1.4.0,The adjusted ranks is normalized by the expected rank of a random scoring
v1.4.0,TODO adjusted_worst_rank
v1.4.0,TODO adjusted_best_rank
v1.4.0,: The mean over all ranks: mean_i r_i. Lower is better.
v1.4.0,: The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.
v1.4.0,": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k."
v1.4.0,: Higher is better.
v1.4.0,: The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.
v1.4.0,: Described by [berrendorf2020]_.
v1.4.0,Check if it a side or rank type
v1.4.0,Clear buffers
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used
v1.4.0,to create the class
v1.4.0,"TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default regularizer class
v1.4.0,: The default parameters for the default regularizer class
v1.4.0,cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501
v1.4.0,Make sure that all modules with parameters do have a reset_parameters method.
v1.4.0,Recursively visit all sub-modules
v1.4.0,skip self
v1.4.0,Track parents for blaming
v1.4.0,call reset_parameters if possible
v1.4.0,initialize from bottom to top
v1.4.0,This ensures that specialized initializations will take priority over the default ones of its components.
v1.4.0,emit warning if there where parameters which were not initialised by reset_parameters.
v1.4.0,Additional debug information
v1.4.0,Important: use ModuleList to ensure that Pytorch correctly handles their devices and parameters
v1.4.0,: The entity representations
v1.4.0,: The relation representations
v1.4.0,: The weight regularizers
v1.4.0,"Comment: it is important that the regularizers are stored in a module list, in order to appear in"
v1.4.0,"model.modules(). Thereby, we can collect them automatically."
v1.4.0,normalize input
v1.4.0,normalization
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Train a model (quickly)
v1.4.0,Get scores for *all* triples
v1.4.0,Get scores for top 15 triples
v1.4.0,initialize buffer on cpu
v1.4.0,calculate batch scores
v1.4.0,Explicitly create triples
v1.4.0,initialize buffer on device
v1.4.0,calculate batch scores
v1.4.0,get top scores within batch
v1.4.0,append to global top scores
v1.4.0,reduce size if necessary
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: Keep track of if this is a base model
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: A triples factory with the training triples
v1.4.0,: The device on which this model and its submodules are stored
v1.4.0,: The default loss function class
v1.4.0,: The default parameters for the default loss function class
v1.4.0,: The instance of the loss
v1.4.0,Initialize the device
v1.4.0,Random seeds have to set before the embeddings are initialized
v1.4.0,Loss
v1.4.0,The triples factory facilitates access to the dataset.
v1.4.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.4.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.4.0,Id of inverse relation: relation + 1
v1.4.0,: The default regularizer class
v1.4.0,: The default parameters for the default regularizer class
v1.4.0,: The instance of the regularizer
v1.4.0,Regularizer
v1.4.0,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.4.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.4.0,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.4.0,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.4.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.4.0,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.4.0,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.4.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.4.0,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.4.0,"make sure to call this first, to reset regularizer state!"
v1.4.0,"make sure to call this first, to reset regularizer state!"
v1.4.0,The following lines add in a post-init hook to all subclasses
v1.4.0,such that the reset_parameters_() function is run
v1.4.0,"sorry mypy, but this kind of evil must be permitted."
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Base Models
v1.4.0,Concrete Models
v1.4.0,Utils
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,TODO rethink after RGCN update
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default loss function class
v1.4.0,: The default parameters for the default loss function class
v1.4.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.4.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.4.0,ConvE should be trained with inverse triples
v1.4.0,ConvE uses one bias for each entity
v1.4.0,Automatic calculation of remaining dimensions
v1.4.0,Parameter need to fulfil:
v1.4.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.4.0,weights
v1.4.0,"batch_size, num_input_channels, 2*height, width"
v1.4.0,"batch_size, num_input_channels, 2*height, width"
v1.4.0,"batch_size, num_input_channels, 2*height, width"
v1.4.0,"(N,C_out,H_out,W_out)"
v1.4.0,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.4.0,Embedding Regularization
v1.4.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.4.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.4.0,Embedding Regularization
v1.4.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.4.0,Embedding Regularization
v1.4.0,Code to repeat each item successively instead of the entire tensor
v1.4.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,Embeddings
v1.4.0,Initialise relation embeddings to unit length
v1.4.0,Get embeddings
v1.4.0,Project entities
v1.4.0,Get embeddings
v1.4.0,Project entities
v1.4.0,Project entities
v1.4.0,Project entities
v1.4.0,Get embeddings
v1.4.0,Project entities
v1.4.0,Project entities
v1.4.0,Project entities
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default loss function class
v1.4.0,: The default parameters for the default loss function class
v1.4.0,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.4.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.4.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.4.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.4.0,split into real and imaginary part
v1.4.0,ComplEx space bilinear product
v1.4.0,*: Elementwise multiplication
v1.4.0,get embeddings
v1.4.0,Regularization
v1.4.0,Compute scores
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.4.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.4.0,: a normalized weight of 10 is used.
v1.4.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.4.0,Get embeddings
v1.4.0,"shape: (b, d)"
v1.4.0,"shape: (b, d, d)"
v1.4.0,"shape: (b, d)"
v1.4.0,Compute scores
v1.4.0,Regularization
v1.4.0,Compute scores
v1.4.0,Regularization
v1.4.0,Get embeddings
v1.4.0,Compute scores
v1.4.0,Regularization
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.4.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.4.0,The interaction model
v1.4.0,embeddings
v1.4.0,Use Xavier initialization for weight; bias to zero
v1.4.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.4.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.4.0,Output layer regularization
v1.4.0,In the code base only the weights of the output layer are used for regularization
v1.4.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.4.0,Stack to convolution input
v1.4.0,Convolution
v1.4.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.4.0,Linear layer for final scores
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,comment:
v1.4.0,https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52
v1.4.0,uses float64
v1.4.0,entity bias for head
v1.4.0,entity bias for tail
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default entity normalizer parameters
v1.4.0,: The entity representations are normalized to L2 unit length
v1.4.0,: cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501
v1.4.0,"update initializer settings, cf."
v1.4.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49
v1.4.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29
v1.4.0,https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50
v1.4.0,in the original implementation the embeddings are initialized in one parameter
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,": shape: (batch_size, num_entities, d)"
v1.4.0,": Prepare h: (b, e, d) -> (b, e, 1, 1, d)"
v1.4.0,": Prepare t: (b, e, d) -> (b, e, 1, d, 1)"
v1.4.0,": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)"
v1.4.0,"h.T @ W @ t, shape: (b, e, k, 1, 1)"
v1.4.0,": reduce (b, e, k, 1, 1) -> (b, e, k)"
v1.4.0,": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.4.0,": Prepare h: (b, e, d) -> (b, e, d, 1)"
v1.4.0,"V_h @ h, shape: (b, e, k, 1)"
v1.4.0,": reduce (b, e, k, 1) -> (b, e, k)"
v1.4.0,": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.4.0,": Prepare t: (b, e, d) -> (b, e, d, 1)"
v1.4.0,"V_t @ t, shape: (b, e, k, 1)"
v1.4.0,": reduce (b, e, k, 1) -> (b, e, k)"
v1.4.0,": Prepare b: (R, k) -> (b, k) -> (b, 1, k)"
v1.4.0,"a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)"
v1.4.0,"prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)"
v1.4.0,"prepare act: (b, e, k) -> (b, e, 1, k)"
v1.4.0,"compute score, shape: (b, e, 1, 1)"
v1.4.0,reduce
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The regularizer used by [yang2014]_ for DistMult
v1.4.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.4.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.4.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.4.0,: The LP settings used by [yang2014]_ for DistMult
v1.4.0,Bilinear product
v1.4.0,*: Elementwise multiplication
v1.4.0,Get embeddings
v1.4.0,Compute score
v1.4.0,Only regularize relation embeddings
v1.4.0,Get embeddings
v1.4.0,Rank against all entities
v1.4.0,Only regularize relation embeddings
v1.4.0,Get embeddings
v1.4.0,Rank against all entities
v1.4.0,Only regularize relation embeddings
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default settings for the entity constrainer
v1.4.0,Similarity function used for distributions
v1.4.0,element-wise covariance bounds
v1.4.0,Additional covariance embeddings
v1.4.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.4.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.4.0,Constraints are applied through post_parameter_update
v1.4.0,Get embeddings
v1.4.0,Compute entity distribution
v1.4.0,: a = \mu^T\Sigma^{-1}\mu
v1.4.0,: b = \log \det \Sigma
v1.4.0,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.4.0,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.4.0,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.4.0,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The custom regularizer used by [wang2014]_ for TransH
v1.4.0,: The settings used by [wang2014]_ for TransH
v1.4.0,embeddings
v1.4.0,Normalise the normal vectors by their l2 norms
v1.4.0,TODO: Add initialization
v1.4.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.4.0,which enforces the defined soft constraints.
v1.4.0,Get embeddings
v1.4.0,Project to hyperplane
v1.4.0,Regularization term
v1.4.0,Get embeddings
v1.4.0,Project to hyperplane
v1.4.0,Regularization term
v1.4.0,Get embeddings
v1.4.0,Project to hyperplane
v1.4.0,Regularization term
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,TODO: Initialize from TransE
v1.4.0,embeddings
v1.4.0,"project to relation specific subspace, shape: (b, e, d_r)"
v1.4.0,ensure constraints
v1.4.0,"evaluate score function, shape: (b, e)"
v1.4.0,Get embeddings
v1.4.0,Get embeddings
v1.4.0,Get embeddings
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Construct node neighbourhood mask
v1.4.0,Set nodes in batch to true
v1.4.0,Compute k-neighbourhood
v1.4.0,"if the target node needs an embeddings, so does the source node"
v1.4.0,Create edge mask
v1.4.0,pylint: disable=unused-argument
v1.4.0,"Calculate in-degree, i.e. number of incoming edges"
v1.4.0,pylint: disable=unused-argument
v1.4.0,"Calculate in-degree, i.e. number of incoming edges"
v1.4.0,normalize representations
v1.4.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.4.0,check decomposition
v1.4.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.4.0,Weights
v1.4.0,buffering of messages
v1.4.0,allocate weight
v1.4.0,Get blocks
v1.4.0,"self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)"
v1.4.0,note: embedding_dim is guaranteed to be divisible by num_bases in the constructor
v1.4.0,"The current basis weights, shape: (num_bases)"
v1.4.0,"the current bases, shape: (num_bases, embedding_dim, embedding_dim)"
v1.4.0,"compute the current relation weights, shape: (embedding_dim, embedding_dim)"
v1.4.0,use buffered messages if applicable
v1.4.0,Bind fields
v1.4.0,"shape: (num_entities, embedding_dim)"
v1.4.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.4.0,Get random dropout mask
v1.4.0,Apply to edges
v1.4.0,Different dropout for self-loops (only in training mode)
v1.4.0,Initialize embeddings in the next layer for all nodes
v1.4.0,TODO: Can we vectorize this loop?
v1.4.0,Choose the edges which are of the specific relation
v1.4.0,No edges available? Skip rest of inner loop
v1.4.0,Get source and target node indices
v1.4.0,send messages in both directions
v1.4.0,Select source node embeddings
v1.4.0,get relation weights
v1.4.0,Compute message (b x d) * (d x d) = (b x d)
v1.4.0,Normalize messages by relation-specific in-degree
v1.4.0,Aggregate messages in target
v1.4.0,Self-loop
v1.4.0,"Apply bias, if requested"
v1.4.0,"Apply batch normalization, if requested"
v1.4.0,Apply non-linearity
v1.4.0,invalidate enriched embeddings
v1.4.0,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.4.0,initialized properly)
v1.4.0,We have one additional relation for self-loops
v1.4.0,Xavier Glorot initialization of each block
v1.4.0,Reset biases
v1.4.0,Reset batch norm parameters
v1.4.0,"Reset activation parameters, if any"
v1.4.0,"TODO: Replace this by interaction function, once https://github.com/pykeen/pykeen/pull/107 is merged."
v1.4.0,: Interaction model used as decoder
v1.4.0,: The blocks of the relation-specific weight matrices
v1.4.0,": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)"
v1.4.0,: The base weight matrices to generate relation-specific weights
v1.4.0,": shape: (num_bases, embedding_dim, embedding_dim)"
v1.4.0,: The relation-specific weights for each base
v1.4.0,": shape: (num_relations, num_bases)"
v1.4.0,: The biases for each layer (if used)
v1.4.0,": shape of each element: (embedding_dim,)"
v1.4.0,: Batch normalization for each layer (if used)
v1.4.0,: Activations for each layer (if used)
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,TODO: Dummy
v1.4.0,Enrich embeddings
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default loss function class
v1.4.0,: The default parameters for the default loss function class
v1.4.0,Core tensor
v1.4.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.4.0,Dropout
v1.4.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.4.0,Abbreviation
v1.4.0,Compute h_n = DO(BN(h))
v1.4.0,Compute wr = DO(W x_2 r)
v1.4.0,compute whr = DO(BN(h_n x_1 wr))
v1.4.0,Compute whr x_3 t
v1.4.0,Get embeddings
v1.4.0,Compute scores
v1.4.0,Get embeddings
v1.4.0,Compute scores
v1.4.0,Get embeddings
v1.4.0,Compute scores
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,Get embeddings
v1.4.0,TODO: Use torch.dist
v1.4.0,Get embeddings
v1.4.0,TODO: Use torch.cdist
v1.4.0,Get embeddings
v1.4.0,TODO: Use torch.cdist
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default loss function class
v1.4.0,: The default parameters for the default loss function class
v1.4.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.4.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.4.0,": regularization term by the number of elements, which is 200."
v1.4.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.4.0,extra embeddings
v1.4.0,forward model
v1.4.0,Regularization
v1.4.0,backward model
v1.4.0,Regularization
v1.4.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.4.0,"That is not mentioned in the paper, so it is omitted here."
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.4.0,weight initialization
v1.4.0,Get embeddings
v1.4.0,Embedding Regularization
v1.4.0,Concatenate them
v1.4.0,Compute scores
v1.4.0,Get embeddings
v1.4.0,Embedding Regularization
v1.4.0,First layer can be unrolled
v1.4.0,Send scores through rest of the network
v1.4.0,Get embeddings
v1.4.0,Embedding Regularization
v1.4.0,First layer can be unrolled
v1.4.0,Send scores through rest of the network
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,The dimensions affected by e'
v1.4.0,Project entities
v1.4.0,r_p (e_p.T e) + e'
v1.4.0,Enforce constraints
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,Project entities
v1.4.0,score = -||h_bot + r - t_bot||_2^2
v1.4.0,Head
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,Decompose into real and imaginary part
v1.4.0,Rotate (=Hadamard product in complex space).
v1.4.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.4.0,Get embeddings
v1.4.0,Compute scores
v1.4.0,Embedding Regularization
v1.4.0,Get embeddings
v1.4.0,Rank against all entities
v1.4.0,Compute scores
v1.4.0,Embedding Regularization
v1.4.0,Get embeddings
v1.4.0,r expresses a rotation in complex plane.
v1.4.0,The inverse rotation is expressed by the complex conjugate of r.
v1.4.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.4.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.4.0,|h * r - t| = |h - conj(r) * t|
v1.4.0,Rank against all entities
v1.4.0,Compute scores
v1.4.0,Embedding Regularization
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default loss function class
v1.4.0,: The default parameters for the default loss function class
v1.4.0,Global entity projection
v1.4.0,Global relation projection
v1.4.0,Global combination bias
v1.4.0,Global combination bias
v1.4.0,Get embeddings
v1.4.0,Compute score
v1.4.0,Get embeddings
v1.4.0,Rank against all entities
v1.4.0,Get embeddings
v1.4.0,Rank against all entities
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default settings for the entity constrainer
v1.4.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.4.0,Circular correlation of entity embeddings
v1.4.0,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.4.0,Hadamard product in frequency domain
v1.4.0,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.4.0,inner product with relation embedding
v1.4.0,Embedding Regularization
v1.4.0,Embedding Regularization
v1.4.0,Embedding Regularization
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default loss function class
v1.4.0,: The default parameters for the default loss function class
v1.4.0,Get embeddings
v1.4.0,Embedding Regularization
v1.4.0,Concatenate them
v1.4.0,Predict t embedding
v1.4.0,compare with all t's
v1.4.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.4.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.4.0,Embedding Regularization
v1.4.0,Concatenate them
v1.4.0,Predict t embedding
v1.4.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.4.0,Embedding Regularization
v1.4.0,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.4.0,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.4.0,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.4.0,Extend t
v1.4.0,Concatenate them
v1.4.0,Predict t embedding
v1.4.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.4.0,The results have to be realigned with the expected output of the score_h function
v1.4.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default loss function class
v1.4.0,: The default parameters for the default loss function class
v1.4.0,Literal
v1.4.0,num_ent x num_lit
v1.4.0,Number of columns corresponds to number of literals
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the model's hyper-parameters
v1.4.0,: The default parameters for the default loss function class
v1.4.0,Literal
v1.4.0,num_ent x num_lit
v1.4.0,Number of columns corresponds to number of literals
v1.4.0,"TODO: this is very similar to ComplExLiteral, except a few dropout differences"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The WANDB run
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Base classes
v1.4.0,Concrete classes
v1.4.0,Utilities
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The file extension for this writer (do not include dot)
v1.4.0,: The file where the results are written to.
v1.4.0,as_uri() requires the path to be absolute. resolve additionally also normalizes the path
v1.4.0,: The column names
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.4.0,Set the indices
v1.4.0,Bind number of negatives to sample
v1.4.0,Equally corrupt all sides
v1.4.0,Copy positive batch for corruption.
v1.4.0,"Do not detach, as no gradients should flow into the indices."
v1.4.0,Relations have a different index maximum than entities
v1.4.0,"To make sure we don't replace the {head, relation, tail} by the"
v1.4.0,original value we shift all values greater or equal than the original value by one up
v1.4.0,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.4.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.4.0,Create mapped triples attribute that is required for filtering
v1.4.0,Make sure the mapped triples are initiated
v1.4.0,Copy the mapped triples to the device for efficient filtering
v1.4.0,Check which heads of the mapped triples are also in the negative triples
v1.4.0,Reduce the search space by only using possible matches that at least contain the head we look for
v1.4.0,Check in this subspace which relations of the mapped triples are also in the negative triples
v1.4.0,Reduce the search space by only using possible matches that at least contain head and relation we look for
v1.4.0,Create a filter indicating which of the proposed negative triples are positive in the training dataset
v1.4.0,"In cases where no triples should be filtered, the subspace reduction technique above will fail"
v1.4.0,Return only those proposed negative triples that are not positive in the training dataset
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,Utils
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.4.0,Preprocessing: Compute corruption probabilities
v1.4.0,"compute tph, i.e. the average number of tail entities per head"
v1.4.0,"compute hpt, i.e. the average number of head entities per tail"
v1.4.0,Set parameter for Bernoulli distribution
v1.4.0,Bind number of negatives to sample
v1.4.0,Copy positive batch for corruption.
v1.4.0,"Do not detach, as no gradients should flow into the indices."
v1.4.0,Decide whether to corrupt head or tail
v1.4.0,Tails are corrupted if heads are not corrupted
v1.4.0,Randomly sample corruption. See below for explanation of
v1.4.0,"why this is on a range of [0, num_entities - 1]"
v1.4.0,Replace heads
v1.4.0,Replace tails
v1.4.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.4.0,To make sure we don't replace the head by the original value
v1.4.0,we shift all values greater or equal than the original value by one up
v1.4.0,"for that reason we choose the random value from [0, num_entities -1]"
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The random seed used at the beginning of the pipeline
v1.4.0,: The model trained by the pipeline
v1.4.0,: The training loop used by the pipeline
v1.4.0,: The losses during training
v1.4.0,: The results evaluated by the pipeline
v1.4.0,: How long in seconds did training take?
v1.4.0,: How long in seconds did evaluation take?
v1.4.0,: An early stopper
v1.4.0,: Any additional metadata as a dictionary
v1.4.0,: The version of PyKEEN used to create these results
v1.4.0,: The git hash of PyKEEN used to create these results
v1.4.0,FIXME this should never happen.
v1.4.0,1. Dataset
v1.4.0,2. Model
v1.4.0,3. Loss
v1.4.0,4. Regularizer
v1.4.0,5. Optimizer
v1.4.0,6. Training Loop
v1.4.0,7. Training (ronaldo style)
v1.4.0,8. Evaluation
v1.4.0,9. Tracking
v1.4.0,Misc
v1.4.0,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.4.0,used random_seed to ensure reproducible results
v1.4.0,We have to set clear optimizer to False since training should be continued
v1.4.0,Start tracking
v1.4.0,evaluation restriction to a subset of entities/relations
v1.4.0,TODO should training be reset?
v1.4.0,TODO should kwargs for loss and regularizer be checked and raised for?
v1.4.0,Log model parameters
v1.4.0,Stopping
v1.4.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.4.0,Add logging for debugging
v1.4.0,Train like Cristiano Ronaldo
v1.4.0,Evaluate
v1.4.0,Reuse optimal evaluation parameters from training if available
v1.4.0,Add logging about evaluator for debugging
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,TODO what happens if already exists?
v1.4.0,TODO incorporate setting of random seed
v1.4.0,pipeline_kwargs=dict(
v1.4.0,"random_seed=random_non_negative_int(),"
v1.4.0,"),"
v1.4.0,Add dataset to current_pipeline
v1.4.0,"Training, test, and validation paths are provided"
v1.4.0,Add loss function to current_pipeline
v1.4.0,Add regularizer to current_pipeline
v1.4.0,Add optimizer to current_pipeline
v1.4.0,Add training approach to current_pipeline
v1.4.0,Add training kwargs and kwargs_ranges
v1.4.0,Add evaluation
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.4.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.4.0,GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is
v1.4.0,"normally the default for all of PyKEEN's remote datasets, so just switch the default here."
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is
v1.4.0,"normally the default for all of PyKEEN's remote datasets, so just switch the default here."
v1.4.0,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.4.0,the columns are not ordered properly.
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is
v1.4.0,"normally the default for all of PyKEEN's remote datasets, so just switch the default here."
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The name of the dataset to download
v1.4.0,FIXME these are already identifiers
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,don't call this function by itself. assumes called through the `validation`
v1.4.0,property and the _training factory has already been loaded
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is
v1.4.0,"normally the default for all of PyKEEN's remote datasets, so just switch the default here."
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: A factory wrapping the training triples
v1.4.0,": A factory wrapping the testing triples, that share indices with the training triples"
v1.4.0,": A factory wrapping the validation triples, that share indices with the training triples"
v1.4.0,: All datasets should take care of inverse triple creation
v1.4.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.4.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.4.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.4.0,: The directory in which the cached data is stored
v1.4.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.4.0,don't call this function by itself. assumes called through the `validation`
v1.4.0,property and the _training factory has already been loaded
v1.4.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.4.0,see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content
v1.4.0,pattern from https://stackoverflow.com/a/39217788/5775947
v1.4.0,TODO replace this with the new zip remote dataset class
v1.4.0,: URL to the data to download
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: A mapping of datasets' names to their classes
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,TODO update docs with table and CLI wtih generator
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,1. Dataset
v1.4.0,2. Model
v1.4.0,3. Loss
v1.4.0,4. Regularizer
v1.4.0,5. Optimizer
v1.4.0,6. Training Loop
v1.4.0,7. Training
v1.4.0,8. Evaluation
v1.4.0,9. Trackers
v1.4.0,Misc.
v1.4.0,2. Model
v1.4.0,3. Loss
v1.4.0,4. Regularizer
v1.4.0,5. Optimizer
v1.4.0,1. Dataset
v1.4.0,2. Model
v1.4.0,3. Loss
v1.4.0,4. Regularizer
v1.4.0,5. Optimizer
v1.4.0,6. Training Loop
v1.4.0,7. Training
v1.4.0,8. Evaluation
v1.4.0,9. Tracker
v1.4.0,Misc.
v1.4.0,Will trigger Optuna to set the state of the trial as failed
v1.4.0,: The :mod:`optuna` study object
v1.4.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.4.0,Output study information
v1.4.0,Output all trials
v1.4.0,Output best trial as pipeline configuration file
v1.4.0,1. Dataset
v1.4.0,2. Model
v1.4.0,3. Loss
v1.4.0,4. Regularizer
v1.4.0,5. Optimizer
v1.4.0,6. Training Loop
v1.4.0,7. Training
v1.4.0,8. Evaluation
v1.4.0,9. Tracking
v1.4.0,6. Misc
v1.4.0,Optuna Study Settings
v1.4.0,Optuna Optimization Settings
v1.4.0,0. Metadata/Provenance
v1.4.0,1. Dataset
v1.4.0,2. Model
v1.4.0,3. Loss
v1.4.0,4. Regularizer
v1.4.0,5. Optimizer
v1.4.0,6. Training Loop
v1.4.0,7. Training
v1.4.0,8. Evaluation
v1.4.0,9. Tracking
v1.4.0,1. Dataset
v1.4.0,2. Model
v1.4.0,3. Loss
v1.4.0,4. Regularizer
v1.4.0,5. Optimizer
v1.4.0,6. Training Loop
v1.4.0,7. Training
v1.4.0,8. Evaluation
v1.4.0,9. Tracker
v1.4.0,Optuna Misc.
v1.4.0,Pipeline Misc.
v1.4.0,Invoke optimization of the objective function.
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.4.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,
v1.3.0,Configuration file for the Sphinx documentation builder.
v1.3.0,
v1.3.0,This file does only contain a selection of the most common options. For a
v1.3.0,full list see the documentation:
v1.3.0,http://www.sphinx-doc.org/en/master/config
v1.3.0,-- Path setup --------------------------------------------------------------
v1.3.0,"If extensions (or modules to document with autodoc) are in another directory,"
v1.3.0,add these directories to sys.path here. If the directory is relative to the
v1.3.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.3.0,
v1.3.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.3.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.3.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.3.0,from unittest.mock import Mock
v1.3.0,sys.modules['numpy'] = Mock()
v1.3.0,sys.modules['numpy.linalg'] = Mock()
v1.3.0,sys.modules['scipy'] = Mock()
v1.3.0,sys.modules['scipy.optimize'] = Mock()
v1.3.0,sys.modules['scipy.interpolate'] = Mock()
v1.3.0,sys.modules['scipy.sparse'] = Mock()
v1.3.0,sys.modules['scipy.ndimage'] = Mock()
v1.3.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.3.0,sys.modules['tensorflow'] = Mock()
v1.3.0,sys.modules['theano'] = Mock()
v1.3.0,sys.modules['theano.tensor'] = Mock()
v1.3.0,sys.modules['torch'] = Mock()
v1.3.0,sys.modules['torch.optim'] = Mock()
v1.3.0,sys.modules['torch.nn'] = Mock()
v1.3.0,sys.modules['torch.nn.init'] = Mock()
v1.3.0,sys.modules['torch.autograd'] = Mock()
v1.3.0,sys.modules['sklearn'] = Mock()
v1.3.0,sys.modules['sklearn.model_selection'] = Mock()
v1.3.0,sys.modules['sklearn.utils'] = Mock()
v1.3.0,-- Project information -----------------------------------------------------
v1.3.0,"The full version, including alpha/beta/rc tags."
v1.3.0,The short X.Y version.
v1.3.0,-- General configuration ---------------------------------------------------
v1.3.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.3.0,
v1.3.0,needs_sphinx = '1.0'
v1.3.0,"If true, the current module name will be prepended to all description"
v1.3.0,unit titles (such as .. function::).
v1.3.0,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.3.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.3.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.3.0,ones.
v1.3.0,show todo's
v1.3.0,generate autosummary pages
v1.3.0,"Add any paths that contain templates here, relative to this directory."
v1.3.0,The suffix(es) of source filenames.
v1.3.0,You can specify multiple suffix as a list of string:
v1.3.0,
v1.3.0,"source_suffix = ['.rst', '.md']"
v1.3.0,The master toctree document.
v1.3.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.3.0,for a list of supported languages.
v1.3.0,
v1.3.0,This is also used if you do content translation via gettext catalogs.
v1.3.0,"Usually you set ""language"" from the command line for these cases."
v1.3.0,"List of patterns, relative to source directory, that match files and"
v1.3.0,directories to ignore when looking for source files.
v1.3.0,This pattern also affects html_static_path and html_extra_path.
v1.3.0,The name of the Pygments (syntax highlighting) style to use.
v1.3.0,-- Options for HTML output -------------------------------------------------
v1.3.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.3.0,a list of builtin themes.
v1.3.0,
v1.3.0,Theme options are theme-specific and customize the look and feel of a theme
v1.3.0,"further.  For a list of options available for each theme, see the"
v1.3.0,documentation.
v1.3.0,
v1.3.0,html_theme_options = {}
v1.3.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.3.0,"relative to this directory. They are copied after the builtin static files,"
v1.3.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.3.0,html_static_path = ['_static']
v1.3.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.3.0,to template names.
v1.3.0,
v1.3.0,The default sidebars (for documents that don't match any pattern) are
v1.3.0,defined by theme itself.  Builtin themes are using these templates by
v1.3.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.3.0,'searchbox.html']``.
v1.3.0,
v1.3.0,html_sidebars = {}
v1.3.0,The name of an image file (relative to this directory) to place at the top
v1.3.0,of the sidebar.
v1.3.0,
v1.3.0,-- Options for HTMLHelp output ---------------------------------------------
v1.3.0,Output file base name for HTML help builder.
v1.3.0,-- Options for LaTeX output ------------------------------------------------
v1.3.0,latex_elements = {
v1.3.0,The paper size ('letterpaper' or 'a4paper').
v1.3.0,
v1.3.0,"'papersize': 'letterpaper',"
v1.3.0,
v1.3.0,"The font size ('10pt', '11pt' or '12pt')."
v1.3.0,
v1.3.0,"'pointsize': '10pt',"
v1.3.0,
v1.3.0,Additional stuff for the LaTeX preamble.
v1.3.0,
v1.3.0,"'preamble': '',"
v1.3.0,
v1.3.0,Latex figure (float) alignment
v1.3.0,
v1.3.0,"'figure_align': 'htbp',"
v1.3.0,}
v1.3.0,Grouping the document tree into LaTeX files. List of tuples
v1.3.0,"(source start file, target name, title,"
v1.3.0,"author, documentclass [howto, manual, or own class])."
v1.3.0,latex_documents = [
v1.3.0,(
v1.3.0,"master_doc,"
v1.3.0,"'pykeen.tex',"
v1.3.0,"'PyKEEN Documentation',"
v1.3.0,"author,"
v1.3.0,"'manual',"
v1.3.0,"),"
v1.3.0,]
v1.3.0,-- Options for manual page output ------------------------------------------
v1.3.0,One entry per manual page. List of tuples
v1.3.0,"(source start file, name, description, authors, manual section)."
v1.3.0,-- Options for Texinfo output ----------------------------------------------
v1.3.0,Grouping the document tree into Texinfo files. List of tuples
v1.3.0,"(source start file, target name, title, author,"
v1.3.0,"dir menu entry, description, category)"
v1.3.0,-- Options for Epub output -------------------------------------------------
v1.3.0,Bibliographic Dublin Core info.
v1.3.0,epub_title = project
v1.3.0,The unique identifier of the text. This can be a ISBN number
v1.3.0,or the project homepage.
v1.3.0,
v1.3.0,epub_identifier = ''
v1.3.0,A unique identification for the text.
v1.3.0,
v1.3.0,epub_uid = ''
v1.3.0,A list of files that should not be packed into the epub file.
v1.3.0,epub_exclude_files = ['search.html']
v1.3.0,-- Extension configuration -------------------------------------------------
v1.3.0,-- Options for intersphinx extension ---------------------------------------
v1.3.0,Example configuration for intersphinx: refer to the Python standard library.
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Check a model param is optimized
v1.3.0,Check a loss param is optimized
v1.3.0,Check a model param is NOT optimized
v1.3.0,Check a loss param is optimized
v1.3.0,Check a model param is optimized
v1.3.0,Check a loss param is NOT optimized
v1.3.0,Check a model param is NOT optimized
v1.3.0,Check a loss param is NOT optimized
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,check for empty batches
v1.3.0,Train a model in one shot
v1.3.0,Train a model for the first half
v1.3.0,Continue training of the first part
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,The triples factory and model
v1.3.0,: The evaluator to be tested
v1.3.0,Settings
v1.3.0,: The evaluator instantiation
v1.3.0,Settings
v1.3.0,Initialize evaluator
v1.3.0,Use small test dataset
v1.3.0,Use small model (untrained)
v1.3.0,Get batch
v1.3.0,Compute scores
v1.3.0,Compute mask only if required
v1.3.0,TODO: Re-use filtering code
v1.3.0,"shape: (batch_size, num_triples)"
v1.3.0,"shape: (batch_size, num_entities)"
v1.3.0,Process one batch
v1.3.0,Check for correct class
v1.3.0,Check value ranges
v1.3.0,TODO: Validate with data?
v1.3.0,Check for correct class
v1.3.0,check value
v1.3.0,filtering
v1.3.0,"true_score: (2, 3, 3)"
v1.3.0,head based filter
v1.3.0,preprocessing for faster lookup
v1.3.0,check that all found positives are positive
v1.3.0,check in-place
v1.3.0,Test head scores
v1.3.0,Assert in-place modification
v1.3.0,Assert correct filtering
v1.3.0,Test tail scores
v1.3.0,Assert in-place modification
v1.3.0,Assert correct filtering
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,check if within 0.5 std of observed
v1.3.0,test error is raised
v1.3.0,Tests that exception will be thrown when more than or less than three tensors are passed
v1.3.0,Test that regularization term is computed correctly
v1.3.0,Entity soft constraint
v1.3.0,Orthogonality soft constraint
v1.3.0,ensure regularizer is on correct device
v1.3.0,"After first update, should change the term"
v1.3.0,"After second update, no change should happen"
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,create broadcastable shapes
v1.3.0,check correct value range
v1.3.0,check maximum norm constraint
v1.3.0,unchanged values for small norms
v1.3.0,generate random query tensor
v1.3.0,random entity embeddings & projections
v1.3.0,random relation embeddings & projections
v1.3.0,project
v1.3.0,check shape:
v1.3.0,check normalization
v1.3.0,check equivalence of re-formulation
v1.3.0,e_{\bot} = M_{re} e = (r_p e_p^T + I^{d_r \times d_e}) e
v1.3.0,= r_p (e_p^T e) + e'
v1.3.0,"create random array, estimate the costs of addition, and measure some execution times."
v1.3.0,"then, compute correlation between the estimated cost, and the measured time."
v1.3.0,check for strong correlation between estimated costs and measured execution time
v1.3.0,get optimal sequence
v1.3.0,check caching
v1.3.0,get optimal sequence
v1.3.0,check correct cost
v1.3.0,check optimality
v1.3.0,compare result to sequential addition
v1.3.0,compare result to sequential addition
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,equal value; larger is better
v1.3.0,equal value; smaller is better
v1.3.0,larger is better; improvement
v1.3.0,larger is better; improvement; but not significant
v1.3.0,: The window size used by the early stopper
v1.3.0,: The mock losses the mock evaluator will return
v1.3.0,: The (zeroed) index  - 1 at which stopping will occur
v1.3.0,: The minimum improvement
v1.3.0,: The best results
v1.3.0,Set automatic_memory_optimization to false for tests
v1.3.0,Step early stopper
v1.3.0,check storing of results
v1.3.0,check ring buffer
v1.3.0,: The window size used by the early stopper
v1.3.0,: The (zeroed) index  - 1 at which stopping will occur
v1.3.0,: The minimum improvement
v1.3.0,: The random seed to use for reproducibility
v1.3.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.3.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.3.0,: The batch size to use.
v1.3.0,Fix seed for reproducibility
v1.3.0,Set automatic_memory_optimization to false during testing
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.3.0,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.3.0,"first triple, and passing create_inverse_triples=True."
v1.3.0,check for warning
v1.3.0,check for filtered triples
v1.3.0,check for correct inverse triples flag
v1.3.0,check correct translation
v1.3.0,check column order
v1.3.0,apply restriction
v1.3.0,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.3.0,check that inverse_triples is correctly carried over
v1.3.0,verify that the label-to-ID mapping has not been changed
v1.3.0,verify that triples have been filtered
v1.3.0,check compressed triples
v1.3.0,reconstruct triples from compressed form
v1.3.0,check data loader
v1.3.0,set create inverse triple to true
v1.3.0,split factory
v1.3.0,check that in *training* inverse triple are to be created
v1.3.0,check that in all other splits no inverse triples are to be created
v1.3.0,verify that all entities and relations are present in the training factory
v1.3.0,verify that no triple got lost
v1.3.0,verify that the label-to-id mappings match
v1.3.0,check type
v1.3.0,check format
v1.3.0,check coverage
v1.3.0,Check if multilabels are working correctly
v1.3.0,generate random ratios
v1.3.0,check size
v1.3.0,check value range
v1.3.0,check total split
v1.3.0,check consistency with ratios
v1.3.0,the number of decimal digits equivalent to 1 / n_total
v1.3.0,check type
v1.3.0,check values
v1.3.0,compare against expected
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The batch size
v1.3.0,: The random seed
v1.3.0,: The triples factory
v1.3.0,: The sLCWA instances
v1.3.0,: Class of negative sampling to test
v1.3.0,": The negative sampler instance, initialized in setUp"
v1.3.0,: A positive batch
v1.3.0,Generate negative sample
v1.3.0,check shape
v1.3.0,check bounds: heads
v1.3.0,check bounds: relations
v1.3.0,check bounds: tails
v1.3.0,Check that all elements got corrupted
v1.3.0,Check whether filtering works correctly
v1.3.0,First giving an example where all triples have to be filtered
v1.3.0,The filter should remove all triples
v1.3.0,Create an example where no triples will be filtered
v1.3.0,The filter should not remove any triple
v1.3.0,Generate scaled negative sample
v1.3.0,Generate negative samples
v1.3.0,test that the relations were not changed
v1.3.0,Test that half of the subjects and half of the objects are corrupted
v1.3.0,Generate negative sample for additional tests
v1.3.0,test that the relations were not changed
v1.3.0,sample a batch
v1.3.0,check shape
v1.3.0,get triples
v1.3.0,check connected components
v1.3.0,super inefficient
v1.3.0,join
v1.3.0,already joined
v1.3.0,check that there is only a single component
v1.3.0,check content of comp_adj_lists
v1.3.0,check edge ids
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,3x batch norm: bias + scale --> 6
v1.3.0,entity specific bias        --> 1
v1.3.0,==================================
v1.3.0,7
v1.3.0,"two bias terms, one conv-filter"
v1.3.0,check type
v1.3.0,check shape
v1.3.0,check ID ranges
v1.3.0,this is only done in one of the models
v1.3.0,this is only done in one of the models
v1.3.0,Two linear layer biases
v1.3.0,"Two BN layers, bias & scale"
v1.3.0,: one bias per layer
v1.3.0,: (scale & bias for BN) * layers
v1.3.0,entity embeddings
v1.3.0,relation embeddings
v1.3.0,Compute Scores
v1.3.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.3.0,relation embeddings
v1.3.0,Compute Scores
v1.3.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.3.0,entity embeddings
v1.3.0,relation embeddings
v1.3.0,Compute Scores
v1.3.0,random entity embeddings & projections
v1.3.0,random relation embeddings & projections
v1.3.0,project
v1.3.0,check shape:
v1.3.0,check normalization
v1.3.0,entity embeddings
v1.3.0,relation embeddings
v1.3.0,Compute Scores
v1.3.0,second_score = scores[1].item()
v1.3.0,: 2xBN (bias & scale)
v1.3.0,: The number of entities
v1.3.0,: The number of triples
v1.3.0,check shape
v1.3.0,check dtype
v1.3.0,check finite values (e.g. due to division by zero)
v1.3.0,check non-negativity
v1.3.0,check shape
v1.3.0,check content
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.3.0,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.3.0,pipeline is checked.
v1.3.0,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.3.0,Resume the previous pipeline
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.3.0,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.3.0,Test looking up inverse triples
v1.3.0,test new label to ID
v1.3.0,type
v1.3.0,old labels
v1.3.0,"new, compact IDs"
v1.3.0,test vectorized lookup
v1.3.0,type
v1.3.0,shape
v1.3.0,value range
v1.3.0,only occurring Ids get mapped to non-negative numbers
v1.3.0,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.3.0,check type
v1.3.0,check shape
v1.3.0,check content
v1.3.0,check type
v1.3.0,check shape
v1.3.0,check 1-hot
v1.3.0,check type
v1.3.0,check shape
v1.3.0,check value range
v1.3.0,check self-similarity = 1
v1.3.0,base relation
v1.3.0,exact duplicate
v1.3.0,99% duplicate
v1.3.0,50% duplicate
v1.3.0,exact inverse
v1.3.0,99% inverse
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,fix seeds for reproducibility
v1.3.0,: The expected number of entities
v1.3.0,: The expected number of relations
v1.3.0,: The expected number of triples
v1.3.0,": The tolerance on expected number of triples, for randomized situations"
v1.3.0,: The dataset to test
v1.3.0,: The instantiated dataset
v1.3.0,: Should the validation be assumed to have been loaded with train/test?
v1.3.0,Not loaded
v1.3.0,Load
v1.3.0,Test caching
v1.3.0,assert (end - start) < 1.0e-02
v1.3.0,": The directory, if there is caching"
v1.3.0,TODO update
v1.3.0,: The batch size
v1.3.0,test reduction
v1.3.0,test finite loss value
v1.3.0,Test backward
v1.3.0,TODO update
v1.3.0,: The number of entities.
v1.3.0,TODO update
v1.3.0,: The number of negative samples
v1.3.0,TODO update
v1.3.0,: The number of entities.
v1.3.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.3.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.3.0,: The equivalence for models with batch norm only holds in evaluation mode
v1.3.0,check whether the error originates from batch norm for single element batches
v1.3.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.3.0,set in eval mode (otherwise there are non-deterministic factors like Dropout
v1.3.0,test multiple different initializations
v1.3.0,calculate by functional
v1.3.0,calculate manually
v1.3.0,simple
v1.3.0,nested
v1.3.0,nested
v1.3.0,prepare a temporary test directory
v1.3.0,check that file was created
v1.3.0,make sure to close file before trying to delete it
v1.3.0,delete intermediate files
v1.3.0,: The batch size
v1.3.0,: The triples factory
v1.3.0,: Class of regularizer to test
v1.3.0,: The constructor parameters to pass to the regularizer
v1.3.0,": The regularizer instance, initialized in setUp"
v1.3.0,: A positive batch
v1.3.0,: The device
v1.3.0,move test instance to device
v1.3.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.3.0,Check if regularizer is stored correctly.
v1.3.0,Forward pass (should update regularizer)
v1.3.0,Call post_parameter_update (should reset regularizer)
v1.3.0,Check if regularization term is reset
v1.3.0,Call method
v1.3.0,Generate random tensors
v1.3.0,Call update
v1.3.0,check shape
v1.3.0,compute expected term
v1.3.0,Generate random tensor
v1.3.0,calculate penalty
v1.3.0,check shape
v1.3.0,check value
v1.3.0,FIXME isn't any finite number allowed now?
v1.3.0,: The class of the model to test
v1.3.0,: Additional arguments passed to the model's constructor method
v1.3.0,: Additional arguments passed to the training loop's constructor method
v1.3.0,: The triples factory instance
v1.3.0,: The model instance
v1.3.0,: The batch size for use for forward_* tests
v1.3.0,: The embedding dimensionality
v1.3.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.3.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.3.0,: The batch size for use when testing training procedures
v1.3.0,: The number of epochs to train the model
v1.3.0,: A random number generator from torch
v1.3.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.3.0,initialization
v1.3.0,assert there is at least one trainable parameter
v1.3.0,Check that all the parameters actually require a gradient
v1.3.0,Try to initialize an optimizer
v1.3.0,get model parameters
v1.3.0,re-initialize
v1.3.0,check that the operation works in-place
v1.3.0,check that the parameters where modified
v1.3.0,check for finite values by default
v1.3.0,check whether a gradient can be back-propgated
v1.3.0,"assert batch comprises (head, relation) pairs"
v1.3.0,"assert batch comprises (relation, tail) pairs"
v1.3.0,"For the high/low memory test cases of NTN, SE, etc."
v1.3.0,"else, leave to default"
v1.3.0,TODO: Catch HolE MKL error?
v1.3.0,set regularizer term to something that isn't zero
v1.3.0,call post_parameter_update
v1.3.0,assert that the regularization term has been reset
v1.3.0,do one optimization step
v1.3.0,call post_parameter_update
v1.3.0,check model constraints
v1.3.0,"assert batch comprises (relation, tail) pairs"
v1.3.0,"assert batch comprises (relation, tail) pairs"
v1.3.0,"assert batch comprises (relation, tail) pairs"
v1.3.0,call some functions
v1.3.0,reset to old state
v1.3.0,call some functions
v1.3.0,reset to old state
v1.3.0,Distance-based model
v1.3.0,check type
v1.3.0,check shape
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,check for finite values by default
v1.3.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.3.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.3.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.3.0,Get embeddings
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,≈ result of softmax
v1.3.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.3.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.3.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.3.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.3.0,≈ result of sigmoid
v1.3.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.3.0,expected_loss ≈ 0.34
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Create dummy dense labels
v1.3.0,Check if labels form a probability distribution
v1.3.0,Apply label smoothing
v1.3.0,Check if smooth labels form probability distribution
v1.3.0,Create dummy sLCWA labels
v1.3.0,Apply label smoothing
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,W_L drop(act(W_C \ast ([h; r; t]) + b_C)) + b_L
v1.3.0,"prepare conv input (N, C, H, W)"
v1.3.0,"f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)"
v1.3.0,"shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)"
v1.3.0,remove batch/num dimension
v1.3.0,"f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)"
v1.3.0,"f(h, r, t) = h @ r @ t"
v1.3.0,DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t
v1.3.0,normalize length of r
v1.3.0,check for unit length
v1.3.0,entity embeddings
v1.3.0,relation embeddings
v1.3.0,Compute Scores
v1.3.0,entity embeddings
v1.3.0,relation embeddings
v1.3.0,Compute Scores
v1.3.0,Compute Scores
v1.3.0,-\|R_h h - R_t t\|
v1.3.0,-\|h - t\|
v1.3.0,"LiteralInteraction,"
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,TODO: use triple generation
v1.3.0,generate random triples
v1.3.0,: The number of embeddings
v1.3.0,check shape
v1.3.0,check attributes
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,ensure positivity
v1.3.0,compute using pytorch
v1.3.0,prepare distributions
v1.3.0,compute using pykeen
v1.3.0,"e: (batch_size, num_heads, num_tails, d)"
v1.3.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.3.0,divergence = 0 => similarity = -divergence = 0
v1.3.0,"(h - t), r"
v1.3.0,https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties
v1.3.0,divergence >= 0 => similarity = -divergence <= 0
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: A mapping of optimizers' names to their implementations
v1.3.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Helpers
v1.3.0,Base Classes
v1.3.0,Concrete Classes
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,"scale labels from [0, 1] to [-1, 1]"
v1.3.0,cross entropy expects a proper probability distribution -> normalize labels
v1.3.0,Use numerically stable variant to compute log(softmax)
v1.3.0,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.3.0,: A mapping of losses' names to their implementations
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.3.0,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.3.0,": For more information, see https://github.com/cthoyt/pystow"
v1.3.0,: A path representing the PyKEEN data folder
v1.3.0,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.3.0,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.3.0,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.3.0,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.3.0,: A subdirectory for PyKEEN logs
v1.3.0,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.3.0,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.3.0,lower bound
v1.3.0,upper bound
v1.3.0,normalize input
v1.3.0,create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the
v1.3.0,"input sorting, as the order is determined by re-ordering the sequence anyway)"
v1.3.0,Determine optimal order and cost
v1.3.0,translate back to original order
v1.3.0,determine optimal processing order
v1.3.0,heuristic
v1.3.0,workaround for complex numbers: manually compute norm
v1.3.0,TODO: check if einsum is still very slow.
v1.3.0,TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation
v1.3.0,unsqueeze
v1.3.0,The dimensions affected by e'
v1.3.0,Project entities
v1.3.0,r_p (e_p.T e) + e'
v1.3.0,Enforce constraints
v1.3.0,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.3.0,Create a tensor of all IDs
v1.3.0,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.3.0,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.3.0,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.3.0,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.3.0,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.3.0,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.3.0,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The overall regularization weight
v1.3.0,: The current regularization term (a scalar)
v1.3.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.3.0,: Has this regularizer been updated since last being reset?
v1.3.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.3.0,"If there are tracked parameters, update based on them"
v1.3.0,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.3.0,no need to compute anything
v1.3.0,always return zero
v1.3.0,: The dimension along which to compute the vector-based regularization terms.
v1.3.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.3.0,: This allows dimensionality-independent weight tuning.
v1.3.0,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.3.0,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.3.0,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.3.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.3.0,"Therefore, apply_only_once is always set to True."
v1.3.0,Entity soft constraint
v1.3.0,Orthogonality soft constraint
v1.3.0,The normalization factor to balance individual regularizers' contribution.
v1.3.0,: A mapping of regularizers' names to their implementations
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Add HPO command
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The random seed used at the beginning of the pipeline
v1.3.0,: The model trained by the pipeline
v1.3.0,: The training loop used by the pipeline
v1.3.0,: The losses during training
v1.3.0,: The results evaluated by the pipeline
v1.3.0,: How long in seconds did training take?
v1.3.0,: How long in seconds did evaluation take?
v1.3.0,: An early stopper
v1.3.0,: Any additional metadata as a dictionary
v1.3.0,: The version of PyKEEN used to create these results
v1.3.0,: The git hash of PyKEEN used to create these results
v1.3.0,1. Dataset
v1.3.0,2. Model
v1.3.0,3. Loss
v1.3.0,4. Regularizer
v1.3.0,5. Optimizer
v1.3.0,6. Training Loop
v1.3.0,7. Training (ronaldo style)
v1.3.0,8. Evaluation
v1.3.0,9. Tracking
v1.3.0,Misc
v1.3.0,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.3.0,used random_seed to ensure reproducible results
v1.3.0,We have to set clear optimizer to False since training should be continued
v1.3.0,Start tracking
v1.3.0,evaluation restriction to a subset of entities/relations
v1.3.0,FIXME this should never happen.
v1.3.0,Log model parameters
v1.3.0,Log optimizer parameters
v1.3.0,Stopping
v1.3.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.3.0,By default there's a stopper that does nothing interesting
v1.3.0,Add logging for debugging
v1.3.0,Train like Cristiano Ronaldo
v1.3.0,Evaluate
v1.3.0,Reuse optimal evaluation parameters from training if available
v1.3.0,Add logging about evaluator for debugging
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,General types
v1.3.0,Triples
v1.3.0,Others
v1.3.0,Tensor Functions
v1.3.0,Tensors
v1.3.0,Dataclasses
v1.3.0,: A function that mutates the input and returns a new object of the same type as output
v1.3.0,: A function that can be applied to a tensor to initialize it
v1.3.0,: A function that can be applied to a tensor to normalize it
v1.3.0,: A function that can be applied to a tensor to constrain it
v1.3.0,: A hint for a :class:`torch.device`
v1.3.0,: A hint for a :class:`torch.Generator`
v1.3.0,": A type variable for head representations used in :class:`pykeen.models.Model`,"
v1.3.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.3.0,": A type variable for relation representations used in :class:`pykeen.models.Model`,"
v1.3.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.3.0,": A type variable for tail representations used in :class:`pykeen.models.Model`,"
v1.3.0,": :class:`pykeen.nn.modules.Interaction`, etc."
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: the maximum ID (exclusively)
v1.3.0,: the shape of an individual representation
v1.3.0,TODO: Remove this property and update code to use shape instead
v1.3.0,normalize embedding_dim vs. shape
v1.3.0,work-around until full complex support
v1.3.0,TODO: verify that this is our understanding of complex!
v1.3.0,"wrapper around max_id, for backward compatibility"
v1.3.0,initialize weights in-place
v1.3.0,apply constraints in-place
v1.3.0,verify that contiguity is preserved
v1.3.0,TODO: move normalizer / regularizer to base class?
v1.3.0,TODO add normalization functions
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,TODO test
v1.3.0,"subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)"
v1.3.0,: a = \mu^T\Sigma^{-1}\mu
v1.3.0,: b = \log \det \Sigma
v1.3.0,1. Component
v1.3.0,\sum_i \Sigma_e[i] / Sigma_r[i]
v1.3.0,2. Component
v1.3.0,(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)
v1.3.0,with mu = (mu_1 - mu_0)
v1.3.0,= mu * Sigma_1^-1 mu
v1.3.0,since Sigma_1 is diagonal
v1.3.0,= mu**2 / sigma_1
v1.3.0,3. Component
v1.3.0,4. Component
v1.3.0,ln (det(\Sigma_1) / det(\Sigma_0))
v1.3.0,= ln det Sigma_1 - ln det Sigma_0
v1.3.0,"since Sigma is diagonal, we have det Sigma = prod Sigma[ii]"
v1.3.0,= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]
v1.3.0,= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]
v1.3.0,allocate result
v1.3.0,prepare distributions
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,TODO benchmark
v1.3.0,TODO benchmark
v1.3.0,TODO benchmark
v1.3.0,TODO benchmark
v1.3.0,TODO benchmark
v1.3.0,TODO benchmark
v1.3.0,TODO benchmark
v1.3.0,TODO benchmark
v1.3.0,TODO benchmark
v1.3.0,"h = h_re, -h_im"
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Base Classes
v1.3.0,Concrete Classes
v1.3.0,: The symbolic shapes for entity representations
v1.3.0,": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE."
v1.3.0,: The symbolic shapes for relation representations
v1.3.0,"bring to (b, n, *)"
v1.3.0,"bring to (b, h, r, t, *)"
v1.3.0,unpack singleton
v1.3.0,: The functional interaction form
v1.3.0,Store initial input for error message
v1.3.0,All are None -> try and make closest to square
v1.3.0,Only input channels is None
v1.3.0,Only width is None
v1.3.0,Only height is none
v1.3.0,Width and input_channels are None -> set input_channels to 1 and calculage height
v1.3.0,Width and input channels are None -> set input channels to 1 and calculate width
v1.3.0,": The head-relation encoder operating on 2D ""images"""
v1.3.0,: The head-relation encoder operating on the 1D flattened version
v1.3.0,: The interaction function
v1.3.0,Automatic calculation of remaining dimensions
v1.3.0,Parameter need to fulfil:
v1.3.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.3.0,encoders
v1.3.0,"1: 2D encoder: BN?, DO, Conv, BN?, Act, DO"
v1.3.0,"2: 1D encoder: FC, DO, BN?, Act"
v1.3.0,store reshaping dimensions
v1.3.0,The interaction model
v1.3.0,Use Xavier initialization for weight; bias to zero
v1.3.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.3.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.3.0,Initialize biases with zero
v1.3.0,"In the original formulation,"
v1.3.0,Global entity projection
v1.3.0,Global relation projection
v1.3.0,Global combination bias
v1.3.0,Global combination bias
v1.3.0,Core tensor
v1.3.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.3.0,Dropout
v1.3.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.3.0,"batch norm gets reset automatically, since it defines reset_parameters"
v1.3.0,shapes
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The batch size of the head representations.
v1.3.0,: The number of head representations per batch
v1.3.0,: The batch size of the relation representations.
v1.3.0,: The number of relation representations per batch
v1.3.0,: The batch size of the tail representations.
v1.3.0,: The number of tail representations per batch
v1.3.0,"repeat if necessary, and concat head and relation, batch_size', num_input_channels, 2*height, width"
v1.3.0,with batch_size' = batch_size * num_heads * num_relations
v1.3.0,"batch_size', num_input_channels, 2*height, width"
v1.3.0,"batch_size', num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.3.0,"reshape: (batch_size', embedding_dim) -> (b, h, r, 1, d)"
v1.3.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.3.0,"output_shape: (batch_size, num_heads, num_relations, num_tails)"
v1.3.0,add bias term
v1.3.0,decompose convolution for faster computation in 1-n case
v1.3.0,"compute conv(stack(h, r, t))"
v1.3.0,prepare input shapes for broadcasting
v1.3.0,"(b, h, r, t, 1, d)"
v1.3.0,"conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])"
v1.3.0,"here, kernel_size = (1, 3), C_in = 1, C_out = num_filters"
v1.3.0,"-> conv_head, conv_rel, conv_tail shapes: (num_filters,)"
v1.3.0,"reshape to (1, 1, 1, 1, f, 1)"
v1.3.0,"convolve -> output.shape: (*, embedding_dim, num_filters)"
v1.3.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.3.0,"Linear layer for final scores; use flattened representations, shape: (b, h, r, t, d * f)"
v1.3.0,same shape
v1.3.0,"split, shape: (embedding_dim, hidden_dim)"
v1.3.0,"repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)"
v1.3.0,"Predict t embedding, shape: (b, h, r, 1, d)"
v1.3.0,"transpose t, (b, 1, 1, d, t)"
v1.3.0,"dot product, (b, h, r, 1, t)"
v1.3.0,Circular correlation of entity embeddings
v1.3.0,complex conjugate
v1.3.0,Hadamard product in frequency domain
v1.3.0,"inverse real FFT, shape: (b, h, 1, t, d)"
v1.3.0,"transpose composite: (b, h, 1, d, t)"
v1.3.0,inner product with relation embedding
v1.3.0,global projections
v1.3.0,"combination, shape: (b, h, r, 1, d)"
v1.3.0,"dot product with t, shape: (b, h, r, t)"
v1.3.0,r expresses a rotation in complex plane.
v1.3.0,rotate head by relation (=Hadamard product in complex space)
v1.3.0,rotate tail by inverse of relation
v1.3.0,The inverse rotation is expressed by the complex conjugate of r.
v1.3.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.3.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.3.0,|h * r - t| = |h - conj(r) * t|
v1.3.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.3.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.3.0,"That is not mentioned in the paper, so it is made optional here."
v1.3.0,Project entities
v1.3.0,h projection to hyperplane
v1.3.0,r
v1.3.0,-t projection to hyperplane
v1.3.0,project to relation specific subspace and ensure constraints
v1.3.0,x_3 contraction
v1.3.0,x_1 contraction
v1.3.0,x_2 contraction
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,don't worry about functions because they can't be specified by JSON.
v1.3.0,Could make a better mo
v1.3.0,later could extend for other non-JSON valid types
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Score with original triples
v1.3.0,Score with inverse triples
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Create directory in which all experimental artifacts are saved
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: Functions for specifying exotic resources with a given prefix
v1.3.0,: Functions for specifying exotic resources based on their file extension
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.3.0,TODO vectorize code
v1.3.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Split triples
v1.3.0,Sorting ensures consistent results when the triples are permuted
v1.3.0,Create mapping
v1.3.0,Sorting ensures consistent results when the triples are permuted
v1.3.0,Create mapping
v1.3.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.3.0,Filter all non-existent triples
v1.3.0,Note: Unique changes the order of the triples
v1.3.0,Note: Using unique means implicit balancing of training samples
v1.3.0,normalize input
v1.3.0,: The mapping from labels to IDs.
v1.3.0,: The inverse mapping for label_to_id; initialized automatically
v1.3.0,: A vectorized version of entity_label_to_id; initialized automatically
v1.3.0,: A vectorized version of entity_id_to_label; initialized automatically
v1.3.0,Normalize input
v1.3.0,label
v1.3.0,check new label to ID mappings
v1.3.0,Make new triples factories for each group
v1.3.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.3.0,Input validation
v1.3.0,convert to numpy
v1.3.0,Additional columns
v1.3.0,convert PyTorch tensors to numpy
v1.3.0,convert to dataframe
v1.3.0,Re-order columns
v1.3.0,Filter for entities
v1.3.0,Filter for relations
v1.3.0,No filtering happened
v1.3.0,Check if the triples are inverted already
v1.3.0,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.3.0,contained if and only if create_inverse_triples is True.
v1.3.0,Generate entity mapping if necessary
v1.3.0,Generate relation mapping if necessary
v1.3.0,Map triples of labels to triples of IDs.
v1.3.0,TODO: Check if lazy evaluation would make sense
v1.3.0,pre-filter to keep only topk
v1.3.0,generate text
v1.3.0,vectorized label lookup
v1.3.0,Re-order columns
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Split indices
v1.3.0,Split triples
v1.3.0,index
v1.3.0,select
v1.3.0,Prepare split index
v1.3.0,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.3.0,[...] is necessary for Python 3.7 compatibility
v1.3.0,While there are still triples that should be moved to the training set
v1.3.0,Pick a random triple to move over to the training triples
v1.3.0,add to training
v1.3.0,remove from testing
v1.3.0,Recalculate the move_id_mask
v1.3.0,base cases
v1.3.0,IDs not in training
v1.3.0,triples with exclusive test IDs
v1.3.0,Make sure that the first element has all the right stuff in it
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,": The mapped triples, shape: (num_triples, 3)"
v1.3.0,: The unique pairs
v1.3.0,: The compressed triples in CSR format
v1.3.0,convert to csr for fast row slicing
v1.3.0,: TODO: do we need these?
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,check validity
v1.3.0,path compression
v1.3.0,collect connected components using union find with path compression
v1.3.0,get representatives
v1.3.0,already merged
v1.3.0,make x the smaller one
v1.3.0,merge
v1.3.0,extract partitions
v1.3.0,safe division for empty sets
v1.3.0,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.3.0,duplicates
v1.3.0,we are not interested in self-similarity
v1.3.0,compute similarities
v1.3.0,Calculate which relations are the inverse ones
v1.3.0,get existing IDs
v1.3.0,remove non-existing ID from label mapping
v1.3.0,create translation tensor
v1.3.0,get entities and relations occurring in triples
v1.3.0,generate ID translation and new label to Id mappings
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,preprocessing
v1.3.0,initialize
v1.3.0,sample iteratively
v1.3.0,determine weights
v1.3.0,only happens at first iteration
v1.3.0,normalize to probabilities
v1.3.0,sample a start node
v1.3.0,get list of neighbors
v1.3.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.3.0,visit target node
v1.3.0,decrease sample counts
v1.3.0,return chosen edges
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.3.0,seamless loading and saving of training checkpoints
v1.3.0,Create training instances
v1.3.0,During size probing the training instances should not show the tqdm progress bar
v1.3.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.3.0,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.3.0,"If a checkpoint file is given, it must be loaded if it exists already"
v1.3.0,"If the stopper dict has any keys, those are written back to the stopper"
v1.3.0,The checkpoint frequency needs to be set to save checkpoints
v1.3.0,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.3.0,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.3.0,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.3.0,Ensure the release of memory
v1.3.0,Clear optimizer
v1.3.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.3.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.3.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.3.0,return the relevant parameters slice_size and batch_size
v1.3.0,Create dummy result tracker
v1.3.0,Sanity check
v1.3.0,Force weight initialization if training continuation is not explicitly requested.
v1.3.0,Reset the weights
v1.3.0,Create new optimizer
v1.3.0,Ensure the model is on the correct device
v1.3.0,Create Sampler
v1.3.0,Bind
v1.3.0,"When size probing, we don't want progress bars"
v1.3.0,Create progress bar
v1.3.0,Save the time to track when the saved point was available
v1.3.0,Training Loop
v1.3.0,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.3.0,Enforce training mode
v1.3.0,Accumulate loss over epoch
v1.3.0,Batching
v1.3.0,Only create a progress bar when not in size probing mode
v1.3.0,Flag to check when to quit the size probing
v1.3.0,Recall that torch *accumulates* gradients. Before passing in a
v1.3.0,"new instance, you need to zero out the gradients from the old instance"
v1.3.0,Get batch size of current batch (last batch may be incomplete)
v1.3.0,accumulate gradients for whole batch
v1.3.0,forward pass call
v1.3.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.3.0,update parameters according to optimizer
v1.3.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.3.0,constraints are no longer applied
v1.3.0,For testing purposes we're only interested in processing one batch
v1.3.0,When size probing we don't need the losses
v1.3.0,Track epoch loss
v1.3.0,Print loss information to console
v1.3.0,Save the last successful finished epoch
v1.3.0,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.3.0,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.3.0,MyPy overrides are because you should
v1.3.0,forward pass
v1.3.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.3.0,correction for loss reduction
v1.3.0,backward pass
v1.3.0,TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?
v1.3.0,Set upper bound
v1.3.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.3.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.3.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.3.0,"Only if a cuda device is available, the random state is accessed"
v1.3.0,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Shuffle each epoch
v1.3.0,Lazy-splitting into batches
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Slicing is not possible in sLCWA training loops
v1.3.0,Send positive batch to device
v1.3.0,Create negative samples
v1.3.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.3.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.3.0,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.3.0,Compute negative and positive scores
v1.3.0,Repeat positives scores (necessary for more than one negative per positive)
v1.3.0,Stack predictions
v1.3.0,Create target
v1.3.0,Normalize the loss to have the average loss per positive triple
v1.3.0,This allows comparability of sLCWA and LCWA losses
v1.3.0,Slicing is not possible for sLCWA
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: A mapping of training loops' names to their implementations
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Split batch components
v1.3.0,Send batch to device
v1.3.0,Apply label smoothing
v1.3.0,This shows how often one row has to be repeated
v1.3.0,Create boolean indices for negative labels in the repeated rows
v1.3.0,Repeat the predictions and filter for negative labels
v1.3.0,This tells us how often each true label should be repeated
v1.3.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.3.0,Split positive and negative scores
v1.3.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.3.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,now: smaller is better
v1.3.0,: The model
v1.3.0,: The evaluator
v1.3.0,: The triples to use for evaluation
v1.3.0,: Size of the evaluation batches
v1.3.0,: Slice size of the evaluation batches
v1.3.0,: The number of epochs after which the model is evaluated on validation set
v1.3.0,: The number of iterations (one iteration can correspond to various epochs)
v1.3.0,: with no improvement after which training will be stopped.
v1.3.0,: The name of the metric to use
v1.3.0,: The minimum relative improvement necessary to consider it an improved result
v1.3.0,: The best result so far
v1.3.0,: The epoch at which the best result occurred
v1.3.0,: The remaining patience
v1.3.0,: The metric results from all evaluations
v1.3.0,": Whether a larger value is better, or a smaller"
v1.3.0,: The result tracker
v1.3.0,: Callbacks when after results are calculated
v1.3.0,: Callbacks when training gets continued
v1.3.0,: Callbacks when training is stopped early
v1.3.0,: Did the stopper ever decide to stop?
v1.3.0,TODO: Fix this
v1.3.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.3.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.3.0,Evaluate
v1.3.0,Only perform time consuming checks for the first call.
v1.3.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.3.0,Append to history
v1.3.0,check for improvement
v1.3.0,Stop if the result did not improve more than delta for patience evaluations
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: A mapping of stoppers' names to their implementations
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.3.0,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.3.0,Clear the ranks from the current evaluator
v1.3.0,"Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that"
v1.3.0,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.3.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.3.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.3.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.3.0,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.3.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.3.0,values_dict[key] will always be an int at this point
v1.3.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.3.0,Test if slicing is implemented for the required functions of this model
v1.3.0,Split batch
v1.3.0,Bind shape
v1.3.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.3.0,Warn if all entities will be filtered
v1.3.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.3.0,verify that the triples have been filtered
v1.3.0,Send to device
v1.3.0,Ensure evaluation mode
v1.3.0,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.3.0,Check whether we need to be prepared for filtering
v1.3.0,Check whether an evaluator needs access to the masks
v1.3.0,This can only be an unfiltered evaluator.
v1.3.0,Prepare for result filtering
v1.3.0,Send tensors to device
v1.3.0,Prepare batches
v1.3.0,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.3.0,Show progressbar
v1.3.0,Flag to check when to quit the size probing
v1.3.0,Disable gradient tracking
v1.3.0,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.3.0,batch-wise processing
v1.3.0,If we only probe sizes we do not need more than one batch
v1.3.0,Finalize
v1.3.0,Predict scores once
v1.3.0,Select scores of true
v1.3.0,Create positive filter for all corrupted
v1.3.0,Needs all positive triples
v1.3.0,Create filter
v1.3.0,Create a positive mask with the size of the scores from the positive filter
v1.3.0,Restrict to entities of interest
v1.3.0,Evaluate metrics on these *unfiltered* scores
v1.3.0,Filter
v1.3.0,The scores for the true triples have to be rewritten to the scores tensor
v1.3.0,Restrict to entities of interest
v1.3.0,Evaluate metrics on these *filtered* scores
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The area under the ROC curve
v1.3.0,: The area under the precision-recall curve
v1.3.0,: The coverage error
v1.3.0,coverage_error: float = field(metadata=dict(
v1.3.0,"doc='The coverage error',"
v1.3.0,"f=metrics.coverage_error,"
v1.3.0,))
v1.3.0,: The label ranking loss (APS)
v1.3.0,label_ranking_average_precision_score: float = field(metadata=dict(
v1.3.0,"doc='The label ranking loss (APS)',"
v1.3.0,"f=metrics.label_ranking_average_precision_score,"
v1.3.0,))
v1.3.0,#: The label ranking loss
v1.3.0,label_ranking_loss: float = field(metadata=dict(
v1.3.0,"doc='The label ranking loss',"
v1.3.0,"f=metrics.label_ranking_loss,"
v1.3.0,))
v1.3.0,Transfer to cpu and convert to numpy
v1.3.0,Ensure that each key gets counted only once
v1.3.0,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.3.0,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.3.0,masks using the exact same key order.
v1.3.0,TODO how to define a cutoff on y_scores to make binary?
v1.3.0,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.3.0,Clear buffers
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: A mapping of evaluators' names to their implementations
v1.3.0,: A mapping of results' names to their implementations
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,The best rank is the rank when assuming all options with an equal score are placed behind the currently
v1.3.0,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.3.0,The worst rank is the rank when assuming all options with an equal score are placed in front of the currently
v1.3.0,"considered. Hence, the rank is the number of options which have at least the same score minus one (as the"
v1.3.0,"currently considered option in included in all options). As the rank is one-based, we have to add 1, which"
v1.3.0,"nullifies the ""minus 1"" from before."
v1.3.0,"The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of"
v1.3.0,the elements with the same score as the currently considered option.
v1.3.0,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.3.0,The expected rank of a random scoring
v1.3.0,The adjusted ranks is normalized by the expected rank of a random scoring
v1.3.0,TODO adjusted_worst_rank
v1.3.0,TODO adjusted_best_rank
v1.3.0,: The mean over all ranks: mean_i r_i. Lower is better.
v1.3.0,: The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.
v1.3.0,": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k."
v1.3.0,: Higher is better.
v1.3.0,: The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.
v1.3.0,: Described by [berrendorf2020]_.
v1.3.0,Check if it a side or rank type
v1.3.0,Clear buffers
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Train a model (quickly)
v1.3.0,Get scores for *all* triples
v1.3.0,Get scores for top 15 triples
v1.3.0,initialize buffer on cpu
v1.3.0,calculate batch scores
v1.3.0,Explicitly create triples
v1.3.0,initialize buffer on device
v1.3.0,calculate batch scores
v1.3.0,get top scores within batch
v1.3.0,append to global top scores
v1.3.0,reduce size if necessary
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: Keep track of if this is a base model
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: A triples factory with the training triples
v1.3.0,: The device on which this model and its submodules are stored
v1.3.0,: The default loss function class
v1.3.0,: The default parameters for the default loss function class
v1.3.0,: The instance of the loss
v1.3.0,Initialize the device
v1.3.0,Random seeds have to set before the embeddings are initialized
v1.3.0,Loss
v1.3.0,The triples factory facilitates access to the dataset.
v1.3.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.3.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.3.0,Id of inverse relation: relation + 1
v1.3.0,: The default regularizer class
v1.3.0,: The default parameters for the default regularizer class
v1.3.0,: The instance of the regularizer
v1.3.0,Regularizer
v1.3.0,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.3.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.3.0,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.3.0,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.3.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.3.0,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.3.0,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.3.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.3.0,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.3.0,"make sure to call this first, to reset regularizer state!"
v1.3.0,"make sure to call this first, to reset regularizer state!"
v1.3.0,The following lines add in a post-init hook to all subclasses
v1.3.0,such that the reset_parameters_() function is run
v1.3.0,"sorry mypy, but this kind of evil must be permitted."
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Base Models
v1.3.0,Concrete Models
v1.3.0,: A mapping of models' names to their implementations
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,TODO rethink after RGCN update
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The default loss function class
v1.3.0,: The default parameters for the default loss function class
v1.3.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.3.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.3.0,ConvE should be trained with inverse triples
v1.3.0,ConvE uses one bias for each entity
v1.3.0,Automatic calculation of remaining dimensions
v1.3.0,Parameter need to fulfil:
v1.3.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.3.0,weights
v1.3.0,"batch_size, num_input_channels, 2*height, width"
v1.3.0,"batch_size, num_input_channels, 2*height, width"
v1.3.0,"batch_size, num_input_channels, 2*height, width"
v1.3.0,"(N,C_out,H_out,W_out)"
v1.3.0,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.3.0,Embedding Regularization
v1.3.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.3.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.3.0,Embedding Regularization
v1.3.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.3.0,Embedding Regularization
v1.3.0,Code to repeat each item successively instead of the entire tensor
v1.3.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,Embeddings
v1.3.0,Initialise relation embeddings to unit length
v1.3.0,Get embeddings
v1.3.0,Project entities
v1.3.0,Get embeddings
v1.3.0,Project entities
v1.3.0,Project entities
v1.3.0,Project entities
v1.3.0,Get embeddings
v1.3.0,Project entities
v1.3.0,Project entities
v1.3.0,Project entities
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The default loss function class
v1.3.0,: The default parameters for the default loss function class
v1.3.0,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.3.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.3.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.3.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.3.0,split into real and imaginary part
v1.3.0,ComplEx space bilinear product
v1.3.0,*: Elementwise multiplication
v1.3.0,get embeddings
v1.3.0,Regularization
v1.3.0,Compute scores
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.3.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.3.0,: a normalized weight of 10 is used.
v1.3.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.3.0,Get embeddings
v1.3.0,"shape: (b, d)"
v1.3.0,"shape: (b, d, d)"
v1.3.0,"shape: (b, d)"
v1.3.0,Compute scores
v1.3.0,Regularization
v1.3.0,Compute scores
v1.3.0,Regularization
v1.3.0,Get embeddings
v1.3.0,Compute scores
v1.3.0,Regularization
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.3.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.3.0,The interaction model
v1.3.0,embeddings
v1.3.0,Use Xavier initialization for weight; bias to zero
v1.3.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.3.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.3.0,Output layer regularization
v1.3.0,In the code base only the weights of the output layer are used for regularization
v1.3.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.3.0,Stack to convolution input
v1.3.0,Convolution
v1.3.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.3.0,Linear layer for final scores
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,": shape: (batch_size, num_entities, d)"
v1.3.0,": Prepare h: (b, e, d) -> (b, e, 1, 1, d)"
v1.3.0,": Prepare t: (b, e, d) -> (b, e, 1, d, 1)"
v1.3.0,": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)"
v1.3.0,"h.T @ W @ t, shape: (b, e, k, 1, 1)"
v1.3.0,": reduce (b, e, k, 1, 1) -> (b, e, k)"
v1.3.0,": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.3.0,": Prepare h: (b, e, d) -> (b, e, d, 1)"
v1.3.0,"V_h @ h, shape: (b, e, k, 1)"
v1.3.0,": reduce (b, e, k, 1) -> (b, e, k)"
v1.3.0,": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.3.0,": Prepare t: (b, e, d) -> (b, e, d, 1)"
v1.3.0,"V_t @ t, shape: (b, e, k, 1)"
v1.3.0,": reduce (b, e, k, 1) -> (b, e, k)"
v1.3.0,": Prepare b: (R, k) -> (b, k) -> (b, 1, k)"
v1.3.0,"a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)"
v1.3.0,"prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)"
v1.3.0,"prepare act: (b, e, k) -> (b, e, 1, k)"
v1.3.0,"compute score, shape: (b, e, 1, 1)"
v1.3.0,reduce
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The regularizer used by [yang2014]_ for DistMult
v1.3.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.3.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.3.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.3.0,: The LP settings used by [yang2014]_ for DistMult
v1.3.0,Bilinear product
v1.3.0,*: Elementwise multiplication
v1.3.0,Get embeddings
v1.3.0,Compute score
v1.3.0,Only regularize relation embeddings
v1.3.0,Get embeddings
v1.3.0,Rank against all entities
v1.3.0,Only regularize relation embeddings
v1.3.0,Get embeddings
v1.3.0,Rank against all entities
v1.3.0,Only regularize relation embeddings
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The default settings for the entity constrainer
v1.3.0,Similarity function used for distributions
v1.3.0,element-wise covariance bounds
v1.3.0,Additional covariance embeddings
v1.3.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.3.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.3.0,Constraints are applied through post_parameter_update
v1.3.0,Get embeddings
v1.3.0,Compute entity distribution
v1.3.0,: a = \mu^T\Sigma^{-1}\mu
v1.3.0,: b = \log \det \Sigma
v1.3.0,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.3.0,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.3.0,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.3.0,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The custom regularizer used by [wang2014]_ for TransH
v1.3.0,: The settings used by [wang2014]_ for TransH
v1.3.0,embeddings
v1.3.0,Normalise the normal vectors by their l2 norms
v1.3.0,TODO: Add initialization
v1.3.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.3.0,which enforces the defined soft constraints.
v1.3.0,Get embeddings
v1.3.0,Project to hyperplane
v1.3.0,Regularization term
v1.3.0,Get embeddings
v1.3.0,Project to hyperplane
v1.3.0,Regularization term
v1.3.0,Get embeddings
v1.3.0,Project to hyperplane
v1.3.0,Regularization term
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,TODO: Initialize from TransE
v1.3.0,embeddings
v1.3.0,"project to relation specific subspace, shape: (b, e, d_r)"
v1.3.0,ensure constraints
v1.3.0,"evaluate score function, shape: (b, e)"
v1.3.0,Get embeddings
v1.3.0,Get embeddings
v1.3.0,Get embeddings
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Construct node neighbourhood mask
v1.3.0,Set nodes in batch to true
v1.3.0,Compute k-neighbourhood
v1.3.0,"if the target node needs an embeddings, so does the source node"
v1.3.0,Create edge mask
v1.3.0,pylint: disable=unused-argument
v1.3.0,"Calculate in-degree, i.e. number of incoming edges"
v1.3.0,pylint: disable=unused-argument
v1.3.0,"Calculate in-degree, i.e. number of incoming edges"
v1.3.0,normalize representations
v1.3.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.3.0,check decomposition
v1.3.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.3.0,Weights
v1.3.0,buffering of messages
v1.3.0,allocate weight
v1.3.0,Get blocks
v1.3.0,"self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)"
v1.3.0,note: embedding_dim is guaranteed to be divisible by num_bases in the constructor
v1.3.0,"The current basis weights, shape: (num_bases)"
v1.3.0,"the current bases, shape: (num_bases, embedding_dim, embedding_dim)"
v1.3.0,"compute the current relation weights, shape: (embedding_dim, embedding_dim)"
v1.3.0,use buffered messages if applicable
v1.3.0,Bind fields
v1.3.0,"shape: (num_entities, embedding_dim)"
v1.3.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.3.0,Get random dropout mask
v1.3.0,Apply to edges
v1.3.0,Different dropout for self-loops (only in training mode)
v1.3.0,Initialize embeddings in the next layer for all nodes
v1.3.0,TODO: Can we vectorize this loop?
v1.3.0,Choose the edges which are of the specific relation
v1.3.0,No edges available? Skip rest of inner loop
v1.3.0,Get source and target node indices
v1.3.0,send messages in both directions
v1.3.0,Select source node embeddings
v1.3.0,get relation weights
v1.3.0,Compute message (b x d) * (d x d) = (b x d)
v1.3.0,Normalize messages by relation-specific in-degree
v1.3.0,Aggregate messages in target
v1.3.0,Self-loop
v1.3.0,"Apply bias, if requested"
v1.3.0,"Apply batch normalization, if requested"
v1.3.0,Apply non-linearity
v1.3.0,invalidate enriched embeddings
v1.3.0,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.3.0,initialized properly)
v1.3.0,We have one additional relation for self-loops
v1.3.0,Xavier Glorot initialization of each block
v1.3.0,Reset biases
v1.3.0,Reset batch norm parameters
v1.3.0,"Reset activation parameters, if any"
v1.3.0,"TODO: Replace this by interaction function, once https://github.com/pykeen/pykeen/pull/107 is merged."
v1.3.0,: Interaction model used as decoder
v1.3.0,: The blocks of the relation-specific weight matrices
v1.3.0,": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)"
v1.3.0,: The base weight matrices to generate relation-specific weights
v1.3.0,": shape: (num_bases, embedding_dim, embedding_dim)"
v1.3.0,: The relation-specific weights for each base
v1.3.0,": shape: (num_relations, num_bases)"
v1.3.0,: The biases for each layer (if used)
v1.3.0,": shape of each element: (embedding_dim,)"
v1.3.0,: Batch normalization for each layer (if used)
v1.3.0,: Activations for each layer (if used)
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,TODO: Dummy
v1.3.0,Enrich embeddings
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The default loss function class
v1.3.0,: The default parameters for the default loss function class
v1.3.0,Core tensor
v1.3.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.3.0,Dropout
v1.3.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.3.0,Abbreviation
v1.3.0,Compute h_n = DO(BN(h))
v1.3.0,Compute wr = DO(W x_2 r)
v1.3.0,compute whr = DO(BN(h_n x_1 wr))
v1.3.0,Compute whr x_3 t
v1.3.0,Get embeddings
v1.3.0,Compute scores
v1.3.0,Get embeddings
v1.3.0,Compute scores
v1.3.0,Get embeddings
v1.3.0,Compute scores
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,Get embeddings
v1.3.0,TODO: Use torch.dist
v1.3.0,Get embeddings
v1.3.0,TODO: Use torch.cdist
v1.3.0,Get embeddings
v1.3.0,TODO: Use torch.cdist
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The default loss function class
v1.3.0,: The default parameters for the default loss function class
v1.3.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.3.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.3.0,": regularization term by the number of elements, which is 200."
v1.3.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.3.0,extra embeddings
v1.3.0,forward model
v1.3.0,Regularization
v1.3.0,backward model
v1.3.0,Regularization
v1.3.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.3.0,"That is not mentioned in the paper, so it is omitted here."
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.3.0,weight initialization
v1.3.0,Get embeddings
v1.3.0,Embedding Regularization
v1.3.0,Concatenate them
v1.3.0,Compute scores
v1.3.0,Get embeddings
v1.3.0,Embedding Regularization
v1.3.0,First layer can be unrolled
v1.3.0,Send scores through rest of the network
v1.3.0,Get embeddings
v1.3.0,Embedding Regularization
v1.3.0,First layer can be unrolled
v1.3.0,Send scores through rest of the network
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,The dimensions affected by e'
v1.3.0,Project entities
v1.3.0,r_p (e_p.T e) + e'
v1.3.0,Enforce constraints
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,Project entities
v1.3.0,score = -||h_bot + r - t_bot||_2^2
v1.3.0,Head
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,Decompose into real and imaginary part
v1.3.0,Rotate (=Hadamard product in complex space).
v1.3.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.3.0,Get embeddings
v1.3.0,Compute scores
v1.3.0,Embedding Regularization
v1.3.0,Get embeddings
v1.3.0,Rank against all entities
v1.3.0,Compute scores
v1.3.0,Embedding Regularization
v1.3.0,Get embeddings
v1.3.0,r expresses a rotation in complex plane.
v1.3.0,The inverse rotation is expressed by the complex conjugate of r.
v1.3.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.3.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.3.0,|h * r - t| = |h - conj(r) * t|
v1.3.0,Rank against all entities
v1.3.0,Compute scores
v1.3.0,Embedding Regularization
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The default loss function class
v1.3.0,: The default parameters for the default loss function class
v1.3.0,Global entity projection
v1.3.0,Global relation projection
v1.3.0,Global combination bias
v1.3.0,Global combination bias
v1.3.0,Get embeddings
v1.3.0,Compute score
v1.3.0,Get embeddings
v1.3.0,Rank against all entities
v1.3.0,Get embeddings
v1.3.0,Rank against all entities
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The default settings for the entity constrainer
v1.3.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.3.0,Circular correlation of entity embeddings
v1.3.0,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.3.0,Hadamard product in frequency domain
v1.3.0,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.3.0,inner product with relation embedding
v1.3.0,Embedding Regularization
v1.3.0,Embedding Regularization
v1.3.0,Embedding Regularization
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The default loss function class
v1.3.0,: The default parameters for the default loss function class
v1.3.0,Get embeddings
v1.3.0,Embedding Regularization
v1.3.0,Concatenate them
v1.3.0,Predict t embedding
v1.3.0,compare with all t's
v1.3.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.3.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.3.0,Embedding Regularization
v1.3.0,Concatenate them
v1.3.0,Predict t embedding
v1.3.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.3.0,Embedding Regularization
v1.3.0,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.3.0,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.3.0,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.3.0,Extend t
v1.3.0,Concatenate them
v1.3.0,Predict t embedding
v1.3.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.3.0,The results have to be realigned with the expected output of the score_h function
v1.3.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The default loss function class
v1.3.0,: The default parameters for the default loss function class
v1.3.0,Literal
v1.3.0,num_ent x num_lit
v1.3.0,Number of columns corresponds to number of literals
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the model's hyper-parameters
v1.3.0,: The default parameters for the default loss function class
v1.3.0,Literal
v1.3.0,num_ent x num_lit
v1.3.0,Number of columns corresponds to number of literals
v1.3.0,"TODO: this is very similar to ComplExLiteral, except a few dropout differences"
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The WANDB run
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,Base classes
v1.3.0,Concrete classes
v1.3.0,Utilities
v1.3.0,: A mapping of trackers' names to their implementations
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The file extension for this writer (do not include dot)
v1.3.0,: The file where the results are written to.
v1.3.0,as_uri() requires the path to be absolute. resolve additionally also normalizes the path
v1.3.0,: The column names
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.3.0,Set the indices
v1.3.0,Bind number of negatives to sample
v1.3.0,Equally corrupt all sides
v1.3.0,Copy positive batch for corruption.
v1.3.0,"Do not detach, as no gradients should flow into the indices."
v1.3.0,Relations have a different index maximum than entities
v1.3.0,"To make sure we don't replace the {head, relation, tail} by the"
v1.3.0,original value we shift all values greater or equal than the original value by one up
v1.3.0,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.3.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.3.0,Create mapped triples attribute that is required for filtering
v1.3.0,Make sure the mapped triples are initiated
v1.3.0,Copy the mapped triples to the device for efficient filtering
v1.3.0,Check which heads of the mapped triples are also in the negative triples
v1.3.0,Reduce the search space by only using possible matches that at least contain the head we look for
v1.3.0,Check in this subspace which relations of the mapped triples are also in the negative triples
v1.3.0,Reduce the search space by only using possible matches that at least contain head and relation we look for
v1.3.0,Create a filter indicating which of the proposed negative triples are positive in the training dataset
v1.3.0,"In cases where no triples should be filtered, the subspace reduction technique above will fail"
v1.3.0,Return only those proposed negative triples that are not positive in the training dataset
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: A mapping of negative samplers' names to their implementations
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.3.0,Preprocessing: Compute corruption probabilities
v1.3.0,"compute tph, i.e. the average number of tail entities per head"
v1.3.0,"compute hpt, i.e. the average number of head entities per tail"
v1.3.0,Set parameter for Bernoulli distribution
v1.3.0,Bind number of negatives to sample
v1.3.0,Copy positive batch for corruption.
v1.3.0,"Do not detach, as no gradients should flow into the indices."
v1.3.0,Decide whether to corrupt head or tail
v1.3.0,Tails are corrupted if heads are not corrupted
v1.3.0,Randomly sample corruption. See below for explanation of
v1.3.0,"why this is on a range of [0, num_entities - 1]"
v1.3.0,Replace heads
v1.3.0,Replace tails
v1.3.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.3.0,To make sure we don't replace the head by the original value
v1.3.0,we shift all values greater or equal than the original value by one up
v1.3.0,"for that reason we choose the random value from [0, num_entities -1]"
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,TODO what happens if already exists?
v1.3.0,TODO incorporate setting of random seed
v1.3.0,pipeline_kwargs=dict(
v1.3.0,"random_seed=random_non_negative_int(),"
v1.3.0,"),"
v1.3.0,Add dataset to current_pipeline
v1.3.0,"Training, test, and validation paths are provided"
v1.3.0,Add loss function to current_pipeline
v1.3.0,Add regularizer to current_pipeline
v1.3.0,Add optimizer to current_pipeline
v1.3.0,Add training approach to current_pipeline
v1.3.0,Add training kwargs and kwargs_ranges
v1.3.0,Add evaluation
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.3.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.3.0,GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is
v1.3.0,"normally the default for all of PyKEEN's remote datasets, so just switch the default here."
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is
v1.3.0,"normally the default for all of PyKEEN's remote datasets, so just switch the default here."
v1.3.0,"as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,"
v1.3.0,the columns are not ordered properly.
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: The name of the dataset to download
v1.3.0,FIXME these are already identifiers
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,don't call this function by itself. assumes called through the `validation`
v1.3.0,property and the _training factory has already been loaded
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: A factory wrapping the training triples
v1.3.0,": A factory wrapping the testing triples, that share indices with the training triples"
v1.3.0,": A factory wrapping the validation triples, that share indices with the training triples"
v1.3.0,: All datasets should take care of inverse triple creation
v1.3.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.3.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.3.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.3.0,: The directory in which the cached data is stored
v1.3.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.3.0,don't call this function by itself. assumes called through the `validation`
v1.3.0,property and the _training factory has already been loaded
v1.3.0,do not explicitly create inverse triples for testing; this is handled by the evaluation code
v1.3.0,see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content
v1.3.0,pattern from https://stackoverflow.com/a/39217788/5775947
v1.3.0,TODO replace this with the new zip remote dataset class
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: A mapping of datasets' names to their classes
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,TODO update docs with table and CLI wtih generator
v1.3.0,: A mapping of HPO samplers' names to their implementations
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,1. Dataset
v1.3.0,2. Model
v1.3.0,3. Loss
v1.3.0,4. Regularizer
v1.3.0,5. Optimizer
v1.3.0,6. Training Loop
v1.3.0,7. Training
v1.3.0,8. Evaluation
v1.3.0,9. Trackers
v1.3.0,Misc.
v1.3.0,2. Model
v1.3.0,3. Loss
v1.3.0,4. Regularizer
v1.3.0,5. Optimizer
v1.3.0,1. Dataset
v1.3.0,2. Model
v1.3.0,3. Loss
v1.3.0,4. Regularizer
v1.3.0,5. Optimizer
v1.3.0,6. Training Loop
v1.3.0,7. Training
v1.3.0,8. Evaluation
v1.3.0,9. Tracker
v1.3.0,Misc.
v1.3.0,Will trigger Optuna to set the state of the trial as failed
v1.3.0,: The :mod:`optuna` study object
v1.3.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.3.0,Output study information
v1.3.0,Output all trials
v1.3.0,Output best trial as pipeline configuration file
v1.3.0,1. Dataset
v1.3.0,2. Model
v1.3.0,3. Loss
v1.3.0,4. Regularizer
v1.3.0,5. Optimizer
v1.3.0,6. Training Loop
v1.3.0,7. Training
v1.3.0,8. Evaluation
v1.3.0,9. Tracking
v1.3.0,6. Misc
v1.3.0,Optuna Study Settings
v1.3.0,Optuna Optimization Settings
v1.3.0,0. Metadata/Provenance
v1.3.0,1. Dataset
v1.3.0,2. Model
v1.3.0,3. Loss
v1.3.0,4. Regularizer
v1.3.0,5. Optimizer
v1.3.0,6. Training Loop
v1.3.0,7. Training
v1.3.0,8. Evaluation
v1.3.0,9. Tracking
v1.3.0,1. Dataset
v1.3.0,2. Model
v1.3.0,3. Loss
v1.3.0,4. Regularizer
v1.3.0,5. Optimizer
v1.3.0,6. Training Loop
v1.3.0,7. Training
v1.3.0,8. Evaluation
v1.3.0,9. Tracker
v1.3.0,Optuna Misc.
v1.3.0,Pipeline Misc.
v1.3.0,Invoke optimization of the objective function.
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,-*- coding: utf-8 -*-
v1.3.0,: A mapping of HPO pruners' names to their implementations
v1.3.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,
v1.1.0,Configuration file for the Sphinx documentation builder.
v1.1.0,
v1.1.0,This file does only contain a selection of the most common options. For a
v1.1.0,full list see the documentation:
v1.1.0,http://www.sphinx-doc.org/en/master/config
v1.1.0,-- Path setup --------------------------------------------------------------
v1.1.0,"If extensions (or modules to document with autodoc) are in another directory,"
v1.1.0,add these directories to sys.path here. If the directory is relative to the
v1.1.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.1.0,
v1.1.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.1.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.1.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.1.0,from unittest.mock import Mock
v1.1.0,sys.modules['numpy'] = Mock()
v1.1.0,sys.modules['numpy.linalg'] = Mock()
v1.1.0,sys.modules['scipy'] = Mock()
v1.1.0,sys.modules['scipy.optimize'] = Mock()
v1.1.0,sys.modules['scipy.interpolate'] = Mock()
v1.1.0,sys.modules['scipy.sparse'] = Mock()
v1.1.0,sys.modules['scipy.ndimage'] = Mock()
v1.1.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.1.0,sys.modules['tensorflow'] = Mock()
v1.1.0,sys.modules['theano'] = Mock()
v1.1.0,sys.modules['theano.tensor'] = Mock()
v1.1.0,sys.modules['torch'] = Mock()
v1.1.0,sys.modules['torch.optim'] = Mock()
v1.1.0,sys.modules['torch.nn'] = Mock()
v1.1.0,sys.modules['torch.nn.init'] = Mock()
v1.1.0,sys.modules['torch.autograd'] = Mock()
v1.1.0,sys.modules['sklearn'] = Mock()
v1.1.0,sys.modules['sklearn.model_selection'] = Mock()
v1.1.0,sys.modules['sklearn.utils'] = Mock()
v1.1.0,-- Project information -----------------------------------------------------
v1.1.0,"The full version, including alpha/beta/rc tags."
v1.1.0,The short X.Y version.
v1.1.0,-- General configuration ---------------------------------------------------
v1.1.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.1.0,
v1.1.0,needs_sphinx = '1.0'
v1.1.0,"If true, the current module name will be prepended to all description"
v1.1.0,unit titles (such as .. function::).
v1.1.0,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.1.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.1.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.1.0,ones.
v1.1.0,generate autosummary pages
v1.1.0,"Add any paths that contain templates here, relative to this directory."
v1.1.0,The suffix(es) of source filenames.
v1.1.0,You can specify multiple suffix as a list of string:
v1.1.0,
v1.1.0,"source_suffix = ['.rst', '.md']"
v1.1.0,The master toctree document.
v1.1.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.1.0,for a list of supported languages.
v1.1.0,
v1.1.0,This is also used if you do content translation via gettext catalogs.
v1.1.0,"Usually you set ""language"" from the command line for these cases."
v1.1.0,"List of patterns, relative to source directory, that match files and"
v1.1.0,directories to ignore when looking for source files.
v1.1.0,This pattern also affects html_static_path and html_extra_path.
v1.1.0,The name of the Pygments (syntax highlighting) style to use.
v1.1.0,-- Options for HTML output -------------------------------------------------
v1.1.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.1.0,a list of builtin themes.
v1.1.0,
v1.1.0,Theme options are theme-specific and customize the look and feel of a theme
v1.1.0,"further.  For a list of options available for each theme, see the"
v1.1.0,documentation.
v1.1.0,
v1.1.0,html_theme_options = {}
v1.1.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.1.0,"relative to this directory. They are copied after the builtin static files,"
v1.1.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.1.0,html_static_path = ['_static']
v1.1.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.1.0,to template names.
v1.1.0,
v1.1.0,The default sidebars (for documents that don't match any pattern) are
v1.1.0,defined by theme itself.  Builtin themes are using these templates by
v1.1.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.1.0,'searchbox.html']``.
v1.1.0,
v1.1.0,html_sidebars = {}
v1.1.0,The name of an image file (relative to this directory) to place at the top
v1.1.0,of the sidebar.
v1.1.0,
v1.1.0,-- Options for HTMLHelp output ---------------------------------------------
v1.1.0,Output file base name for HTML help builder.
v1.1.0,-- Options for LaTeX output ------------------------------------------------
v1.1.0,latex_elements = {
v1.1.0,The paper size ('letterpaper' or 'a4paper').
v1.1.0,
v1.1.0,"'papersize': 'letterpaper',"
v1.1.0,
v1.1.0,"The font size ('10pt', '11pt' or '12pt')."
v1.1.0,
v1.1.0,"'pointsize': '10pt',"
v1.1.0,
v1.1.0,Additional stuff for the LaTeX preamble.
v1.1.0,
v1.1.0,"'preamble': '',"
v1.1.0,
v1.1.0,Latex figure (float) alignment
v1.1.0,
v1.1.0,"'figure_align': 'htbp',"
v1.1.0,}
v1.1.0,Grouping the document tree into LaTeX files. List of tuples
v1.1.0,"(source start file, target name, title,"
v1.1.0,"author, documentclass [howto, manual, or own class])."
v1.1.0,latex_documents = [
v1.1.0,(
v1.1.0,"master_doc,"
v1.1.0,"'pykeen.tex',"
v1.1.0,"'PyKEEN Documentation',"
v1.1.0,"author,"
v1.1.0,"'manual',"
v1.1.0,"),"
v1.1.0,]
v1.1.0,-- Options for manual page output ------------------------------------------
v1.1.0,One entry per manual page. List of tuples
v1.1.0,"(source start file, name, description, authors, manual section)."
v1.1.0,-- Options for Texinfo output ----------------------------------------------
v1.1.0,Grouping the document tree into Texinfo files. List of tuples
v1.1.0,"(source start file, target name, title, author,"
v1.1.0,"dir menu entry, description, category)"
v1.1.0,-- Options for Epub output -------------------------------------------------
v1.1.0,Bibliographic Dublin Core info.
v1.1.0,epub_title = project
v1.1.0,The unique identifier of the text. This can be a ISBN number
v1.1.0,or the project homepage.
v1.1.0,
v1.1.0,epub_identifier = ''
v1.1.0,A unique identification for the text.
v1.1.0,
v1.1.0,epub_uid = ''
v1.1.0,A list of files that should not be packed into the epub file.
v1.1.0,epub_exclude_files = ['search.html']
v1.1.0,-- Extension configuration -------------------------------------------------
v1.1.0,-- Options for intersphinx extension ---------------------------------------
v1.1.0,Example configuration for intersphinx: refer to the Python standard library.
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Check a model param is optimized
v1.1.0,Check a loss param is optimized
v1.1.0,Check a model param is NOT optimized
v1.1.0,Check a loss param is optimized
v1.1.0,Check a model param is optimized
v1.1.0,Check a loss param is NOT optimized
v1.1.0,Check a model param is NOT optimized
v1.1.0,Check a loss param is NOT optimized
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,check for empty batches
v1.1.0,Train a model in one shot
v1.1.0,Train a model for the first half
v1.1.0,Continue training of the first part
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,The triples factory and model
v1.1.0,: The evaluator to be tested
v1.1.0,Settings
v1.1.0,: The evaluator instantiation
v1.1.0,Settings
v1.1.0,Initialize evaluator
v1.1.0,Use small test dataset
v1.1.0,Use small model (untrained)
v1.1.0,Get batch
v1.1.0,Compute scores
v1.1.0,Compute mask only if required
v1.1.0,TODO: Re-use filtering code
v1.1.0,"shape: (batch_size, num_triples)"
v1.1.0,"shape: (batch_size, num_entities)"
v1.1.0,Process one batch
v1.1.0,Check for correct class
v1.1.0,Check value ranges
v1.1.0,TODO: Validate with data?
v1.1.0,Check for correct class
v1.1.0,check value
v1.1.0,filtering
v1.1.0,"true_score: (2, 3, 3)"
v1.1.0,head based filter
v1.1.0,preprocessing for faster lookup
v1.1.0,check that all found positives are positive
v1.1.0,check in-place
v1.1.0,Test head scores
v1.1.0,Assert in-place modification
v1.1.0,Assert correct filtering
v1.1.0,Test tail scores
v1.1.0,Assert in-place modification
v1.1.0,Assert correct filtering
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The batch size
v1.1.0,: The triples factory
v1.1.0,: Class of regularizer to test
v1.1.0,: The constructor parameters to pass to the regularizer
v1.1.0,": The regularizer instance, initialized in setUp"
v1.1.0,: A positive batch
v1.1.0,: The device
v1.1.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.1.0,Check if regularizer is stored correctly.
v1.1.0,Forward pass (should update regularizer)
v1.1.0,Call post_parameter_update (should reset regularizer)
v1.1.0,Check if regularization term is reset
v1.1.0,Call method
v1.1.0,Generate random tensors
v1.1.0,Call update
v1.1.0,check shape
v1.1.0,compute expected term
v1.1.0,Generate random tensor
v1.1.0,calculate penalty
v1.1.0,check shape
v1.1.0,check value
v1.1.0,check if within 0.5 std of observed
v1.1.0,test error is raised
v1.1.0,Tests that exception will be thrown when more than or less than three tensors are passed
v1.1.0,Test that regularization term is computed correctly
v1.1.0,Entity soft constraint
v1.1.0,Orthogonality soft constraint
v1.1.0,"After first update, should change the term"
v1.1.0,"After second update, no change should happen"
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The number of embeddings
v1.1.0,: The embedding dimension
v1.1.0,check shape
v1.1.0,check values
v1.1.0,check shape
v1.1.0,check values
v1.1.0,check correct value range
v1.1.0,check maximum norm constraint
v1.1.0,unchanged values for small norms
v1.1.0,generate random query tensor
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,equal value; larger is better
v1.1.0,equal value; smaller is better
v1.1.0,larger is better; improvement
v1.1.0,larger is better; improvement; but not significant
v1.1.0,: The window size used by the early stopper
v1.1.0,: The mock losses the mock evaluator will return
v1.1.0,: The (zeroed) index  - 1 at which stopping will occur
v1.1.0,: The minimum improvement
v1.1.0,: The best results
v1.1.0,Set automatic_memory_optimization to false for tests
v1.1.0,Step early stopper
v1.1.0,check storing of results
v1.1.0,check ring buffer
v1.1.0,: The window size used by the early stopper
v1.1.0,: The (zeroed) index  - 1 at which stopping will occur
v1.1.0,: The minimum improvement
v1.1.0,: The random seed to use for reproducibility
v1.1.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.1.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.1.0,: The batch size to use.
v1.1.0,Fix seed for reproducibility
v1.1.0,Set automatic_memory_optimization to false during testing
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains
v1.1.0,"inverse relations, although the triples contained in it are not the same we would have when removing the"
v1.1.0,"first triple, and passing create_inverse_triples=True."
v1.1.0,check for warning
v1.1.0,check for filtered triples
v1.1.0,check for correct inverse triples flag
v1.1.0,check correct translation
v1.1.0,check column order
v1.1.0,apply restriction
v1.1.0,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.1.0,check that inverse_triples is correctly carried over
v1.1.0,verify that the label-to-ID mapping has not been changed
v1.1.0,verify that triples have been filtered
v1.1.0,check compressed triples
v1.1.0,reconstruct triples from compressed form
v1.1.0,check data loader
v1.1.0,verify that all entities and relations are present in the training factory
v1.1.0,verify that no triple got lost
v1.1.0,verify that the label-to-id mappings match
v1.1.0,check type
v1.1.0,check format
v1.1.0,check coverage
v1.1.0,Check if multilabels are working correctly
v1.1.0,generate random ratios
v1.1.0,check size
v1.1.0,check value range
v1.1.0,check total split
v1.1.0,check consistency with ratios
v1.1.0,the number of decimal digits equivalent to 1 / n_total
v1.1.0,check type
v1.1.0,check values
v1.1.0,compare against expected
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The batch size
v1.1.0,: The random seed
v1.1.0,: The triples factory
v1.1.0,: The sLCWA instances
v1.1.0,: Class of negative sampling to test
v1.1.0,": The negative sampler instance, initialized in setUp"
v1.1.0,: A positive batch
v1.1.0,Generate negative sample
v1.1.0,check shape
v1.1.0,check bounds: heads
v1.1.0,check bounds: relations
v1.1.0,check bounds: tails
v1.1.0,Check that all elements got corrupted
v1.1.0,Check whether filtering works correctly
v1.1.0,First giving an example where all triples have to be filtered
v1.1.0,The filter should remove all triples
v1.1.0,Create an example where no triples will be filtered
v1.1.0,The filter should not remove any triple
v1.1.0,Generate scaled negative sample
v1.1.0,Generate negative samples
v1.1.0,test that the relations were not changed
v1.1.0,Test that half of the subjects and half of the objects are corrupted
v1.1.0,Generate negative sample for additional tests
v1.1.0,test that the relations were not changed
v1.1.0,sample a batch
v1.1.0,check shape
v1.1.0,get triples
v1.1.0,check connected components
v1.1.0,super inefficient
v1.1.0,join
v1.1.0,already joined
v1.1.0,check that there is only a single component
v1.1.0,check content of comp_adj_lists
v1.1.0,check edge ids
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The class of the model to test
v1.1.0,: Additional arguments passed to the model's constructor method
v1.1.0,: Additional arguments passed to the training loop's constructor method
v1.1.0,: The triples factory instance
v1.1.0,: The model instance
v1.1.0,: The batch size for use for forward_* tests
v1.1.0,: The embedding dimensionality
v1.1.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.1.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.1.0,: The batch size for use when testing training procedures
v1.1.0,: The number of epochs to train the model
v1.1.0,: A random number generator from torch
v1.1.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.1.0,initialization
v1.1.0,assert there is at least one trainable parameter
v1.1.0,Check that all the parameters actually require a gradient
v1.1.0,Try to initialize an optimizer
v1.1.0,get model parameters
v1.1.0,re-initialize
v1.1.0,check that the operation works in-place
v1.1.0,check that the parameters where modified
v1.1.0,check for finite values by default
v1.1.0,check whether a gradient can be back-propgated
v1.1.0,"assert batch comprises (head, relation) pairs"
v1.1.0,"assert batch comprises (relation, tail) pairs"
v1.1.0,"For the high/low memory test cases of NTN, SE, etc."
v1.1.0,"else, leave to default"
v1.1.0,TODO: Catch HolE MKL error?
v1.1.0,set regularizer term
v1.1.0,call post_parameter_update
v1.1.0,assert that the regularization term has been reset
v1.1.0,do one optimization step
v1.1.0,call post_parameter_update
v1.1.0,check model constraints
v1.1.0,"assert batch comprises (relation, tail) pairs"
v1.1.0,"assert batch comprises (relation, tail) pairs"
v1.1.0,"assert batch comprises (relation, tail) pairs"
v1.1.0,call some functions
v1.1.0,reset to old state
v1.1.0,call some functions
v1.1.0,reset to old state
v1.1.0,Distance-based model
v1.1.0,3x batch norm: bias + scale --> 6
v1.1.0,entity specific bias        --> 1
v1.1.0,==================================
v1.1.0,7
v1.1.0,"two bias terms, one conv-filter"
v1.1.0,check type
v1.1.0,check shape
v1.1.0,check ID ranges
v1.1.0,this is only done in one of the models
v1.1.0,this is only done in one of the models
v1.1.0,Two linear layer biases
v1.1.0,"Two BN layers, bias & scale"
v1.1.0,: one bias per layer
v1.1.0,: (scale & bias for BN) * layers
v1.1.0,entity embeddings
v1.1.0,relation embeddings
v1.1.0,Compute Scores
v1.1.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.1.0,relation embeddings
v1.1.0,Compute Scores
v1.1.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.1.0,entity embeddings
v1.1.0,relation embeddings
v1.1.0,Compute Scores
v1.1.0,random entity embeddings & projections
v1.1.0,random relation embeddings & projections
v1.1.0,project
v1.1.0,check shape:
v1.1.0,check normalization
v1.1.0,entity embeddings
v1.1.0,relation embeddings
v1.1.0,Compute Scores
v1.1.0,second_score = scores[1].item()
v1.1.0,: 2xBN (bias & scale)
v1.1.0,: The number of entities
v1.1.0,: The number of triples
v1.1.0,check shape
v1.1.0,check dtype
v1.1.0,check finite values (e.g. due to division by zero)
v1.1.0,check non-negativity
v1.1.0,check shape
v1.1.0,check content
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,"As the resumption capability currently is a function of the training loop, more thorough tests can be found"
v1.1.0,in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the
v1.1.0,pipeline is checked.
v1.1.0,Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.
v1.1.0,Resume the previous pipeline
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))
v1.1.0,"self.assertLessEqual(min_frequency, expected_frequency)"
v1.1.0,Test looking up inverse triples
v1.1.0,test new label to ID
v1.1.0,type
v1.1.0,old labels
v1.1.0,"new, compact IDs"
v1.1.0,test vectorized lookup
v1.1.0,type
v1.1.0,shape
v1.1.0,value range
v1.1.0,only occurring Ids get mapped to non-negative numbers
v1.1.0,"Ids are mapped to (0, ..., num_unique_ids-1)"
v1.1.0,check type
v1.1.0,check shape
v1.1.0,check content
v1.1.0,check type
v1.1.0,check shape
v1.1.0,check 1-hot
v1.1.0,check type
v1.1.0,check shape
v1.1.0,check value range
v1.1.0,check self-similarity = 1
v1.1.0,base relation
v1.1.0,exact duplicate
v1.1.0,99% duplicate
v1.1.0,50% duplicate
v1.1.0,exact inverse
v1.1.0,99% inverse
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The expected number of entities
v1.1.0,: The expected number of relations
v1.1.0,: The expected number of triples
v1.1.0,": The tolerance on expected number of triples, for randomized situations"
v1.1.0,: The dataset to test
v1.1.0,: The instantiated dataset
v1.1.0,: Should the validation be assumed to have been loaded with train/test?
v1.1.0,Not loaded
v1.1.0,Load
v1.1.0,Test caching
v1.1.0,assert (end - start) < 1.0e-02
v1.1.0,": The directory, if there is caching"
v1.1.0,: The class
v1.1.0,: Constructor keyword arguments
v1.1.0,: The loss instance
v1.1.0,: The batch size
v1.1.0,test reduction
v1.1.0,Test backward
v1.1.0,: The number of entities.
v1.1.0,: The number of negative samples
v1.1.0,: The number of entities.
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,check for finite values by default
v1.1.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.1.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.1.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.1.0,Get embeddings
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,≈ result of softmax
v1.1.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.1.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.1.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.1.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.1.0,≈ result of sigmoid
v1.1.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.1.0,expected_loss ≈ 0.34
v1.1.0,TODO reimplement then test MarginRankingLoss using PR #18 solution
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Create dummy dense labels
v1.1.0,Check if labels form a probability distribution
v1.1.0,Apply label smoothing
v1.1.0,Check if smooth labels form probability distribution
v1.1.0,Create dummy sLCWA labels
v1.1.0,Apply label smoothing
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A mapping of optimizers' names to their implementations
v1.1.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Helpers
v1.1.0,Base Classes
v1.1.0,Concrete Classes
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,"scale labels from [0, 1] to [-1, 1]"
v1.1.0,cross entropy expects a proper probability distribution -> normalize labels
v1.1.0,Use numerically stable variant to compute log(softmax)
v1.1.0,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.1.0,: A mapping of losses' names to their implementations
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.
v1.1.0,This can be overridden with the envvar ``PYKEEN_HOME``.
v1.1.0,": For more information, see https://github.com/cthoyt/pystow"
v1.1.0,: A path representing the PyKEEN data folder
v1.1.0,": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``"
v1.1.0,": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``"
v1.1.0,": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``"
v1.1.0,": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``"
v1.1.0,: We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)
v1.1.0,: see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.1.0,lower bound
v1.1.0,upper bound
v1.1.0,normalize input
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The overall regularization weight
v1.1.0,: The current regularization term (a scalar)
v1.1.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.1.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.1.0,: The default strategy for optimizing the no-op regularizer's hyper-parameters
v1.1.0,no need to compute anything
v1.1.0,always return zero
v1.1.0,"TODO: this only works for x ~ N(0, 1), but not for |x|"
v1.1.0,cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution
v1.1.0,mean = scipy.stats.norm.ppf(1 - 1/d)
v1.1.0,scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean
v1.1.0,"return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)"
v1.1.0,: The dimension along which to compute the vector-based regularization terms.
v1.1.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.1.0,: This allows dimensionality-independent weight tuning.
v1.1.0,: The default strategy for optimizing the LP regularizer's hyper-parameters
v1.1.0,: The default strategy for optimizing the power sum regularizer's hyper-parameters
v1.1.0,: The default strategy for optimizing the TransH regularizer's hyper-parameters
v1.1.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.1.0,"Therefore, apply_only_once is always set to True."
v1.1.0,Entity soft constraint
v1.1.0,Orthogonality soft constraint
v1.1.0,The normalization factor to balance individual regularizers' contribution.
v1.1.0,: A mapping of regularizers' names to their implementations
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Add HPO command
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The random seed used at the beginning of the pipeline
v1.1.0,: The model trained by the pipeline
v1.1.0,: The training loop used by the pipeline
v1.1.0,: The losses during training
v1.1.0,: The results evaluated by the pipeline
v1.1.0,: How long in seconds did training take?
v1.1.0,: How long in seconds did evaluation take?
v1.1.0,: An early stopper
v1.1.0,: Any additional metadata as a dictionary
v1.1.0,: The version of PyKEEN used to create these results
v1.1.0,: The git hash of PyKEEN used to create these results
v1.1.0,1. Dataset
v1.1.0,2. Model
v1.1.0,3. Loss
v1.1.0,4. Regularizer
v1.1.0,5. Optimizer
v1.1.0,6. Training Loop
v1.1.0,7. Training (ronaldo style)
v1.1.0,8. Evaluation
v1.1.0,9. Tracking
v1.1.0,Misc
v1.1.0,"To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the"
v1.1.0,used random_seed to ensure reproducible results
v1.1.0,We have to set clear optimizer to False since training should be continued
v1.1.0,Start tracking
v1.1.0,evaluation restriction to a subset of entities/relations
v1.1.0,FIXME this should never happen.
v1.1.0,Log model parameters
v1.1.0,Log optimizer parameters
v1.1.0,Stopping
v1.1.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.1.0,By default there's a stopper that does nothing interesting
v1.1.0,Add logging for debugging
v1.1.0,Train like Cristiano Ronaldo
v1.1.0,Evaluate
v1.1.0,Reuse optimal evaluation parameters from training if available
v1.1.0,Add logging about evaluator for debugging
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,"comment: TypeVar expects none, or at least two super-classes"
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,initialize weights in-place
v1.1.0,apply constraints in-place
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,don't worry about functions because they can't be specified by JSON.
v1.1.0,Could make a better mo
v1.1.0,later could extend for other non-JSON valid types
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Score with original triples
v1.1.0,Score with inverse triples
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Create directory in which all experimental artifacts are saved
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: Functions for specifying exotic resources with a given prefix
v1.1.0,: Functions for specifying exotic resources based on their file extension
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.1.0,TODO vectorize code
v1.1.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Split triples
v1.1.0,Sorting ensures consistent results when the triples are permuted
v1.1.0,Create mapping
v1.1.0,Sorting ensures consistent results when the triples are permuted
v1.1.0,Create mapping
v1.1.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.1.0,Filter all non-existent triples
v1.1.0,Note: Unique changes the order of the triples
v1.1.0,Note: Using unique means implicit balancing of training samples
v1.1.0,normalize input
v1.1.0,: The mapping from entities' labels to their indices
v1.1.0,: The mapping from relations' labels to their indices
v1.1.0,": A three-column matrix where each row are the head identifier,"
v1.1.0,": relation identifier, then tail identifier"
v1.1.0,: Whether to create inverse triples
v1.1.0,: Arbitrary metadata to go with the graph
v1.1.0,The following fields get generated automatically
v1.1.0,: The inverse mapping for entity_label_to_id; initialized automatically
v1.1.0,: The inverse mapping for relation_label_to_id; initialized automatically
v1.1.0,: A vectorized version of entity_label_to_id; initialized automatically
v1.1.0,: A vectorized version of relation_label_to_id; initialized automatically
v1.1.0,: A vectorized version of entity_id_to_label; initialized automatically
v1.1.0,: A vectorized version of relation_id_to_label; initialized automatically
v1.1.0,ID to label mapping
v1.1.0,vectorized versions
v1.1.0,Check if the triples are inverted already
v1.1.0,We re-create them pure index based to ensure that _all_ inverse triples are present and that they are
v1.1.0,contained if and only if create_inverse_triples is True.
v1.1.0,Generate entity mapping if necessary
v1.1.0,Generate relation mapping if necessary
v1.1.0,Map triples of labels to triples of IDs.
v1.1.0,TODO: Check if lazy evaluation would make sense
v1.1.0,Make new triples factories for each group
v1.1.0,pre-filter to keep only topk
v1.1.0,generate text
v1.1.0,Input validation
v1.1.0,convert to numpy
v1.1.0,vectorized label lookup
v1.1.0,Additional columns
v1.1.0,convert PyTorch tensors to numpy
v1.1.0,convert to dataframe
v1.1.0,Re-order columns
v1.1.0,Filter for entities
v1.1.0,Filter for relations
v1.1.0,No filtering happened
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Split indices
v1.1.0,Split triples
v1.1.0,index
v1.1.0,select
v1.1.0,Prepare split index
v1.1.0,"due to rounding errors we might lose a few points, thus we use cumulative ratio"
v1.1.0,[...] is necessary for Python 3.7 compatibility
v1.1.0,While there are still triples that should be moved to the training set
v1.1.0,Pick a random triple to move over to the training triples
v1.1.0,add to training
v1.1.0,remove from testing
v1.1.0,Recalculate the move_id_mask
v1.1.0,base cases
v1.1.0,IDs not in training
v1.1.0,triples with exclusive test IDs
v1.1.0,Make sure that the first element has all the right stuff in it
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,": The mapped triples, shape: (num_triples, 3)"
v1.1.0,: The unique pairs
v1.1.0,: The compressed triples in CSR format
v1.1.0,convert to csr for fast row slicing
v1.1.0,: TODO: do we need these?
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,check validity
v1.1.0,path compression
v1.1.0,collect connected components using union find with path compression
v1.1.0,get representatives
v1.1.0,already merged
v1.1.0,make x the smaller one
v1.1.0,merge
v1.1.0,extract partitions
v1.1.0,safe division for empty sets
v1.1.0,compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping
v1.1.0,duplicates
v1.1.0,we are not interested in self-similarity
v1.1.0,compute similarities
v1.1.0,Calculate which relations are the inverse ones
v1.1.0,get existing IDs
v1.1.0,remove non-existing ID from label mapping
v1.1.0,create translation tensor
v1.1.0,get entities and relations occurring in triples
v1.1.0,generate ID translation and new label to Id mappings
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,preprocessing
v1.1.0,initialize
v1.1.0,sample iteratively
v1.1.0,determine weights
v1.1.0,only happens at first iteration
v1.1.0,normalize to probabilities
v1.1.0,sample a start node
v1.1.0,get list of neighbors
v1.1.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.1.0,visit target node
v1.1.0,decrease sample counts
v1.1.0,return chosen edges
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,The internal epoch state tracks the last finished epoch of the training loop to allow for
v1.1.0,seamless loading and saving of training checkpoints
v1.1.0,Create training instances
v1.1.0,During size probing the training instances should not show the tqdm progress bar
v1.1.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.1.0,A checkpoint root is always created to ensure a fallback checkpoint can be saved
v1.1.0,"If a checkpoint file is given, it must be loaded if it exists already"
v1.1.0,"If the stopper dict has any keys, those are written back to the stopper"
v1.1.0,The checkpoint frequency needs to be set to save checkpoints
v1.1.0,"In case a checkpoint frequency was set, we warn that no checkpoints will be saved"
v1.1.0,"If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes"
v1.1.0,"If the stopper loaded from the training loop checkpoint stopped the training, we return those results"
v1.1.0,Ensure the release of memory
v1.1.0,Clear optimizer
v1.1.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.1.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.1.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.1.0,return the relevant parameters slice_size and batch_size
v1.1.0,Create dummy result tracker
v1.1.0,Sanity check
v1.1.0,Force weight initialization if training continuation is not explicitly requested.
v1.1.0,Reset the weights
v1.1.0,Create new optimizer
v1.1.0,Ensure the model is on the correct device
v1.1.0,Create Sampler
v1.1.0,Bind
v1.1.0,"When size probing, we don't want progress bars"
v1.1.0,Create progress bar
v1.1.0,Save the time to track when the saved point was available
v1.1.0,Training Loop
v1.1.0,"When training with an early stopper the memory pressure changes, which may allow for errors each epoch"
v1.1.0,Enforce training mode
v1.1.0,Accumulate loss over epoch
v1.1.0,Batching
v1.1.0,Only create a progress bar when not in size probing mode
v1.1.0,Flag to check when to quit the size probing
v1.1.0,Recall that torch *accumulates* gradients. Before passing in a
v1.1.0,"new instance, you need to zero out the gradients from the old instance"
v1.1.0,Get batch size of current batch (last batch may be incomplete)
v1.1.0,accumulate gradients for whole batch
v1.1.0,forward pass call
v1.1.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.1.0,update parameters according to optimizer
v1.1.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.1.0,constraints are no longer applied
v1.1.0,For testing purposes we're only interested in processing one batch
v1.1.0,When size probing we don't need the losses
v1.1.0,Track epoch loss
v1.1.0,Print loss information to console
v1.1.0,Save the last successful finished epoch
v1.1.0,"When the training loop failed, a fallback checkpoint is created to resume training."
v1.1.0,"If a checkpoint file is given, we check whether it is time to save a checkpoint"
v1.1.0,forward pass
v1.1.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.1.0,correction for loss reduction
v1.1.0,backward pass
v1.1.0,reset the regularizer to free the computational graph
v1.1.0,Set upper bound
v1.1.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.1.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.1.0,The regularizer has to be reset to free the computational graph
v1.1.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.1.0,"Only if a cuda device is available, the random state is accessed"
v1.1.0,"Cuda requires its own random state, which can only be set when a cuda device is available"
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Shuffle each epoch
v1.1.0,Lazy-splitting into batches
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Slicing is not possible in sLCWA training loops
v1.1.0,Send positive batch to device
v1.1.0,Create negative samples
v1.1.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.1.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.1.0,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.1.0,Compute negative and positive scores
v1.1.0,Repeat positives scores (necessary for more than one negative per positive)
v1.1.0,Stack predictions
v1.1.0,Create target
v1.1.0,Normalize the loss to have the average loss per positive triple
v1.1.0,This allows comparability of sLCWA and LCWA losses
v1.1.0,Slicing is not possible for sLCWA
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A mapping of training loops' names to their implementations
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Split batch components
v1.1.0,Send batch to device
v1.1.0,Apply label smoothing
v1.1.0,This shows how often one row has to be repeated
v1.1.0,Create boolean indices for negative labels in the repeated rows
v1.1.0,Repeat the predictions and filter for negative labels
v1.1.0,This tells us how often each true label should be repeated
v1.1.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.1.0,Split positive and negative scores
v1.1.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.1.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,now: smaller is better
v1.1.0,: The model
v1.1.0,: The evaluator
v1.1.0,: The triples to use for evaluation
v1.1.0,: Size of the evaluation batches
v1.1.0,: Slice size of the evaluation batches
v1.1.0,: The number of epochs after which the model is evaluated on validation set
v1.1.0,: The number of iterations (one iteration can correspond to various epochs)
v1.1.0,: with no improvement after which training will be stopped.
v1.1.0,: The name of the metric to use
v1.1.0,: The minimum relative improvement necessary to consider it an improved result
v1.1.0,: The best result so far
v1.1.0,: The epoch at which the best result occurred
v1.1.0,: The remaining patience
v1.1.0,: The metric results from all evaluations
v1.1.0,": Whether a larger value is better, or a smaller"
v1.1.0,: The result tracker
v1.1.0,: Callbacks when after results are calculated
v1.1.0,: Callbacks when training gets continued
v1.1.0,: Callbacks when training is stopped early
v1.1.0,: Did the stopper ever decide to stop?
v1.1.0,TODO: Fix this
v1.1.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.1.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.1.0,Dummy result tracker
v1.1.0,Evaluate
v1.1.0,Only perform time consuming checks for the first call.
v1.1.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.1.0,Append to history
v1.1.0,check for improvement
v1.1.0,Stop if the result did not improve more than delta for patience evaluations
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A mapping of stoppers' names to their implementations
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.
v1.1.0,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.1.0,Clear the ranks from the current evaluator
v1.1.0,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.1.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.1.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.1.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.1.0,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.1.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.1.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.1.0,Test if slicing is implemented for the required functions of this model
v1.1.0,Split batch
v1.1.0,Bind shape
v1.1.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.1.0,Warn if all entities will be filtered
v1.1.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.1.0,verify that the triples have been filtered
v1.1.0,Send to device
v1.1.0,Ensure evaluation mode
v1.1.0,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.1.0,Check whether we need to be prepared for filtering
v1.1.0,Check whether an evaluator needs access to the masks
v1.1.0,This can only be an unfiltered evaluator.
v1.1.0,Prepare for result filtering
v1.1.0,Send tensors to device
v1.1.0,Prepare batches
v1.1.0,This should be a reasonable default size that works on most setups while being faster than batch_size=1
v1.1.0,Show progressbar
v1.1.0,Flag to check when to quit the size probing
v1.1.0,Disable gradient tracking
v1.1.0,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.1.0,batch-wise processing
v1.1.0,If we only probe sizes we do not need more than one batch
v1.1.0,Finalize
v1.1.0,Predict scores once
v1.1.0,Select scores of true
v1.1.0,Create positive filter for all corrupted
v1.1.0,Needs all positive triples
v1.1.0,Create filter
v1.1.0,Create a positive mask with the size of the scores from the positive filter
v1.1.0,Restrict to entities of interest
v1.1.0,Evaluate metrics on these *unfiltered* scores
v1.1.0,Filter
v1.1.0,The scores for the true triples have to be rewritten to the scores tensor
v1.1.0,Restrict to entities of interest
v1.1.0,Evaluate metrics on these *filtered* scores
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The area under the ROC curve
v1.1.0,: The area under the precision-recall curve
v1.1.0,: The coverage error
v1.1.0,coverage_error: float = field(metadata=dict(
v1.1.0,"doc='The coverage error',"
v1.1.0,"f=metrics.coverage_error,"
v1.1.0,))
v1.1.0,: The label ranking loss (APS)
v1.1.0,label_ranking_average_precision_score: float = field(metadata=dict(
v1.1.0,"doc='The label ranking loss (APS)',"
v1.1.0,"f=metrics.label_ranking_average_precision_score,"
v1.1.0,))
v1.1.0,#: The label ranking loss
v1.1.0,label_ranking_loss: float = field(metadata=dict(
v1.1.0,"doc='The label ranking loss',"
v1.1.0,"f=metrics.label_ranking_loss,"
v1.1.0,))
v1.1.0,Transfer to cpu and convert to numpy
v1.1.0,Ensure that each key gets counted only once
v1.1.0,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.1.0,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.1.0,masks using the exact same key order.
v1.1.0,TODO how to define a cutoff on y_scores to make binary?
v1.1.0,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.1.0,Clear buffers
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A mapping of evaluators' names to their implementations
v1.1.0,: A mapping of results' names to their implementations
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,The best rank is the rank when assuming all options with an equal score are placed behind the currently
v1.1.0,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.1.0,The worst rank is the rank when assuming all options with an equal score are placed in front of the currently
v1.1.0,"considered. Hence, the rank is the number of options which have at least the same score minus one (as the"
v1.1.0,"currently considered option in included in all options). As the rank is one-based, we have to add 1, which"
v1.1.0,"nullifies the ""minus 1"" from before."
v1.1.0,"The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of"
v1.1.0,the elements with the same score as the currently considered option.
v1.1.0,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.1.0,The expected rank of a random scoring
v1.1.0,The adjusted ranks is normalized by the expected rank of a random scoring
v1.1.0,TODO adjusted_worst_rank
v1.1.0,TODO adjusted_best_rank
v1.1.0,: The mean over all ranks: mean_i r_i. Lower is better.
v1.1.0,: The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.
v1.1.0,": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k."
v1.1.0,: Higher is better.
v1.1.0,: The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.
v1.1.0,: Described by [berrendorf2020]_.
v1.1.0,Check if it a side or rank type
v1.1.0,Clear buffers
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.1.0,Create a tensor of all IDs
v1.1.0,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.1.0,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.1.0,Keep track of the hyper-parameters that are used across all
v1.1.0,subclasses of BaseModule
v1.1.0,The following lines add in a post-init hook to all subclasses
v1.1.0,such that the reset_parameters_() function is run
v1.1.0,"sorry mypy, but this kind of evil must be permitted."
v1.1.0,: A dictionary of hyper-parameters to the models that use them
v1.1.0,: Keep track of if this is a base model
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The default loss function class
v1.1.0,: The default parameters for the default loss function class
v1.1.0,: The instance of the loss
v1.1.0,: The default regularizer class
v1.1.0,: The default parameters for the default regularizer class
v1.1.0,: The instance of the regularizer
v1.1.0,Initialize the device
v1.1.0,Random seeds have to set before the embeddings are initialized
v1.1.0,Loss
v1.1.0,TODO: Check loss functions that require 1 and -1 as label but only
v1.1.0,Regularizer
v1.1.0,The triples factory facilitates access to the dataset.
v1.1.0,Enforce evaluation mode
v1.1.0,Enforce evaluation mode
v1.1.0,Enforce evaluation mode
v1.1.0,Enforce evaluation mode
v1.1.0,initialize buffer on cpu
v1.1.0,calculate batch scores
v1.1.0,Explicitly create triples
v1.1.0,Sort final result
v1.1.0,Train a model (quickly)
v1.1.0,Get scores for *all* triples
v1.1.0,Get scores for top 15 triples
v1.1.0,set model to evaluation mode
v1.1.0,Do not track gradients
v1.1.0,initialize buffer on device
v1.1.0,calculate batch scores
v1.1.0,get top scores within batch
v1.1.0,append to global top scores
v1.1.0,reduce size if necessary
v1.1.0,Sort final result
v1.1.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.1.0,Id of inverse relation: relation + 1
v1.1.0,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.1.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.1.0,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.1.0,TODO UNUSED
v1.1.0,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.1.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.1.0,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.1.0,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.1.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.1.0,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.1.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.1.0,"make sure to call this first, to reset regularizer state!"
v1.1.0,Default for relation dimensionality
v1.1.0,"make sure to call this first, to reset regularizer state!"
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A mapping of models' names to their implementations
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Store initial input for error message
v1.1.0,All are None
v1.1.0,"input_channels is None, and any of height or width is None -> set input_channels=1"
v1.1.0,"input channels is not None, and one of height or width is None"
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The default loss function class
v1.1.0,: The default parameters for the default loss function class
v1.1.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.1.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.1.0,ConvE should be trained with inverse triples
v1.1.0,ConvE uses one bias for each entity
v1.1.0,Automatic calculation of remaining dimensions
v1.1.0,Parameter need to fulfil:
v1.1.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.1.0,weights
v1.1.0,"batch_size, num_input_channels, 2*height, width"
v1.1.0,"batch_size, num_input_channels, 2*height, width"
v1.1.0,"batch_size, num_input_channels, 2*height, width"
v1.1.0,"(N,C_out,H_out,W_out)"
v1.1.0,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.1.0,Embedding Regularization
v1.1.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.1.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.1.0,Embedding Regularization
v1.1.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.1.0,Embedding Regularization
v1.1.0,Code to repeat each item successively instead of the entire tensor
v1.1.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,Embeddings
v1.1.0,Initialise relation embeddings to unit length
v1.1.0,Get embeddings
v1.1.0,Project entities
v1.1.0,Get embeddings
v1.1.0,Project entities
v1.1.0,Project entities
v1.1.0,Project entities
v1.1.0,Get embeddings
v1.1.0,Project entities
v1.1.0,Project entities
v1.1.0,Project entities
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The default loss function class
v1.1.0,: The default parameters for the default loss function class
v1.1.0,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.1.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.1.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.1.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.1.0,split into real and imaginary part
v1.1.0,ComplEx space bilinear product
v1.1.0,*: Elementwise multiplication
v1.1.0,get embeddings
v1.1.0,Regularization
v1.1.0,Compute scores
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.1.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.1.0,: a normalized weight of 10 is used.
v1.1.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.1.0,Get embeddings
v1.1.0,"shape: (b, d)"
v1.1.0,"shape: (b, d, d)"
v1.1.0,"shape: (b, d)"
v1.1.0,Compute scores
v1.1.0,Regularization
v1.1.0,Compute scores
v1.1.0,Regularization
v1.1.0,Get embeddings
v1.1.0,Compute scores
v1.1.0,Regularization
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.1.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.1.0,The interaction model
v1.1.0,embeddings
v1.1.0,Use Xavier initialization for weight; bias to zero
v1.1.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.1.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.1.0,Output layer regularization
v1.1.0,In the code base only the weights of the output layer are used for regularization
v1.1.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.1.0,Stack to convolution input
v1.1.0,Convolution
v1.1.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.1.0,Linear layer for final scores
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,": shape: (batch_size, num_entities, d)"
v1.1.0,": Prepare h: (b, e, d) -> (b, e, 1, 1, d)"
v1.1.0,": Prepare t: (b, e, d) -> (b, e, 1, d, 1)"
v1.1.0,": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)"
v1.1.0,"h.T @ W @ t, shape: (b, e, k, 1, 1)"
v1.1.0,": reduce (b, e, k, 1, 1) -> (b, e, k)"
v1.1.0,": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.1.0,": Prepare h: (b, e, d) -> (b, e, d, 1)"
v1.1.0,"V_h @ h, shape: (b, e, k, 1)"
v1.1.0,": reduce (b, e, k, 1) -> (b, e, k)"
v1.1.0,": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.1.0,": Prepare t: (b, e, d) -> (b, e, d, 1)"
v1.1.0,"V_t @ t, shape: (b, e, k, 1)"
v1.1.0,": reduce (b, e, k, 1) -> (b, e, k)"
v1.1.0,": Prepare b: (R, k) -> (b, k) -> (b, 1, k)"
v1.1.0,"a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)"
v1.1.0,"prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)"
v1.1.0,"prepare act: (b, e, k) -> (b, e, 1, k)"
v1.1.0,"compute score, shape: (b, e, 1, 1)"
v1.1.0,reduce
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The regularizer used by [yang2014]_ for DistMult
v1.1.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.1.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.1.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.1.0,: The LP settings used by [yang2014]_ for DistMult
v1.1.0,"xavier uniform, cf."
v1.1.0,https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17
v1.1.0,Constrain entity embeddings to unit length
v1.1.0,relations are initialized to unit length (but not constraint)
v1.1.0,Bilinear product
v1.1.0,*: Elementwise multiplication
v1.1.0,Get embeddings
v1.1.0,Compute score
v1.1.0,Only regularize relation embeddings
v1.1.0,Get embeddings
v1.1.0,Rank against all entities
v1.1.0,Only regularize relation embeddings
v1.1.0,Get embeddings
v1.1.0,Rank against all entities
v1.1.0,Only regularize relation embeddings
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,Similarity function used for distributions
v1.1.0,element-wise covariance bounds
v1.1.0,Additional covariance embeddings
v1.1.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.1.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.1.0,Constraints are applied through post_parameter_update
v1.1.0,Get embeddings
v1.1.0,Compute entity distribution
v1.1.0,: a = \mu^T\Sigma^{-1}\mu
v1.1.0,: b = \log \det \Sigma
v1.1.0,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.1.0,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.1.0,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.1.0,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The custom regularizer used by [wang2014]_ for TransH
v1.1.0,: The settings used by [wang2014]_ for TransH
v1.1.0,embeddings
v1.1.0,Normalise the normal vectors by their l2 norms
v1.1.0,TODO: Add initialization
v1.1.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.1.0,which enforces the defined soft constraints.
v1.1.0,Get embeddings
v1.1.0,Project to hyperplane
v1.1.0,Regularization term
v1.1.0,Get embeddings
v1.1.0,Project to hyperplane
v1.1.0,Regularization term
v1.1.0,Get embeddings
v1.1.0,Project to hyperplane
v1.1.0,Regularization term
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,TODO: Initialize from TransE
v1.1.0,embeddings
v1.1.0,"project to relation specific subspace, shape: (b, e, d_r)"
v1.1.0,ensure constraints
v1.1.0,"evaluate score function, shape: (b, e)"
v1.1.0,Get embeddings
v1.1.0,Get embeddings
v1.1.0,Get embeddings
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,Construct node neighbourhood mask
v1.1.0,Set nodes in batch to true
v1.1.0,Compute k-neighbourhood
v1.1.0,"if the target node needs an embeddings, so does the source node"
v1.1.0,Create edge mask
v1.1.0,pylint: disable=unused-argument
v1.1.0,"Calculate in-degree, i.e. number of incoming edges"
v1.1.0,pylint: disable=unused-argument
v1.1.0,"Calculate in-degree, i.e. number of incoming edges"
v1.1.0,normalize representations
v1.1.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.1.0,check decomposition
v1.1.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.1.0,Weights
v1.1.0,buffering of messages
v1.1.0,allocate weight
v1.1.0,Get blocks
v1.1.0,"self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)"
v1.1.0,note: embedding_dim is guaranteed to be divisible by num_bases in the constructor
v1.1.0,"The current basis weights, shape: (num_bases)"
v1.1.0,"the current bases, shape: (num_bases, embedding_dim, embedding_dim)"
v1.1.0,"compute the current relation weights, shape: (embedding_dim, embedding_dim)"
v1.1.0,use buffered messages if applicable
v1.1.0,Bind fields
v1.1.0,"shape: (num_entities, embedding_dim)"
v1.1.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.1.0,Get random dropout mask
v1.1.0,Apply to edges
v1.1.0,Different dropout for self-loops (only in training mode)
v1.1.0,Initialize embeddings in the next layer for all nodes
v1.1.0,TODO: Can we vectorize this loop?
v1.1.0,Choose the edges which are of the specific relation
v1.1.0,No edges available? Skip rest of inner loop
v1.1.0,Get source and target node indices
v1.1.0,send messages in both directions
v1.1.0,Select source node embeddings
v1.1.0,get relation weights
v1.1.0,Compute message (b x d) * (d x d) = (b x d)
v1.1.0,Normalize messages by relation-specific in-degree
v1.1.0,Aggregate messages in target
v1.1.0,Self-loop
v1.1.0,"Apply bias, if requested"
v1.1.0,"Apply batch normalization, if requested"
v1.1.0,Apply non-linearity
v1.1.0,invalidate enriched embeddings
v1.1.0,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.1.0,initialized properly)
v1.1.0,We have one additional relation for self-loops
v1.1.0,Xavier Glorot initialization of each block
v1.1.0,Reset biases
v1.1.0,Reset batch norm parameters
v1.1.0,"Reset activation parameters, if any"
v1.1.0,"TODO: Replace this by interaction function, once https://github.com/pykeen/pykeen/pull/107 is merged."
v1.1.0,: Interaction model used as decoder
v1.1.0,: The blocks of the relation-specific weight matrices
v1.1.0,": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)"
v1.1.0,: The base weight matrices to generate relation-specific weights
v1.1.0,": shape: (num_bases, embedding_dim, embedding_dim)"
v1.1.0,: The relation-specific weights for each base
v1.1.0,": shape: (num_relations, num_bases)"
v1.1.0,: The biases for each layer (if used)
v1.1.0,": shape of each element: (embedding_dim,)"
v1.1.0,: Batch normalization for each layer (if used)
v1.1.0,: Activations for each layer (if used)
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,TODO: Dummy
v1.1.0,Enrich embeddings
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The default loss function class
v1.1.0,: The default parameters for the default loss function class
v1.1.0,Core tensor
v1.1.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.1.0,Dropout
v1.1.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.1.0,Abbreviation
v1.1.0,Compute h_n = DO(BN(h))
v1.1.0,Compute wr = DO(W x_2 r)
v1.1.0,compute whr = DO(BN(h_n x_1 wr))
v1.1.0,Compute whr x_3 t
v1.1.0,Get embeddings
v1.1.0,Compute scores
v1.1.0,Get embeddings
v1.1.0,Compute scores
v1.1.0,Get embeddings
v1.1.0,Compute scores
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,Get embeddings
v1.1.0,TODO: Use torch.dist
v1.1.0,Get embeddings
v1.1.0,TODO: Use torch.cdist
v1.1.0,Get embeddings
v1.1.0,TODO: Use torch.cdist
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The default loss function class
v1.1.0,: The default parameters for the default loss function class
v1.1.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.1.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.1.0,": regularization term by the number of elements, which is 200."
v1.1.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.1.0,extra embeddings
v1.1.0,forward model
v1.1.0,Regularization
v1.1.0,backward model
v1.1.0,Regularization
v1.1.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.1.0,"That is not mentioned in the paper, so it is omitted here."
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.1.0,weight initialization
v1.1.0,Get embeddings
v1.1.0,Embedding Regularization
v1.1.0,Concatenate them
v1.1.0,Compute scores
v1.1.0,Get embeddings
v1.1.0,Embedding Regularization
v1.1.0,First layer can be unrolled
v1.1.0,Send scores through rest of the network
v1.1.0,Get embeddings
v1.1.0,Embedding Regularization
v1.1.0,First layer can be unrolled
v1.1.0,Send scores through rest of the network
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,The dimensions affected by e'
v1.1.0,Project entities
v1.1.0,r_p (e_p.T e) + e'
v1.1.0,Enforce constraints
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,Project entities
v1.1.0,score = -||h_bot + r - t_bot||_2^2
v1.1.0,Head
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,Decompose into real and imaginary part
v1.1.0,Rotate (=Hadamard product in complex space).
v1.1.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.1.0,Get embeddings
v1.1.0,Compute scores
v1.1.0,Embedding Regularization
v1.1.0,Get embeddings
v1.1.0,Rank against all entities
v1.1.0,Compute scores
v1.1.0,Embedding Regularization
v1.1.0,Get embeddings
v1.1.0,r expresses a rotation in complex plane.
v1.1.0,The inverse rotation is expressed by the complex conjugate of r.
v1.1.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.1.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.1.0,|h * r - t| = |h - conj(r) * t|
v1.1.0,Rank against all entities
v1.1.0,Compute scores
v1.1.0,Embedding Regularization
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The default loss function class
v1.1.0,: The default parameters for the default loss function class
v1.1.0,Global entity projection
v1.1.0,Global relation projection
v1.1.0,Global combination bias
v1.1.0,Global combination bias
v1.1.0,Get embeddings
v1.1.0,Compute score
v1.1.0,Get embeddings
v1.1.0,Rank against all entities
v1.1.0,Get embeddings
v1.1.0,Rank against all entities
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.1.0,Circular correlation of entity embeddings
v1.1.0,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.1.0,Hadamard product in frequency domain
v1.1.0,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.1.0,inner product with relation embedding
v1.1.0,Embedding Regularization
v1.1.0,Embedding Regularization
v1.1.0,Embedding Regularization
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The default loss function class
v1.1.0,: The default parameters for the default loss function class
v1.1.0,Get embeddings
v1.1.0,Embedding Regularization
v1.1.0,Concatenate them
v1.1.0,Predict t embedding
v1.1.0,compare with all t's
v1.1.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.1.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.1.0,Embedding Regularization
v1.1.0,Concatenate them
v1.1.0,Predict t embedding
v1.1.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.1.0,Embedding Regularization
v1.1.0,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.1.0,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.1.0,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.1.0,Extend t
v1.1.0,Concatenate them
v1.1.0,Predict t embedding
v1.1.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.1.0,The results have to be realigned with the expected output of the score_h function
v1.1.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The default loss function class
v1.1.0,: The default parameters for the default loss function class
v1.1.0,Literal
v1.1.0,num_ent x num_lit
v1.1.0,Number of columns corresponds to number of literals
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the model's hyper-parameters
v1.1.0,: The default parameters for the default loss function class
v1.1.0,Literal
v1.1.0,num_ent x num_lit
v1.1.0,Number of columns corresponds to number of literals
v1.1.0,"TODO: this is very similar to ComplExLiteral, except a few dropout differences"
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The WANDB run
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A mapping of trackers' names to their implementations
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.1.0,Set the indices
v1.1.0,Bind number of negatives to sample
v1.1.0,Equally corrupt all sides
v1.1.0,Copy positive batch for corruption.
v1.1.0,"Do not detach, as no gradients should flow into the indices."
v1.1.0,Relations have a different index maximum than entities
v1.1.0,"To make sure we don't replace the {head, relation, tail} by the"
v1.1.0,original value we shift all values greater or equal than the original value by one up
v1.1.0,"for that reason we choose the random value from [0, num_{heads, relations, tails} -1]"
v1.1.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.1.0,Create mapped triples attribute that is required for filtering
v1.1.0,Make sure the mapped triples are initiated
v1.1.0,Copy the mapped triples to the device for efficient filtering
v1.1.0,Check which heads of the mapped triples are also in the negative triples
v1.1.0,Reduce the search space by only using possible matches that at least contain the head we look for
v1.1.0,Check in this subspace which relations of the mapped triples are also in the negative triples
v1.1.0,Reduce the search space by only using possible matches that at least contain head and relation we look for
v1.1.0,Create a filter indicating which of the proposed negative triples are positive in the training dataset
v1.1.0,"In cases where no triples should be filtered, the subspace reduction technique above will fail"
v1.1.0,Return only those proposed negative triples that are not positive in the training dataset
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A mapping of negative samplers' names to their implementations
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.1.0,Preprocessing: Compute corruption probabilities
v1.1.0,"compute tph, i.e. the average number of tail entities per head"
v1.1.0,"compute hpt, i.e. the average number of head entities per tail"
v1.1.0,Set parameter for Bernoulli distribution
v1.1.0,Bind number of negatives to sample
v1.1.0,Copy positive batch for corruption.
v1.1.0,"Do not detach, as no gradients should flow into the indices."
v1.1.0,Decide whether to corrupt head or tail
v1.1.0,Tails are corrupted if heads are not corrupted
v1.1.0,Randomly sample corruption. See below for explanation of
v1.1.0,"why this is on a range of [0, num_entities - 1]"
v1.1.0,Replace heads
v1.1.0,Replace tails
v1.1.0,"If filtering is activated, all negative triples that are positive in the training dataset will be removed"
v1.1.0,To make sure we don't replace the head by the original value
v1.1.0,we shift all values greater or equal than the original value by one up
v1.1.0,"for that reason we choose the random value from [0, num_entities -1]"
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,TODO what happens if already exists?
v1.1.0,TODO incorporate setting of random seed
v1.1.0,pipeline_kwargs=dict(
v1.1.0,"random_seed=random_non_negative_int(),"
v1.1.0,"),"
v1.1.0,Add dataset to current_pipeline
v1.1.0,"Training, test, and validation paths are provided"
v1.1.0,Add loss function to current_pipeline
v1.1.0,Add regularizer to current_pipeline
v1.1.0,Add optimizer to current_pipeline
v1.1.0,Add training approach to current_pipeline
v1.1.0,Add training kwargs and kwargs_ranges
v1.1.0,Add evaluation
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,"If GitHub ever gets upset from too many downloads, we can switch to"
v1.1.0,the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039
v1.1.0,GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is
v1.1.0,"normally the default for all of PyKEEN's remote datasets, so just switch the default here."
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: The name of the dataset to download
v1.1.0,FIXME these are already identifiers
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,don't call this function by itself. assumes called through the `validation`
v1.1.0,property and the _training factory has already been loaded
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A factory wrapping the training triples
v1.1.0,": A factory wrapping the testing triples, that share indices with the training triples"
v1.1.0,": A factory wrapping the validation triples, that share indices with the training triples"
v1.1.0,: All datasets should take care of inverse triple creation
v1.1.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.1.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.1.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.1.0,: The directory in which the cached data is stored
v1.1.0,don't call this function by itself. assumes called through the `validation`
v1.1.0,property and the _training factory has already been loaded
v1.1.0,see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content
v1.1.0,pattern from https://stackoverflow.com/a/39217788/5775947
v1.1.0,TODO replace this with the new zip remote dataset class
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A mapping of datasets' names to their classes
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,TODO update docs with table and CLI wtih generator
v1.1.0,: A mapping of HPO samplers' names to their implementations
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,1. Dataset
v1.1.0,2. Model
v1.1.0,3. Loss
v1.1.0,4. Regularizer
v1.1.0,5. Optimizer
v1.1.0,6. Training Loop
v1.1.0,7. Training
v1.1.0,8. Evaluation
v1.1.0,9. Trackers
v1.1.0,Misc.
v1.1.0,2. Model
v1.1.0,3. Loss
v1.1.0,4. Regularizer
v1.1.0,5. Optimizer
v1.1.0,1. Dataset
v1.1.0,2. Model
v1.1.0,3. Loss
v1.1.0,4. Regularizer
v1.1.0,5. Optimizer
v1.1.0,6. Training Loop
v1.1.0,7. Training
v1.1.0,8. Evaluation
v1.1.0,9. Tracker
v1.1.0,Misc.
v1.1.0,Will trigger Optuna to set the state of the trial as failed
v1.1.0,: The :mod:`optuna` study object
v1.1.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.1.0,Output study information
v1.1.0,Output all trials
v1.1.0,Output best trial as pipeline configuration file
v1.1.0,1. Dataset
v1.1.0,2. Model
v1.1.0,3. Loss
v1.1.0,4. Regularizer
v1.1.0,5. Optimizer
v1.1.0,6. Training Loop
v1.1.0,7. Training
v1.1.0,8. Evaluation
v1.1.0,9. Tracking
v1.1.0,6. Misc
v1.1.0,Optuna Study Settings
v1.1.0,Optuna Optimization Settings
v1.1.0,0. Metadata/Provenance
v1.1.0,1. Dataset
v1.1.0,2. Model
v1.1.0,3. Loss
v1.1.0,4. Regularizer
v1.1.0,5. Optimizer
v1.1.0,6. Training Loop
v1.1.0,7. Training
v1.1.0,8. Evaluation
v1.1.0,9. Tracking
v1.1.0,1. Dataset
v1.1.0,2. Model
v1.1.0,3. Loss
v1.1.0,4. Regularizer
v1.1.0,5. Optimizer
v1.1.0,6. Training Loop
v1.1.0,7. Training
v1.1.0,8. Evaluation
v1.1.0,9. Tracker
v1.1.0,Optuna Misc.
v1.1.0,Pipeline Misc.
v1.1.0,Invoke optimization of the objective function.
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,-*- coding: utf-8 -*-
v1.1.0,: A mapping of HPO pruners' names to their implementations
v1.1.0,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,
v1.0.5,Configuration file for the Sphinx documentation builder.
v1.0.5,
v1.0.5,This file does only contain a selection of the most common options. For a
v1.0.5,full list see the documentation:
v1.0.5,http://www.sphinx-doc.org/en/master/config
v1.0.5,-- Path setup --------------------------------------------------------------
v1.0.5,"If extensions (or modules to document with autodoc) are in another directory,"
v1.0.5,add these directories to sys.path here. If the directory is relative to the
v1.0.5,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.0.5,
v1.0.5,"sys.path.insert(0, os.path.abspath('..'))"
v1.0.5,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.0.5,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.0.5,from unittest.mock import Mock
v1.0.5,sys.modules['numpy'] = Mock()
v1.0.5,sys.modules['numpy.linalg'] = Mock()
v1.0.5,sys.modules['scipy'] = Mock()
v1.0.5,sys.modules['scipy.optimize'] = Mock()
v1.0.5,sys.modules['scipy.interpolate'] = Mock()
v1.0.5,sys.modules['scipy.sparse'] = Mock()
v1.0.5,sys.modules['scipy.ndimage'] = Mock()
v1.0.5,sys.modules['scipy.ndimage.filters'] = Mock()
v1.0.5,sys.modules['tensorflow'] = Mock()
v1.0.5,sys.modules['theano'] = Mock()
v1.0.5,sys.modules['theano.tensor'] = Mock()
v1.0.5,sys.modules['torch'] = Mock()
v1.0.5,sys.modules['torch.optim'] = Mock()
v1.0.5,sys.modules['torch.nn'] = Mock()
v1.0.5,sys.modules['torch.nn.init'] = Mock()
v1.0.5,sys.modules['torch.autograd'] = Mock()
v1.0.5,sys.modules['sklearn'] = Mock()
v1.0.5,sys.modules['sklearn.model_selection'] = Mock()
v1.0.5,sys.modules['sklearn.utils'] = Mock()
v1.0.5,-- Project information -----------------------------------------------------
v1.0.5,"The full version, including alpha/beta/rc tags."
v1.0.5,The short X.Y version.
v1.0.5,-- General configuration ---------------------------------------------------
v1.0.5,"If your documentation needs a minimal Sphinx version, state it here."
v1.0.5,
v1.0.5,needs_sphinx = '1.0'
v1.0.5,"If true, the current module name will be prepended to all description"
v1.0.5,unit titles (such as .. function::).
v1.0.5,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.0.5,"Add any Sphinx extension module names here, as strings. They can be"
v1.0.5,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.0.5,ones.
v1.0.5,generate autosummary pages
v1.0.5,"Add any paths that contain templates here, relative to this directory."
v1.0.5,The suffix(es) of source filenames.
v1.0.5,You can specify multiple suffix as a list of string:
v1.0.5,
v1.0.5,"source_suffix = ['.rst', '.md']"
v1.0.5,The master toctree document.
v1.0.5,The language for content autogenerated by Sphinx. Refer to documentation
v1.0.5,for a list of supported languages.
v1.0.5,
v1.0.5,This is also used if you do content translation via gettext catalogs.
v1.0.5,"Usually you set ""language"" from the command line for these cases."
v1.0.5,"List of patterns, relative to source directory, that match files and"
v1.0.5,directories to ignore when looking for source files.
v1.0.5,This pattern also affects html_static_path and html_extra_path.
v1.0.5,The name of the Pygments (syntax highlighting) style to use.
v1.0.5,-- Options for HTML output -------------------------------------------------
v1.0.5,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.0.5,a list of builtin themes.
v1.0.5,
v1.0.5,Theme options are theme-specific and customize the look and feel of a theme
v1.0.5,"further.  For a list of options available for each theme, see the"
v1.0.5,documentation.
v1.0.5,
v1.0.5,html_theme_options = {}
v1.0.5,"Add any paths that contain custom static files (such as style sheets) here,"
v1.0.5,"relative to this directory. They are copied after the builtin static files,"
v1.0.5,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.0.5,html_static_path = ['_static']
v1.0.5,"Custom sidebar templates, must be a dictionary that maps document names"
v1.0.5,to template names.
v1.0.5,
v1.0.5,The default sidebars (for documents that don't match any pattern) are
v1.0.5,defined by theme itself.  Builtin themes are using these templates by
v1.0.5,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.0.5,'searchbox.html']``.
v1.0.5,
v1.0.5,html_sidebars = {}
v1.0.5,The name of an image file (relative to this directory) to place at the top
v1.0.5,of the sidebar.
v1.0.5,
v1.0.5,-- Options for HTMLHelp output ---------------------------------------------
v1.0.5,Output file base name for HTML help builder.
v1.0.5,-- Options for LaTeX output ------------------------------------------------
v1.0.5,latex_elements = {
v1.0.5,The paper size ('letterpaper' or 'a4paper').
v1.0.5,
v1.0.5,"'papersize': 'letterpaper',"
v1.0.5,
v1.0.5,"The font size ('10pt', '11pt' or '12pt')."
v1.0.5,
v1.0.5,"'pointsize': '10pt',"
v1.0.5,
v1.0.5,Additional stuff for the LaTeX preamble.
v1.0.5,
v1.0.5,"'preamble': '',"
v1.0.5,
v1.0.5,Latex figure (float) alignment
v1.0.5,
v1.0.5,"'figure_align': 'htbp',"
v1.0.5,}
v1.0.5,Grouping the document tree into LaTeX files. List of tuples
v1.0.5,"(source start file, target name, title,"
v1.0.5,"author, documentclass [howto, manual, or own class])."
v1.0.5,latex_documents = [
v1.0.5,(
v1.0.5,"master_doc,"
v1.0.5,"'pykeen.tex',"
v1.0.5,"'PyKEEN Documentation',"
v1.0.5,"author,"
v1.0.5,"'manual',"
v1.0.5,"),"
v1.0.5,]
v1.0.5,-- Options for manual page output ------------------------------------------
v1.0.5,One entry per manual page. List of tuples
v1.0.5,"(source start file, name, description, authors, manual section)."
v1.0.5,-- Options for Texinfo output ----------------------------------------------
v1.0.5,Grouping the document tree into Texinfo files. List of tuples
v1.0.5,"(source start file, target name, title, author,"
v1.0.5,"dir menu entry, description, category)"
v1.0.5,-- Options for Epub output -------------------------------------------------
v1.0.5,Bibliographic Dublin Core info.
v1.0.5,epub_title = project
v1.0.5,The unique identifier of the text. This can be a ISBN number
v1.0.5,or the project homepage.
v1.0.5,
v1.0.5,epub_identifier = ''
v1.0.5,A unique identification for the text.
v1.0.5,
v1.0.5,epub_uid = ''
v1.0.5,A list of files that should not be packed into the epub file.
v1.0.5,epub_exclude_files = ['search.html']
v1.0.5,-- Extension configuration -------------------------------------------------
v1.0.5,-- Options for intersphinx extension ---------------------------------------
v1.0.5,Example configuration for intersphinx: refer to the Python standard library.
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Check a model param is optimized
v1.0.5,Check a loss param is optimized
v1.0.5,Check a model param is NOT optimized
v1.0.5,Check a loss param is optimized
v1.0.5,Check a model param is optimized
v1.0.5,Check a loss param is NOT optimized
v1.0.5,Check a model param is NOT optimized
v1.0.5,Check a loss param is NOT optimized
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,check for empty batches
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,The triples factory and model
v1.0.5,: The evaluator to be tested
v1.0.5,Settings
v1.0.5,: The evaluator instantiation
v1.0.5,Settings
v1.0.5,Initialize evaluator
v1.0.5,Use small test dataset
v1.0.5,Use small model (untrained)
v1.0.5,Get batch
v1.0.5,Compute scores
v1.0.5,Compute mask only if required
v1.0.5,TODO: Re-use filtering code
v1.0.5,"shape: (batch_size, num_triples)"
v1.0.5,"shape: (batch_size, num_entities)"
v1.0.5,Process one batch
v1.0.5,Check for correct class
v1.0.5,Check value ranges
v1.0.5,TODO: Validate with data?
v1.0.5,Check for correct class
v1.0.5,check value
v1.0.5,filtering
v1.0.5,"true_score: (2, 3, 3)"
v1.0.5,head based filter
v1.0.5,preprocessing for faster lookup
v1.0.5,check that all found positives are positive
v1.0.5,check in-place
v1.0.5,Test head scores
v1.0.5,Assert in-place modification
v1.0.5,Assert correct filtering
v1.0.5,Test tail scores
v1.0.5,Assert in-place modification
v1.0.5,Assert correct filtering
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The batch size
v1.0.5,: The triples factory
v1.0.5,: Class of regularizer to test
v1.0.5,: The constructor parameters to pass to the regularizer
v1.0.5,": The regularizer instance, initialized in setUp"
v1.0.5,: A positive batch
v1.0.5,: The device
v1.0.5,Use RESCAL as it regularizes multiple tensors of different shape.
v1.0.5,Check if regularizer is stored correctly.
v1.0.5,Forward pass (should update regularizer)
v1.0.5,Call post_parameter_update (should reset regularizer)
v1.0.5,Check if regularization term is reset
v1.0.5,Call method
v1.0.5,Generate random tensors
v1.0.5,Call update
v1.0.5,check shape
v1.0.5,compute expected term
v1.0.5,Generate random tensor
v1.0.5,calculate penalty
v1.0.5,check shape
v1.0.5,check value
v1.0.5,Tests that exception will be thrown when more than or less than three tensors are passed
v1.0.5,Test that regularization term is computed correctly
v1.0.5,Entity soft constraint
v1.0.5,Orthogonality soft constraint
v1.0.5,"After first update, should change the term"
v1.0.5,"After second update, no change should happen"
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The number of embeddings
v1.0.5,: The embedding dimension
v1.0.5,check shape
v1.0.5,check values
v1.0.5,check shape
v1.0.5,check values
v1.0.5,check correct value range
v1.0.5,check maximum norm constraint
v1.0.5,unchanged values for small norms
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,equal value; larger is better
v1.0.5,equal value; smaller is better
v1.0.5,larger is better; improvement
v1.0.5,larger is better; improvement; but not significant
v1.0.5,: The window size used by the early stopper
v1.0.5,: The mock losses the mock evaluator will return
v1.0.5,: The (zeroed) index  - 1 at which stopping will occur
v1.0.5,: The minimum improvement
v1.0.5,: The best results
v1.0.5,Set automatic_memory_optimization to false for tests
v1.0.5,Step early stopper
v1.0.5,check storing of results
v1.0.5,check ring buffer
v1.0.5,: The window size used by the early stopper
v1.0.5,: The (zeroed) index  - 1 at which stopping will occur
v1.0.5,: The minimum improvement
v1.0.5,: The random seed to use for reproducibility
v1.0.5,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.0.5,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.0.5,: The batch size to use.
v1.0.5,Fix seed for reproducibility
v1.0.5,Set automatic_memory_optimization to false during testing
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,check correct translation
v1.0.5,check column order
v1.0.5,apply restriction
v1.0.5,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.0.5,check that inverse_triples is correctly carried over
v1.0.5,verify that the label-to-ID mapping has not been changed
v1.0.5,verify that triples have been filtered
v1.0.5,verify that all entities and relations are present in the training factory
v1.0.5,verify that no triple got lost
v1.0.5,verify that the label-to-id mappings match
v1.0.5,Check if multilabels are working correctly
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The batch size
v1.0.5,: The random seed
v1.0.5,: The triples factory
v1.0.5,: The sLCWA instances
v1.0.5,: Class of negative sampling to test
v1.0.5,": The negative sampler instance, initialized in setUp"
v1.0.5,: A positive batch
v1.0.5,Generate negative sample
v1.0.5,check shape
v1.0.5,check bounds: heads
v1.0.5,check bounds: relations
v1.0.5,check bounds: tails
v1.0.5,Check that all elements got corrupted
v1.0.5,Generate scaled negative sample
v1.0.5,Generate negative samples
v1.0.5,test that the relations were not changed
v1.0.5,Test that half of the subjects and half of the objects are corrupted
v1.0.5,Generate negative sample for additional tests
v1.0.5,test that the relations were not changed
v1.0.5,sample a batch
v1.0.5,check shape
v1.0.5,get triples
v1.0.5,check connected components
v1.0.5,super inefficient
v1.0.5,join
v1.0.5,already joined
v1.0.5,check that there is only a single component
v1.0.5,check content of comp_adj_lists
v1.0.5,check edge ids
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The expected number of entities
v1.0.5,: The expected number of relations
v1.0.5,: The dataset to test
v1.0.5,Not loaded
v1.0.5,Load
v1.0.5,Test caching
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The class of the model to test
v1.0.5,: Additional arguments passed to the model's constructor method
v1.0.5,: The triples factory instance
v1.0.5,: The model instance
v1.0.5,: The batch size for use for forward_* tests
v1.0.5,: The embedding dimensionality
v1.0.5,: Whether to create inverse triples (needed e.g. by ConvE)
v1.0.5,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.0.5,: The batch size for use when testing training procedures
v1.0.5,: The number of epochs to train the model
v1.0.5,: A random number generator from torch
v1.0.5,: The number of parameters which receive a constant (i.e. non-randomized)
v1.0.5,initialization
v1.0.5,assert there is at least one trainable parameter
v1.0.5,Check that all the parameters actually require a gradient
v1.0.5,Try to initialize an optimizer
v1.0.5,get model parameters
v1.0.5,re-initialize
v1.0.5,check that the operation works in-place
v1.0.5,check that the parameters where modified
v1.0.5,check for finite values by default
v1.0.5,check whether a gradient can be back-propgated
v1.0.5,"assert batch comprises (head, relation) pairs"
v1.0.5,"assert batch comprises (relation, tail) pairs"
v1.0.5,TODO: Catch HolE MKL error?
v1.0.5,set regularizer term
v1.0.5,call post_parameter_update
v1.0.5,assert that the regularization term has been reset
v1.0.5,do one optimization step
v1.0.5,call post_parameter_update
v1.0.5,check model constraints
v1.0.5,"assert batch comprises (relation, tail) pairs"
v1.0.5,"assert batch comprises (relation, tail) pairs"
v1.0.5,"assert batch comprises (relation, tail) pairs"
v1.0.5,Distance-based model
v1.0.5,3x batch norm: bias + scale --> 6
v1.0.5,entity specific bias        --> 1
v1.0.5,==================================
v1.0.5,7
v1.0.5,"two bias terms, one conv-filter"
v1.0.5,check type
v1.0.5,check shape
v1.0.5,check ID ranges
v1.0.5,this is only done in one of the models
v1.0.5,this is only done in one of the models
v1.0.5,Two linear layer biases
v1.0.5,"Two BN layers, bias & scale"
v1.0.5,: one bias per layer
v1.0.5,: (scale & bias for BN) * layers
v1.0.5,entity embeddings
v1.0.5,relation embeddings
v1.0.5,Compute Scores
v1.0.5,"self.assertAlmostEqual(second_score, -16, delta=0.01)"
v1.0.5,Use different dimension for relation embedding: relation_dim > entity_dim
v1.0.5,relation embeddings
v1.0.5,Compute Scores
v1.0.5,Use different dimension for relation embedding: relation_dim < entity_dim
v1.0.5,entity embeddings
v1.0.5,relation embeddings
v1.0.5,Compute Scores
v1.0.5,random entity embeddings & projections
v1.0.5,random relation embeddings & projections
v1.0.5,project
v1.0.5,check shape:
v1.0.5,check normalization
v1.0.5,entity embeddings
v1.0.5,relation embeddings
v1.0.5,Compute Scores
v1.0.5,second_score = scores[1].item()
v1.0.5,: 2xBN (bias & scale)
v1.0.5,check shape
v1.0.5,check content
v1.0.5,: The number of entities
v1.0.5,: The number of triples
v1.0.5,check shape
v1.0.5,check dtype
v1.0.5,check finite values (e.g. due to division by zero)
v1.0.5,check non-negativity
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,
v1.0.5,
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,check for finite values by default
v1.0.5,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.5,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.5,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.5,Get embeddings
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The class
v1.0.5,: Constructor keyword arguments
v1.0.5,: The loss instance
v1.0.5,: The batch size
v1.0.5,test reduction
v1.0.5,Test backward
v1.0.5,: The number of entities.
v1.0.5,: The number of negative samples
v1.0.5,≈ result of softmax
v1.0.5,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.0.5,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.0.5,"pos_distances = [0., 0., 0.5, 0.5]"
v1.0.5,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.0.5,≈ result of sigmoid
v1.0.5,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.0.5,expected_loss ≈ 0.34
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Create dummy dense labels
v1.0.5,Check if labels form a probability distribution
v1.0.5,Apply label smoothing
v1.0.5,Check if smooth labels form probability distribution
v1.0.5,Create dummy sLCWA labels
v1.0.5,Apply label smoothing
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: A mapping of optimizers' names to their implementations
v1.0.5,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,"scale labels from [0, 1] to [-1, 1]"
v1.0.5,cross entropy expects a proper probability distribution -> normalize labels
v1.0.5,Use numerically stable variant to compute log(softmax)
v1.0.5,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.0.5,"To add *all* losses implemented in Torch, uncomment:"
v1.0.5,_LOSSES.update({
v1.0.5,loss
v1.0.5,for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()
v1.0.5,if not loss.__name__.startswith('_')
v1.0.5,})
v1.0.5,: A mapping of losses' names to their implementations
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.0.5,Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.
v1.0.5,lower bound
v1.0.5,upper bound
v1.0.5,Allocate weight on device
v1.0.5,Initialize if initializer is provided
v1.0.5,Wrap embedding around it.
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The overall regularization weight
v1.0.5,: The current regularization term (a scalar)
v1.0.5,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.0.5,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.5,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.5,no need to compute anything
v1.0.5,always return zero
v1.0.5,: The dimension along which to compute the vector-based regularization terms.
v1.0.5,: Whether to normalize the regularization term by the dimension of the vectors.
v1.0.5,: This allows dimensionality-independent weight tuning.
v1.0.5,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.5,expected value of |x|_1 = d*E[x_i] for x_i i.i.d.
v1.0.5,expected value of |x|_2 when x_i are normally distributed
v1.0.5,cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1
v1.0.5,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.5,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.5,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.0.5,"Therefore, apply_only_once is always set to True."
v1.0.5,Entity soft constraint
v1.0.5,Orthogonality soft constraint
v1.0.5,The normalization factor to balance individual regularizers' contribution.
v1.0.5,: A mapping of regularizers' names to their implementations
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Add HPO command
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The random seed used at the beginning of the pipeline
v1.0.5,: The model trained by the pipeline
v1.0.5,: The training loop used by the pipeline
v1.0.5,: The losses during training
v1.0.5,: The results evaluated by the pipeline
v1.0.5,: How long in seconds did training take?
v1.0.5,: How long in seconds did evaluation take?
v1.0.5,: An early stopper
v1.0.5,: Any additional metadata as a dictionary
v1.0.5,: The version of PyKEEN used to create these results
v1.0.5,: The git hash of PyKEEN used to create these results
v1.0.5,1. Dataset
v1.0.5,2. Model
v1.0.5,3. Loss
v1.0.5,4. Regularizer
v1.0.5,5. Optimizer
v1.0.5,6. Training Loop
v1.0.5,7. Training (ronaldo style)
v1.0.5,8. Evaluation
v1.0.5,9. Tracking
v1.0.5,Misc
v1.0.5,Start tracking
v1.0.5,evaluation restriction to a subset of entities/relations
v1.0.5,FIXME this should never happen.
v1.0.5,Log model parameters
v1.0.5,Log optimizer parameters
v1.0.5,Stopping
v1.0.5,"Load the evaluation batch size for the stopper, if it has been set"
v1.0.5,By default there's a stopper that does nothing interesting
v1.0.5,Add logging for debugging
v1.0.5,Train like Cristiano Ronaldo
v1.0.5,Evaluate
v1.0.5,Reuse optimal evaluation parameters from training if available
v1.0.5,Add logging about evaluator for debugging
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The WANDB run
v1.0.5,: A mapping of trackers' names to their implementations
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Create directory in which all experimental artifacts are saved
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: Functions for specifying exotic resources with a given prefix
v1.0.5,: Functions for specifying exotic resources based on their file extension
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.0.5,TODO vectorize code
v1.0.5,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.0.5,"FIXME is this ever possible, since this function is called in __init__?"
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Create lists out of sets for proper numpy indexing when loading the labels
v1.0.5,TODO is there a need to have a canonical sort order here?
v1.0.5,Split triples
v1.0.5,Sorting ensures consistent results when the triples are permuted
v1.0.5,Create mapping
v1.0.5,Sorting ensures consistent results when the triples are permuted
v1.0.5,Create mapping
v1.0.5,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.0.5,Filter all non-existent triples
v1.0.5,Note: Unique changes the order of the triples
v1.0.5,Note: Using unique means implicit balancing of training samples
v1.0.5,: The mapping from entities' labels to their indexes
v1.0.5,: The mapping from relations' labels to their indexes
v1.0.5,": A three-column matrix where each row are the head label,"
v1.0.5,": relation label, then tail label"
v1.0.5,": A three-column matrix where each row are the head identifier,"
v1.0.5,": relation identifier, then tail identifier"
v1.0.5,": A dictionary mapping each relation to its inverse, if inverse triples were created"
v1.0.5,TODO: Check if lazy evaluation would make sense
v1.0.5,Check if the triples are inverted already
v1.0.5,extend original triples with inverse ones
v1.0.5,Generate entity mapping if necessary
v1.0.5,Generate relation mapping if necessary
v1.0.5,Map triples of labels to triples of IDs.
v1.0.5,We can terminate the search after finding the first inverse occurrence
v1.0.5,Ensure 2d array in case only one triple was given
v1.0.5,FIXME this function is only ever used in tests
v1.0.5,Prepare shuffle index
v1.0.5,Prepare split index
v1.0.5,Take cumulative sum so the get separated properly
v1.0.5,Split triples
v1.0.5,Make sure that the first element has all the right stuff in it
v1.0.5,Make new triples factories for each group
v1.0.5,Input validation
v1.0.5,convert to numpy
v1.0.5,vectorized label lookup
v1.0.5,Additional columns
v1.0.5,convert PyTorch tensors to numpy
v1.0.5,convert to dataframe
v1.0.5,Re-order columns
v1.0.5,Filter for entities
v1.0.5,Filter for relations
v1.0.5,No filtering happened
v1.0.5,manually copy the inverse relation mappings
v1.0.5,While there are still triples that should be moved to the training set
v1.0.5,Pick a random triple to move over to the training triples
v1.0.5,Recalculate the testing triples without that index
v1.0.5,"Recalculate the training entities, testing entities, to_move, and move_id_mask"
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: A PyTorch tensor of triples
v1.0.5,: A mapping from relation labels to integer identifiers
v1.0.5,: A mapping from relation labels to integer identifiers
v1.0.5,Create dense target
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,basically take all candidates
v1.0.5,Calculate which relations are the inverse ones
v1.0.5,FIXME doesn't carry flag of create_inverse_triples through
v1.0.5,A dictionary of all of the head/tail pairs for a given relation
v1.0.5,A dictionary for all of the tail/head pairs for a given relation
v1.0.5,Calculate the similarity between each relationship (entries in ``forward``)
v1.0.5,with all other candidate inverse relationships (entries in ``inverse``)
v1.0.5,"Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the"
v1.0.5,"same as for ``(b, a)``"
v1.0.5,A dictionary of all of the head/tail pairs for a given relation
v1.0.5,Filter out results between a given relationship and itself
v1.0.5,Filter out results below a minimum frequency
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,preprocessing
v1.0.5,initialize
v1.0.5,sample iteratively
v1.0.5,determine weights
v1.0.5,only happens at first iteration
v1.0.5,normalize to probabilities
v1.0.5,sample a start node
v1.0.5,get list of neighbors
v1.0.5,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.0.5,visit target node
v1.0.5,decrease sample counts
v1.0.5,return chosen edges
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Create training instances
v1.0.5,During size probing the training instances should not show the tqdm progress bar
v1.0.5,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.0.5,Ensure the release of memory
v1.0.5,Clear optimizer
v1.0.5,"Take the biggest possible training batch_size, if batch_size not set"
v1.0.5,This will find necessary parameters to optimize the use of the hardware at hand
v1.0.5,return the relevant parameters slice_size and batch_size
v1.0.5,Create dummy result tracker
v1.0.5,Sanity check
v1.0.5,Force weight initialization if training continuation is not explicitly requested.
v1.0.5,Reset the weights
v1.0.5,Create new optimizer
v1.0.5,Ensure the model is on the correct device
v1.0.5,Create Sampler
v1.0.5,Bind
v1.0.5,"When size probing, we don't want progress bars"
v1.0.5,Create progress bar
v1.0.5,Training Loop
v1.0.5,Enforce training mode
v1.0.5,Accumulate loss over epoch
v1.0.5,Batching
v1.0.5,Only create a progress bar when not in size probing mode
v1.0.5,Flag to check when to quit the size probing
v1.0.5,Recall that torch *accumulates* gradients. Before passing in a
v1.0.5,"new instance, you need to zero out the gradients from the old instance"
v1.0.5,Get batch size of current batch (last batch may be incomplete)
v1.0.5,accumulate gradients for whole batch
v1.0.5,forward pass call
v1.0.5,"when called by batch_size_search(), the parameter update should not be applied."
v1.0.5,update parameters according to optimizer
v1.0.5,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.0.5,constraints are no longer applied
v1.0.5,For testing purposes we're only interested in processing one batch
v1.0.5,When size probing we don't need the losses
v1.0.5,Track epoch loss
v1.0.5,Print loss information to console
v1.0.5,forward pass
v1.0.5,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.0.5,correction for loss reduction
v1.0.5,backward pass
v1.0.5,reset the regularizer to free the computational graph
v1.0.5,Set upper bound
v1.0.5,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.0.5,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.5,The regularizer has to be reset to free the computational graph
v1.0.5,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Shuffle each epoch
v1.0.5,Lazy-splitting into batches
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Slicing is not possible in sLCWA training loops
v1.0.5,Send positive batch to device
v1.0.5,Create negative samples
v1.0.5,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.0.5,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.0.5,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.0.5,Compute negative and positive scores
v1.0.5,Repeat positives scores (necessary for more than one negative per positive)
v1.0.5,Stack predictions
v1.0.5,Create target
v1.0.5,Normalize the loss to have the average loss per positive triple
v1.0.5,This allows comparability of sLCWA and LCWA losses
v1.0.5,Slicing is not possible for sLCWA
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: A mapping of training loops' names to their implementations
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Split batch components
v1.0.5,Send batch to device
v1.0.5,Apply label smoothing
v1.0.5,This shows how often one row has to be repeated
v1.0.5,Create boolean indices for negative labels in the repeated rows
v1.0.5,Repeat the predictions and filter for negative labels
v1.0.5,This tells us how often each true label should be repeated
v1.0.5,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.0.5,Split positive and negative scores
v1.0.5,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.5,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,now: smaller is better
v1.0.5,: The model
v1.0.5,: The evaluator
v1.0.5,: The triples to use for evaluation
v1.0.5,: Size of the evaluation batches
v1.0.5,: Slice size of the evaluation batches
v1.0.5,: The number of epochs after which the model is evaluated on validation set
v1.0.5,: The number of iterations (one iteration can correspond to various epochs)
v1.0.5,: with no improvement after which training will be stopped.
v1.0.5,: The name of the metric to use
v1.0.5,: The minimum relative improvement necessary to consider it an improved result
v1.0.5,: The best result so far
v1.0.5,: The epoch at which the best result occurred
v1.0.5,: The remaining patience
v1.0.5,: The metric results from all evaluations
v1.0.5,": Whether a larger value is better, or a smaller"
v1.0.5,: The result tracker
v1.0.5,: Callbacks when after results are calculated
v1.0.5,: Callbacks when training gets continued
v1.0.5,: Callbacks when training is stopped early
v1.0.5,: Did the stopper ever decide to stop?
v1.0.5,TODO: Fix this
v1.0.5,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.0.5,raise ValueError(f'Invalid metric name: {self.metric}')
v1.0.5,Dummy result tracker
v1.0.5,Evaluate
v1.0.5,Only perform time consuming checks for the first call.
v1.0.5,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.0.5,Append to history
v1.0.5,check for improvement
v1.0.5,Stop if the result did not improve more than delta for patience evaluations
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: A mapping of stoppers' names to their implementations
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.0.5,Clear the ranks from the current evaluator
v1.0.5,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.0.5,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.5,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.5,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.5,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.0.5,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.5,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.5,Test if slicing is implemented for the required functions of this model
v1.0.5,Split batch
v1.0.5,Bind shape
v1.0.5,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.0.5,Warn if all entities will be filtered
v1.0.5,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.0.5,verify that the triples have been filtered
v1.0.5,Send to device
v1.0.5,Ensure evaluation mode
v1.0.5,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.0.5,Check whether we need to be prepared for filtering
v1.0.5,Check whether an evaluator needs access to the masks
v1.0.5,This can only be an unfiltered evaluator.
v1.0.5,Prepare for result filtering
v1.0.5,Send tensors to device
v1.0.5,Prepare batches
v1.0.5,Show progressbar
v1.0.5,Flag to check when to quit the size probing
v1.0.5,Disable gradient tracking
v1.0.5,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.0.5,batch-wise processing
v1.0.5,If we only probe sizes we do not need more than one batch
v1.0.5,Finalize
v1.0.5,Predict scores once
v1.0.5,Select scores of true
v1.0.5,Create positive filter for all corrupted
v1.0.5,Needs all positive triples
v1.0.5,Create filter
v1.0.5,Create a positive mask with the size of the scores from the positive filter
v1.0.5,Restrict to entities of interest
v1.0.5,Evaluate metrics on these *unfiltered* scores
v1.0.5,Filter
v1.0.5,The scores for the true triples have to be rewritten to the scores tensor
v1.0.5,Restrict to entities of interest
v1.0.5,Evaluate metrics on these *filtered* scores
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The area under the ROC curve
v1.0.5,: The area under the precision-recall curve
v1.0.5,: The coverage error
v1.0.5,coverage_error: float = field(metadata=dict(
v1.0.5,"doc='The coverage error',"
v1.0.5,"f=metrics.coverage_error,"
v1.0.5,))
v1.0.5,: The label ranking loss (APS)
v1.0.5,label_ranking_average_precision_score: float = field(metadata=dict(
v1.0.5,"doc='The label ranking loss (APS)',"
v1.0.5,"f=metrics.label_ranking_average_precision_score,"
v1.0.5,))
v1.0.5,#: The label ranking loss
v1.0.5,label_ranking_loss: float = field(metadata=dict(
v1.0.5,"doc='The label ranking loss',"
v1.0.5,"f=metrics.label_ranking_loss,"
v1.0.5,))
v1.0.5,Transfer to cpu and convert to numpy
v1.0.5,Ensure that each key gets counted only once
v1.0.5,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.0.5,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.0.5,masks using the exact same key order.
v1.0.5,TODO how to define a cutoff on y_scores to make binary?
v1.0.5,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.0.5,Clear buffers
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: A mapping of evaluators' names to their implementations
v1.0.5,: A mapping of results' names to their implementations
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,The best rank is the rank when assuming all options with an equal score are placed behind the currently
v1.0.5,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.0.5,The worst rank is the rank when assuming all options with an equal score are placed in front of the currently
v1.0.5,"considered. Hence, the rank is the number of options which have at least the same score minus one (as the"
v1.0.5,"currently considered option in included in all options). As the rank is one-based, we have to add 1, which"
v1.0.5,"nullifies the ""minus 1"" from before."
v1.0.5,"The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of"
v1.0.5,the elements with the same score as the currently considered option.
v1.0.5,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.0.5,The expected rank of a random scoring
v1.0.5,The adjusted ranks is normalized by the expected rank of a random scoring
v1.0.5,TODO adjusted_worst_rank
v1.0.5,TODO adjusted_best_rank
v1.0.5,: The mean over all ranks: mean_i r_i. Lower is better.
v1.0.5,: The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.
v1.0.5,": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k."
v1.0.5,: Higher is better.
v1.0.5,: The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.
v1.0.5,: Described by [berrendorf2020]_.
v1.0.5,Check if it a side or rank type
v1.0.5,Clear buffers
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.0.5,Create a tensor of all IDs
v1.0.5,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.0.5,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.0.5,: A dictionary of hyper-parameters to the models that use them
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The default loss function class
v1.0.5,: The default parameters for the default loss function class
v1.0.5,: The instance of the loss
v1.0.5,: The default regularizer class
v1.0.5,: The default parameters for the default regularizer class
v1.0.5,: The instance of the regularizer
v1.0.5,Initialize the device
v1.0.5,Random seeds have to set before the embeddings are initialized
v1.0.5,Loss
v1.0.5,TODO: Check loss functions that require 1 and -1 as label but only
v1.0.5,Regularizer
v1.0.5,The triples factory facilitates access to the dataset.
v1.0.5,This allows to store the optimized parameters
v1.0.5,Keep track of the hyper-parameters that are used across all
v1.0.5,subclasses of BaseModule
v1.0.5,Enforce evaluation mode
v1.0.5,Enforce evaluation mode
v1.0.5,Enforce evaluation mode
v1.0.5,Enforce evaluation mode
v1.0.5,The number of relations stored in the triples factory includes the number of inverse relations
v1.0.5,Id of inverse relation: relation + 1
v1.0.5,"The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs"
v1.0.5,initialize buffer on cpu
v1.0.5,calculate batch scores
v1.0.5,Explicitly create triples
v1.0.5,Sort final result
v1.0.5,Train a model (quickly)
v1.0.5,Get scores for *all* triples
v1.0.5,Get scores for top 15 triples
v1.0.5,set model to evaluation mode
v1.0.5,Do not track gradients
v1.0.5,initialize buffer on device
v1.0.5,calculate batch scores
v1.0.5,get top scores within batch
v1.0.5,append to global top scores
v1.0.5,reduce size if necessary
v1.0.5,Sort final result
v1.0.5,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.0.5,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.5,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.0.5,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.0.5,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.5,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.0.5,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.0.5,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.5,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.0.5,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.0.5,Default for relation dimensionality
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: A mapping of models' names to their implementations
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Store initial input for error message
v1.0.5,All are None
v1.0.5,"input_channels is None, and any of height or width is None -> set input_channels=1"
v1.0.5,"input channels is not None, and one of height or width is None"
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The default loss function class
v1.0.5,: The default parameters for the default loss function class
v1.0.5,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.0.5,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.0.5,ConvE should be trained with inverse triples
v1.0.5,ConvE uses one bias for each entity
v1.0.5,Automatic calculation of remaining dimensions
v1.0.5,Parameter need to fulfil:
v1.0.5,input_channels * embedding_height * embedding_width = embedding_dim
v1.0.5,Finalize initialization
v1.0.5,embeddings
v1.0.5,weights
v1.0.5,"batch_size, num_input_channels, 2*height, width"
v1.0.5,"batch_size, num_input_channels, 2*height, width"
v1.0.5,"batch_size, num_input_channels, 2*height, width"
v1.0.5,"(N,C_out,H_out,W_out)"
v1.0.5,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.0.5,Embedding Regularization
v1.0.5,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.0.5,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.5,Embedding Regularization
v1.0.5,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.5,Embedding Regularization
v1.0.5,Code to repeat each item successively instead of the entire tensor
v1.0.5,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,Embeddings
v1.0.5,Finalize initialization
v1.0.5,Initialise left relation embeddings to unit length
v1.0.5,Make sure to call super first
v1.0.5,Normalise embeddings of entities
v1.0.5,Get embeddings
v1.0.5,Project entities
v1.0.5,Get embeddings
v1.0.5,Project entities
v1.0.5,Project entities
v1.0.5,Project entities
v1.0.5,Get embeddings
v1.0.5,Project entities
v1.0.5,Project entities
v1.0.5,Project entities
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The default loss function class
v1.0.5,: The default parameters for the default loss function class
v1.0.5,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.0.5,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.0.5,Finalize initialization
v1.0.5,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.0.5,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.0.5,split into real and imaginary part
v1.0.5,ComplEx space bilinear product
v1.0.5,*: Elementwise multiplication
v1.0.5,get embeddings
v1.0.5,Compute scores
v1.0.5,Regularization
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The regularizer used by [nickel2011]_ for for RESCAL
v1.0.5,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.0.5,: a normalized weight of 10 is used.
v1.0.5,: The LP settings used by [nickel2011]_ for for RESCAL
v1.0.5,Finalize initialization
v1.0.5,Get embeddings
v1.0.5,"shape: (b, d)"
v1.0.5,"shape: (b, d, d)"
v1.0.5,"shape: (b, d)"
v1.0.5,Compute scores
v1.0.5,Regularization
v1.0.5,Compute scores
v1.0.5,Regularization
v1.0.5,Get embeddings
v1.0.5,Compute scores
v1.0.5,Regularization
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,Finalize initialization
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.0.5,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.0.5,The interaction model
v1.0.5,Finalize initialization
v1.0.5,embeddings
v1.0.5,Use Xavier initialization for weight; bias to zero
v1.0.5,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.0.5,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.0.5,Output layer regularization
v1.0.5,In the code base only the weights of the output layer are used for regularization
v1.0.5,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.0.5,Stack to convolution input
v1.0.5,Convolution
v1.0.5,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.0.5,Linear layer for final scores
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,Finalize initialization
v1.0.5,": shape: (batch_size, num_entities, d)"
v1.0.5,": Prepare h: (b, e, d) -> (b, e, 1, 1, d)"
v1.0.5,": Prepare t: (b, e, d) -> (b, e, 1, d, 1)"
v1.0.5,": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)"
v1.0.5,"h.T @ W @ t, shape: (b, e, k, 1, 1)"
v1.0.5,": reduce (b, e, k, 1, 1) -> (b, e, k)"
v1.0.5,": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.5,": Prepare h: (b, e, d) -> (b, e, d, 1)"
v1.0.5,"V_h @ h, shape: (b, e, k, 1)"
v1.0.5,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.5,": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.5,": Prepare t: (b, e, d) -> (b, e, d, 1)"
v1.0.5,"V_t @ t, shape: (b, e, k, 1)"
v1.0.5,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.5,": Prepare b: (R, k) -> (b, k) -> (b, 1, k)"
v1.0.5,"a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)"
v1.0.5,"prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)"
v1.0.5,"prepare act: (b, e, k) -> (b, e, 1, k)"
v1.0.5,"compute score, shape: (b, e, 1, 1)"
v1.0.5,reduce
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The regularizer used by [yang2014]_ for DistMult
v1.0.5,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.0.5,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.0.5,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.0.5,: The LP settings used by [yang2014]_ for DistMult
v1.0.5,Finalize initialization
v1.0.5,"xavier uniform, cf."
v1.0.5,https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17
v1.0.5,Initialise relation embeddings to unit length
v1.0.5,Make sure to call super first
v1.0.5,Normalize embeddings of entities
v1.0.5,Bilinear product
v1.0.5,*: Elementwise multiplication
v1.0.5,Get embeddings
v1.0.5,Compute score
v1.0.5,Only regularize relation embeddings
v1.0.5,Get embeddings
v1.0.5,Rank against all entities
v1.0.5,Only regularize relation embeddings
v1.0.5,Get embeddings
v1.0.5,Rank against all entities
v1.0.5,Only regularize relation embeddings
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,Similarity function used for distributions
v1.0.5,element-wise covariance bounds
v1.0.5,Additional covariance embeddings
v1.0.5,Finalize initialization
v1.0.5,Constraints are applied through post_parameter_update
v1.0.5,Make sure to call super first
v1.0.5,Normalize entity embeddings
v1.0.5,Ensure positive definite covariances matrices and appropriate size by clamping
v1.0.5,Get embeddings
v1.0.5,Compute entity distribution
v1.0.5,: a = \mu^T\Sigma^{-1}\mu
v1.0.5,: b = \log \det \Sigma
v1.0.5,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.0.5,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.0.5,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.0.5,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The custom regularizer used by [wang2014]_ for TransH
v1.0.5,: The settings used by [wang2014]_ for TransH
v1.0.5,embeddings
v1.0.5,Finalize initialization
v1.0.5,TODO: Add initialization
v1.0.5,Make sure to call super first
v1.0.5,Normalise the normal vectors by their l2 norms
v1.0.5,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.0.5,which enforces the defined soft constraints.
v1.0.5,Get embeddings
v1.0.5,Project to hyperplane
v1.0.5,Regularization term
v1.0.5,Get embeddings
v1.0.5,Project to hyperplane
v1.0.5,Regularization term
v1.0.5,Get embeddings
v1.0.5,Project to hyperplane
v1.0.5,Regularization term
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,embeddings
v1.0.5,Finalize initialization
v1.0.5,Make sure to call super first
v1.0.5,Normalize entity embeddings
v1.0.5,TODO: Initialize from TransE
v1.0.5,Initialise relation embeddings to unit length
v1.0.5,"project to relation specific subspace, shape: (b, e, d_r)"
v1.0.5,ensure constraints
v1.0.5,"evaluate score function, shape: (b, e)"
v1.0.5,Get embeddings
v1.0.5,Get embeddings
v1.0.5,Get embeddings
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,Construct node neighbourhood mask
v1.0.5,Set nodes in batch to true
v1.0.5,Compute k-neighbourhood
v1.0.5,"if the target node needs an embeddings, so does the source node"
v1.0.5,Create edge mask
v1.0.5,pylint: disable=unused-argument
v1.0.5,"Calculate in-degree, i.e. number of incoming edges"
v1.0.5,pylint: disable=unused-argument
v1.0.5,"Calculate in-degree, i.e. number of incoming edges"
v1.0.5,: Interaction model used as decoder
v1.0.5,: The blocks of the relation-specific weight matrices
v1.0.5,": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)"
v1.0.5,: The base weight matrices to generate relation-specific weights
v1.0.5,": shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.5,: The relation-specific weights for each base
v1.0.5,": shape: (num_relations, num_bases)"
v1.0.5,: The biases for each layer (if used)
v1.0.5,": shape of each element: (embedding_dim,)"
v1.0.5,: Batch normalization for each layer (if used)
v1.0.5,: Activations for each layer (if used)
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,Instantiate model
v1.0.5,Heuristic
v1.0.5,buffering of messages
v1.0.5,"Save graph using buffers, such that the tensors are moved together with the model"
v1.0.5,Weights
v1.0.5,Finalize initialization
v1.0.5,invalidate enriched embeddings
v1.0.5,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.0.5,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.0.5,initialized properly)
v1.0.5,We have one additional relation for self-loops
v1.0.5,Xavier Glorot initialization of each block
v1.0.5,Reset biases
v1.0.5,Reset batch norm parameters
v1.0.5,"Reset activation parameters, if any"
v1.0.5,use buffered messages if applicable
v1.0.5,Bind fields
v1.0.5,"shape: (num_entities, embedding_dim)"
v1.0.5,Edge dropout: drop the same edges on all layers (only in training mode)
v1.0.5,Get random dropout mask
v1.0.5,Apply to edges
v1.0.5,Different dropout for self-loops (only in training mode)
v1.0.5,"If batch is given, compute (num_layers)-hop neighbourhood"
v1.0.5,Initialize embeddings in the next layer for all nodes
v1.0.5,TODO: Can we vectorize this loop?
v1.0.5,Choose the edges which are of the specific relation
v1.0.5,Only propagate messages on subset of edges
v1.0.5,No edges available? Skip rest of inner loop
v1.0.5,Get source and target node indices
v1.0.5,send messages in both directions
v1.0.5,Select source node embeddings
v1.0.5,get relation weights
v1.0.5,Compute message (b x d) * (d x d) = (b x d)
v1.0.5,Normalize messages by relation-specific in-degree
v1.0.5,Aggregate messages in target
v1.0.5,Self-loop
v1.0.5,"Apply bias, if requested"
v1.0.5,"Apply batch normalization, if requested"
v1.0.5,Apply non-linearity
v1.0.5,allocate weight
v1.0.5,Get blocks
v1.0.5,"self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)"
v1.0.5,note: embedding_dim is guaranteed to be divisible by num_bases in the constructor
v1.0.5,"The current basis weights, shape: (num_bases)"
v1.0.5,"the current bases, shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.5,"compute the current relation weights, shape: (embedding_dim, embedding_dim)"
v1.0.5,Enrich embeddings
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The default loss function class
v1.0.5,: The default parameters for the default loss function class
v1.0.5,Core tensor
v1.0.5,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.0.5,Dropout
v1.0.5,Finalize initialization
v1.0.5,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.0.5,Abbreviation
v1.0.5,Compute h_n = DO(BN(h))
v1.0.5,Compute wr = DO(W x_2 r)
v1.0.5,compute whr = DO(BN(h_n x_1 wr))
v1.0.5,Compute whr x_3 t
v1.0.5,Get embeddings
v1.0.5,Compute scores
v1.0.5,Get embeddings
v1.0.5,Compute scores
v1.0.5,Get embeddings
v1.0.5,Compute scores
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,Finalize initialization
v1.0.5,Initialise relation embeddings to unit length
v1.0.5,Make sure to call super first
v1.0.5,Normalize entity embeddings
v1.0.5,Get embeddings
v1.0.5,Get embeddings
v1.0.5,Get embeddings
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The default loss function class
v1.0.5,: The default parameters for the default loss function class
v1.0.5,: The regularizer used by [trouillon2016]_ for SimplE
v1.0.5,": In the paper, they use weight of 0.1, and do not normalize the"
v1.0.5,": regularization term by the number of elements, which is 200."
v1.0.5,: The power sum settings used by [trouillon2016]_ for SimplE
v1.0.5,extra embeddings
v1.0.5,Finalize initialization
v1.0.5,forward model
v1.0.5,Regularization
v1.0.5,backward model
v1.0.5,Regularization
v1.0.5,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.0.5,"That is not mentioned in the paper, so it is omitted here."
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,Finalize initialization
v1.0.5,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.0.5,weight initialization
v1.0.5,Get embeddings
v1.0.5,Embedding Regularization
v1.0.5,Concatenate them
v1.0.5,Compute scores
v1.0.5,Get embeddings
v1.0.5,Embedding Regularization
v1.0.5,First layer can be unrolled
v1.0.5,Send scores through rest of the network
v1.0.5,Get embeddings
v1.0.5,Embedding Regularization
v1.0.5,First layer can be unrolled
v1.0.5,Send scores through rest of the network
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,The dimensions affected by e'
v1.0.5,Project entities
v1.0.5,r_p (e_p.T e) + e'
v1.0.5,Enforce constraints
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,Finalize initialization
v1.0.5,Make sure to call super first
v1.0.5,Normalize entity embeddings
v1.0.5,Project entities
v1.0.5,score = -||h_bot + r - t_bot||_2^2
v1.0.5,Head
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,Finalize initialization
v1.0.5,phases randomly between 0 and 2 pi
v1.0.5,Make sure to call super first
v1.0.5,Normalize relation embeddings
v1.0.5,Decompose into real and imaginary part
v1.0.5,Rotate (=Hadamard product in complex space).
v1.0.5,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.0.5,Get embeddings
v1.0.5,Compute scores
v1.0.5,Embedding Regularization
v1.0.5,Get embeddings
v1.0.5,Rank against all entities
v1.0.5,Compute scores
v1.0.5,Embedding Regularization
v1.0.5,Get embeddings
v1.0.5,r expresses a rotation in complex plane.
v1.0.5,The inverse rotation is expressed by the complex conjugate of r.
v1.0.5,The score is computed as the distance of the relation-rotated head to the tail.
v1.0.5,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.0.5,|h * r - t| = |h - conj(r) * t|
v1.0.5,Rank against all entities
v1.0.5,Compute scores
v1.0.5,Embedding Regularization
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The default loss function class
v1.0.5,: The default parameters for the default loss function class
v1.0.5,Global entity projection
v1.0.5,Global relation projection
v1.0.5,Global combination bias
v1.0.5,Global combination bias
v1.0.5,Finalize initialization
v1.0.5,Get embeddings
v1.0.5,Compute score
v1.0.5,Get embeddings
v1.0.5,Rank against all entities
v1.0.5,Get embeddings
v1.0.5,Rank against all entities
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,Finalize initialization
v1.0.5,Make sure to call super first
v1.0.5,Normalize entity embeddings
v1.0.5,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.0.5,Circular correlation of entity embeddings
v1.0.5,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.0.5,Hadamard product in frequency domain
v1.0.5,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.0.5,inner product with relation embedding
v1.0.5,Embedding Regularization
v1.0.5,Embedding Regularization
v1.0.5,Embedding Regularization
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The default loss function class
v1.0.5,: The default parameters for the default loss function class
v1.0.5,Finalize initialization
v1.0.5,Get embeddings
v1.0.5,Embedding Regularization
v1.0.5,Concatenate them
v1.0.5,Predict t embedding
v1.0.5,compare with all t's
v1.0.5,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.5,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.5,Embedding Regularization
v1.0.5,Concatenate them
v1.0.5,Predict t embedding
v1.0.5,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.5,Embedding Regularization
v1.0.5,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.0.5,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.0.5,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.0.5,Extend t
v1.0.5,Concatenate them
v1.0.5,Predict t embedding
v1.0.5,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.5,The results have to be realigned with the expected output of the score_h function
v1.0.5,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,TODO: Check entire build of the model
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The default loss function class
v1.0.5,: The default parameters for the default loss function class
v1.0.5,Literal
v1.0.5,num_ent x num_lit
v1.0.5,Number of columns corresponds to number of literals
v1.0.5,Literals
v1.0.5,End literals
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,TODO: Check entire build of the model
v1.0.5,: The default strategy for optimizing the model's hyper-parameters
v1.0.5,: The default parameters for the default loss function class
v1.0.5,Embeddings
v1.0.5,Number of columns corresponds to number of literals
v1.0.5,apply dropout
v1.0.5,"-, because lower score shall correspond to a more plausible triple."
v1.0.5,TODO check if this is the same as the BaseModule
v1.0.5,Choose y = -1 since a smaller score is better.
v1.0.5,"In TransE for example, the scores represent distances"
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.5,Bind number of negatives to sample
v1.0.5,Equally corrupt head and tail
v1.0.5,Copy positive batch for corruption.
v1.0.5,"Do not detach, as no gradients should flow into the indices."
v1.0.5,Sample random entities as replacement
v1.0.5,Replace heads – To make sure we don't replace the head by the original value
v1.0.5,we shift all values greater or equal than the original value by one up
v1.0.5,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.5,Corrupt tails
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: A mapping of negative samplers' names to their implementations
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.5,Preprocessing: Compute corruption probabilities
v1.0.5,"compute tph, i.e. the average number of tail entities per head"
v1.0.5,"compute hpt, i.e. the average number of head entities per tail"
v1.0.5,Set parameter for Bernoulli distribution
v1.0.5,Bind number of negatives to sample
v1.0.5,Copy positive batch for corruption.
v1.0.5,"Do not detach, as no gradients should flow into the indices."
v1.0.5,Decide whether to corrupt head or tail
v1.0.5,Tails are corrupted if heads are not corrupted
v1.0.5,Randomly sample corruption
v1.0.5,Replace heads – To make sure we don't replace the head by the original value
v1.0.5,we shift all values greater or equal than the original value by one up
v1.0.5,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.5,Replace tails
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,TODO what happens if already exists?
v1.0.5,TODO incorporate setting of random seed
v1.0.5,pipeline_kwargs=dict(
v1.0.5,"random_seed=random_non_negative_int(),"
v1.0.5,"),"
v1.0.5,Add dataset to current_pipeline
v1.0.5,"Training, test, and validation paths are provided"
v1.0.5,Add loss function to current_pipeline
v1.0.5,Add regularizer to current_pipeline
v1.0.5,Add optimizer to current_pipeline
v1.0.5,Add training approach to current_pipeline
v1.0.5,Add training kwargs and kwargs_ranges
v1.0.5,Add evaluation
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: A factory wrapping the training triples
v1.0.5,": A factory wrapping the testing triples, that share indexes with the training triples"
v1.0.5,": A factory wrapping the validation triples, that share indexes with the training triples"
v1.0.5,: All data sets should take care of inverse triple creation
v1.0.5,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.0.5,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.0.5,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.0.5,don't call this function by itself. assumes called through the `validation`
v1.0.5,property and the _training factory has already been loaded
v1.0.5,see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content
v1.0.5,pattern from https://stackoverflow.com/a/39217788/5775947
v1.0.5,TODO replace this with the new zip remote dataset class
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: A mapping of datasets' names to their classes
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,TODO update docs with table and CLI wtih generator
v1.0.5,: A mapping of HPO samplers' names to their implementations
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,1. Dataset
v1.0.5,2. Model
v1.0.5,3. Loss
v1.0.5,4. Regularizer
v1.0.5,5. Optimizer
v1.0.5,6. Training Loop
v1.0.5,7. Training
v1.0.5,8. Evaluation
v1.0.5,9. Trackers
v1.0.5,Misc.
v1.0.5,2. Model
v1.0.5,3. Loss
v1.0.5,4. Regularizer
v1.0.5,5. Optimizer
v1.0.5,1. Dataset
v1.0.5,2. Model
v1.0.5,3. Loss
v1.0.5,4. Regularizer
v1.0.5,5. Optimizer
v1.0.5,6. Training Loop
v1.0.5,7. Training
v1.0.5,8. Evaluation
v1.0.5,9. Tracker
v1.0.5,Misc.
v1.0.5,Will trigger Optuna to set the state of the trial as failed
v1.0.5,: The :mod:`optuna` study object
v1.0.5,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.0.5,Output study information
v1.0.5,Output all trials
v1.0.5,Output best trial as pipeline configuration file
v1.0.5,1. Dataset
v1.0.5,2. Model
v1.0.5,3. Loss
v1.0.5,4. Regularizer
v1.0.5,5. Optimizer
v1.0.5,6. Training Loop
v1.0.5,7. Training
v1.0.5,8. Evaluation
v1.0.5,9. Tracking
v1.0.5,6. Misc
v1.0.5,Optuna Study Settings
v1.0.5,Optuna Optimization Settings
v1.0.5,0. Metadata/Provenance
v1.0.5,1. Dataset
v1.0.5,2. Model
v1.0.5,3. Loss
v1.0.5,4. Regularizer
v1.0.5,5. Optimizer
v1.0.5,6. Training Loop
v1.0.5,7. Training
v1.0.5,8. Evaluation
v1.0.5,9. Tracking
v1.0.5,1. Dataset
v1.0.5,2. Model
v1.0.5,3. Loss
v1.0.5,4. Regularizer
v1.0.5,5. Optimizer
v1.0.5,6. Training Loop
v1.0.5,7. Training
v1.0.5,8. Evaluation
v1.0.5,9. Tracker
v1.0.5,Optuna Misc.
v1.0.5,Pipeline Misc.
v1.0.5,Invoke optimization of the objective function.
v1.0.5,TODO make more informative
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,-*- coding: utf-8 -*-
v1.0.5,: A mapping of HPO pruners' names to their implementations
v1.0.5,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,
v1.0.4,Configuration file for the Sphinx documentation builder.
v1.0.4,
v1.0.4,This file does only contain a selection of the most common options. For a
v1.0.4,full list see the documentation:
v1.0.4,http://www.sphinx-doc.org/en/master/config
v1.0.4,-- Path setup --------------------------------------------------------------
v1.0.4,"If extensions (or modules to document with autodoc) are in another directory,"
v1.0.4,add these directories to sys.path here. If the directory is relative to the
v1.0.4,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.0.4,
v1.0.4,"sys.path.insert(0, os.path.abspath('..'))"
v1.0.4,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.0.4,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.0.4,from unittest.mock import Mock
v1.0.4,sys.modules['numpy'] = Mock()
v1.0.4,sys.modules['numpy.linalg'] = Mock()
v1.0.4,sys.modules['scipy'] = Mock()
v1.0.4,sys.modules['scipy.optimize'] = Mock()
v1.0.4,sys.modules['scipy.interpolate'] = Mock()
v1.0.4,sys.modules['scipy.sparse'] = Mock()
v1.0.4,sys.modules['scipy.ndimage'] = Mock()
v1.0.4,sys.modules['scipy.ndimage.filters'] = Mock()
v1.0.4,sys.modules['tensorflow'] = Mock()
v1.0.4,sys.modules['theano'] = Mock()
v1.0.4,sys.modules['theano.tensor'] = Mock()
v1.0.4,sys.modules['torch'] = Mock()
v1.0.4,sys.modules['torch.optim'] = Mock()
v1.0.4,sys.modules['torch.nn'] = Mock()
v1.0.4,sys.modules['torch.nn.init'] = Mock()
v1.0.4,sys.modules['torch.autograd'] = Mock()
v1.0.4,sys.modules['sklearn'] = Mock()
v1.0.4,sys.modules['sklearn.model_selection'] = Mock()
v1.0.4,sys.modules['sklearn.utils'] = Mock()
v1.0.4,-- Project information -----------------------------------------------------
v1.0.4,"The full version, including alpha/beta/rc tags."
v1.0.4,The short X.Y version.
v1.0.4,-- General configuration ---------------------------------------------------
v1.0.4,"If your documentation needs a minimal Sphinx version, state it here."
v1.0.4,
v1.0.4,needs_sphinx = '1.0'
v1.0.4,"If true, the current module name will be prepended to all description"
v1.0.4,unit titles (such as .. function::).
v1.0.4,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.0.4,"Add any Sphinx extension module names here, as strings. They can be"
v1.0.4,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.0.4,ones.
v1.0.4,generate autosummary pages
v1.0.4,"Add any paths that contain templates here, relative to this directory."
v1.0.4,The suffix(es) of source filenames.
v1.0.4,You can specify multiple suffix as a list of string:
v1.0.4,
v1.0.4,"source_suffix = ['.rst', '.md']"
v1.0.4,The master toctree document.
v1.0.4,The language for content autogenerated by Sphinx. Refer to documentation
v1.0.4,for a list of supported languages.
v1.0.4,
v1.0.4,This is also used if you do content translation via gettext catalogs.
v1.0.4,"Usually you set ""language"" from the command line for these cases."
v1.0.4,"List of patterns, relative to source directory, that match files and"
v1.0.4,directories to ignore when looking for source files.
v1.0.4,This pattern also affects html_static_path and html_extra_path.
v1.0.4,The name of the Pygments (syntax highlighting) style to use.
v1.0.4,-- Options for HTML output -------------------------------------------------
v1.0.4,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.0.4,a list of builtin themes.
v1.0.4,
v1.0.4,Theme options are theme-specific and customize the look and feel of a theme
v1.0.4,"further.  For a list of options available for each theme, see the"
v1.0.4,documentation.
v1.0.4,
v1.0.4,html_theme_options = {}
v1.0.4,"Add any paths that contain custom static files (such as style sheets) here,"
v1.0.4,"relative to this directory. They are copied after the builtin static files,"
v1.0.4,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.0.4,html_static_path = ['_static']
v1.0.4,"Custom sidebar templates, must be a dictionary that maps document names"
v1.0.4,to template names.
v1.0.4,
v1.0.4,The default sidebars (for documents that don't match any pattern) are
v1.0.4,defined by theme itself.  Builtin themes are using these templates by
v1.0.4,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.0.4,'searchbox.html']``.
v1.0.4,
v1.0.4,html_sidebars = {}
v1.0.4,The name of an image file (relative to this directory) to place at the top
v1.0.4,of the sidebar.
v1.0.4,
v1.0.4,-- Options for HTMLHelp output ---------------------------------------------
v1.0.4,Output file base name for HTML help builder.
v1.0.4,-- Options for LaTeX output ------------------------------------------------
v1.0.4,The paper size ('letterpaper' or 'a4paper').
v1.0.4,
v1.0.4,"'papersize': 'letterpaper',"
v1.0.4,"The font size ('10pt', '11pt' or '12pt')."
v1.0.4,
v1.0.4,"'pointsize': '10pt',"
v1.0.4,Additional stuff for the LaTeX preamble.
v1.0.4,
v1.0.4,"'preamble': '',"
v1.0.4,Latex figure (float) alignment
v1.0.4,
v1.0.4,"'figure_align': 'htbp',"
v1.0.4,Grouping the document tree into LaTeX files. List of tuples
v1.0.4,"(source start file, target name, title,"
v1.0.4,"author, documentclass [howto, manual, or own class])."
v1.0.4,-- Options for manual page output ------------------------------------------
v1.0.4,One entry per manual page. List of tuples
v1.0.4,"(source start file, name, description, authors, manual section)."
v1.0.4,-- Options for Texinfo output ----------------------------------------------
v1.0.4,Grouping the document tree into Texinfo files. List of tuples
v1.0.4,"(source start file, target name, title, author,"
v1.0.4,"dir menu entry, description, category)"
v1.0.4,-- Options for Epub output -------------------------------------------------
v1.0.4,Bibliographic Dublin Core info.
v1.0.4,The unique identifier of the text. This can be a ISBN number
v1.0.4,or the project homepage.
v1.0.4,
v1.0.4,epub_identifier = ''
v1.0.4,A unique identification for the text.
v1.0.4,
v1.0.4,epub_uid = ''
v1.0.4,A list of files that should not be packed into the epub file.
v1.0.4,-- Extension configuration -------------------------------------------------
v1.0.4,-- Options for intersphinx extension ---------------------------------------
v1.0.4,Example configuration for intersphinx: refer to the Python standard library.
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Check a model param is optimized
v1.0.4,Check a loss param is optimized
v1.0.4,Check a model param is NOT optimized
v1.0.4,Check a loss param is optimized
v1.0.4,Check a model param is optimized
v1.0.4,Check a loss param is NOT optimized
v1.0.4,Check a model param is NOT optimized
v1.0.4,Check a loss param is NOT optimized
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,check for empty batches
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,The triples factory and model
v1.0.4,: The evaluator to be tested
v1.0.4,Settings
v1.0.4,: The evaluator instantiation
v1.0.4,Settings
v1.0.4,Initialize evaluator
v1.0.4,Use small test dataset
v1.0.4,Use small model (untrained)
v1.0.4,Get batch
v1.0.4,Compute scores
v1.0.4,Compute mask only if required
v1.0.4,TODO: Re-use filtering code
v1.0.4,"shape: (batch_size, num_triples)"
v1.0.4,"shape: (batch_size, num_entities)"
v1.0.4,Process one batch
v1.0.4,Check for correct class
v1.0.4,Check value ranges
v1.0.4,TODO: Validate with data?
v1.0.4,Check for correct class
v1.0.4,check value
v1.0.4,filtering
v1.0.4,"true_score: (2, 3, 3)"
v1.0.4,head based filter
v1.0.4,preprocessing for faster lookup
v1.0.4,check that all found positives are positive
v1.0.4,check in-place
v1.0.4,Test head scores
v1.0.4,Assert in-place modification
v1.0.4,Assert correct filtering
v1.0.4,Test tail scores
v1.0.4,Assert in-place modification
v1.0.4,Assert correct filtering
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The batch size
v1.0.4,: The triples factory
v1.0.4,: Class of regularizer to test
v1.0.4,: The constructor parameters to pass to the regularizer
v1.0.4,": The regularizer instance, initialized in setUp"
v1.0.4,: A positive batch
v1.0.4,: The device
v1.0.4,Use RESCAL as it regularizes multiple tensors of different shape.
v1.0.4,Check if regularizer is stored correctly.
v1.0.4,Forward pass (should update regularizer)
v1.0.4,Call post_parameter_update (should reset regularizer)
v1.0.4,Check if regularization term is reset
v1.0.4,Call method
v1.0.4,Generate random tensors
v1.0.4,Call update
v1.0.4,check shape
v1.0.4,compute expected term
v1.0.4,Generate random tensor
v1.0.4,calculate penalty
v1.0.4,check shape
v1.0.4,check value
v1.0.4,Tests that exception will be thrown when more than or less than three tensors are passed
v1.0.4,Test that regularization term is computed correctly
v1.0.4,Entity soft constraint
v1.0.4,Orthogonality soft constraint
v1.0.4,"After first update, should change the term"
v1.0.4,"After second update, no change should happen"
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The number of embeddings
v1.0.4,: The embedding dimension
v1.0.4,check shape
v1.0.4,check values
v1.0.4,check shape
v1.0.4,check values
v1.0.4,check correct value range
v1.0.4,check maximum norm constraint
v1.0.4,unchanged values for small norms
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,equal value; larger is better
v1.0.4,equal value; smaller is better
v1.0.4,larger is better; improvement
v1.0.4,larger is better; improvement; but not significant
v1.0.4,: The window size used by the early stopper
v1.0.4,: The mock losses the mock evaluator will return
v1.0.4,: The (zeroed) index  - 1 at which stopping will occur
v1.0.4,: The minimum improvement
v1.0.4,: The best results
v1.0.4,Set automatic_memory_optimization to false for tests
v1.0.4,Step early stopper
v1.0.4,check storing of results
v1.0.4,check ring buffer
v1.0.4,: The window size used by the early stopper
v1.0.4,: The (zeroed) index  - 1 at which stopping will occur
v1.0.4,: The minimum improvement
v1.0.4,: The random seed to use for reproducibility
v1.0.4,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.0.4,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.0.4,: The batch size to use.
v1.0.4,Fix seed for reproducibility
v1.0.4,Set automatic_memory_optimization to false during testing
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,check correct translation
v1.0.4,check column order
v1.0.4,apply restriction
v1.0.4,"check that the triples factory is returned as is, if and only if no restriction is to apply"
v1.0.4,check that inverse_triples is correctly carried over
v1.0.4,verify that the label-to-ID mapping has not been changed
v1.0.4,verify that triples have been filtered
v1.0.4,verify that all entities and relations are present in the training factory
v1.0.4,verify that no triple got lost
v1.0.4,verify that the label-to-id mappings match
v1.0.4,Check if multilabels are working correctly
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The batch size
v1.0.4,: The random seed
v1.0.4,: The triples factory
v1.0.4,: The sLCWA instances
v1.0.4,: Class of negative sampling to test
v1.0.4,": The negative sampler instance, initialized in setUp"
v1.0.4,: A positive batch
v1.0.4,Generate negative sample
v1.0.4,check shape
v1.0.4,check bounds: heads
v1.0.4,check bounds: relations
v1.0.4,check bounds: tails
v1.0.4,Check that all elements got corrupted
v1.0.4,Generate scaled negative sample
v1.0.4,Generate negative samples
v1.0.4,test that the relations were not changed
v1.0.4,Test that half of the subjects and half of the objects are corrupted
v1.0.4,Generate negative sample for additional tests
v1.0.4,test that the relations were not changed
v1.0.4,sample a batch
v1.0.4,check shape
v1.0.4,get triples
v1.0.4,check connected components
v1.0.4,super inefficient
v1.0.4,join
v1.0.4,already joined
v1.0.4,check that there is only a single component
v1.0.4,check content of comp_adj_lists
v1.0.4,check edge ids
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The expected number of entities
v1.0.4,: The expected number of relations
v1.0.4,: The dataset to test
v1.0.4,Not loaded
v1.0.4,Load
v1.0.4,Test caching
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The class of the model to test
v1.0.4,: Additional arguments passed to the model's constructor method
v1.0.4,: The triples factory instance
v1.0.4,: The model instance
v1.0.4,: The batch size for use for forward_* tests
v1.0.4,: The embedding dimensionality
v1.0.4,: Whether to create inverse triples (needed e.g. by ConvE)
v1.0.4,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.0.4,: The batch size for use when testing training procedures
v1.0.4,: The number of epochs to train the model
v1.0.4,: A random number generator from torch
v1.0.4,: The number of parameters which receive a constant (i.e. non-randomized)
v1.0.4,initialization
v1.0.4,assert there is at least one trainable parameter
v1.0.4,Check that all the parameters actually require a gradient
v1.0.4,Try to initialize an optimizer
v1.0.4,get model parameters
v1.0.4,re-initialize
v1.0.4,check that the operation works in-place
v1.0.4,check that the parameters where modified
v1.0.4,check for finite values by default
v1.0.4,check whether a gradient can be back-propgated
v1.0.4,"assert batch comprises (head, relation) pairs"
v1.0.4,"assert batch comprises (relation, tail) pairs"
v1.0.4,TODO: Catch HolE MKL error?
v1.0.4,set regularizer term
v1.0.4,call post_parameter_update
v1.0.4,assert that the regularization term has been reset
v1.0.4,do one optimization step
v1.0.4,call post_parameter_update
v1.0.4,check model constraints
v1.0.4,"assert batch comprises (relation, tail) pairs"
v1.0.4,"assert batch comprises (relation, tail) pairs"
v1.0.4,"assert batch comprises (relation, tail) pairs"
v1.0.4,Distance-based model
v1.0.4,3x batch norm: bias + scale --> 6
v1.0.4,entity specific bias        --> 1
v1.0.4,==================================
v1.0.4,7
v1.0.4,"two bias terms, one conv-filter"
v1.0.4,check type
v1.0.4,check shape
v1.0.4,check ID ranges
v1.0.4,this is only done in one of the models
v1.0.4,this is only done in one of the models
v1.0.4,Two linear layer biases
v1.0.4,"Two BN layers, bias & scale"
v1.0.4,: one bias per layer
v1.0.4,: (scale & bias for BN) * layers
v1.0.4,entity embeddings
v1.0.4,relation embeddings
v1.0.4,Compute Scores
v1.0.4,"self.assertAlmostEqual(second_score, -16, delta=0.01)"
v1.0.4,Use different dimension for relation embedding: relation_dim > entity_dim
v1.0.4,relation embeddings
v1.0.4,Compute Scores
v1.0.4,Use different dimension for relation embedding: relation_dim < entity_dim
v1.0.4,entity embeddings
v1.0.4,relation embeddings
v1.0.4,Compute Scores
v1.0.4,random entity embeddings & projections
v1.0.4,random relation embeddings & projections
v1.0.4,project
v1.0.4,check shape:
v1.0.4,check normalization
v1.0.4,entity embeddings
v1.0.4,relation embeddings
v1.0.4,Compute Scores
v1.0.4,second_score = scores[1].item()
v1.0.4,: 2xBN (bias & scale)
v1.0.4,check shape
v1.0.4,check content
v1.0.4,: The number of entities
v1.0.4,: The number of triples
v1.0.4,check shape
v1.0.4,check dtype
v1.0.4,check finite values (e.g. due to division by zero)
v1.0.4,check non-negativity
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,
v1.0.4,
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,check for finite values by default
v1.0.4,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.4,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.4,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.4,Get embeddings
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The class
v1.0.4,: Constructor keyword arguments
v1.0.4,: The loss instance
v1.0.4,: The batch size
v1.0.4,test reduction
v1.0.4,Test backward
v1.0.4,: The number of entities.
v1.0.4,: The number of negative samples
v1.0.4,≈ result of softmax
v1.0.4,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.0.4,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.0.4,"pos_distances = [0., 0., 0.5, 0.5]"
v1.0.4,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.0.4,≈ result of sigmoid
v1.0.4,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.0.4,expected_loss ≈ 0.34
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Create dummy dense labels
v1.0.4,Check if labels form a probability distribution
v1.0.4,Apply label smoothing
v1.0.4,Check if smooth labels form probability distribution
v1.0.4,Create dummy sLCWA labels
v1.0.4,Apply label smoothing
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A mapping of optimizers' names to their implementations
v1.0.4,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,"scale labels from [0, 1] to [-1, 1]"
v1.0.4,cross entropy expects a proper probability distribution -> normalize labels
v1.0.4,Use numerically stable variant to compute log(softmax)
v1.0.4,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.0.4,"To add *all* losses implemented in Torch, uncomment:"
v1.0.4,_LOSSES.update({
v1.0.4,loss
v1.0.4,for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()
v1.0.4,if not loss.__name__.startswith('_')
v1.0.4,})
v1.0.4,: A mapping of losses' names to their implementations
v1.0.4,: HPO Defaults for losses
v1.0.4,Add empty dictionaries as defaults for all remaining losses
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.0.4,Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.
v1.0.4,lower bound
v1.0.4,upper bound
v1.0.4,Allocate weight on device
v1.0.4,Initialize if initializer is provided
v1.0.4,Wrap embedding around it.
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The overall regularization weight
v1.0.4,: The current regularization term (a scalar)
v1.0.4,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.0.4,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.4,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.4,no need to compute anything
v1.0.4,always return zero
v1.0.4,: The dimension along which to compute the vector-based regularization terms.
v1.0.4,: Whether to normalize the regularization term by the dimension of the vectors.
v1.0.4,: This allows dimensionality-independent weight tuning.
v1.0.4,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.4,expected value of |x|_1 = d*E[x_i] for x_i i.i.d.
v1.0.4,expected value of |x|_2 when x_i are normally distributed
v1.0.4,cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1
v1.0.4,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.4,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.4,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.0.4,"Therefore, apply_only_once is always set to True."
v1.0.4,Entity soft constraint
v1.0.4,Orthogonality soft constraint
v1.0.4,The normalization factor to balance individual regularizers' contribution.
v1.0.4,: A mapping of regularizers' names to their implementations
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Add HPO command
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The random seed used at the beginning of the pipeline
v1.0.4,: The model trained by the pipeline
v1.0.4,: The training loop used by the pipeline
v1.0.4,: The losses during training
v1.0.4,: The results evaluated by the pipeline
v1.0.4,: How long in seconds did training take?
v1.0.4,: How long in seconds did evaluation take?
v1.0.4,: An early stopper
v1.0.4,: Any additional metadata as a dictionary
v1.0.4,: The version of PyKEEN used to create these results
v1.0.4,: The git hash of PyKEEN used to create these results
v1.0.4,1. Dataset
v1.0.4,2. Model
v1.0.4,3. Loss
v1.0.4,4. Regularizer
v1.0.4,5. Optimizer
v1.0.4,6. Training Loop
v1.0.4,7. Training (ronaldo style)
v1.0.4,8. Evaluation
v1.0.4,9. Tracking
v1.0.4,Misc
v1.0.4,Start tracking
v1.0.4,evaluation restriction to a subset of entities/relations
v1.0.4,FIXME this should never happen.
v1.0.4,Log model parameters
v1.0.4,Log optimizer parameters
v1.0.4,Stopping
v1.0.4,"Load the evaluation batch size for the stopper, if it has been set"
v1.0.4,By default there's a stopper that does nothing interesting
v1.0.4,Add logging for debugging
v1.0.4,Train like Cristiano Ronaldo
v1.0.4,Evaluate
v1.0.4,Reuse optimal evaluation parameters from training if available
v1.0.4,Add logging about evaluator for debugging
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A mapping of trackers' names to their implementations
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Create directory in which all experimental artifacts are saved
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: Functions for specifying exotic resources with a given prefix
v1.0.4,: Functions for specifying exotic resources based on their file extension
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.0.4,TODO vectorize code
v1.0.4,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.0.4,"FIXME is this ever possible, since this function is called in __init__?"
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Create lists out of sets for proper numpy indexing when loading the labels
v1.0.4,TODO is there a need to have a canonical sort order here?
v1.0.4,Split triples
v1.0.4,Sorting ensures consistent results when the triples are permuted
v1.0.4,Create mapping
v1.0.4,Sorting ensures consistent results when the triples are permuted
v1.0.4,Create mapping
v1.0.4,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.0.4,Filter all non-existent triples
v1.0.4,Note: Unique changes the order of the triples
v1.0.4,Note: Using unique means implicit balancing of training samples
v1.0.4,: The mapping from entities' labels to their indexes
v1.0.4,: The mapping from relations' labels to their indexes
v1.0.4,": A three-column matrix where each row are the head label,"
v1.0.4,": relation label, then tail label"
v1.0.4,": A three-column matrix where each row are the head identifier,"
v1.0.4,": relation identifier, then tail identifier"
v1.0.4,": A dictionary mapping each relation to its inverse, if inverse triples were created"
v1.0.4,TODO: Check if lazy evaluation would make sense
v1.0.4,Check if the triples are inverted already
v1.0.4,extend original triples with inverse ones
v1.0.4,Generate entity mapping if necessary
v1.0.4,Generate relation mapping if necessary
v1.0.4,Map triples of labels to triples of IDs.
v1.0.4,We can terminate the search after finding the first inverse occurrence
v1.0.4,Ensure 2d array in case only one triple was given
v1.0.4,FIXME this function is only ever used in tests
v1.0.4,Prepare shuffle index
v1.0.4,Prepare split index
v1.0.4,Take cumulative sum so the get separated properly
v1.0.4,Split triples
v1.0.4,Make sure that the first element has all the right stuff in it
v1.0.4,Make new triples factories for each group
v1.0.4,Input validation
v1.0.4,convert to numpy
v1.0.4,vectorized label lookup
v1.0.4,Additional columns
v1.0.4,convert PyTorch tensors to numpy
v1.0.4,convert to dataframe
v1.0.4,Re-order columns
v1.0.4,Filter for entities
v1.0.4,Filter for relations
v1.0.4,No filtering happened
v1.0.4,manually copy the inverse relation mappings
v1.0.4,While there are still triples that should be moved to the training set
v1.0.4,Pick a random triple to move over to the training triples
v1.0.4,Recalculate the testing triples without that index
v1.0.4,"Recalculate the training entities, testing entities, to_move, and move_id_mask"
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A PyTorch tensor of triples
v1.0.4,: A mapping from relation labels to integer identifiers
v1.0.4,: A mapping from relation labels to integer identifiers
v1.0.4,Create dense target
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,basically take all candidates
v1.0.4,Calculate which relations are the inverse ones
v1.0.4,FIXME doesn't carry flag of create_inverse_triples through
v1.0.4,A dictionary of all of the head/tail pairs for a given relation
v1.0.4,A dictionary for all of the tail/head pairs for a given relation
v1.0.4,Calculate the similarity between each relationship (entries in ``forward``)
v1.0.4,with all other candidate inverse relationships (entries in ``inverse``)
v1.0.4,"Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the"
v1.0.4,"same as for ``(b, a)``"
v1.0.4,A dictionary of all of the head/tail pairs for a given relation
v1.0.4,Filter out results between a given relationship and itself
v1.0.4,Filter out results below a minimum frequency
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,preprocessing
v1.0.4,initialize
v1.0.4,sample iteratively
v1.0.4,determine weights
v1.0.4,only happens at first iteration
v1.0.4,normalize to probabilities
v1.0.4,sample a start node
v1.0.4,get list of neighbors
v1.0.4,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.0.4,visit target node
v1.0.4,decrease sample counts
v1.0.4,return chosen edges
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Create training instances
v1.0.4,During size probing the training instances should not show the tqdm progress bar
v1.0.4,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.0.4,Ensure the release of memory
v1.0.4,Clear optimizer
v1.0.4,"Take the biggest possible training batch_size, if batch_size not set"
v1.0.4,This will find necessary parameters to optimize the use of the hardware at hand
v1.0.4,return the relevant parameters slice_size and batch_size
v1.0.4,Create dummy result tracker
v1.0.4,Sanity check
v1.0.4,Force weight initialization if training continuation is not explicitly requested.
v1.0.4,Reset the weights
v1.0.4,Create new optimizer
v1.0.4,Ensure the model is on the correct device
v1.0.4,Create Sampler
v1.0.4,Bind
v1.0.4,"When size probing, we don't want progress bars"
v1.0.4,Create progress bar
v1.0.4,Training Loop
v1.0.4,Enforce training mode
v1.0.4,Accumulate loss over epoch
v1.0.4,Batching
v1.0.4,Only create a progress bar when not in size probing mode
v1.0.4,Flag to check when to quit the size probing
v1.0.4,Recall that torch *accumulates* gradients. Before passing in a
v1.0.4,"new instance, you need to zero out the gradients from the old instance"
v1.0.4,Get batch size of current batch (last batch may be incomplete)
v1.0.4,accumulate gradients for whole batch
v1.0.4,forward pass call
v1.0.4,"when called by batch_size_search(), the parameter update should not be applied."
v1.0.4,update parameters according to optimizer
v1.0.4,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.0.4,constraints are no longer applied
v1.0.4,For testing purposes we're only interested in processing one batch
v1.0.4,When size probing we don't need the losses
v1.0.4,Track epoch loss
v1.0.4,Print loss information to console
v1.0.4,forward pass
v1.0.4,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.0.4,correction for loss reduction
v1.0.4,backward pass
v1.0.4,reset the regularizer to free the computational graph
v1.0.4,Set upper bound
v1.0.4,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.0.4,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.4,The regularizer has to be reset to free the computational graph
v1.0.4,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Shuffle each epoch
v1.0.4,Lazy-splitting into batches
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Slicing is not possible in sLCWA training loops
v1.0.4,Send positive batch to device
v1.0.4,Create negative samples
v1.0.4,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.0.4,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.0.4,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.0.4,Compute negative and positive scores
v1.0.4,Repeat positives scores (necessary for more than one negative per positive)
v1.0.4,Stack predictions
v1.0.4,Create target
v1.0.4,Normalize the loss to have the average loss per positive triple
v1.0.4,This allows comparability of sLCWA and LCWA losses
v1.0.4,Slicing is not possible for sLCWA
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A mapping of training loops' names to their implementations
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Split batch components
v1.0.4,Send batch to device
v1.0.4,Apply label smoothing
v1.0.4,This shows how often one row has to be repeated
v1.0.4,Create boolean indices for negative labels in the repeated rows
v1.0.4,Repeat the predictions and filter for negative labels
v1.0.4,This tells us how often each true label should be repeated
v1.0.4,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.0.4,Split positive and negative scores
v1.0.4,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.4,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,now: smaller is better
v1.0.4,: The model
v1.0.4,: The evaluator
v1.0.4,: The triples to use for evaluation
v1.0.4,: Size of the evaluation batches
v1.0.4,: Slice size of the evaluation batches
v1.0.4,: The number of epochs after which the model is evaluated on validation set
v1.0.4,: The number of iterations (one iteration can correspond to various epochs)
v1.0.4,: with no improvement after which training will be stopped.
v1.0.4,: The name of the metric to use
v1.0.4,: The minimum relative improvement necessary to consider it an improved result
v1.0.4,: The best result so far
v1.0.4,: The epoch at which the best result occurred
v1.0.4,: The remaining patience
v1.0.4,: The metric results from all evaluations
v1.0.4,": Whether a larger value is better, or a smaller"
v1.0.4,: The result tracker
v1.0.4,: Callbacks when after results are calculated
v1.0.4,: Callbacks when training gets continued
v1.0.4,: Callbacks when training is stopped early
v1.0.4,: Did the stopper ever decide to stop?
v1.0.4,TODO: Fix this
v1.0.4,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.0.4,raise ValueError(f'Invalid metric name: {self.metric}')
v1.0.4,Dummy result tracker
v1.0.4,Evaluate
v1.0.4,Only perform time consuming checks for the first call.
v1.0.4,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.0.4,Append to history
v1.0.4,check for improvement
v1.0.4,Stop if the result did not improve more than delta for patience evaluations
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A mapping of stoppers' names to their implementations
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.0.4,Clear the ranks from the current evaluator
v1.0.4,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.0.4,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.4,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.4,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.4,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.0.4,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.4,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.4,Test if slicing is implemented for the required functions of this model
v1.0.4,Split batch
v1.0.4,Bind shape
v1.0.4,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.0.4,Warn if all entities will be filtered
v1.0.4,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.0.4,verify that the triples have been filtered
v1.0.4,Send to device
v1.0.4,Ensure evaluation mode
v1.0.4,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.0.4,Check whether we need to be prepared for filtering
v1.0.4,Check whether an evaluator needs access to the masks
v1.0.4,This can only be an unfiltered evaluator.
v1.0.4,Prepare for result filtering
v1.0.4,Send tensors to device
v1.0.4,Prepare batches
v1.0.4,Show progressbar
v1.0.4,Flag to check when to quit the size probing
v1.0.4,Disable gradient tracking
v1.0.4,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.0.4,batch-wise processing
v1.0.4,If we only probe sizes we do not need more than one batch
v1.0.4,Finalize
v1.0.4,Predict scores once
v1.0.4,Select scores of true
v1.0.4,Create positive filter for all corrupted
v1.0.4,Needs all positive triples
v1.0.4,Create filter
v1.0.4,Create a positive mask with the size of the scores from the positive filter
v1.0.4,Restrict to entities of interest
v1.0.4,Evaluate metrics on these *unfiltered* scores
v1.0.4,Filter
v1.0.4,The scores for the true triples have to be rewritten to the scores tensor
v1.0.4,Restrict to entities of interest
v1.0.4,Evaluate metrics on these *filtered* scores
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The area under the ROC curve
v1.0.4,: The area under the precision-recall curve
v1.0.4,: The coverage error
v1.0.4,coverage_error: float = field(metadata=dict(
v1.0.4,"doc='The coverage error',"
v1.0.4,"f=metrics.coverage_error,"
v1.0.4,))
v1.0.4,: The label ranking loss (APS)
v1.0.4,label_ranking_average_precision_score: float = field(metadata=dict(
v1.0.4,"doc='The label ranking loss (APS)',"
v1.0.4,"f=metrics.label_ranking_average_precision_score,"
v1.0.4,))
v1.0.4,#: The label ranking loss
v1.0.4,label_ranking_loss: float = field(metadata=dict(
v1.0.4,"doc='The label ranking loss',"
v1.0.4,"f=metrics.label_ranking_loss,"
v1.0.4,))
v1.0.4,Transfer to cpu and convert to numpy
v1.0.4,Ensure that each key gets counted only once
v1.0.4,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.0.4,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.0.4,masks using the exact same key order.
v1.0.4,TODO how to define a cutoff on y_scores to make binary?
v1.0.4,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.0.4,Clear buffers
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A mapping of evaluators' names to their implementations
v1.0.4,: A mapping of results' names to their implementations
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,The best rank is the rank when assuming all options with an equal score are placed behind the currently
v1.0.4,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.0.4,The worst rank is the rank when assuming all options with an equal score are placed in front of the currently
v1.0.4,"considered. Hence, the rank is the number of options which have at least the same score minus one (as the"
v1.0.4,"currently considered option in included in all options). As the rank is one-based, we have to add 1, which"
v1.0.4,"nullifies the ""minus 1"" from before."
v1.0.4,"The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of"
v1.0.4,the elements with the same score as the currently considered option.
v1.0.4,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.0.4,The expected rank of a random scoring
v1.0.4,The adjusted ranks is normalized by the expected rank of a random scoring
v1.0.4,TODO adjusted_worst_rank
v1.0.4,TODO adjusted_best_rank
v1.0.4,: The mean over all ranks: mean_i r_i. Lower is better.
v1.0.4,: The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.
v1.0.4,": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k."
v1.0.4,: Higher is better.
v1.0.4,: The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.
v1.0.4,: Described by [berrendorf2020]_.
v1.0.4,Check if it a side or rank type
v1.0.4,Clear buffers
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.0.4,Create a tensor of all IDs
v1.0.4,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.0.4,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.0.4,: A dictionary of hyper-parameters to the models that use them
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The default loss function class
v1.0.4,: The default parameters for the default loss function class
v1.0.4,: The instance of the loss
v1.0.4,: The default regularizer class
v1.0.4,: The default parameters for the default regularizer class
v1.0.4,: The instance of the regularizer
v1.0.4,Initialize the device
v1.0.4,Random seeds have to set before the embeddings are initialized
v1.0.4,Loss
v1.0.4,TODO: Check loss functions that require 1 and -1 as label but only
v1.0.4,Regularizer
v1.0.4,The triples factory facilitates access to the dataset.
v1.0.4,This allows to store the optimized parameters
v1.0.4,Keep track of the hyper-parameters that are used across all
v1.0.4,subclasses of BaseModule
v1.0.4,Enforce evaluation mode
v1.0.4,Enforce evaluation mode
v1.0.4,Enforce evaluation mode
v1.0.4,Enforce evaluation mode
v1.0.4,The number of relations stored in the triples factory includes the number of inverse relations
v1.0.4,Id of inverse relation: relation + 1
v1.0.4,"The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs"
v1.0.4,initialize buffer on cpu
v1.0.4,calculate batch scores
v1.0.4,Explicitly create triples
v1.0.4,Sort final result
v1.0.4,Train a model (quickly)
v1.0.4,Get scores for *all* triples
v1.0.4,Get scores for top 15 triples
v1.0.4,set model to evaluation mode
v1.0.4,Do not track gradients
v1.0.4,initialize buffer on device
v1.0.4,calculate batch scores
v1.0.4,get top scores within batch
v1.0.4,append to global top scores
v1.0.4,reduce size if necessary
v1.0.4,Sort final result
v1.0.4,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.0.4,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.4,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.0.4,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.0.4,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.4,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.0.4,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.0.4,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.4,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.0.4,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.0.4,Default for relation dimensionality
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A mapping of models' names to their implementations
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Store initial input for error message
v1.0.4,All are None
v1.0.4,"input_channels is None, and any of height or width is None -> set input_channels=1"
v1.0.4,"input channels is not None, and one of height or width is None"
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The default loss function class
v1.0.4,: The default parameters for the default loss function class
v1.0.4,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.0.4,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.0.4,ConvE should be trained with inverse triples
v1.0.4,ConvE uses one bias for each entity
v1.0.4,Automatic calculation of remaining dimensions
v1.0.4,Parameter need to fulfil:
v1.0.4,input_channels * embedding_height * embedding_width = embedding_dim
v1.0.4,Finalize initialization
v1.0.4,embeddings
v1.0.4,weights
v1.0.4,"batch_size, num_input_channels, 2*height, width"
v1.0.4,"batch_size, num_input_channels, 2*height, width"
v1.0.4,"batch_size, num_input_channels, 2*height, width"
v1.0.4,"(N,C_out,H_out,W_out)"
v1.0.4,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.0.4,Embedding Regularization
v1.0.4,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.0.4,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.4,Embedding Regularization
v1.0.4,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.4,Embedding Regularization
v1.0.4,Code to repeat each item successively instead of the entire tensor
v1.0.4,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,Embeddings
v1.0.4,Finalize initialization
v1.0.4,Initialise left relation embeddings to unit length
v1.0.4,Make sure to call super first
v1.0.4,Normalise embeddings of entities
v1.0.4,Get embeddings
v1.0.4,Project entities
v1.0.4,Get embeddings
v1.0.4,Project entities
v1.0.4,Project entities
v1.0.4,Project entities
v1.0.4,Get embeddings
v1.0.4,Project entities
v1.0.4,Project entities
v1.0.4,Project entities
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The default loss function class
v1.0.4,: The default parameters for the default loss function class
v1.0.4,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.0.4,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.0.4,Finalize initialization
v1.0.4,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.0.4,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.0.4,split into real and imaginary part
v1.0.4,ComplEx space bilinear product
v1.0.4,*: Elementwise multiplication
v1.0.4,get embeddings
v1.0.4,Compute scores
v1.0.4,Regularization
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The regularizer used by [nickel2011]_ for for RESCAL
v1.0.4,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.0.4,: a normalized weight of 10 is used.
v1.0.4,: The LP settings used by [nickel2011]_ for for RESCAL
v1.0.4,Finalize initialization
v1.0.4,Get embeddings
v1.0.4,"shape: (b, d)"
v1.0.4,"shape: (b, d, d)"
v1.0.4,"shape: (b, d)"
v1.0.4,Compute scores
v1.0.4,Regularization
v1.0.4,Compute scores
v1.0.4,Regularization
v1.0.4,Get embeddings
v1.0.4,Compute scores
v1.0.4,Regularization
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,Finalize initialization
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.0.4,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.0.4,The interaction model
v1.0.4,Finalize initialization
v1.0.4,embeddings
v1.0.4,Use Xavier initialization for weight; bias to zero
v1.0.4,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.0.4,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.0.4,Output layer regularization
v1.0.4,In the code base only the weights of the output layer are used for regularization
v1.0.4,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.0.4,Stack to convolution input
v1.0.4,Convolution
v1.0.4,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.0.4,Linear layer for final scores
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,Finalize initialization
v1.0.4,": shape: (batch_size, num_entities, d)"
v1.0.4,": Prepare h: (b, e, d) -> (b, e, 1, 1, d)"
v1.0.4,": Prepare t: (b, e, d) -> (b, e, 1, d, 1)"
v1.0.4,": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)"
v1.0.4,"h.T @ W @ t, shape: (b, e, k, 1, 1)"
v1.0.4,": reduce (b, e, k, 1, 1) -> (b, e, k)"
v1.0.4,": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.4,": Prepare h: (b, e, d) -> (b, e, d, 1)"
v1.0.4,"V_h @ h, shape: (b, e, k, 1)"
v1.0.4,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.4,": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.4,": Prepare t: (b, e, d) -> (b, e, d, 1)"
v1.0.4,"V_t @ t, shape: (b, e, k, 1)"
v1.0.4,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.4,": Prepare b: (R, k) -> (b, k) -> (b, 1, k)"
v1.0.4,"a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)"
v1.0.4,"prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)"
v1.0.4,"prepare act: (b, e, k) -> (b, e, 1, k)"
v1.0.4,"compute score, shape: (b, e, 1, 1)"
v1.0.4,reduce
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The regularizer used by [yang2014]_ for DistMult
v1.0.4,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.0.4,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.0.4,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.0.4,: The LP settings used by [yang2014]_ for DistMult
v1.0.4,Finalize initialization
v1.0.4,"xavier uniform, cf."
v1.0.4,https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17
v1.0.4,Initialise relation embeddings to unit length
v1.0.4,Make sure to call super first
v1.0.4,Normalize embeddings of entities
v1.0.4,Bilinear product
v1.0.4,*: Elementwise multiplication
v1.0.4,Get embeddings
v1.0.4,Compute score
v1.0.4,Only regularize relation embeddings
v1.0.4,Get embeddings
v1.0.4,Rank against all entities
v1.0.4,Only regularize relation embeddings
v1.0.4,Get embeddings
v1.0.4,Rank against all entities
v1.0.4,Only regularize relation embeddings
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,Similarity function used for distributions
v1.0.4,element-wise covariance bounds
v1.0.4,Additional covariance embeddings
v1.0.4,Finalize initialization
v1.0.4,Constraints are applied through post_parameter_update
v1.0.4,Make sure to call super first
v1.0.4,Normalize entity embeddings
v1.0.4,Ensure positive definite covariances matrices and appropriate size by clamping
v1.0.4,Get embeddings
v1.0.4,Compute entity distribution
v1.0.4,: a = \mu^T\Sigma^{-1}\mu
v1.0.4,: b = \log \det \Sigma
v1.0.4,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.0.4,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.0.4,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.0.4,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The custom regularizer used by [wang2014]_ for TransH
v1.0.4,: The settings used by [wang2014]_ for TransH
v1.0.4,embeddings
v1.0.4,Finalize initialization
v1.0.4,TODO: Add initialization
v1.0.4,Make sure to call super first
v1.0.4,Normalise the normal vectors by their l2 norms
v1.0.4,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.0.4,which enforces the defined soft constraints.
v1.0.4,Get embeddings
v1.0.4,Project to hyperplane
v1.0.4,Regularization term
v1.0.4,Get embeddings
v1.0.4,Project to hyperplane
v1.0.4,Regularization term
v1.0.4,Get embeddings
v1.0.4,Project to hyperplane
v1.0.4,Regularization term
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,embeddings
v1.0.4,Finalize initialization
v1.0.4,Make sure to call super first
v1.0.4,Normalize entity embeddings
v1.0.4,TODO: Initialize from TransE
v1.0.4,Initialise relation embeddings to unit length
v1.0.4,"project to relation specific subspace, shape: (b, e, d_r)"
v1.0.4,ensure constraints
v1.0.4,"evaluate score function, shape: (b, e)"
v1.0.4,Get embeddings
v1.0.4,Get embeddings
v1.0.4,Get embeddings
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,Construct node neighbourhood mask
v1.0.4,Set nodes in batch to true
v1.0.4,Compute k-neighbourhood
v1.0.4,"if the target node needs an embeddings, so does the source node"
v1.0.4,Create edge mask
v1.0.4,pylint: disable=unused-argument
v1.0.4,"Calculate in-degree, i.e. number of incoming edges"
v1.0.4,pylint: disable=unused-argument
v1.0.4,"Calculate in-degree, i.e. number of incoming edges"
v1.0.4,: Interaction model used as decoder
v1.0.4,: The blocks of the relation-specific weight matrices
v1.0.4,": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)"
v1.0.4,: The base weight matrices to generate relation-specific weights
v1.0.4,": shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.4,: The relation-specific weights for each base
v1.0.4,": shape: (num_relations, num_bases)"
v1.0.4,: The biases for each layer (if used)
v1.0.4,": shape of each element: (embedding_dim,)"
v1.0.4,: Batch normalization for each layer (if used)
v1.0.4,: Activations for each layer (if used)
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,Instantiate model
v1.0.4,Heuristic
v1.0.4,buffering of messages
v1.0.4,"Save graph using buffers, such that the tensors are moved together with the model"
v1.0.4,Weights
v1.0.4,Finalize initialization
v1.0.4,invalidate enriched embeddings
v1.0.4,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.0.4,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.0.4,initialized properly)
v1.0.4,We have one additional relation for self-loops
v1.0.4,Xavier Glorot initialization of each block
v1.0.4,Reset biases
v1.0.4,Reset batch norm parameters
v1.0.4,"Reset activation parameters, if any"
v1.0.4,use buffered messages if applicable
v1.0.4,Bind fields
v1.0.4,"shape: (num_entities, embedding_dim)"
v1.0.4,Edge dropout: drop the same edges on all layers (only in training mode)
v1.0.4,Get random dropout mask
v1.0.4,Apply to edges
v1.0.4,Different dropout for self-loops (only in training mode)
v1.0.4,"If batch is given, compute (num_layers)-hop neighbourhood"
v1.0.4,Initialize embeddings in the next layer for all nodes
v1.0.4,TODO: Can we vectorize this loop?
v1.0.4,Choose the edges which are of the specific relation
v1.0.4,Only propagate messages on subset of edges
v1.0.4,No edges available? Skip rest of inner loop
v1.0.4,Get source and target node indices
v1.0.4,send messages in both directions
v1.0.4,Select source node embeddings
v1.0.4,get relation weights
v1.0.4,Compute message (b x d) * (d x d) = (b x d)
v1.0.4,Normalize messages by relation-specific in-degree
v1.0.4,Aggregate messages in target
v1.0.4,Self-loop
v1.0.4,"Apply bias, if requested"
v1.0.4,"Apply batch normalization, if requested"
v1.0.4,Apply non-linearity
v1.0.4,allocate weight
v1.0.4,Get blocks
v1.0.4,"self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)"
v1.0.4,note: embedding_dim is guaranteed to be divisible by num_bases in the constructor
v1.0.4,"The current basis weights, shape: (num_bases)"
v1.0.4,"the current bases, shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.4,"compute the current relation weights, shape: (embedding_dim, embedding_dim)"
v1.0.4,Enrich embeddings
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The default loss function class
v1.0.4,: The default parameters for the default loss function class
v1.0.4,Core tensor
v1.0.4,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.0.4,Dropout
v1.0.4,Finalize initialization
v1.0.4,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.0.4,Abbreviation
v1.0.4,Compute h_n = DO(BN(h))
v1.0.4,Compute wr = DO(W x_2 r)
v1.0.4,compute whr = DO(BN(h_n x_1 wr))
v1.0.4,Compute whr x_3 t
v1.0.4,Get embeddings
v1.0.4,Compute scores
v1.0.4,Get embeddings
v1.0.4,Compute scores
v1.0.4,Get embeddings
v1.0.4,Compute scores
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,Finalize initialization
v1.0.4,Initialise relation embeddings to unit length
v1.0.4,Make sure to call super first
v1.0.4,Normalize entity embeddings
v1.0.4,Get embeddings
v1.0.4,Get embeddings
v1.0.4,Get embeddings
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The default loss function class
v1.0.4,: The default parameters for the default loss function class
v1.0.4,: The regularizer used by [trouillon2016]_ for SimplE
v1.0.4,": In the paper, they use weight of 0.1, and do not normalize the"
v1.0.4,": regularization term by the number of elements, which is 200."
v1.0.4,: The power sum settings used by [trouillon2016]_ for SimplE
v1.0.4,extra embeddings
v1.0.4,Finalize initialization
v1.0.4,forward model
v1.0.4,Regularization
v1.0.4,backward model
v1.0.4,Regularization
v1.0.4,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.0.4,"That is not mentioned in the paper, so it is omitted here."
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,Finalize initialization
v1.0.4,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.0.4,weight initialization
v1.0.4,Get embeddings
v1.0.4,Embedding Regularization
v1.0.4,Concatenate them
v1.0.4,Compute scores
v1.0.4,Get embeddings
v1.0.4,Embedding Regularization
v1.0.4,First layer can be unrolled
v1.0.4,Send scores through rest of the network
v1.0.4,Get embeddings
v1.0.4,Embedding Regularization
v1.0.4,First layer can be unrolled
v1.0.4,Send scores through rest of the network
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,The dimensions affected by e'
v1.0.4,Project entities
v1.0.4,r_p (e_p.T e) + e'
v1.0.4,Enforce constraints
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,Finalize initialization
v1.0.4,Make sure to call super first
v1.0.4,Normalize entity embeddings
v1.0.4,Project entities
v1.0.4,score = -||h_bot + r - t_bot||_2^2
v1.0.4,Head
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,Finalize initialization
v1.0.4,phases randomly between 0 and 2 pi
v1.0.4,Make sure to call super first
v1.0.4,Normalize relation embeddings
v1.0.4,Decompose into real and imaginary part
v1.0.4,Rotate (=Hadamard product in complex space).
v1.0.4,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.0.4,Get embeddings
v1.0.4,Compute scores
v1.0.4,Embedding Regularization
v1.0.4,Get embeddings
v1.0.4,Rank against all entities
v1.0.4,Compute scores
v1.0.4,Embedding Regularization
v1.0.4,Get embeddings
v1.0.4,r expresses a rotation in complex plane.
v1.0.4,The inverse rotation is expressed by the complex conjugate of r.
v1.0.4,The score is computed as the distance of the relation-rotated head to the tail.
v1.0.4,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.0.4,|h * r - t| = |h - conj(r) * t|
v1.0.4,Rank against all entities
v1.0.4,Compute scores
v1.0.4,Embedding Regularization
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The default loss function class
v1.0.4,: The default parameters for the default loss function class
v1.0.4,Global entity projection
v1.0.4,Global relation projection
v1.0.4,Global combination bias
v1.0.4,Global combination bias
v1.0.4,Finalize initialization
v1.0.4,Get embeddings
v1.0.4,Compute score
v1.0.4,Get embeddings
v1.0.4,Rank against all entities
v1.0.4,Get embeddings
v1.0.4,Rank against all entities
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,Finalize initialization
v1.0.4,Make sure to call super first
v1.0.4,Normalize entity embeddings
v1.0.4,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.0.4,Circular correlation of entity embeddings
v1.0.4,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.0.4,Hadamard product in frequency domain
v1.0.4,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.0.4,inner product with relation embedding
v1.0.4,Embedding Regularization
v1.0.4,Embedding Regularization
v1.0.4,Embedding Regularization
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The default loss function class
v1.0.4,: The default parameters for the default loss function class
v1.0.4,Finalize initialization
v1.0.4,Get embeddings
v1.0.4,Embedding Regularization
v1.0.4,Concatenate them
v1.0.4,Predict t embedding
v1.0.4,compare with all t's
v1.0.4,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.4,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.4,Embedding Regularization
v1.0.4,Concatenate them
v1.0.4,Predict t embedding
v1.0.4,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.4,Embedding Regularization
v1.0.4,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.0.4,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.0.4,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.0.4,Extend t
v1.0.4,Concatenate them
v1.0.4,Predict t embedding
v1.0.4,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.4,The results have to be realigned with the expected output of the score_h function
v1.0.4,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,TODO: Check entire build of the model
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The default loss function class
v1.0.4,: The default parameters for the default loss function class
v1.0.4,Literal
v1.0.4,num_ent x num_lit
v1.0.4,Number of columns corresponds to number of literals
v1.0.4,Literals
v1.0.4,End literals
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,TODO: Check entire build of the model
v1.0.4,: The default strategy for optimizing the model's hyper-parameters
v1.0.4,: The default parameters for the default loss function class
v1.0.4,Embeddings
v1.0.4,Number of columns corresponds to number of literals
v1.0.4,apply dropout
v1.0.4,"-, because lower score shall correspond to a more plausible triple."
v1.0.4,TODO check if this is the same as the BaseModule
v1.0.4,Choose y = -1 since a smaller score is better.
v1.0.4,"In TransE for example, the scores represent distances"
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.4,Bind number of negatives to sample
v1.0.4,Equally corrupt head and tail
v1.0.4,Copy positive batch for corruption.
v1.0.4,"Do not detach, as no gradients should flow into the indices."
v1.0.4,Sample random entities as replacement
v1.0.4,Replace heads – To make sure we don't replace the head by the original value
v1.0.4,we shift all values greater or equal than the original value by one up
v1.0.4,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.4,Corrupt tails
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A mapping of negative samplers' names to their implementations
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.4,Preprocessing: Compute corruption probabilities
v1.0.4,"compute tph, i.e. the average number of tail entities per head"
v1.0.4,"compute hpt, i.e. the average number of head entities per tail"
v1.0.4,Set parameter for Bernoulli distribution
v1.0.4,Bind number of negatives to sample
v1.0.4,Copy positive batch for corruption.
v1.0.4,"Do not detach, as no gradients should flow into the indices."
v1.0.4,Decide whether to corrupt head or tail
v1.0.4,Tails are corrupted if heads are not corrupted
v1.0.4,Randomly sample corruption
v1.0.4,Replace heads – To make sure we don't replace the head by the original value
v1.0.4,we shift all values greater or equal than the original value by one up
v1.0.4,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.4,Replace tails
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,TODO what happens if already exists?
v1.0.4,TODO incorporate setting of random seed
v1.0.4,pipeline_kwargs=dict(
v1.0.4,"random_seed=random.randint(1, 2 ** 32 - 1),"
v1.0.4,"),"
v1.0.4,Add dataset to current_pipeline
v1.0.4,Add loss function to current_pipeline
v1.0.4,Add regularizer to current_pipeline
v1.0.4,Add optimizer to current_pipeline
v1.0.4,Add training approach to current_pipeline
v1.0.4,Add training kwargs and kwargs_ranges
v1.0.4,Add evaluation
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A factory wrapping the training triples
v1.0.4,": A factory wrapping the testing triples, that share indexes with the training triples"
v1.0.4,": A factory wrapping the validation triples, that share indexes with the training triples"
v1.0.4,: All data sets should take care of inverse triple creation
v1.0.4,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.0.4,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.0.4,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.0.4,don't call this function by itself. assumes called through the `validation`
v1.0.4,property and the _training factory has already been loaded
v1.0.4,see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content
v1.0.4,pattern from https://stackoverflow.com/a/39217788/5775947
v1.0.4,TODO replace this with the new zip remote dataset class
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A mapping of datasets' names to their classes
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,TODO update docs with table and CLI wtih generator
v1.0.4,: A mapping of HPO samplers' names to their implementations
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,1. Dataset
v1.0.4,2. Model
v1.0.4,3. Loss
v1.0.4,4. Regularizer
v1.0.4,5. Optimizer
v1.0.4,6. Training Loop
v1.0.4,7. Training
v1.0.4,8. Evaluation
v1.0.4,9. Trackers
v1.0.4,Misc.
v1.0.4,2. Model
v1.0.4,3. Loss
v1.0.4,4. Regularizer
v1.0.4,5. Optimizer
v1.0.4,1. Dataset
v1.0.4,2. Model
v1.0.4,3. Loss
v1.0.4,4. Regularizer
v1.0.4,5. Optimizer
v1.0.4,6. Training Loop
v1.0.4,7. Training
v1.0.4,8. Evaluation
v1.0.4,9. Tracker
v1.0.4,Misc.
v1.0.4,Will trigger Optuna to set the state of the trial as failed
v1.0.4,: The :mod:`optuna` study object
v1.0.4,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.0.4,Output study information
v1.0.4,Output all trials
v1.0.4,Output best trial as pipeline configuration file
v1.0.4,1. Dataset
v1.0.4,2. Model
v1.0.4,3. Loss
v1.0.4,4. Regularizer
v1.0.4,5. Optimizer
v1.0.4,6. Training Loop
v1.0.4,7. Training
v1.0.4,8. Evaluation
v1.0.4,9. Tracking
v1.0.4,6. Misc
v1.0.4,Optuna Study Settings
v1.0.4,Optuna Optimization Settings
v1.0.4,0. Metadata/Provenance
v1.0.4,1. Dataset
v1.0.4,FIXME difference between dataset class and string
v1.0.4,FIXME how to handle if dataset or factories were set? Should have been
v1.0.4,part of https://github.com/mali-git/POEM_develop/pull/483
v1.0.4,2. Model
v1.0.4,3. Loss
v1.0.4,4. Regularizer
v1.0.4,5. Optimizer
v1.0.4,6. Training Loop
v1.0.4,7. Training
v1.0.4,8. Evaluation
v1.0.4,9. Tracking
v1.0.4,1. Dataset
v1.0.4,2. Model
v1.0.4,3. Loss
v1.0.4,4. Regularizer
v1.0.4,5. Optimizer
v1.0.4,6. Training Loop
v1.0.4,7. Training
v1.0.4,8. Evaluation
v1.0.4,9. Tracker
v1.0.4,Optuna Misc.
v1.0.4,Pipeline Misc.
v1.0.4,Invoke optimization of the objective function.
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,-*- coding: utf-8 -*-
v1.0.4,: A mapping of HPO pruners' names to their implementations
v1.0.4,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,
v1.0.3,Configuration file for the Sphinx documentation builder.
v1.0.3,
v1.0.3,This file does only contain a selection of the most common options. For a
v1.0.3,full list see the documentation:
v1.0.3,http://www.sphinx-doc.org/en/master/config
v1.0.3,-- Path setup --------------------------------------------------------------
v1.0.3,"If extensions (or modules to document with autodoc) are in another directory,"
v1.0.3,add these directories to sys.path here. If the directory is relative to the
v1.0.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.0.3,
v1.0.3,"sys.path.insert(0, os.path.abspath('..'))"
v1.0.3,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.0.3,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.0.3,from unittest.mock import Mock
v1.0.3,sys.modules['numpy'] = Mock()
v1.0.3,sys.modules['numpy.linalg'] = Mock()
v1.0.3,sys.modules['scipy'] = Mock()
v1.0.3,sys.modules['scipy.optimize'] = Mock()
v1.0.3,sys.modules['scipy.interpolate'] = Mock()
v1.0.3,sys.modules['scipy.sparse'] = Mock()
v1.0.3,sys.modules['scipy.ndimage'] = Mock()
v1.0.3,sys.modules['scipy.ndimage.filters'] = Mock()
v1.0.3,sys.modules['tensorflow'] = Mock()
v1.0.3,sys.modules['theano'] = Mock()
v1.0.3,sys.modules['theano.tensor'] = Mock()
v1.0.3,sys.modules['torch'] = Mock()
v1.0.3,sys.modules['torch.optim'] = Mock()
v1.0.3,sys.modules['torch.nn'] = Mock()
v1.0.3,sys.modules['torch.nn.init'] = Mock()
v1.0.3,sys.modules['torch.autograd'] = Mock()
v1.0.3,sys.modules['sklearn'] = Mock()
v1.0.3,sys.modules['sklearn.model_selection'] = Mock()
v1.0.3,sys.modules['sklearn.utils'] = Mock()
v1.0.3,-- Project information -----------------------------------------------------
v1.0.3,"The full version, including alpha/beta/rc tags."
v1.0.3,The short X.Y version.
v1.0.3,-- General configuration ---------------------------------------------------
v1.0.3,"If your documentation needs a minimal Sphinx version, state it here."
v1.0.3,
v1.0.3,needs_sphinx = '1.0'
v1.0.3,"If true, the current module name will be prepended to all description"
v1.0.3,unit titles (such as .. function::).
v1.0.3,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.0.3,"Add any Sphinx extension module names here, as strings. They can be"
v1.0.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.0.3,ones.
v1.0.3,generate autosummary pages
v1.0.3,"Add any paths that contain templates here, relative to this directory."
v1.0.3,The suffix(es) of source filenames.
v1.0.3,You can specify multiple suffix as a list of string:
v1.0.3,
v1.0.3,"source_suffix = ['.rst', '.md']"
v1.0.3,The master toctree document.
v1.0.3,The language for content autogenerated by Sphinx. Refer to documentation
v1.0.3,for a list of supported languages.
v1.0.3,
v1.0.3,This is also used if you do content translation via gettext catalogs.
v1.0.3,"Usually you set ""language"" from the command line for these cases."
v1.0.3,"List of patterns, relative to source directory, that match files and"
v1.0.3,directories to ignore when looking for source files.
v1.0.3,This pattern also affects html_static_path and html_extra_path.
v1.0.3,The name of the Pygments (syntax highlighting) style to use.
v1.0.3,-- Options for HTML output -------------------------------------------------
v1.0.3,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.0.3,a list of builtin themes.
v1.0.3,
v1.0.3,Theme options are theme-specific and customize the look and feel of a theme
v1.0.3,"further.  For a list of options available for each theme, see the"
v1.0.3,documentation.
v1.0.3,
v1.0.3,html_theme_options = {}
v1.0.3,"Add any paths that contain custom static files (such as style sheets) here,"
v1.0.3,"relative to this directory. They are copied after the builtin static files,"
v1.0.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.0.3,html_static_path = ['_static']
v1.0.3,"Custom sidebar templates, must be a dictionary that maps document names"
v1.0.3,to template names.
v1.0.3,
v1.0.3,The default sidebars (for documents that don't match any pattern) are
v1.0.3,defined by theme itself.  Builtin themes are using these templates by
v1.0.3,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.0.3,'searchbox.html']``.
v1.0.3,
v1.0.3,html_sidebars = {}
v1.0.3,The name of an image file (relative to this directory) to place at the top
v1.0.3,of the sidebar.
v1.0.3,
v1.0.3,-- Options for HTMLHelp output ---------------------------------------------
v1.0.3,Output file base name for HTML help builder.
v1.0.3,-- Options for LaTeX output ------------------------------------------------
v1.0.3,The paper size ('letterpaper' or 'a4paper').
v1.0.3,
v1.0.3,"'papersize': 'letterpaper',"
v1.0.3,"The font size ('10pt', '11pt' or '12pt')."
v1.0.3,
v1.0.3,"'pointsize': '10pt',"
v1.0.3,Additional stuff for the LaTeX preamble.
v1.0.3,
v1.0.3,"'preamble': '',"
v1.0.3,Latex figure (float) alignment
v1.0.3,
v1.0.3,"'figure_align': 'htbp',"
v1.0.3,Grouping the document tree into LaTeX files. List of tuples
v1.0.3,"(source start file, target name, title,"
v1.0.3,"author, documentclass [howto, manual, or own class])."
v1.0.3,-- Options for manual page output ------------------------------------------
v1.0.3,One entry per manual page. List of tuples
v1.0.3,"(source start file, name, description, authors, manual section)."
v1.0.3,-- Options for Texinfo output ----------------------------------------------
v1.0.3,Grouping the document tree into Texinfo files. List of tuples
v1.0.3,"(source start file, target name, title, author,"
v1.0.3,"dir menu entry, description, category)"
v1.0.3,-- Options for Epub output -------------------------------------------------
v1.0.3,Bibliographic Dublin Core info.
v1.0.3,The unique identifier of the text. This can be a ISBN number
v1.0.3,or the project homepage.
v1.0.3,
v1.0.3,epub_identifier = ''
v1.0.3,A unique identification for the text.
v1.0.3,
v1.0.3,epub_uid = ''
v1.0.3,A list of files that should not be packed into the epub file.
v1.0.3,-- Extension configuration -------------------------------------------------
v1.0.3,-- Options for intersphinx extension ---------------------------------------
v1.0.3,Example configuration for intersphinx: refer to the Python standard library.
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Check a model param is optimized
v1.0.3,Check a loss param is optimized
v1.0.3,Check a model param is NOT optimized
v1.0.3,Check a loss param is optimized
v1.0.3,Check a model param is optimized
v1.0.3,Check a loss param is NOT optimized
v1.0.3,Check a model param is NOT optimized
v1.0.3,Check a loss param is NOT optimized
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,check for empty batches
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,The triples factory and model
v1.0.3,: The evaluator to be tested
v1.0.3,Settings
v1.0.3,: The evaluator instantiation
v1.0.3,Settings
v1.0.3,Initialize evaluator
v1.0.3,Use small test dataset
v1.0.3,Use small model (untrained)
v1.0.3,Get batch
v1.0.3,Compute scores
v1.0.3,Compute mask only if required
v1.0.3,TODO: Re-use filtering code
v1.0.3,"shape: (batch_size, num_triples)"
v1.0.3,"shape: (batch_size, num_entities)"
v1.0.3,Process one batch
v1.0.3,Check for correct class
v1.0.3,Check value ranges
v1.0.3,TODO: Validate with data?
v1.0.3,Check for correct class
v1.0.3,check value
v1.0.3,filtering
v1.0.3,"true_score: (2, 3, 3)"
v1.0.3,head based filter
v1.0.3,preprocessing for faster lookup
v1.0.3,check that all found positives are positive
v1.0.3,check in-place
v1.0.3,Test head scores
v1.0.3,Assert in-place modification
v1.0.3,Assert correct filtering
v1.0.3,Test tail scores
v1.0.3,Assert in-place modification
v1.0.3,Assert correct filtering
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The batch size
v1.0.3,: The triples factory
v1.0.3,: Class of regularizer to test
v1.0.3,: The constructor parameters to pass to the regularizer
v1.0.3,": The regularizer instance, initialized in setUp"
v1.0.3,: A positive batch
v1.0.3,: The device
v1.0.3,Use RESCAL as it regularizes multiple tensors of different shape.
v1.0.3,Check if regularizer is stored correctly.
v1.0.3,Forward pass (should update regularizer)
v1.0.3,Call post_parameter_update (should reset regularizer)
v1.0.3,Check if regularization term is reset
v1.0.3,Call method
v1.0.3,Generate random tensors
v1.0.3,Call update
v1.0.3,check shape
v1.0.3,compute expected term
v1.0.3,Generate random tensor
v1.0.3,calculate penalty
v1.0.3,check shape
v1.0.3,check value
v1.0.3,Tests that exception will be thrown when more than or less than three tensors are passed
v1.0.3,Test that regularization term is computed correctly
v1.0.3,Entity soft constraint
v1.0.3,Orthogonality soft constraint
v1.0.3,"After first update, should change the term"
v1.0.3,"After second update, no change should happen"
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The number of embeddings
v1.0.3,: The embedding dimension
v1.0.3,check shape
v1.0.3,check values
v1.0.3,check shape
v1.0.3,check values
v1.0.3,check correct value range
v1.0.3,check maximum norm constraint
v1.0.3,unchanged values for small norms
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The window size used by the early stopper
v1.0.3,: The mock losses the mock evaluator will return
v1.0.3,: The (zeroed) index  - 1 at which stopping will occur
v1.0.3,: The minimum improvement
v1.0.3,Set automatic_memory_optimization to false for tests
v1.0.3,Step early stopper
v1.0.3,check storing of results
v1.0.3,check ring buffer
v1.0.3,: The window size used by the early stopper
v1.0.3,: The (zeroed) index  - 1 at which stopping will occur
v1.0.3,: The minimum improvement
v1.0.3,: The random seed to use for reproducibility
v1.0.3,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.0.3,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.0.3,: The batch size to use.
v1.0.3,Fix seed for reproducibility
v1.0.3,Set automatic_memory_optimization to false during testing
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,check correct translation
v1.0.3,check column order
v1.0.3,verify that all entities and relations are present in the training factory
v1.0.3,verify that no triple got lost
v1.0.3,verify that the label-to-id mappings match
v1.0.3,Check if multilabels are working correctly
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The batch size
v1.0.3,: The random seed
v1.0.3,: The triples factory
v1.0.3,: The sLCWA instances
v1.0.3,: Class of negative sampling to test
v1.0.3,": The negative sampler instance, initialized in setUp"
v1.0.3,: A positive batch
v1.0.3,Generate negative sample
v1.0.3,check shape
v1.0.3,check bounds: heads
v1.0.3,check bounds: relations
v1.0.3,check bounds: tails
v1.0.3,Check that all elements got corrupted
v1.0.3,Generate scaled negative sample
v1.0.3,Generate negative samples
v1.0.3,test that the relations were not changed
v1.0.3,Test that half of the subjects and half of the objects are corrupted
v1.0.3,Generate negative sample for additional tests
v1.0.3,test that the relations were not changed
v1.0.3,sample a batch
v1.0.3,check shape
v1.0.3,get triples
v1.0.3,check connected components
v1.0.3,super inefficient
v1.0.3,join
v1.0.3,already joined
v1.0.3,check that there is only a single component
v1.0.3,check content of comp_adj_lists
v1.0.3,check edge ids
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The expected number of entities
v1.0.3,: The expected number of relations
v1.0.3,: The dataset to test
v1.0.3,Not loaded
v1.0.3,Load
v1.0.3,Test caching
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The class of the model to test
v1.0.3,: Additional arguments passed to the model's constructor method
v1.0.3,: The triples factory instance
v1.0.3,: The model instance
v1.0.3,: The batch size for use for forward_* tests
v1.0.3,: The embedding dimensionality
v1.0.3,: Whether to create inverse triples (needed e.g. by ConvE)
v1.0.3,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.0.3,: The batch size for use when testing training procedures
v1.0.3,: The number of epochs to train the model
v1.0.3,: A random number generator from torch
v1.0.3,: The number of parameters which receive a constant (i.e. non-randomized)
v1.0.3,initialization
v1.0.3,assert there is at least one trainable parameter
v1.0.3,Check that all the parameters actually require a gradient
v1.0.3,Try to initialize an optimizer
v1.0.3,get model parameters
v1.0.3,re-initialize
v1.0.3,check that the operation works in-place
v1.0.3,check that the parameters where modified
v1.0.3,check for finite values by default
v1.0.3,check whether a gradient can be back-propgated
v1.0.3,"assert batch comprises (head, relation) pairs"
v1.0.3,"assert batch comprises (relation, tail) pairs"
v1.0.3,TODO: Catch HolE MKL error?
v1.0.3,set regularizer term
v1.0.3,call post_parameter_update
v1.0.3,assert that the regularization term has been reset
v1.0.3,do one optimization step
v1.0.3,call post_parameter_update
v1.0.3,check model constraints
v1.0.3,"assert batch comprises (relation, tail) pairs"
v1.0.3,"assert batch comprises (relation, tail) pairs"
v1.0.3,"assert batch comprises (relation, tail) pairs"
v1.0.3,Distance-based model
v1.0.3,3x batch norm: bias + scale --> 6
v1.0.3,entity specific bias        --> 1
v1.0.3,==================================
v1.0.3,7
v1.0.3,"two bias terms, one conv-filter"
v1.0.3,check type
v1.0.3,check shape
v1.0.3,check ID ranges
v1.0.3,this is only done in one of the models
v1.0.3,this is only done in one of the models
v1.0.3,Two linear layer biases
v1.0.3,"Two BN layers, bias & scale"
v1.0.3,: one bias per layer
v1.0.3,: (scale & bias for BN) * layers
v1.0.3,entity embeddings
v1.0.3,relation embeddings
v1.0.3,Compute Scores
v1.0.3,"self.assertAlmostEqual(second_score, -16, delta=0.01)"
v1.0.3,Use different dimension for relation embedding: relation_dim > entity_dim
v1.0.3,relation embeddings
v1.0.3,Compute Scores
v1.0.3,Use different dimension for relation embedding: relation_dim < entity_dim
v1.0.3,entity embeddings
v1.0.3,relation embeddings
v1.0.3,Compute Scores
v1.0.3,random entity embeddings & projections
v1.0.3,random relation embeddings & projections
v1.0.3,project
v1.0.3,check shape:
v1.0.3,check normalization
v1.0.3,entity embeddings
v1.0.3,relation embeddings
v1.0.3,Compute Scores
v1.0.3,second_score = scores[1].item()
v1.0.3,: 2xBN (bias & scale)
v1.0.3,check shape
v1.0.3,check content
v1.0.3,: The number of entities
v1.0.3,: The number of triples
v1.0.3,check shape
v1.0.3,check dtype
v1.0.3,check finite values (e.g. due to division by zero)
v1.0.3,check non-negativity
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,
v1.0.3,
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,check for finite values by default
v1.0.3,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.3,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.3,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.3,Get embeddings
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The class
v1.0.3,: Constructor keyword arguments
v1.0.3,: The loss instance
v1.0.3,: The batch size
v1.0.3,test reduction
v1.0.3,Test backward
v1.0.3,: The number of entities.
v1.0.3,: The number of negative samples
v1.0.3,≈ result of softmax
v1.0.3,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.0.3,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.0.3,"pos_distances = [0., 0., 0.5, 0.5]"
v1.0.3,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.0.3,≈ result of sigmoid
v1.0.3,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.0.3,expected_loss ≈ 0.34
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Create dummy dense labels
v1.0.3,Check if labels form a probability distribution
v1.0.3,Apply label smoothing
v1.0.3,Check if smooth labels form probability distribution
v1.0.3,Create dummy sLCWA labels
v1.0.3,Apply label smoothing
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A mapping of optimizers' names to their implementations
v1.0.3,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,"scale labels from [0, 1] to [-1, 1]"
v1.0.3,cross entropy expects a proper probability distribution -> normalize labels
v1.0.3,Use numerically stable variant to compute log(softmax)
v1.0.3,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.0.3,"To add *all* losses implemented in Torch, uncomment:"
v1.0.3,_LOSSES.update({
v1.0.3,loss
v1.0.3,for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()
v1.0.3,if not loss.__name__.startswith('_')
v1.0.3,})
v1.0.3,: A mapping of losses' names to their implementations
v1.0.3,: HPO Defaults for losses
v1.0.3,Add empty dictionaries as defaults for all remaining losses
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.0.3,Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.
v1.0.3,lower bound
v1.0.3,upper bound
v1.0.3,Allocate weight on device
v1.0.3,Initialize if initializer is provided
v1.0.3,Wrap embedding around it.
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The overall regularization weight
v1.0.3,: The current regularization term (a scalar)
v1.0.3,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.0.3,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.3,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.3,no need to compute anything
v1.0.3,always return zero
v1.0.3,: The dimension along which to compute the vector-based regularization terms.
v1.0.3,: Whether to normalize the regularization term by the dimension of the vectors.
v1.0.3,: This allows dimensionality-independent weight tuning.
v1.0.3,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.3,expected value of |x|_1 = d*E[x_i] for x_i i.i.d.
v1.0.3,expected value of |x|_2 when x_i are normally distributed
v1.0.3,cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1
v1.0.3,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.3,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.3,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.0.3,"Therefore, apply_only_once is always set to True."
v1.0.3,Entity soft constraint
v1.0.3,Orthogonality soft constraint
v1.0.3,The normalization factor to balance individual regularizers' contribution.
v1.0.3,: A mapping of regularizers' names to their implementations
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Add HPO command
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The random seed used at the beginning of the pipeline
v1.0.3,: The model trained by the pipeline
v1.0.3,: The training loop used by the pipeline
v1.0.3,: The losses during training
v1.0.3,: The results evaluated by the pipeline
v1.0.3,: How long in seconds did training take?
v1.0.3,: How long in seconds did evaluation take?
v1.0.3,: An early stopper
v1.0.3,: Any additional metadata as a dictionary
v1.0.3,: The version of PyKEEN used to create these results
v1.0.3,: The git hash of PyKEEN used to create these results
v1.0.3,1. Dataset
v1.0.3,2. Model
v1.0.3,3. Loss
v1.0.3,4. Regularizer
v1.0.3,5. Optimizer
v1.0.3,6. Training Loop
v1.0.3,7. Training (ronaldo style)
v1.0.3,8. Evaluation
v1.0.3,9. Tracking
v1.0.3,Misc
v1.0.3,Start tracking
v1.0.3,FIXME this should never happen.
v1.0.3,Log model parameters
v1.0.3,Log optimizer parameters
v1.0.3,Stopping
v1.0.3,"Load the evaluation batch size for the stopper, if it has been set"
v1.0.3,By default there's a stopper that does nothing interesting
v1.0.3,Add logging for debugging
v1.0.3,Train like Cristiano Ronaldo
v1.0.3,Evaluate
v1.0.3,Reuse optimal evaluation parameters from training if available
v1.0.3,Add logging about evaluator for debugging
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A mapping of trackers' names to their implementations
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Create directory in which all experimental artifacts are saved
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: Functions for specifying exotic resources with a given prefix
v1.0.3,: Functions for specifying exotic resources based on their file extension
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.0.3,TODO vectorize code
v1.0.3,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.0.3,"FIXME is this ever possible, since this function is called in __init__?"
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Create lists out of sets for proper numpy indexing when loading the labels
v1.0.3,TODO is there a need to have a canonical sort order here?
v1.0.3,Split triples
v1.0.3,Sorting ensures consistent results when the triples are permuted
v1.0.3,Create mapping
v1.0.3,Sorting ensures consistent results when the triples are permuted
v1.0.3,Create mapping
v1.0.3,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.0.3,Filter all non-existent triples
v1.0.3,Note: Unique changes the order of the triples
v1.0.3,Note: Using unique means implicit balancing of training samples
v1.0.3,: The mapping from entities' labels to their indexes
v1.0.3,: The mapping from relations' labels to their indexes
v1.0.3,": A three-column matrix where each row are the head label,"
v1.0.3,": relation label, then tail label"
v1.0.3,": A three-column matrix where each row are the head identifier,"
v1.0.3,": relation identifier, then tail identifier"
v1.0.3,": A dictionary mapping each relation to its inverse, if inverse triples were created"
v1.0.3,TODO: Check if lazy evaluation would make sense
v1.0.3,Check if the triples are inverted already
v1.0.3,extend original triples with inverse ones
v1.0.3,Generate entity mapping if necessary
v1.0.3,Generate relation mapping if necessary
v1.0.3,Map triples of labels to triples of IDs.
v1.0.3,We can terminate the search after finding the first inverse occurrence
v1.0.3,Ensure 2d array in case only one triple was given
v1.0.3,FIXME this function is only ever used in tests
v1.0.3,Prepare shuffle index
v1.0.3,Prepare split index
v1.0.3,Take cumulative sum so the get separated properly
v1.0.3,Split triples
v1.0.3,Make sure that the first element has all the right stuff in it
v1.0.3,Make new triples factories for each group
v1.0.3,Input validation
v1.0.3,convert to numpy
v1.0.3,vectorized label lookup
v1.0.3,Additional columns
v1.0.3,convert PyTorch tensors to numpy
v1.0.3,convert to dataframe
v1.0.3,Re-order columns
v1.0.3,While there are still triples that should be moved to the training set
v1.0.3,Pick a random triple to move over to the training triples
v1.0.3,Recalculate the testing triples without that index
v1.0.3,"Recalculate the training entities, testing entities, to_move, and move_id_mask"
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A PyTorch tensor of triples
v1.0.3,: A mapping from relation labels to integer identifiers
v1.0.3,: A mapping from relation labels to integer identifiers
v1.0.3,Create dense target
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,basically take all candidates
v1.0.3,Calculate which relations are the inverse ones
v1.0.3,FIXME doesn't carry flag of create_inverse_triples through
v1.0.3,A dictionary of all of the head/tail pairs for a given relation
v1.0.3,A dictionary for all of the tail/head pairs for a given relation
v1.0.3,Calculate the similarity between each relationship (entries in ``forward``)
v1.0.3,with all other candidate inverse relationships (entries in ``inverse``)
v1.0.3,"Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the"
v1.0.3,"same as for ``(b, a)``"
v1.0.3,A dictionary of all of the head/tail pairs for a given relation
v1.0.3,Filter out results between a given relationship and itself
v1.0.3,Filter out results below a minimum frequency
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,preprocessing
v1.0.3,initialize
v1.0.3,sample iteratively
v1.0.3,determine weights
v1.0.3,only happens at first iteration
v1.0.3,normalize to probabilities
v1.0.3,sample a start node
v1.0.3,get list of neighbors
v1.0.3,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.0.3,visit target node
v1.0.3,decrease sample counts
v1.0.3,return chosen edges
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Create training instances
v1.0.3,During size probing the training instances should not show the tqdm progress bar
v1.0.3,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.0.3,Ensure the release of memory
v1.0.3,Clear optimizer
v1.0.3,"Take the biggest possible training batch_size, if batch_size not set"
v1.0.3,This will find necessary parameters to optimize the use of the hardware at hand
v1.0.3,return the relevant parameters slice_size and batch_size
v1.0.3,Create dummy result tracker
v1.0.3,Sanity check
v1.0.3,Force weight initialization if training continuation is not explicitly requested.
v1.0.3,Reset the weights
v1.0.3,Create new optimizer
v1.0.3,Ensure the model is on the correct device
v1.0.3,Create Sampler
v1.0.3,Bind
v1.0.3,"When size probing, we don't want progress bars"
v1.0.3,Create progress bar
v1.0.3,Training Loop
v1.0.3,Enforce training mode
v1.0.3,Accumulate loss over epoch
v1.0.3,Batching
v1.0.3,Only create a progress bar when not in size probing mode
v1.0.3,Flag to check when to quit the size probing
v1.0.3,Recall that torch *accumulates* gradients. Before passing in a
v1.0.3,"new instance, you need to zero out the gradients from the old instance"
v1.0.3,Get batch size of current batch (last batch may be incomplete)
v1.0.3,accumulate gradients for whole batch
v1.0.3,forward pass call
v1.0.3,"when called by batch_size_search(), the parameter update should not be applied."
v1.0.3,update parameters according to optimizer
v1.0.3,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.0.3,constraints are no longer applied
v1.0.3,For testing purposes we're only interested in processing one batch
v1.0.3,When size probing we don't need the losses
v1.0.3,Track epoch loss
v1.0.3,Print loss information to console
v1.0.3,forward pass
v1.0.3,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.0.3,correction for loss reduction
v1.0.3,backward pass
v1.0.3,reset the regularizer to free the computational graph
v1.0.3,Set upper bound
v1.0.3,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.0.3,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.3,The regularizer has to be reset to free the computational graph
v1.0.3,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Shuffle each epoch
v1.0.3,Lazy-splitting into batches
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Slicing is not possible in sLCWA training loops
v1.0.3,Send positive batch to device
v1.0.3,Create negative samples
v1.0.3,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.0.3,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.0.3,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.0.3,Compute negative and positive scores
v1.0.3,Repeat positives scores (necessary for more than one negative per positive)
v1.0.3,Stack predictions
v1.0.3,Create target
v1.0.3,Normalize the loss to have the average loss per positive triple
v1.0.3,This allows comparability of sLCWA and LCWA losses
v1.0.3,Slicing is not possible for sLCWA
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A mapping of training loops' names to their implementations
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Split batch components
v1.0.3,Send batch to device
v1.0.3,Apply label smoothing
v1.0.3,This shows how often one row has to be repeated
v1.0.3,Create boolean indices for negative labels in the repeated rows
v1.0.3,Repeat the predictions and filter for negative labels
v1.0.3,This tells us how often each true label should be repeated
v1.0.3,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.0.3,Split positive and negative scores
v1.0.3,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.3,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The model
v1.0.3,: The evaluator
v1.0.3,: The triples to use for evaluation
v1.0.3,: Size of the evaluation batches
v1.0.3,: Slice size of the evaluation batches
v1.0.3,: The number of epochs after which the model is evaluated on validation set
v1.0.3,: The number of iterations (one iteration can correspond to various epochs)
v1.0.3,: with no improvement after which training will be stopped.
v1.0.3,: The name of the metric to use
v1.0.3,: The minimum improvement between two iterations
v1.0.3,: The metric results from all evaluations
v1.0.3,: A ring buffer to store the recent results
v1.0.3,: A counter for the ring buffer
v1.0.3,": Whether a larger value is better, or a smaller"
v1.0.3,: The criterion. Set in the constructor based on larger_is_better
v1.0.3,: The result tracker
v1.0.3,: Callbacks when training gets continued
v1.0.3,: Callbacks when training is stopped early
v1.0.3,: Did the stopper ever decide to stop?
v1.0.3,TODO: Fix this
v1.0.3,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.0.3,raise ValueError(f'Invalid metric name: {self.metric}')
v1.0.3,Dummy result tracker
v1.0.3,Evaluate
v1.0.3,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.0.3,Only check if enough values are already collected
v1.0.3,Stop if the result did not improve more than delta for patience epochs.
v1.0.3,Update ring buffer
v1.0.3,Append to history
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A mapping of stoppers' names to their implementations
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.0.3,Clear the ranks from the current evaluator
v1.0.3,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.0.3,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.3,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.3,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.3,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.0.3,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.3,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.3,Test if slicing is implemented for the required functions of this model
v1.0.3,Split batch
v1.0.3,Bind shape
v1.0.3,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.0.3,Warn if all entities will be filtered
v1.0.3,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.0.3,Send to device
v1.0.3,Ensure evaluation mode
v1.0.3,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.0.3,Check whether we need to be prepared for filtering
v1.0.3,Check whether an evaluator needs access to the masks
v1.0.3,This can only be an unfiltered evaluator.
v1.0.3,Prepare for result filtering
v1.0.3,Send tensors to device
v1.0.3,Prepare batches
v1.0.3,Show progressbar
v1.0.3,Flag to check when to quit the size probing
v1.0.3,Disable gradient tracking
v1.0.3,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.0.3,batch-wise processing
v1.0.3,Predict tail scores once
v1.0.3,Create positive filter for all corrupted tails
v1.0.3,Create a positive mask with the size of the scores from the positive tails filter
v1.0.3,Evaluate metrics on these *unfiltered* tail scores
v1.0.3,Filter
v1.0.3,The scores for the true triples have to be rewritten to the scores tensor
v1.0.3,Evaluate metrics on these *filtered* tail scores
v1.0.3,Predict head scores once
v1.0.3,Create positive filter for all corrupted heads
v1.0.3,Create a positive mask with the size of the scores from the positive heads filter
v1.0.3,Evaluate metrics on these head scores
v1.0.3,Filter
v1.0.3,The scores for the true triples have to be rewritten to the scores tensor
v1.0.3,Evaluate metrics on these *filtered* tail scores
v1.0.3,If we only probe sizes we do not need more than one batch
v1.0.3,Finalize
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The area under the ROC curve
v1.0.3,: The area under the precision-recall curve
v1.0.3,: The coverage error
v1.0.3,coverage_error: float = field(metadata=dict(
v1.0.3,"doc='The coverage error',"
v1.0.3,"f=metrics.coverage_error,"
v1.0.3,))
v1.0.3,: The label ranking loss (APS)
v1.0.3,label_ranking_average_precision_score: float = field(metadata=dict(
v1.0.3,"doc='The label ranking loss (APS)',"
v1.0.3,"f=metrics.label_ranking_average_precision_score,"
v1.0.3,))
v1.0.3,#: The label ranking loss
v1.0.3,label_ranking_loss: float = field(metadata=dict(
v1.0.3,"doc='The label ranking loss',"
v1.0.3,"f=metrics.label_ranking_loss,"
v1.0.3,))
v1.0.3,Transfer to cpu and convert to numpy
v1.0.3,Ensure that each key gets counted only once
v1.0.3,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.0.3,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.0.3,masks using the exact same key order.
v1.0.3,TODO how to define a cutoff on y_scores to make binary?
v1.0.3,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.0.3,Clear buffers
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A mapping of evaluators' names to their implementations
v1.0.3,: A mapping of results' names to their implementations
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,The best rank is the rank when assuming all options with an equal score are placed behind the currently
v1.0.3,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.0.3,The worst rank is the rank when assuming all options with an equal score are placed in front of the currently
v1.0.3,"considered. Hence, the rank is the number of options which have at least the same score minus one (as the"
v1.0.3,"currently considered option in included in all options). As the rank is one-based, we have to add 1, which"
v1.0.3,"nullifies the ""minus 1"" from before."
v1.0.3,"The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of"
v1.0.3,the elements with the same score as the currently considered option.
v1.0.3,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.0.3,The expected rank of a random scoring
v1.0.3,The adjusted ranks is normalized by the expected rank of a random scoring
v1.0.3,TODO adjusted_worst_rank
v1.0.3,TODO adjusted_best_rank
v1.0.3,: The mean over all ranks: mean_i r_i. Lower is better.
v1.0.3,: The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.
v1.0.3,": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k."
v1.0.3,: Higher is better.
v1.0.3,: The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.
v1.0.3,: Described by [berrendorf2020]_.
v1.0.3,Check if it a side or rank type
v1.0.3,Clear buffers
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.0.3,Create a tensor of all IDs
v1.0.3,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.0.3,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.0.3,: A dictionary of hyper-parameters to the models that use them
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The default loss function class
v1.0.3,: The default parameters for the default loss function class
v1.0.3,: The instance of the loss
v1.0.3,: The default regularizer class
v1.0.3,: The default parameters for the default regularizer class
v1.0.3,: The instance of the regularizer
v1.0.3,Initialize the device
v1.0.3,Random seeds have to set before the embeddings are initialized
v1.0.3,Loss
v1.0.3,TODO: Check loss functions that require 1 and -1 as label but only
v1.0.3,Regularizer
v1.0.3,The triples factory facilitates access to the dataset.
v1.0.3,This allows to store the optimized parameters
v1.0.3,Keep track of the hyper-parameters that are used across all
v1.0.3,subclasses of BaseModule
v1.0.3,Enforce evaluation mode
v1.0.3,Enforce evaluation mode
v1.0.3,Enforce evaluation mode
v1.0.3,Enforce evaluation mode
v1.0.3,The number of relations stored in the triples factory includes the number of inverse relations
v1.0.3,Id of inverse relation: relation + 1
v1.0.3,"The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs"
v1.0.3,initialize buffer on cpu
v1.0.3,calculate batch scores
v1.0.3,Explicitly create triples
v1.0.3,Sort final result
v1.0.3,Train a model (quickly)
v1.0.3,Get scores for *all* triples
v1.0.3,Get scores for top 15 triples
v1.0.3,set model to evaluation mode
v1.0.3,Do not track gradients
v1.0.3,initialize buffer on device
v1.0.3,calculate batch scores
v1.0.3,get top scores within batch
v1.0.3,append to global top scores
v1.0.3,reduce size if necessary
v1.0.3,Sort final result
v1.0.3,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.0.3,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.3,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.0.3,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.0.3,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.3,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.0.3,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.0.3,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.3,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.0.3,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.0.3,Default for relation dimensionality
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A mapping of models' names to their implementations
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Store initial input for error message
v1.0.3,All are None
v1.0.3,"input_channels is None, and any of height or width is None -> set input_channels=1"
v1.0.3,"input channels is not None, and one of height or width is None"
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The default loss function class
v1.0.3,: The default parameters for the default loss function class
v1.0.3,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.0.3,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.0.3,ConvE should be trained with inverse triples
v1.0.3,ConvE uses one bias for each entity
v1.0.3,Automatic calculation of remaining dimensions
v1.0.3,Parameter need to fulfil:
v1.0.3,input_channels * embedding_height * embedding_width = embedding_dim
v1.0.3,Finalize initialization
v1.0.3,embeddings
v1.0.3,weights
v1.0.3,"batch_size, num_input_channels, 2*height, width"
v1.0.3,"batch_size, num_input_channels, 2*height, width"
v1.0.3,"batch_size, num_input_channels, 2*height, width"
v1.0.3,"(N,C_out,H_out,W_out)"
v1.0.3,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.0.3,Embedding Regularization
v1.0.3,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.0.3,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.3,Embedding Regularization
v1.0.3,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.3,Embedding Regularization
v1.0.3,Code to repeat each item successively instead of the entire tensor
v1.0.3,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,Embeddings
v1.0.3,Finalize initialization
v1.0.3,Initialise left relation embeddings to unit length
v1.0.3,Make sure to call super first
v1.0.3,Normalise embeddings of entities
v1.0.3,Get embeddings
v1.0.3,Project entities
v1.0.3,Get embeddings
v1.0.3,Project entities
v1.0.3,Project entities
v1.0.3,Project entities
v1.0.3,Get embeddings
v1.0.3,Project entities
v1.0.3,Project entities
v1.0.3,Project entities
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The default loss function class
v1.0.3,: The default parameters for the default loss function class
v1.0.3,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.0.3,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.0.3,Finalize initialization
v1.0.3,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.0.3,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.0.3,split into real and imaginary part
v1.0.3,ComplEx space bilinear product
v1.0.3,*: Elementwise multiplication
v1.0.3,get embeddings
v1.0.3,Compute scores
v1.0.3,Regularization
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The regularizer used by [nickel2011]_ for for RESCAL
v1.0.3,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.0.3,: a normalized weight of 10 is used.
v1.0.3,: The LP settings used by [nickel2011]_ for for RESCAL
v1.0.3,Finalize initialization
v1.0.3,Get embeddings
v1.0.3,"shape: (b, d)"
v1.0.3,"shape: (b, d, d)"
v1.0.3,"shape: (b, d)"
v1.0.3,Compute scores
v1.0.3,Regularization
v1.0.3,Compute scores
v1.0.3,Regularization
v1.0.3,Get embeddings
v1.0.3,Compute scores
v1.0.3,Regularization
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,Finalize initialization
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.0.3,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.0.3,The interaction model
v1.0.3,Finalize initialization
v1.0.3,embeddings
v1.0.3,Use Xavier initialization for weight; bias to zero
v1.0.3,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.0.3,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.0.3,Output layer regularization
v1.0.3,In the code base only the weights of the output layer are used for regularization
v1.0.3,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.0.3,Stack to convolution input
v1.0.3,Convolution
v1.0.3,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.0.3,Linear layer for final scores
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,Finalize initialization
v1.0.3,": shape: (batch_size, num_entities, d)"
v1.0.3,": Prepare h: (b, e, d) -> (b, e, 1, 1, d)"
v1.0.3,": Prepare t: (b, e, d) -> (b, e, 1, d, 1)"
v1.0.3,": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)"
v1.0.3,"h.T @ W @ t, shape: (b, e, k, 1, 1)"
v1.0.3,": reduce (b, e, k, 1, 1) -> (b, e, k)"
v1.0.3,": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.3,": Prepare h: (b, e, d) -> (b, e, d, 1)"
v1.0.3,"V_h @ h, shape: (b, e, k, 1)"
v1.0.3,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.3,": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.3,": Prepare t: (b, e, d) -> (b, e, d, 1)"
v1.0.3,"V_t @ t, shape: (b, e, k, 1)"
v1.0.3,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.3,": Prepare b: (R, k) -> (b, k) -> (b, 1, k)"
v1.0.3,"a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)"
v1.0.3,"prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)"
v1.0.3,"prepare act: (b, e, k) -> (b, e, 1, k)"
v1.0.3,"compute score, shape: (b, e, 1, 1)"
v1.0.3,reduce
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The regularizer used by [yang2014]_ for DistMult
v1.0.3,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.0.3,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.0.3,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.0.3,: The LP settings used by [yang2014]_ for DistMult
v1.0.3,Finalize initialization
v1.0.3,"xavier uniform, cf."
v1.0.3,https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17
v1.0.3,Initialise relation embeddings to unit length
v1.0.3,Make sure to call super first
v1.0.3,Normalize embeddings of entities
v1.0.3,Bilinear product
v1.0.3,*: Elementwise multiplication
v1.0.3,Get embeddings
v1.0.3,Compute score
v1.0.3,Only regularize relation embeddings
v1.0.3,Get embeddings
v1.0.3,Rank against all entities
v1.0.3,Only regularize relation embeddings
v1.0.3,Get embeddings
v1.0.3,Rank against all entities
v1.0.3,Only regularize relation embeddings
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,Similarity function used for distributions
v1.0.3,element-wise covariance bounds
v1.0.3,Additional covariance embeddings
v1.0.3,Finalize initialization
v1.0.3,Constraints are applied through post_parameter_update
v1.0.3,Make sure to call super first
v1.0.3,Normalize entity embeddings
v1.0.3,Ensure positive definite covariances matrices and appropriate size by clamping
v1.0.3,Get embeddings
v1.0.3,Compute entity distribution
v1.0.3,: a = \mu^T\Sigma^{-1}\mu
v1.0.3,: b = \log \det \Sigma
v1.0.3,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.0.3,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.0.3,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.0.3,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The custom regularizer used by [wang2014]_ for TransH
v1.0.3,: The settings used by [wang2014]_ for TransH
v1.0.3,embeddings
v1.0.3,Finalize initialization
v1.0.3,TODO: Add initialization
v1.0.3,Make sure to call super first
v1.0.3,Normalise the normal vectors by their l2 norms
v1.0.3,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.0.3,which enforces the defined soft constraints.
v1.0.3,Get embeddings
v1.0.3,Project to hyperplane
v1.0.3,Regularization term
v1.0.3,Get embeddings
v1.0.3,Project to hyperplane
v1.0.3,Regularization term
v1.0.3,Get embeddings
v1.0.3,Project to hyperplane
v1.0.3,Regularization term
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,embeddings
v1.0.3,Finalize initialization
v1.0.3,Make sure to call super first
v1.0.3,Normalize entity embeddings
v1.0.3,TODO: Initialize from TransE
v1.0.3,Initialise relation embeddings to unit length
v1.0.3,"project to relation specific subspace, shape: (b, e, d_r)"
v1.0.3,ensure constraints
v1.0.3,"evaluate score function, shape: (b, e)"
v1.0.3,Get embeddings
v1.0.3,Get embeddings
v1.0.3,Get embeddings
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,Construct node neighbourhood mask
v1.0.3,Set nodes in batch to true
v1.0.3,Compute k-neighbourhood
v1.0.3,"if the target node needs an embeddings, so does the source node"
v1.0.3,Create edge mask
v1.0.3,pylint: disable=unused-argument
v1.0.3,"Calculate in-degree, i.e. number of incoming edges"
v1.0.3,pylint: disable=unused-argument
v1.0.3,"Calculate in-degree, i.e. number of incoming edges"
v1.0.3,: Interaction model used as decoder
v1.0.3,: The blocks of the relation-specific weight matrices
v1.0.3,": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)"
v1.0.3,: The base weight matrices to generate relation-specific weights
v1.0.3,": shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.3,: The relation-specific weights for each base
v1.0.3,": shape: (num_relations, num_bases)"
v1.0.3,: The biases for each layer (if used)
v1.0.3,": shape of each element: (embedding_dim,)"
v1.0.3,: Batch normalization for each layer (if used)
v1.0.3,: Activations for each layer (if used)
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,Instantiate model
v1.0.3,Heuristic
v1.0.3,buffering of messages
v1.0.3,"Save graph using buffers, such that the tensors are moved together with the model"
v1.0.3,Weights
v1.0.3,Finalize initialization
v1.0.3,invalidate enriched embeddings
v1.0.3,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.0.3,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.0.3,initialized properly)
v1.0.3,We have one additional relation for self-loops
v1.0.3,Xavier Glorot initialization of each block
v1.0.3,Reset biases
v1.0.3,Reset batch norm parameters
v1.0.3,"Reset activation parameters, if any"
v1.0.3,use buffered messages if applicable
v1.0.3,Bind fields
v1.0.3,"shape: (num_entities, embedding_dim)"
v1.0.3,Edge dropout: drop the same edges on all layers (only in training mode)
v1.0.3,Get random dropout mask
v1.0.3,Apply to edges
v1.0.3,Different dropout for self-loops (only in training mode)
v1.0.3,"If batch is given, compute (num_layers)-hop neighbourhood"
v1.0.3,Initialize embeddings in the next layer for all nodes
v1.0.3,TODO: Can we vectorize this loop?
v1.0.3,Choose the edges which are of the specific relation
v1.0.3,Only propagate messages on subset of edges
v1.0.3,No edges available? Skip rest of inner loop
v1.0.3,Get source and target node indices
v1.0.3,send messages in both directions
v1.0.3,Select source node embeddings
v1.0.3,get relation weights
v1.0.3,Compute message (b x d) * (d x d) = (b x d)
v1.0.3,Normalize messages by relation-specific in-degree
v1.0.3,Aggregate messages in target
v1.0.3,Self-loop
v1.0.3,"Apply bias, if requested"
v1.0.3,"Apply batch normalization, if requested"
v1.0.3,Apply non-linearity
v1.0.3,allocate weight
v1.0.3,Get blocks
v1.0.3,"self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)"
v1.0.3,note: embedding_dim is guaranteed to be divisible by num_bases in the constructor
v1.0.3,"The current basis weights, shape: (num_bases)"
v1.0.3,"the current bases, shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.3,"compute the current relation weights, shape: (embedding_dim, embedding_dim)"
v1.0.3,Enrich embeddings
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The default loss function class
v1.0.3,: The default parameters for the default loss function class
v1.0.3,Core tensor
v1.0.3,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.0.3,Dropout
v1.0.3,Finalize initialization
v1.0.3,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.0.3,Abbreviation
v1.0.3,Compute h_n = DO(BN(h))
v1.0.3,Compute wr = DO(W x_2 r)
v1.0.3,compute whr = DO(BN(h_n x_1 wr))
v1.0.3,Compute whr x_3 t
v1.0.3,Get embeddings
v1.0.3,Compute scores
v1.0.3,Get embeddings
v1.0.3,Compute scores
v1.0.3,Get embeddings
v1.0.3,Compute scores
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,Finalize initialization
v1.0.3,Initialise relation embeddings to unit length
v1.0.3,Make sure to call super first
v1.0.3,Normalize entity embeddings
v1.0.3,Get embeddings
v1.0.3,Get embeddings
v1.0.3,Get embeddings
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The default loss function class
v1.0.3,: The default parameters for the default loss function class
v1.0.3,: The regularizer used by [trouillon2016]_ for SimplE
v1.0.3,": In the paper, they use weight of 0.1, and do not normalize the"
v1.0.3,": regularization term by the number of elements, which is 200."
v1.0.3,: The power sum settings used by [trouillon2016]_ for SimplE
v1.0.3,extra embeddings
v1.0.3,Finalize initialization
v1.0.3,forward model
v1.0.3,Regularization
v1.0.3,backward model
v1.0.3,Regularization
v1.0.3,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.0.3,"That is not mentioned in the paper, so it is omitted here."
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,Finalize initialization
v1.0.3,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.0.3,weight initialization
v1.0.3,Get embeddings
v1.0.3,Embedding Regularization
v1.0.3,Concatenate them
v1.0.3,Compute scores
v1.0.3,Get embeddings
v1.0.3,Embedding Regularization
v1.0.3,First layer can be unrolled
v1.0.3,Send scores through rest of the network
v1.0.3,Get embeddings
v1.0.3,Embedding Regularization
v1.0.3,First layer can be unrolled
v1.0.3,Send scores through rest of the network
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,The dimensions affected by e'
v1.0.3,Project entities
v1.0.3,r_p (e_p.T e) + e'
v1.0.3,Enforce constraints
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,Finalize initialization
v1.0.3,Make sure to call super first
v1.0.3,Normalize entity embeddings
v1.0.3,Project entities
v1.0.3,score = -||h_bot + r - t_bot||_2^2
v1.0.3,Head
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,Finalize initialization
v1.0.3,phases randomly between 0 and 2 pi
v1.0.3,Make sure to call super first
v1.0.3,Normalize relation embeddings
v1.0.3,Decompose into real and imaginary part
v1.0.3,Rotate (=Hadamard product in complex space).
v1.0.3,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.0.3,Get embeddings
v1.0.3,Compute scores
v1.0.3,Embedding Regularization
v1.0.3,Get embeddings
v1.0.3,Rank against all entities
v1.0.3,Compute scores
v1.0.3,Embedding Regularization
v1.0.3,Get embeddings
v1.0.3,r expresses a rotation in complex plane.
v1.0.3,The inverse rotation is expressed by the complex conjugate of r.
v1.0.3,The score is computed as the distance of the relation-rotated head to the tail.
v1.0.3,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.0.3,|h * r - t| = |h - conj(r) * t|
v1.0.3,Rank against all entities
v1.0.3,Compute scores
v1.0.3,Embedding Regularization
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The default loss function class
v1.0.3,: The default parameters for the default loss function class
v1.0.3,Global entity projection
v1.0.3,Global relation projection
v1.0.3,Global combination bias
v1.0.3,Global combination bias
v1.0.3,Finalize initialization
v1.0.3,Get embeddings
v1.0.3,Compute score
v1.0.3,Get embeddings
v1.0.3,Rank against all entities
v1.0.3,Get embeddings
v1.0.3,Rank against all entities
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,Finalize initialization
v1.0.3,Make sure to call super first
v1.0.3,Normalize entity embeddings
v1.0.3,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.0.3,Circular correlation of entity embeddings
v1.0.3,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.0.3,Hadamard product in frequency domain
v1.0.3,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.0.3,inner product with relation embedding
v1.0.3,Embedding Regularization
v1.0.3,Embedding Regularization
v1.0.3,Embedding Regularization
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The default loss function class
v1.0.3,: The default parameters for the default loss function class
v1.0.3,Finalize initialization
v1.0.3,Get embeddings
v1.0.3,Embedding Regularization
v1.0.3,Concatenate them
v1.0.3,Predict t embedding
v1.0.3,compare with all t's
v1.0.3,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.3,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.3,Embedding Regularization
v1.0.3,Concatenate them
v1.0.3,Predict t embedding
v1.0.3,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.3,Embedding Regularization
v1.0.3,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.0.3,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.0.3,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.0.3,Extend t
v1.0.3,Concatenate them
v1.0.3,Predict t embedding
v1.0.3,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.3,The results have to be realigned with the expected output of the score_h function
v1.0.3,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,TODO: Check entire build of the model
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The default loss function class
v1.0.3,: The default parameters for the default loss function class
v1.0.3,Literal
v1.0.3,num_ent x num_lit
v1.0.3,Number of columns corresponds to number of literals
v1.0.3,Literals
v1.0.3,End literals
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,TODO: Check entire build of the model
v1.0.3,: The default strategy for optimizing the model's hyper-parameters
v1.0.3,: The default parameters for the default loss function class
v1.0.3,Embeddings
v1.0.3,Number of columns corresponds to number of literals
v1.0.3,apply dropout
v1.0.3,"-, because lower score shall correspond to a more plausible triple."
v1.0.3,TODO check if this is the same as the BaseModule
v1.0.3,Choose y = -1 since a smaller score is better.
v1.0.3,"In TransE for example, the scores represent distances"
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.3,Bind number of negatives to sample
v1.0.3,Equally corrupt head and tail
v1.0.3,Copy positive batch for corruption.
v1.0.3,"Do not detach, as no gradients should flow into the indices."
v1.0.3,Sample random entities as replacement
v1.0.3,Replace heads – To make sure we don't replace the head by the original value
v1.0.3,we shift all values greater or equal than the original value by one up
v1.0.3,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.3,Corrupt tails
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A mapping of negative samplers' names to their implementations
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.3,Preprocessing: Compute corruption probabilities
v1.0.3,"compute tph, i.e. the average number of tail entities per head"
v1.0.3,"compute hpt, i.e. the average number of head entities per tail"
v1.0.3,Set parameter for Bernoulli distribution
v1.0.3,Bind number of negatives to sample
v1.0.3,Copy positive batch for corruption.
v1.0.3,"Do not detach, as no gradients should flow into the indices."
v1.0.3,Decide whether to corrupt head or tail
v1.0.3,Tails are corrupted if heads are not corrupted
v1.0.3,Randomly sample corruption
v1.0.3,Replace heads – To make sure we don't replace the head by the original value
v1.0.3,we shift all values greater or equal than the original value by one up
v1.0.3,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.3,Replace tails
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,TODO what happens if already exists?
v1.0.3,TODO incorporate setting of random seed
v1.0.3,pipeline_kwargs=dict(
v1.0.3,"random_seed=random.randint(1, 2 ** 32 - 1),"
v1.0.3,"),"
v1.0.3,Add dataset to current_pipeline
v1.0.3,Add loss function to current_pipeline
v1.0.3,Add regularizer to current_pipeline
v1.0.3,Add optimizer to current_pipeline
v1.0.3,Add training approach to current_pipeline
v1.0.3,Add training kwargs and kwargs_ranges
v1.0.3,Add evaluation
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A factory wrapping the training triples
v1.0.3,": A factory wrapping the testing triples, that share indexes with the training triples"
v1.0.3,": A factory wrapping the validation triples, that share indexes with the training triples"
v1.0.3,: All data sets should take care of inverse triple creation
v1.0.3,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.0.3,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.0.3,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.0.3,don't call this function by itself. assumes called through the `validation`
v1.0.3,property and the _training factory has already been loaded
v1.0.3,see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content
v1.0.3,pattern from https://stackoverflow.com/a/39217788/5775947
v1.0.3,TODO replace this with the new zip remote dataset class
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A mapping of datasets' names to their classes
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,TODO update docs with table and CLI wtih generator
v1.0.3,: A mapping of HPO samplers' names to their implementations
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,1. Dataset
v1.0.3,2. Model
v1.0.3,3. Loss
v1.0.3,4. Regularizer
v1.0.3,5. Optimizer
v1.0.3,6. Training Loop
v1.0.3,7. Training
v1.0.3,8. Evaluation
v1.0.3,9. Trackers
v1.0.3,Misc.
v1.0.3,2. Model
v1.0.3,3. Loss
v1.0.3,4. Regularizer
v1.0.3,5. Optimizer
v1.0.3,1. Dataset
v1.0.3,2. Model
v1.0.3,3. Loss
v1.0.3,4. Regularizer
v1.0.3,5. Optimizer
v1.0.3,6. Training Loop
v1.0.3,7. Training
v1.0.3,8. Evaluation
v1.0.3,9. Tracker
v1.0.3,Misc.
v1.0.3,Will trigger Optuna to set the state of the trial as failed
v1.0.3,: The :mod:`optuna` study object
v1.0.3,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.0.3,Output study information
v1.0.3,Output all trials
v1.0.3,Output best trial as pipeline configuration file
v1.0.3,1. Dataset
v1.0.3,2. Model
v1.0.3,3. Loss
v1.0.3,4. Regularizer
v1.0.3,5. Optimizer
v1.0.3,6. Training Loop
v1.0.3,7. Training
v1.0.3,8. Evaluation
v1.0.3,9. Tracking
v1.0.3,6. Misc
v1.0.3,Optuna Study Settings
v1.0.3,Optuna Optimization Settings
v1.0.3,0. Metadata/Provenance
v1.0.3,1. Dataset
v1.0.3,FIXME difference between dataset class and string
v1.0.3,FIXME how to handle if dataset or factories were set? Should have been
v1.0.3,part of https://github.com/mali-git/POEM_develop/pull/483
v1.0.3,2. Model
v1.0.3,3. Loss
v1.0.3,4. Regularizer
v1.0.3,5. Optimizer
v1.0.3,6. Training Loop
v1.0.3,7. Training
v1.0.3,8. Evaluation
v1.0.3,9. Tracking
v1.0.3,1. Dataset
v1.0.3,2. Model
v1.0.3,3. Loss
v1.0.3,4. Regularizer
v1.0.3,5. Optimizer
v1.0.3,6. Training Loop
v1.0.3,7. Training
v1.0.3,8. Evaluation
v1.0.3,9. Tracker
v1.0.3,Optuna Misc.
v1.0.3,Pipeline Misc.
v1.0.3,Invoke optimization of the objective function.
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,-*- coding: utf-8 -*-
v1.0.3,: A mapping of HPO pruners' names to their implementations
v1.0.3,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,
v1.0.2,Configuration file for the Sphinx documentation builder.
v1.0.2,
v1.0.2,This file does only contain a selection of the most common options. For a
v1.0.2,full list see the documentation:
v1.0.2,http://www.sphinx-doc.org/en/master/config
v1.0.2,-- Path setup --------------------------------------------------------------
v1.0.2,"If extensions (or modules to document with autodoc) are in another directory,"
v1.0.2,add these directories to sys.path here. If the directory is relative to the
v1.0.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.0.2,
v1.0.2,"sys.path.insert(0, os.path.abspath('..'))"
v1.0.2,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.0.2,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.0.2,from unittest.mock import Mock
v1.0.2,sys.modules['numpy'] = Mock()
v1.0.2,sys.modules['numpy.linalg'] = Mock()
v1.0.2,sys.modules['scipy'] = Mock()
v1.0.2,sys.modules['scipy.optimize'] = Mock()
v1.0.2,sys.modules['scipy.interpolate'] = Mock()
v1.0.2,sys.modules['scipy.sparse'] = Mock()
v1.0.2,sys.modules['scipy.ndimage'] = Mock()
v1.0.2,sys.modules['scipy.ndimage.filters'] = Mock()
v1.0.2,sys.modules['tensorflow'] = Mock()
v1.0.2,sys.modules['theano'] = Mock()
v1.0.2,sys.modules['theano.tensor'] = Mock()
v1.0.2,sys.modules['torch'] = Mock()
v1.0.2,sys.modules['torch.optim'] = Mock()
v1.0.2,sys.modules['torch.nn'] = Mock()
v1.0.2,sys.modules['torch.nn.init'] = Mock()
v1.0.2,sys.modules['torch.autograd'] = Mock()
v1.0.2,sys.modules['sklearn'] = Mock()
v1.0.2,sys.modules['sklearn.model_selection'] = Mock()
v1.0.2,sys.modules['sklearn.utils'] = Mock()
v1.0.2,-- Project information -----------------------------------------------------
v1.0.2,"The full version, including alpha/beta/rc tags."
v1.0.2,The short X.Y version.
v1.0.2,-- General configuration ---------------------------------------------------
v1.0.2,"If your documentation needs a minimal Sphinx version, state it here."
v1.0.2,
v1.0.2,needs_sphinx = '1.0'
v1.0.2,"If true, the current module name will be prepended to all description"
v1.0.2,unit titles (such as .. function::).
v1.0.2,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.0.2,"Add any Sphinx extension module names here, as strings. They can be"
v1.0.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.0.2,ones.
v1.0.2,generate autosummary pages
v1.0.2,"Add any paths that contain templates here, relative to this directory."
v1.0.2,The suffix(es) of source filenames.
v1.0.2,You can specify multiple suffix as a list of string:
v1.0.2,
v1.0.2,"source_suffix = ['.rst', '.md']"
v1.0.2,The master toctree document.
v1.0.2,The language for content autogenerated by Sphinx. Refer to documentation
v1.0.2,for a list of supported languages.
v1.0.2,
v1.0.2,This is also used if you do content translation via gettext catalogs.
v1.0.2,"Usually you set ""language"" from the command line for these cases."
v1.0.2,"List of patterns, relative to source directory, that match files and"
v1.0.2,directories to ignore when looking for source files.
v1.0.2,This pattern also affects html_static_path and html_extra_path.
v1.0.2,The name of the Pygments (syntax highlighting) style to use.
v1.0.2,-- Options for HTML output -------------------------------------------------
v1.0.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.0.2,a list of builtin themes.
v1.0.2,
v1.0.2,Theme options are theme-specific and customize the look and feel of a theme
v1.0.2,"further.  For a list of options available for each theme, see the"
v1.0.2,documentation.
v1.0.2,
v1.0.2,html_theme_options = {}
v1.0.2,"Add any paths that contain custom static files (such as style sheets) here,"
v1.0.2,"relative to this directory. They are copied after the builtin static files,"
v1.0.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.0.2,html_static_path = ['_static']
v1.0.2,"Custom sidebar templates, must be a dictionary that maps document names"
v1.0.2,to template names.
v1.0.2,
v1.0.2,The default sidebars (for documents that don't match any pattern) are
v1.0.2,defined by theme itself.  Builtin themes are using these templates by
v1.0.2,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.0.2,'searchbox.html']``.
v1.0.2,
v1.0.2,html_sidebars = {}
v1.0.2,The name of an image file (relative to this directory) to place at the top
v1.0.2,of the sidebar.
v1.0.2,
v1.0.2,-- Options for HTMLHelp output ---------------------------------------------
v1.0.2,Output file base name for HTML help builder.
v1.0.2,-- Options for LaTeX output ------------------------------------------------
v1.0.2,The paper size ('letterpaper' or 'a4paper').
v1.0.2,
v1.0.2,"'papersize': 'letterpaper',"
v1.0.2,"The font size ('10pt', '11pt' or '12pt')."
v1.0.2,
v1.0.2,"'pointsize': '10pt',"
v1.0.2,Additional stuff for the LaTeX preamble.
v1.0.2,
v1.0.2,"'preamble': '',"
v1.0.2,Latex figure (float) alignment
v1.0.2,
v1.0.2,"'figure_align': 'htbp',"
v1.0.2,Grouping the document tree into LaTeX files. List of tuples
v1.0.2,"(source start file, target name, title,"
v1.0.2,"author, documentclass [howto, manual, or own class])."
v1.0.2,-- Options for manual page output ------------------------------------------
v1.0.2,One entry per manual page. List of tuples
v1.0.2,"(source start file, name, description, authors, manual section)."
v1.0.2,-- Options for Texinfo output ----------------------------------------------
v1.0.2,Grouping the document tree into Texinfo files. List of tuples
v1.0.2,"(source start file, target name, title, author,"
v1.0.2,"dir menu entry, description, category)"
v1.0.2,-- Options for Epub output -------------------------------------------------
v1.0.2,Bibliographic Dublin Core info.
v1.0.2,The unique identifier of the text. This can be a ISBN number
v1.0.2,or the project homepage.
v1.0.2,
v1.0.2,epub_identifier = ''
v1.0.2,A unique identification for the text.
v1.0.2,
v1.0.2,epub_uid = ''
v1.0.2,A list of files that should not be packed into the epub file.
v1.0.2,-- Extension configuration -------------------------------------------------
v1.0.2,-- Options for intersphinx extension ---------------------------------------
v1.0.2,Example configuration for intersphinx: refer to the Python standard library.
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Check a model param is optimized
v1.0.2,Check a loss param is optimized
v1.0.2,Check a model param is NOT optimized
v1.0.2,Check a loss param is optimized
v1.0.2,Check a model param is optimized
v1.0.2,Check a loss param is NOT optimized
v1.0.2,Check a model param is NOT optimized
v1.0.2,Check a loss param is NOT optimized
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,check for empty batches
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,The triples factory and model
v1.0.2,: The evaluator to be tested
v1.0.2,Settings
v1.0.2,: The evaluator instantiation
v1.0.2,Settings
v1.0.2,Initialize evaluator
v1.0.2,Use small test dataset
v1.0.2,Use small model (untrained)
v1.0.2,Get batch
v1.0.2,Compute scores
v1.0.2,Compute mask only if required
v1.0.2,TODO: Re-use filtering code
v1.0.2,"shape: (batch_size, num_triples)"
v1.0.2,"shape: (batch_size, num_entities)"
v1.0.2,Process one batch
v1.0.2,Check for correct class
v1.0.2,Check value ranges
v1.0.2,TODO: Validate with data?
v1.0.2,Check for correct class
v1.0.2,check value
v1.0.2,filtering
v1.0.2,"true_score: (2, 3, 3)"
v1.0.2,head based filter
v1.0.2,preprocessing for faster lookup
v1.0.2,check that all found positives are positive
v1.0.2,check in-place
v1.0.2,Test head scores
v1.0.2,Assert in-place modification
v1.0.2,Assert correct filtering
v1.0.2,Test tail scores
v1.0.2,Assert in-place modification
v1.0.2,Assert correct filtering
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The batch size
v1.0.2,: The triples factory
v1.0.2,: Class of regularizer to test
v1.0.2,: The constructor parameters to pass to the regularizer
v1.0.2,": The regularizer instance, initialized in setUp"
v1.0.2,: A positive batch
v1.0.2,: The device
v1.0.2,Use RESCAL as it regularizes multiple tensors of different shape.
v1.0.2,Check if regularizer is stored correctly.
v1.0.2,Forward pass (should update regularizer)
v1.0.2,Call post_parameter_update (should reset regularizer)
v1.0.2,Check if regularization term is reset
v1.0.2,Call method
v1.0.2,Generate random tensors
v1.0.2,Call update
v1.0.2,check shape
v1.0.2,compute expected term
v1.0.2,Generate random tensor
v1.0.2,calculate penalty
v1.0.2,check shape
v1.0.2,check value
v1.0.2,Tests that exception will be thrown when more than or less than three tensors are passed
v1.0.2,Test that regularization term is computed correctly
v1.0.2,Entity soft constraint
v1.0.2,Orthogonality soft constraint
v1.0.2,"After first update, should change the term"
v1.0.2,"After second update, no change should happen"
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The number of embeddings
v1.0.2,: The embedding dimension
v1.0.2,check shape
v1.0.2,check values
v1.0.2,check shape
v1.0.2,check values
v1.0.2,check correct value range
v1.0.2,check maximum norm constraint
v1.0.2,unchanged values for small norms
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The window size used by the early stopper
v1.0.2,: The mock losses the mock evaluator will return
v1.0.2,: The (zeroed) index  - 1 at which stopping will occur
v1.0.2,: The minimum improvement
v1.0.2,Set automatic_memory_optimization to false for tests
v1.0.2,Step early stopper
v1.0.2,check storing of results
v1.0.2,check ring buffer
v1.0.2,: The window size used by the early stopper
v1.0.2,: The (zeroed) index  - 1 at which stopping will occur
v1.0.2,: The minimum improvement
v1.0.2,: The random seed to use for reproducibility
v1.0.2,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.0.2,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.0.2,: The batch size to use.
v1.0.2,Fix seed for reproducibility
v1.0.2,Set automatic_memory_optimization to false during testing
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Check if multilabels are working correctly
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The batch size
v1.0.2,: The random seed
v1.0.2,: The triples factory
v1.0.2,: The sLCWA instances
v1.0.2,: Class of negative sampling to test
v1.0.2,": The negative sampler instance, initialized in setUp"
v1.0.2,: A positive batch
v1.0.2,Generate negative sample
v1.0.2,check shape
v1.0.2,check bounds: heads
v1.0.2,check bounds: relations
v1.0.2,check bounds: tails
v1.0.2,Check that all elements got corrupted
v1.0.2,Generate scaled negative sample
v1.0.2,Generate negative samples
v1.0.2,test that the relations were not changed
v1.0.2,Test that half of the subjects and half of the objects are corrupted
v1.0.2,Generate negative sample for additional tests
v1.0.2,test that the relations were not changed
v1.0.2,sample a batch
v1.0.2,check shape
v1.0.2,get triples
v1.0.2,check connected components
v1.0.2,super inefficient
v1.0.2,join
v1.0.2,already joined
v1.0.2,check that there is only a single component
v1.0.2,check content of comp_adj_lists
v1.0.2,check edge ids
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The expected number of entities
v1.0.2,: The expected number of relations
v1.0.2,: The dataset to test
v1.0.2,Not loaded
v1.0.2,Load
v1.0.2,Test caching
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The class of the model to test
v1.0.2,: Additional arguments passed to the model's constructor method
v1.0.2,: The triples factory instance
v1.0.2,: The model instance
v1.0.2,: The batch size for use for forward_* tests
v1.0.2,: The embedding dimensionality
v1.0.2,: Whether to create inverse triples (needed e.g. by ConvE)
v1.0.2,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.0.2,: The batch size for use when testing training procedures
v1.0.2,: The number of epochs to train the model
v1.0.2,: A random number generator from torch
v1.0.2,: The number of parameters which receive a constant (i.e. non-randomized)
v1.0.2,initialization
v1.0.2,assert there is at least one trainable parameter
v1.0.2,Check that all the parameters actually require a gradient
v1.0.2,Try to initialize an optimizer
v1.0.2,get model parameters
v1.0.2,re-initialize
v1.0.2,check that the operation works in-place
v1.0.2,check that the parameters where modified
v1.0.2,check for finite values by default
v1.0.2,check whether a gradient can be back-propgated
v1.0.2,"assert batch comprises (head, relation) pairs"
v1.0.2,"assert batch comprises (relation, tail) pairs"
v1.0.2,TODO: Catch HolE MKL error?
v1.0.2,set regularizer term
v1.0.2,call post_parameter_update
v1.0.2,assert that the regularization term has been reset
v1.0.2,do one optimization step
v1.0.2,call post_parameter_update
v1.0.2,check model constraints
v1.0.2,"assert batch comprises (relation, tail) pairs"
v1.0.2,"assert batch comprises (relation, tail) pairs"
v1.0.2,"assert batch comprises (relation, tail) pairs"
v1.0.2,Distance-based model
v1.0.2,3x batch norm: bias + scale --> 6
v1.0.2,entity specific bias        --> 1
v1.0.2,==================================
v1.0.2,7
v1.0.2,"two bias terms, one conv-filter"
v1.0.2,Two linear layer biases
v1.0.2,"Two BN layers, bias & scale"
v1.0.2,: one bias per layer
v1.0.2,: (scale & bias for BN) * layers
v1.0.2,entity embeddings
v1.0.2,relation embeddings
v1.0.2,Compute Scores
v1.0.2,"self.assertAlmostEqual(second_score, -16, delta=0.01)"
v1.0.2,Use different dimension for relation embedding: relation_dim > entity_dim
v1.0.2,relation embeddings
v1.0.2,Compute Scores
v1.0.2,Use different dimension for relation embedding: relation_dim < entity_dim
v1.0.2,entity embeddings
v1.0.2,relation embeddings
v1.0.2,Compute Scores
v1.0.2,random entity embeddings & projections
v1.0.2,random relation embeddings & projections
v1.0.2,project
v1.0.2,check shape:
v1.0.2,check normalization
v1.0.2,entity embeddings
v1.0.2,relation embeddings
v1.0.2,Compute Scores
v1.0.2,second_score = scores[1].item()
v1.0.2,: 2xBN (bias & scale)
v1.0.2,check shape
v1.0.2,check content
v1.0.2,: The number of entities
v1.0.2,: The number of triples
v1.0.2,check shape
v1.0.2,check dtype
v1.0.2,check finite values (e.g. due to division by zero)
v1.0.2,check non-negativity
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,
v1.0.2,
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,check for finite values by default
v1.0.2,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.2,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.2,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.2,Get embeddings
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The class
v1.0.2,: Constructor keyword arguments
v1.0.2,: The loss instance
v1.0.2,: The batch size
v1.0.2,test reduction
v1.0.2,Test backward
v1.0.2,: The number of entities.
v1.0.2,: The number of negative samples
v1.0.2,≈ result of softmax
v1.0.2,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.0.2,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.0.2,"pos_distances = [0., 0., 0.5, 0.5]"
v1.0.2,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.0.2,≈ result of sigmoid
v1.0.2,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.0.2,expected_loss ≈ 0.34
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Create dummy dense labels
v1.0.2,Check if labels form a probability distribution
v1.0.2,Apply label smoothing
v1.0.2,Check if smooth labels form probability distribution
v1.0.2,Create dummy sLCWA labels
v1.0.2,Apply label smoothing
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: A mapping of optimizers' names to their implementations
v1.0.2,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,"scale labels from [0, 1] to [-1, 1]"
v1.0.2,cross entropy expects a proper probability distribution -> normalize labels
v1.0.2,Use numerically stable variant to compute log(softmax)
v1.0.2,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.0.2,"To add *all* losses implemented in Torch, uncomment:"
v1.0.2,_LOSSES.update({
v1.0.2,loss
v1.0.2,for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()
v1.0.2,if not loss.__name__.startswith('_')
v1.0.2,})
v1.0.2,: A mapping of losses' names to their implementations
v1.0.2,: HPO Defaults for losses
v1.0.2,Add empty dictionaries as defaults for all remaining losses
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.0.2,Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.
v1.0.2,lower bound
v1.0.2,upper bound
v1.0.2,Allocate weight on device
v1.0.2,Initialize if initializer is provided
v1.0.2,Wrap embedding around it.
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The overall regularization weight
v1.0.2,: The current regularization term (a scalar)
v1.0.2,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.0.2,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.2,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.2,no need to compute anything
v1.0.2,always return zero
v1.0.2,: The dimension along which to compute the vector-based regularization terms.
v1.0.2,: Whether to normalize the regularization term by the dimension of the vectors.
v1.0.2,: This allows dimensionality-independent weight tuning.
v1.0.2,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.2,expected value of |x|_1 = d*E[x_i] for x_i i.i.d.
v1.0.2,expected value of |x|_2 when x_i are normally distributed
v1.0.2,cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1
v1.0.2,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.2,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.2,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.0.2,"Therefore, apply_only_once is always set to True."
v1.0.2,Entity soft constraint
v1.0.2,Orthogonality soft constraint
v1.0.2,The normalization factor to balance individual regularizers' contribution.
v1.0.2,: A mapping of regularizers' names to their implementations
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Add HPO command
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The random seed used at the beginning of the pipeline
v1.0.2,: The model trained by the pipeline
v1.0.2,: The training loop used by the pipeline
v1.0.2,: The losses during training
v1.0.2,: The results evaluated by the pipeline
v1.0.2,: How long in seconds did training take?
v1.0.2,: How long in seconds did evaluation take?
v1.0.2,: An early stopper
v1.0.2,: Any additional metadata as a dictionary
v1.0.2,: The version of PyKEEN used to create these results
v1.0.2,: The git hash of PyKEEN used to create these results
v1.0.2,1. Dataset
v1.0.2,2. Model
v1.0.2,3. Loss
v1.0.2,4. Regularizer
v1.0.2,5. Optimizer
v1.0.2,6. Training Loop
v1.0.2,7. Training (ronaldo style)
v1.0.2,8. Evaluation
v1.0.2,9. MLFlow
v1.0.2,Misc
v1.0.2,Create result store
v1.0.2,Start tracking
v1.0.2,FIXME this should never happen.
v1.0.2,Log model parameters
v1.0.2,Log optimizer parameters
v1.0.2,Stopping
v1.0.2,"Load the evaluation batch size for the stopper, if it has been set"
v1.0.2,By default there's a stopper that does nothing interesting
v1.0.2,Add logging for debugging
v1.0.2,Train like Cristiano Ronaldo
v1.0.2,Evaluate
v1.0.2,Reuse optimal evaluation parameters from training if available
v1.0.2,Add logging about evaluator for debugging
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Create directory in which all experimental artifacts are saved
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: Functions for specifying exotic resources with a given prefix
v1.0.2,: Functions for specifying exotic resources based on their file extension
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.0.2,TODO vectorize code
v1.0.2,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.0.2,"FIXME is this ever possible, since this function is called in __init__?"
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Create lists out of sets for proper numpy indexing when loading the labels
v1.0.2,TODO is there a need to have a canonical sort order here?
v1.0.2,Split triples
v1.0.2,Sorting ensures consistent results when the triples are permuted
v1.0.2,Create mapping
v1.0.2,Sorting ensures consistent results when the triples are permuted
v1.0.2,Create mapping
v1.0.2,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.0.2,Filter all non-existent triples
v1.0.2,Note: Unique changes the order of the triples
v1.0.2,Note: Using unique means implicit balancing of training samples
v1.0.2,: The mapping from entities' labels to their indexes
v1.0.2,: The mapping from relations' labels to their indexes
v1.0.2,": A three-column matrix where each row are the head label,"
v1.0.2,": relation label, then tail label"
v1.0.2,": A three-column matrix where each row are the head identifier,"
v1.0.2,": relation identifier, then tail identifier"
v1.0.2,": A dictionary mapping each relation to its inverse, if inverse triples were created"
v1.0.2,TODO: Check if lazy evaluation would make sense
v1.0.2,Check if the triples are inverted already
v1.0.2,extend original triples with inverse ones
v1.0.2,Generate entity mapping if necessary
v1.0.2,Generate relation mapping if necessary
v1.0.2,Map triples of labels to triples of IDs.
v1.0.2,We can terminate the search after finding the first inverse occurrence
v1.0.2,Ensure 2d array in case only one triple was given
v1.0.2,FIXME this function is only ever used in tests
v1.0.2,Prepare shuffle index
v1.0.2,Prepare split index
v1.0.2,Take cumulative sum so the get separated properly
v1.0.2,Split triples
v1.0.2,Make sure that the first element has all the right stuff in it
v1.0.2,Make new triples factories for each group
v1.0.2,While there are still triples that should be moved to the training set
v1.0.2,Pick a random triple to move over to the training triples
v1.0.2,Recalculate the testing triples without that index
v1.0.2,"Recalculate the training entities, testing entities, to_move, and move_id_mask"
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: A PyTorch tensor of triples
v1.0.2,: A mapping from relation labels to integer identifiers
v1.0.2,: A mapping from relation labels to integer identifiers
v1.0.2,Create dense target
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,basically take all candidates
v1.0.2,Calculate which relations are the inverse ones
v1.0.2,FIXME doesn't carry flag of create_inverse_triples through
v1.0.2,A dictionary of all of the head/tail pairs for a given relation
v1.0.2,A dictionary for all of the tail/head pairs for a given relation
v1.0.2,Calculate the similarity between each relationship (entries in ``forward``)
v1.0.2,with all other candidate inverse relationships (entries in ``inverse``)
v1.0.2,"Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the"
v1.0.2,"same as for ``(b, a)``"
v1.0.2,A dictionary of all of the head/tail pairs for a given relation
v1.0.2,Filter out results between a given relationship and itself
v1.0.2,Filter out results below a minimum frequency
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,preprocessing
v1.0.2,initialize
v1.0.2,sample iteratively
v1.0.2,determine weights
v1.0.2,only happens at first iteration
v1.0.2,normalize to probabilities
v1.0.2,sample a start node
v1.0.2,get list of neighbors
v1.0.2,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.0.2,visit target node
v1.0.2,decrease sample counts
v1.0.2,return chosen edges
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Create training instances
v1.0.2,During size probing the training instances should not show the tqdm progress bar
v1.0.2,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.0.2,Ensure the release of memory
v1.0.2,Clear optimizer
v1.0.2,"Take the biggest possible training batch_size, if batch_size not set"
v1.0.2,This will find necessary parameters to optimize the use of the hardware at hand
v1.0.2,return the relevant parameters slice_size and batch_size
v1.0.2,Create dummy result tracker
v1.0.2,Sanity check
v1.0.2,Force weight initialization if training continuation is not explicitly requested.
v1.0.2,Reset the weights
v1.0.2,Create new optimizer
v1.0.2,Ensure the model is on the correct device
v1.0.2,Create Sampler
v1.0.2,Bind
v1.0.2,"When size probing, we don't want progress bars"
v1.0.2,Create progress bar
v1.0.2,Training Loop
v1.0.2,Enforce training mode
v1.0.2,Accumulate loss over epoch
v1.0.2,Batching
v1.0.2,Only create a progress bar when not in size probing mode
v1.0.2,Flag to check when to quit the size probing
v1.0.2,Recall that torch *accumulates* gradients. Before passing in a
v1.0.2,"new instance, you need to zero out the gradients from the old instance"
v1.0.2,Get batch size of current batch (last batch may be incomplete)
v1.0.2,accumulate gradients for whole batch
v1.0.2,forward pass call
v1.0.2,"when called by batch_size_search(), the parameter update should not be applied."
v1.0.2,update parameters according to optimizer
v1.0.2,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.0.2,constraints are no longer applied
v1.0.2,For testing purposes we're only interested in processing one batch
v1.0.2,When size probing we don't need the losses
v1.0.2,Track epoch loss
v1.0.2,Print loss information to console
v1.0.2,forward pass
v1.0.2,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.0.2,correction for loss reduction
v1.0.2,backward pass
v1.0.2,reset the regularizer to free the computational graph
v1.0.2,Set upper bound
v1.0.2,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.0.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.2,The regularizer has to be reset to free the computational graph
v1.0.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Shuffle each epoch
v1.0.2,Lazy-splitting into batches
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Slicing is not possible in sLCWA training loops
v1.0.2,Send positive batch to device
v1.0.2,Create negative samples
v1.0.2,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.0.2,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.0.2,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.0.2,Compute negative and positive scores
v1.0.2,Repeat positives scores (necessary for more than one negative per positive)
v1.0.2,Stack predictions
v1.0.2,Create target
v1.0.2,Normalize the loss to have the average loss per positive triple
v1.0.2,This allows comparability of sLCWA and LCWA losses
v1.0.2,Slicing is not possible for sLCWA
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: A mapping of training loops' names to their implementations
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Split batch components
v1.0.2,Send batch to device
v1.0.2,Apply label smoothing
v1.0.2,This shows how often one row has to be repeated
v1.0.2,Create boolean indices for negative labels in the repeated rows
v1.0.2,Repeat the predictions and filter for negative labels
v1.0.2,This tells us how often each true label should be repeated
v1.0.2,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.0.2,Split positive and negative scores
v1.0.2,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.2,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The model
v1.0.2,: The evaluator
v1.0.2,: The triples to use for evaluation
v1.0.2,: Size of the evaluation batches
v1.0.2,: Slice size of the evaluation batches
v1.0.2,: The number of epochs after which the model is evaluated on validation set
v1.0.2,: The number of iterations (one iteration can correspond to various epochs)
v1.0.2,: with no improvement after which training will be stopped.
v1.0.2,: The name of the metric to use
v1.0.2,: The minimum improvement between two iterations
v1.0.2,: The metric results from all evaluations
v1.0.2,: A ring buffer to store the recent results
v1.0.2,: A counter for the ring buffer
v1.0.2,": Whether a larger value is better, or a smaller"
v1.0.2,: The criterion. Set in the constructor based on larger_is_better
v1.0.2,: The result tracker
v1.0.2,: Callbacks when training gets continued
v1.0.2,: Callbacks when training is stopped early
v1.0.2,: Did the stopper ever decide to stop?
v1.0.2,TODO: Fix this
v1.0.2,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.0.2,raise ValueError(f'Invalid metric name: {self.metric}')
v1.0.2,Dummy result tracker
v1.0.2,Evaluate
v1.0.2,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.0.2,Only check if enough values are already collected
v1.0.2,Stop if the result did not improve more than delta for patience epochs.
v1.0.2,Update ring buffer
v1.0.2,Append to history
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: A mapping of stoppers' names to their implementations
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.0.2,Clear the ranks from the current evaluator
v1.0.2,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.0.2,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.2,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.2,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.0.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.2,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.2,Test if slicing is implemented for the required functions of this model
v1.0.2,Split batch
v1.0.2,Bind shape
v1.0.2,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.0.2,Warn if all entities will be filtered
v1.0.2,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.0.2,Send to device
v1.0.2,Ensure evaluation mode
v1.0.2,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.0.2,Check whether we need to be prepared for filtering
v1.0.2,Check whether an evaluator needs access to the masks
v1.0.2,This can only be an unfiltered evaluator.
v1.0.2,Prepare for result filtering
v1.0.2,Send tensors to device
v1.0.2,Prepare batches
v1.0.2,Show progressbar
v1.0.2,Flag to check when to quit the size probing
v1.0.2,Disable gradient tracking
v1.0.2,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.0.2,batch-wise processing
v1.0.2,Predict tail scores once
v1.0.2,Create positive filter for all corrupted tails
v1.0.2,Create a positive mask with the size of the scores from the positive tails filter
v1.0.2,Evaluate metrics on these *unfiltered* tail scores
v1.0.2,Filter
v1.0.2,The scores for the true triples have to be rewritten to the scores tensor
v1.0.2,Evaluate metrics on these *filtered* tail scores
v1.0.2,Predict head scores once
v1.0.2,Create positive filter for all corrupted heads
v1.0.2,Create a positive mask with the size of the scores from the positive heads filter
v1.0.2,Evaluate metrics on these head scores
v1.0.2,Filter
v1.0.2,The scores for the true triples have to be rewritten to the scores tensor
v1.0.2,Evaluate metrics on these *filtered* tail scores
v1.0.2,If we only probe sizes we do not need more than one batch
v1.0.2,Finalize
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The area under the ROC curve
v1.0.2,: The area under the precision-recall curve
v1.0.2,: The coverage error
v1.0.2,coverage_error: float = field(metadata=dict(
v1.0.2,"doc='The coverage error',"
v1.0.2,"f=metrics.coverage_error,"
v1.0.2,))
v1.0.2,: The label ranking loss (APS)
v1.0.2,label_ranking_average_precision_score: float = field(metadata=dict(
v1.0.2,"doc='The label ranking loss (APS)',"
v1.0.2,"f=metrics.label_ranking_average_precision_score,"
v1.0.2,))
v1.0.2,#: The label ranking loss
v1.0.2,label_ranking_loss: float = field(metadata=dict(
v1.0.2,"doc='The label ranking loss',"
v1.0.2,"f=metrics.label_ranking_loss,"
v1.0.2,))
v1.0.2,Transfer to cpu and convert to numpy
v1.0.2,Ensure that each key gets counted only once
v1.0.2,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.0.2,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.0.2,masks using the exact same key order.
v1.0.2,TODO how to define a cutoff on y_scores to make binary?
v1.0.2,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.0.2,Clear buffers
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: A mapping of evaluators' names to their implementations
v1.0.2,: A mapping of results' names to their implementations
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,The best rank is the rank when assuming all options with an equal score are placed behind the currently
v1.0.2,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.0.2,The worst rank is the rank when assuming all options with an equal score are placed in front of the currently
v1.0.2,"considered. Hence, the rank is the number of options which have at least the same score minus one (as the"
v1.0.2,"currently considered option in included in all options). As the rank is one-based, we have to add 1, which"
v1.0.2,"nullifies the ""minus 1"" from before."
v1.0.2,"The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of"
v1.0.2,the elements with the same score as the currently considered option.
v1.0.2,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.0.2,The expected rank of a random scoring
v1.0.2,The adjusted ranks is normalized by the expected rank of a random scoring
v1.0.2,TODO adjusted_worst_rank
v1.0.2,TODO adjusted_best_rank
v1.0.2,: The mean over all ranks: mean_i r_i. Lower is better.
v1.0.2,: The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.
v1.0.2,": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k."
v1.0.2,: Higher is better.
v1.0.2,: The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.
v1.0.2,: Described by [berrendorf2020]_.
v1.0.2,Clear buffers
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.0.2,Create a tensor of all IDs
v1.0.2,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.0.2,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.0.2,: A dictionary of hyper-parameters to the models that use them
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The default loss function class
v1.0.2,: The default parameters for the default loss function class
v1.0.2,: The instance of the loss
v1.0.2,: The default regularizer class
v1.0.2,: The default parameters for the default regularizer class
v1.0.2,: The instance of the regularizer
v1.0.2,Initialize the device
v1.0.2,Random seeds have to set before the embeddings are initialized
v1.0.2,Loss
v1.0.2,TODO: Check loss functions that require 1 and -1 as label but only
v1.0.2,Regularizer
v1.0.2,The triples factory facilitates access to the dataset.
v1.0.2,This allows to store the optimized parameters
v1.0.2,Keep track of the hyper-parameters that are used across all
v1.0.2,subclasses of BaseModule
v1.0.2,Enforce evaluation mode
v1.0.2,Enforce evaluation mode
v1.0.2,Enforce evaluation mode
v1.0.2,Enforce evaluation mode
v1.0.2,The number of relations stored in the triples factory includes the number of inverse relations
v1.0.2,Id of inverse relation: relation + 1
v1.0.2,"The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs"
v1.0.2,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.0.2,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.2,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.0.2,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.0.2,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.2,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.0.2,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.0.2,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.2,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.0.2,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.0.2,Default for relation dimensionality
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: A mapping of models' names to their implementations
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Store initial input for error message
v1.0.2,All are None
v1.0.2,"input_channels is None, and any of height or width is None -> set input_channels=1"
v1.0.2,"input channels is not None, and one of height or width is None"
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The default loss function class
v1.0.2,: The default parameters for the default loss function class
v1.0.2,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.0.2,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.0.2,ConvE should be trained with inverse triples
v1.0.2,ConvE uses one bias for each entity
v1.0.2,Automatic calculation of remaining dimensions
v1.0.2,Parameter need to fulfil:
v1.0.2,input_channels * embedding_height * embedding_width = embedding_dim
v1.0.2,Finalize initialization
v1.0.2,embeddings
v1.0.2,weights
v1.0.2,"batch_size, num_input_channels, 2*height, width"
v1.0.2,"batch_size, num_input_channels, 2*height, width"
v1.0.2,"batch_size, num_input_channels, 2*height, width"
v1.0.2,"(N,C_out,H_out,W_out)"
v1.0.2,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.0.2,Embedding Regularization
v1.0.2,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.0.2,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.2,Embedding Regularization
v1.0.2,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.2,Embedding Regularization
v1.0.2,Code to repeat each item successively instead of the entire tensor
v1.0.2,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,Embeddings
v1.0.2,Finalize initialization
v1.0.2,Initialise left relation embeddings to unit length
v1.0.2,Make sure to call super first
v1.0.2,Normalise embeddings of entities
v1.0.2,Get embeddings
v1.0.2,Project entities
v1.0.2,Get embeddings
v1.0.2,Project entities
v1.0.2,Project entities
v1.0.2,Project entities
v1.0.2,Get embeddings
v1.0.2,Project entities
v1.0.2,Project entities
v1.0.2,Project entities
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The default loss function class
v1.0.2,: The default parameters for the default loss function class
v1.0.2,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.0.2,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.0.2,Finalize initialization
v1.0.2,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.0.2,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.0.2,split into real and imaginary part
v1.0.2,ComplEx space bilinear product
v1.0.2,*: Elementwise multiplication
v1.0.2,get embeddings
v1.0.2,Compute scores
v1.0.2,Regularization
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The regularizer used by [nickel2011]_ for for RESCAL
v1.0.2,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.0.2,: a normalized weight of 10 is used.
v1.0.2,: The LP settings used by [nickel2011]_ for for RESCAL
v1.0.2,Finalize initialization
v1.0.2,Get embeddings
v1.0.2,"shape: (b, d)"
v1.0.2,"shape: (b, d, d)"
v1.0.2,"shape: (b, d)"
v1.0.2,Compute scores
v1.0.2,Regularization
v1.0.2,Compute scores
v1.0.2,Regularization
v1.0.2,Get embeddings
v1.0.2,Compute scores
v1.0.2,Regularization
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,Finalize initialization
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.0.2,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.0.2,The interaction model
v1.0.2,Finalize initialization
v1.0.2,embeddings
v1.0.2,Use Xavier initialization for weight; bias to zero
v1.0.2,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.0.2,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.0.2,Output layer regularization
v1.0.2,In the code base only the weights of the output layer are used for regularization
v1.0.2,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.0.2,Stack to convolution input
v1.0.2,Convolution
v1.0.2,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.0.2,Linear layer for final scores
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,Finalize initialization
v1.0.2,": shape: (batch_size, num_entities, d)"
v1.0.2,": Prepare h: (b, e, d) -> (b, e, 1, 1, d)"
v1.0.2,": Prepare t: (b, e, d) -> (b, e, 1, d, 1)"
v1.0.2,": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)"
v1.0.2,"h.T @ W @ t, shape: (b, e, k, 1, 1)"
v1.0.2,": reduce (b, e, k, 1, 1) -> (b, e, k)"
v1.0.2,": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.2,": Prepare h: (b, e, d) -> (b, e, d, 1)"
v1.0.2,"V_h @ h, shape: (b, e, k, 1)"
v1.0.2,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.2,": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.2,": Prepare t: (b, e, d) -> (b, e, d, 1)"
v1.0.2,"V_t @ t, shape: (b, e, k, 1)"
v1.0.2,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.2,": Prepare b: (R, k) -> (b, k) -> (b, 1, k)"
v1.0.2,"a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)"
v1.0.2,"prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)"
v1.0.2,"prepare act: (b, e, k) -> (b, e, 1, k)"
v1.0.2,"compute score, shape: (b, e, 1, 1)"
v1.0.2,reduce
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The regularizer used by [yang2014]_ for DistMult
v1.0.2,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.0.2,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.0.2,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.0.2,: The LP settings used by [yang2014]_ for DistMult
v1.0.2,Finalize initialization
v1.0.2,"xavier uniform, cf."
v1.0.2,https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17
v1.0.2,Initialise relation embeddings to unit length
v1.0.2,Make sure to call super first
v1.0.2,Normalize embeddings of entities
v1.0.2,Bilinear product
v1.0.2,*: Elementwise multiplication
v1.0.2,Get embeddings
v1.0.2,Compute score
v1.0.2,Only regularize relation embeddings
v1.0.2,Get embeddings
v1.0.2,Rank against all entities
v1.0.2,Only regularize relation embeddings
v1.0.2,Get embeddings
v1.0.2,Rank against all entities
v1.0.2,Only regularize relation embeddings
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,Similarity function used for distributions
v1.0.2,element-wise covariance bounds
v1.0.2,Additional covariance embeddings
v1.0.2,Finalize initialization
v1.0.2,Constraints are applied through post_parameter_update
v1.0.2,Make sure to call super first
v1.0.2,Normalize entity embeddings
v1.0.2,Ensure positive definite covariances matrices and appropriate size by clamping
v1.0.2,Get embeddings
v1.0.2,Compute entity distribution
v1.0.2,: a = \mu^T\Sigma^{-1}\mu
v1.0.2,: b = \log \det \Sigma
v1.0.2,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.0.2,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.0.2,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.0.2,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The custom regularizer used by [wang2014]_ for TransH
v1.0.2,: The settings used by [wang2014]_ for TransH
v1.0.2,embeddings
v1.0.2,Finalize initialization
v1.0.2,TODO: Add initialization
v1.0.2,Make sure to call super first
v1.0.2,Normalise the normal vectors by their l2 norms
v1.0.2,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.0.2,which enforces the defined soft constraints.
v1.0.2,Get embeddings
v1.0.2,Project to hyperplane
v1.0.2,Regularization term
v1.0.2,Get embeddings
v1.0.2,Project to hyperplane
v1.0.2,Regularization term
v1.0.2,Get embeddings
v1.0.2,Project to hyperplane
v1.0.2,Regularization term
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,embeddings
v1.0.2,Finalize initialization
v1.0.2,Make sure to call super first
v1.0.2,Normalize entity embeddings
v1.0.2,TODO: Initialize from TransE
v1.0.2,Initialise relation embeddings to unit length
v1.0.2,"project to relation specific subspace, shape: (b, e, d_r)"
v1.0.2,ensure constraints
v1.0.2,"evaluate score function, shape: (b, e)"
v1.0.2,Get embeddings
v1.0.2,Get embeddings
v1.0.2,Get embeddings
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,Construct node neighbourhood mask
v1.0.2,Set nodes in batch to true
v1.0.2,Compute k-neighbourhood
v1.0.2,"if the target node needs an embeddings, so does the source node"
v1.0.2,Create edge mask
v1.0.2,pylint: disable=unused-argument
v1.0.2,"Calculate in-degree, i.e. number of incoming edges"
v1.0.2,pylint: disable=unused-argument
v1.0.2,"Calculate in-degree, i.e. number of incoming edges"
v1.0.2,: Interaction model used as decoder
v1.0.2,: The blocks of the relation-specific weight matrices
v1.0.2,": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)"
v1.0.2,: The base weight matrices to generate relation-specific weights
v1.0.2,": shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.2,: The relation-specific weights for each base
v1.0.2,": shape: (num_relations, num_bases)"
v1.0.2,: The biases for each layer (if used)
v1.0.2,": shape of each element: (embedding_dim,)"
v1.0.2,: Batch normalization for each layer (if used)
v1.0.2,: Activations for each layer (if used)
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,Instantiate model
v1.0.2,Heuristic
v1.0.2,buffering of messages
v1.0.2,"Save graph using buffers, such that the tensors are moved together with the model"
v1.0.2,Weights
v1.0.2,Finalize initialization
v1.0.2,invalidate enriched embeddings
v1.0.2,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.0.2,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.0.2,initialized properly)
v1.0.2,We have one additional relation for self-loops
v1.0.2,Xavier Glorot initialization of each block
v1.0.2,Reset biases
v1.0.2,Reset batch norm parameters
v1.0.2,"Reset activation parameters, if any"
v1.0.2,use buffered messages if applicable
v1.0.2,Bind fields
v1.0.2,"shape: (num_entities, embedding_dim)"
v1.0.2,Edge dropout: drop the same edges on all layers (only in training mode)
v1.0.2,Get random dropout mask
v1.0.2,Apply to edges
v1.0.2,Different dropout for self-loops (only in training mode)
v1.0.2,"If batch is given, compute (num_layers)-hop neighbourhood"
v1.0.2,Initialize embeddings in the next layer for all nodes
v1.0.2,TODO: Can we vectorize this loop?
v1.0.2,Choose the edges which are of the specific relation
v1.0.2,Only propagate messages on subset of edges
v1.0.2,No edges available? Skip rest of inner loop
v1.0.2,Get source and target node indices
v1.0.2,send messages in both directions
v1.0.2,Select source node embeddings
v1.0.2,get relation weights
v1.0.2,Compute message (b x d) * (d x d) = (b x d)
v1.0.2,Normalize messages by relation-specific in-degree
v1.0.2,Aggregate messages in target
v1.0.2,Self-loop
v1.0.2,"Apply bias, if requested"
v1.0.2,"Apply batch normalization, if requested"
v1.0.2,Apply non-linearity
v1.0.2,allocate weight
v1.0.2,Get blocks
v1.0.2,"self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)"
v1.0.2,note: embedding_dim is guaranteed to be divisible by num_bases in the constructor
v1.0.2,"The current basis weights, shape: (num_bases)"
v1.0.2,"the current bases, shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.2,"compute the current relation weights, shape: (embedding_dim, embedding_dim)"
v1.0.2,Enrich embeddings
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The default loss function class
v1.0.2,: The default parameters for the default loss function class
v1.0.2,Core tensor
v1.0.2,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.0.2,Dropout
v1.0.2,Finalize initialization
v1.0.2,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.0.2,Abbreviation
v1.0.2,Compute h_n = DO(BN(h))
v1.0.2,Compute wr = DO(W x_2 r)
v1.0.2,compute whr = DO(BN(h_n x_1 wr))
v1.0.2,Compute whr x_3 t
v1.0.2,Get embeddings
v1.0.2,Compute scores
v1.0.2,Get embeddings
v1.0.2,Compute scores
v1.0.2,Get embeddings
v1.0.2,Compute scores
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,Finalize initialization
v1.0.2,Initialise relation embeddings to unit length
v1.0.2,Make sure to call super first
v1.0.2,Normalize entity embeddings
v1.0.2,Get embeddings
v1.0.2,Get embeddings
v1.0.2,Get embeddings
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The default loss function class
v1.0.2,: The default parameters for the default loss function class
v1.0.2,: The regularizer used by [trouillon2016]_ for SimplE
v1.0.2,": In the paper, they use weight of 0.1, and do not normalize the"
v1.0.2,": regularization term by the number of elements, which is 200."
v1.0.2,: The power sum settings used by [trouillon2016]_ for SimplE
v1.0.2,extra embeddings
v1.0.2,Finalize initialization
v1.0.2,forward model
v1.0.2,Regularization
v1.0.2,backward model
v1.0.2,Regularization
v1.0.2,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.0.2,"That is not mentioned in the paper, so it is omitted here."
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,Finalize initialization
v1.0.2,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.0.2,weight initialization
v1.0.2,Get embeddings
v1.0.2,Embedding Regularization
v1.0.2,Concatenate them
v1.0.2,Compute scores
v1.0.2,Get embeddings
v1.0.2,Embedding Regularization
v1.0.2,First layer can be unrolled
v1.0.2,Send scores through rest of the network
v1.0.2,Get embeddings
v1.0.2,Embedding Regularization
v1.0.2,First layer can be unrolled
v1.0.2,Send scores through rest of the network
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,The dimensions affected by e'
v1.0.2,Project entities
v1.0.2,r_p (e_p.T e) + e'
v1.0.2,Enforce constraints
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,Finalize initialization
v1.0.2,Make sure to call super first
v1.0.2,Normalize entity embeddings
v1.0.2,Project entities
v1.0.2,score = -||h_bot + r - t_bot||_2^2
v1.0.2,Head
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,Finalize initialization
v1.0.2,phases randomly between 0 and 2 pi
v1.0.2,Make sure to call super first
v1.0.2,Normalize relation embeddings
v1.0.2,Decompose into real and imaginary part
v1.0.2,Rotate (=Hadamard product in complex space).
v1.0.2,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.0.2,Get embeddings
v1.0.2,Compute scores
v1.0.2,Embedding Regularization
v1.0.2,Get embeddings
v1.0.2,Rank against all entities
v1.0.2,Compute scores
v1.0.2,Embedding Regularization
v1.0.2,Get embeddings
v1.0.2,r expresses a rotation in complex plane.
v1.0.2,The inverse rotation is expressed by the complex conjugate of r.
v1.0.2,The score is computed as the distance of the relation-rotated head to the tail.
v1.0.2,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.0.2,|h * r - t| = |h - conj(r) * t|
v1.0.2,Rank against all entities
v1.0.2,Compute scores
v1.0.2,Embedding Regularization
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The default loss function class
v1.0.2,: The default parameters for the default loss function class
v1.0.2,Global entity projection
v1.0.2,Global relation projection
v1.0.2,Global combination bias
v1.0.2,Global combination bias
v1.0.2,Finalize initialization
v1.0.2,Get embeddings
v1.0.2,Compute score
v1.0.2,Get embeddings
v1.0.2,Rank against all entities
v1.0.2,Get embeddings
v1.0.2,Rank against all entities
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,Finalize initialization
v1.0.2,Make sure to call super first
v1.0.2,Normalize entity embeddings
v1.0.2,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.0.2,Circular correlation of entity embeddings
v1.0.2,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.0.2,Hadamard product in frequency domain
v1.0.2,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.0.2,inner product with relation embedding
v1.0.2,Embedding Regularization
v1.0.2,Embedding Regularization
v1.0.2,Embedding Regularization
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The default loss function class
v1.0.2,: The default parameters for the default loss function class
v1.0.2,Finalize initialization
v1.0.2,Get embeddings
v1.0.2,Embedding Regularization
v1.0.2,Concatenate them
v1.0.2,Predict t embedding
v1.0.2,compare with all t's
v1.0.2,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.2,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.2,Embedding Regularization
v1.0.2,Concatenate them
v1.0.2,Predict t embedding
v1.0.2,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.2,Embedding Regularization
v1.0.2,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.0.2,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.0.2,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.0.2,Extend t
v1.0.2,Concatenate them
v1.0.2,Predict t embedding
v1.0.2,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.2,The results have to be realigned with the expected output of the score_h function
v1.0.2,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,TODO: Check entire build of the model
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The default loss function class
v1.0.2,: The default parameters for the default loss function class
v1.0.2,Literal
v1.0.2,num_ent x num_lit
v1.0.2,Number of columns corresponds to number of literals
v1.0.2,Literals
v1.0.2,End literals
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,TODO: Check entire build of the model
v1.0.2,: The default strategy for optimizing the model's hyper-parameters
v1.0.2,: The default parameters for the default loss function class
v1.0.2,Embeddings
v1.0.2,Number of columns corresponds to number of literals
v1.0.2,apply dropout
v1.0.2,"-, because lower score shall correspond to a more plausible triple."
v1.0.2,TODO check if this is the same as the BaseModule
v1.0.2,Choose y = -1 since a smaller score is better.
v1.0.2,"In TransE for example, the scores represent distances"
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.2,Bind number of negatives to sample
v1.0.2,Equally corrupt head and tail
v1.0.2,Copy positive batch for corruption.
v1.0.2,"Do not detach, as no gradients should flow into the indices."
v1.0.2,Sample random entities as replacement
v1.0.2,Replace heads – To make sure we don't replace the head by the original value
v1.0.2,we shift all values greater or equal than the original value by one up
v1.0.2,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.2,Corrupt tails
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: A mapping of negative samplers' names to their implementations
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.2,Preprocessing: Compute corruption probabilities
v1.0.2,"compute tph, i.e. the average number of tail entities per head"
v1.0.2,"compute hpt, i.e. the average number of head entities per tail"
v1.0.2,Set parameter for Bernoulli distribution
v1.0.2,Bind number of negatives to sample
v1.0.2,Copy positive batch for corruption.
v1.0.2,"Do not detach, as no gradients should flow into the indices."
v1.0.2,Decide whether to corrupt head or tail
v1.0.2,Tails are corrupted if heads are not corrupted
v1.0.2,Randomly sample corruption
v1.0.2,Replace heads – To make sure we don't replace the head by the original value
v1.0.2,we shift all values greater or equal than the original value by one up
v1.0.2,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.2,Replace tails
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,TODO what happens if already exists?
v1.0.2,TODO incorporate setting of random seed
v1.0.2,pipeline_kwargs=dict(
v1.0.2,"random_seed=random.randint(1, 2 ** 32 - 1),"
v1.0.2,"),"
v1.0.2,Add dataset to current_pipeline
v1.0.2,Add loss function to current_pipeline
v1.0.2,Add regularizer to current_pipeline
v1.0.2,Add optimizer to current_pipeline
v1.0.2,Add training approach to current_pipeline
v1.0.2,Add training kwargs and kwargs_ranges
v1.0.2,Add evaluation
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: A factory wrapping the training triples
v1.0.2,": A factory wrapping the testing triples, that share indexes with the training triples"
v1.0.2,": A factory wrapping the validation triples, that share indexes with the training triples"
v1.0.2,: All data sets should take care of inverse triple creation
v1.0.2,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.0.2,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.0.2,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.0.2,don't call this function by itself. assumes called through the `validation`
v1.0.2,property and the _training factory has already been loaded
v1.0.2,see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content
v1.0.2,pattern from https://stackoverflow.com/a/39217788/5775947
v1.0.2,TODO replace this with the new zip remote dataset class
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: A mapping of datasets' names to their classes
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,TODO update docs with table and CLI wtih generator
v1.0.2,: A mapping of HPO samplers' names to their implementations
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,1. Dataset
v1.0.2,2. Model
v1.0.2,3. Loss
v1.0.2,4. Regularizer
v1.0.2,5. Optimizer
v1.0.2,6. Training Loop
v1.0.2,7. Training
v1.0.2,8. Evaluation
v1.0.2,9. MLFlow
v1.0.2,Misc.
v1.0.2,2. Model
v1.0.2,3. Loss
v1.0.2,4. Regularizer
v1.0.2,5. Optimizer
v1.0.2,1. Dataset
v1.0.2,2. Model
v1.0.2,3. Loss
v1.0.2,4. Regularizer
v1.0.2,5. Optimizer
v1.0.2,6. Training Loop
v1.0.2,7. Training
v1.0.2,8. Evaluation
v1.0.2,9. MLFlow
v1.0.2,Misc.
v1.0.2,Will trigger Optuna to set the state of the trial as failed
v1.0.2,: The :mod:`optuna` study object
v1.0.2,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.0.2,Output study information
v1.0.2,Output all trials
v1.0.2,Output best trial as pipeline configuration file
v1.0.2,1. Dataset
v1.0.2,2. Model
v1.0.2,3. Loss
v1.0.2,4. Regularizer
v1.0.2,5. Optimizer
v1.0.2,6. Training Loop
v1.0.2,7. Training
v1.0.2,8. Evaluation
v1.0.2,MLFlow
v1.0.2,6. Misc
v1.0.2,Optuna Study Settings
v1.0.2,Optuna Optimization Settings
v1.0.2,0. Metadata/Provenance
v1.0.2,1. Dataset
v1.0.2,FIXME difference between dataset class and string
v1.0.2,FIXME how to handle if dataset or factories were set? Should have been
v1.0.2,part of https://github.com/mali-git/POEM_develop/pull/483
v1.0.2,2. Model
v1.0.2,3. Loss
v1.0.2,4. Regularizer
v1.0.2,5. Optimizer
v1.0.2,6. Training Loop
v1.0.2,7. Training
v1.0.2,8. Evaluation
v1.0.2,1. Dataset
v1.0.2,2. Model
v1.0.2,3. Loss
v1.0.2,4. Regularizer
v1.0.2,5. Optimizer
v1.0.2,6. Training Loop
v1.0.2,7. Training
v1.0.2,8. Evaluation
v1.0.2,9. MLFlow
v1.0.2,Optuna Misc.
v1.0.2,Pipeline Misc.
v1.0.2,Invoke optimization of the objective function.
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,-*- coding: utf-8 -*-
v1.0.2,: A mapping of HPO pruners' names to their implementations
v1.0.2,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,
v1.0.1,Configuration file for the Sphinx documentation builder.
v1.0.1,
v1.0.1,This file does only contain a selection of the most common options. For a
v1.0.1,full list see the documentation:
v1.0.1,http://www.sphinx-doc.org/en/master/config
v1.0.1,-- Path setup --------------------------------------------------------------
v1.0.1,"If extensions (or modules to document with autodoc) are in another directory,"
v1.0.1,add these directories to sys.path here. If the directory is relative to the
v1.0.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.0.1,
v1.0.1,"sys.path.insert(0, os.path.abspath('..'))"
v1.0.1,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.0.1,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.0.1,from unittest.mock import Mock
v1.0.1,sys.modules['numpy'] = Mock()
v1.0.1,sys.modules['numpy.linalg'] = Mock()
v1.0.1,sys.modules['scipy'] = Mock()
v1.0.1,sys.modules['scipy.optimize'] = Mock()
v1.0.1,sys.modules['scipy.interpolate'] = Mock()
v1.0.1,sys.modules['scipy.sparse'] = Mock()
v1.0.1,sys.modules['scipy.ndimage'] = Mock()
v1.0.1,sys.modules['scipy.ndimage.filters'] = Mock()
v1.0.1,sys.modules['tensorflow'] = Mock()
v1.0.1,sys.modules['theano'] = Mock()
v1.0.1,sys.modules['theano.tensor'] = Mock()
v1.0.1,sys.modules['torch'] = Mock()
v1.0.1,sys.modules['torch.optim'] = Mock()
v1.0.1,sys.modules['torch.nn'] = Mock()
v1.0.1,sys.modules['torch.nn.init'] = Mock()
v1.0.1,sys.modules['torch.autograd'] = Mock()
v1.0.1,sys.modules['sklearn'] = Mock()
v1.0.1,sys.modules['sklearn.model_selection'] = Mock()
v1.0.1,sys.modules['sklearn.utils'] = Mock()
v1.0.1,-- Project information -----------------------------------------------------
v1.0.1,"The full version, including alpha/beta/rc tags."
v1.0.1,The short X.Y version.
v1.0.1,-- General configuration ---------------------------------------------------
v1.0.1,"If your documentation needs a minimal Sphinx version, state it here."
v1.0.1,
v1.0.1,needs_sphinx = '1.0'
v1.0.1,"If true, the current module name will be prepended to all description"
v1.0.1,unit titles (such as .. function::).
v1.0.1,A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)
v1.0.1,"Add any Sphinx extension module names here, as strings. They can be"
v1.0.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.0.1,ones.
v1.0.1,generate autosummary pages
v1.0.1,"Add any paths that contain templates here, relative to this directory."
v1.0.1,The suffix(es) of source filenames.
v1.0.1,You can specify multiple suffix as a list of string:
v1.0.1,
v1.0.1,"source_suffix = ['.rst', '.md']"
v1.0.1,The master toctree document.
v1.0.1,The language for content autogenerated by Sphinx. Refer to documentation
v1.0.1,for a list of supported languages.
v1.0.1,
v1.0.1,This is also used if you do content translation via gettext catalogs.
v1.0.1,"Usually you set ""language"" from the command line for these cases."
v1.0.1,"List of patterns, relative to source directory, that match files and"
v1.0.1,directories to ignore when looking for source files.
v1.0.1,This pattern also affects html_static_path and html_extra_path.
v1.0.1,The name of the Pygments (syntax highlighting) style to use.
v1.0.1,-- Options for HTML output -------------------------------------------------
v1.0.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.0.1,a list of builtin themes.
v1.0.1,
v1.0.1,Theme options are theme-specific and customize the look and feel of a theme
v1.0.1,"further.  For a list of options available for each theme, see the"
v1.0.1,documentation.
v1.0.1,
v1.0.1,html_theme_options = {}
v1.0.1,"Add any paths that contain custom static files (such as style sheets) here,"
v1.0.1,"relative to this directory. They are copied after the builtin static files,"
v1.0.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.0.1,html_static_path = ['_static']
v1.0.1,"Custom sidebar templates, must be a dictionary that maps document names"
v1.0.1,to template names.
v1.0.1,
v1.0.1,The default sidebars (for documents that don't match any pattern) are
v1.0.1,defined by theme itself.  Builtin themes are using these templates by
v1.0.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.0.1,'searchbox.html']``.
v1.0.1,
v1.0.1,html_sidebars = {}
v1.0.1,The name of an image file (relative to this directory) to place at the top
v1.0.1,of the sidebar.
v1.0.1,
v1.0.1,-- Options for HTMLHelp output ---------------------------------------------
v1.0.1,Output file base name for HTML help builder.
v1.0.1,-- Options for LaTeX output ------------------------------------------------
v1.0.1,The paper size ('letterpaper' or 'a4paper').
v1.0.1,
v1.0.1,"'papersize': 'letterpaper',"
v1.0.1,"The font size ('10pt', '11pt' or '12pt')."
v1.0.1,
v1.0.1,"'pointsize': '10pt',"
v1.0.1,Additional stuff for the LaTeX preamble.
v1.0.1,
v1.0.1,"'preamble': '',"
v1.0.1,Latex figure (float) alignment
v1.0.1,
v1.0.1,"'figure_align': 'htbp',"
v1.0.1,Grouping the document tree into LaTeX files. List of tuples
v1.0.1,"(source start file, target name, title,"
v1.0.1,"author, documentclass [howto, manual, or own class])."
v1.0.1,-- Options for manual page output ------------------------------------------
v1.0.1,One entry per manual page. List of tuples
v1.0.1,"(source start file, name, description, authors, manual section)."
v1.0.1,-- Options for Texinfo output ----------------------------------------------
v1.0.1,Grouping the document tree into Texinfo files. List of tuples
v1.0.1,"(source start file, target name, title, author,"
v1.0.1,"dir menu entry, description, category)"
v1.0.1,-- Options for Epub output -------------------------------------------------
v1.0.1,Bibliographic Dublin Core info.
v1.0.1,The unique identifier of the text. This can be a ISBN number
v1.0.1,or the project homepage.
v1.0.1,
v1.0.1,epub_identifier = ''
v1.0.1,A unique identification for the text.
v1.0.1,
v1.0.1,epub_uid = ''
v1.0.1,A list of files that should not be packed into the epub file.
v1.0.1,-- Extension configuration -------------------------------------------------
v1.0.1,-- Options for intersphinx extension ---------------------------------------
v1.0.1,Example configuration for intersphinx: refer to the Python standard library.
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Check a model param is optimized
v1.0.1,Check a loss param is optimized
v1.0.1,Check a model param is NOT optimized
v1.0.1,Check a loss param is optimized
v1.0.1,Check a model param is optimized
v1.0.1,Check a loss param is NOT optimized
v1.0.1,Check a model param is NOT optimized
v1.0.1,Check a loss param is NOT optimized
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,check for empty batches
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,The triples factory and model
v1.0.1,: The evaluator to be tested
v1.0.1,Settings
v1.0.1,: The evaluator instantiation
v1.0.1,Settings
v1.0.1,Initialize evaluator
v1.0.1,Use small test dataset
v1.0.1,Use small model (untrained)
v1.0.1,Get batch
v1.0.1,Compute scores
v1.0.1,Compute mask only if required
v1.0.1,TODO: Re-use filtering code
v1.0.1,"shape: (batch_size, num_triples)"
v1.0.1,"shape: (batch_size, num_entities)"
v1.0.1,Process one batch
v1.0.1,Check for correct class
v1.0.1,Check value ranges
v1.0.1,TODO: Validate with data?
v1.0.1,Check for correct class
v1.0.1,check value
v1.0.1,filtering
v1.0.1,"true_score: (2, 3, 3)"
v1.0.1,head based filter
v1.0.1,preprocessing for faster lookup
v1.0.1,check that all found positives are positive
v1.0.1,check in-place
v1.0.1,Test head scores
v1.0.1,Assert in-place modification
v1.0.1,Assert correct filtering
v1.0.1,Test tail scores
v1.0.1,Assert in-place modification
v1.0.1,Assert correct filtering
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The batch size
v1.0.1,: The triples factory
v1.0.1,: Class of regularizer to test
v1.0.1,: The constructor parameters to pass to the regularizer
v1.0.1,": The regularizer instance, initialized in setUp"
v1.0.1,: A positive batch
v1.0.1,: The device
v1.0.1,Use RESCAL as it regularizes multiple tensors of different shape.
v1.0.1,Check if regularizer is stored correctly.
v1.0.1,Forward pass (should update regularizer)
v1.0.1,Call post_parameter_update (should reset regularizer)
v1.0.1,Check if regularization term is reset
v1.0.1,Call method
v1.0.1,Generate random tensors
v1.0.1,Call update
v1.0.1,check shape
v1.0.1,compute expected term
v1.0.1,Generate random tensor
v1.0.1,calculate penalty
v1.0.1,check shape
v1.0.1,check value
v1.0.1,Tests that exception will be thrown when more than or less than three tensors are passed
v1.0.1,Test that regularization term is computed correctly
v1.0.1,Entity soft constraint
v1.0.1,Orthogonality soft constraint
v1.0.1,"After first update, should change the term"
v1.0.1,"After second update, no change should happen"
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The number of embeddings
v1.0.1,: The embedding dimension
v1.0.1,check shape
v1.0.1,check values
v1.0.1,check shape
v1.0.1,check values
v1.0.1,check correct value range
v1.0.1,check maximum norm constraint
v1.0.1,unchanged values for small norms
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The window size used by the early stopper
v1.0.1,: The mock losses the mock evaluator will return
v1.0.1,: The (zeroed) index  - 1 at which stopping will occur
v1.0.1,: The minimum improvement
v1.0.1,Set automatic_memory_optimization to false for tests
v1.0.1,Step early stopper
v1.0.1,check storing of results
v1.0.1,check ring buffer
v1.0.1,: The window size used by the early stopper
v1.0.1,: The (zeroed) index  - 1 at which stopping will occur
v1.0.1,: The minimum improvement
v1.0.1,: The random seed to use for reproducibility
v1.0.1,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.0.1,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.0.1,: The batch size to use.
v1.0.1,Fix seed for reproducibility
v1.0.1,Set automatic_memory_optimization to false during testing
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Check if multilabels are working correctly
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The batch size
v1.0.1,: The random seed
v1.0.1,: The triples factory
v1.0.1,: The sLCWA instances
v1.0.1,: Class of negative sampling to test
v1.0.1,": The negative sampler instance, initialized in setUp"
v1.0.1,: A positive batch
v1.0.1,Generate negative sample
v1.0.1,check shape
v1.0.1,check bounds: heads
v1.0.1,check bounds: relations
v1.0.1,check bounds: tails
v1.0.1,Check that all elements got corrupted
v1.0.1,Generate scaled negative sample
v1.0.1,Generate negative samples
v1.0.1,test that the relations were not changed
v1.0.1,Test that half of the subjects and half of the objects are corrupted
v1.0.1,Generate negative sample for additional tests
v1.0.1,test that the relations were not changed
v1.0.1,sample a batch
v1.0.1,check shape
v1.0.1,get triples
v1.0.1,check connected components
v1.0.1,super inefficient
v1.0.1,join
v1.0.1,already joined
v1.0.1,check that there is only a single component
v1.0.1,check content of comp_adj_lists
v1.0.1,check edge ids
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The expected number of entities
v1.0.1,: The expected number of relations
v1.0.1,: The dataset to test
v1.0.1,Not loaded
v1.0.1,Load
v1.0.1,Test caching
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The class of the model to test
v1.0.1,: Additional arguments passed to the model's constructor method
v1.0.1,: The triples factory instance
v1.0.1,: The model instance
v1.0.1,: The batch size for use for forward_* tests
v1.0.1,: The embedding dimensionality
v1.0.1,: Whether to create inverse triples (needed e.g. by ConvE)
v1.0.1,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.0.1,: The batch size for use when testing training procedures
v1.0.1,: The number of epochs to train the model
v1.0.1,: A random number generator from torch
v1.0.1,: The number of parameters which receive a constant (i.e. non-randomized)
v1.0.1,initialization
v1.0.1,assert there is at least one trainable parameter
v1.0.1,Check that all the parameters actually require a gradient
v1.0.1,Try to initialize an optimizer
v1.0.1,get model parameters
v1.0.1,re-initialize
v1.0.1,check that the operation works in-place
v1.0.1,check that the parameters where modified
v1.0.1,check for finite values by default
v1.0.1,check whether a gradient can be back-propgated
v1.0.1,"assert batch comprises (head, relation) pairs"
v1.0.1,"assert batch comprises (relation, tail) pairs"
v1.0.1,TODO: Catch HolE MKL error?
v1.0.1,set regularizer term
v1.0.1,call post_parameter_update
v1.0.1,assert that the regularization term has been reset
v1.0.1,do one optimization step
v1.0.1,call post_parameter_update
v1.0.1,check model constraints
v1.0.1,"assert batch comprises (relation, tail) pairs"
v1.0.1,"assert batch comprises (relation, tail) pairs"
v1.0.1,"assert batch comprises (relation, tail) pairs"
v1.0.1,Distance-based model
v1.0.1,3x batch norm: bias + scale --> 6
v1.0.1,entity specific bias        --> 1
v1.0.1,==================================
v1.0.1,7
v1.0.1,"two bias terms, one conv-filter"
v1.0.1,Two linear layer biases
v1.0.1,"Two BN layers, bias & scale"
v1.0.1,: one bias per layer
v1.0.1,: (scale & bias for BN) * layers
v1.0.1,entity embeddings
v1.0.1,relation embeddings
v1.0.1,Compute Scores
v1.0.1,"self.assertAlmostEqual(second_score, -16, delta=0.01)"
v1.0.1,Use different dimension for relation embedding: relation_dim > entity_dim
v1.0.1,relation embeddings
v1.0.1,Compute Scores
v1.0.1,Use different dimension for relation embedding: relation_dim < entity_dim
v1.0.1,entity embeddings
v1.0.1,relation embeddings
v1.0.1,Compute Scores
v1.0.1,random entity embeddings & projections
v1.0.1,random relation embeddings & projections
v1.0.1,project
v1.0.1,check shape:
v1.0.1,check normalization
v1.0.1,entity embeddings
v1.0.1,relation embeddings
v1.0.1,Compute Scores
v1.0.1,second_score = scores[1].item()
v1.0.1,: 2xBN (bias & scale)
v1.0.1,check shape
v1.0.1,check content
v1.0.1,: The number of entities
v1.0.1,: The number of triples
v1.0.1,check shape
v1.0.1,check dtype
v1.0.1,check finite values (e.g. due to division by zero)
v1.0.1,check non-negativity
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,
v1.0.1,
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,check for finite values by default
v1.0.1,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.1,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.1,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.1,Get embeddings
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The class
v1.0.1,: Constructor keyword arguments
v1.0.1,: The loss instance
v1.0.1,: The batch size
v1.0.1,test reduction
v1.0.1,Test backward
v1.0.1,: The number of entities.
v1.0.1,: The number of negative samples
v1.0.1,≈ result of softmax
v1.0.1,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.0.1,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.0.1,"pos_distances = [0., 0., 0.5, 0.5]"
v1.0.1,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.0.1,≈ result of sigmoid
v1.0.1,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.0.1,expected_loss ≈ 0.34
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Create dummy dense labels
v1.0.1,Check if labels form a probability distribution
v1.0.1,Apply label smoothing
v1.0.1,Check if smooth labels form probability distribution
v1.0.1,Create dummy sLCWA labels
v1.0.1,Apply label smoothing
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: A mapping of optimizers' names to their implementations
v1.0.1,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,"scale labels from [0, 1] to [-1, 1]"
v1.0.1,cross entropy expects a proper probability distribution -> normalize labels
v1.0.1,Use numerically stable variant to compute log(softmax)
v1.0.1,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.0.1,"To add *all* losses implemented in Torch, uncomment:"
v1.0.1,_LOSSES.update({
v1.0.1,loss
v1.0.1,for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()
v1.0.1,if not loss.__name__.startswith('_')
v1.0.1,})
v1.0.1,: A mapping of losses' names to their implementations
v1.0.1,: HPO Defaults for losses
v1.0.1,Add empty dictionaries as defaults for all remaining losses
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.0.1,Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.
v1.0.1,lower bound
v1.0.1,upper bound
v1.0.1,Allocate weight on device
v1.0.1,Initialize if initializer is provided
v1.0.1,Wrap embedding around it.
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The overall regularization weight
v1.0.1,: The current regularization term (a scalar)
v1.0.1,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.0.1,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.1,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.1,no need to compute anything
v1.0.1,always return zero
v1.0.1,: The dimension along which to compute the vector-based regularization terms.
v1.0.1,: Whether to normalize the regularization term by the dimension of the vectors.
v1.0.1,: This allows dimensionality-independent weight tuning.
v1.0.1,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.1,expected value of |x|_1 = d*E[x_i] for x_i i.i.d.
v1.0.1,expected value of |x|_2 when x_i are normally distributed
v1.0.1,cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1
v1.0.1,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.1,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.1,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.0.1,"Therefore, apply_only_once is always set to True."
v1.0.1,Entity soft constraint
v1.0.1,Orthogonality soft constraint
v1.0.1,The normalization factor to balance individual regularizers' contribution.
v1.0.1,: A mapping of regularizers' names to their implementations
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Add HPO command
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The random seed used at the beginning of the pipeline
v1.0.1,: The model trained by the pipeline
v1.0.1,: The training loop used by the pipeline
v1.0.1,: The losses during training
v1.0.1,: The results evaluated by the pipeline
v1.0.1,: How long in seconds did training take?
v1.0.1,: How long in seconds did evaluation take?
v1.0.1,: An early stopper
v1.0.1,: Any additional metadata as a dictionary
v1.0.1,: The version of PyKEEN used to create these results
v1.0.1,: The git hash of PyKEEN used to create these results
v1.0.1,1. Dataset
v1.0.1,2. Model
v1.0.1,3. Loss
v1.0.1,4. Regularizer
v1.0.1,5. Optimizer
v1.0.1,6. Training Loop
v1.0.1,7. Training (ronaldo style)
v1.0.1,8. Evaluation
v1.0.1,Misc
v1.0.1,Create result store
v1.0.1,Start tracking
v1.0.1,FIXME this should never happen.
v1.0.1,Log model parameters
v1.0.1,Log optimizer parameters
v1.0.1,Stopping
v1.0.1,"Load the evaluation batch size for the stopper, if it has been set"
v1.0.1,By default there's a stopper that does nothing interesting
v1.0.1,Add logging for debugging
v1.0.1,Train like Cristiano Ronaldo
v1.0.1,Evaluate
v1.0.1,Reuse optimal evaluation parameters from training if available
v1.0.1,Add logging about evaluator for debugging
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Create directory in which all experimental artifacts are saved
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: Functions for specifying exotic resources with a given prefix
v1.0.1,: Functions for specifying exotic resources based on their file extension
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.0.1,TODO vectorize code
v1.0.1,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.0.1,"FIXME is this ever possible, since this function is called in __init__?"
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Create lists out of sets for proper numpy indexing when loading the labels
v1.0.1,TODO is there a need to have a canonical sort order here?
v1.0.1,Split triples
v1.0.1,Sorting ensures consistent results when the triples are permuted
v1.0.1,Create mapping
v1.0.1,Sorting ensures consistent results when the triples are permuted
v1.0.1,Create mapping
v1.0.1,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.0.1,Filter all non-existent triples
v1.0.1,Note: Unique changes the order of the triples
v1.0.1,Note: Using unique means implicit balancing of training samples
v1.0.1,: The mapping from entities' labels to their indexes
v1.0.1,: The mapping from relations' labels to their indexes
v1.0.1,": A three-column matrix where each row are the head label,"
v1.0.1,": relation label, then tail label"
v1.0.1,": A three-column matrix where each row are the head identifier,"
v1.0.1,": relation identifier, then tail identifier"
v1.0.1,": A dictionary mapping each relation to its inverse, if inverse triples were created"
v1.0.1,TODO: Check if lazy evaluation would make sense
v1.0.1,Check if the triples are inverted already
v1.0.1,extend original triples with inverse ones
v1.0.1,Generate entity mapping if necessary
v1.0.1,Generate relation mapping if necessary
v1.0.1,Map triples of labels to triples of IDs.
v1.0.1,We can terminate the search after finding the first inverse occurrence
v1.0.1,Ensure 2d array in case only one triple was given
v1.0.1,FIXME this function is only ever used in tests
v1.0.1,Prepare shuffle index
v1.0.1,Prepare split index
v1.0.1,Take cumulative sum so the get separated properly
v1.0.1,Split triples
v1.0.1,Make new triples factories for each group
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: A PyTorch tensor of triples
v1.0.1,: A mapping from relation labels to integer identifiers
v1.0.1,: A mapping from relation labels to integer identifiers
v1.0.1,Create dense target
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,basically take all candidates
v1.0.1,Calculate which relations are the inverse ones
v1.0.1,FIXME doesn't carry flag of create_inverse_triples through
v1.0.1,A dictionary of all of the head/tail pairs for a given relation
v1.0.1,A dictionary for all of the tail/head pairs for a given relation
v1.0.1,Calculate the similarity between each relationship (entries in ``forward``)
v1.0.1,with all other candidate inverse relationships (entries in ``inverse``)
v1.0.1,"Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the"
v1.0.1,"same as for ``(b, a)``"
v1.0.1,A dictionary of all of the head/tail pairs for a given relation
v1.0.1,Filter out results between a given relationship and itself
v1.0.1,Filter out results below a minimum frequency
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,preprocessing
v1.0.1,initialize
v1.0.1,sample iteratively
v1.0.1,determine weights
v1.0.1,only happens at first iteration
v1.0.1,normalize to probabilities
v1.0.1,sample a start node
v1.0.1,get list of neighbors
v1.0.1,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.0.1,visit target node
v1.0.1,decrease sample counts
v1.0.1,return chosen edges
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Create training instances
v1.0.1,During size probing the training instances should not show the tqdm progress bar
v1.0.1,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.0.1,Ensure the release of memory
v1.0.1,Clear optimizer
v1.0.1,"Take the biggest possible training batch_size, if batch_size not set"
v1.0.1,This will find necessary parameters to optimize the use of the hardware at hand
v1.0.1,return the relevant parameters slice_size and batch_size
v1.0.1,Create dummy result tracker
v1.0.1,Sanity check
v1.0.1,Force weight initialization if training continuation is not explicitly requested.
v1.0.1,Reset the weights
v1.0.1,Create new optimizer
v1.0.1,Ensure the model is on the correct device
v1.0.1,Create Sampler
v1.0.1,Bind
v1.0.1,"When size probing, we don't want progress bars"
v1.0.1,Create progress bar
v1.0.1,Training Loop
v1.0.1,Enforce training mode
v1.0.1,Accumulate loss over epoch
v1.0.1,Batching
v1.0.1,Only create a progress bar when not in size probing mode
v1.0.1,Flag to check when to quit the size probing
v1.0.1,Recall that torch *accumulates* gradients. Before passing in a
v1.0.1,"new instance, you need to zero out the gradients from the old instance"
v1.0.1,Get batch size of current batch (last batch may be incomplete)
v1.0.1,accumulate gradients for whole batch
v1.0.1,forward pass call
v1.0.1,"when called by batch_size_search(), the parameter update should not be applied."
v1.0.1,update parameters according to optimizer
v1.0.1,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.0.1,constraints are no longer applied
v1.0.1,For testing purposes we're only interested in processing one batch
v1.0.1,When size probing we don't need the losses
v1.0.1,Track epoch loss
v1.0.1,Print loss information to console
v1.0.1,forward pass
v1.0.1,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.0.1,correction for loss reduction
v1.0.1,backward pass
v1.0.1,reset the regularizer to free the computational graph
v1.0.1,Set upper bound
v1.0.1,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.0.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.1,The regularizer has to be reset to free the computational graph
v1.0.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Shuffle each epoch
v1.0.1,Lazy-splitting into batches
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Slicing is not possible in sLCWA training loops
v1.0.1,Send positive batch to device
v1.0.1,Create negative samples
v1.0.1,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.0.1,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.0.1,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.0.1,Compute negative and positive scores
v1.0.1,Repeat positives scores (necessary for more than one negative per positive)
v1.0.1,Stack predictions
v1.0.1,Create target
v1.0.1,Normalize the loss to have the average loss per positive triple
v1.0.1,This allows comparability of sLCWA and LCWA losses
v1.0.1,Slicing is not possible for sLCWA
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: A mapping of training loops' names to their implementations
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Split batch components
v1.0.1,Send batch to device
v1.0.1,Apply label smoothing
v1.0.1,This shows how often one row has to be repeated
v1.0.1,Create boolean indices for negative labels in the repeated rows
v1.0.1,Repeat the predictions and filter for negative labels
v1.0.1,This tells us how often each true label should be repeated
v1.0.1,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.0.1,Split positive and negative scores
v1.0.1,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.1,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The model
v1.0.1,: The evaluator
v1.0.1,: The triples to use for evaluation
v1.0.1,: Size of the evaluation batches
v1.0.1,: Slice size of the evaluation batches
v1.0.1,: The number of epochs after which the model is evaluated on validation set
v1.0.1,: The number of iterations (one iteration can correspond to various epochs)
v1.0.1,: with no improvement after which training will be stopped.
v1.0.1,: The name of the metric to use
v1.0.1,: The minimum improvement between two iterations
v1.0.1,: The metric results from all evaluations
v1.0.1,: A ring buffer to store the recent results
v1.0.1,: A counter for the ring buffer
v1.0.1,": Whether a larger value is better, or a smaller"
v1.0.1,: The criterion. Set in the constructor based on larger_is_better
v1.0.1,: The result tracker
v1.0.1,: Callbacks when training gets continued
v1.0.1,: Callbacks when training is stopped early
v1.0.1,: Did the stopper ever decide to stop?
v1.0.1,TODO: Fix this
v1.0.1,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.0.1,raise ValueError(f'Invalid metric name: {self.metric}')
v1.0.1,Dummy result tracker
v1.0.1,Evaluate
v1.0.1,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.0.1,Only check if enough values are already collected
v1.0.1,Stop if the result did not improve more than delta for patience epochs.
v1.0.1,Update ring buffer
v1.0.1,Append to history
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: A mapping of stoppers' names to their implementations
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.0.1,Clear the ranks from the current evaluator
v1.0.1,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.0.1,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.1,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.1,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.0.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.1,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.1,Test if slicing is implemented for the required functions of this model
v1.0.1,Split batch
v1.0.1,Bind shape
v1.0.1,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.0.1,Warn if all entities will be filtered
v1.0.1,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.0.1,Send to device
v1.0.1,Ensure evaluation mode
v1.0.1,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.0.1,Check whether we need to be prepared for filtering
v1.0.1,Check whether an evaluator needs access to the masks
v1.0.1,This can only be an unfiltered evaluator.
v1.0.1,Prepare for result filtering
v1.0.1,Send tensors to device
v1.0.1,Prepare batches
v1.0.1,Show progressbar
v1.0.1,Flag to check when to quit the size probing
v1.0.1,Disable gradient tracking
v1.0.1,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.0.1,batch-wise processing
v1.0.1,Predict tail scores once
v1.0.1,Create positive filter for all corrupted tails
v1.0.1,Create a positive mask with the size of the scores from the positive tails filter
v1.0.1,Evaluate metrics on these *unfiltered* tail scores
v1.0.1,Filter
v1.0.1,The scores for the true triples have to be rewritten to the scores tensor
v1.0.1,Evaluate metrics on these *filtered* tail scores
v1.0.1,Predict head scores once
v1.0.1,Create positive filter for all corrupted heads
v1.0.1,Create a positive mask with the size of the scores from the positive heads filter
v1.0.1,Evaluate metrics on these head scores
v1.0.1,Filter
v1.0.1,The scores for the true triples have to be rewritten to the scores tensor
v1.0.1,Evaluate metrics on these *filtered* tail scores
v1.0.1,If we only probe sizes we do not need more than one batch
v1.0.1,Finalize
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The area under the ROC curve
v1.0.1,: The area under the precision-recall curve
v1.0.1,: The coverage error
v1.0.1,coverage_error: float = field(metadata=dict(
v1.0.1,"doc='The coverage error',"
v1.0.1,"f=metrics.coverage_error,"
v1.0.1,))
v1.0.1,: The label ranking loss (APS)
v1.0.1,label_ranking_average_precision_score: float = field(metadata=dict(
v1.0.1,"doc='The label ranking loss (APS)',"
v1.0.1,"f=metrics.label_ranking_average_precision_score,"
v1.0.1,))
v1.0.1,#: The label ranking loss
v1.0.1,label_ranking_loss: float = field(metadata=dict(
v1.0.1,"doc='The label ranking loss',"
v1.0.1,"f=metrics.label_ranking_loss,"
v1.0.1,))
v1.0.1,Transfer to cpu and convert to numpy
v1.0.1,Ensure that each key gets counted only once
v1.0.1,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.0.1,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.0.1,masks using the exact same key order.
v1.0.1,TODO how to define a cutoff on y_scores to make binary?
v1.0.1,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.0.1,Clear buffers
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: A mapping of evaluators' names to their implementations
v1.0.1,: A mapping of results' names to their implementations
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,The best rank is the rank when assuming all options with an equal score are placed behind the currently
v1.0.1,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.0.1,The worst rank is the rank when assuming all options with an equal score are placed in front of the currently
v1.0.1,"considered. Hence, the rank is the number of options which have at least the same score minus one (as the"
v1.0.1,"currently considered option in included in all options). As the rank is one-based, we have to add 1, which"
v1.0.1,"nullifies the ""minus 1"" from before."
v1.0.1,"The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of"
v1.0.1,the elements with the same score as the currently considered option.
v1.0.1,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.0.1,The expected rank of a random scoring
v1.0.1,The adjusted ranks is normalized by the expected rank of a random scoring
v1.0.1,TODO adjusted_worst_rank
v1.0.1,TODO adjusted_best_rank
v1.0.1,: The mean over all ranks: mean_i r_i. Lower is better.
v1.0.1,: The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.
v1.0.1,": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k."
v1.0.1,: Higher is better.
v1.0.1,: The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.
v1.0.1,: Described by [berrendorf2020]_.
v1.0.1,Clear buffers
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.0.1,Create a tensor of all IDs
v1.0.1,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.0.1,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.0.1,: A dictionary of hyper-parameters to the models that use them
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The default loss function class
v1.0.1,: The default parameters for the default loss function class
v1.0.1,: The instance of the loss
v1.0.1,: The default regularizer class
v1.0.1,: The default parameters for the default regularizer class
v1.0.1,: The instance of the regularizer
v1.0.1,Initialize the device
v1.0.1,Random seeds have to set before the embeddings are initialized
v1.0.1,Loss
v1.0.1,TODO: Check loss functions that require 1 and -1 as label but only
v1.0.1,Regularizer
v1.0.1,The triples factory facilitates access to the dataset.
v1.0.1,This allows to store the optimized parameters
v1.0.1,Keep track of the hyper-parameters that are used across all
v1.0.1,subclasses of BaseModule
v1.0.1,Enforce evaluation mode
v1.0.1,Enforce evaluation mode
v1.0.1,Enforce evaluation mode
v1.0.1,Enforce evaluation mode
v1.0.1,The number of relations stored in the triples factory includes the number of inverse relations
v1.0.1,Id of inverse relation: relation + 1
v1.0.1,"The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs"
v1.0.1,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.0.1,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.1,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.0.1,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.0.1,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.1,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.0.1,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.0.1,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.1,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.0.1,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.0.1,Default for relation dimensionality
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: A mapping of models' names to their implementations
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Store initial input for error message
v1.0.1,All are None
v1.0.1,"input_channels is None, and any of height or width is None -> set input_channels=1"
v1.0.1,"input channels is not None, and one of height or width is None"
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The default loss function class
v1.0.1,: The default parameters for the default loss function class
v1.0.1,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.0.1,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.0.1,ConvE should be trained with inverse triples
v1.0.1,ConvE uses one bias for each entity
v1.0.1,Automatic calculation of remaining dimensions
v1.0.1,Parameter need to fulfil:
v1.0.1,input_channels * embedding_height * embedding_width = embedding_dim
v1.0.1,Finalize initialization
v1.0.1,embeddings
v1.0.1,weights
v1.0.1,"batch_size, num_input_channels, 2*height, width"
v1.0.1,"batch_size, num_input_channels, 2*height, width"
v1.0.1,"batch_size, num_input_channels, 2*height, width"
v1.0.1,"(N,C_out,H_out,W_out)"
v1.0.1,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.0.1,Embedding Regularization
v1.0.1,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.0.1,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.1,Embedding Regularization
v1.0.1,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.1,Embedding Regularization
v1.0.1,Code to repeat each item successively instead of the entire tensor
v1.0.1,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,Embeddings
v1.0.1,Finalize initialization
v1.0.1,Initialise left relation embeddings to unit length
v1.0.1,Make sure to call super first
v1.0.1,Normalise embeddings of entities
v1.0.1,Get embeddings
v1.0.1,Project entities
v1.0.1,Get embeddings
v1.0.1,Project entities
v1.0.1,Project entities
v1.0.1,Project entities
v1.0.1,Get embeddings
v1.0.1,Project entities
v1.0.1,Project entities
v1.0.1,Project entities
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The default loss function class
v1.0.1,: The default parameters for the default loss function class
v1.0.1,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.0.1,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.0.1,Finalize initialization
v1.0.1,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.0.1,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.0.1,split into real and imaginary part
v1.0.1,ComplEx space bilinear product
v1.0.1,*: Elementwise multiplication
v1.0.1,get embeddings
v1.0.1,Compute scores
v1.0.1,Regularization
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The regularizer used by [nickel2011]_ for for RESCAL
v1.0.1,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.0.1,: a normalized weight of 10 is used.
v1.0.1,: The LP settings used by [nickel2011]_ for for RESCAL
v1.0.1,Finalize initialization
v1.0.1,Get embeddings
v1.0.1,"shape: (b, d)"
v1.0.1,"shape: (b, d, d)"
v1.0.1,"shape: (b, d)"
v1.0.1,Compute scores
v1.0.1,Regularization
v1.0.1,Compute scores
v1.0.1,Regularization
v1.0.1,Get embeddings
v1.0.1,Compute scores
v1.0.1,Regularization
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,Finalize initialization
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.0.1,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.0.1,The interaction model
v1.0.1,Finalize initialization
v1.0.1,embeddings
v1.0.1,Use Xavier initialization for weight; bias to zero
v1.0.1,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.0.1,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.0.1,Output layer regularization
v1.0.1,In the code base only the weights of the output layer are used for regularization
v1.0.1,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.0.1,Stack to convolution input
v1.0.1,Convolution
v1.0.1,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.0.1,Linear layer for final scores
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,Finalize initialization
v1.0.1,": shape: (batch_size, num_entities, d)"
v1.0.1,": Prepare h: (b, e, d) -> (b, e, 1, 1, d)"
v1.0.1,": Prepare t: (b, e, d) -> (b, e, 1, d, 1)"
v1.0.1,": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)"
v1.0.1,"h.T @ W @ t, shape: (b, e, k, 1, 1)"
v1.0.1,": reduce (b, e, k, 1, 1) -> (b, e, k)"
v1.0.1,": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.1,": Prepare h: (b, e, d) -> (b, e, d, 1)"
v1.0.1,"V_h @ h, shape: (b, e, k, 1)"
v1.0.1,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.1,": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.1,": Prepare t: (b, e, d) -> (b, e, d, 1)"
v1.0.1,"V_t @ t, shape: (b, e, k, 1)"
v1.0.1,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.1,": Prepare b: (R, k) -> (b, k) -> (b, 1, k)"
v1.0.1,"a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)"
v1.0.1,"prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)"
v1.0.1,"prepare act: (b, e, k) -> (b, e, 1, k)"
v1.0.1,"compute score, shape: (b, e, 1, 1)"
v1.0.1,reduce
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The regularizer used by [yang2014]_ for DistMult
v1.0.1,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.0.1,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.0.1,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.0.1,: The LP settings used by [yang2014]_ for DistMult
v1.0.1,Finalize initialization
v1.0.1,"xavier uniform, cf."
v1.0.1,https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17
v1.0.1,Initialise relation embeddings to unit length
v1.0.1,Make sure to call super first
v1.0.1,Normalize embeddings of entities
v1.0.1,Bilinear product
v1.0.1,*: Elementwise multiplication
v1.0.1,Get embeddings
v1.0.1,Compute score
v1.0.1,Only regularize relation embeddings
v1.0.1,Get embeddings
v1.0.1,Rank against all entities
v1.0.1,Only regularize relation embeddings
v1.0.1,Get embeddings
v1.0.1,Rank against all entities
v1.0.1,Only regularize relation embeddings
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,Similarity function used for distributions
v1.0.1,element-wise covariance bounds
v1.0.1,Additional covariance embeddings
v1.0.1,Finalize initialization
v1.0.1,Constraints are applied through post_parameter_update
v1.0.1,Make sure to call super first
v1.0.1,Normalize entity embeddings
v1.0.1,Ensure positive definite covariances matrices and appropriate size by clamping
v1.0.1,Get embeddings
v1.0.1,Compute entity distribution
v1.0.1,: a = \mu^T\Sigma^{-1}\mu
v1.0.1,: b = \log \det \Sigma
v1.0.1,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.0.1,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.0.1,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.0.1,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The custom regularizer used by [wang2014]_ for TransH
v1.0.1,: The settings used by [wang2014]_ for TransH
v1.0.1,embeddings
v1.0.1,Finalize initialization
v1.0.1,TODO: Add initialization
v1.0.1,Make sure to call super first
v1.0.1,Normalise the normal vectors by their l2 norms
v1.0.1,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.0.1,which enforces the defined soft constraints.
v1.0.1,Get embeddings
v1.0.1,Project to hyperplane
v1.0.1,Regularization term
v1.0.1,Get embeddings
v1.0.1,Project to hyperplane
v1.0.1,Regularization term
v1.0.1,Get embeddings
v1.0.1,Project to hyperplane
v1.0.1,Regularization term
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,embeddings
v1.0.1,Finalize initialization
v1.0.1,Make sure to call super first
v1.0.1,Normalize entity embeddings
v1.0.1,TODO: Initialize from TransE
v1.0.1,Initialise relation embeddings to unit length
v1.0.1,"project to relation specific subspace, shape: (b, e, d_r)"
v1.0.1,ensure constraints
v1.0.1,"evaluate score function, shape: (b, e)"
v1.0.1,Get embeddings
v1.0.1,Get embeddings
v1.0.1,Get embeddings
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,Construct node neighbourhood mask
v1.0.1,Set nodes in batch to true
v1.0.1,Compute k-neighbourhood
v1.0.1,"if the target node needs an embeddings, so does the source node"
v1.0.1,Create edge mask
v1.0.1,pylint: disable=unused-argument
v1.0.1,"Calculate in-degree, i.e. number of incoming edges"
v1.0.1,pylint: disable=unused-argument
v1.0.1,"Calculate in-degree, i.e. number of incoming edges"
v1.0.1,: Interaction model used as decoder
v1.0.1,: The blocks of the relation-specific weight matrices
v1.0.1,": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)"
v1.0.1,: The base weight matrices to generate relation-specific weights
v1.0.1,": shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.1,: The relation-specific weights for each base
v1.0.1,": shape: (num_relations, num_bases)"
v1.0.1,: The biases for each layer (if used)
v1.0.1,": shape of each element: (embedding_dim,)"
v1.0.1,: Batch normalization for each layer (if used)
v1.0.1,: Activations for each layer (if used)
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,Instantiate model
v1.0.1,Heuristic
v1.0.1,buffering of messages
v1.0.1,"Save graph using buffers, such that the tensors are moved together with the model"
v1.0.1,Weights
v1.0.1,Finalize initialization
v1.0.1,invalidate enriched embeddings
v1.0.1,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.0.1,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.0.1,initialized properly)
v1.0.1,We have one additional relation for self-loops
v1.0.1,Xavier Glorot initialization of each block
v1.0.1,Reset biases
v1.0.1,Reset batch norm parameters
v1.0.1,"Reset activation parameters, if any"
v1.0.1,use buffered messages if applicable
v1.0.1,Bind fields
v1.0.1,"shape: (num_entities, embedding_dim)"
v1.0.1,Edge dropout: drop the same edges on all layers (only in training mode)
v1.0.1,Get random dropout mask
v1.0.1,Apply to edges
v1.0.1,Different dropout for self-loops (only in training mode)
v1.0.1,"If batch is given, compute (num_layers)-hop neighbourhood"
v1.0.1,Initialize embeddings in the next layer for all nodes
v1.0.1,TODO: Can we vectorize this loop?
v1.0.1,Choose the edges which are of the specific relation
v1.0.1,Only propagate messages on subset of edges
v1.0.1,No edges available? Skip rest of inner loop
v1.0.1,Get source and target node indices
v1.0.1,send messages in both directions
v1.0.1,Select source node embeddings
v1.0.1,get relation weights
v1.0.1,Compute message (b x d) * (d x d) = (b x d)
v1.0.1,Normalize messages by relation-specific in-degree
v1.0.1,Aggregate messages in target
v1.0.1,Self-loop
v1.0.1,"Apply bias, if requested"
v1.0.1,"Apply batch normalization, if requested"
v1.0.1,Apply non-linearity
v1.0.1,allocate weight
v1.0.1,Get blocks
v1.0.1,"self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)"
v1.0.1,note: embedding_dim is guaranteed to be divisible by num_bases in the constructor
v1.0.1,"The current basis weights, shape: (num_bases)"
v1.0.1,"the current bases, shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.1,"compute the current relation weights, shape: (embedding_dim, embedding_dim)"
v1.0.1,Enrich embeddings
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The default loss function class
v1.0.1,: The default parameters for the default loss function class
v1.0.1,Core tensor
v1.0.1,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.0.1,Dropout
v1.0.1,Finalize initialization
v1.0.1,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.0.1,Abbreviation
v1.0.1,Compute h_n = DO(BN(h))
v1.0.1,Compute wr = DO(W x_2 r)
v1.0.1,compute whr = DO(BN(h_n x_1 wr))
v1.0.1,Compute whr x_3 t
v1.0.1,Get embeddings
v1.0.1,Compute scores
v1.0.1,Get embeddings
v1.0.1,Compute scores
v1.0.1,Get embeddings
v1.0.1,Compute scores
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,Finalize initialization
v1.0.1,Initialise relation embeddings to unit length
v1.0.1,Make sure to call super first
v1.0.1,Normalize entity embeddings
v1.0.1,Get embeddings
v1.0.1,Get embeddings
v1.0.1,Get embeddings
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The default loss function class
v1.0.1,: The default parameters for the default loss function class
v1.0.1,: The regularizer used by [trouillon2016]_ for SimplE
v1.0.1,": In the paper, they use weight of 0.1, and do not normalize the"
v1.0.1,": regularization term by the number of elements, which is 200."
v1.0.1,: The power sum settings used by [trouillon2016]_ for SimplE
v1.0.1,extra embeddings
v1.0.1,Finalize initialization
v1.0.1,forward model
v1.0.1,Regularization
v1.0.1,backward model
v1.0.1,Regularization
v1.0.1,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.0.1,"That is not mentioned in the paper, so it is omitted here."
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,Finalize initialization
v1.0.1,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.0.1,weight initialization
v1.0.1,Get embeddings
v1.0.1,Embedding Regularization
v1.0.1,Concatenate them
v1.0.1,Compute scores
v1.0.1,Get embeddings
v1.0.1,Embedding Regularization
v1.0.1,First layer can be unrolled
v1.0.1,Send scores through rest of the network
v1.0.1,Get embeddings
v1.0.1,Embedding Regularization
v1.0.1,First layer can be unrolled
v1.0.1,Send scores through rest of the network
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,The dimensions affected by e'
v1.0.1,Project entities
v1.0.1,r_p (e_p.T e) + e'
v1.0.1,Enforce constraints
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,Finalize initialization
v1.0.1,Make sure to call super first
v1.0.1,Normalize entity embeddings
v1.0.1,Project entities
v1.0.1,score = -||h_bot + r - t_bot||_2^2
v1.0.1,Head
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,Finalize initialization
v1.0.1,phases randomly between 0 and 2 pi
v1.0.1,Make sure to call super first
v1.0.1,Normalize relation embeddings
v1.0.1,Decompose into real and imaginary part
v1.0.1,Rotate (=Hadamard product in complex space).
v1.0.1,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.0.1,Get embeddings
v1.0.1,Compute scores
v1.0.1,Embedding Regularization
v1.0.1,Get embeddings
v1.0.1,Rank against all entities
v1.0.1,Compute scores
v1.0.1,Embedding Regularization
v1.0.1,Get embeddings
v1.0.1,r expresses a rotation in complex plane.
v1.0.1,The inverse rotation is expressed by the complex conjugate of r.
v1.0.1,The score is computed as the distance of the relation-rotated head to the tail.
v1.0.1,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.0.1,|h * r - t| = |h - conj(r) * t|
v1.0.1,Rank against all entities
v1.0.1,Compute scores
v1.0.1,Embedding Regularization
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The default loss function class
v1.0.1,: The default parameters for the default loss function class
v1.0.1,Global entity projection
v1.0.1,Global relation projection
v1.0.1,Global combination bias
v1.0.1,Global combination bias
v1.0.1,Finalize initialization
v1.0.1,Get embeddings
v1.0.1,Compute score
v1.0.1,Get embeddings
v1.0.1,Rank against all entities
v1.0.1,Get embeddings
v1.0.1,Rank against all entities
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,Finalize initialization
v1.0.1,Make sure to call super first
v1.0.1,Normalize entity embeddings
v1.0.1,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.0.1,Circular correlation of entity embeddings
v1.0.1,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.0.1,Hadamard product in frequency domain
v1.0.1,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.0.1,inner product with relation embedding
v1.0.1,Embedding Regularization
v1.0.1,Embedding Regularization
v1.0.1,Embedding Regularization
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The default loss function class
v1.0.1,: The default parameters for the default loss function class
v1.0.1,Finalize initialization
v1.0.1,Get embeddings
v1.0.1,Embedding Regularization
v1.0.1,Concatenate them
v1.0.1,Predict t embedding
v1.0.1,compare with all t's
v1.0.1,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.1,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.1,Embedding Regularization
v1.0.1,Concatenate them
v1.0.1,Predict t embedding
v1.0.1,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.1,Embedding Regularization
v1.0.1,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.0.1,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.0.1,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.0.1,Extend t
v1.0.1,Concatenate them
v1.0.1,Predict t embedding
v1.0.1,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.1,The results have to be realigned with the expected output of the score_h function
v1.0.1,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,TODO: Check entire build of the model
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The default loss function class
v1.0.1,: The default parameters for the default loss function class
v1.0.1,Literal
v1.0.1,num_ent x num_lit
v1.0.1,Number of columns corresponds to number of literals
v1.0.1,Literals
v1.0.1,End literals
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,TODO: Check entire build of the model
v1.0.1,: The default strategy for optimizing the model's hyper-parameters
v1.0.1,: The default parameters for the default loss function class
v1.0.1,Embeddings
v1.0.1,Number of columns corresponds to number of literals
v1.0.1,apply dropout
v1.0.1,"-, because lower score shall correspond to a more plausible triple."
v1.0.1,TODO check if this is the same as the BaseModule
v1.0.1,Choose y = -1 since a smaller score is better.
v1.0.1,"In TransE for example, the scores represent distances"
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.1,Bind number of negatives to sample
v1.0.1,Equally corrupt head and tail
v1.0.1,Copy positive batch for corruption.
v1.0.1,"Do not detach, as no gradients should flow into the indices."
v1.0.1,Sample random entities as replacement
v1.0.1,Replace heads – To make sure we don't replace the head by the original value
v1.0.1,we shift all values greater or equal than the original value by one up
v1.0.1,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.1,Corrupt tails
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: A mapping of negative samplers' names to their implementations
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.1,Preprocessing: Compute corruption probabilities
v1.0.1,"compute tph, i.e. the average number of tail entities per head"
v1.0.1,"compute hpt, i.e. the average number of head entities per tail"
v1.0.1,Set parameter for Bernoulli distribution
v1.0.1,Bind number of negatives to sample
v1.0.1,Copy positive batch for corruption.
v1.0.1,"Do not detach, as no gradients should flow into the indices."
v1.0.1,Decide whether to corrupt head or tail
v1.0.1,Tails are corrupted if heads are not corrupted
v1.0.1,Randomly sample corruption
v1.0.1,Replace heads – To make sure we don't replace the head by the original value
v1.0.1,we shift all values greater or equal than the original value by one up
v1.0.1,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.1,Replace tails
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,TODO what happens if already exists?
v1.0.1,TODO incorporate setting of random seed
v1.0.1,pipeline_kwargs=dict(
v1.0.1,"random_seed=random.randint(1, 2 ** 32 - 1),"
v1.0.1,"),"
v1.0.1,Add dataset to current_pipeline
v1.0.1,Add loss function to current_pipeline
v1.0.1,Add regularizer to current_pipeline
v1.0.1,Add optimizer to current_pipeline
v1.0.1,Add training approach to current_pipeline
v1.0.1,Add training kwargs and kwargs_ranges
v1.0.1,Add evaluation
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: A factory wrapping the training triples
v1.0.1,": A factory wrapping the testing triples, that share indexes with the training triples"
v1.0.1,": A factory wrapping the validation triples, that share indexes with the training triples"
v1.0.1,: All data sets should take care of inverse triple creation
v1.0.1,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.0.1,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.0.1,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.0.1,don't call this function by itself. assumes called through the `validation`
v1.0.1,property and the _training factory has already been loaded
v1.0.1,see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content
v1.0.1,pattern from https://stackoverflow.com/a/39217788/5775947
v1.0.1,TODO replace this with the new zip remote dataset class
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: A mapping of datasets' names to their classes
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,TODO update docs with table and CLI wtih generator
v1.0.1,: A mapping of HPO samplers' names to their implementations
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,1. Dataset
v1.0.1,2. Model
v1.0.1,3. Loss
v1.0.1,4. Regularizer
v1.0.1,5. Optimizer
v1.0.1,6. Training Loop
v1.0.1,7. Training
v1.0.1,8. Evaluation
v1.0.1,Misc.
v1.0.1,2. Model
v1.0.1,3. Loss
v1.0.1,4. Regularizer
v1.0.1,5. Optimizer
v1.0.1,1. Dataset
v1.0.1,2. Model
v1.0.1,3. Loss
v1.0.1,4. Regularizer
v1.0.1,5. Optimizer
v1.0.1,6. Training Loop
v1.0.1,7. Training
v1.0.1,8. Evaluation
v1.0.1,Misc.
v1.0.1,Will trigger Optuna to set the state of the trial as failed
v1.0.1,: The :mod:`optuna` study object
v1.0.1,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.0.1,Output study information
v1.0.1,Output all trials
v1.0.1,Output best trial as pipeline configuration file
v1.0.1,1. Dataset
v1.0.1,2. Model
v1.0.1,3. Loss
v1.0.1,4. Regularizer
v1.0.1,5. Optimizer
v1.0.1,6. Training Loop
v1.0.1,7. Training
v1.0.1,8. Evaluation
v1.0.1,6. Misc
v1.0.1,Optuna Study Settings
v1.0.1,Optuna Optimization Settings
v1.0.1,0. Metadata/Provenance
v1.0.1,1. Dataset
v1.0.1,FIXME difference between dataset class and string
v1.0.1,FIXME how to handle if dataset or factories were set? Should have been
v1.0.1,part of https://github.com/mali-git/POEM_develop/pull/483
v1.0.1,2. Model
v1.0.1,3. Loss
v1.0.1,4. Regularizer
v1.0.1,5. Optimizer
v1.0.1,6. Training Loop
v1.0.1,7. Training
v1.0.1,8. Evaluation
v1.0.1,1. Dataset
v1.0.1,2. Model
v1.0.1,3. Loss
v1.0.1,4. Regularizer
v1.0.1,5. Optimizer
v1.0.1,6. Training Loop
v1.0.1,7. Training
v1.0.1,8. Evaluation
v1.0.1,Optuna Misc.
v1.0.1,Pipeline Misc.
v1.0.1,Invoke optimization of the objective function.
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,-*- coding: utf-8 -*-
v1.0.1,: A mapping of HPO pruners' names to their implementations
v1.0.1,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,
v1.0.0,Configuration file for the Sphinx documentation builder.
v1.0.0,
v1.0.0,This file does only contain a selection of the most common options. For a
v1.0.0,full list see the documentation:
v1.0.0,http://www.sphinx-doc.org/en/master/config
v1.0.0,-- Path setup --------------------------------------------------------------
v1.0.0,"If extensions (or modules to document with autodoc) are in another directory,"
v1.0.0,add these directories to sys.path here. If the directory is relative to the
v1.0.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v1.0.0,
v1.0.0,"sys.path.insert(0, os.path.abspath('..'))"
v1.0.0,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v1.0.0,"autodoc_mock_imports = ['torch', 'torchvision']"
v1.0.0,from unittest.mock import Mock
v1.0.0,sys.modules['numpy'] = Mock()
v1.0.0,sys.modules['numpy.linalg'] = Mock()
v1.0.0,sys.modules['scipy'] = Mock()
v1.0.0,sys.modules['scipy.optimize'] = Mock()
v1.0.0,sys.modules['scipy.interpolate'] = Mock()
v1.0.0,sys.modules['scipy.sparse'] = Mock()
v1.0.0,sys.modules['scipy.ndimage'] = Mock()
v1.0.0,sys.modules['scipy.ndimage.filters'] = Mock()
v1.0.0,sys.modules['tensorflow'] = Mock()
v1.0.0,sys.modules['theano'] = Mock()
v1.0.0,sys.modules['theano.tensor'] = Mock()
v1.0.0,sys.modules['torch'] = Mock()
v1.0.0,sys.modules['torch.optim'] = Mock()
v1.0.0,sys.modules['torch.nn'] = Mock()
v1.0.0,sys.modules['torch.nn.init'] = Mock()
v1.0.0,sys.modules['torch.autograd'] = Mock()
v1.0.0,sys.modules['sklearn'] = Mock()
v1.0.0,sys.modules['sklearn.model_selection'] = Mock()
v1.0.0,sys.modules['sklearn.utils'] = Mock()
v1.0.0,-- Project information -----------------------------------------------------
v1.0.0,"The full version, including alpha/beta/rc tags."
v1.0.0,The short X.Y version.
v1.0.0,-- General configuration ---------------------------------------------------
v1.0.0,"If your documentation needs a minimal Sphinx version, state it here."
v1.0.0,
v1.0.0,needs_sphinx = '1.0'
v1.0.0,"Add any Sphinx extension module names here, as strings. They can be"
v1.0.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v1.0.0,ones.
v1.0.0,"Add any paths that contain templates here, relative to this directory."
v1.0.0,The suffix(es) of source filenames.
v1.0.0,You can specify multiple suffix as a list of string:
v1.0.0,
v1.0.0,"source_suffix = ['.rst', '.md']"
v1.0.0,The master toctree document.
v1.0.0,The language for content autogenerated by Sphinx. Refer to documentation
v1.0.0,for a list of supported languages.
v1.0.0,
v1.0.0,This is also used if you do content translation via gettext catalogs.
v1.0.0,"Usually you set ""language"" from the command line for these cases."
v1.0.0,"List of patterns, relative to source directory, that match files and"
v1.0.0,directories to ignore when looking for source files.
v1.0.0,This pattern also affects html_static_path and html_extra_path.
v1.0.0,The name of the Pygments (syntax highlighting) style to use.
v1.0.0,-- Options for HTML output -------------------------------------------------
v1.0.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v1.0.0,a list of builtin themes.
v1.0.0,
v1.0.0,Theme options are theme-specific and customize the look and feel of a theme
v1.0.0,"further.  For a list of options available for each theme, see the"
v1.0.0,documentation.
v1.0.0,
v1.0.0,html_theme_options = {}
v1.0.0,"Add any paths that contain custom static files (such as style sheets) here,"
v1.0.0,"relative to this directory. They are copied after the builtin static files,"
v1.0.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v1.0.0,html_static_path = ['_static']
v1.0.0,"Custom sidebar templates, must be a dictionary that maps document names"
v1.0.0,to template names.
v1.0.0,
v1.0.0,The default sidebars (for documents that don't match any pattern) are
v1.0.0,defined by theme itself.  Builtin themes are using these templates by
v1.0.0,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v1.0.0,'searchbox.html']``.
v1.0.0,
v1.0.0,html_sidebars = {}
v1.0.0,The name of an image file (relative to this directory) to place at the top
v1.0.0,of the sidebar.
v1.0.0,
v1.0.0,-- Options for HTMLHelp output ---------------------------------------------
v1.0.0,Output file base name for HTML help builder.
v1.0.0,-- Options for LaTeX output ------------------------------------------------
v1.0.0,The paper size ('letterpaper' or 'a4paper').
v1.0.0,
v1.0.0,"'papersize': 'letterpaper',"
v1.0.0,"The font size ('10pt', '11pt' or '12pt')."
v1.0.0,
v1.0.0,"'pointsize': '10pt',"
v1.0.0,Additional stuff for the LaTeX preamble.
v1.0.0,
v1.0.0,"'preamble': '',"
v1.0.0,Latex figure (float) alignment
v1.0.0,
v1.0.0,"'figure_align': 'htbp',"
v1.0.0,Grouping the document tree into LaTeX files. List of tuples
v1.0.0,"(source start file, target name, title,"
v1.0.0,"author, documentclass [howto, manual, or own class])."
v1.0.0,-- Options for manual page output ------------------------------------------
v1.0.0,One entry per manual page. List of tuples
v1.0.0,"(source start file, name, description, authors, manual section)."
v1.0.0,-- Options for Texinfo output ----------------------------------------------
v1.0.0,Grouping the document tree into Texinfo files. List of tuples
v1.0.0,"(source start file, target name, title, author,"
v1.0.0,"dir menu entry, description, category)"
v1.0.0,-- Options for Epub output -------------------------------------------------
v1.0.0,Bibliographic Dublin Core info.
v1.0.0,The unique identifier of the text. This can be a ISBN number
v1.0.0,or the project homepage.
v1.0.0,
v1.0.0,epub_identifier = ''
v1.0.0,A unique identification for the text.
v1.0.0,
v1.0.0,epub_uid = ''
v1.0.0,A list of files that should not be packed into the epub file.
v1.0.0,-- Extension configuration -------------------------------------------------
v1.0.0,-- Options for intersphinx extension ---------------------------------------
v1.0.0,Example configuration for intersphinx: refer to the Python standard library.
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Check a model param is optimized
v1.0.0,Check a loss param is optimized
v1.0.0,Check a model param is NOT optimized
v1.0.0,Check a loss param is optimized
v1.0.0,Check a model param is optimized
v1.0.0,Check a loss param is NOT optimized
v1.0.0,Check a model param is NOT optimized
v1.0.0,Check a loss param is NOT optimized
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,check for empty batches
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,The triples factory and model
v1.0.0,: The evaluator to be tested
v1.0.0,Settings
v1.0.0,: The evaluator instantiation
v1.0.0,Settings
v1.0.0,Initialize evaluator
v1.0.0,Use small test dataset
v1.0.0,Use small model (untrained)
v1.0.0,Get batch
v1.0.0,Compute scores
v1.0.0,Compute mask only if required
v1.0.0,TODO: Re-use filtering code
v1.0.0,"shape: (batch_size, num_triples)"
v1.0.0,"shape: (batch_size, num_entities)"
v1.0.0,Process one batch
v1.0.0,Check for correct class
v1.0.0,Check value ranges
v1.0.0,TODO: Validate with data?
v1.0.0,Check for correct class
v1.0.0,check value
v1.0.0,filtering
v1.0.0,"true_score: (2, 3, 3)"
v1.0.0,head based filter
v1.0.0,preprocessing for faster lookup
v1.0.0,check that all found positives are positive
v1.0.0,check in-place
v1.0.0,Test head scores
v1.0.0,Assert in-place modification
v1.0.0,Assert correct filtering
v1.0.0,Test tail scores
v1.0.0,Assert in-place modification
v1.0.0,Assert correct filtering
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The batch size
v1.0.0,: The triples factory
v1.0.0,: Class of regularizer to test
v1.0.0,: The constructor parameters to pass to the regularizer
v1.0.0,": The regularizer instance, initialized in setUp"
v1.0.0,: A positive batch
v1.0.0,: The device
v1.0.0,Use RESCAL as it regularizes multiple tensors of different shape.
v1.0.0,Check if regularizer is stored correctly.
v1.0.0,Forward pass (should update regularizer)
v1.0.0,Call post_parameter_update (should reset regularizer)
v1.0.0,Check if regularization term is reset
v1.0.0,Call method
v1.0.0,Generate random tensors
v1.0.0,Call update
v1.0.0,check shape
v1.0.0,compute expected term
v1.0.0,Generate random tensor
v1.0.0,calculate penalty
v1.0.0,check shape
v1.0.0,check value
v1.0.0,Tests that exception will be thrown when more than or less than three tensors are passed
v1.0.0,Test that regularization term is computed correctly
v1.0.0,Entity soft constraint
v1.0.0,Orthogonality soft constraint
v1.0.0,"After first update, should change the term"
v1.0.0,"After second update, no change should happen"
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The number of embeddings
v1.0.0,: The embedding dimension
v1.0.0,check shape
v1.0.0,check values
v1.0.0,check shape
v1.0.0,check values
v1.0.0,check correct value range
v1.0.0,check maximum norm constraint
v1.0.0,unchanged values for small norms
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The window size used by the early stopper
v1.0.0,: The mock losses the mock evaluator will return
v1.0.0,: The (zeroed) index  - 1 at which stopping will occur
v1.0.0,: The minimum improvement
v1.0.0,Set automatic_memory_optimization to false for tests
v1.0.0,Step early stopper
v1.0.0,check storing of results
v1.0.0,check ring buffer
v1.0.0,: The window size used by the early stopper
v1.0.0,: The (zeroed) index  - 1 at which stopping will occur
v1.0.0,: The minimum improvement
v1.0.0,: The random seed to use for reproducibility
v1.0.0,: The maximum number of epochs to train. Should be large enough to allow for early stopping.
v1.0.0,: The epoch at which the stop should happen. Depends on the choice of random seed.
v1.0.0,: The batch size to use.
v1.0.0,Fix seed for reproducibility
v1.0.0,Set automatic_memory_optimization to false during testing
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Check if multilabels are working correctly
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The batch size
v1.0.0,: The random seed
v1.0.0,: The triples factory
v1.0.0,: The sLCWA instances
v1.0.0,: Class of negative sampling to test
v1.0.0,": The negative sampler instance, initialized in setUp"
v1.0.0,: A positive batch
v1.0.0,Generate negative sample
v1.0.0,check shape
v1.0.0,check bounds: heads
v1.0.0,check bounds: relations
v1.0.0,check bounds: tails
v1.0.0,Check that all elements got corrupted
v1.0.0,Generate scaled negative sample
v1.0.0,Generate negative samples
v1.0.0,test that the relations were not changed
v1.0.0,Test that half of the subjects and half of the objects are corrupted
v1.0.0,Generate negative sample for additional tests
v1.0.0,test that the relations were not changed
v1.0.0,sample a batch
v1.0.0,check shape
v1.0.0,get triples
v1.0.0,check connected components
v1.0.0,super inefficient
v1.0.0,join
v1.0.0,already joined
v1.0.0,check that there is only a single component
v1.0.0,check content of comp_adj_lists
v1.0.0,check edge ids
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The expected number of entities
v1.0.0,: The expected number of relations
v1.0.0,: The dataset to test
v1.0.0,Not loaded
v1.0.0,Load
v1.0.0,Test caching
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The class of the model to test
v1.0.0,: Additional arguments passed to the model's constructor method
v1.0.0,: The triples factory instance
v1.0.0,: The model instance
v1.0.0,: The batch size for use for forward_* tests
v1.0.0,: The embedding dimensionality
v1.0.0,: Whether to create inverse triples (needed e.g. by ConvE)
v1.0.0,: The sampler to use for sLCWA (different e.g. for R-GCN)
v1.0.0,: The batch size for use when testing training procedures
v1.0.0,: The number of epochs to train the model
v1.0.0,: A random number generator from torch
v1.0.0,: The number of parameters which receive a constant (i.e. non-randomized)
v1.0.0,initialization
v1.0.0,assert there is at least one trainable parameter
v1.0.0,Check that all the parameters actually require a gradient
v1.0.0,Try to initialize an optimizer
v1.0.0,get model parameters
v1.0.0,re-initialize
v1.0.0,check that the operation works in-place
v1.0.0,check that the parameters where modified
v1.0.0,check for finite values by default
v1.0.0,check whether a gradient can be back-propgated
v1.0.0,"assert batch comprises (head, relation) pairs"
v1.0.0,"assert batch comprises (relation, tail) pairs"
v1.0.0,TODO: Catch HolE MKL error?
v1.0.0,set regularizer term
v1.0.0,call post_parameter_update
v1.0.0,assert that the regularization term has been reset
v1.0.0,do one optimization step
v1.0.0,call post_parameter_update
v1.0.0,check model constraints
v1.0.0,"assert batch comprises (relation, tail) pairs"
v1.0.0,"assert batch comprises (relation, tail) pairs"
v1.0.0,"assert batch comprises (relation, tail) pairs"
v1.0.0,Distance-based model
v1.0.0,3x batch norm: bias + scale --> 6
v1.0.0,entity specific bias        --> 1
v1.0.0,==================================
v1.0.0,7
v1.0.0,"two bias terms, one conv-filter"
v1.0.0,Two linear layer biases
v1.0.0,"Two BN layers, bias & scale"
v1.0.0,: one bias per layer
v1.0.0,: (scale & bias for BN) * layers
v1.0.0,entity embeddings
v1.0.0,relation embeddings
v1.0.0,Compute Scores
v1.0.0,"self.assertAlmostEqual(second_score, -16, delta=0.01)"
v1.0.0,Use different dimension for relation embedding: relation_dim > entity_dim
v1.0.0,relation embeddings
v1.0.0,Compute Scores
v1.0.0,Use different dimension for relation embedding: relation_dim < entity_dim
v1.0.0,entity embeddings
v1.0.0,relation embeddings
v1.0.0,Compute Scores
v1.0.0,random entity embeddings & projections
v1.0.0,random relation embeddings & projections
v1.0.0,project
v1.0.0,check shape:
v1.0.0,check normalization
v1.0.0,entity embeddings
v1.0.0,relation embeddings
v1.0.0,Compute Scores
v1.0.0,second_score = scores[1].item()
v1.0.0,: 2xBN (bias & scale)
v1.0.0,check shape
v1.0.0,check content
v1.0.0,: The number of entities
v1.0.0,: The number of triples
v1.0.0,check shape
v1.0.0,check dtype
v1.0.0,check finite values (e.g. due to division by zero)
v1.0.0,check non-negativity
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,
v1.0.0,
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,check for finite values by default
v1.0.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.0,Set into training mode to check if it is correctly set to evaluation mode.
v1.0.0,Get embeddings
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The class
v1.0.0,: Constructor keyword arguments
v1.0.0,: The loss instance
v1.0.0,: The batch size
v1.0.0,test reduction
v1.0.0,Test backward
v1.0.0,: The number of entities.
v1.0.0,: The number of negative samples
v1.0.0,≈ result of softmax
v1.0.0,"neg_distances - margin = [-1., -1., 0., 0.]"
v1.0.0,"sigmoids ≈ [0.27, 0.27, 0.5, 0.5]"
v1.0.0,"pos_distances = [0., 0., 0.5, 0.5]"
v1.0.0,"margin - pos_distances = [1. 1., 0.5, 0.5]"
v1.0.0,≈ result of sigmoid
v1.0.0,"sigmoids ≈ [0.73, 0.73, 0.62, 0.62]"
v1.0.0,expected_loss ≈ 0.34
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Create dummy dense labels
v1.0.0,Check if labels form a probability distribution
v1.0.0,Apply label smoothing
v1.0.0,Check if smooth labels form probability distribution
v1.0.0,Create dummy sLCWA labels
v1.0.0,Apply label smoothing
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,"scale labels from [0, 1] to [-1, 1]"
v1.0.0,cross entropy expects a proper probability distribution -> normalize labels
v1.0.0,Use numerically stable variant to compute log(softmax)
v1.0.0,"compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)"
v1.0.0,"To add *all* losses implemented in Torch, uncomment:"
v1.0.0,_LOSSES.update({
v1.0.0,loss
v1.0.0,for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()
v1.0.0,if not loss.__name__.startswith('_')
v1.0.0,})
v1.0.0,Add empty dictionaries as defaults for all remaining losses
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: An error that occurs because the input in CUDA is too big. See ConvE for an example.
v1.0.0,Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.
v1.0.0,lower bound
v1.0.0,upper bound
v1.0.0,Allocate weight on device
v1.0.0,Initialize if initializer is provided
v1.0.0,Wrap embedding around it.
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The overall regularization weight
v1.0.0,: The current regularization term (a scalar)
v1.0.0,: Should the regularization only be applied once? This was used for ConvKB and defaults to False.
v1.0.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.0,no need to compute anything
v1.0.0,always return zero
v1.0.0,: The dimension along which to compute the vector-based regularization terms.
v1.0.0,: Whether to normalize the regularization term by the dimension of the vectors.
v1.0.0,: This allows dimensionality-independent weight tuning.
v1.0.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.0,expected value of |x|_1 = d*E[x_i] for x_i i.i.d.
v1.0.0,expected value of |x|_2 when x_i are normally distributed
v1.0.0,cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1
v1.0.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.0,: The default strategy for optimizing the regularizer's hyper-parameters
v1.0.0,The regularization in TransH enforces the defined soft constraints that should computed only for every batch.
v1.0.0,"Therefore, apply_only_once is always set to True."
v1.0.0,Entity soft constraint
v1.0.0,Orthogonality soft constraint
v1.0.0,The normalization factor to balance individual regularizers' contribution.
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Add HPO command
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The random seed used at the beginning of the pipeline
v1.0.0,: The model trained by the pipeline
v1.0.0,: The training loop used by the pipeline
v1.0.0,: The losses during training
v1.0.0,: The results evaluated by the pipeline
v1.0.0,: How long in seconds did training take?
v1.0.0,: How long in seconds did evaluation take?
v1.0.0,: An early stopper
v1.0.0,: Any additional metadata as a dictionary
v1.0.0,: The version of PyKEEN used to create these results
v1.0.0,: The git hash of PyKEEN used to create these results
v1.0.0,1. Dataset
v1.0.0,2. Model
v1.0.0,3. Loss
v1.0.0,4. Regularizer
v1.0.0,5. Optimizer
v1.0.0,6. Training Loop
v1.0.0,7. Training (ronaldo style)
v1.0.0,8. Evaluation
v1.0.0,Misc
v1.0.0,Create result store
v1.0.0,Start tracking
v1.0.0,FIXME this should never happen.
v1.0.0,Log model parameters
v1.0.0,Log optimizer parameters
v1.0.0,Stopping
v1.0.0,"Load the evaluation batch size for the stopper, if it has been set"
v1.0.0,By default there's a stopper that does nothing interesting
v1.0.0,Add logging for debugging
v1.0.0,Train like Cristiano Ronaldo
v1.0.0,Evaluate
v1.0.0,Reuse optimal evaluation parameters from training if available
v1.0.0,Add logging about evaluator for debugging
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,This will set the global logging level to info to ensure that info messages are shown in all parts of the software.
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Create directory in which all experimental artifacts are saved
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: Functions for specifying exotic resources with a given prefix
v1.0.0,: Functions for specifying exotic resources based on their file extension
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,"Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available"
v1.0.0,TODO vectorize code
v1.0.0,"row define entity, and column the literal. Set the corresponding literal for the entity"
v1.0.0,"FIXME is this ever possible, since this function is called in __init__?"
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Create lists out of sets for proper numpy indexing when loading the labels
v1.0.0,TODO is there a need to have a canonical sort order here?
v1.0.0,Split triples
v1.0.0,Sorting ensures consistent results when the triples are permuted
v1.0.0,Create mapping
v1.0.0,Sorting ensures consistent results when the triples are permuted
v1.0.0,Create mapping
v1.0.0,"When triples that don't exist are trying to be mapped, they get the id ""-1"""
v1.0.0,Filter all non-existent triples
v1.0.0,Note: Unique changes the order of the triples
v1.0.0,Note: Using unique means implicit balancing of training samples
v1.0.0,: The mapping from entities' labels to their indexes
v1.0.0,: The mapping from relations' labels to their indexes
v1.0.0,": A three-column matrix where each row are the head label,"
v1.0.0,": relation label, then tail label"
v1.0.0,": A three-column matrix where each row are the head identifier,"
v1.0.0,": relation identifier, then tail identifier"
v1.0.0,": A dictionary mapping each relation to its inverse, if inverse triples were created"
v1.0.0,TODO: Check if lazy evaluation would make sense
v1.0.0,Check if the triples are inverted already
v1.0.0,extend original triples with inverse ones
v1.0.0,Generate entity mapping if necessary
v1.0.0,Generate relation mapping if necessary
v1.0.0,Map triples of labels to triples of IDs.
v1.0.0,We can terminate the search after finding the first inverse occurrence
v1.0.0,Ensure 2d array in case only one triple was given
v1.0.0,FIXME this function is only ever used in tests
v1.0.0,Prepare shuffle index
v1.0.0,Prepare split index
v1.0.0,Take cumulative sum so the get separated properly
v1.0.0,Split triples
v1.0.0,Make new triples factories for each group
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Create dense target
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,basically take all candidates
v1.0.0,Calculate which relations are the inverse ones
v1.0.0,FIXME doesn't carry flag of create_inverse_triples through
v1.0.0,A dictionary of all of the head/tail pairs for a given relation
v1.0.0,A dictionary for all of the tail/head pairs for a given relation
v1.0.0,Calculate the similarity between each relationship (entries in ``forward``)
v1.0.0,with all other candidate inverse relationships (entries in ``inverse``)
v1.0.0,"Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the"
v1.0.0,"same as for ``(b, a)``"
v1.0.0,A dictionary of all of the head/tail pairs for a given relation
v1.0.0,Filter out results between a given relationship and itself
v1.0.0,Filter out results below a minimum frequency
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,preprocessing
v1.0.0,initialize
v1.0.0,sample iteratively
v1.0.0,determine weights
v1.0.0,only happens at first iteration
v1.0.0,normalize to probabilities
v1.0.0,sample a start node
v1.0.0,get list of neighbors
v1.0.0,sample an outgoing edge at random which has not been chosen yet using rejection sampling
v1.0.0,visit target node
v1.0.0,decrease sample counts
v1.0.0,return chosen edges
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Create training instances
v1.0.0,During size probing the training instances should not show the tqdm progress bar
v1.0.0,"In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared"
v1.0.0,Ensure the release of memory
v1.0.0,Clear optimizer
v1.0.0,"Take the biggest possible training batch_size, if batch_size not set"
v1.0.0,This will find necessary parameters to optimize the use of the hardware at hand
v1.0.0,return the relevant parameters slice_size and batch_size
v1.0.0,Create dummy result tracker
v1.0.0,Sanity check
v1.0.0,Force weight initialization if training continuation is not explicitly requested.
v1.0.0,Reset the weights
v1.0.0,Create new optimizer
v1.0.0,Ensure the model is on the correct device
v1.0.0,Create Sampler
v1.0.0,Bind
v1.0.0,"When size probing, we don't want progress bars"
v1.0.0,Create progress bar
v1.0.0,Training Loop
v1.0.0,Enforce training mode
v1.0.0,Accumulate loss over epoch
v1.0.0,Batching
v1.0.0,Only create a progress bar when not in size probing mode
v1.0.0,Flag to check when to quit the size probing
v1.0.0,Recall that torch *accumulates* gradients. Before passing in a
v1.0.0,"new instance, you need to zero out the gradients from the old instance"
v1.0.0,Get batch size of current batch (last batch may be incomplete)
v1.0.0,accumulate gradients for whole batch
v1.0.0,forward pass call
v1.0.0,"when called by batch_size_search(), the parameter update should not be applied."
v1.0.0,update parameters according to optimizer
v1.0.0,"After changing applying the gradients to the embeddings, the model is notified that the forward"
v1.0.0,constraints are no longer applied
v1.0.0,For testing purposes we're only interested in processing one batch
v1.0.0,When size probing we don't need the losses
v1.0.0,Track epoch loss
v1.0.0,Print loss information to console
v1.0.0,forward pass
v1.0.0,"raise error when non-finite loss occurs (NaN, +/-inf)"
v1.0.0,correction for loss reduction
v1.0.0,backward pass
v1.0.0,reset the regularizer to free the computational graph
v1.0.0,Set upper bound
v1.0.0,"If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing"
v1.0.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.0,The regularizer has to be reset to free the computational graph
v1.0.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Shuffle each epoch
v1.0.0,Lazy-splitting into batches
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Slicing is not possible in sLCWA training loops
v1.0.0,Send positive batch to device
v1.0.0,Create negative samples
v1.0.0,"Ensure they reside on the device (should hold already for most simple negative samplers, e.g."
v1.0.0,"BasicNegativeSampler, BernoulliNegativeSampler"
v1.0.0,Make it negative batch broadcastable (required for num_negs_per_pos > 1).
v1.0.0,Compute negative and positive scores
v1.0.0,Repeat positives scores (necessary for more than one negative per positive)
v1.0.0,Stack predictions
v1.0.0,Create target
v1.0.0,Normalize the loss to have the average loss per positive triple
v1.0.0,This allows comparability of sLCWA and LCWA losses
v1.0.0,Slicing is not possible for sLCWA
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: A mapping of training loops' names to their implementations
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Split batch components
v1.0.0,Send batch to device
v1.0.0,Apply label smoothing
v1.0.0,This shows how often one row has to be repeated
v1.0.0,Create boolean indices for negative labels in the repeated rows
v1.0.0,Repeat the predictions and filter for negative labels
v1.0.0,This tells us how often each true label should be repeated
v1.0.0,First filter the predictions for true labels and then repeat them based on the repeat vector
v1.0.0,Split positive and negative scores
v1.0.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The model
v1.0.0,: The evaluator
v1.0.0,: The triples to use for evaluation
v1.0.0,: Size of the evaluation batches
v1.0.0,: Slice size of the evaluation batches
v1.0.0,: The number of epochs after which the model is evaluated on validation set
v1.0.0,: The number of iterations (one iteration can correspond to various epochs)
v1.0.0,: with no improvement after which training will be stopped.
v1.0.0,: The name of the metric to use
v1.0.0,: The minimum improvement between two iterations
v1.0.0,: The metric results from all evaluations
v1.0.0,: A ring buffer to store the recent results
v1.0.0,: A counter for the ring buffer
v1.0.0,": Whether a larger value is better, or a smaller"
v1.0.0,: The criterion. Set in the constructor based on larger_is_better
v1.0.0,: The result tracker
v1.0.0,: Callbacks when training gets continued
v1.0.0,: Callbacks when training is stopped early
v1.0.0,: Did the stopper ever decide to stop?
v1.0.0,TODO: Fix this
v1.0.0,if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):
v1.0.0,raise ValueError(f'Invalid metric name: {self.metric}')
v1.0.0,Dummy result tracker
v1.0.0,Evaluate
v1.0.0,After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use
v1.0.0,Only check if enough values are already collected
v1.0.0,Stop if the result did not improve more than delta for patience epochs.
v1.0.0,Update ring buffer
v1.0.0,Append to history
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: A mapping of training loops' names to their implementations
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,"The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers."
v1.0.0,Clear the ranks from the current evaluator
v1.0.0,"We need to try slicing, if the evaluation for the batch_size search never succeeded"
v1.0.0,"Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,"
v1.0.0,"must have failed to start slice_size search, we start with trying half the entities."
v1.0.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.0,"Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage"
v1.0.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.0,The cache of the previous run has to be freed to allow accurate memory availability estimates
v1.0.0,Test if slicing is implemented for the required functions of this model
v1.0.0,Split batch
v1.0.0,Bind shape
v1.0.0,Set all filtered triples to NaN to ensure their exclusion in subsequent calculations
v1.0.0,Warn if all entities will be filtered
v1.0.0,"(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples."
v1.0.0,Send to device
v1.0.0,Ensure evaluation mode
v1.0.0,"Split evaluators into those which need unfiltered results, and those which require filtered ones"
v1.0.0,Check whether we need to be prepared for filtering
v1.0.0,Check whether an evaluator needs access to the masks
v1.0.0,This can only be an unfiltered evaluator.
v1.0.0,Prepare for result filtering
v1.0.0,Send tensors to device
v1.0.0,Prepare batches
v1.0.0,Show progressbar
v1.0.0,Flag to check when to quit the size probing
v1.0.0,Disable gradient tracking
v1.0.0,Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True
v1.0.0,batch-wise processing
v1.0.0,Predict tail scores once
v1.0.0,Create positive filter for all corrupted tails
v1.0.0,Create a positive mask with the size of the scores from the positive tails filter
v1.0.0,Evaluate metrics on these *unfiltered* tail scores
v1.0.0,Filter
v1.0.0,The scores for the true triples have to be rewritten to the scores tensor
v1.0.0,Evaluate metrics on these *filtered* tail scores
v1.0.0,Predict head scores once
v1.0.0,Create positive filter for all corrupted heads
v1.0.0,Create a positive mask with the size of the scores from the positive heads filter
v1.0.0,Evaluate metrics on these head scores
v1.0.0,Filter
v1.0.0,The scores for the true triples have to be rewritten to the scores tensor
v1.0.0,Evaluate metrics on these *filtered* tail scores
v1.0.0,If we only probe sizes we do not need more than one batch
v1.0.0,Finalize
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The area under the ROC curve
v1.0.0,: The area under the precision-recall curve
v1.0.0,: The coverage error
v1.0.0,coverage_error: float = field(metadata=dict(
v1.0.0,"doc='The coverage error',"
v1.0.0,"f=metrics.coverage_error,"
v1.0.0,))
v1.0.0,: The label ranking loss (APS)
v1.0.0,label_ranking_average_precision_score: float = field(metadata=dict(
v1.0.0,"doc='The label ranking loss (APS)',"
v1.0.0,"f=metrics.label_ranking_average_precision_score,"
v1.0.0,))
v1.0.0,#: The label ranking loss
v1.0.0,label_ranking_loss: float = field(metadata=dict(
v1.0.0,"doc='The label ranking loss',"
v1.0.0,"f=metrics.label_ranking_loss,"
v1.0.0,))
v1.0.0,Transfer to cpu and convert to numpy
v1.0.0,Ensure that each key gets counted only once
v1.0.0,"include head_side flag into key to differentiate between (h, r) and (r, t)"
v1.0.0,"Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and"
v1.0.0,masks using the exact same key order.
v1.0.0,TODO how to define a cutoff on y_scores to make binary?
v1.0.0,see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33
v1.0.0,Clear buffers
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: A mapping of evaluators' names to their implementations
v1.0.0,: A mapping of results' names to their implementations
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,The best rank is the rank when assuming all options with an equal score are placed behind the currently
v1.0.0,"considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based."
v1.0.0,The worst rank is the rank when assuming all options with an equal score are placed in front of the currently
v1.0.0,"considered. Hence, the rank is the number of options which have at least the same score minus one (as the"
v1.0.0,"currently considered option in included in all options). As the rank is one-based, we have to add 1, which"
v1.0.0,"nullifies the ""minus 1"" from before."
v1.0.0,"The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of"
v1.0.0,the elements with the same score as the currently considered option.
v1.0.0,"We set values which should be ignored to NaN, hence the number of options which should be considered is given by"
v1.0.0,The expected rank of a random scoring
v1.0.0,The adjusted ranks is normalized by the expected rank of a random scoring
v1.0.0,TODO adjusted_worst_rank
v1.0.0,TODO adjusted_best_rank
v1.0.0,: The mean over all ranks: mean_i r_i. Lower is better.
v1.0.0,: The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.
v1.0.0,": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k."
v1.0.0,: Higher is better.
v1.0.0,: The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.
v1.0.0,: Described by [berrendorf2020]_.
v1.0.0,Clear buffers
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Extend the batch to the number of IDs such that each pair can be combined with all possible IDs
v1.0.0,Create a tensor of all IDs
v1.0.0,Extend all IDs to the number of pairs such that each ID can be combined with every pair
v1.0.0,"Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor."
v1.0.0,: A dictionary of hyper-parameters to the models that use them
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The default loss function class
v1.0.0,: The default parameters for the default loss function class
v1.0.0,: The instance of the loss
v1.0.0,: The default regularizer class
v1.0.0,: The default parameters for the default regularizer class
v1.0.0,: The instance of the regularizer
v1.0.0,Initialize the device
v1.0.0,Random seeds have to set before the embeddings are initialized
v1.0.0,Loss
v1.0.0,TODO: Check loss functions that require 1 and -1 as label but only
v1.0.0,Regularizer
v1.0.0,The triples factory facilitates access to the dataset.
v1.0.0,This allows to store the optimized parameters
v1.0.0,Keep track of the hyper-parameters that are used across all
v1.0.0,subclasses of BaseModule
v1.0.0,Enforce evaluation mode
v1.0.0,Enforce evaluation mode
v1.0.0,Enforce evaluation mode
v1.0.0,Enforce evaluation mode
v1.0.0,The number of relations stored in the triples factory includes the number of inverse relations
v1.0.0,Id of inverse relation: relation + 1
v1.0.0,"The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs"
v1.0.0,"Extend the hr_batch such that each (h, r) pair is combined with all possible tails"
v1.0.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.0,Reshape the scores to match the pre-defined output shape of the score_t function.
v1.0.0,"Extend the rt_batch such that each (r, t) pair is combined with all possible heads"
v1.0.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.0,Reshape the scores to match the pre-defined output shape of the score_h function.
v1.0.0,"Extend the ht_batch such that each (h, t) pair is combined with all possible relations"
v1.0.0,"Calculate the scores for each (h, r, t) triple using the generic interaction function"
v1.0.0,Reshape the scores to match the pre-defined output shape of the score_r function.
v1.0.0,TODO: Why do we need that? The optimizer takes care of filtering the parameters.
v1.0.0,Default for relation dimensionality
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: A mapping of models' names to their implementations
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Store initial input for error message
v1.0.0,All are None
v1.0.0,"input_channels is None, and any of height or width is None -> set input_channels=1"
v1.0.0,"input channels is not None, and one of height or width is None"
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The default loss function class
v1.0.0,: The default parameters for the default loss function class
v1.0.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,L)"
v1.0.0,": If batch normalization is enabled, this is: num_features – C from an expected input of size (N,C,H,W)"
v1.0.0,ConvE should be trained with inverse triples
v1.0.0,ConvE uses one bias for each entity
v1.0.0,Automatic calculation of remaining dimensions
v1.0.0,Parameter need to fulfil:
v1.0.0,input_channels * embedding_height * embedding_width = embedding_dim
v1.0.0,Finalize initialization
v1.0.0,embeddings
v1.0.0,weights
v1.0.0,"batch_size, num_input_channels, 2*height, width"
v1.0.0,"batch_size, num_input_channels, 2*height, width"
v1.0.0,"batch_size, num_input_channels, 2*height, width"
v1.0.0,"(N,C_out,H_out,W_out)"
v1.0.0,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v1.0.0,Embedding Regularization
v1.0.0,"For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row"
v1.0.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.0,Embedding Regularization
v1.0.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.0,Embedding Regularization
v1.0.0,Code to repeat each item successively instead of the entire tensor
v1.0.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,Embeddings
v1.0.0,Finalize initialization
v1.0.0,Initialise left relation embeddings to unit length
v1.0.0,Make sure to call super first
v1.0.0,Normalise embeddings of entities
v1.0.0,Get embeddings
v1.0.0,Project entities
v1.0.0,Get embeddings
v1.0.0,Project entities
v1.0.0,Project entities
v1.0.0,Project entities
v1.0.0,Get embeddings
v1.0.0,Project entities
v1.0.0,Project entities
v1.0.0,Project entities
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The default loss function class
v1.0.0,: The default parameters for the default loss function class
v1.0.0,: The regularizer used by [trouillon2016]_ for ComplEx.
v1.0.0,: The LP settings used by [trouillon2016]_ for ComplEx.
v1.0.0,Finalize initialization
v1.0.0,"initialize with entity and relation embeddings with standard normal distribution, cf."
v1.0.0,https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487
v1.0.0,split into real and imaginary part
v1.0.0,ComplEx space bilinear product
v1.0.0,*: Elementwise multiplication
v1.0.0,get embeddings
v1.0.0,Compute scores
v1.0.0,Regularization
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The regularizer used by [nickel2011]_ for for RESCAL
v1.0.0,: According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py
v1.0.0,: a normalized weight of 10 is used.
v1.0.0,: The LP settings used by [nickel2011]_ for for RESCAL
v1.0.0,Finalize initialization
v1.0.0,Get embeddings
v1.0.0,"shape: (b, d)"
v1.0.0,"shape: (b, d, d)"
v1.0.0,"shape: (b, d)"
v1.0.0,Compute scores
v1.0.0,Regularization
v1.0.0,Compute scores
v1.0.0,Regularization
v1.0.0,Get embeddings
v1.0.0,Compute scores
v1.0.0,Regularization
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,Finalize initialization
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The regularizer used by [nguyen2018]_ for ConvKB.
v1.0.0,: The LP settings used by [nguyen2018]_ for ConvKB.
v1.0.0,The interaction model
v1.0.0,Finalize initialization
v1.0.0,embeddings
v1.0.0,Use Xavier initialization for weight; bias to zero
v1.0.0,"Initialize all filters to [0.1, 0.1, -0.1],"
v1.0.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36
v1.0.0,Output layer regularization
v1.0.0,In the code base only the weights of the output layer are used for regularization
v1.0.0,c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66
v1.0.0,Stack to convolution input
v1.0.0,Convolution
v1.0.0,"Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56"
v1.0.0,Linear layer for final scores
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,Finalize initialization
v1.0.0,": shape: (batch_size, num_entities, d)"
v1.0.0,": Prepare h: (b, e, d) -> (b, e, 1, 1, d)"
v1.0.0,": Prepare t: (b, e, d) -> (b, e, 1, d, 1)"
v1.0.0,": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)"
v1.0.0,"h.T @ W @ t, shape: (b, e, k, 1, 1)"
v1.0.0,": reduce (b, e, k, 1, 1) -> (b, e, k)"
v1.0.0,": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.0,": Prepare h: (b, e, d) -> (b, e, d, 1)"
v1.0.0,"V_h @ h, shape: (b, e, k, 1)"
v1.0.0,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.0,": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)"
v1.0.0,": Prepare t: (b, e, d) -> (b, e, d, 1)"
v1.0.0,"V_t @ t, shape: (b, e, k, 1)"
v1.0.0,": reduce (b, e, k, 1) -> (b, e, k)"
v1.0.0,": Prepare b: (R, k) -> (b, k) -> (b, 1, k)"
v1.0.0,"a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)"
v1.0.0,"prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)"
v1.0.0,"prepare act: (b, e, k) -> (b, e, 1, k)"
v1.0.0,"compute score, shape: (b, e, 1, 1)"
v1.0.0,reduce
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The regularizer used by [yang2014]_ for DistMult
v1.0.0,": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100"
v1.0.0,": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is"
v1.0.0,: why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.
v1.0.0,: The LP settings used by [yang2014]_ for DistMult
v1.0.0,Finalize initialization
v1.0.0,"xavier uniform, cf."
v1.0.0,https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17
v1.0.0,Initialise relation embeddings to unit length
v1.0.0,Make sure to call super first
v1.0.0,Normalize embeddings of entities
v1.0.0,Bilinear product
v1.0.0,*: Elementwise multiplication
v1.0.0,Get embeddings
v1.0.0,Compute score
v1.0.0,Only regularize relation embeddings
v1.0.0,Get embeddings
v1.0.0,Rank against all entities
v1.0.0,Only regularize relation embeddings
v1.0.0,Get embeddings
v1.0.0,Rank against all entities
v1.0.0,Only regularize relation embeddings
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: a = \mu^T\Sigma^{-1}\mu
v1.0.0,: b = \log \det \Sigma
v1.0.0,: a = tr(\Sigma_r^{-1}\Sigma_e)
v1.0.0,: b = (\mu_r - \mu_e)^T\Sigma_r^{-1}(\mu_r - \mu_e)
v1.0.0,: c = \log \frac{det(\Sigma_e)}{det(\Sigma_r)}
v1.0.0,= sum log (sigma_e)_i - sum log (sigma_r)_i
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,Similarity function used for distributions
v1.0.0,element-wise covariance bounds
v1.0.0,Additional covariance embeddings
v1.0.0,Finalize initialization
v1.0.0,Constraints are applied through post_parameter_update
v1.0.0,Make sure to call super first
v1.0.0,Normalize entity embeddings
v1.0.0,Ensure positive definite covariances matrices and appropriate size by clamping
v1.0.0,Get embeddings
v1.0.0,Compute entity distribution
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The custom regularizer used by [wang2014]_ for TransH
v1.0.0,: The settings used by [wang2014]_ for TransH
v1.0.0,embeddings
v1.0.0,Finalize initialization
v1.0.0,TODO: Add initialization
v1.0.0,Make sure to call super first
v1.0.0,Normalise the normal vectors by their l2 norms
v1.0.0,"As described in [wang2014], all entities and relations are used to compute the regularization term"
v1.0.0,which enforces the defined soft constraints.
v1.0.0,Get embeddings
v1.0.0,Project to hyperplane
v1.0.0,Regularization term
v1.0.0,Get embeddings
v1.0.0,Project to hyperplane
v1.0.0,Regularization term
v1.0.0,Get embeddings
v1.0.0,Project to hyperplane
v1.0.0,Regularization term
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,embeddings
v1.0.0,Finalize initialization
v1.0.0,Make sure to call super first
v1.0.0,Normalize entity embeddings
v1.0.0,TODO: Initialize from TransE
v1.0.0,Initialise relation embeddings to unit length
v1.0.0,"project to relation specific subspace, shape: (b, e, d_r)"
v1.0.0,ensure constraints
v1.0.0,"evaluate score function, shape: (b, e)"
v1.0.0,Get embeddings
v1.0.0,Get embeddings
v1.0.0,Get embeddings
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,Construct node neighbourhood mask
v1.0.0,Set nodes in batch to true
v1.0.0,Compute k-neighbourhood
v1.0.0,"if the target node needs an embeddings, so does the source node"
v1.0.0,Create edge mask
v1.0.0,pylint: disable=unused-argument
v1.0.0,"Calculate in-degree, i.e. number of incoming edges"
v1.0.0,pylint: disable=unused-argument
v1.0.0,"Calculate in-degree, i.e. number of incoming edges"
v1.0.0,: Interaction model used as decoder
v1.0.0,: The blocks of the relation-specific weight matrices
v1.0.0,": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)"
v1.0.0,: The base weight matrices to generate relation-specific weights
v1.0.0,": shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.0,: The relation-specific weights for each base
v1.0.0,": shape: (num_relations, num_bases)"
v1.0.0,: The biases for each layer (if used)
v1.0.0,": shape of each element: (embedding_dim,)"
v1.0.0,: Batch normalization for each layer (if used)
v1.0.0,: Activations for each layer (if used)
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,Instantiate model
v1.0.0,Heuristic
v1.0.0,buffering of messages
v1.0.0,"Save graph using buffers, such that the tensors are moved together with the model"
v1.0.0,Weights
v1.0.0,Finalize initialization
v1.0.0,invalidate enriched embeddings
v1.0.0,https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28
v1.0.0,Random convex-combination of bases for initialization (guarantees that initial weight matrices are
v1.0.0,initialized properly)
v1.0.0,We have one additional relation for self-loops
v1.0.0,Xavier Glorot initialization of each block
v1.0.0,Reset biases
v1.0.0,Reset batch norm parameters
v1.0.0,"Reset activation parameters, if any"
v1.0.0,use buffered messages if applicable
v1.0.0,Bind fields
v1.0.0,"shape: (num_entities, embedding_dim)"
v1.0.0,Edge dropout: drop the same edges on all layers (only in training mode)
v1.0.0,Get random dropout mask
v1.0.0,Apply to edges
v1.0.0,Different dropout for self-loops (only in training mode)
v1.0.0,"If batch is given, compute (num_layers)-hop neighbourhood"
v1.0.0,Initialize embeddings in the next layer for all nodes
v1.0.0,TODO: Can we vectorize this loop?
v1.0.0,Choose the edges which are of the specific relation
v1.0.0,Only propagate messages on subset of edges
v1.0.0,No edges available? Skip rest of inner loop
v1.0.0,Get source and target node indices
v1.0.0,send messages in both directions
v1.0.0,Select source node embeddings
v1.0.0,get relation weights
v1.0.0,Compute message (b x d) * (d x d) = (b x d)
v1.0.0,Normalize messages by relation-specific in-degree
v1.0.0,Aggregate messages in target
v1.0.0,Self-loop
v1.0.0,"Apply bias, if requested"
v1.0.0,"Apply batch normalization, if requested"
v1.0.0,Apply non-linearity
v1.0.0,allocate weight
v1.0.0,Get blocks
v1.0.0,"self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)"
v1.0.0,note: embedding_dim is guaranteed to be divisible by num_bases in the constructor
v1.0.0,"The current basis weights, shape: (num_bases)"
v1.0.0,"the current bases, shape: (num_bases, embedding_dim, embedding_dim)"
v1.0.0,"compute the current relation weights, shape: (embedding_dim, embedding_dim)"
v1.0.0,Enrich embeddings
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The default loss function class
v1.0.0,: The default parameters for the default loss function class
v1.0.0,Core tensor
v1.0.0,Note: we use a different dimension permutation as in the official implementation to match the paper.
v1.0.0,Dropout
v1.0.0,Finalize initialization
v1.0.0,"Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12"
v1.0.0,Abbreviation
v1.0.0,Compute h_n = DO(BN(h))
v1.0.0,Compute wr = DO(W x_2 r)
v1.0.0,compute whr = DO(BN(h_n x_1 wr))
v1.0.0,Compute whr x_3 t
v1.0.0,Get embeddings
v1.0.0,Compute scores
v1.0.0,Get embeddings
v1.0.0,Compute scores
v1.0.0,Get embeddings
v1.0.0,Compute scores
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,Finalize initialization
v1.0.0,Initialise relation embeddings to unit length
v1.0.0,Make sure to call super first
v1.0.0,Normalize entity embeddings
v1.0.0,Get embeddings
v1.0.0,Get embeddings
v1.0.0,Get embeddings
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The default loss function class
v1.0.0,: The default parameters for the default loss function class
v1.0.0,: The regularizer used by [trouillon2016]_ for SimplE
v1.0.0,": In the paper, they use weight of 0.1, and do not normalize the"
v1.0.0,": regularization term by the number of elements, which is 200."
v1.0.0,: The power sum settings used by [trouillon2016]_ for SimplE
v1.0.0,extra embeddings
v1.0.0,Finalize initialization
v1.0.0,forward model
v1.0.0,Regularization
v1.0.0,backward model
v1.0.0,Regularization
v1.0.0,"Note: In the code in their repository, the score is clamped to [-20, 20]."
v1.0.0,"That is not mentioned in the paper, so it is omitted here."
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,Finalize initialization
v1.0.0,"The authors do not specify which initialization was used. Hence, we use the pytorch default."
v1.0.0,weight initialization
v1.0.0,Get embeddings
v1.0.0,Embedding Regularization
v1.0.0,Concatenate them
v1.0.0,Compute scores
v1.0.0,Get embeddings
v1.0.0,Embedding Regularization
v1.0.0,First layer can be unrolled
v1.0.0,Send scores through rest of the network
v1.0.0,Get embeddings
v1.0.0,Embedding Regularization
v1.0.0,First layer can be unrolled
v1.0.0,Send scores through rest of the network
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,The dimensions affected by e'
v1.0.0,Project entities
v1.0.0,r_p (e_p.T e) + e'
v1.0.0,Enforce constraints
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,Finalize initialization
v1.0.0,Make sure to call super first
v1.0.0,Normalize entity embeddings
v1.0.0,Project entities
v1.0.0,score = -||h_bot + r - t_bot||_2^2
v1.0.0,Head
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,Finalize initialization
v1.0.0,phases randomly between 0 and 2 pi
v1.0.0,Make sure to call super first
v1.0.0,Normalize relation embeddings
v1.0.0,Decompose into real and imaginary part
v1.0.0,Rotate (=Hadamard product in complex space).
v1.0.0,Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed
v1.0.0,Get embeddings
v1.0.0,Compute scores
v1.0.0,Embedding Regularization
v1.0.0,Get embeddings
v1.0.0,Rank against all entities
v1.0.0,Compute scores
v1.0.0,Embedding Regularization
v1.0.0,Get embeddings
v1.0.0,r expresses a rotation in complex plane.
v1.0.0,The inverse rotation is expressed by the complex conjugate of r.
v1.0.0,The score is computed as the distance of the relation-rotated head to the tail.
v1.0.0,"Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e."
v1.0.0,|h * r - t| = |h - conj(r) * t|
v1.0.0,Rank against all entities
v1.0.0,Compute scores
v1.0.0,Embedding Regularization
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The default loss function class
v1.0.0,: The default parameters for the default loss function class
v1.0.0,Global entity projection
v1.0.0,Global relation projection
v1.0.0,Global combination bias
v1.0.0,Global combination bias
v1.0.0,Finalize initialization
v1.0.0,Get embeddings
v1.0.0,Compute score
v1.0.0,Get embeddings
v1.0.0,Rank against all entities
v1.0.0,Get embeddings
v1.0.0,Rank against all entities
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,Finalize initialization
v1.0.0,Make sure to call super first
v1.0.0,Normalize entity embeddings
v1.0.0,"Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27"
v1.0.0,Circular correlation of entity embeddings
v1.0.0,"complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)"
v1.0.0,Hadamard product in frequency domain
v1.0.0,"inverse real FFT, shape: (batch_size, num_entities, d)"
v1.0.0,inner product with relation embedding
v1.0.0,Embedding Regularization
v1.0.0,Embedding Regularization
v1.0.0,Embedding Regularization
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The default loss function class
v1.0.0,: The default parameters for the default loss function class
v1.0.0,Finalize initialization
v1.0.0,Get embeddings
v1.0.0,Embedding Regularization
v1.0.0,Concatenate them
v1.0.0,Predict t embedding
v1.0.0,compare with all t's
v1.0.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.0,Embedding Regularization
v1.0.0,Concatenate them
v1.0.0,Predict t embedding
v1.0.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.0,Embedding Regularization
v1.0.0,"Extend each rt_batch of ""r"" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]"
v1.0.0,"Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]"
v1.0.0,"h = torch.repeat_interleave(h, rt_batch_size, dim=0)"
v1.0.0,Extend t
v1.0.0,Concatenate them
v1.0.0,Predict t embedding
v1.0.0,"For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row"
v1.0.0,The results have to be realigned with the expected output of the score_h function
v1.0.0,The application of the sigmoid during training is automatically handled by the default loss.
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,TODO: Check entire build of the model
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The default loss function class
v1.0.0,: The default parameters for the default loss function class
v1.0.0,Literal
v1.0.0,num_ent x num_lit
v1.0.0,Number of columns corresponds to number of literals
v1.0.0,Literals
v1.0.0,End literals
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,TODO: Check entire build of the model
v1.0.0,: The default strategy for optimizing the model's hyper-parameters
v1.0.0,: The default parameters for the default loss function class
v1.0.0,Embeddings
v1.0.0,Number of columns corresponds to number of literals
v1.0.0,apply dropout
v1.0.0,"-, because lower score shall correspond to a more plausible triple."
v1.0.0,TODO check if this is the same as the BaseModule
v1.0.0,Choose y = -1 since a smaller score is better.
v1.0.0,"In TransE for example, the scores represent distances"
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.0,Bind number of negatives to sample
v1.0.0,Equally corrupt head and tail
v1.0.0,Copy positive batch for corruption.
v1.0.0,"Do not detach, as no gradients should flow into the indices."
v1.0.0,Sample random entities as replacement
v1.0.0,Replace heads – To make sure we don't replace the head by the original value
v1.0.0,we shift all values greater or equal than the original value by one up
v1.0.0,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.0,Corrupt tails
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: A mapping of negative samplers' names to their implementations
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: The default strategy for optimizing the negative sampler's hyper-parameters
v1.0.0,Preprocessing: Compute corruption probabilities
v1.0.0,"compute tph, i.e. the average number of tail entities per head"
v1.0.0,"compute hpt, i.e. the average number of head entities per tail"
v1.0.0,Set parameter for Bernoulli distribution
v1.0.0,Bind number of negatives to sample
v1.0.0,Copy positive batch for corruption.
v1.0.0,"Do not detach, as no gradients should flow into the indices."
v1.0.0,Decide whether to corrupt head or tail
v1.0.0,Tails are corrupted if heads are not corrupted
v1.0.0,Randomly sample corruption
v1.0.0,Replace heads – To make sure we don't replace the head by the original value
v1.0.0,we shift all values greater or equal than the original value by one up
v1.0.0,"for that reason we choose the random value from [0, num_entities -1]"
v1.0.0,Replace tails
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,TODO what happens if already exists?
v1.0.0,TODO incorporate setting of random seed
v1.0.0,pipeline_kwargs=dict(
v1.0.0,"random_seed=random.randint(1, 2 ** 32 - 1),"
v1.0.0,"),"
v1.0.0,Add dataset to current_pipeline
v1.0.0,Add loss function to current_pipeline
v1.0.0,Add regularizer to current_pipeline
v1.0.0,Add optimizer to current_pipeline
v1.0.0,Add training approach to current_pipeline
v1.0.0,Add training kwargs and kwargs_ranges
v1.0.0,Add evaluation
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: A factory wrapping the training triples
v1.0.0,": A factory wrapping the testing triples, that share indexes with the training triples"
v1.0.0,": A factory wrapping the validation triples, that share indexes with the training triples"
v1.0.0,: All data sets should take care of inverse triple creation
v1.0.0,": The actual instance of the training factory, which is exposed to the user through `training`"
v1.0.0,": The actual instance of the testing factory, which is exposed to the user through `testing`"
v1.0.0,": The actual instance of the validation factory, which is exposed to the user through `validation`"
v1.0.0,don't call this function by itself. assumes called through the `validation`
v1.0.0,property and the _training factory has already been loaded
v1.0.0,see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content
v1.0.0,pattern from https://stackoverflow.com/a/39217788/5775947
v1.0.0,TODO replace this with the new zip remote dataset class
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,: A mapping of data sets' names to their classes
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,TODO update docs with table and CLI wtih generator
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,1. Dataset
v1.0.0,2. Model
v1.0.0,3. Loss
v1.0.0,4. Regularizer
v1.0.0,5. Optimizer
v1.0.0,6. Training Loop
v1.0.0,7. Training
v1.0.0,8. Evaluation
v1.0.0,Misc.
v1.0.0,2. Model
v1.0.0,3. Loss
v1.0.0,4. Regularizer
v1.0.0,5. Optimizer
v1.0.0,1. Dataset
v1.0.0,2. Model
v1.0.0,3. Loss
v1.0.0,4. Regularizer
v1.0.0,5. Optimizer
v1.0.0,6. Training Loop
v1.0.0,7. Training
v1.0.0,8. Evaluation
v1.0.0,Misc.
v1.0.0,Will trigger Optuna to set the state of the trial as failed
v1.0.0,: The :mod:`optuna` study object
v1.0.0,": The objective class, containing information on preset hyper-parameters and those to optimize"
v1.0.0,Output study information
v1.0.0,Output all trials
v1.0.0,Output best trial as pipeline configuration file
v1.0.0,1. Dataset
v1.0.0,2. Model
v1.0.0,3. Loss
v1.0.0,4. Regularizer
v1.0.0,5. Optimizer
v1.0.0,6. Training Loop
v1.0.0,7. Training
v1.0.0,8. Evaluation
v1.0.0,6. Misc
v1.0.0,Optuna Study Settings
v1.0.0,Optuna Optimization Settings
v1.0.0,0. Metadata/Provenance
v1.0.0,1. Dataset
v1.0.0,FIXME difference between dataset class and string
v1.0.0,FIXME how to handle if dataset or factories were set? Should have been
v1.0.0,part of https://github.com/mali-git/POEM_develop/pull/483
v1.0.0,2. Model
v1.0.0,3. Loss
v1.0.0,4. Regularizer
v1.0.0,5. Optimizer
v1.0.0,6. Training Loop
v1.0.0,7. Training
v1.0.0,8. Evaluation
v1.0.0,1. Dataset
v1.0.0,2. Model
v1.0.0,3. Loss
v1.0.0,4. Regularizer
v1.0.0,5. Optimizer
v1.0.0,6. Training Loop
v1.0.0,7. Training
v1.0.0,8. Evaluation
v1.0.0,Optuna Misc.
v1.0.0,Pipeline Misc.
v1.0.0,Invoke optimization of the objective function.
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v1.0.0,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,
v0.0.26,Configuration file for the Sphinx documentation builder.
v0.0.26,
v0.0.26,This file does only contain a selection of the most common options. For a
v0.0.26,full list see the documentation:
v0.0.26,http://www.sphinx-doc.org/en/master/config
v0.0.26,-- Path setup --------------------------------------------------------------
v0.0.26,"If extensions (or modules to document with autodoc) are in another directory,"
v0.0.26,add these directories to sys.path here. If the directory is relative to the
v0.0.26,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.0.26,
v0.0.26,"sys.path.insert(0, os.path.abspath('..'))"
v0.0.26,-- Mockup PyTorch to exclude it while compiling the docs--------------------
v0.0.26,from unittest.mock import Mock
v0.0.26,sys.modules['numpy'] = Mock()
v0.0.26,sys.modules['numpy.linalg'] = Mock()
v0.0.26,sys.modules['scipy'] = Mock()
v0.0.26,sys.modules['scipy.optimize'] = Mock()
v0.0.26,sys.modules['scipy.interpolate'] = Mock()
v0.0.26,sys.modules['scipy.sparse'] = Mock()
v0.0.26,sys.modules['scipy.ndimage'] = Mock()
v0.0.26,sys.modules['scipy.ndimage.filters'] = Mock()
v0.0.26,sys.modules['tensorflow'] = Mock()
v0.0.26,sys.modules['theano'] = Mock()
v0.0.26,sys.modules['theano.tensor'] = Mock()
v0.0.26,sys.modules['torch'] = Mock()
v0.0.26,sys.modules['torch.optim'] = Mock()
v0.0.26,sys.modules['torch.nn'] = Mock()
v0.0.26,sys.modules['torch.nn.init'] = Mock()
v0.0.26,sys.modules['torch.autograd'] = Mock()
v0.0.26,sys.modules['sklearn'] = Mock()
v0.0.26,sys.modules['sklearn.model_selection'] = Mock()
v0.0.26,sys.modules['sklearn.utils'] = Mock()
v0.0.26,-- Project information -----------------------------------------------------
v0.0.26,"The full version, including alpha/beta/rc tags."
v0.0.26,The short X.Y version.
v0.0.26,-- General configuration ---------------------------------------------------
v0.0.26,"If your documentation needs a minimal Sphinx version, state it here."
v0.0.26,
v0.0.26,needs_sphinx = '1.0'
v0.0.26,"Add any Sphinx extension module names here, as strings. They can be"
v0.0.26,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.0.26,ones.
v0.0.26,"Add any paths that contain templates here, relative to this directory."
v0.0.26,The suffix(es) of source filenames.
v0.0.26,You can specify multiple suffix as a list of string:
v0.0.26,
v0.0.26,"source_suffix = ['.rst', '.md']"
v0.0.26,The master toctree document.
v0.0.26,The language for content autogenerated by Sphinx. Refer to documentation
v0.0.26,for a list of supported languages.
v0.0.26,
v0.0.26,This is also used if you do content translation via gettext catalogs.
v0.0.26,"Usually you set ""language"" from the command line for these cases."
v0.0.26,"List of patterns, relative to source directory, that match files and"
v0.0.26,directories to ignore when looking for source files.
v0.0.26,This pattern also affects html_static_path and html_extra_path.
v0.0.26,The name of the Pygments (syntax highlighting) style to use.
v0.0.26,-- Options for HTML output -------------------------------------------------
v0.0.26,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.0.26,a list of builtin themes.
v0.0.26,
v0.0.26,Theme options are theme-specific and customize the look and feel of a theme
v0.0.26,"further.  For a list of options available for each theme, see the"
v0.0.26,documentation.
v0.0.26,
v0.0.26,html_theme_options = {}
v0.0.26,"Add any paths that contain custom static files (such as style sheets) here,"
v0.0.26,"relative to this directory. They are copied after the builtin static files,"
v0.0.26,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.0.26,html_static_path = ['_static']
v0.0.26,"Custom sidebar templates, must be a dictionary that maps document names"
v0.0.26,to template names.
v0.0.26,
v0.0.26,The default sidebars (for documents that don't match any pattern) are
v0.0.26,defined by theme itself.  Builtin themes are using these templates by
v0.0.26,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.0.26,'searchbox.html']``.
v0.0.26,
v0.0.26,html_sidebars = {}
v0.0.26,-- Options for HTMLHelp output ---------------------------------------------
v0.0.26,Output file base name for HTML help builder.
v0.0.26,-- Options for LaTeX output ------------------------------------------------
v0.0.26,The paper size ('letterpaper' or 'a4paper').
v0.0.26,
v0.0.26,"'papersize': 'letterpaper',"
v0.0.26,"The font size ('10pt', '11pt' or '12pt')."
v0.0.26,
v0.0.26,"'pointsize': '10pt',"
v0.0.26,Additional stuff for the LaTeX preamble.
v0.0.26,
v0.0.26,"'preamble': '',"
v0.0.26,Latex figure (float) alignment
v0.0.26,
v0.0.26,"'figure_align': 'htbp',"
v0.0.26,Grouping the document tree into LaTeX files. List of tuples
v0.0.26,"(source start file, target name, title,"
v0.0.26,"author, documentclass [howto, manual, or own class])."
v0.0.26,-- Options for manual page output ------------------------------------------
v0.0.26,One entry per manual page. List of tuples
v0.0.26,"(source start file, name, description, authors, manual section)."
v0.0.26,-- Options for Texinfo output ----------------------------------------------
v0.0.26,Grouping the document tree into Texinfo files. List of tuples
v0.0.26,"(source start file, target name, title, author,"
v0.0.26,"dir menu entry, description, category)"
v0.0.26,-- Options for Epub output -------------------------------------------------
v0.0.26,Bibliographic Dublin Core info.
v0.0.26,The unique identifier of the text. This can be a ISBN number
v0.0.26,or the project homepage.
v0.0.26,
v0.0.26,epub_identifier = ''
v0.0.26,A unique identification for the text.
v0.0.26,
v0.0.26,epub_uid = ''
v0.0.26,A list of files that should not be packed into the epub file.
v0.0.26,-- Extension configuration -------------------------------------------------
v0.0.26,-- Options for intersphinx extension ---------------------------------------
v0.0.26,Example configuration for intersphinx: refer to the Python standard library.
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,10 % of training set will be used as a test set
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Load Drugbank
v0.0.26,Load BioPlex 2.0
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,: Functions for specifying exotic resources with a given prefix
v0.0.26,KG embedding model
v0.0.26,Model names
v0.0.26,Evaluator
v0.0.26,Output paths
v0.0.26,Device related
v0.0.26,ML params
v0.0.26,TransH related
v0.0.26,ConvE related
v0.0.26,OPTIMIZER
v0.0.26,Further Constants
v0.0.26,Pipeline outcome parameters
v0.0.26,-----------------Command line interface messages-----------------
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,: The configuration used to train the KGE model
v0.0.26,: The pipeline used to train the KGE model
v0.0.26,: The results of training the KGE model
v0.0.26,"In HPO model initial configuration is different from final configurations, that's why we differentiate"
v0.0.26,Save trained model
v0.0.26,Export experimental artifacts
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Load configuration file
v0.0.26,Load entity to id mapping
v0.0.26,Load relation to id mapping
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Device selection
v0.0.26,Entity dimensions
v0.0.26,Embeddings
v0.0.26,"num_features – C from an expected input of size (N,C,L)"
v0.0.26,"num_features – C from an expected input of size (N,C,H,W)"
v0.0.26,"batch_size, num_input_channels, 2*height, width"
v0.0.26,"batch_size, num_input_channels, 2*height, width"
v0.0.26,"batch_size, num_input_channels, 2*height, width"
v0.0.26,"(N,C_out,H_out,W_out)"
v0.0.26,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v0.0.26,Class 0 represents false fact and class 1 represents true fact
v0.0.26,"batch_size, num_input_channels, width, height"
v0.0.26,"batch_size, num_input_channels, 2*height, width"
v0.0.26,"batch_size, num_input_channels, 2*height, width"
v0.0.26,"batch_size, num_input_channels, 2*height, width"
v0.0.26,"(N,C_out,H_out,W_out)"
v0.0.26,"batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)"
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Embeddings
v0.0.26,FIXME @mehdi why aren't the right relation embeddings initialized?
v0.0.26,"triples = torch.tensor(triples, dtype=torch.long, device=self.device)"
v0.0.26,Normalise embeddings of entities
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Embeddings
v0.0.26,"triples = torch.tensor(triples, dtype=torch.long, device=self.device)"
v0.0.26,Compute score and transform result to 1D tensor
v0.0.26,"scores = torch.bmm(torch.transpose(h_emb, 1, 2), M)  # h^T M"
v0.0.26,"scores = torch.bmm(scores, t_emb)  # (h^T M) h"
v0.0.26,"scores = score.view(-1, 1)"
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,"triples = torch.tensor(triples, dtype=torch.long, device=self.device)"
v0.0.26,Normalize embeddings of entities
v0.0.26,Add the vector element wise
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,A simple lookup table that stores embeddings of a fixed dictionary and size
v0.0.26,TODO: Add initialization
v0.0.26,Add the vector element wise
v0.0.26,Normalise the normal vectors by their l2 norms
v0.0.26,"Shape: (batch_size, 1, embedding_dimension)"
v0.0.26,Reshape relation embeddings to the same shape of the projected entities
v0.0.26,Reshape relation embeddings to the same shape of the projected entities
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Embeddings
v0.0.26,"triples = torch.tensor(triples, dtype=torch.long, device=self.device)"
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,A simple lookup table that stores embeddings of a fixed dictionary and size
v0.0.26,TODO: Add initialization
v0.0.26,Add the vector element wise
v0.0.26,Normalise the normal vectors by their l2 norms
v0.0.26,"Shape: (batch_size, 1, embedding_dimension)"
v0.0.26,Reshape relation embeddings to the same shape of the projected entities
v0.0.26,Reshape relation embeddings to the same shape of the projected entities
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,TODO: max_norm < 1.
v0.0.26,max_norm = 1 according to the paper
v0.0.26,Embeddings
v0.0.26,max_norm = 1 according to the paper
v0.0.26,Add the vector element wise
v0.0.26,"triples = torch.tensor(triples, dtype=torch.long, device=self.device)"
v0.0.26,Project entities into relation space
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,: A mapping from KGE model names to KGE model classes
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Embeddings
v0.0.26,Normalize embeddings of entities
v0.0.26,Add the vector element wise
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,: Embeddings for relations in the knowledge graph
v0.0.26,"triples = torch.tensor(triples, dtype=torch.long, device=self.device)"
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Embeddings
v0.0.26,A simple lookup table that stores embeddings of a fixed dictionary and size
v0.0.26,"triples = torch.tensor(triples, dtype=torch.long, device=self.device)"
v0.0.26,Add the vector element wise
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Device selection
v0.0.26,Loss
v0.0.26,Entity dimensions
v0.0.26,: The number of entities in the knowledge graph
v0.0.26,: The number of unique relation types in the knowledge graph
v0.0.26,: The dimension of the embeddings to generate
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Ask whether to evaluate the model
v0.0.26,"Step 2: Specify test set, if is_evaluation_mode==True"
v0.0.26,Ask whether to use filtered negative triples
v0.0.26,Query number of HPO iterations
v0.0.26,Step 1: Welcome + Intro
v0.0.26,Step 2: Ask for training file
v0.0.26,Step 3: Ask for execution mode
v0.0.26,Step 4: Ask for model
v0.0.26,Step 5: Query parameters depending on the selected execution mode
v0.0.26,Step 5.5: Prompt for evaluation parameters depending on the selected execution mode
v0.0.26,Step 6: Please select a random seed
v0.0.26,Step 7: Query device to train on
v0.0.26,Step 8: Define output directory
v0.0.26,"config_path = os.path.join(config[OUTPUT_DIREC], 'configuration.json')"
v0.0.26,"with open(config_path, 'w') as file:"
v0.0.26,"json.dump(config, file, indent=2)"
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Query embedding dimension
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norm as scoring function
v0.0.26,Step 4: Query learning rate
v0.0.26,Step 5: Query batch size
v0.0.26,Step 6: Query number of epochs
v0.0.26,Step 1: Query embedding dimensions
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norms to use as scoring function
v0.0.26,Step 4: Query L_p norms for normalizing the entities
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Query embedding dimension
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norm as scoring function
v0.0.26,Step 4: Query L_p norm for normalizing the entities
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,Step 1: Query embedding dimensions
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norms to use as scoring function
v0.0.26,Step 4: Query L_p norms for normalizing the entities
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Query embedding dimension for entities
v0.0.26,Step 2: Query embedding dimension for relations
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norm as scoring function
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,Step 1: Query embedding dimensions for entities
v0.0.26,Step 2: Query embedding dimensions for relations
v0.0.26,Step 3: Query margin losses
v0.0.26,Step 4: Query L_p norms to use as scoring function
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Query embedding dimension
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norm as scoring function
v0.0.26,Step 4: Query L_p norm for normalizing the entities
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,Step 1: Query embedding dimensions
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norms to use as scoring function
v0.0.26,Step 4: Query L_p norms for normalizing the entities
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Query embedding dimension
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query learning rate
v0.0.26,Step 4: Query batch size
v0.0.26,Step 5: Query number of epochs
v0.0.26,Step 1: Query embedding dimensions
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query learning rate
v0.0.26,Step 4: Query batch size
v0.0.26,Step 5: Query number of epochs
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Query embedding dimension
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norm as scoring function
v0.0.26,Step 4: Query weight for the soft constraints
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,Step 1: Query embedding dimensions
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norms to use as scoring function
v0.0.26,Step 4: Query weight for the soft constraints
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Query embedding dimension for entities
v0.0.26,Step 2: Query embedding dimension for relations
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norm as scoring function
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,Step 1: Query embedding dimensions for entities
v0.0.26,Step 2: Query embedding dimensions for relations
v0.0.26,Step 3: Query margin losses
v0.0.26,Step 4: Query L_p norms to use as scoring function
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Query embedding dimension
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norm for normalizing the entities
v0.0.26,print_entity_normalization_message()
v0.0.26,"entity_normalization_norm = select_integer_value(print_msg=ENTITIES_NORMALIZATION_PRINT_MSG,"
v0.0.26,"prompt_msg=ENTITIES_NORMALIZATION_PROMPT_MSG,"
v0.0.26,error_msg=ENTITIES_NORMALIZATION_ERROR_MSG)
v0.0.26,config[NORM_FOR_NORMALIZATION_OF_ENTITIES] = entity_normalization_norm
v0.0.26,print_section_divider()
v0.0.26,Step 4: Query learning rate
v0.0.26,Step 5: Query batch size
v0.0.26,Step 6: Query number of epochs
v0.0.26,Step 1: Query embedding dimensions
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norms for normalizing the entities
v0.0.26,print_hpo_entity_normalization_norms_message()
v0.0.26,entity_normalization_norm = select_positive_integer_values(
v0.0.26,"print_msg=NORMS_FOR_NORMALIZATION_OF_ENTITIES_PRINT_MSG,"
v0.0.26,"prompt_msg=NORMS_FOR_NORMALIZATION_OF_ENTITIES_PROMPT_MSG,"
v0.0.26,error_msg=NORMS_FOR_NORMALIZATION_OF_ENTITIES_ERROR_MSG
v0.0.26,)
v0.0.26,config[NORM_FOR_NORMALIZATION_OF_ENTITIES] = entity_normalization_norm
v0.0.26,print_section_divider()
v0.0.26,Step 4: Query learning rate
v0.0.26,Step 5: Query batch size
v0.0.26,Step 6: Query number of epochs
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Query embedding dimension
v0.0.26,Step 2: Query height and width
v0.0.26,Step 3: Query number of input channels
v0.0.26,Step 4: Query number of output channels
v0.0.26,Step 4: Query kernel height
v0.0.26,Step 5: Query kernel width
v0.0.26,Step 6: Query dropout for input layer
v0.0.26,Step 7: Query dropout for output layer
v0.0.26,Step 8: Query feature map dropout for output layer
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,Step 1: Query embedding dimension
v0.0.26,Step 2: Query height and width
v0.0.26,Step 3: Query number of input channels
v0.0.26,Step 4: Query number of output channels
v0.0.26,Step 4: Query kernel height
v0.0.26,Step 5: Query kernel width
v0.0.26,Step 6: Query dropout for input layer
v0.0.26,Step 7: Query dropout for output layer
v0.0.26,Step 8: Query feature map dropout for output layer
v0.0.26,Step 9: Query learning rate
v0.0.26,Step 10: Query batch size
v0.0.26,Step 11: Query number of epochs
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Step 1: Query embedding dimension
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norm as scoring function
v0.0.26,Step 4: Query L_p norm for normalizing the entities
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,Step 1: Query embedding dimensions
v0.0.26,Step 2: Query margin loss
v0.0.26,Step 3: Query L_p norms to use as scoring function
v0.0.26,Step 4: Query L_p norms for normalizing the entities
v0.0.26,Step 5: Query learning rate
v0.0.26,Step 6: Query batch size
v0.0.26,Step 7: Query number of epochs
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,"model_name in {TRANS_E_NAME, TRANS_H_NAME, TRANS_D_NAME, TRANS_R_NAME, DISTMULT_NAME, UM_NAME, SE_NAME, ERMLP_NAME, RESCAL_NAME}"
v0.0.26,Recall that torch *accumulates* gradients. Before passing in a
v0.0.26,"new instance, you need to zero out the gradients from the old instance"
v0.0.26,"log.info(""Epoch %s took %s seconds \n"" % (str(epoch), str(round(stop - start))))"
v0.0.26,Track epoch loss
v0.0.26,TODO: Make sure that batch = num_pos + num_negs
v0.0.26,num_negatives = batch_size - num_positives
v0.0.26,TODO: Remove original subject and object from entity set
v0.0.26,Recall that torch *accumulates* gradients. Before passing in a
v0.0.26,"new instance, you need to zero out the gradients from the old"
v0.0.26,instance
v0.0.26,"log.info(""Epoch %s took %s seconds \n"" % (str(epoch), str(round(stop - start))))"
v0.0.26,Track epoch loss
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Initialize KG embedding model
v0.0.26,Prepare Output
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,"Extract current test tuple: Either (subject,predicate) or (predicate,object)"
v0.0.26,Copy current test tuple
v0.0.26,TODO: Check
v0.0.26,Get index of first occurrence that fulfills the condition
v0.0.26,"Compute hits@k for k in {1,3,5,10}"
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Note: Unique changes the order
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,-*- coding: utf-8 -*-
v0.0.26,Sample params which are dependent on each other
v0.0.26,Sample hyper-params
v0.0.26,Configure defined model
v0.0.26,Evaluate trained model
v0.0.26,TODO: Define HPO metric
v0.0.26,-*- coding: utf-8 -*-
