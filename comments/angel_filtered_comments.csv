Version,Commit Message
Release-3.2.0,Long type node id
Release-3.2.0,"TODO: support String, Int, and Any type node id"
Release-3.2.0,@maxIndex: this variable contains the max index of node/word
Release-3.2.0,some params
Release-3.2.0,max index for node/word
Release-3.2.0,compute number of nodes for one row
Release-3.2.0,check the length of dot values
Release-3.2.0,merge dot values from all partitions
Release-3.2.0,Skip-Gram model
Release-3.2.0,Negative sampling
Release-3.2.0,used to accumulate the updates for input vectors
Release-3.2.0,Negative sampling
Release-3.2.0,accumulate for the hidden layer
Release-3.2.0,update output layer
Release-3.2.0,update the hidden layer
Release-3.2.0,update input
Release-3.2.0,Skip-Gram model
Release-3.2.0,Negative sampling
Release-3.2.0,used to accumulate the updates for input vectors
Release-3.2.0,Negative sampling
Release-3.2.0,accumulate for the hidden layer
Release-3.2.0,update output layer
Release-3.2.0,update the hidden layer
Release-3.2.0,update input
Release-3.2.0,update output
Release-3.2.0,Some params
Release-3.2.0,compute number of nodes for one row
Release-3.2.0,get nodeId
Release-3.2.0,get  data the nodeId mapped 1.neighbors 2.tags 3.attrs
Release-3.2.0,write out edges
Release-3.2.0,write out tags
Release-3.2.0,write out attrs
Release-3.2.0,Get node neighbor number
Release-3.2.0,start sampling by alias table for count times
Release-3.2.0,Long type node id
Release-3.2.0,Get node neighbor number
Release-3.2.0,start sampling by alias table for count times
Release-3.2.0,int index = Math.abs(r.nextInt()) % nodeNeighbors.length;
Release-3.2.0,float ac = Math.abs(r.nextFloat());
Release-3.2.0,Get matrix meta
Release-3.2.0,Split
Release-3.2.0,Generate Part psf get param
Release-3.2.0,Get nodes and features
Release-3.2.0,return null;
Release-3.2.0,compress the neighbor IDs
Release-3.2.0,write out edges
Release-3.2.0,write out tags
Release-3.2.0,Get node neighbors
Release-3.2.0,Get nodes and features
Release-3.2.0,Get nodes and features
Release-3.2.0,Get nodes and features
Release-3.2.0,Get matrix meta
Release-3.2.0,Split
Release-3.2.0,Generate Part psf get param
Release-3.2.0,Get nodes and features
Release-3.2.0,Long type node id
Release-3.2.0,Get data
Release-3.2.0,Get matrix meta
Release-3.2.0,Split
Release-3.2.0,Generate Part psf get param
Release-3.2.0,Get matrix meta
Release-3.2.0,Split
Release-3.2.0,Generate Part psf get param
Release-3.2.0,Just return original features
Release-3.2.0,Use by line with weight
Release-3.2.0,get nodeId
Release-3.2.0,get  data the nodeId mapped 1.neighbors 2.tags 3.attrs
Release-3.2.0,write out edges
Release-3.2.0,write out tags
Release-3.2.0,write out attrs
Release-3.2.0,clear();
Release-3.2.0,evict entry with the smallest degree
Release-3.2.0,// calculate bias
Release-3.2.0,if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {
Release-3.2.0,"double zVal = VectorUtils.getDouble(z, 0);"
Release-3.2.0,"double nVal = VectorUtils.getDouble(n, 0);"
Release-3.2.0,"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));"
Release-3.2.0,}
Release-3.2.0,Do nothing.
Release-3.2.0,split updates
Release-3.2.0,shuffle update splits
Release-3.2.0,generate part update splits
Release-3.2.0,"set split context: partition key, use int key for long key vector or net"
Release-3.2.0,how to do intersection for two dense vector with a given indices ??
Release-3.2.0,copy the highest levels
Release-3.2.0,copy baseBuffer
Release-3.2.0,merge two non-empty quantile sketches
Release-3.2.0,"if not -1, sufficient space will be allocated at once"
Release-3.2.0,InstanceRow ins = instanceRows[insId];
Release-3.2.0,int[] indices = ins.indices();
Release-3.2.0,int[] bins = ins.bins();
Release-3.2.0,int nnz = indices.length;
Release-3.2.0,for (int j = 0; j < nnz; j++) {
Release-3.2.0,int fid = indices[j];
Release-3.2.0,if (isFeatUsed[fid - featLo]) {
Release-3.2.0,"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);"
Release-3.2.0,}
Release-3.2.0,}
Release-3.2.0,1. allocate histogram
Release-3.2.0,"2. loop non-zero instances, accumulate to histogram"
Release-3.2.0,if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature
Release-3.2.0,3. add remaining grad and hess to default bin
Release-3.2.0,"return param.calcWeights(grad, hess);"
Release-3.2.0,"numClass is usually small, so we do not use arraycopy here"
Release-3.2.0,"numClass is usually small, so we do not use arraycopy here"
Release-3.2.0,TODO: use more schema on default bin
Release-3.2.0,1. set default bin to left child
Release-3.2.0,"2. for other bins, find its location"
Release-3.2.0,3. create split set
Release-3.2.0,this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];
Release-3.2.0,predict sparse instance with indices and values
Release-3.2.0,predict libsvm data
Release-3.2.0,"Preconditions.checkArgument(preds.length == labels.length,"
Release-3.2.0,"""LogLossMetric should be used for binary-label classification"");"
Release-3.2.0,double loss = 0.0;
Release-3.2.0,for (int i = 0; i < preds.length; i++) {
Release-3.2.0,"loss += evalOne(preds[i], labels[i]);"
Release-3.2.0,}
Release-3.2.0,return loss / labels.length;
Release-3.2.0,double error = 0.0;
Release-3.2.0,if (preds.length == labels.length) {
Release-3.2.0,for (int i = 0; i < preds.length; i++) {
Release-3.2.0,"error += evalOne(preds[i], labels[i]);"
Release-3.2.0,}
Release-3.2.0,} else {
Release-3.2.0,int numLabel = preds.length / labels.length;
Release-3.2.0,float[] pred = new float[numLabel];
Release-3.2.0,for (int i = 0; i < labels.length; i++) {
Release-3.2.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-3.2.0,"error += evalOne(pred, labels[i]);"
Release-3.2.0,}
Release-3.2.0,}
Release-3.2.0,return error / labels.length;
Release-3.2.0,Preconditions.checkArgument(preds.length != labels.length
Release-3.2.0,"&& preds.length % labels.length == 0,"
Release-3.2.0,"""CrossEntropyMetric should be used for multi-label classification"");"
Release-3.2.0,double loss = 0.0;
Release-3.2.0,int numLabel = preds.length / labels.length;
Release-3.2.0,float[] pred = new float[numLabel];
Release-3.2.0,for (int i = 0; i < labels.length; i++) {
Release-3.2.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-3.2.0,"loss += evalOne(pred, labels[i]);"
Release-3.2.0,}
Release-3.2.0,return loss / labels.length;
Release-3.2.0,double correct = 0.0;
Release-3.2.0,if (preds.length == labels.length) {
Release-3.2.0,for (int i = 0; i < preds.length; i++) {
Release-3.2.0,"correct += evalOne(preds[i], labels[i]);"
Release-3.2.0,}
Release-3.2.0,} else {
Release-3.2.0,int numLabel = preds.length / labels.length;
Release-3.2.0,float[] pred = new float[numLabel];
Release-3.2.0,for (int i = 0; i < labels.length; i++) {
Release-3.2.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-3.2.0,"correct += evalOne(pred, labels[i]);"
Release-3.2.0,}
Release-3.2.0,}
Release-3.2.0,return (float) (correct / labels.length);
Release-3.2.0,double errSum = 0.0f;
Release-3.2.0,if (preds.length == labels.length) {
Release-3.2.0,for (int i = 0; i < preds.length; i++) {
Release-3.2.0,"errSum += evalOne(preds[i], labels[i]);"
Release-3.2.0,}
Release-3.2.0,} else {
Release-3.2.0,int numLabel = preds.length / labels.length;
Release-3.2.0,float[] pred = new float[numLabel];
Release-3.2.0,for (int i = 0; i < labels.length; i++) {
Release-3.2.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-3.2.0,"errSum += evalOne(pred, labels[i]);"
Release-3.2.0,}
Release-3.2.0,}
Release-3.2.0,return Math.sqrt(errSum / labels.length);
Release-3.2.0,"System.out.println(""----------"");"
Release-3.2.0,"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)"
Release-3.2.0,"+ "", mask = "" + Integer.toBinaryString(readMaskT));"
Release-3.2.0,readMaskT <<= 1;
Release-3.2.0,"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};"
Release-3.2.0,int n = bits.length;
Release-3.2.0,BufferedBitSet writeBitSet = new BufferedBitSet(n);
Release-3.2.0,"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);"
Release-3.2.0,if (bitSet.get(i) != bits[i]) {
Release-3.2.0,"throw new RuntimeException("""" + i);"
Release-3.2.0,}
Release-3.2.0,private final ByteBuffer bytes;
Release-3.2.0,"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {"
Release-3.2.0,int capacity = bytes.capacity() * 8;
Release-3.2.0,readIndexT = bytes.capacity() - 1;
Release-3.2.0,return bytes.get(index);
Release-3.2.0,TODO: use arraycopy to make it faster
Release-3.2.0,assert from >= this.from && to <= this.to;
Release-3.2.0,"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));"
Release-3.2.0,"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));"
Release-3.2.0,return bits.clone();
Release-3.2.0,private final SerializableBuffer bytes;
Release-3.2.0,private final ByteBuffer bytes;
Release-3.2.0,this.bytes = ByteBuffer.allocate(numBytes);
Release-3.2.0,public BufferedBitSetWriter(ByteBuffer bytes) {
Release-3.2.0,this.bytes = bytes;
Release-3.2.0,}
Release-3.2.0,"bytes.put(writeIndex++, (byte) writeBuffer);"
Release-3.2.0,public ByteBuffer getBytes() {
Release-3.2.0,return bytes;
Release-3.2.0,}
Release-3.2.0,ML TreeConf
Release-3.2.0,GBDT TreeConf
Release-3.2.0,"edges=[x,...] firstFlow=1 => go to right if < x and go to left if > x"
Release-3.2.0,"edges=[x,...] firstFlow=0 => go to left if < x and go to right if > x"
Release-3.2.0,"different types of tree node splits, enumerated by their complexity"
Release-3.2.0,"in order to reduce model size, we give priority to split point"
Release-3.2.0,"comparison between two split points, we give priority to lower feature index"
Release-3.2.0,TODO: comparison between two split sets
Release-3.2.0,"public boolean leafwise;  // true if leaf-wise training, false if level-wise training"
Release-3.2.0,TODO: regularization
Release-3.2.0,TODO: regularization
Release-3.2.0,public float insSampleRatio;  // subsample ratio for instances
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy data spliter
Release-3.2.0,get a angel client
Release-3.2.0,add matrix
Release-3.2.0,TODO Auto-generated constructor stub
Release-3.2.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighbor;
Release-3.2.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborParam;
Release-3.2.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborResult;
Release-3.2.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighbor;
Release-3.2.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighborParam;
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add matrix
Release-3.2.0,@Test
Release-3.2.0,"public void testInitAndGet() throws ExecutionException, InterruptedException {"
Release-3.2.0,Worker worker = LocalClusterContext.get().getWorker(worker0Attempt0Id).getWorker();
Release-3.2.0,"MatrixClient client1 = worker.getPSAgent().getMatrixClient(""w2"", 0);"
Release-3.2.0,int matrixW1Id = client1.getMatrixId();
Release-3.2.0,// Generate graph data
Release-3.2.0,"Map<Integer, int []> adjMap = generateAdjTable(nodeNum, maxNeighborNum);"
Release-3.2.0,
Release-3.2.0,// Init graph adj table
Release-3.2.0,"InitNeighbor func = new InitNeighbor(new InitNeighborParam(matrixW1Id, adjMap));"
Release-3.2.0,client1.update(func);
Release-3.2.0,
Release-3.2.0,int [] nodeIds = new int[adjMap.size()];
Release-3.2.0,int i = 0;
Release-3.2.0,for(int nodeId : adjMap.keySet()) {
Release-3.2.0,nodeIds[i++] = nodeId;
Release-3.2.0,}
Release-3.2.0,
Release-3.2.0,// Get graph adj table from PS
Release-3.2.0,"GetNeighbor getFunc = new GetNeighbor(new GetNeighborParam(matrixW1Id, nodeIds, maxNeighborNum));"
Release-3.2.0,"Map<Integer, int[]> getResults = ((GetNeighborResult) (client1.get(getFunc)))"
Release-3.2.0,.getNodeIdToNeighborIndices();
Release-3.2.0,
Release-3.2.0,// Check the result
Release-3.2.0,"for(Entry<Integer, int[]> entry : getResults.entrySet()) {"
Release-3.2.0,"Assert.assertArrayEquals(entry.getValue(), adjMap.get(entry.getKey()));"
Release-3.2.0,}
Release-3.2.0,}
Release-3.2.0,row 0 is a random uniform
Release-3.2.0,row 1 is a random normal
Release-3.2.0,row 2 is filled with 1.0
Release-3.2.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
Release-3.2.0,"paras[1] = ""abc"";"
Release-3.2.0,"paras[2] = ""123"";"
Release-3.2.0,Add standard Hadoop classes
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,set sgd LR algorithm parameters #feature #epoch
Release-3.2.0,Set input data path
Release-3.2.0,Set save model path
Release-3.2.0,Set actionType train
Release-3.2.0,QSLRRunner runner = new QSLRRunner();
Release-3.2.0,runner.train(conf);
Release-3.2.0,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
Release-3.2.0,Dataset
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,class number
Release-3.2.0,Model type
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,Train batch number per epoch.
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set file system
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Use local deploy mode and data format
Release-3.2.0,Set data path
Release-3.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set Softmax algorithm parameters
Release-3.2.0,Set model class
Release-3.2.0,Dataset
Release-3.2.0,Data format
Release-3.2.0,Feature number of train data
Release-3.2.0,Tree number
Release-3.2.0,Tree depth
Release-3.2.0,Split number
Release-3.2.0,Feature sample ratio
Release-3.2.0,Ratio of validation
Release-3.2.0,Learning rate
Release-3.2.0,Set file system
Release-3.2.0,Use local deploy mode and data format
Release-3.2.0,Set data path
Release-3.2.0,"Set angel resource, #worker, #task, #PS"
Release-3.2.0,Set GBDT algorithm parameters
Release-3.2.0,Dataset
Release-3.2.0,Set file system
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Use local deploy mode
Release-3.2.0,Set data path
Release-3.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set DeepFM algorithm parameters
Release-3.2.0,Set model class
Release-3.2.0,Dataset
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Model type
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set file system
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Use local deploy mode and data format
Release-3.2.0,Set data path
Release-3.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set LR algorithm parameters
Release-3.2.0,Set model class
Release-3.2.0,Dataset
Release-3.2.0,Data format
Release-3.2.0,Model type
Release-3.2.0,Cluster center number
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Sample ratio per mini-batch
Release-3.2.0,C
Release-3.2.0,Set file system
Release-3.2.0,Use local deploy mode and data format
Release-3.2.0,Set data path
Release-3.2.0,"conf.set(MLConf.ML_EMBEDDING_MATRIX_OUTPUT_FORMAT(),""TextColumnFormat"");"
Release-3.2.0,"System.out.println(""-------------"");"
Release-3.2.0,System.out.println(predictInput);
Release-3.2.0,"System.out.println(LOCAL_FS + TMP_PATH + ""/predict/kmeans"");"
Release-3.2.0,"System.out.println(""-------------"");"
Release-3.2.0,"Set angel resource, #worker, #task, #PS"
Release-3.2.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-3.2.0,"conf.setBoolean(MLConf.KMEANS_SILHOUETTE_FLAG(),true);"
Release-3.2.0,Dataset
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Model type
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set file system
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Use local deploy mode and data format
Release-3.2.0,Set data path
Release-3.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set FM algorithm parameters
Release-3.2.0,Set model class
Release-3.2.0,Dataset
Release-3.2.0,Set file system
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Use local deploy mode
Release-3.2.0,Set data path
Release-3.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set WideAndDeep algorithm parameters
Release-3.2.0,Set model class
Release-3.2.0,Dataset
Release-3.2.0,Data format
Release-3.2.0,"Set LDA parameters #V, #K"
Release-3.2.0,Set file system
Release-3.2.0,Use local deploy mode and data format
Release-3.2.0,Set data path
Release-3.2.0,"Set angel resource, #worker, #task, #PS"
Release-3.2.0,Set LDA algorithm parameters
Release-3.2.0,Dataset
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Model type
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set file system
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Use local deploy mode and data format
Release-3.2.0,Set data path
Release-3.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set SVM algorithm parameters
Release-3.2.0,Set model class
Release-3.2.0,Dataset
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Model type
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,Model is classification
Release-3.2.0,Train batch number per epoch.
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set file system
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Use local deploy mode and data format
Release-3.2.0,Set data path
Release-3.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set LR algorithm parameters
Release-3.2.0,Set model class
Release-3.2.0,Dataset
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Model type
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,Model is classification
Release-3.2.0,Train batch number per epoch.
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set file system
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Use local deploy mode and data format
Release-3.2.0,Set data path
Release-3.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set LR algorithm parameters
Release-3.2.0,Set model class
Release-3.2.0,Load model meta
Release-3.2.0,Convert model
Release-3.2.0,"Get input path, output path"
Release-3.2.0,Init serde
Release-3.2.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-3.2.0,Load model meta
Release-3.2.0,Convert model
Release-3.2.0,load hadoop configuration
Release-3.2.0,"Get input path, output path"
Release-3.2.0,Init serde
Release-3.2.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-3.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.2.0,input.seek(rowOffset.getOffset());
Release-3.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.2.0,input.seek(rowOffset.getOffset());
Release-3.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.2.0,input.seek(rowOffset.getOffset());
Release-3.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.2.0,input.seek(rowOffset.getOffset());
Release-3.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.2.0,input.seek(rowOffset.getOffset());
Release-3.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.2.0,input.seek(rowOffset.getOffset());
Release-3.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.2.0,input.seek(rowOffset.getOffset());
Release-3.2.0,Load model meta
Release-3.2.0,Check row type
Release-3.2.0,Load model
Release-3.2.0,Load model meta
Release-3.2.0,Check row type
Release-3.2.0,Load model
Release-3.2.0,Load model meta
Release-3.2.0,Check row type
Release-3.2.0,Load model
Release-3.2.0,Load model meta
Release-3.2.0,Check row type
Release-3.2.0,Load model
Release-3.2.0,Load model meta
Release-3.2.0,Check row type
Release-3.2.0,Load model
Release-3.2.0,Load model meta
Release-3.2.0,Check row type
Release-3.2.0,Load model
Release-3.2.0,Load model meta
Release-3.2.0,Check row type
Release-3.2.0,Load model
Release-3.2.0,Load model
Release-3.2.0,load hadoop configuration
Release-3.2.0,"partitioner.init(mMatrix, conf);"
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add matrix
Release-3.2.0,worker register
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add matrix
Release-3.2.0,add matrix
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add matrix
Release-3.2.0,attempt 0
Release-3.2.0,attempt1
Release-3.2.0,attempt1
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add matrix
Release-3.2.0,TODO Auto-generated constructor stub
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add matrix
Release-3.2.0,set basic configuration keys
Release-3.2.0,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add matrix
Release-3.2.0,Thread.sleep(5000);
Release-3.2.0,"response = master.getJobReport(null, request);"
Release-3.2.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
Release-3.2.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
Release-3.2.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add dense double matrix
Release-3.2.0,add sparse double matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense long matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key double matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add sparse double matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key double matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add dense double matrix
Release-3.2.0,add sparse double matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense long matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key double matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add dense double matrix
Release-3.2.0,add sparse double matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense long matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,client1.clock().get();
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,client1.clock().get();
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add dense double matrix
Release-3.2.0,add sparse double matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense long matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key double matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add sparse double matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key double matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,TODO Auto-generated constructor stub
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add sparse double matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add sparse double matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key double matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add dense double matrix
Release-3.2.0,add sparse double matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense long matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.2.0,IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add sparse double matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key double matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add sparse double matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key double matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,Assert.assertTrue(index.length == row.size());
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,for (int i = 0; i < feaNum; i++) {
Release-3.2.0,"deltaVec.set(i, i);"
Release-3.2.0,}
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add dense double matrix
Release-3.2.0,add sparse double matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense long matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key double matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,get a angel client
Release-3.2.0,add dense double matrix
Release-3.2.0,add sparse double matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense float matrix
Release-3.2.0,add sparse float matrix
Release-3.2.0,add dense long matrix
Release-3.2.0,add sparse long matrix
Release-3.2.0,add sparse long-key double matrix
Release-3.2.0,add sparse long-key float matrix
Release-3.2.0,add sparse long-key int matrix
Release-3.2.0,add sparse long-key long matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,@RunWith(MockitoJUnitRunner.class)
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
Release-3.2.0,get a angel client
Release-3.2.0,add matrix
Release-3.2.0,psAgent.initAndStart();
Release-3.2.0,test conf
Release-3.2.0,test master location
Release-3.2.0,test app id
Release-3.2.0,test user
Release-3.2.0,test ps agent attempt id
Release-3.2.0,test connection
Release-3.2.0,test master client
Release-3.2.0,test ip
Release-3.2.0,test loc
Release-3.2.0,test master location
Release-3.2.0,test ps location
Release-3.2.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
Release-3.2.0,test all ps ids
Release-3.2.0,test all matrix ids
Release-3.2.0,test all matrix names
Release-3.2.0,test matrix attribute
Release-3.2.0,test matrix meta
Release-3.2.0,test ps location
Release-3.2.0,test partitions
Release-3.2.0,assertTrue(psAgentContext.getOpLogCache() != null);
Release-3.2.0,ConsistencyController consistControl = psAgent.getConsistencyController();
Release-3.2.0,assertTrue(consistControl != null);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,System.out.println(content);
Release-3.2.0,https://blog.csdn.net/cocoonyang/article/details/63068108
Release-3.2.0,v1[i] = v1[i] + da * v2[i];
Release-3.2.0,"dgemm(String transa, String transb,"
Release-3.2.0,"int m, int n, int k,"
Release-3.2.0,"double alpha,"
Release-3.2.0,"double[] a, int lda,"
Release-3.2.0,"double[] b, int ldb,"
Release-3.2.0,"double beta,"
Release-3.2.0,"double[] c, int ldc);"
Release-3.2.0,C := alpha*op( A )*op( B ) + beta*C
Release-3.2.0,v1[i] = v1[i] + da * v2[i];
Release-3.2.0,y := alpha*A*x + beta*y
Release-3.2.0,y := alpha*A*x + beta*y
Release-3.2.0,y := alpha*A*x + beta*y
Release-3.2.0,"dgemm(String transa, String transb,"
Release-3.2.0,"int m, int n, int k,"
Release-3.2.0,"double alpha,"
Release-3.2.0,"double[] a, int lda,"
Release-3.2.0,"double[] b, int ldb,"
Release-3.2.0,"double beta,"
Release-3.2.0,"double[] c, int ldc);"
Release-3.2.0,C := alpha*op( A )*op( B ) + beta*C
Release-3.2.0,Default does nothing.
Release-3.2.0,The app injection is optional
Release-3.2.0,"renderText(""hello world"");"
Release-3.2.0,"user choose a workerGroupID from the workergroups page,"
Release-3.2.0,now we should change the AngelApp params and render the workergroup page;
Release-3.2.0,"static final String WORKER_ID = ""worker.id"";"
Release-3.2.0,"div(""#logo"")."
Release-3.2.0,"img(""/static/hadoop-st.png"")._()."
Release-3.2.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
Release-3.2.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
Release-3.2.0,JQueryUI.jsnotice(html);
Release-3.2.0,import org.apache.hadoop.conf.Configuration;
Release-3.2.0,import java.lang.reflect.Field;
Release-3.2.0,all the files in input set
Release-3.2.0,Shuffle the file
Release-3.2.0,Get the blocks for all files
Release-3.2.0,Adjust the maxSize to make the split more balanced
Release-3.2.0,Handle the splittable files
Release-3.2.0,Handle the unsplittable files
Release-3.2.0,Split the blocks
Release-3.2.0,"If the remaining size of the current block is smaller than the required size,"
Release-3.2.0,the remaining blocks are divided into the current split
Release-3.2.0,Update current split length and move to next block
Release-3.2.0,Clear the current block offset
Release-3.2.0,"Current split length is > maxSize, split the block and generate a new split"
Release-3.2.0,Clear blocks list for next split
Release-3.2.0,Clear the current split length
Release-3.2.0,"If splitBlocks is not empty, just genetate a split for it"
Release-3.2.0,get block locations from file system
Release-3.2.0,create an input split
Release-3.2.0,get block locations from file system
Release-3.2.0,create a list of all block and their locations
Release-3.2.0,"if the file is not splitable, just create the one block with"
Release-3.2.0,full file length
Release-3.2.0,each split can be a maximum of maxSize
Release-3.2.0,if remainder is between max and 2*max - then
Release-3.2.0,"instead of creating splits of size max, left-max we"
Release-3.2.0,create splits of size left/2 and left/2. This is
Release-3.2.0,a heuristic to avoid creating really really small
Release-3.2.0,splits.
Release-3.2.0,add this block to the block --> node locations map
Release-3.2.0,"For blocks that do not have host/rack information,"
Release-3.2.0,assign to default  rack.
Release-3.2.0,add this block to the rack --> block map
Release-3.2.0,Add this host to rackToNodes map
Release-3.2.0,add this block to the node --> block map
Release-3.2.0,"if the file system does not have any rack information, then"
Release-3.2.0,use dummy rack location.
Release-3.2.0,The topology paths have the host name included as the last
Release-3.2.0,component. Strip it.
Release-3.2.0,get tokens for all the required FileSystems..
Release-3.2.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
Release-3.2.0,job.getConfiguration());
Release-3.2.0,Whether we need to recursive look into the directory structure
Release-3.2.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-3.2.0,user provided one (if any).
Release-3.2.0,all the files in input set
Release-3.2.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
Release-3.2.0,process all nodes and create splits that are local to a node. Generate
Release-3.2.0,"one split per node iteration, and walk over nodes multiple times to"
Release-3.2.0,distribute the splits across nodes.
Release-3.2.0,Skip the node if it has previously been marked as completed.
Release-3.2.0,"for each block, copy it into validBlocks. Delete it from"
Release-3.2.0,blockToNodes so that the same block does not appear in
Release-3.2.0,two different splits.
Release-3.2.0,Remove all blocks which may already have been assigned to other
Release-3.2.0,splits.
Release-3.2.0,"if the accumulated split size exceeds the maximum, then"
Release-3.2.0,create this split.
Release-3.2.0,create an input split and add it to the splits array
Release-3.2.0,Remove entries from blocksInNode so that we don't walk these
Release-3.2.0,again.
Release-3.2.0,Done creating a single split for this node. Move on to the next
Release-3.2.0,node so that splits are distributed across nodes.
Release-3.2.0,This implies that the last few blocks (or all in case maxSize=0)
Release-3.2.0,were not part of a split. The node is complete.
Release-3.2.0,if there were any blocks left over and their combined size is
Release-3.2.0,"larger than minSplitNode, then combine them into one split."
Release-3.2.0,Otherwise add them back to the unprocessed pool. It is likely
Release-3.2.0,that they will be combined with other blocks from the
Release-3.2.0,same rack later on.
Release-3.2.0,This condition also kicks in when max split size is not set. All
Release-3.2.0,blocks on a node will be grouped together into a single split.
Release-3.2.0,haven't created any split on this machine. so its ok to add a
Release-3.2.0,smaller one for parallelism. Otherwise group it in the rack for
Release-3.2.0,balanced size create an input split and add it to the splits
Release-3.2.0,array
Release-3.2.0,Remove entries from blocksInNode so that we don't walk this again.
Release-3.2.0,The node is done. This was the last set of blocks for this node.
Release-3.2.0,Put the unplaced blocks back into the pool for later rack-allocation.
Release-3.2.0,Node is done. All blocks were fit into node-local splits.
Release-3.2.0,Check if node-local assignments are complete.
Release-3.2.0,All nodes have been walked over and marked as completed or all blocks
Release-3.2.0,have been assigned. The rest should be handled via rackLock assignment.
Release-3.2.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
Release-3.2.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
Release-3.2.0,"if blocks in a rack are below the specified minimum size, then keep them"
Release-3.2.0,"in 'overflow'. After the processing of all racks is complete, these"
Release-3.2.0,overflow blocks will be combined into splits.
Release-3.2.0,Process all racks over and over again until there is no more work to do.
Release-3.2.0,Create one split for this rack before moving over to the next rack.
Release-3.2.0,Come back to this rack after creating a single split for each of the
Release-3.2.0,remaining racks.
Release-3.2.0,"Process one rack location at a time, Combine all possible blocks that"
Release-3.2.0,reside on this rack as one split. (constrained by minimum and maximum
Release-3.2.0,split size).
Release-3.2.0,iterate over all racks
Release-3.2.0,"for each block, copy it into validBlocks. Delete it from"
Release-3.2.0,blockToNodes so that the same block does not appear in
Release-3.2.0,two different splits.
Release-3.2.0,"if the accumulated split size exceeds the maximum, then"
Release-3.2.0,create this split.
Release-3.2.0,create an input split and add it to the splits array
Release-3.2.0,"if we created a split, then just go to the next rack"
Release-3.2.0,"if there is a minimum size specified, then create a single split"
Release-3.2.0,"otherwise, store these blocks into overflow data structure"
Release-3.2.0,There were a few blocks in this rack that
Release-3.2.0,remained to be processed. Keep them in 'overflow' block list.
Release-3.2.0,These will be combined later.
Release-3.2.0,Process all overflow blocks
Release-3.2.0,"This might cause an exiting rack location to be re-added,"
Release-3.2.0,but it should be ok.
Release-3.2.0,"if the accumulated split size exceeds the maximum, then"
Release-3.2.0,create this split.
Release-3.2.0,create an input split and add it to the splits array
Release-3.2.0,"Process any remaining blocks, if any."
Release-3.2.0,create an input split
Release-3.2.0,add this split to the list that is returned
Release-3.2.0,long num = totLength / maxSize;
Release-3.2.0,all blocks for all the files in input set
Release-3.2.0,mapping from a rack name to the list of blocks it has
Release-3.2.0,mapping from a block to the nodes on which it has replicas
Release-3.2.0,mapping from a node to the list of blocks that it contains
Release-3.2.0,populate all the blocks for all files
Release-3.2.0,stop all services
Release-3.2.0,1.write application state to file so that the client can get the state of the application
Release-3.2.0,if master exit
Release-3.2.0,2.clear tmp and staging directory
Release-3.2.0,waiting for client to get application state
Release-3.2.0,stop the RPC server
Release-3.2.0,"Security framework already loaded the tokens into current UGI, just use"
Release-3.2.0,them
Release-3.2.0,Now remove the AM->RM token so tasks don't have it
Release-3.2.0,add a shutdown hook
Release-3.2.0,init app state storage
Release-3.2.0,init event dispacher
Release-3.2.0,init location manager
Release-3.2.0,init container allocator
Release-3.2.0,init a rpc service
Release-3.2.0,recover matrix meta if needed
Release-3.2.0,recover ps attempt information if need
Release-3.2.0,Init Client manager
Release-3.2.0,Init PS Client manager
Release-3.2.0,init parameter server manager
Release-3.2.0,recover task information if needed
Release-3.2.0,a dummy data spliter is just for test now
Release-3.2.0,recover data splits information if needed
Release-3.2.0,init worker manager and register worker manager event
Release-3.2.0,register slow worker/ps checker
Release-3.2.0,register app manager event and finish event
Release-3.2.0,Init model saver & loader
Release-3.2.0,start a web service if use yarn deploy mode
Release-3.2.0,load from app state storage first if attempt index great than 1(the master is not the first
Release-3.2.0,retry)
Release-3.2.0,"if load failed, just build a new MatrixMetaManager"
Release-3.2.0,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-3.2.0,is not the first retry)
Release-3.2.0,load task information from app state storage first if attempt index great than 1(the master
Release-3.2.0,is not the first retry)
Release-3.2.0,"if load failed, just build a new AMTaskManager"
Release-3.2.0,load data splits information from app state storage first if attempt index great than 1(the
Release-3.2.0,master is not the first retry)
Release-3.2.0,"if load failed, we need to recalculate the data splits"
Release-3.2.0,Check Workers
Release-3.2.0,Check PSS
Release-3.2.0,Check Clients
Release-3.2.0,Check PS Clients
Release-3.2.0,parse parameter server counters
Release-3.2.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-3.2.0,refresh last heartbeat timestamp
Release-3.2.0,send a state update event to the specific PSAttempt
Release-3.2.0,Check is there save request
Release-3.2.0,"LOG.info(""PS "" + psAttemptId + "" need save "" + subSaveContext);"
Release-3.2.0,Check is there load request
Release-3.2.0,"LOG.info(""PS "" + psAttemptId + "" need load "" + subLoadContext);"
Release-3.2.0,check matrix metadata inconsistencies between master and parameter server.
Release-3.2.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
Release-3.2.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
Release-3.2.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-3.2.0,choose a unused port
Release-3.2.0,start RPC server
Release-3.2.0,remove this parameter server attempt from monitor set
Release-3.2.0,remove this parameter server attempt from monitor set
Release-3.2.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-3.2.0,find workergroup in worker manager
Release-3.2.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
Release-3.2.0,"if this worker group run over, just return WORKERGROUP_EXITED"
Release-3.2.0,"if this worker group is running now, return tasks, workers, data splits for it"
Release-3.2.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-3.2.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-3.2.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-3.2.0,"in ANGEL_PS mode, task id may can not know advance"
Release-3.2.0,update the clock for this matrix
Release-3.2.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-3.2.0,"in ANGEL_PS mode, task id may can not know advance"
Release-3.2.0,update task iteration
Release-3.2.0,"LOG.info(""error happened in psAttempt "" + psAttemptId + "" error msg="" + request.getMsg());"
Release-3.2.0,remove this parameter server attempt from monitor set
Release-3.2.0,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
Release-3.2.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
Release-3.2.0,the number of splits equal to the number of tasks
Release-3.2.0,split data
Release-3.2.0,dispatch the splits to workergroups
Release-3.2.0,split data
Release-3.2.0,dispatch the splits to workergroups
Release-3.2.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-3.2.0,"first, then divided by expected split number"
Release-3.2.0,get input format class from configuration and then instantiation a input format object
Release-3.2.0,split data
Release-3.2.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-3.2.0,"first, then divided by expected split number"
Release-3.2.0,get input format class from configuration and then instantiation a input format object
Release-3.2.0,split data
Release-3.2.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-3.2.0,need to fine tune the number of workergroup and task based on the actual split number
Release-3.2.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-3.2.0,Record the location information for the splits in order to data localized schedule
Release-3.2.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-3.2.0,need to fine tune the number of workergroup and task based on the actual split number
Release-3.2.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-3.2.0,Record the location information for the splits in order to data localized schedule
Release-3.2.0,write meta data to a temporary file
Release-3.2.0,rename the temporary file to final file
Release-3.2.0,"if the file exists, read from file and deserialize it"
Release-3.2.0,write task meta
Release-3.2.0,write ps meta
Release-3.2.0,generate a temporary file
Release-3.2.0,write task meta to the temporary file first
Release-3.2.0,rename the temporary file to the final file
Release-3.2.0,"if last final task file exist, remove it"
Release-3.2.0,find task meta file which has max timestamp
Release-3.2.0,"if the file does not exist, just return null"
Release-3.2.0,read task meta from file and deserialize it
Release-3.2.0,generate a temporary file
Release-3.2.0,write ps meta to the temporary file first.
Release-3.2.0,rename the temporary file to the final file
Release-3.2.0,"if the old final file exist, just remove it"
Release-3.2.0,find ps meta file
Release-3.2.0,"if ps meta file does not exist, just return null"
Release-3.2.0,read ps meta from file and deserialize it
Release-3.2.0,"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),"
Release-3.2.0,String.valueOf(requestId));
Release-3.2.0,Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));
Release-3.2.0,saveContext.setTmpSavePath(tmpPath.toString());
Release-3.2.0,Filter old epoch trigger first
Release-3.2.0,Split the user request to sub-requests to pss
Release-3.2.0,Init matrix files meta
Release-3.2.0,Move output files
Release-3.2.0,Write the meta file
Release-3.2.0,Split the user request to sub-requests to pss
Release-3.2.0,check whether psagent heartbeat timeout
Release-3.2.0,Set up the launch command
Release-3.2.0,Duplicate the ByteBuffers for access by multiple containers.
Release-3.2.0,Construct the actual Container
Release-3.2.0,Application resources
Release-3.2.0,Application environment
Release-3.2.0,Service data
Release-3.2.0,Tokens
Release-3.2.0,Set up JobConf to be localized properly on the remote NM.
Release-3.2.0,Setup DistributedCache
Release-3.2.0,Setup up task credentials buffer
Release-3.2.0,LocalStorageToken is needed irrespective of whether security is enabled
Release-3.2.0,or not.
Release-3.2.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
Release-3.2.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
Release-3.2.0,Construct the actual Container
Release-3.2.0,The null fields are per-container and will be constructed for each
Release-3.2.0,container separately.
Release-3.2.0,Set up the launch command
Release-3.2.0,Duplicate the ByteBuffers for access by multiple containers.
Release-3.2.0,Construct the actual Container
Release-3.2.0,"a * in the classpath will only find a .jar, so we need to filter out"
Release-3.2.0,all .jars and add everything else
Release-3.2.0,Propagate the system classpath when using the mini cluster
Release-3.2.0,Add standard Hadoop classes
Release-3.2.0,Add mr
Release-3.2.0,Cache archives
Release-3.2.0,Cache files
Release-3.2.0,Sanity check
Release-3.2.0,Add URI fragment or just the filename
Release-3.2.0,Add the env variables passed by the user
Release-3.2.0,Set logging level in the environment.
Release-3.2.0,Old parameter name
Release-3.2.0,Parallel GC parameters
Release-3.2.0,G1 params
Release-3.2.0,Parallel Scavenge + Parallel Old
Release-3.2.0,G1
Release-3.2.0,".append("" -XX:G1NewSizePercent="").append(minNewRatio)"
Release-3.2.0,".append("" -XX:G1MaxNewSizePercent="").append(maxNewRatio)"
Release-3.2.0,CMS
Release-3.2.0,Setup the log4j prop
Release-3.2.0,Add main class and its arguments
Release-3.2.0,Finally add the jvmID
Release-3.2.0,vargs.add(String.valueOf(jvmID.getId()));
Release-3.2.0,Final commmand
Release-3.2.0,G1 params
Release-3.2.0,Add the env variables passed by the user
Release-3.2.0,Set logging level in the environment.
Release-3.2.0,Setup the log4j prop
Release-3.2.0,Add main class and its arguments
Release-3.2.0,Final commmand
Release-3.2.0,"if amTask is not null, we should clone task state from it"
Release-3.2.0,"if all parameter server complete commit, master can commit now"
Release-3.2.0,restartPS(psLoc);
Release-3.2.0,check whether parameter server heartbeat timeout
Release-3.2.0,Transitions from the NEW state.
Release-3.2.0,Transitions from the UNASSIGNED state.
Release-3.2.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
Release-3.2.0,Transitions from the ASSIGNED state.
Release-3.2.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
Release-3.2.0,PA_CONTAINER_LAUNCHED event
Release-3.2.0,Transitions from the PSAttemptStateInternal.RUNNING state.
Release-3.2.0,Transitions from the PSAttemptStateInternal.COMMITTING state
Release-3.2.0,Transitions from the PSAttemptStateInternal.KILLED state
Release-3.2.0,Transitions from the PSAttemptStateInternal.FAILED state
Release-3.2.0,create the topology tables
Release-3.2.0,reqeuest resource:send a resource request to the resource allocator
Release-3.2.0,"Once the resource is applied, build and send the launch request to the container launcher"
Release-3.2.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
Release-3.2.0,resource allocator
Release-3.2.0,set the launch time
Release-3.2.0,add the ps attempt to the heartbeat timeout monitoring list
Release-3.2.0,parse ps attempt location and put it to location manager
Release-3.2.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
Release-3.2.0,or failed
Release-3.2.0,remove ps attempt id from heartbeat timeout monitor list
Release-3.2.0,release container:send a release request to container launcher
Release-3.2.0,set the finish time only if launch time is set
Release-3.2.0,private long scheduledTime;
Release-3.2.0,Transitions from the NEW state.
Release-3.2.0,Transitions from the SCHEDULED state.
Release-3.2.0,Transitions from the RUNNING state.
Release-3.2.0,"another attempt launched,"
Release-3.2.0,Transitions from the SUCCEEDED state
Release-3.2.0,Transitions from the KILLED state
Release-3.2.0,Transitions from the FAILED state
Release-3.2.0,add diagnostic
Release-3.2.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-3.2.0,Refresh ps location & matrix meta
Release-3.2.0,start a new attempt for this ps
Release-3.2.0,notify ps manager
Release-3.2.0,"getContext().getLocationManager().setPsLocation(id, null);"
Release-3.2.0,add diagnostic
Release-3.2.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-3.2.0,start a new attempt for this ps
Release-3.2.0,notify ps manager
Release-3.2.0,notify the event handler of state change
Release-3.2.0,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
Release-3.2.0,"if forcedState is set, just return"
Release-3.2.0,else get state from state machine
Release-3.2.0,add this worker group to the success set
Release-3.2.0,check if all worker group run or run over
Release-3.2.0,add this worker group to the success set
Release-3.2.0,check if all worker group run over
Release-3.2.0,add this worker group to the failed set
Release-3.2.0,check if too many worker groups are failed or killed
Release-3.2.0,notify a run failed event
Release-3.2.0,add this worker group to the failed set
Release-3.2.0,check if too many worker groups are failed or killed
Release-3.2.0,notify a run failed event
Release-3.2.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
Release-3.2.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
Release-3.2.0,just return the total task number now
Release-3.2.0,TODO
Release-3.2.0,check whether worker heartbeat timeout
Release-3.2.0,"if workerAttempt is not null, we should clone task state from it"
Release-3.2.0,from NEW state
Release-3.2.0,from SCHEDULED state
Release-3.2.0,get data splits location for data locality
Release-3.2.0,reqeuest resource:send a resource request to the resource allocator
Release-3.2.0,"once the resource is applied, build and send the launch request to the container launcher"
Release-3.2.0,notify failed message to the worker
Release-3.2.0,notify killed message to the worker
Release-3.2.0,release the allocated container
Release-3.2.0,notify failed message to the worker
Release-3.2.0,remove the worker attempt from heartbeat timeout listen list
Release-3.2.0,release the allocated container
Release-3.2.0,notify killed message to the worker
Release-3.2.0,remove the worker attempt from heartbeat timeout listen list
Release-3.2.0,clean the container
Release-3.2.0,notify failed message to the worker
Release-3.2.0,remove the worker attempt from heartbeat timeout listen list
Release-3.2.0,record the finish time
Release-3.2.0,clean the container
Release-3.2.0,notify killed message to the worker
Release-3.2.0,remove the worker attempt from heartbeat timeout listening list
Release-3.2.0,record the finish time
Release-3.2.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
Release-3.2.0,set worker attempt location
Release-3.2.0,notify the register message to the worker
Release-3.2.0,record the launch time
Release-3.2.0,update worker attempt metrics
Release-3.2.0,update tasks metrics
Release-3.2.0,clean the container
Release-3.2.0,notify the worker attempt run successfully message to the worker
Release-3.2.0,record the finish time
Release-3.2.0,todo
Release-3.2.0,init a worker attempt for the worker
Release-3.2.0,schedule the worker attempt
Release-3.2.0,add diagnostic
Release-3.2.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-3.2.0,init and start a new attempt for this ps
Release-3.2.0,notify worker manager
Release-3.2.0,add diagnostic
Release-3.2.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-3.2.0,init and start a new attempt for this ps
Release-3.2.0,notify worker manager
Release-3.2.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
Release-3.2.0,register to Yarn RM
Release-3.2.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
Release-3.2.0,"catch YarnException or YarnRuntimeException, we should exit and need not retry"
Release-3.2.0,build heartbeat request
Release-3.2.0,send heartbeat request to rm
Release-3.2.0,"This can happen if the RM has been restarted. If it is in that state,"
Release-3.2.0,this application must clean itself up.
Release-3.2.0,Setting NMTokens
Release-3.2.0,assgin containers
Release-3.2.0,"if some container is not assigned, release them"
Release-3.2.0,handle finish containers
Release-3.2.0,dispatch container exit message to corresponding components
Release-3.2.0,killed by framework
Release-3.2.0,killed by framework
Release-3.2.0,get application finish state
Release-3.2.0,build application diagnostics
Release-3.2.0,TODO:add a job history for angel
Release-3.2.0,build unregister request
Release-3.2.0,send unregister request to rm
Release-3.2.0,Note this down for next interaction with ResourceManager
Release-3.2.0,based on blacklisting comments above we can end up decrementing more
Release-3.2.0,than requested. so guard for that.
Release-3.2.0,send the updated resource request to RM
Release-3.2.0,send 0 container count requests also to cancel previous requests
Release-3.2.0,Update resource requests
Release-3.2.0,try to assign to all nodes first to match node local
Release-3.2.0,try to match all rack local
Release-3.2.0,assign remaining
Release-3.2.0,Update resource requests
Release-3.2.0,send the container-assigned event to task attempt
Release-3.2.0,build the start container request use launch context
Release-3.2.0,send the start request to Yarn nm
Release-3.2.0,send the message that the container starts successfully to the corresponding component
Release-3.2.0,"after launching, send launched event to task attempt to move"
Release-3.2.0,it from ASSIGNED to RUNNING state
Release-3.2.0,send the message that the container starts failed to the corresponding component
Release-3.2.0,kill the remote container if already launched
Release-3.2.0,start a thread pool to startup the container
Release-3.2.0,See if we need up the pool size only if haven't reached the
Release-3.2.0,maximum limit yet.
Release-3.2.0,nodes where containers will run at *this* point of time. This is
Release-3.2.0,*not* the cluster size and doesn't need to be.
Release-3.2.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
Release-3.2.0,later is just a buffer so we are not always increasing the
Release-3.2.0,pool-size
Release-3.2.0,the events from the queue are handled in parallel
Release-3.2.0,using a thread pool
Release-3.2.0,return if already stopped
Release-3.2.0,shutdown any containers that might be left running
Release-3.2.0,Add one sync matrix
Release-3.2.0,addSyncMatrix();
Release-3.2.0,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
Release-3.2.0,"matrixContext.set(MatrixConf.MATRIX_LOAD_PATH, """");"
Release-3.2.0,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
Release-3.2.0,"LOG.info(""ps id = "" + psEntry.getKey());"
Release-3.2.0,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
Release-3.2.0,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
Release-3.2.0,"LOG.info(""matrix id = "" + metaEntry.getKey());"
Release-3.2.0,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
Release-3.2.0,}
Release-3.2.0,}
Release-3.2.0,get matrix ids in the parameter server report
Release-3.2.0,get the matrices parameter server need to create and delete
Release-3.2.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
Release-3.2.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
Release-3.2.0,private volatile MatrixOpLogCache opLogCache;
Release-3.2.0,Init control connection manager
Release-3.2.0,Get ps locations from master and put them to the location cache.
Release-3.2.0,Build and initialize rpc client to master
Release-3.2.0,Get psagent id
Release-3.2.0,Build PS control rpc client manager
Release-3.2.0,Build local location
Release-3.2.0,Initialize matrix meta information
Release-3.2.0,clockCache = new ClockCache();
Release-3.2.0,opLogCache = new MatrixOpLogCache();
Release-3.2.0,"int staleness = conf.getInt(AngelConf.ANGEL_STALENESS, AngelConf.DEFAULT_ANGEL_STALENESS);"
Release-3.2.0,consistencyController = new ConsistencyController(staleness);
Release-3.2.0,consistencyController.init();
Release-3.2.0,Start all services
Release-3.2.0,clockCache.start();
Release-3.2.0,opLogCache.start();
Release-3.2.0,Stop all modules
Release-3.2.0,Stop all modules
Release-3.2.0,public MatrixOpLogCache getOpLogCache() {
Release-3.2.0,return opLogCache;
Release-3.2.0,}
Release-3.2.0,clock first
Release-3.2.0,"if (cache.getClock(matrixId, pkeys.get(0)) < clock) {"
Release-3.2.0,}
Release-3.2.0,Update generic resource counters
Release-3.2.0,Updating resources specified in ResourceCalculatorProcessTree
Release-3.2.0,Remove the CPU time consumed previously by JVM reuse
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,/ Plus a vector/matrix to the matrix stored in pss
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,/ Update a vector/matrix to the matrix stored in pss
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,/ Get values from pss use row/column indices
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"/ PSF get/update, use can implement their own psf"
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,/ Get a row or a batch of rows
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Just return
Release-3.2.0,Just return
Release-3.2.0,Just return
Release-3.2.0,Just return
Release-3.2.0,Return a empty vector
Release-3.2.0,Return a empty vector
Release-3.2.0,Return a empty vector
Release-3.2.0,Return a empty vector
Release-3.2.0,Return a empty vector
Release-3.2.0,Return a empty vector
Release-3.2.0,Return a empty vector
Release-3.2.0,Return a empty vector
Release-3.2.0,"checkNotNull(func, ""func"");"
Release-3.2.0,Return a empty vector
Release-3.2.0,"checkNotNull(func, ""func"");"
Release-3.2.0,Return a empty vector
Release-3.2.0,"checkNotNull(func, ""func"");"
Release-3.2.0,Return a empty vector
Release-3.2.0,"checkNotNull(func, ""func"");"
Release-3.2.0,Return a empty vector
Release-3.2.0,"checkNotNull(func, ""func"");"
Release-3.2.0,Return a empty vector
Release-3.2.0,"checkNotNull(func, ""func"");"
Release-3.2.0,Return a empty vector
Release-3.2.0,"checkNotNull(func, ""func"");"
Release-3.2.0,Return a empty vector
Release-3.2.0,"checkNotNull(func, ""func"");"
Release-3.2.0,Return a empty vector
Release-3.2.0,"return PSAgentContext.get().getMatrixOpLogCache().flush(taskContext, matrixId);"
Release-3.2.0,"return PSAgentContext.get().getConsistencyController().clock(taskContext, matrixId, flushFirst);"
Release-3.2.0,this.partClockCache = partClockCache;
Release-3.2.0,Generate a flush request and put it to request queue
Release-3.2.0,Generate a clock request and put it to request queue
Release-3.2.0,Generate a merge request and put it to request queue
Release-3.2.0,Generate a merge request and put it to request queue
Release-3.2.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
Release-3.2.0,matrix
Release-3.2.0,and add it to cache maps
Release-3.2.0,Add the message to the tree map
Release-3.2.0,"If there are flush / clock requests blocked, we need to put this merge request into"
Release-3.2.0,the waiting queue
Release-3.2.0,Launch a merge worker to merge the update to matrix op log cache
Release-3.2.0,Remove the message from the tree map
Release-3.2.0,Wake up blocked flush/clock request
Release-3.2.0,Add flush/clock request to listener list to waiting for all the existing
Release-3.2.0,updates are merged
Release-3.2.0,Wake up blocked flush/clock request
Release-3.2.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
Release-3.2.0,blocked.
Release-3.2.0,Get next merge message sequence id
Release-3.2.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
Release-3.2.0,position
Release-3.2.0,Wake up blocked merge requests
Release-3.2.0,Get minimal sequence id from listeners
Release-3.2.0,Future<VoidResult> flushFuture = adapter
Release-3.2.0,".flush(message.getMatrixId(), message.getContext(), matrixOpLog,"
Release-3.2.0,message.getType() == OpLogMessageType.CLOCK);
Release-3.2.0,VoidResult result = flushFuture.get();
Release-3.2.0,((FutureResult<VoidResult>) messageToFutureMap.remove(message)).set(result);
Release-3.2.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
Release-3.2.0,should flush updates to local matrix storage
Release-3.2.0,Doing average or not
Release-3.2.0,Filter un-important update
Release-3.2.0,Split this row according the matrix partitions
Release-3.2.0,Set split context
Release-3.2.0,Remove the row from matrix
Release-3.2.0,buf.writeDouble(0.0);
Release-3.2.0,TODO:
Release-3.2.0,"For each partition, we generate a update split."
Release-3.2.0,"Although the split is empty for partitions those without any update data,"
Release-3.2.0,we still need to generate a update split to update the clock info on ps.
Release-3.2.0,"For each partition, we generate a update split."
Release-3.2.0,"Although the split is empty for partitions those without any update data,"
Release-3.2.0,we still need to generate a update split to update the clock info on ps.
Release-3.2.0,"For each partition, we generate a update split."
Release-3.2.0,"Although the split is empty for partitions those without any update data,"
Release-3.2.0,we still need to generate a update split to update the clock info on ps.
Release-3.2.0,"For each partition, we generate a update split."
Release-3.2.0,"Although the split is empty for partitions those without any update data,"
Release-3.2.0,we still need to generate a update split to update the clock info on ps.
Release-3.2.0,"For each partition, we generate a update split."
Release-3.2.0,"Although the split is empty for partitions those without any update data,"
Release-3.2.0,we still need to generate a update split to update the clock info on ps.
Release-3.2.0,"For each partition, we generate a update split."
Release-3.2.0,"Although the split is empty for partitions those without any update data,"
Release-3.2.0,we still need to generate a update split to update the clock info on ps.
Release-3.2.0,"For each partition, we generate a update split."
Release-3.2.0,"Although the split is empty for partitions those without any update data,"
Release-3.2.0,we still need to generate a update split to update the clock info on ps.
Release-3.2.0,"For each partition, we generate a update split."
Release-3.2.0,"Although the split is empty for partitions those without any update data,"
Release-3.2.0,we still need to generate a update split to update the clock info on ps.
Release-3.2.0,Init response handler
Release-3.2.0,Init network parameters
Release-3.2.0,Use Epoll for linux
Release-3.2.0,Request header
Release-3.2.0,Request body
Release-3.2.0,Request
Release-3.2.0,Send the request
Release-3.2.0,Request header
Release-3.2.0,Request body
Release-3.2.0,Request
Release-3.2.0,Send the request
Release-3.2.0,Update location table
Release-3.2.0,Remove the server from failed list
Release-3.2.0,Notify refresh success message to request dispatcher
Release-3.2.0,Check PS exist or not
Release-3.2.0,Check heartbeat timeout
Release-3.2.0,getPSState(entry.getKey());
Release-3.2.0,Check PS restart or not
Release-3.2.0,private final HashSet<ParameterServerId> refreshingServerSet;
Release-3.2.0,Add it to failed rpc list
Release-3.2.0,Add the server to gray server list
Release-3.2.0,Add it to failed rpc list
Release-3.2.0,Add the server to gray server list
Release-3.2.0,Move from gray server list to failed server list
Release-3.2.0,Handle the RPCS to this server
Release-3.2.0,Submit the schedulable failed get RPCS
Release-3.2.0,Submit new get RPCS
Release-3.2.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-3.2.0,"If the queue is empty, just return 0"
Release-3.2.0,"If request is not over limit, just submit it"
Release-3.2.0,Submit the schedulable failed get RPCS
Release-3.2.0,Submit new put RPCS
Release-3.2.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-3.2.0,"LOG.info(""choose put server "" + psIds[index]);"
Release-3.2.0,Check all pending RPCS
Release-3.2.0,Check get channel context
Release-3.2.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-3.2.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-3.2.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-3.2.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-3.2.0,channelManager.printPools();
Release-3.2.0,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
Release-3.2.0,if(ts - entry.getValue() > requestTimeOut * 2)  {
Release-3.2.0,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
Release-3.2.0,"+ "" milliseconds, close all channels to it"");"
Release-3.2.0,closeChannels(entry.getKey());
Release-3.2.0,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
Release-3.2.0,}
Release-3.2.0,}
Release-3.2.0,Check need retry or not
Release-3.2.0,Clear the cache for this request
Release-3.2.0,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
Release-3.2.0,Remove all pending RPCS
Release-3.2.0,Close all channel to this PS
Release-3.2.0,Get server id and location for this request
Release-3.2.0,"If location is null, means that the server is not ready"
Release-3.2.0,Get the channel for the location
Release-3.2.0,Check if need get token first
Release-3.2.0,"LOG.info(""Send request "" + request.getHeader());"
Release-3.2.0,Serialize the request
Release-3.2.0,Send the request
Release-3.2.0,get a channel to server from pool
Release-3.2.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-3.2.0,request.getContext().setChannelPool(pool);
Release-3.2.0,Allocate the bytebuf and serialize the request
Release-3.2.0,Parse response
Release-3.2.0,"LOG.info(""ResponseHeader="" + header);"
Release-3.2.0,Get Request
Release-3.2.0,"TODO: for LDA, will be remove future"
Release-3.2.0,Update Server state
Release-3.2.0,"LOG.info(""Handle request "" + request.getHeader() + "", response "" + response.getHeader());"
Release-3.2.0,Get user request and result cache
Release-3.2.0,"LOG.info(""userRequest="" + userRequest + "", responseCache="" + responseCache + "", futureResult="" + futureResult);"
Release-3.2.0,"Some error happens, just return"
Release-3.2.0,"LOG.info(""responseCache "" + responseCache.getExpectedResponseNum());"
Release-3.2.0,Get response handler
Release-3.2.0,Add the response to the cache
Release-3.2.0,Check can merge or not
Release-3.2.0,Merge
Release-3.2.0,Clear response cache
Release-3.2.0,Remove the response cache
Release-3.2.0,Handle success
Release-3.2.0,"Server is busy now, retry"
Release-3.2.0,"Server is not ready, retry"
Release-3.2.0,"Handle failed, just return error"
Release-3.2.0,Parse response msg failed
Release-3.2.0,Merge the sub-response
Release-3.2.0,Set matrix/row information
Release-3.2.0,Set result
Release-3.2.0,Merge
Release-3.2.0,Set final result
Release-3.2.0,Check update result
Release-3.2.0,Set the final result
Release-3.2.0,Merge
Release-3.2.0,Set matrix/row meta
Release-3.2.0,Set final result
Release-3.2.0,Just
Release-3.2.0,Check update result
Release-3.2.0,Set the final result
Release-3.2.0,Get matrix meta
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Int key Double value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Int key Float value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Int key Long value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Int key Int value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Long key Double value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Long key Float value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Long key Int value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Long key Long value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Merge the sub-response
Release-3.2.0,Set the result
Release-3.2.0,Adaptor to Get PSF merge
Release-3.2.0,Merge the sub-results
Release-3.2.0,Split the user request to partition requests
Release-3.2.0,filter empty partition requests
Release-3.2.0,Create partition results cache
Release-3.2.0,Send all the partition requests
Release-3.2.0,Request header
Release-3.2.0,Request body
Release-3.2.0,Request
Release-3.2.0,Send the request
Release-3.2.0,Only support column-partitioned matrix now!!
Release-3.2.0,Split the user request to partition requests
Release-3.2.0,filter empty partition requests
Release-3.2.0,Create partition results cache
Release-3.2.0,Request header
Release-3.2.0,Request body
Release-3.2.0,Request
Release-3.2.0,Send the request
Release-3.2.0,Split param use matrix partitons
Release-3.2.0,Request header
Release-3.2.0,Request body
Release-3.2.0,Request
Release-3.2.0,Send the request
Release-3.2.0,Get partitions for this row
Release-3.2.0,Request header
Release-3.2.0,Request body
Release-3.2.0,Request
Release-3.2.0,Send the request
Release-3.2.0,Get partitions for this row
Release-3.2.0,Request header
Release-3.2.0,Request body
Release-3.2.0,Request
Release-3.2.0,Send the request
Release-3.2.0,Split the param use matrix partitions
Release-3.2.0,Send request to PSS
Release-3.2.0,Request header
Release-3.2.0,Request body
Release-3.2.0,Request
Release-3.2.0,Send the request
Release-3.2.0,Request header
Release-3.2.0,Request body
Release-3.2.0,Request
Release-3.2.0,Send the request
Release-3.2.0,Get matrix meta
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Int key Double value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Int key Float value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Int key Long value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Int key Int value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Long key Double value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Long key Float value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Long key Int value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Combine Long key Long value vector
Release-3.2.0,//////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Need not call super.serialize
Release-3.2.0,Valid sub data part number
Release-3.2.0,Just use for stream model
Release-3.2.0,Need not call super.serialize
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Keys split
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Keys and values split: int/long/string/object key, float value"
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Keys and values split: int/long/string/object key, double value"
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Keys and values split: int/long/string/object key, long value"
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Keys and values split: int/long/string/object key, object value"
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Keys and values split: vector
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,keys split: range split only support int/long key
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"keys/values pair split: int key, float/double/int/long/any values"
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"keys/values pair split: long key, float/double/int/long/any values"
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,values pair split: float/double/int/long/any values
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Get keys and values
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Get keys and values
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Get keys and values
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Get keys and values
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Get keys and values
Release-3.2.0,Key and value array pair
Release-3.2.0,Get keys and values
Release-3.2.0,Key and value array pair
Release-3.2.0,Get keys and values
Release-3.2.0,Key and value array pair
Release-3.2.0,Get keys and values
Release-3.2.0,Key and value array pair
Release-3.2.0,IElement class name
Release-3.2.0,IElement class name
Release-3.2.0,IElement class name
Release-3.2.0,IElement class name
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,Use comp key value part
Release-3.2.0,Split each vector
Release-3.2.0,Combine sub data part
Release-3.2.0,Use iterator
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Get values
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Key and value array pair
Release-3.2.0,Use iterator
Release-3.2.0,Key and value array pair
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,It is recommended not to call this method on the client
Release-3.2.0,"task index, it must be unique for whole application"
Release-3.2.0,Deserialize data splits meta
Release-3.2.0,Get workers
Release-3.2.0,no more retries.
Release-3.2.0,calculate sleep time and return.
Release-3.2.0,parse the i-th sleep-time
Release-3.2.0,parse the i-th number-of-retries
Release-3.2.0,calculateSleepTime may overflow.
Release-3.2.0,"A few common retry policies, with no delays."
Release-3.2.0,Read matrix meta from meta file
Release-3.2.0,Save partitions to files use fork-join
Release-3.2.0,Write the ps matrix meta to the meta file
Release-3.2.0,matrix.startServering();
Release-3.2.0,return;
Release-3.2.0,Read matrix meta from meta file
Release-3.2.0,Load partitions from file use fork-join
Release-3.2.0,Read matrix meta from meta file
Release-3.2.0,Sort partitions
Release-3.2.0,TODO:
Release-3.2.0,int size = rows.length;
Release-3.2.0,int size = rows.length;
Release-3.2.0,int size = rows.size();
Release-3.2.0,int size = rows.size();
Release-3.2.0,int size = rows.size();
Release-3.2.0,int size = rows.size();
Release-3.2.0,int size = rows.size();
Release-3.2.0,int size = rows.size();
Release-3.2.0,close is a local operation and should finish within milliseconds; timeout just to be safe
Release-3.2.0,response will be null for one way messages.
Release-3.2.0,maxFrameLength = 2G
Release-3.2.0,lengthFieldOffset = 0
Release-3.2.0,lengthFieldLength = 8
Release-3.2.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
Release-3.2.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
Release-3.2.0,"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())"
Release-3.2.0,.toString();
Release-3.2.0,indicates whether this connection's life cycle is managed
Release-3.2.0,See if we already have a connection (common case)
Release-3.2.0,create a unique lock for this RS + protocol (if necessary)
Release-3.2.0,get the RS lock
Release-3.2.0,do one more lookup in case we were stalled above
Release-3.2.0,Only create isa when we need to.
Release-3.2.0,definitely a cache miss. establish an RPC for
Release-3.2.0,this RS
Release-3.2.0,Throw what the RemoteException was carrying.
Release-3.2.0,check
Release-3.2.0,every
Release-3.2.0,minutes
Release-3.2.0,TODO
Release-3.2.0,创建failoverHandler
Release-3.2.0,"The number of times this invocation handler has ever been failed over,"
Release-3.2.0,before this method invocation attempt. Used to prevent concurrent
Release-3.2.0,failed method invocations from triggering multiple failover attempts.
Release-3.2.0,Make sure that concurrent failed method invocations
Release-3.2.0,only cause a
Release-3.2.0,single actual fail over.
Release-3.2.0,RpcController + Message in the method args
Release-3.2.0,(generated code from RPC bits in .proto files have
Release-3.2.0,RpcController)
Release-3.2.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
Release-3.2.0,+ (System.currentTimeMillis() - beforeConstructTs));
Release-3.2.0,get an instance of the method arg type
Release-3.2.0,RpcController + Message in the method args
Release-3.2.0,(generated code from RPC bits in .proto files have
Release-3.2.0,RpcController)
Release-3.2.0,Message (hand written code usually has only a single
Release-3.2.0,argument)
Release-3.2.0,log any RPC responses that are slower than the configured
Release-3.2.0,warn
Release-3.2.0,response time or larger than configured warning size
Release-3.2.0,"when tagging, we let TooLarge trump TooSmall to keep"
Release-3.2.0,output simple
Release-3.2.0,note that large responses will often also be slow.
Release-3.2.0,provides a count of log-reported slow responses
Release-3.2.0,RpcController + Message in the method args
Release-3.2.0,(generated code from RPC bits in .proto files have
Release-3.2.0,RpcController)
Release-3.2.0,unexpected
Release-3.2.0,"in the protobuf methods, args[1] is the only significant argument"
Release-3.2.0,for JSON encoding
Release-3.2.0,base information that is reported regardless of type of call
Release-3.2.0,Disable Nagle's Algorithm since we don't want packets to wait
Release-3.2.0,Configure the event pipeline factory.
Release-3.2.0,Make a new connection.
Release-3.2.0,Remove all pending requests (will be canceled after relinquishing
Release-3.2.0,write lock).
Release-3.2.0,Cancel any pending requests by sending errors to the callbacks:
Release-3.2.0,Close the channel:
Release-3.2.0,Close the connection:
Release-3.2.0,Shut down all thread pools to exit.
Release-3.2.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
Release-3.2.0,See NettyServer.prepareResponse for where we write out the response.
Release-3.2.0,"It writes the call.id (int), a boolean signifying any error (and if"
Release-3.2.0,"so the exception name/trace), and the response bytes"
Release-3.2.0,Read the call id.
Release-3.2.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
Release-3.2.0,"instead, it returns a null message object."
Release-3.2.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
Release-3.2.0,System.currentTimeMillis());
Release-3.2.0,"It would be good widen this to just Throwable, but IOException is what we"
Release-3.2.0,allow now
Release-3.2.0,not implemented
Release-3.2.0,not implemented
Release-3.2.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
Release-3.2.0,cache of RpcEngines by protocol
Release-3.2.0,return the RpcEngine configured to handle a protocol
Release-3.2.0,We only handle the ConnectException.
Release-3.2.0,This is the exception we can't handle.
Release-3.2.0,check if timed out
Release-3.2.0,wait for retry
Release-3.2.0,IGNORE
Release-3.2.0,return the RpcEngine that handles a proxy object
Release-3.2.0,The default implementation works synchronously
Release-3.2.0,punt: allocate a new buffer & copy into it
Release-3.2.0,Parse cmd parameters
Release-3.2.0,load hadoop configuration
Release-3.2.0,load angel system configuration
Release-3.2.0,load user configuration:
Release-3.2.0,load user config file
Release-3.2.0,load command line parameters
Release-3.2.0,load user job resource files
Release-3.2.0,load ml conf file for graph based algorithm
Release-3.2.0,load user job jar if it exist
Release-3.2.0,Expand the environment variable
Release-3.2.0,Add default fs(local fs) for lib jars.
Release-3.2.0,"LOG.info(System.getProperty(""user.dir""));"
Release-3.2.0,get tokens for all the required FileSystems..
Release-3.2.0,Whether we need to recursive look into the directory structure
Release-3.2.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-3.2.0,user provided one (if any).
Release-3.2.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-3.2.0,get tokens for all the required FileSystems..
Release-3.2.0,Whether we need to recursive look into the directory structure
Release-3.2.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-3.2.0,user provided one (if any).
Release-3.2.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-3.2.0,a simple hdfs copy function assume src path and dest path are in same hdfs
Release-3.2.0,and FileSystem object has same schema
Release-3.2.0,"If out path exist , just remove it first"
Release-3.2.0,Create parent directory if not exist
Release-3.2.0,Rename
Release-3.2.0,"LOG.warn(""interrupted while sleeping"", ie);"
Release-3.2.0,Release the input buffer
Release-3.2.0,"If is stream response, just return bytebuf"
Release-3.2.0,Reset the response and allocate buffer again
Release-3.2.0,Release response data
Release-3.2.0,public static String getHostname() {
Release-3.2.0,try {
Release-3.2.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
Release-3.2.0,} catch (UnknownHostException uhe) {
Release-3.2.0,}
Release-3.2.0,"return new StringBuilder().append("""").append(uhe).toString();"
Release-3.2.0,}
Release-3.2.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
Release-3.2.0,String hostname = getHostname();
Release-3.2.0,String classname = clazz.getSimpleName();
Release-3.2.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
Release-3.2.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
Release-3.2.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
Release-3.2.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
Release-3.2.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
Release-3.2.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
Release-3.2.0,
Release-3.2.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
Release-3.2.0,public void run() {
Release-3.2.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
Release-3.2.0,"this.val$classname + "" at "" + this.val$hostname}));"
Release-3.2.0,}
Release-3.2.0,});
Release-3.2.0,}
Release-3.2.0,for (int i = 0; i < indices.length; i++) {
Release-3.2.0,"System.out.println("""" + i + "", index = "" + indices[i] + "", value = "" + values[i]);"
Release-3.2.0,}
Release-3.2.0,"We we interrupted because we're meant to stop? If not, just"
Release-3.2.0,continue ignoring the interruption
Release-3.2.0,Recalculate waitTime.
Release-3.2.0,// Begin delegation to Thread
Release-3.2.0,// End delegation to Thread
Release-3.2.0,instance submitter class
Release-3.2.0,Obtain filename from path
Release-3.2.0,Split filename to prexif and suffix (extension)
Release-3.2.0,Check if the filename is okay
Release-3.2.0,Prepare temporary file
Release-3.2.0,Prepare buffer for data copying
Release-3.2.0,Open and check input stream
Release-3.2.0,Open output stream and copy data between source file in JAR and the temporary file
Release-3.2.0,"If read/write fails, close streams safely before throwing an exception"
Release-3.2.0,"Finally, load the library"
Release-3.2.0,little endian load order
Release-3.2.0,tail
Release-3.2.0,fallthrough
Release-3.2.0,fallthrough
Release-3.2.0,finalization
Release-3.2.0,fmix(h1);
Release-3.2.0,----------
Release-3.2.0,body
Release-3.2.0,----------
Release-3.2.0,tail
Release-3.2.0,----------
Release-3.2.0,finalization
Release-3.2.0,----------
Release-3.2.0,body
Release-3.2.0,----------
Release-3.2.0,tail
Release-3.2.0,----------
Release-3.2.0,finalization
Release-3.2.0,throw new AngelException(e);
Release-3.2.0,JobStateProto jobState = report.getJobState();
Release-3.2.0,Check need load matrices
Release-3.2.0,Used for java code to get a AngelClient instance
Release-3.2.0,Used for python code to get a AngelClient instance
Release-3.2.0,load user job resource files
Release-3.2.0,setLocalAddr();
Release-3.2.0,2.get job id
Release-3.2.0,5.write configuration to a xml file
Release-3.2.0,8.get app master client
Release-3.2.0,Write job file to JobTracker's fs
Release-3.2.0,the leaf level file should be readable by others
Release-3.2.0,the subdirs in the path should have execute permissions for
Release-3.2.0,others
Release-3.2.0,2.get job id
Release-3.2.0,Credentials credentials = new Credentials();
Release-3.2.0,4.copy resource files to hdfs
Release-3.2.0,5.write configuration to a xml file
Release-3.2.0,6.create am container context
Release-3.2.0,7.Submit to ResourceManager
Release-3.2.0,8.get app master client
Release-3.2.0,Create a number of filenames in the JobTracker's fs namespace
Release-3.2.0,add all the command line files/ jars and archive
Release-3.2.0,first copy them to jobtrackers filesystem
Release-3.2.0,should not throw a uri exception
Release-3.2.0,should not throw an uri excpetion
Release-3.2.0,set the timestamps of the archives and files
Release-3.2.0,set the public/private visibility of the archives and files
Release-3.2.0,get DelegationToken for each cached file
Release-3.2.0,check if we do not need to copy the files
Release-3.2.0,is jt using the same file system.
Release-3.2.0,just checking for uri strings... doing no dns lookups
Release-3.2.0,to see if the filesystems are the same. This is not optimal.
Release-3.2.0,but avoids name resolution.
Release-3.2.0,this might have name collisions. copy will throw an exception
Release-3.2.0,parse the original path to create new path
Release-3.2.0,check for ports
Release-3.2.0,Write job file to JobTracker's fs
Release-3.2.0,Setup resource requirements
Release-3.2.0,Setup LocalResources
Release-3.2.0,Setup security tokens
Release-3.2.0,Setup the command to run the AM
Release-3.2.0,Add AM user command opts
Release-3.2.0,Final command
Release-3.2.0,Setup the CLASSPATH in environment
Release-3.2.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
Release-3.2.0,Setup the environment variables for Admin first
Release-3.2.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
Release-3.2.0,Parse distributed cache
Release-3.2.0,Setup ContainerLaunchContext for AM container
Release-3.2.0,Set up the ApplicationSubmissionContext
Release-3.2.0,private volatile PS2PSPusherImpl ps2PSPusher;
Release-3.2.0,TODO
Release-3.2.0,Add tokens to new user so that it may execute its task correctly.
Release-3.2.0,to exit
Release-3.2.0,calculate data size of all partitions of ps
Release-3.2.0,totalRPC
Release-3.2.0,request size
Release-3.2.0,data size
Release-3.2.0,Recover PS from snapshot or load path
Release-3.2.0,1. First check old snapshot
Release-3.2.0,2. Check new checkpoints
Release-3.2.0,3. Check load path setting and old save result
Release-3.2.0,Just init it again
Release-3.2.0,TODO
Release-3.2.0,if(ps2PSPusher != null) {
Release-3.2.0,ps2PSPusher.start();
Release-3.2.0,}
Release-3.2.0,public PS2PSPusherImpl getPs2PSPusher() {
Release-3.2.0,return ps2PSPusher;
Release-3.2.0,}
Release-3.2.0,Deserialize head
Release-3.2.0,Get method type
Release-3.2.0,Check method
Release-3.2.0,Check if data request
Release-3.2.0,Use async handler or not
Release-3.2.0,Get and init the queue
Release-3.2.0,"LOG.info(""Request header = "" + requestHeader);"
Release-3.2.0,"Idle, handle the request"
Release-3.2.0,Parse head success
Release-3.2.0,Handle request
Release-3.2.0,"Handle error, generate response"
Release-3.2.0,Release the input buffer
Release-3.2.0,Create response
Release-3.2.0,"LOG.info(""Response header = "" + responseHeader);"
Release-3.2.0,Serialize the response
Release-3.2.0,2. Serialize the response
Release-3.2.0,Send the serialized response
Release-3.2.0,"int maxRPCCounter = Math.max(estSize, (int) (workerNum * factor));"
Release-3.2.0,"for (Map.Entry<Integer, ClientRunningContext> clientEntry : clientRPCCounters.entrySet()) {"
Release-3.2.0,"LOG.info(""client "" + clientEntry.getKey() + "" running context:"");"
Release-3.2.0,clientEntry.getValue().printToken();
Release-3.2.0,}
Release-3.2.0,return ServerState.GENERAL;
Release-3.2.0,Use Epoll for linux
Release-3.2.0,Read router type
Release-3.2.0,Key type
Release-3.2.0,Row id
Release-3.2.0,Key number
Release-3.2.0,Calculate final output buffer len
Release-3.2.0,Response header
Release-3.2.0,Data flag
Release-3.2.0,Value type
Release-3.2.0,Row id
Release-3.2.0,Size
Release-3.2.0,Data
Release-3.2.0,Allocate result buffer
Release-3.2.0,Header
Release-3.2.0,Serialize Value part head
Release-3.2.0,response.setOutputBuffer(resultBuf);
Release-3.2.0,Read router type
Release-3.2.0,Key type
Release-3.2.0,Row id
Release-3.2.0,Key number
Release-3.2.0,Calculate final output buffer len
Release-3.2.0,Response header
Release-3.2.0,Data flag
Release-3.2.0,Value type
Release-3.2.0,Row id
Release-3.2.0,Size
Release-3.2.0,Data
Release-3.2.0,Allocate final result byte buffer
Release-3.2.0,Write response header
Release-3.2.0,Data
Release-3.2.0,Value part flag
Release-3.2.0,Value type
Release-3.2.0,Row id
Release-3.2.0,Values number
Release-3.2.0,Result data
Release-3.2.0,Filter comp key value
Release-3.2.0,"LOG.info(""Get PSF func = "" + func.getClass().getName());"
Release-3.2.0,Data is not de-serialized first
Release-3.2.0,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
Release-3.2.0,this.channelPool = channelPool;
Release-3.2.0,}
Release-3.2.0,private final ParameterServer psServer;
Release-3.2.0,Create and start workers
Release-3.2.0,Set workers
Release-3.2.0,Create and start workers
Release-3.2.0,Set workers
Release-3.2.0,"If matrix checkpoint path not exist, just return null"
Release-3.2.0,Return the path with maximum checkpoint id
Release-3.2.0,Rename temp to item path
Release-3.2.0,Checkpoint base path = Base dir/matrix name
Release-3.2.0,Path for this checkpoint
Release-3.2.0,Generate tmp path
Release-3.2.0,Delete old checkpoints
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.2.0,and call endWrite/endRead after
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.2.0,and call endWrite/endRead after
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"////// network io method, for model transform"
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Serailize the head
Release-3.2.0,Serialize the storage
Release-3.2.0,Deserailze the head
Release-3.2.0,Deseralize the storage
Release-3.2.0,Serailize the head
Release-3.2.0,Serialize the storage
Release-3.2.0,Deserailze the head
Release-3.2.0,Deseralize the storage
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.2.0,and call endWrite/endRead after
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.2.0,and call endWrite/endRead after
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.2.0,and call endWrite/endRead after
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.2.0,and call endWrite/endRead after
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.2.0,and call endWrite/endRead after
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.2.0,and call endWrite/endRead after
Release-3.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,"Notice: Basic type ServerRow only support Vector as inner storage now, so you can use methods"
Release-3.2.0,to get inner vector for basic type ServerRow.
Release-3.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.2.0,Index is int
Release-3.2.0,Index is long
Release-3.2.0,Just update the exist element now!!
Release-3.2.0,Just update the exist element now!!
Release-3.2.0,TODO: just check the value is 0 or not now
Release-3.2.0,Valid element number
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Deserialize the data
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Deserialize the data
Release-3.2.0,Element data
Release-3.2.0,Attention: Only update exist element
Release-3.2.0,Attention: Only update exist element
Release-3.2.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-3.2.0,Valid element number
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Deserialize the data
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Deserialize the data
Release-3.2.0,Element data
Release-3.2.0,Attention: Only update exist element
Release-3.2.0,TODO: just check the value is 0 or not now
Release-3.2.0,Just update the exist element now!!
Release-3.2.0,Just update the exist element now!!
Release-3.2.0,"Use sparse storage method, as some elements in the array maybe null"
Release-3.2.0,Array length
Release-3.2.0,Valid element number
Release-3.2.0,Element data
Release-3.2.0,Array len
Release-3.2.0,Valid element number
Release-3.2.0,"Use sparse storage method, as some elements in the array maybe null"
Release-3.2.0,Array length
Release-3.2.0,Valid element number
Release-3.2.0,Element data
Release-3.2.0,Element data
Release-3.2.0,Array len
Release-3.2.0,Valid element number
Release-3.2.0,Attention: Only update exist element
Release-3.2.0,Row type
Release-3.2.0,Storage method
Release-3.2.0,Key type
Release-3.2.0,Value type
Release-3.2.0,Vector dim
Release-3.2.0,Vector length
Release-3.2.0,Vector data
Release-3.2.0,Row type
Release-3.2.0,Storage method
Release-3.2.0,Key type
Release-3.2.0,Value type
Release-3.2.0,Vector dim
Release-3.2.0,Vector length
Release-3.2.0,Init the vector
Release-3.2.0,Vector data
Release-3.2.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.2.0,Get the array pair
Release-3.2.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.2.0,Get the array pair
Release-3.2.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.2.0,Get the array pair
Release-3.2.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.2.0,Get the array pair
Release-3.2.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.2.0,Get the array pair
Release-3.2.0,Impossible now
Release-3.2.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.2.0,Get the array pair
Release-3.2.0,Impossible now
Release-3.2.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.2.0,Get the array pair
Release-3.2.0,Impossible now
Release-3.2.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.2.0,Get the array pair
Release-3.2.0,Impossible now
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,"If use sorted storage, we should get the array pair first"
Release-3.2.0,Just update the exist element now!!
Release-3.2.0,Just update the exist element now!!
Release-3.2.0,TODO: just check the value is 0 or not now
Release-3.2.0,Just update the exist element now!!
Release-3.2.0,Just update the exist element now!!
Release-3.2.0,TODO: just check the value is 0 or not now
Release-3.2.0,Valid element number
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Deserialize the data
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Deserialize the data
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Deserialize the data
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Element data
Release-3.2.0,Valid element number
Release-3.2.0,Deserialize the data
Release-3.2.0,Element data
Release-3.2.0,private final List<PartitionKey> partitionKeys;
Release-3.2.0,Get server partition class
Release-3.2.0,"If partition class is not set, just use the default partition class"
Release-3.2.0,Get server partition storage class type
Release-3.2.0,Get value class
Release-3.2.0,Partition number use set
Release-3.2.0,Use total partition number if set
Release-3.2.0,Use partition number per server
Release-3.2.0,"if col == -1, we use the start/end index to calculate range,"
Release-3.2.0,we use double to store the range value since two long minus might exceed the
Release-3.2.0,range of long.
Release-3.2.0,"partitioner.init(matrix1, conf);"
Release-3.2.0,Serialize the head
Release-3.2.0,Serialize the storage
Release-3.2.0,Deserialize the head
Release-3.2.0,Deseralize the storage
Release-3.2.0,Serialize the head
Release-3.2.0,Serialize the storage
Release-3.2.0,Deserialize the head
Release-3.2.0,Deseralize the storage
Release-3.2.0,Row base partition
Release-3.2.0,"If storage class is not set, use default DenseServerRowsStorage"
Release-3.2.0,Serialize values
Release-3.2.0,Deserialize values
Release-3.2.0,Filter head
Release-3.2.0,Array size
Release-3.2.0,Actual write size
Release-3.2.0,Rows data
Release-3.2.0,Row id
Release-3.2.0,Row type
Release-3.2.0,Row data
Release-3.2.0,Array size
Release-3.2.0,Actual write row number
Release-3.2.0,Rows data
Release-3.2.0,Row id
Release-3.2.0,Create empty server row
Release-3.2.0,Row data
Release-3.2.0,Rows data
Release-3.2.0,Rows data
Release-3.2.0,TODO
Release-3.2.0,Serialize row offsets
Release-3.2.0,Serialize column offsets
Release-3.2.0,Deserialize row offset
Release-3.2.0,Deserialize row offset
Release-3.2.0,"If storage is set, just get a instance"
Release-3.2.0,"If storage is not set, use default"
Release-3.2.0,"If storage is set, just get a instance"
Release-3.2.0,"If storage is not set, use default"
Release-3.2.0,Map size
Release-3.2.0,Actual write size
Release-3.2.0,Rows data
Release-3.2.0,Row id
Release-3.2.0,Row type
Release-3.2.0,Row data
Release-3.2.0,Array size
Release-3.2.0,Actual write row number
Release-3.2.0,Rows data
Release-3.2.0,Row id
Release-3.2.0,Create empty server row
Release-3.2.0,Row data
Release-3.2.0,Rows data
Release-3.2.0,Rows data
Release-3.2.0,=======================================================
Release-3.2.0,Boolean
Release-3.2.0,=======================================================
Release-3.2.0,Byte
Release-3.2.0,=======================================================
Release-3.2.0,Int
Release-3.2.0,=======================================================
Release-3.2.0,Int
Release-3.2.0,=======================================================
Release-3.2.0,Long
Release-3.2.0,=======================================================
Release-3.2.0,Float
Release-3.2.0,=======================================================
Release-3.2.0,String
Release-3.2.0,=======================================================
Release-3.2.0,Double
Release-3.2.0,=======================================================
Release-3.2.0,Byte array
Release-3.2.0,=======================================================
Release-3.2.0,Int array
Release-3.2.0,=======================================================
Release-3.2.0,Long array
Release-3.2.0,=======================================================
Release-3.2.0,Float array
Release-3.2.0,=======================================================
Release-3.2.0,Double array
Release-3.2.0,=======================================================
Release-3.2.0,String array
Release-3.2.0,=======================================================
Release-3.2.0,2-D Int array
Release-3.2.0,=======================================================
Release-3.2.0,2-D Long array
Release-3.2.0,=======================================================
Release-3.2.0,2-D Float array
Release-3.2.0,=======================================================
Release-3.2.0,2-D Double array
Release-3.2.0,=======================================================
Release-3.2.0,Vector
Release-3.2.0,Meta data
Release-3.2.0,TODO: other vector type
Release-3.2.0,TODO: other vector type
Release-3.2.0,TODO: other vector type
Release-3.2.0,IntDoubleVector
Release-3.2.0,IntFloatVector
Release-3.2.0,IntFloatVector array
Release-3.2.0,"serializeInt(out, vector.getType().getNumber()); // no need to record type"
Release-3.2.0,RowType type = RowType.valueOf(deserializeInt(in));
Release-3.2.0,LongFloatVector
Release-3.2.0,LongFloatVector array
Release-3.2.0,"serializeInt(out, vector.getType().getNumber()); // no need to record type"
Release-3.2.0,RowType type = RowType.valueOf(deserializeInt(in));
Release-3.2.0,LongFloatVector
Release-3.2.0,LongFloatVector array
Release-3.2.0,"serializeInt(out, vector.getType().getNumber()); // no need to record type"
Release-3.2.0,RowType type = RowType.valueOf(deserializeInt(in));
Release-3.2.0,////////////////////////////////////////////////////////////////////////////
Release-3.2.0,=======================================================
Release-3.2.0,Boolean
Release-3.2.0,=======================================================
Release-3.2.0,Byte
Release-3.2.0,=======================================================
Release-3.2.0,Int
Release-3.2.0,=======================================================
Release-3.2.0,Int
Release-3.2.0,=======================================================
Release-3.2.0,Long
Release-3.2.0,=======================================================
Release-3.2.0,Float
Release-3.2.0,=======================================================
Release-3.2.0,String
Release-3.2.0,=======================================================
Release-3.2.0,Double
Release-3.2.0,=======================================================
Release-3.2.0,Byte array
Release-3.2.0,=======================================================
Release-3.2.0,Int array
Release-3.2.0,=======================================================
Release-3.2.0,Long array
Release-3.2.0,=======================================================
Release-3.2.0,Float array
Release-3.2.0,=======================================================
Release-3.2.0,Double array
Release-3.2.0,=======================================================
Release-3.2.0,String array
Release-3.2.0,=======================================================
Release-3.2.0,2-D Int array
Release-3.2.0,=======================================================
Release-3.2.0,2-D Long array
Release-3.2.0,=======================================================
Release-3.2.0,2-D Float array
Release-3.2.0,=======================================================
Release-3.2.0,2-D Double array
Release-3.2.0,=======================================================
Release-3.2.0,Vector
Release-3.2.0,Meta data
Release-3.2.0,TODO: other vector type
Release-3.2.0,TODO: other vector type
Release-3.2.0,TODO: other vector type
Release-3.2.0,IntDoubleVector
Release-3.2.0,IntFloatVector
Release-3.2.0,IntFloatVector array
Release-3.2.0,"serializeInt(out, vector.getType().getNumber()); // no need to record type"
Release-3.2.0,RowType type = RowType.valueOf(deserializeInt(in));
Release-3.2.0,LongFloatVector
Release-3.2.0,LongFloatVector array
Release-3.2.0,"serializeInt(out, vector.getType().getNumber()); // no need to record type"
Release-3.2.0,RowType type = RowType.valueOf(deserializeInt(in));
Release-3.2.0,LongFloatVector
Release-3.2.0,LongFloatVector array
Release-3.2.0,"serializeInt(out, vector.getType().getNumber()); // no need to record type"
Release-3.2.0,RowType type = RowType.valueOf(deserializeInt(in));
Release-3.2.0,"First check the state of the channels in the pool, if a channel is unused, just return"
Release-3.2.0,"If all channels are in use, create a new channel or wait"
Release-3.2.0,Create a new channel
Release-3.2.0,checker.start();
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,Keys and values
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,Keys
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,Keys
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,Keys
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,Keys
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,Keys
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,Key size
Release-3.2.0,Key class name
Release-3.2.0,Keys
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,Values
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,Keys
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,Keys
Release-3.2.0,"[0, currentBatchIndex - 1] batchs"
Release-3.2.0,Last batch
Release-3.2.0,"add the PSAgentContext,need fix"
Release-3.2.0,If col == -1 and start/end not set
Release-3.2.0,start/end set
Release-3.2.0,"for dense type, we need to set the colNum to set dim for vectors"
Release-3.2.0,"colNum set, start/end not set"
Release-3.2.0,Hash partition does not support dense type row
Release-3.2.0,Row number must > 0
Release-3.2.0,"both set, check its valid"
Release-3.2.0,Sort the partitions by start column index
Release-3.2.0,public static final int T_INT_ARBITRARY_VALUE = 28;
Release-3.2.0,public static final int T_INVALID_VALUE = 29;
Release-3.2.0,TODO:add more vector type
Release-3.2.0,TODO : subDim set
Release-3.2.0,Sort the parts by partitionId
Release-3.2.0,Sort partition keys use start column index
Release-3.2.0,"For each partition, we generate a update split."
Release-3.2.0,"Although the split is empty for partitions those without any update data,"
Release-3.2.0,we still need to generate a update split to update the clock info on ps.
Release-3.2.0,Sort the parts by partitionId
Release-3.2.0,Sort partition keys use start column index
Release-3.2.0,"For each partition, we generate a update split."
Release-3.2.0,"Although the split is empty for partitions those without any update data,"
Release-3.2.0,we still need to generate a update split to update the clock info on ps.
Release-3.2.0,Split updates
Release-3.2.0,Shuffle update splits
Release-3.2.0,Generate part update parameters
Release-3.2.0,"Set split context: partition key, use int key for long key vector or not ect"
Release-3.2.0,write the max abs
Release-3.2.0,---------------------------------------------------
Release-3.2.0,---------------------------------------------------
Release-3.2.0,---------------------------------------------------------------
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,axis = 0: on rows
Release-3.2.0,axis = 1: on cols
Release-3.2.0,1. find the insert point
Release-3.2.0,2. check the capacity and insert
Release-3.2.0,3. increase size
Release-3.2.0,1. find the insert point
Release-3.2.0,2. check the capacity and insert
Release-3.2.0,3. increase size
Release-3.2.0,1. find the insert point
Release-3.2.0,2. check the capacity and insert
Release-3.2.0,3. increase size
Release-3.2.0,1. find the insert point
Release-3.2.0,2. check the capacity and insert
Release-3.2.0,3. increase size
Release-3.2.0,1. find the insert point
Release-3.2.0,2. check the capacity and insert
Release-3.2.0,3. increase size
Release-3.2.0,1. find the insert point
Release-3.2.0,2. check the capacity and insert
Release-3.2.0,3. increase size
Release-3.2.0,1. find the insert point
Release-3.2.0,2. check the capacity and insert
Release-3.2.0,3. increase size
Release-3.2.0,1. find the insert point
Release-3.2.0,2. check the capacity and insert
Release-3.2.0,3. increase size
Release-3.2.0,-----------------
Release-3.2.0,-----------------
Release-3.2.0,-----------------
Release-3.2.0,-----------------
Release-3.2.0,-----------------
Release-3.2.0,KeepStorage is guaranteed
Release-3.2.0,"ignore the isInplace option, since v2 is dense"
Release-3.2.0,"the value in old storage can be changed safe, so switch a storage"
Release-3.2.0,"but user required keep storage, we can prevent rehash"
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,KeepStorage is guaranteed
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,v1Size < v2Size * Constant.sparseThreshold
Release-3.2.0,KeepStorage is guaranteed
Release-3.2.0,"ignore the isInplace option, since v2 is dense"
Release-3.2.0,"the value in old storage can be changed safe, so switch a storage"
Release-3.2.0,"but user required keep storage, we can prevent rehash"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,prevent rehash
Release-3.2.0,KeepStorage is guaranteed
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,dense preferred
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,sorted preferred
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,multi-rehash
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"slower but memory efficient, for small vector only"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"slower but memory efficient, for small vector only"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"slower but memory efficient, for small vector only"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"slower but memory efficient, for small vector only"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"slower but memory efficient, for small vector only"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"slower but memory efficient, for small vector only"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"slower but memory efficient, for small vector only"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"slower but memory efficient, for small vector only"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"slower but memory efficient, for small vector only"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"slower but memory efficient, for small vector only"
Release-3.2.0,"dense preferred, KeepStorage is guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,"sparse preferred, keep storage guaranteed"
Release-3.2.0,preferred dense
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,we gauss dense storage is more efficient
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-3.2.0,multi-rehash
Release-3.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,to avoid multi-rehash
Release-3.2.0,"Transform mat1, generate a new matrix"
Release-3.2.0,Split the row indices of mat1Trans
Release-3.2.0,Parallel execute use fork-join
Release-3.2.0,"Get the sub-matrix of left matrix, split by row"
Release-3.2.0,"Transform mat1, generate a new matrix"
Release-3.2.0,Split the row indices of mat1Trans
Release-3.2.0,Parallel execute use fork-join
Release-3.2.0,"Get the sub-matrix of left matrix, split by row"
Release-3.2.0,"mat1 trans true, mat trans true"
Release-3.2.0,"mat1 trans true, mat trans false"
Release-3.2.0,"mat1 trans false, mat trans true, important"
Release-3.2.0,"mat1 trans false, mat trans false"
Release-3.2.0,"mat1 trans true, mat trans true"
Release-3.2.0,"mat1 trans true, mat trans false"
Release-3.2.0,"mat1 trans false, mat trans true, important"
Release-3.2.0,"mat1 trans false, mat trans false"
Release-3.2.0,"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-3.2.0,"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-3.2.0,not the first time
Release-3.2.0,first time and do the sample
Release-3.2.0,set to zero
Release-3.2.0,not the first time
Release-3.2.0,first time and do the sample
Release-3.2.0,set to zero
Release-3.2.0,not the first time
Release-3.2.0,first time and do the sample
Release-3.2.0,set to zero
Release-3.2.0,not the first time
Release-3.2.0,first time and do the sample
Release-3.2.0,set to zero
Release-3.2.0,not the first time
Release-3.2.0,first time and do the sample
Release-3.2.0,set to zero
Release-3.2.0,not the first time
Release-3.2.0,first time and do the sample
Release-3.2.0,set to zero
Release-3.2.0,not the first time
Release-3.2.0,first time and do the sample
Release-3.2.0,set to zero
Release-3.2.0,not the first time
Release-3.2.0,first time and do the sample
Release-3.2.0,set to zero
Release-3.2.0,not the first time
Release-3.2.0,first time and do the sample
Release-3.2.0,set to zero
Release-3.2.0,not the first time
Release-3.2.0,first time and do the sample
Release-3.2.0,set to zero
Release-3.2.0,get configuration from envs
Release-3.2.0,get master location
Release-3.2.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-3.2.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-3.2.0,add dense double matrix
Release-3.2.0,TODO Auto-generated method stub
Release-3.2.0,TODO Auto-generated method stub
Release-3.2.0,TODO Auto-generated method stub
Release-3.2.0,public volatile static int colNum = 1000000;
Release-3.2.0,public volatile static int nnz = 10000;
Release-3.2.0,public volatile static int blockColNum = 20000;
Release-3.2.0,"test the colNum is 1000000, 1 million"
Release-3.2.0,"rowNum = conf.getInt(""row"", 1);"
Release-3.2.0,"colNum = conf.getInt(""col"", 1000000);"
Release-3.2.0,"nnz = conf.getInt(""nnz"", 10000);"
Release-3.2.0,"blockRowNum = conf.getInt(""blockRow"", 1);"
Release-3.2.0,"blockColNum = conf.getInt(""blockCol"", 20000);"
Release-3.2.0,"rowNum = conf.getInt(""row"", 1);"
Release-3.2.0,"colNum = conf.getInt(""col"", 10);"
Release-3.2.0,"nnz = conf.getInt(""nnz"", 2);"
Release-3.2.0,"blockRowNum = conf.getInt(""blockRow"", 1);"
Release-3.2.0,"blockColNum = conf.getInt(""blockCol"", 10);"
Release-3.2.0,add sparse longkey float matrix of 1 million
Release-3.2.0,MatrixContext dMat = new MatrixContext();
Release-3.2.0,dMat.setName(SPARSE_LONGKEY_FLOAT_MAT);
Release-3.2.0,dMat.setRowNum(rowNum);
Release-3.2.0,dMat.setColNum(colNum);
Release-3.2.0,dMat.setMaxRowNumInBlock(blockRowNum);
Release-3.2.0,dMat.setMaxColNumInBlock(blockColNum);
Release-3.2.0,dMat.setRowType(RowType.T_FLOAT_SPARSE_LONGKEY);
Release-3.2.0,"dMat.set(MatrixConf.MATRIX_SAVE_PATH, conf.get(""angel.save.model.path""));"
Release-3.2.0,angelClient.addMatrix(dMat);
Release-3.2.0,test the colNum is Long.MAX_VALUE
Release-3.2.0,add sparse longkey float matrix of Long.MAX_VALUE
Release-3.2.0,get configuration from config file
Release-3.2.0,set localDir with enviroment set by nm.
Release-3.2.0,get master location
Release-3.2.0,init task manager and start tasks
Release-3.2.0,start heartbeat thread
Release-3.2.0,taskManager.assignTaskIds(response.getTaskidsList());
Release-3.2.0,todo
Release-3.2.0,"if worker timeout, it may be knocked off."
Release-3.2.0,"SUCCESS, do nothing"
Release-3.2.0,heartbeatFailedTime = 0;
Release-3.2.0,private KEY currentKey;
Release-3.2.0,will be created
Release-3.2.0,TODO Auto-generated method stub
Release-3.2.0,Bitmap bitmap = new Bitmap();
Release-3.2.0,int max = indexArray[size - 1];
Release-3.2.0,byte [] bitIndexArray = new byte[max / 8 + 1];
Release-3.2.0,for(int i = 0; i < size; i++){
Release-3.2.0,int bitIndex = indexArray[i] >> 3;
Release-3.2.0,int bitOffset = indexArray[i] - (bitIndex << 3);
Release-3.2.0,switch(bitOffset){
Release-3.2.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
Release-3.2.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
Release-3.2.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
Release-3.2.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
Release-3.2.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
Release-3.2.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
Release-3.2.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
Release-3.2.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
Release-3.2.0,}
Release-3.2.0,}
Release-3.2.0,"true, false"
Release-3.2.0,//////////////////////////////
Release-3.2.0,Application Configs
Release-3.2.0,//////////////////////////////
Release-3.2.0,//////////////////////////////
Release-3.2.0,Master Configs
Release-3.2.0,//////////////////////////////
Release-3.2.0,//////////////////////////////
Release-3.2.0,Worker Configs
Release-3.2.0,//////////////////////////////
Release-3.2.0,//////////////////////////////
Release-3.2.0,Task Configs
Release-3.2.0,//////////////////////////////
Release-3.2.0,//////////////////////////////
Release-3.2.0,ParameterServer Configs
Release-3.2.0,//////////////////////////////
Release-3.2.0,//////////////////////////////
Release-3.2.0,Kubernetes Configs.
Release-3.2.0,//////////////////////////////
Release-3.2.0,////////////////// IPC //////////////////////////
Release-3.2.0,//////////////////////////////
Release-3.2.0,Matrix transfer Configs.
Release-3.2.0,//////////////////////////////
Release-3.2.0,//////////////////////////////
Release-3.2.0,Matrix transfer Configs.
Release-3.2.0,//////////////////////////////
Release-3.2.0,Configs used to ANGEL_PS_PSAGENT running mode future.
Release-3.2.0,model parse
Release-3.2.0,Mark whether use pyangel or not.
Release-3.2.0,private Configuration conf;
Release-3.2.0,"Configuration that should be used in python environment, there should only be one"
Release-3.2.0,configuration instance in each Angel context.
Release-3.2.0,Use private access means jconf should not be changed or modified in this way.
Release-3.2.0,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
Release-3.2.0,Do nothing
Release-3.2.0,To-DO: add other ways to justify different value types
Release-3.2.0,"This is so ugly, must re-implement by more elegance way"
Release-3.2.0,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
Release-3.2.0,and other files submitted by user.
Release-3.2.0,Launch python process
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-3.2.0,get a angel client
Release-3.2.0,add sparse float matrix
Release-3.2.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-3.2.0,siMat.setPartitionClass(CSRPartition.class);
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,Init node neighbors
Release-3.2.0,client.asyncUpdate(new InitNeighborOver(new InitNeighborOverParam(matrixId))).get();
Release-3.2.0,Sample the neighbors
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-3.2.0,get a angel client
Release-3.2.0,add sparse float matrix
Release-3.2.0,siMat.setValidIndexNum(100);
Release-3.2.0,siMat.setColNum(10000000000L);
Release-3.2.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-3.2.0,siMat.setPartitionClass(CSRPartition.class);
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,Init node neighbors and feats
Release-3.2.0,Sample the neighbors
Release-3.2.0,TODO Auto-generated constructor stub
Release-3.2.0,set basic configuration keys
Release-3.2.0,use local deploy mode and dummy dataspliter
Release-3.2.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-3.2.0,get a angel client
Release-3.2.0,add sparse float matrix
Release-3.2.0,Start PS
Release-3.2.0,Start to run application
Release-3.2.0,Init node neighbors
Release-3.2.0,Sample the neighbors
Release-3.2.0,flags len for init value: 7*4
Release-3.2.0,Just return original features
Release-3.2.0,clear();
Release-3.2.0,Long type node id
Release-3.2.0,"If node not exist, just skip"
Release-3.2.0,"TODO: support String, Int, and Any type node id"
Release-3.2.0,Get matrix meta
Release-3.2.0,Long type node id
Release-3.2.0,"If node not exist, just skip"
Release-3.2.0,"TODO: support String, Int, and Any type node id"
Release-3.2.0,Long type node id
Release-3.2.0,"If node not exist, just skip"
Release-3.2.0,"TODO: support String, Int, and Any type node id"
Release-3.2.0,Long type node id
Release-3.2.0,"If node not exist, just skip"
Release-3.2.0,"TODO: support String, Int, and Any type node id"
Release-3.2.0,Get matrix meta
Release-3.2.0,Split
Release-3.2.0,Generate Part psf get param
Release-3.2.0,Long type node id
Release-3.2.0,"If node not exist, just skip"
Release-3.2.0,"TODO: support String, Int, and Any type node id"
Release-3.2.0,Long type node id
Release-3.2.0,"If node not exist, just skip"
Release-3.2.0,"TODO: support String, Int, and Any type node id"
Release-3.2.0,Long type node id
Release-3.2.0,"If node not exist, just skip"
Release-3.2.0,"TODO: support String, Int, and Any type node id"
Release-3.2.0,Get node neighbor number
Release-3.2.0,"If the neighbor number > count, just copy a range of neighbors to"
Release-3.2.0,"the result array, the copy position is random"
Release-3.2.0,Get node neighbor number
Release-3.2.0,"If the neighbor number > count, just copy a range of neighbors to"
Release-3.2.0,"the result array, the copy position is random"
Release-3.2.0,sample types
Release-3.2.0,Get node neighbor number
Release-3.2.0,Get node neighbor number
Release-3.2.0,"If the neighbor number > count, just copy a range of neighbors to"
Release-3.2.0,"the result array, the copy position is random"
Release-3.2.0,sample node types
Release-3.2.0,sample edge types
Release-3.2.0,Get node neighbor number
Release-3.2.0,"If the neighbor number > count, just copy a range of neighbors to"
Release-3.2.0,"the result array, the copy position is random"
Release-3.2.0,sample types
Release-3.2.0,Get matrix meta
Release-3.2.0,sample (count / partNum + 1) in every partition randomly
Release-3.2.0,Generate rpc params
Release-3.2.0,Sample part result
Release-3.2.0,Neighbors
Release-3.2.0,Neighbors type
Release-3.2.0,Edge type
Release-3.2.0,Sample part result
Release-3.2.0,Neighbors
Release-3.2.0,Get matrix meta
Release-3.2.0,Split nodeIds
Release-3.2.0,Generate node ids
Release-3.2.0,Get matrix meta
Release-3.2.0,Split nodeIds
Release-3.2.0,Generate rpc params
Release-3.2.0,Get matrix meta
Release-3.2.0,Split nodeIds
Release-3.2.0,Generate node ids
Release-3.2.0,Sample part result
Release-3.2.0,Neighbors
Release-3.2.0,EdgeFeatures
Release-3.2.0,Get matrix meta
Release-3.2.0,Split nodeIds
Release-3.2.0,Generate rpc params
Release-3.2.0,Get node ids and features
Release-3.2.0,Get nodes and features
Release-3.2.0,Get nodes and features
Release-3.2.0,sample continuously beginning from a random index
Release-3.2.0,Get node neighbor number
Release-3.2.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-3.2.0,Get node neighbor number
Release-3.2.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-3.2.0,"int matrixId, PartitionKey partKey, long[] keyIds, int startIdx, int endIdx"
Release-3.2.0,clear();
Release-3.2.0,int completeCount = 0;
Release-3.2.0,ArrayList<WalkPath> paths = new ArrayList<>(pathTail.size());
Release-3.2.0,
Release-3.2.0,row.startWrite();
Release-3.2.0,try {
Release-3.2.0,"ObjectIterator<Map.Entry<Long, Long>> iter = pathTail.entrySet().iterator();"
Release-3.2.0,while (iter.hasNext()) {
Release-3.2.0,"Map.Entry<Long, Long> entry = iter.next();"
Release-3.2.0,long key = entry.getKey();
Release-3.2.0,long tail = entry.getValue();
Release-3.2.0,
Release-3.2.0,WalkPath wPath = (WalkPath) row.get(key);
Release-3.2.0,wPath.add2Path(tail);
Release-3.2.0,
Release-3.2.0,if (wPath.isComplete()) {
Release-3.2.0,completeCount += 1;
Release-3.2.0,} else {
Release-3.2.0,paths.add(wPath);
Release-3.2.0,}
Release-3.2.0,}
Release-3.2.0,} finally {
Release-3.2.0,row.endWrite();
Release-3.2.0,}
Release-3.2.0,
Release-3.2.0,if (completeCount != 0) {
Release-3.2.0,"PathQueue.progress(partKey.getPartitionId(), completeCount);"
Release-3.2.0,}
Release-3.2.0,
Release-3.2.0,if (!paths.isEmpty()) {
Release-3.2.0,"PathQueue.pushBatch(partKey.getPartitionId(), paths);"
Release-3.2.0,}
Release-3.2.0,"System.out.println(Thread.currentThread().getId() + ""\t serialize -> "" + ());"
Release-3.2.0,clear();
Release-3.2.0,"System.out.println(Thread.currentThread().getId() + ""\t deserialize -> "" + size);"
Release-3.2.0,Get node neighbor number
Release-3.2.0,"If the neighbor number is 0, just return a int[0]"
Release-3.2.0,"If count <= 0 or the neighbor number is less or equal then count, just copy all neighbors to the result array"
Release-3.2.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-3.2.0,Store the total neighbor number of all nodes in rowOffsets
Release-3.2.0,"Put the node ids, node neighbor number, node neighbors to the cache"
Release-3.2.0,No data in this partition
Release-3.2.0,Get total neighbor number
Release-3.2.0,Final matrix column indices: neighbors node ids
Release-3.2.0,Write positions in cloumnIndices for nodes
Release-3.2.0,Copy all cached sub column indices to final column indices
Release-3.2.0,Read position for a sub column indices
Release-3.2.0,Copy column indices for a node to final column indices
Release-3.2.0,Update write position for this node in final column indices
Release-3.2.0,Update the read position in sub column indices
Release-3.2.0,Clear all temp data
Release-3.2.0,Get node neighbor number
Release-3.2.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-3.2.0,sample happens here to avoid memory copy on servers
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Data format
Release-3.2.0,Feature number of train data
Release-3.2.0,Tree number
Release-3.2.0,Tree depth
Release-3.2.0,Split number
Release-3.2.0,Feature sample ratio
Release-3.2.0,Ratio of validation
Release-3.2.0,Learning rate
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Set data format
Release-3.2.0,"Set angel resource, #worker, #task, #PS"
Release-3.2.0,Set GBDT algorithm parameters
Release-3.2.0,Set training data path
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set predict data path
Release-3.2.0,Set load model path
Release-3.2.0,Set predict result path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Train batch number per epoch.
Release-3.2.0,Batch number
Release-3.2.0,Model type
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Set data format
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,set sgd LR algorithm parameters #feature #epoch
Release-3.2.0,Set trainning data path
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Train batch number per epoch.
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Set data format
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,set sgd FM algorithm parameters #feature #epoch
Release-3.2.0,Set trainning data path
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,Model type
Release-3.2.0,String modelType = String.valueOf(RowType.T_FLOAT_DENSE);
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,set sgd LR algorithm parameters #feature #epoch
Release-3.2.0,"conf.set(MLConf.ML_MODEL_TYPE(), modelType);"
Release-3.2.0,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
Release-3.2.0,predictTest();
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Set data format
Release-3.2.0,Set trainning data path
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set data format
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,class number
Release-3.2.0,Model type
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Set data format
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,set sgd LR algorithm parameters #feature #epoch
Release-3.2.0,Set log path
Release-3.2.0,Set trainning data path
Release-3.2.0,Set save model path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set save model path
Release-3.2.0,Set actionType incremental train
Release-3.2.0,Set log path
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set predict result path
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set training data path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Cluster center number
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Sample ratio per mini-batch
Release-3.2.0,C
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-3.2.0,Set data format
Release-3.2.0,Set trainning data path
Release-3.2.0,Set save model path
Release-3.2.0,Set log save path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set save model path
Release-3.2.0,Set actionType incremental train
Release-3.2.0,Set log path
Release-3.2.0,Set testing data path
Release-3.2.0,Set load model path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Model type
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Set data format
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,set sgd LR algorithm parameters #feature #epoch
Release-3.2.0,Set trainning data path
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set predict result path
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Data is classification
Release-3.2.0,Model is classification
Release-3.2.0,Train batch number per epoch.
Release-3.2.0,loss delta
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Set data format
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,set sgd LR algorithm parameters #feature #epoch
Release-3.2.0,Set trainning data path
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set save model path
Release-3.2.0,Set actionType incremental train
Release-3.2.0,Set log path
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,Feature number of train data
Release-3.2.0,Total iteration number
Release-3.2.0,Validation sample Ratio
Release-3.2.0,"Data format, libsvm or dummy"
Release-3.2.0,Data is classification
Release-3.2.0,Model is classification
Release-3.2.0,Train batch number per epoch.
Release-3.2.0,Learning rate
Release-3.2.0,Decay of learning rate
Release-3.2.0,Regularization coefficient
Release-3.2.0,Set local deploy mode
Release-3.2.0,Set basic configuration keys
Release-3.2.0,Set data format
Release-3.2.0,"set angel resource parameters #worker, #task, #PS"
Release-3.2.0,set sgd LR algorithm parameters #feature #epoch
Release-3.2.0,Set trainning data path
Release-3.2.0,Set save model path
Release-3.2.0,Set log path
Release-3.2.0,Set actionType train
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set save model path
Release-3.2.0,Set actionType incremental train
Release-3.2.0,Set log path
Release-3.2.0,Set trainning data path
Release-3.2.0,Set load model path
Release-3.2.0,Set predict result path
Release-3.2.0,Set actionType prediction
Release-3.2.0,TODO: optimize int key indices
Release-3.2.0,"System.out.println(""deserialize cols.length="" + nCols);"
Release-3.2.0,"System.out.print(""deserialize "");"
Release-3.2.0,"System.out.print(cols[c] + "" "");"
Release-3.2.0,System.out.println();
Release-3.2.0,TODO Auto-generated method stub
Release-3.2.0,"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-3.2.0,"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-3.2.0,"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));"
Release-3.2.0,"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));"
Release-3.2.0,"ground truth: positive, precision: positive"
Release-3.2.0,start row index for words
Release-3.2.0,start row index for docs
Release-3.2.0,doc ids
Release-3.2.0,topic assignments
Release-3.2.0,word to docs reverse index
Release-3.2.0,count word
Release-3.2.0,build word start index
Release-3.2.0,build word to doc reverse idx
Release-3.2.0,build dks
Release-3.2.0,dks = new TraverseHashMap[n_docs];
Release-3.2.0,for (int d = 0; d < n_docs; d++) {
Release-3.2.0,if (K < Short.MAX_VALUE) {
Release-3.2.0,if (docs.get(d).len < Byte.MAX_VALUE)
Release-3.2.0,dks[d] = new S2BTraverseMap(docs.get(d).len);
Release-3.2.0,if (docs.get(d).len < Short.MAX_VALUE)
Release-3.2.0,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
Release-3.2.0,else
Release-3.2.0,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
Release-3.2.0,} else {
Release-3.2.0,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
Release-3.2.0,}
Release-3.2.0,}
Release-3.2.0,build dks
Release-3.2.0,allocate update maps
Release-3.2.0,Skip if no token for this word
Release-3.2.0,Check whether error when fetching word-topic
Release-3.2.0,Build FTree for current word
Release-3.2.0,current doc
Release-3.2.0,old topic assignment
Release-3.2.0,"Check if error happens. if this happen, it's probably that failures happen to servers."
Release-3.2.0,We need to adjust the memory settings or network fetching parameters.
Release-3.2.0,Update statistics if needed
Release-3.2.0,Calculate psum and sample new topic
Release-3.2.0,Update statistics if needed
Release-3.2.0,Assign new topic
Release-3.2.0,Skip if no token for this word
Release-3.2.0,if (u >= p[end]) {
Release-3.2.0,"System.out.println(""u="" + u + "" p[end]="" + p[end] + "" start="" + start + "" end="" + end);"
Release-3.2.0,return end;
Release-3.2.0,}
Release-3.2.0,
Release-3.2.0,if (u < p[start]) {
Release-3.2.0,"System.out.println(""u="" + u + "" p[start]="" + p[start] + "" start="" + start + "" end="" + end);"
Release-3.2.0,return start;
Release-3.2.0,}
Release-3.2.0,The starting point
Release-3.2.0,There's always an unused entry.
Release-3.2.0,The starting point
Release-3.2.0,There's always an unused entry.
Release-3.2.0,The starting point
Release-3.2.0,There's always an unused entry.
Release-3.2.0,The starting point
Release-3.2.0,There's always an unused entry.
Release-3.2.0,The starting point
Release-3.2.0,There's always an unused entry.
Release-3.2.0,The starting point
Release-3.2.0,There's always an unused entry.
Release-3.2.0,print();
Release-3.2.0,The starting point
Release-3.2.0,There's always an unused entry.
Release-3.2.0,The starting point
Release-3.2.0,There's always an unused entry.
Release-3.2.0,The starting point
Release-3.2.0,There's always an unused entry.
Release-3.2.0,The starting point
Release-3.2.0,There's always an unused entry.
Release-3.2.0,print();
Release-3.2.0,Write #rows
Release-3.2.0,Write each row
Release-3.2.0,dense
Release-3.2.0,sparse
Release-3.2.0,LOG.info(buf.refCnt());
Release-3.2.0,dense
Release-3.2.0,sparse
Release-3.2.0,dense
Release-3.2.0,sparse
Release-3.2.0,calculate columns
Release-3.2.0,reset(row);
Release-3.2.0,loss function
Release-3.2.0,gradient and hessian
Release-3.2.0,"categorical feature set, null: none, empty: all, else: partial"
Release-3.2.0,"node's end index in instancePos, instances in [start, end] belong to a tree node"
Release-3.2.0,initialize the phase
Release-3.2.0,current tree and depth
Release-3.2.0,create loss function
Release-3.2.0,calculate grad info of each instance
Release-3.2.0,"create data sketch, push candidate split value to PS"
Release-3.2.0,1. calculate candidate split value
Release-3.2.0,categorical features
Release-3.2.0,2. push local sketch to PS
Release-3.2.0,the leader worker
Release-3.2.0,merge categorical features
Release-3.2.0,create updates
Release-3.2.0,"pull the global sketch from PS, only called once by each worker"
Release-3.2.0,number of categorical feature
Release-3.2.0,sample feature
Release-3.2.0,push sampled feature set to the current tree
Release-3.2.0,create new tree
Release-3.2.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
Release-3.2.0,calculate gradient
Release-3.2.0,"1. create new tree, initialize tree nodes and node stats"
Release-3.2.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
Release-3.2.0,2.1. pull the sampled features of the current tree
Release-3.2.0,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
Release-3.2.0,"2.2. if use all the features, only called one"
Release-3.2.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
Release-3.2.0,4. set root node to active
Release-3.2.0,"5. reset instance position, set the root node's span"
Release-3.2.0,6. calculate gradient
Release-3.2.0,1. decide nodes that should be calculated
Release-3.2.0,2. decide calculated and subtracted tree nodes
Release-3.2.0,3. calculate threads
Release-3.2.0,wait until all threads finish
Release-3.2.0,4. subtract threads
Release-3.2.0,wait until all threads finish
Release-3.2.0,5. send histograms to PS
Release-3.2.0,6. update histogram cache
Release-3.2.0,clock
Release-3.2.0,find split
Release-3.2.0,"1. find responsible tree node, using RR scheme"
Release-3.2.0,2. pull gradient histogram
Release-3.2.0,2.1. get the name of this node's gradient histogram on PS
Release-3.2.0,2.2. pull the histogram
Release-3.2.0,2.3. find best split result of this tree node
Release-3.2.0,2.3.1 using server split
Release-3.2.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-3.2.0,update the grad stats of children node
Release-3.2.0,update the left child
Release-3.2.0,update the right child
Release-3.2.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
Release-3.2.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
Release-3.2.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
Release-3.2.0,2.3.5 reset this tree node's gradient histogram to 0
Release-3.2.0,3. push split feature to PS
Release-3.2.0,4. push split value to PS
Release-3.2.0,5. push split gain to PS
Release-3.2.0,6. set phase to AFTER_SPLIT
Release-3.2.0,this.phase = GBDTPhase.AFTER_SPLIT;
Release-3.2.0,clock
Release-3.2.0,1. get split feature
Release-3.2.0,2. get split value
Release-3.2.0,3. get split gain
Release-3.2.0,4. get node weight
Release-3.2.0,5. split node
Release-3.2.0,update local replica
Release-3.2.0,create AfterSplit task
Release-3.2.0,"2. check thread stats, if all threads finish, return"
Release-3.2.0,6. clock
Release-3.2.0,"split the span of one node, reset the instance position"
Release-3.2.0,in case this worker has no instance on this node
Release-3.2.0,set the span of left child
Release-3.2.0,set the span of right child
Release-3.2.0,"1. left to right, find the first instance that should be in the right child"
Release-3.2.0,"2. right to left, find the first instance that should be in the left child"
Release-3.2.0,3. swap two instances
Release-3.2.0,4. find the cut pos
Release-3.2.0,5. set the span of left child
Release-3.2.0,6. set the span of right child
Release-3.2.0,set tree node to active
Release-3.2.0,set node to leaf
Release-3.2.0,set node to inactive
Release-3.2.0,finish current depth
Release-3.2.0,finish current tree
Release-3.2.0,set the tree phase
Release-3.2.0,check if there is active node
Release-3.2.0,check if finish all the tree
Release-3.2.0,update node's grad stats on PS
Release-3.2.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
Release-3.2.0,the root node's stats is updated by leader worker
Release-3.2.0,1. create the update
Release-3.2.0,2. push the update to PS
Release-3.2.0,1. update predictions of training data
Release-3.2.0,2. update predictions of validation data
Release-3.2.0,the leader task adds node prediction to flush list
Release-3.2.0,1. name of this node's grad histogram on PS
Release-3.2.0,2. build the grad histogram of this node
Release-3.2.0,3. push the histograms to PS
Release-3.2.0,4. reset thread stats to finished
Release-3.2.0,5.1. set the children nodes of this node
Release-3.2.0,5.2. set split info and grad stats to this node
Release-3.2.0,5.2. create children nodes
Release-3.2.0,"5.3. create node stats for children nodes, and add them to the tree"
Release-3.2.0,5.4. reset instance position
Release-3.2.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
Release-3.2.0,5.6. set children nodes to leaf nodes
Release-3.2.0,5.7. set nid to leaf node
Release-3.2.0,5.8. deactivate active node
Release-3.2.0,"get feature type, 0:empty 1:all equal 2:real"
Release-3.2.0,"if not -1, sufficient space will be allocated at once"
Release-3.2.0,copy the highest levels
Release-3.2.0,copy baseBuffer
Release-3.2.0,merge two non-empty quantile sketches
Release-3.2.0,left child <= split value; right child > split value
Release-3.2.0,"the first: minimal, the last: maximal"
Release-3.2.0,categorical features
Release-3.2.0,continuous features
Release-3.2.0,left child <= split value; right child > split value
Release-3.2.0,feature index used to split
Release-3.2.0,feature value used to split
Release-3.2.0,loss change after split this node
Release-3.2.0,grad stats of the left child
Release-3.2.0,grad stats of the right child
Release-3.2.0,"LOG.info(""Constructor with fid = -1"");"
Release-3.2.0,fid = -1: no split currently
Release-3.2.0,the minimal split value is the minimal value of feature
Release-3.2.0,the splits do not include the maximal value of feature
Release-3.2.0,"1. the average distance, (maxValue - minValue) / splitNum"
Release-3.2.0,2. calculate the candidate split value
Release-3.2.0,1. new feature's histogram (grad + hess)
Release-3.2.0,size: sampled_featureNum * (2 * splitNum)
Release-3.2.0,"in other words, concatenate each feature's histogram"
Release-3.2.0,2. get the span of this node
Release-3.2.0,------ 3. using sparse-aware method to build histogram ---
Release-3.2.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
Release-3.2.0,3.1. get the instance index
Release-3.2.0,3.2. get the grad and hess of the instance
Release-3.2.0,3.3. add to the sum
Release-3.2.0,3.4. loop the non-zero entries
Release-3.2.0,3.4.1. get feature value
Release-3.2.0,3.4.2. current feature's position in the sampled feature set
Release-3.2.0,"int fPos = findFidPlace(this.controller.fSet, fid);"
Release-3.2.0,3.4.3. find the position of feature value in a histogram
Release-3.2.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
Release-3.2.0,3.4.4. add the grad and hess to the corresponding bin
Release-3.2.0,3.4.5. add the reverse to the bin that contains 0.0f
Release-3.2.0,4. add the grad and hess sum to the zero bin of all features
Release-3.2.0,find the best split result of the histogram of a tree node
Release-3.2.0,1. calculate the gradStats of the root node
Release-3.2.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
Release-3.2.0,2. loop over features
Release-3.2.0,2.1. get the ture feature id in the sampled feature set
Release-3.2.0,2.2. get the indexes of histogram of this feature
Release-3.2.0,2.3. find the best split of current feature
Release-3.2.0,2.4. update the best split result if possible
Release-3.2.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-3.2.0,3. update the grad stats of children node
Release-3.2.0,3.1. update the left child
Release-3.2.0,3.2. update the right child
Release-3.2.0,find the best split result of one feature
Release-3.2.0,1. set the feature id
Release-3.2.0,2. create the best left stats and right stats
Release-3.2.0,3. the gain of the root node
Release-3.2.0,4. create the temp left and right grad stats
Release-3.2.0,5. loop over all the data in histogram
Release-3.2.0,5.1. get the grad and hess of current hist bin
Release-3.2.0,5.2. check whether we can split with current left hessian
Release-3.2.0,right = root - left
Release-3.2.0,5.3. check whether we can split with current right hessian
Release-3.2.0,5.4. calculate the current loss gain
Release-3.2.0,5.5. check whether we should update the split result with current loss gain
Release-3.2.0,split value = sketches[splitIdx]
Release-3.2.0,"5.6. if should update, also update the best left and right grad stats"
Release-3.2.0,6. set the best left and right grad stats
Release-3.2.0,partition number
Release-3.2.0,cols of each partition
Release-3.2.0,1. calculate the total grad sum and hess sum
Release-3.2.0,2. create the grad stats of the node
Release-3.2.0,1. calculate the total grad sum and hess sum
Release-3.2.0,2. create the grad stats of the node
Release-3.2.0,1. calculate the total grad sum and hess sum
Release-3.2.0,2. create the grad stats of the node
Release-3.2.0,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
Release-3.2.0,assert fvalue >= sketch[start] && fvalue <= sketch[end];
Release-3.2.0,if (left > end) return end - start;
Release-3.2.0,find the best split result of the histogram of a tree node
Release-3.2.0,2.2. get the indexes of histogram of this feature
Release-3.2.0,2.3. find the best split of current feature
Release-3.2.0,2.4. update the best split result if possible
Release-3.2.0,find the best split result of one feature
Release-3.2.0,1. set the feature id
Release-3.2.0,splitEntry.setFid(fid);
Release-3.2.0,2. create the best left stats and right stats
Release-3.2.0,3. the gain of the root node
Release-3.2.0,4. create the temp left and right grad stats
Release-3.2.0,5. loop over all the data in histogram
Release-3.2.0,5.1. get the grad and hess of current hist bin
Release-3.2.0,5.2. check whether we can split with current left hessian
Release-3.2.0,right = root - left
Release-3.2.0,5.3. check whether we can split with current right hessian
Release-3.2.0,5.4. calculate the current loss gain
Release-3.2.0,5.5. check whether we should update the split result with current loss gain
Release-3.2.0,"5.6. if should update, also update the best left and right grad stats"
Release-3.2.0,6. set the best left and right grad stats
Release-3.2.0,find the best split result of a serve row on the PS
Release-3.2.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
Release-3.2.0,2.2. get the start index in histogram of this feature
Release-3.2.0,2.3. find the best split of current feature
Release-3.2.0,2.4. update the best split result if possible
Release-3.2.0,"find the best split result of one feature from a server row, used by the PS"
Release-3.2.0,1. set the feature id
Release-3.2.0,2. create the best left stats and right stats
Release-3.2.0,3. the gain of the root node
Release-3.2.0,4. create the temp left and right grad stats
Release-3.2.0,5. loop over all the data in histogram
Release-3.2.0,5.1. get the grad and hess of current hist bin
Release-3.2.0,5.2. check whether we can split with current left hessian
Release-3.2.0,right = root - left
Release-3.2.0,5.3. check whether we can split with current right hessian
Release-3.2.0,5.4. calculate the current loss gain
Release-3.2.0,5.5. check whether we should update the split result with current loss gain
Release-3.2.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
Release-3.2.0,the task use index to find fvalue
Release-3.2.0,"5.6. if should update, also update the best left and right grad stats"
Release-3.2.0,6. set the best left and right grad stats
Release-3.2.0,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
Release-3.2.0,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
Release-3.2.0,max and min of each feature
Release-3.2.0,clear all the information
Release-3.2.0,calculate the sum of gradient and hess
Release-3.2.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
Release-3.2.0,ridx)
Release-3.2.0,check if necessary information is ready
Release-3.2.0,"same as add, reduce is used in All Reduce"
Release-3.2.0,"features used in this tree, if equals null, means use all the features without sampling"
Release-3.2.0,node in the tree
Release-3.2.0,the gradient info of each instances
Release-3.2.0,initialize nodes
Release-3.2.0,gradient
Release-3.2.0,second order gradient
Release-3.2.0,int sendStartCol = (int) row.getStartCol();
Release-3.2.0,logistic loss for binary classification task.
Release-3.2.0,"logistic loss, but predict un-transformed margin"
Release-3.2.0,check if label in range
Release-3.2.0,return the default evaluation metric for the objective
Release-3.2.0,"task type: classification, regression, or ranking"
Release-3.2.0,"quantile sketch, size = featureNum * splitNum"
Release-3.2.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
Release-3.2.0,"active tree nodes, size = pow(2, treeDepth) -1"
Release-3.2.0,sampled features. size = treeNum * sampleRatio * featureNum
Release-3.2.0,categorical feature. size = workerNum * cateFeatNum * splitNum
Release-3.2.0,"split features, size = treeNum * treeNodeNum"
Release-3.2.0,"split values, size = treeNum * treeNodeNum"
Release-3.2.0,"split gains, size = treeNum * treeNodeNum"
Release-3.2.0,"node weights, size = treeNum * treeNodeNum"
Release-3.2.0,"node preds, size = treeNum * treeNodeNum"
Release-3.2.0,if using PS to perform split
Release-3.2.0,step size for a tree
Release-3.2.0,number of class
Release-3.2.0,minimum loss change required for a split
Release-3.2.0,maximum depth of a tree
Release-3.2.0,number of features
Release-3.2.0,number of nonzero
Release-3.2.0,number of candidates split value
Release-3.2.0,----- the rest parameters are less important ----
Release-3.2.0,base instance weight
Release-3.2.0,minimum amount of hessian(weight) allowed in a child
Release-3.2.0,L2 regularization factor
Release-3.2.0,L1 regularization factor
Release-3.2.0,default direction choice
Release-3.2.0,maximum delta update we can add in weight estimation
Release-3.2.0,this parameter can be used to stabilize update
Release-3.2.0,default=0 means no constraint on weight delta
Release-3.2.0,whether we want to do subsample for row
Release-3.2.0,whether to subsample columns for each tree
Release-3.2.0,accuracy of sketch
Release-3.2.0,accuracy of sketch
Release-3.2.0,leaf vector size
Release-3.2.0,option for parallelization
Release-3.2.0,option to open cacheline optimization
Release-3.2.0,whether to not print info during training.
Release-3.2.0,maximum depth of the tree
Release-3.2.0,number of features used for tree construction
Release-3.2.0,"minimum loss change required for a split, otherwise stop split"
Release-3.2.0,----- the rest parameters are less important ----
Release-3.2.0,default direction choice
Release-3.2.0,whether we want to do sample data
Release-3.2.0,whether to sample columns during tree construction
Release-3.2.0,whether to use histogram for split
Release-3.2.0,number of histogram units
Release-3.2.0,whether to print info during training.
Release-3.2.0,----- the rest parameters are obtained after training ----
Release-3.2.0,total number of nodes
Release-3.2.0,number of deleted nodes */
v3.1.0,@maxIndex: this variable contains the max index of node/word
v3.1.0,some params
v3.1.0,max index for node/word
v3.1.0,compute number of nodes for one row
v3.1.0,check the length of dot values
v3.1.0,merge dot values from all partitions
v3.1.0,Skip-Gram model
v3.1.0,Negative sampling
v3.1.0,used to accumulate the updates for input vectors
v3.1.0,Negative sampling
v3.1.0,accumulate for the hidden layer
v3.1.0,update output layer
v3.1.0,update the hidden layer
v3.1.0,update input
v3.1.0,Skip-Gram model
v3.1.0,Negative sampling
v3.1.0,used to accumulate the updates for input vectors
v3.1.0,Negative sampling
v3.1.0,accumulate for the hidden layer
v3.1.0,update output layer
v3.1.0,update the hidden layer
v3.1.0,update input
v3.1.0,update output
v3.1.0,Some params
v3.1.0,compute number of nodes for one row
v3.1.0,compress the neighbor IDs
v3.1.0,write out edges
v3.1.0,write out tags
v3.1.0,Get node neighbors
v3.1.0,Use by line with weight
v3.1.0,evict entry with the smallest degree
v3.1.0,// calculate bias
v3.1.0,if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {
v3.1.0,"double zVal = VectorUtils.getDouble(z, 0);"
v3.1.0,"double nVal = VectorUtils.getDouble(n, 0);"
v3.1.0,"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));"
v3.1.0,}
v3.1.0,Do nothing.
v3.1.0,split updates
v3.1.0,shuffle update splits
v3.1.0,generate part update splits
v3.1.0,"set split context: partition key, use int key for long key vector or net"
v3.1.0,how to do intersection for two dense vector with a given indices ??
v3.1.0,copy the highest levels
v3.1.0,copy baseBuffer
v3.1.0,merge two non-empty quantile sketches
v3.1.0,"if not -1, sufficient space will be allocated at once"
v3.1.0,InstanceRow ins = instanceRows[insId];
v3.1.0,int[] indices = ins.indices();
v3.1.0,int[] bins = ins.bins();
v3.1.0,int nnz = indices.length;
v3.1.0,for (int j = 0; j < nnz; j++) {
v3.1.0,int fid = indices[j];
v3.1.0,if (isFeatUsed[fid - featLo]) {
v3.1.0,"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);"
v3.1.0,}
v3.1.0,}
v3.1.0,1. allocate histogram
v3.1.0,"2. loop non-zero instances, accumulate to histogram"
v3.1.0,if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature
v3.1.0,3. add remaining grad and hess to default bin
v3.1.0,"return param.calcWeights(grad, hess);"
v3.1.0,"numClass is usually small, so we do not use arraycopy here"
v3.1.0,"numClass is usually small, so we do not use arraycopy here"
v3.1.0,TODO: use more schema on default bin
v3.1.0,1. set default bin to left child
v3.1.0,"2. for other bins, find its location"
v3.1.0,3. create split set
v3.1.0,this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];
v3.1.0,predict sparse instance with indices and values
v3.1.0,predict libsvm data
v3.1.0,"Preconditions.checkArgument(preds.length == labels.length,"
v3.1.0,"""LogLossMetric should be used for binary-label classification"");"
v3.1.0,double loss = 0.0;
v3.1.0,for (int i = 0; i < preds.length; i++) {
v3.1.0,"loss += evalOne(preds[i], labels[i]);"
v3.1.0,}
v3.1.0,return loss / labels.length;
v3.1.0,double error = 0.0;
v3.1.0,if (preds.length == labels.length) {
v3.1.0,for (int i = 0; i < preds.length; i++) {
v3.1.0,"error += evalOne(preds[i], labels[i]);"
v3.1.0,}
v3.1.0,} else {
v3.1.0,int numLabel = preds.length / labels.length;
v3.1.0,float[] pred = new float[numLabel];
v3.1.0,for (int i = 0; i < labels.length; i++) {
v3.1.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
v3.1.0,"error += evalOne(pred, labels[i]);"
v3.1.0,}
v3.1.0,}
v3.1.0,return error / labels.length;
v3.1.0,Preconditions.checkArgument(preds.length != labels.length
v3.1.0,"&& preds.length % labels.length == 0,"
v3.1.0,"""CrossEntropyMetric should be used for multi-label classification"");"
v3.1.0,double loss = 0.0;
v3.1.0,int numLabel = preds.length / labels.length;
v3.1.0,float[] pred = new float[numLabel];
v3.1.0,for (int i = 0; i < labels.length; i++) {
v3.1.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
v3.1.0,"loss += evalOne(pred, labels[i]);"
v3.1.0,}
v3.1.0,return loss / labels.length;
v3.1.0,double correct = 0.0;
v3.1.0,if (preds.length == labels.length) {
v3.1.0,for (int i = 0; i < preds.length; i++) {
v3.1.0,"correct += evalOne(preds[i], labels[i]);"
v3.1.0,}
v3.1.0,} else {
v3.1.0,int numLabel = preds.length / labels.length;
v3.1.0,float[] pred = new float[numLabel];
v3.1.0,for (int i = 0; i < labels.length; i++) {
v3.1.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
v3.1.0,"correct += evalOne(pred, labels[i]);"
v3.1.0,}
v3.1.0,}
v3.1.0,return (float) (correct / labels.length);
v3.1.0,double errSum = 0.0f;
v3.1.0,if (preds.length == labels.length) {
v3.1.0,for (int i = 0; i < preds.length; i++) {
v3.1.0,"errSum += evalOne(preds[i], labels[i]);"
v3.1.0,}
v3.1.0,} else {
v3.1.0,int numLabel = preds.length / labels.length;
v3.1.0,float[] pred = new float[numLabel];
v3.1.0,for (int i = 0; i < labels.length; i++) {
v3.1.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
v3.1.0,"errSum += evalOne(pred, labels[i]);"
v3.1.0,}
v3.1.0,}
v3.1.0,return Math.sqrt(errSum / labels.length);
v3.1.0,"System.out.println(""----------"");"
v3.1.0,"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)"
v3.1.0,"+ "", mask = "" + Integer.toBinaryString(readMaskT));"
v3.1.0,readMaskT <<= 1;
v3.1.0,"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};"
v3.1.0,int n = bits.length;
v3.1.0,BufferedBitSet writeBitSet = new BufferedBitSet(n);
v3.1.0,"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);"
v3.1.0,if (bitSet.get(i) != bits[i]) {
v3.1.0,"throw new RuntimeException("""" + i);"
v3.1.0,}
v3.1.0,private final ByteBuffer bytes;
v3.1.0,"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {"
v3.1.0,int capacity = bytes.capacity() * 8;
v3.1.0,readIndexT = bytes.capacity() - 1;
v3.1.0,return bytes.get(index);
v3.1.0,TODO: use arraycopy to make it faster
v3.1.0,assert from >= this.from && to <= this.to;
v3.1.0,"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));"
v3.1.0,"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));"
v3.1.0,return bits.clone();
v3.1.0,private final SerializableBuffer bytes;
v3.1.0,private final ByteBuffer bytes;
v3.1.0,this.bytes = ByteBuffer.allocate(numBytes);
v3.1.0,public BufferedBitSetWriter(ByteBuffer bytes) {
v3.1.0,this.bytes = bytes;
v3.1.0,}
v3.1.0,"bytes.put(writeIndex++, (byte) writeBuffer);"
v3.1.0,public ByteBuffer getBytes() {
v3.1.0,return bytes;
v3.1.0,}
v3.1.0,ML TreeConf
v3.1.0,GBDT TreeConf
v3.1.0,"edges=[x,...] firstFlow=1 => go to right if < x and go to left if > x"
v3.1.0,"edges=[x,...] firstFlow=0 => go to left if < x and go to right if > x"
v3.1.0,"different types of tree node splits, enumerated by their complexity"
v3.1.0,"in order to reduce model size, we give priority to split point"
v3.1.0,"comparison between two split points, we give priority to lower feature index"
v3.1.0,TODO: comparison between two split sets
v3.1.0,"public boolean leafwise;  // true if leaf-wise training, false if level-wise training"
v3.1.0,TODO: regularization
v3.1.0,TODO: regularization
v3.1.0,public float insSampleRatio;  // subsample ratio for instances
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy data spliter
v3.1.0,get a angel client
v3.1.0,add matrix
v3.1.0,TODO Auto-generated constructor stub
v3.1.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighbor;
v3.1.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborParam;
v3.1.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborResult;
v3.1.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighbor;
v3.1.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighborParam;
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add matrix
v3.1.0,@Test
v3.1.0,"public void testInitAndGet() throws ExecutionException, InterruptedException {"
v3.1.0,Worker worker = LocalClusterContext.get().getWorker(worker0Attempt0Id).getWorker();
v3.1.0,"MatrixClient client1 = worker.getPSAgent().getMatrixClient(""w2"", 0);"
v3.1.0,int matrixW1Id = client1.getMatrixId();
v3.1.0,// Generate graph data
v3.1.0,"Map<Integer, int []> adjMap = generateAdjTable(nodeNum, maxNeighborNum);"
v3.1.0,
v3.1.0,// Init graph adj table
v3.1.0,"InitNeighbor func = new InitNeighbor(new InitNeighborParam(matrixW1Id, adjMap));"
v3.1.0,client1.update(func);
v3.1.0,
v3.1.0,int [] nodeIds = new int[adjMap.size()];
v3.1.0,int i = 0;
v3.1.0,for(int nodeId : adjMap.keySet()) {
v3.1.0,nodeIds[i++] = nodeId;
v3.1.0,}
v3.1.0,
v3.1.0,// Get graph adj table from PS
v3.1.0,"GetNeighbor getFunc = new GetNeighbor(new GetNeighborParam(matrixW1Id, nodeIds, maxNeighborNum));"
v3.1.0,"Map<Integer, int[]> getResults = ((GetNeighborResult) (client1.get(getFunc)))"
v3.1.0,.getNodeIdToNeighborIndices();
v3.1.0,
v3.1.0,// Check the result
v3.1.0,"for(Entry<Integer, int[]> entry : getResults.entrySet()) {"
v3.1.0,"Assert.assertArrayEquals(entry.getValue(), adjMap.get(entry.getKey()));"
v3.1.0,}
v3.1.0,}
v3.1.0,row 0 is a random uniform
v3.1.0,row 1 is a random normal
v3.1.0,row 2 is filled with 1.0
v3.1.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
v3.1.0,"paras[1] = ""abc"";"
v3.1.0,"paras[2] = ""123"";"
v3.1.0,Add standard Hadoop classes
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,set sgd LR algorithm parameters #feature #epoch
v3.1.0,Set input data path
v3.1.0,Set save model path
v3.1.0,Set actionType train
v3.1.0,QSLRRunner runner = new QSLRRunner();
v3.1.0,runner.train(conf);
v3.1.0,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
v3.1.0,Dataset
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,class number
v3.1.0,Model type
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,Train batch number per epoch.
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set file system
v3.1.0,Set basic configuration keys
v3.1.0,Use local deploy mode and data format
v3.1.0,Set data path
v3.1.0,"Set angel resource parameters #worker, #task, #PS"
v3.1.0,Set Softmax algorithm parameters
v3.1.0,Set model class
v3.1.0,Dataset
v3.1.0,Data format
v3.1.0,Feature number of train data
v3.1.0,Tree number
v3.1.0,Tree depth
v3.1.0,Split number
v3.1.0,Feature sample ratio
v3.1.0,Ratio of validation
v3.1.0,Learning rate
v3.1.0,Set file system
v3.1.0,Use local deploy mode and data format
v3.1.0,Set data path
v3.1.0,"Set angel resource, #worker, #task, #PS"
v3.1.0,Set GBDT algorithm parameters
v3.1.0,Dataset
v3.1.0,Set file system
v3.1.0,Set basic configuration keys
v3.1.0,Use local deploy mode
v3.1.0,Set data path
v3.1.0,"Set angel resource parameters #worker, #task, #PS"
v3.1.0,Set DeepFM algorithm parameters
v3.1.0,Set model class
v3.1.0,Dataset
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,Model type
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set file system
v3.1.0,Set basic configuration keys
v3.1.0,Use local deploy mode and data format
v3.1.0,Set data path
v3.1.0,"Set angel resource parameters #worker, #task, #PS"
v3.1.0,Set LR algorithm parameters
v3.1.0,Set model class
v3.1.0,Dataset
v3.1.0,Data format
v3.1.0,Model type
v3.1.0,Cluster center number
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Sample ratio per mini-batch
v3.1.0,C
v3.1.0,Set file system
v3.1.0,Use local deploy mode and data format
v3.1.0,Set data path
v3.1.0,"Set angel resource, #worker, #task, #PS"
v3.1.0,set Kmeans algorithm parameters #cluster #feature #epoch
v3.1.0,Dataset
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,Model type
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set file system
v3.1.0,Set basic configuration keys
v3.1.0,Use local deploy mode and data format
v3.1.0,Set data path
v3.1.0,"Set angel resource parameters #worker, #task, #PS"
v3.1.0,Set FM algorithm parameters
v3.1.0,Set model class
v3.1.0,Dataset
v3.1.0,Set file system
v3.1.0,Set basic configuration keys
v3.1.0,Use local deploy mode
v3.1.0,Set data path
v3.1.0,"Set angel resource parameters #worker, #task, #PS"
v3.1.0,Set WideAndDeep algorithm parameters
v3.1.0,Set model class
v3.1.0,Dataset
v3.1.0,Data format
v3.1.0,"Set LDA parameters #V, #K"
v3.1.0,Set file system
v3.1.0,Use local deploy mode and data format
v3.1.0,Set data path
v3.1.0,"Set angel resource, #worker, #task, #PS"
v3.1.0,Set LDA algorithm parameters
v3.1.0,Dataset
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,Model type
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set file system
v3.1.0,Set basic configuration keys
v3.1.0,Use local deploy mode and data format
v3.1.0,Set data path
v3.1.0,"Set angel resource parameters #worker, #task, #PS"
v3.1.0,Set SVM algorithm parameters
v3.1.0,Set model class
v3.1.0,Dataset
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,Model type
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,Model is classification
v3.1.0,Train batch number per epoch.
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set file system
v3.1.0,Set basic configuration keys
v3.1.0,Use local deploy mode and data format
v3.1.0,Set data path
v3.1.0,"Set angel resource parameters #worker, #task, #PS"
v3.1.0,Set LR algorithm parameters
v3.1.0,Set model class
v3.1.0,Dataset
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,Model type
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,Model is classification
v3.1.0,Train batch number per epoch.
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set file system
v3.1.0,Set basic configuration keys
v3.1.0,Use local deploy mode and data format
v3.1.0,Set data path
v3.1.0,"Set angel resource parameters #worker, #task, #PS"
v3.1.0,Set LR algorithm parameters
v3.1.0,Set model class
v3.1.0,Load model meta
v3.1.0,Convert model
v3.1.0,"Get input path, output path"
v3.1.0,Init serde
v3.1.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v3.1.0,Load model meta
v3.1.0,Convert model
v3.1.0,load hadoop configuration
v3.1.0,"Get input path, output path"
v3.1.0,Init serde
v3.1.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v3.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
v3.1.0,input.seek(rowOffset.getOffset());
v3.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
v3.1.0,input.seek(rowOffset.getOffset());
v3.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
v3.1.0,input.seek(rowOffset.getOffset());
v3.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
v3.1.0,input.seek(rowOffset.getOffset());
v3.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
v3.1.0,input.seek(rowOffset.getOffset());
v3.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
v3.1.0,input.seek(rowOffset.getOffset());
v3.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
v3.1.0,input.seek(rowOffset.getOffset());
v3.1.0,Load model meta
v3.1.0,Check row type
v3.1.0,Load model
v3.1.0,Load model meta
v3.1.0,Check row type
v3.1.0,Load model
v3.1.0,Load model meta
v3.1.0,Check row type
v3.1.0,Load model
v3.1.0,Load model meta
v3.1.0,Check row type
v3.1.0,Load model
v3.1.0,Load model meta
v3.1.0,Check row type
v3.1.0,Load model
v3.1.0,Load model meta
v3.1.0,Check row type
v3.1.0,Load model
v3.1.0,Load model meta
v3.1.0,Check row type
v3.1.0,Load model
v3.1.0,Load model
v3.1.0,load hadoop configuration
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add matrix
v3.1.0,worker register
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add matrix
v3.1.0,add matrix
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add matrix
v3.1.0,attempt 0
v3.1.0,attempt1
v3.1.0,attempt1
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add matrix
v3.1.0,TODO Auto-generated constructor stub
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add matrix
v3.1.0,set basic configuration keys
v3.1.0,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add matrix
v3.1.0,Thread.sleep(5000);
v3.1.0,"response = master.getJobReport(null, request);"
v3.1.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
v3.1.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
v3.1.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add dense double matrix
v3.1.0,add comp dense double matrix
v3.1.0,add sparse double matrix
v3.1.0,add component sparse double matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense long matrix
v3.1.0,add comp dense long matrix
v3.1.0,add sparse long matrix
v3.1.0,add component sparse long matrix
v3.1.0,add comp dense long double matrix
v3.1.0,add sparse long-key double matrix
v3.1.0,add component long-key sparse double matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add sparse long-key float matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add sparse long-key int matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,add sparse long-key long matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add dense double matrix
v3.1.0,add comp dense double matrix
v3.1.0,add sparse double matrix
v3.1.0,add component sparse double matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense long matrix
v3.1.0,add comp dense long matrix
v3.1.0,add sparse long matrix
v3.1.0,add component sparse long matrix
v3.1.0,add comp dense long double matrix
v3.1.0,add sparse long-key double matrix
v3.1.0,add component long-key sparse double matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add sparse long-key float matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add sparse long-key int matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,add sparse long-key long matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,testDenseDoubleUDF();
v3.1.0,testSparseDoubleUDF();
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add dense double matrix
v3.1.0,add comp dense double matrix
v3.1.0,add sparse double matrix
v3.1.0,add component sparse double matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense long matrix
v3.1.0,add comp dense long matrix
v3.1.0,add sparse long matrix
v3.1.0,add component sparse long matrix
v3.1.0,add comp dense long double matrix
v3.1.0,add sparse long-key double matrix
v3.1.0,add component long-key sparse double matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add sparse long-key float matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add sparse long-key int matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,add sparse long-key long matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,client1.clock().get();
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,client1.clock().get();
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add dense double matrix
v3.1.0,add comp dense double matrix
v3.1.0,add sparse double matrix
v3.1.0,add component sparse double matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense long matrix
v3.1.0,add comp dense long matrix
v3.1.0,add sparse long matrix
v3.1.0,add component sparse long matrix
v3.1.0,add comp dense long double matrix
v3.1.0,add sparse long-key double matrix
v3.1.0,add component long-key sparse double matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add sparse long-key float matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add sparse long-key int matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,add sparse long-key long matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,testDenseDoubleUDF();
v3.1.0,testSparseDoubleUDF();
v3.1.0,client1.clock().get();
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,client1.clock().get();
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,TODO Auto-generated constructor stub
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add dense double matrix
v3.1.0,add sparse double matrix
v3.1.0,add comp dense double matrix
v3.1.0,add component sparse double matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense long matrix
v3.1.0,add comp dense long matrix
v3.1.0,add sparse long matrix
v3.1.0,add component sparse long matrix
v3.1.0,add comp dense long double matrix
v3.1.0,add sparse long-key double matrix
v3.1.0,add component long-key sparse double matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add sparse long-key float matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add sparse long-key int matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,add sparse long-key long matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,testDenseDoubleUDF();
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,Assert.assertTrue(index.length == row.size());
v3.1.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
v3.1.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
v3.1.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"LOG.info(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
v3.1.0,IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,for (int i = 0; i < feaNum; i++) {
v3.1.0,"deltaVec.set(i, i);"
v3.1.0,}
v3.1.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
v3.1.0,IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add sparse float matrix
v3.1.0,siMat.setPartitionClass(CSRPartition.class);
v3.1.0,siMat.setPartitionStorageClass(IntCSRStorage.class);
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add dense double matrix
v3.1.0,add comp dense double matrix
v3.1.0,add sparse double matrix
v3.1.0,add component sparse double matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense long matrix
v3.1.0,add comp dense long matrix
v3.1.0,add sparse long matrix
v3.1.0,add component sparse long matrix
v3.1.0,add comp dense long double matrix
v3.1.0,add sparse long-key double matrix
v3.1.0,add component long-key sparse double matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add sparse long-key float matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add sparse long-key int matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,add sparse long-key long matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,get a angel client
v3.1.0,add dense double matrix
v3.1.0,add comp dense double matrix
v3.1.0,add sparse double matrix
v3.1.0,add component sparse double matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense float matrix
v3.1.0,add comp dense float matrix
v3.1.0,add sparse float matrix
v3.1.0,add component sparse float matrix
v3.1.0,add dense long matrix
v3.1.0,add comp dense long matrix
v3.1.0,add sparse long matrix
v3.1.0,add component sparse long matrix
v3.1.0,add comp dense long double matrix
v3.1.0,add sparse long-key double matrix
v3.1.0,add component long-key sparse double matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add sparse long-key float matrix
v3.1.0,add component long-key sparse float matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add sparse long-key int matrix
v3.1.0,add component long-key sparse int matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,add sparse long-key long matrix
v3.1.0,add component long-key sparse long matrix
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
v3.1.0,get a angel client
v3.1.0,add sparse float matrix
v3.1.0,MatrixContext siMat = new MatrixContext();
v3.1.0,siMat.setName(SPARSE_INT_MAT);
v3.1.0,siMat.setRowType(RowType.T_ANY_INTKEY_SPARSE);
v3.1.0,siMat.setRowNum(1);
v3.1.0,siMat.setValidIndexNum(100);
v3.1.0,siMat.setColNum(10000000000L);
v3.1.0,siMat.setValueType(Node.class);
v3.1.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
v3.1.0,siMat.setPartitionClass(CSRPartition.class);
v3.1.0,angelClient.addMatrix(siMat);
v3.1.0,add sparse long-key double matrix
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,client1.clock().get();
v3.1.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
v3.1.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
v3.1.0,@RunWith(MockitoJUnitRunner.class)
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
v3.1.0,get a angel client
v3.1.0,add matrix
v3.1.0,psAgent.initAndStart();
v3.1.0,test conf
v3.1.0,test master location
v3.1.0,test app id
v3.1.0,test user
v3.1.0,test ps agent attempt id
v3.1.0,test connection
v3.1.0,test master client
v3.1.0,test ip
v3.1.0,test loc
v3.1.0,test master location
v3.1.0,test ps location
v3.1.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
v3.1.0,test all ps ids
v3.1.0,test all matrix ids
v3.1.0,test all matrix names
v3.1.0,test matrix attribute
v3.1.0,test matrix meta
v3.1.0,test ps location
v3.1.0,test partitions
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,System.out.println(content);
v3.1.0,https://blog.csdn.net/cocoonyang/article/details/63068108
v3.1.0,v1[i] = v1[i] + da * v2[i];
v3.1.0,"dgemm(String transa, String transb,"
v3.1.0,"int m, int n, int k,"
v3.1.0,"double alpha,"
v3.1.0,"double[] a, int lda,"
v3.1.0,"double[] b, int ldb,"
v3.1.0,"double beta,"
v3.1.0,"double[] c, int ldc);"
v3.1.0,C := alpha*op( A )*op( B ) + beta*C
v3.1.0,v1[i] = v1[i] + da * v2[i];
v3.1.0,y := alpha*A*x + beta*y
v3.1.0,y := alpha*A*x + beta*y
v3.1.0,y := alpha*A*x + beta*y
v3.1.0,"dgemm(String transa, String transb,"
v3.1.0,"int m, int n, int k,"
v3.1.0,"double alpha,"
v3.1.0,"double[] a, int lda,"
v3.1.0,"double[] b, int ldb,"
v3.1.0,"double beta,"
v3.1.0,"double[] c, int ldc);"
v3.1.0,C := alpha*op( A )*op( B ) + beta*C
v3.1.0,Default does nothing.
v3.1.0,The app injection is optional
v3.1.0,"renderText(""hello world"");"
v3.1.0,"user choose a workerGroupID from the workergroups page,"
v3.1.0,now we should change the AngelApp params and render the workergroup page;
v3.1.0,"static final String WORKER_ID = ""worker.id"";"
v3.1.0,"div(""#logo"")."
v3.1.0,"img(""/static/hadoop-st.png"")._()."
v3.1.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
v3.1.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
v3.1.0,JQueryUI.jsnotice(html);
v3.1.0,import org.apache.hadoop.conf.Configuration;
v3.1.0,import java.lang.reflect.Field;
v3.1.0,all the files in input set
v3.1.0,Shuffle the file
v3.1.0,Get the blocks for all files
v3.1.0,Adjust the maxSize to make the split more balanced
v3.1.0,Handle the splittable files
v3.1.0,Handle the unsplittable files
v3.1.0,Split the blocks
v3.1.0,"If the remaining size of the current block is smaller than the required size,"
v3.1.0,the remaining blocks are divided into the current split
v3.1.0,Update current split length and move to next block
v3.1.0,Clear the current block offset
v3.1.0,"Current split length is > maxSize, split the block and generate a new split"
v3.1.0,Clear blocks list for next split
v3.1.0,Clear the current split length
v3.1.0,"If splitBlocks is not empty, just genetate a split for it"
v3.1.0,get block locations from file system
v3.1.0,create an input split
v3.1.0,get block locations from file system
v3.1.0,create a list of all block and their locations
v3.1.0,"if the file is not splitable, just create the one block with"
v3.1.0,full file length
v3.1.0,each split can be a maximum of maxSize
v3.1.0,if remainder is between max and 2*max - then
v3.1.0,"instead of creating splits of size max, left-max we"
v3.1.0,create splits of size left/2 and left/2. This is
v3.1.0,a heuristic to avoid creating really really small
v3.1.0,splits.
v3.1.0,add this block to the block --> node locations map
v3.1.0,"For blocks that do not have host/rack information,"
v3.1.0,assign to default  rack.
v3.1.0,add this block to the rack --> block map
v3.1.0,Add this host to rackToNodes map
v3.1.0,add this block to the node --> block map
v3.1.0,"if the file system does not have any rack information, then"
v3.1.0,use dummy rack location.
v3.1.0,The topology paths have the host name included as the last
v3.1.0,component. Strip it.
v3.1.0,get tokens for all the required FileSystems..
v3.1.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
v3.1.0,job.getConfiguration());
v3.1.0,Whether we need to recursive look into the directory structure
v3.1.0,creates a MultiPathFilter with the hiddenFileFilter and the
v3.1.0,user provided one (if any).
v3.1.0,all the files in input set
v3.1.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
v3.1.0,process all nodes and create splits that are local to a node. Generate
v3.1.0,"one split per node iteration, and walk over nodes multiple times to"
v3.1.0,distribute the splits across nodes.
v3.1.0,Skip the node if it has previously been marked as completed.
v3.1.0,"for each block, copy it into validBlocks. Delete it from"
v3.1.0,blockToNodes so that the same block does not appear in
v3.1.0,two different splits.
v3.1.0,Remove all blocks which may already have been assigned to other
v3.1.0,splits.
v3.1.0,"if the accumulated split size exceeds the maximum, then"
v3.1.0,create this split.
v3.1.0,create an input split and add it to the splits array
v3.1.0,Remove entries from blocksInNode so that we don't walk these
v3.1.0,again.
v3.1.0,Done creating a single split for this node. Move on to the next
v3.1.0,node so that splits are distributed across nodes.
v3.1.0,This implies that the last few blocks (or all in case maxSize=0)
v3.1.0,were not part of a split. The node is complete.
v3.1.0,if there were any blocks left over and their combined size is
v3.1.0,"larger than minSplitNode, then combine them into one split."
v3.1.0,Otherwise add them back to the unprocessed pool. It is likely
v3.1.0,that they will be combined with other blocks from the
v3.1.0,same rack later on.
v3.1.0,This condition also kicks in when max split size is not set. All
v3.1.0,blocks on a node will be grouped together into a single split.
v3.1.0,haven't created any split on this machine. so its ok to add a
v3.1.0,smaller one for parallelism. Otherwise group it in the rack for
v3.1.0,balanced size create an input split and add it to the splits
v3.1.0,array
v3.1.0,Remove entries from blocksInNode so that we don't walk this again.
v3.1.0,The node is done. This was the last set of blocks for this node.
v3.1.0,Put the unplaced blocks back into the pool for later rack-allocation.
v3.1.0,Node is done. All blocks were fit into node-local splits.
v3.1.0,Check if node-local assignments are complete.
v3.1.0,All nodes have been walked over and marked as completed or all blocks
v3.1.0,have been assigned. The rest should be handled via rackLock assignment.
v3.1.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
v3.1.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
v3.1.0,"if blocks in a rack are below the specified minimum size, then keep them"
v3.1.0,"in 'overflow'. After the processing of all racks is complete, these"
v3.1.0,overflow blocks will be combined into splits.
v3.1.0,Process all racks over and over again until there is no more work to do.
v3.1.0,Create one split for this rack before moving over to the next rack.
v3.1.0,Come back to this rack after creating a single split for each of the
v3.1.0,remaining racks.
v3.1.0,"Process one rack location at a time, Combine all possible blocks that"
v3.1.0,reside on this rack as one split. (constrained by minimum and maximum
v3.1.0,split size).
v3.1.0,iterate over all racks
v3.1.0,"for each block, copy it into validBlocks. Delete it from"
v3.1.0,blockToNodes so that the same block does not appear in
v3.1.0,two different splits.
v3.1.0,"if the accumulated split size exceeds the maximum, then"
v3.1.0,create this split.
v3.1.0,create an input split and add it to the splits array
v3.1.0,"if we created a split, then just go to the next rack"
v3.1.0,"if there is a minimum size specified, then create a single split"
v3.1.0,"otherwise, store these blocks into overflow data structure"
v3.1.0,There were a few blocks in this rack that
v3.1.0,remained to be processed. Keep them in 'overflow' block list.
v3.1.0,These will be combined later.
v3.1.0,Process all overflow blocks
v3.1.0,"This might cause an exiting rack location to be re-added,"
v3.1.0,but it should be ok.
v3.1.0,"if the accumulated split size exceeds the maximum, then"
v3.1.0,create this split.
v3.1.0,create an input split and add it to the splits array
v3.1.0,"Process any remaining blocks, if any."
v3.1.0,create an input split
v3.1.0,add this split to the list that is returned
v3.1.0,long num = totLength / maxSize;
v3.1.0,all blocks for all the files in input set
v3.1.0,mapping from a rack name to the list of blocks it has
v3.1.0,mapping from a block to the nodes on which it has replicas
v3.1.0,mapping from a node to the list of blocks that it contains
v3.1.0,populate all the blocks for all files
v3.1.0,stop all services
v3.1.0,1.write application state to file so that the client can get the state of the application
v3.1.0,if master exit
v3.1.0,2.clear tmp and staging directory
v3.1.0,waiting for client to get application state
v3.1.0,stop the RPC server
v3.1.0,"Security framework already loaded the tokens into current UGI, just use"
v3.1.0,them
v3.1.0,Now remove the AM->RM token so tasks don't have it
v3.1.0,add a shutdown hook
v3.1.0,init app state storage
v3.1.0,init event dispacher
v3.1.0,init location manager
v3.1.0,init container allocator
v3.1.0,init a rpc service
v3.1.0,recover matrix meta if needed
v3.1.0,recover ps attempt information if need
v3.1.0,Init Client manager
v3.1.0,Init PS Client manager
v3.1.0,init parameter server manager
v3.1.0,recover task information if needed
v3.1.0,a dummy data spliter is just for test now
v3.1.0,recover data splits information if needed
v3.1.0,init worker manager and register worker manager event
v3.1.0,register slow worker/ps checker
v3.1.0,register app manager event and finish event
v3.1.0,Init model saver & loader
v3.1.0,start a web service if use yarn deploy mode
v3.1.0,load from app state storage first if attempt index great than 1(the master is not the first
v3.1.0,retry)
v3.1.0,"if load failed, just build a new MatrixMetaManager"
v3.1.0,load ps attempt index from app state storage first if attempt index great than 1(the master
v3.1.0,is not the first retry)
v3.1.0,load task information from app state storage first if attempt index great than 1(the master
v3.1.0,is not the first retry)
v3.1.0,"if load failed, just build a new AMTaskManager"
v3.1.0,load data splits information from app state storage first if attempt index great than 1(the
v3.1.0,master is not the first retry)
v3.1.0,"if load failed, we need to recalculate the data splits"
v3.1.0,Check Workers
v3.1.0,Check PSS
v3.1.0,Check Clients
v3.1.0,Check PS Clients
v3.1.0,stop all services
v3.1.0,1.write application state to file so that the client can get the state of the application
v3.1.0,if master exit
v3.1.0,2.clear tmp and staging directory
v3.1.0,waiting for client to get application state
v3.1.0,stop the RPC server
v3.1.0,add a shutdown hook
v3.1.0,init app state storage
v3.1.0,init event dispacher
v3.1.0,init location manager
v3.1.0,init a rpc service
v3.1.0,recover matrix meta if needed
v3.1.0,recover ps attempt information if need
v3.1.0,Init Client manager
v3.1.0,Init PS Client manager
v3.1.0,init parameter server manager
v3.1.0,recover task information if needed
v3.1.0,a dummy data spliter is just for test now
v3.1.0,recover data splits information if needed
v3.1.0,init worker manager and register worker manager event
v3.1.0,register slow worker/ps checker
v3.1.0,register app manager event and finish event
v3.1.0,Init model saver & loader
v3.1.0,k8sClusterManager = new KubernetesClusterManager(appContext);
v3.1.0,load from app state storage first if attempt index great than 1(the master is not the first
v3.1.0,retry)
v3.1.0,"if load failed, just build a new MatrixMetaManager"
v3.1.0,load ps attempt index from app state storage first if attempt index great than 1(the master
v3.1.0,is not the first retry)
v3.1.0,load task information from app state storage first if attempt index great than 1(the master
v3.1.0,is not the first retry)
v3.1.0,"if load failed, just build a new AMTaskManager"
v3.1.0,load data splits information from app state storage first if attempt index great than 1(the
v3.1.0,master is not the first retry)
v3.1.0,"if load failed, we need to recalculate the data splits"
v3.1.0,parse parameter server counters
v3.1.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v3.1.0,refresh last heartbeat timestamp
v3.1.0,send a state update event to the specific PSAttempt
v3.1.0,Check is there save request
v3.1.0,"LOG.info(""PS "" + psAttemptId + "" need save "" + subSaveContext);"
v3.1.0,Check is there load request
v3.1.0,"LOG.info(""PS "" + psAttemptId + "" need load "" + subLoadContext);"
v3.1.0,check matrix metadata inconsistencies between master and parameter server.
v3.1.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
v3.1.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
v3.1.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v3.1.0,choose a unused port
v3.1.0,start RPC server
v3.1.0,remove this parameter server attempt from monitor set
v3.1.0,remove this parameter server attempt from monitor set
v3.1.0,"if worker attempt id is not in monitor set, we should shutdown it"
v3.1.0,find workergroup in worker manager
v3.1.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
v3.1.0,"if this worker group run over, just return WORKERGROUP_EXITED"
v3.1.0,"if this worker group is running now, return tasks, workers, data splits for it"
v3.1.0,"if worker attempt id is not in monitor set, we should shutdown it"
v3.1.0,"if worker attempt id is not in monitor set, we should shutdown it"
v3.1.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v3.1.0,"in ANGEL_PS mode, task id may can not know advance"
v3.1.0,update the clock for this matrix
v3.1.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v3.1.0,"in ANGEL_PS mode, task id may can not know advance"
v3.1.0,update task iteration
v3.1.0,"LOG.info(""error happened in psAttempt "" + psAttemptId + "" error msg="" + request.getMsg());"
v3.1.0,remove this parameter server attempt from monitor set
v3.1.0,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
v3.1.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
v3.1.0,the number of splits equal to the number of tasks
v3.1.0,split data
v3.1.0,dispatch the splits to workergroups
v3.1.0,split data
v3.1.0,dispatch the splits to workergroups
v3.1.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v3.1.0,"first, then divided by expected split number"
v3.1.0,get input format class from configuration and then instantiation a input format object
v3.1.0,split data
v3.1.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v3.1.0,"first, then divided by expected split number"
v3.1.0,get input format class from configuration and then instantiation a input format object
v3.1.0,split data
v3.1.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v3.1.0,need to fine tune the number of workergroup and task based on the actual split number
v3.1.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v3.1.0,Record the location information for the splits in order to data localized schedule
v3.1.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v3.1.0,need to fine tune the number of workergroup and task based on the actual split number
v3.1.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v3.1.0,Record the location information for the splits in order to data localized schedule
v3.1.0,write meta data to a temporary file
v3.1.0,rename the temporary file to final file
v3.1.0,"if the file exists, read from file and deserialize it"
v3.1.0,write task meta
v3.1.0,write ps meta
v3.1.0,generate a temporary file
v3.1.0,write task meta to the temporary file first
v3.1.0,rename the temporary file to the final file
v3.1.0,"if last final task file exist, remove it"
v3.1.0,find task meta file which has max timestamp
v3.1.0,"if the file does not exist, just return null"
v3.1.0,read task meta from file and deserialize it
v3.1.0,generate a temporary file
v3.1.0,write ps meta to the temporary file first.
v3.1.0,rename the temporary file to the final file
v3.1.0,"if the old final file exist, just remove it"
v3.1.0,find ps meta file
v3.1.0,"if ps meta file does not exist, just return null"
v3.1.0,read ps meta from file and deserialize it
v3.1.0,"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),"
v3.1.0,String.valueOf(requestId));
v3.1.0,Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));
v3.1.0,saveContext.setTmpSavePath(tmpPath.toString());
v3.1.0,Filter old epoch trigger first
v3.1.0,Split the user request to sub-requests to pss
v3.1.0,Init matrix files meta
v3.1.0,Move output files
v3.1.0,Write the meta file
v3.1.0,Split the user request to sub-requests to pss
v3.1.0,check whether psagent heartbeat timeout
v3.1.0,Set up the launch command
v3.1.0,Duplicate the ByteBuffers for access by multiple containers.
v3.1.0,Construct the actual Container
v3.1.0,Application resources
v3.1.0,Application environment
v3.1.0,Service data
v3.1.0,Tokens
v3.1.0,Set up JobConf to be localized properly on the remote NM.
v3.1.0,Setup DistributedCache
v3.1.0,Setup up task credentials buffer
v3.1.0,LocalStorageToken is needed irrespective of whether security is enabled
v3.1.0,or not.
v3.1.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
v3.1.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v3.1.0,Construct the actual Container
v3.1.0,The null fields are per-container and will be constructed for each
v3.1.0,container separately.
v3.1.0,Set up the launch command
v3.1.0,Duplicate the ByteBuffers for access by multiple containers.
v3.1.0,Construct the actual Container
v3.1.0,"a * in the classpath will only find a .jar, so we need to filter out"
v3.1.0,all .jars and add everything else
v3.1.0,Propagate the system classpath when using the mini cluster
v3.1.0,Add standard Hadoop classes
v3.1.0,Add mr
v3.1.0,Cache archives
v3.1.0,Cache files
v3.1.0,Sanity check
v3.1.0,Add URI fragment or just the filename
v3.1.0,Add the env variables passed by the user
v3.1.0,Set logging level in the environment.
v3.1.0,Old parameter name
v3.1.0,Parallel GC parameters
v3.1.0,G1 params
v3.1.0,Parallel Scavenge + Parallel Old
v3.1.0,G1
v3.1.0,".append("" -XX:G1NewSizePercent="").append(minNewRatio)"
v3.1.0,".append("" -XX:G1MaxNewSizePercent="").append(maxNewRatio)"
v3.1.0,CMS
v3.1.0,Setup the log4j prop
v3.1.0,Add main class and its arguments
v3.1.0,Finally add the jvmID
v3.1.0,vargs.add(String.valueOf(jvmID.getId()));
v3.1.0,Final commmand
v3.1.0,G1 params
v3.1.0,Add the env variables passed by the user
v3.1.0,Set logging level in the environment.
v3.1.0,Setup the log4j prop
v3.1.0,Add main class and its arguments
v3.1.0,Final commmand
v3.1.0,"if amTask is not null, we should clone task state from it"
v3.1.0,"if all parameter server complete commit, master can commit now"
v3.1.0,restartPS(psLoc);
v3.1.0,check whether parameter server heartbeat timeout
v3.1.0,Transitions from the NEW state.
v3.1.0,Transitions from the UNASSIGNED state.
v3.1.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
v3.1.0,Transitions from the ASSIGNED state.
v3.1.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
v3.1.0,PA_CONTAINER_LAUNCHED event
v3.1.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v3.1.0,Transitions from the PSAttemptStateInternal.COMMITTING state
v3.1.0,Transitions from the PSAttemptStateInternal.KILLED state
v3.1.0,Transitions from the PSAttemptStateInternal.FAILED state
v3.1.0,create the topology tables
v3.1.0,reqeuest resource:send a resource request to the resource allocator
v3.1.0,"Once the resource is applied, build and send the launch request to the container launcher"
v3.1.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
v3.1.0,resource allocator
v3.1.0,set the launch time
v3.1.0,add the ps attempt to the heartbeat timeout monitoring list
v3.1.0,parse ps attempt location and put it to location manager
v3.1.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
v3.1.0,or failed
v3.1.0,remove ps attempt id from heartbeat timeout monitor list
v3.1.0,release container:send a release request to container launcher
v3.1.0,set the finish time only if launch time is set
v3.1.0,private long scheduledTime;
v3.1.0,Transitions from the NEW state.
v3.1.0,Transitions from the SCHEDULED state.
v3.1.0,Transitions from the RUNNING state.
v3.1.0,"another attempt launched,"
v3.1.0,Transitions from the SUCCEEDED state
v3.1.0,Transitions from the KILLED state
v3.1.0,Transitions from the FAILED state
v3.1.0,add diagnostic
v3.1.0,check whether the number of failed attempts is less than the maximum number of allowed
v3.1.0,Refresh ps location & matrix meta
v3.1.0,start a new attempt for this ps
v3.1.0,notify ps manager
v3.1.0,"getContext().getLocationManager().setPsLocation(id, null);"
v3.1.0,add diagnostic
v3.1.0,check whether the number of failed attempts is less than the maximum number of allowed
v3.1.0,start a new attempt for this ps
v3.1.0,notify ps manager
v3.1.0,notify the event handler of state change
v3.1.0,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
v3.1.0,"if forcedState is set, just return"
v3.1.0,else get state from state machine
v3.1.0,add this worker group to the success set
v3.1.0,check if all worker group run or run over
v3.1.0,add this worker group to the success set
v3.1.0,check if all worker group run over
v3.1.0,add this worker group to the failed set
v3.1.0,check if too many worker groups are failed or killed
v3.1.0,notify a run failed event
v3.1.0,add this worker group to the failed set
v3.1.0,check if too many worker groups are failed or killed
v3.1.0,notify a run failed event
v3.1.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
v3.1.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
v3.1.0,just return the total task number now
v3.1.0,TODO
v3.1.0,check whether worker heartbeat timeout
v3.1.0,"if workerAttempt is not null, we should clone task state from it"
v3.1.0,from NEW state
v3.1.0,from SCHEDULED state
v3.1.0,get data splits location for data locality
v3.1.0,reqeuest resource:send a resource request to the resource allocator
v3.1.0,"once the resource is applied, build and send the launch request to the container launcher"
v3.1.0,notify failed message to the worker
v3.1.0,notify killed message to the worker
v3.1.0,release the allocated container
v3.1.0,notify failed message to the worker
v3.1.0,remove the worker attempt from heartbeat timeout listen list
v3.1.0,release the allocated container
v3.1.0,notify killed message to the worker
v3.1.0,remove the worker attempt from heartbeat timeout listen list
v3.1.0,clean the container
v3.1.0,notify failed message to the worker
v3.1.0,remove the worker attempt from heartbeat timeout listen list
v3.1.0,record the finish time
v3.1.0,clean the container
v3.1.0,notify killed message to the worker
v3.1.0,remove the worker attempt from heartbeat timeout listening list
v3.1.0,record the finish time
v3.1.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
v3.1.0,set worker attempt location
v3.1.0,notify the register message to the worker
v3.1.0,record the launch time
v3.1.0,update worker attempt metrics
v3.1.0,update tasks metrics
v3.1.0,clean the container
v3.1.0,notify the worker attempt run successfully message to the worker
v3.1.0,record the finish time
v3.1.0,todo
v3.1.0,init a worker attempt for the worker
v3.1.0,schedule the worker attempt
v3.1.0,add diagnostic
v3.1.0,check whether the number of failed attempts is less than the maximum number of allowed
v3.1.0,init and start a new attempt for this ps
v3.1.0,notify worker manager
v3.1.0,add diagnostic
v3.1.0,check whether the number of failed attempts is less than the maximum number of allowed
v3.1.0,init and start a new attempt for this ps
v3.1.0,notify worker manager
v3.1.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
v3.1.0,register to Yarn RM
v3.1.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
v3.1.0,"catch YarnException or YarnRuntimeException, we should exit and need not retry"
v3.1.0,build heartbeat request
v3.1.0,send heartbeat request to rm
v3.1.0,"This can happen if the RM has been restarted. If it is in that state,"
v3.1.0,this application must clean itself up.
v3.1.0,Setting NMTokens
v3.1.0,assgin containers
v3.1.0,"if some container is not assigned, release them"
v3.1.0,handle finish containers
v3.1.0,dispatch container exit message to corresponding components
v3.1.0,killed by framework
v3.1.0,killed by framework
v3.1.0,get application finish state
v3.1.0,build application diagnostics
v3.1.0,TODO:add a job history for angel
v3.1.0,build unregister request
v3.1.0,send unregister request to rm
v3.1.0,Note this down for next interaction with ResourceManager
v3.1.0,based on blacklisting comments above we can end up decrementing more
v3.1.0,than requested. so guard for that.
v3.1.0,send the updated resource request to RM
v3.1.0,send 0 container count requests also to cancel previous requests
v3.1.0,Update resource requests
v3.1.0,try to assign to all nodes first to match node local
v3.1.0,try to match all rack local
v3.1.0,assign remaining
v3.1.0,Update resource requests
v3.1.0,send the container-assigned event to task attempt
v3.1.0,build the start container request use launch context
v3.1.0,send the start request to Yarn nm
v3.1.0,send the message that the container starts successfully to the corresponding component
v3.1.0,"after launching, send launched event to task attempt to move"
v3.1.0,it from ASSIGNED to RUNNING state
v3.1.0,send the message that the container starts failed to the corresponding component
v3.1.0,kill the remote container if already launched
v3.1.0,start a thread pool to startup the container
v3.1.0,See if we need up the pool size only if haven't reached the
v3.1.0,maximum limit yet.
v3.1.0,nodes where containers will run at *this* point of time. This is
v3.1.0,*not* the cluster size and doesn't need to be.
v3.1.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
v3.1.0,later is just a buffer so we are not always increasing the
v3.1.0,pool-size
v3.1.0,the events from the queue are handled in parallel
v3.1.0,using a thread pool
v3.1.0,return if already stopped
v3.1.0,shutdown any containers that might be left running
v3.1.0,Add one sync matrix
v3.1.0,addSyncMatrix();
v3.1.0,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v3.1.0,"matrixContext.set(MatrixConf.MATRIX_LOAD_PATH, """");"
v3.1.0,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
v3.1.0,"LOG.info(""ps id = "" + psEntry.getKey());"
v3.1.0,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
v3.1.0,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
v3.1.0,"LOG.info(""matrix id = "" + metaEntry.getKey());"
v3.1.0,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
v3.1.0,}
v3.1.0,}
v3.1.0,get matrix ids in the parameter server report
v3.1.0,get the matrices parameter server need to create and delete
v3.1.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
v3.1.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
v3.1.0,Init control connection manager
v3.1.0,Get ps locations from master and put them to the location cache.
v3.1.0,Build and initialize rpc client to master
v3.1.0,Get psagent id
v3.1.0,Build PS control rpc client manager
v3.1.0,Build local location
v3.1.0,Initialize matrix meta information
v3.1.0,Start all services
v3.1.0,Stop all modules
v3.1.0,Stop all modules
v3.1.0,clock first
v3.1.0,wait
v3.1.0,Update generic resource counters
v3.1.0,Updating resources specified in ResourceCalculatorProcessTree
v3.1.0,Remove the CPU time consumed previously by JVM reuse
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,/ Plus a vector/matrix to the matrix stored in pss
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,/ Update a vector/matrix to the matrix stored in pss
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,/ Get values from pss use row/column indices
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"/ PSF get/update, use can implement their own psf"
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,/ Get a row or a batch of rows
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,Just return
v3.1.0,Just return
v3.1.0,Just return
v3.1.0,Just return
v3.1.0,Return a empty vector
v3.1.0,Return a empty vector
v3.1.0,Return a empty vector
v3.1.0,Return a empty vector
v3.1.0,Return a empty vector
v3.1.0,Return a empty vector
v3.1.0,Return a empty vector
v3.1.0,Return a empty vector
v3.1.0,"checkNotNull(func, ""func"");"
v3.1.0,Return a empty vector
v3.1.0,"checkNotNull(func, ""func"");"
v3.1.0,Return a empty vector
v3.1.0,"checkNotNull(func, ""func"");"
v3.1.0,Return a empty vector
v3.1.0,"checkNotNull(func, ""func"");"
v3.1.0,Return a empty vector
v3.1.0,"checkNotNull(func, ""func"");"
v3.1.0,Return a empty vector
v3.1.0,"checkNotNull(func, ""func"");"
v3.1.0,Return a empty vector
v3.1.0,"checkNotNull(func, ""func"");"
v3.1.0,Return a empty vector
v3.1.0,"checkNotNull(func, ""func"");"
v3.1.0,Return a empty vector
v3.1.0,Sort the partitions by start column index
v3.1.0,Generate a flush request and put it to request queue
v3.1.0,Generate a clock request and put it to request queue
v3.1.0,Generate a merge request and put it to request queue
v3.1.0,Generate a merge request and put it to request queue
v3.1.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
v3.1.0,matrix
v3.1.0,and add it to cache maps
v3.1.0,Add the message to the tree map
v3.1.0,"If there are flush / clock requests blocked, we need to put this merge request into"
v3.1.0,the waiting queue
v3.1.0,Launch a merge worker to merge the update to matrix op log cache
v3.1.0,Remove the message from the tree map
v3.1.0,Wake up blocked flush/clock request
v3.1.0,Add flush/clock request to listener list to waiting for all the existing
v3.1.0,updates are merged
v3.1.0,Wake up blocked flush/clock request
v3.1.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
v3.1.0,blocked.
v3.1.0,Get next merge message sequence id
v3.1.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
v3.1.0,position
v3.1.0,Wake up blocked merge requests
v3.1.0,Get minimal sequence id from listeners
v3.1.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
v3.1.0,should flush updates to local matrix storage
v3.1.0,Doing average or not
v3.1.0,Filter un-important update
v3.1.0,Split this row according the matrix partitions
v3.1.0,Set split context
v3.1.0,Remove the row from matrix
v3.1.0,buf.writeDouble(0.0);
v3.1.0,TODO:
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
v3.1.0,"LOG.error(""put response message queue failed "", e);"
v3.1.0,Use Epoll for linux
v3.1.0,Update location table
v3.1.0,Remove the server from failed list
v3.1.0,Notify refresh success message to request dispatcher
v3.1.0,Check PS exist or not
v3.1.0,Check heartbeat timeout
v3.1.0,getPSState(entry.getKey());
v3.1.0,Check PS restart or not
v3.1.0,private final HashSet<ParameterServerId> refreshingServerSet;
v3.1.0,Add it to failed rpc list
v3.1.0,Add the server to gray server list
v3.1.0,Add it to failed rpc list
v3.1.0,Add the server to gray server list
v3.1.0,Move from gray server list to failed server list
v3.1.0,Handle the RPCS to this server
v3.1.0,Submit the schedulable failed get RPCS
v3.1.0,Submit new get RPCS
v3.1.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v3.1.0,"If the queue is empty, just return 0"
v3.1.0,"If request is not over limit, just submit it"
v3.1.0,Submit the schedulable failed get RPCS
v3.1.0,Submit new put RPCS
v3.1.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v3.1.0,"LOG.info(""choose put server "" + psIds[index]);"
v3.1.0,Check all pending RPCS
v3.1.0,Check get channel context
v3.1.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
v3.1.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
v3.1.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
v3.1.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
v3.1.0,channelManager.printPools();
v3.1.0,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
v3.1.0,if(ts - entry.getValue() > requestTimeOut * 2)  {
v3.1.0,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
v3.1.0,"+ "" milliseconds, close all channels to it"");"
v3.1.0,closeChannels(entry.getKey());
v3.1.0,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
v3.1.0,}
v3.1.0,}
v3.1.0,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
v3.1.0,Remove all pending RPCS
v3.1.0,Close all channel to this PS
v3.1.0,private Channel getChannel(Location loc) throws Exception {
v3.1.0,"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));"
v3.1.0,}
v3.1.0,private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {
v3.1.0,"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext"
v3.1.0,.get()
v3.1.0,.getConf()
v3.1.0,".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,"
v3.1.0,AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));
v3.1.0,}
v3.1.0,Get server id and location for this request
v3.1.0,"If location is null, means that the server is not ready"
v3.1.0,Get the channel for the location
v3.1.0,Check if need get token first
v3.1.0,Serialize the request
v3.1.0,Send the request
v3.1.0,get a channel to server from pool
v3.1.0,"if channel is not valid, it means maybe the connections to the server are closed"
v3.1.0,request.getContext().setChannelPool(pool);
v3.1.0,Allocate the bytebuf and serialize the request
v3.1.0,find the partition request context from cache
v3.1.0,"updateMatrixCache(request.getPartKey(), response.getPartition());"
v3.1.0,"updateMatrixCache(request.getPartKey(), response.getRowsSplit());"
v3.1.0,TODO
v3.1.0,ServerRow rowSplit = PSAgentContext.get().getMatricesCache()
v3.1.0,".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),"
v3.1.0,request.getRowIndex());
v3.1.0,response.setRowSplit(rowSplit);
v3.1.0,"updateMatrixCache(request.getPartKey(), response.getRowSplit());"
v3.1.0,"LOG.info(""user request id "" + request.getUserRequestId());"
v3.1.0,"LOG.info(""user request id "" + request.getUserRequestId());"
v3.1.0,TODO
v3.1.0,"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {"
v3.1.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);"
v3.1.0,}
v3.1.0,"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {"
v3.1.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);"
v3.1.0,}
v3.1.0,"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {"
v3.1.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);"
v3.1.0,}
v3.1.0,Get partitions for this row
v3.1.0,Distinct get row requests
v3.1.0,Get row splits of this row from the matrix cache first
v3.1.0,responseCache.addSubResponse(rowSplit);
v3.1.0,"If the row split does not exist in cache, get it from parameter server"
v3.1.0,Split the param use matrix partitions
v3.1.0,Send request to PSS
v3.1.0,Split the matrix oplog according to the matrix partitions
v3.1.0,"If need update clock, we should send requests to all partitions"
v3.1.0,Send request to PSS
v3.1.0,Filter the rowIds which are fetching now
v3.1.0,Send the rowIndex to rpc dispatcher and return immediately
v3.1.0,"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));"
v3.1.0,"LOG.info(""start to request "" + requestId);"
v3.1.0,"LOG.info(""start to request "" + requestId);"
v3.1.0,Split param use matrix partitons
v3.1.0,"If all sub-results are received, just remove request and result cache"
v3.1.0,Split this row according the matrix partitions
v3.1.0,Set split context
v3.1.0,Split this row according the matrix partitions
v3.1.0,Set split context
v3.1.0,long startTs = System.currentTimeMillis();
v3.1.0,"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));"
v3.1.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
v3.1.0,Generate dispatch items and add them to the corresponding queues
v3.1.0,Filter the rowIds which are fetching now
v3.1.0,Sort the parts by partitionId
v3.1.0,Sort partition keys use start column index
v3.1.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
v3.1.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
v3.1.0,});
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,Sort the parts by partitionId
v3.1.0,Sort partition keys use start column index
v3.1.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
v3.1.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
v3.1.0,});
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,Put the row split to the cache(row index to row splits map)
v3.1.0,"If all splits of the row are received, means this row can be merged"
v3.1.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
v3.1.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
v3.1.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
v3.1.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
v3.1.0,TODO
v3.1.0,TODO
v3.1.0,/////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,TODO
v3.1.0,buf.writeDouble(0);
v3.1.0,TODO
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,Now we just support pipelined row splits merging for dense type row
v3.1.0,Pre-fetching is disable default
v3.1.0,matrix id to clock map
v3.1.0,"task index, it must be unique for whole application"
v3.1.0,Deserialize data splits meta
v3.1.0,Get workers
v3.1.0,Send request to every ps
v3.1.0,Wait the responses
v3.1.0,Update clock cache
v3.1.0,if(syncNum % 1024 == 0) {
v3.1.0,}
v3.1.0,"Use simple flow, do not use any cache"
v3.1.0,Get row from cache.
v3.1.0,"if row clock is satisfy ssp staleness limit, just return."
v3.1.0,Get row from ps.
v3.1.0,Wait until the clock value of this row is greater than or equal to the value
v3.1.0,"For ASYNC mode, just get from pss."
v3.1.0,"For BSP/SSP, get rows from storage/cache first"
v3.1.0,Get from ps.
v3.1.0,Wait until the clock value of this row is greater than or equal to the value
v3.1.0,"For ASYNC, just get rows from pss."
v3.1.0,no more retries.
v3.1.0,calculate sleep time and return.
v3.1.0,parse the i-th sleep-time
v3.1.0,parse the i-th number-of-retries
v3.1.0,calculateSleepTime may overflow.
v3.1.0,"A few common retry policies, with no delays."
v3.1.0,Read matrix meta from meta file
v3.1.0,Save partitions to files use fork-join
v3.1.0,Write the ps matrix meta to the meta file
v3.1.0,matrix.startServering();
v3.1.0,return;
v3.1.0,Read matrix meta from meta file
v3.1.0,Load partitions from file use fork-join
v3.1.0,Read matrix meta from meta file
v3.1.0,Sort partitions
v3.1.0,TODO:
v3.1.0,int size = rows.length;
v3.1.0,int size = rows.length;
v3.1.0,int size = rows.size();
v3.1.0,int size = rows.size();
v3.1.0,int size = rows.size();
v3.1.0,int size = rows.size();
v3.1.0,int size = rows.size();
v3.1.0,int size = rows.size();
v3.1.0,close is a local operation and should finish within milliseconds; timeout just to be safe
v3.1.0,response will be null for one way messages.
v3.1.0,maxFrameLength = 2G
v3.1.0,lengthFieldOffset = 0
v3.1.0,lengthFieldLength = 8
v3.1.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
v3.1.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
v3.1.0,"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())"
v3.1.0,.toString();
v3.1.0,indicates whether this connection's life cycle is managed
v3.1.0,See if we already have a connection (common case)
v3.1.0,create a unique lock for this RS + protocol (if necessary)
v3.1.0,get the RS lock
v3.1.0,do one more lookup in case we were stalled above
v3.1.0,Only create isa when we need to.
v3.1.0,definitely a cache miss. establish an RPC for
v3.1.0,this RS
v3.1.0,Throw what the RemoteException was carrying.
v3.1.0,check
v3.1.0,every
v3.1.0,minutes
v3.1.0,TODO
v3.1.0,创建failoverHandler
v3.1.0,"The number of times this invocation handler has ever been failed over,"
v3.1.0,before this method invocation attempt. Used to prevent concurrent
v3.1.0,failed method invocations from triggering multiple failover attempts.
v3.1.0,Make sure that concurrent failed method invocations
v3.1.0,only cause a
v3.1.0,single actual fail over.
v3.1.0,RpcController + Message in the method args
v3.1.0,(generated code from RPC bits in .proto files have
v3.1.0,RpcController)
v3.1.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
v3.1.0,+ (System.currentTimeMillis() - beforeConstructTs));
v3.1.0,get an instance of the method arg type
v3.1.0,RpcController + Message in the method args
v3.1.0,(generated code from RPC bits in .proto files have
v3.1.0,RpcController)
v3.1.0,Message (hand written code usually has only a single
v3.1.0,argument)
v3.1.0,log any RPC responses that are slower than the configured
v3.1.0,warn
v3.1.0,response time or larger than configured warning size
v3.1.0,"when tagging, we let TooLarge trump TooSmall to keep"
v3.1.0,output simple
v3.1.0,note that large responses will often also be slow.
v3.1.0,provides a count of log-reported slow responses
v3.1.0,RpcController + Message in the method args
v3.1.0,(generated code from RPC bits in .proto files have
v3.1.0,RpcController)
v3.1.0,unexpected
v3.1.0,"in the protobuf methods, args[1] is the only significant argument"
v3.1.0,for JSON encoding
v3.1.0,base information that is reported regardless of type of call
v3.1.0,Disable Nagle's Algorithm since we don't want packets to wait
v3.1.0,Configure the event pipeline factory.
v3.1.0,Make a new connection.
v3.1.0,Remove all pending requests (will be canceled after relinquishing
v3.1.0,write lock).
v3.1.0,Cancel any pending requests by sending errors to the callbacks:
v3.1.0,Close the channel:
v3.1.0,Close the connection:
v3.1.0,Shut down all thread pools to exit.
v3.1.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
v3.1.0,See NettyServer.prepareResponse for where we write out the response.
v3.1.0,"It writes the call.id (int), a boolean signifying any error (and if"
v3.1.0,"so the exception name/trace), and the response bytes"
v3.1.0,Read the call id.
v3.1.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
v3.1.0,"instead, it returns a null message object."
v3.1.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
v3.1.0,System.currentTimeMillis());
v3.1.0,"It would be good widen this to just Throwable, but IOException is what we"
v3.1.0,allow now
v3.1.0,not implemented
v3.1.0,not implemented
v3.1.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
v3.1.0,cache of RpcEngines by protocol
v3.1.0,return the RpcEngine configured to handle a protocol
v3.1.0,We only handle the ConnectException.
v3.1.0,This is the exception we can't handle.
v3.1.0,check if timed out
v3.1.0,wait for retry
v3.1.0,IGNORE
v3.1.0,return the RpcEngine that handles a proxy object
v3.1.0,The default implementation works synchronously
v3.1.0,punt: allocate a new buffer & copy into it
v3.1.0,Parse cmd parameters
v3.1.0,load hadoop configuration
v3.1.0,load angel system configuration
v3.1.0,load user configuration:
v3.1.0,load user config file
v3.1.0,load command line parameters
v3.1.0,load user job resource files
v3.1.0,load ml conf file for graph based algorithm
v3.1.0,load user job jar if it exist
v3.1.0,Expand the environment variable
v3.1.0,Add default fs(local fs) for lib jars.
v3.1.0,"LOG.info(System.getProperty(""user.dir""));"
v3.1.0,get tokens for all the required FileSystems..
v3.1.0,Whether we need to recursive look into the directory structure
v3.1.0,creates a MultiPathFilter with the hiddenFileFilter and the
v3.1.0,user provided one (if any).
v3.1.0,"LOG.info(""Total input paths to process : "" + result.size());"
v3.1.0,get tokens for all the required FileSystems..
v3.1.0,Whether we need to recursive look into the directory structure
v3.1.0,creates a MultiPathFilter with the hiddenFileFilter and the
v3.1.0,user provided one (if any).
v3.1.0,"LOG.info(""Total input paths to process : "" + result.size());"
v3.1.0,a simple hdfs copy function assume src path and dest path are in same hdfs
v3.1.0,and FileSystem object has same schema
v3.1.0,"If out path exist , just remove it first"
v3.1.0,Create parent directory if not exist
v3.1.0,Rename
v3.1.0,"LOG.warn(""interrupted while sleeping"", ie);"
v3.1.0,public static String getHostname() {
v3.1.0,try {
v3.1.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
v3.1.0,} catch (UnknownHostException uhe) {
v3.1.0,}
v3.1.0,"return new StringBuilder().append("""").append(uhe).toString();"
v3.1.0,}
v3.1.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
v3.1.0,String hostname = getHostname();
v3.1.0,String classname = clazz.getSimpleName();
v3.1.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
v3.1.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
v3.1.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
v3.1.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
v3.1.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
v3.1.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
v3.1.0,
v3.1.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
v3.1.0,public void run() {
v3.1.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
v3.1.0,"this.val$classname + "" at "" + this.val$hostname}));"
v3.1.0,}
v3.1.0,});
v3.1.0,}
v3.1.0,"We we interrupted because we're meant to stop? If not, just"
v3.1.0,continue ignoring the interruption
v3.1.0,Recalculate waitTime.
v3.1.0,// Begin delegation to Thread
v3.1.0,// End delegation to Thread
v3.1.0,instance submitter class
v3.1.0,Obtain filename from path
v3.1.0,Split filename to prexif and suffix (extension)
v3.1.0,Check if the filename is okay
v3.1.0,Prepare temporary file
v3.1.0,Prepare buffer for data copying
v3.1.0,Open and check input stream
v3.1.0,Open output stream and copy data between source file in JAR and the temporary file
v3.1.0,"If read/write fails, close streams safely before throwing an exception"
v3.1.0,"Finally, load the library"
v3.1.0,little endian load order
v3.1.0,tail
v3.1.0,fallthrough
v3.1.0,fallthrough
v3.1.0,finalization
v3.1.0,fmix(h1);
v3.1.0,----------
v3.1.0,body
v3.1.0,----------
v3.1.0,tail
v3.1.0,----------
v3.1.0,finalization
v3.1.0,----------
v3.1.0,body
v3.1.0,----------
v3.1.0,tail
v3.1.0,----------
v3.1.0,finalization
v3.1.0,throw new AngelException(e);
v3.1.0,JobStateProto jobState = report.getJobState();
v3.1.0,Check need load matrices
v3.1.0,Used for java code to get a AngelClient instance
v3.1.0,Used for python code to get a AngelClient instance
v3.1.0,load user job resource files
v3.1.0,setLocalAddr();
v3.1.0,2.get job id
v3.1.0,5.write configuration to a xml file
v3.1.0,8.get app master client
v3.1.0,Write job file to JobTracker's fs
v3.1.0,the leaf level file should be readable by others
v3.1.0,the subdirs in the path should have execute permissions for
v3.1.0,others
v3.1.0,2.get job id
v3.1.0,Credentials credentials = new Credentials();
v3.1.0,4.copy resource files to hdfs
v3.1.0,5.write configuration to a xml file
v3.1.0,6.create am container context
v3.1.0,7.Submit to ResourceManager
v3.1.0,8.get app master client
v3.1.0,Create a number of filenames in the JobTracker's fs namespace
v3.1.0,add all the command line files/ jars and archive
v3.1.0,first copy them to jobtrackers filesystem
v3.1.0,should not throw a uri exception
v3.1.0,should not throw an uri excpetion
v3.1.0,set the timestamps of the archives and files
v3.1.0,set the public/private visibility of the archives and files
v3.1.0,get DelegationToken for each cached file
v3.1.0,check if we do not need to copy the files
v3.1.0,is jt using the same file system.
v3.1.0,just checking for uri strings... doing no dns lookups
v3.1.0,to see if the filesystems are the same. This is not optimal.
v3.1.0,but avoids name resolution.
v3.1.0,this might have name collisions. copy will throw an exception
v3.1.0,parse the original path to create new path
v3.1.0,check for ports
v3.1.0,Write job file to JobTracker's fs
v3.1.0,Setup resource requirements
v3.1.0,Setup LocalResources
v3.1.0,Setup security tokens
v3.1.0,Setup the command to run the AM
v3.1.0,Add AM user command opts
v3.1.0,Final command
v3.1.0,Setup the CLASSPATH in environment
v3.1.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
v3.1.0,Setup the environment variables for Admin first
v3.1.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
v3.1.0,Parse distributed cache
v3.1.0,Setup ContainerLaunchContext for AM container
v3.1.0,Set up the ApplicationSubmissionContext
v3.1.0,private volatile PS2PSPusherImpl ps2PSPusher;
v3.1.0,TODO
v3.1.0,Add tokens to new user so that it may execute its task correctly.
v3.1.0,TODO
v3.1.0,to exit
v3.1.0,TODO
v3.1.0,TODO
v3.1.0,ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));
v3.1.0,context.getSnapshotManager().processRecovery();
v3.1.0,Recover PS from snapshot or load path
v3.1.0,1. First check old snapshot
v3.1.0,2. Check new checkpoints
v3.1.0,3. Check load path setting and old save result
v3.1.0,Just init it again
v3.1.0,TODO
v3.1.0,if(ps2PSPusher != null) {
v3.1.0,ps2PSPusher.start();
v3.1.0,}
v3.1.0,public PS2PSPusherImpl getPs2PSPusher() {
v3.1.0,return ps2PSPusher;
v3.1.0,}
v3.1.0,Filter the head
v3.1.0,Get the RPC destination
v3.1.0,Get and init the queue
v3.1.0,"If the queue is empty, activate the processor"
v3.1.0,Just put it to the rpc queue
v3.1.0,if(useInDepWorkers) {
v3.1.0,Use independent rpc workers
v3.1.0,if (method == TransportMethod.GET_CLOCKS || method == TransportMethod.UPDATE_CLOCK) {
v3.1.0,"Small rpc request, use sync to avoid thread switch"
v3.1.0,return false;
v3.1.0,}
v3.1.0,return true;
v3.1.0,} else {
v3.1.0,return false;
v3.1.0,}
v3.1.0,if (!useSync && useAyncHandler) {
v3.1.0,"senderPool.execute(new Sender(clientId, seqId, method, ctx, result));"
v3.1.0,} else {
v3.1.0,"send(clientId, seqId, method, ctx, result);"
v3.1.0,}
v3.1.0,Release the input buffer
v3.1.0,Release the input buffer
v3.1.0,"1. handle the rpc, get the response"
v3.1.0,Release the input buffer
v3.1.0,2. Serialize the response
v3.1.0,Send the serialized response
v3.1.0,Exception happened
v3.1.0,write seq id
v3.1.0,Just serialize the head
v3.1.0,Exception happened
v3.1.0,Allocate result buffer
v3.1.0,Exception happened
v3.1.0,Just serialize the head
v3.1.0,Exception happened
v3.1.0,runningContext.printToken();
v3.1.0,Reset the response and allocate buffer again
v3.1.0,Get partition and check the partition state
v3.1.0,Get the stored pss for this partition
v3.1.0,"Check this ps is the master ps for this location, only master ps can accept the update"
v3.1.0,Check the partition state again
v3.1.0,Start to put the update to the slave pss
v3.1.0,TODO
v3.1.0,"context.getPS2PSPusher().put(request, in, partLoc);"
v3.1.0,Get partition and check the partition state
v3.1.0,Get the stored pss for this partition
v3.1.0,"Check this ps is the master ps for this partition, if not, just return failed"
v3.1.0,Start to put the update to the slave pss
v3.1.0,TODO
v3.1.0,"int maxRPCCounter = Math.max(estSize, (int) (workerNum * factor));"
v3.1.0,"for (Map.Entry<Integer, ClientRunningContext> clientEntry : clientRPCCounters.entrySet()) {"
v3.1.0,"LOG.info(""client "" + clientEntry.getKey() + "" running context:"");"
v3.1.0,clientEntry.getValue().printToken();
v3.1.0,}
v3.1.0,return ServerState.GENERAL;
v3.1.0,Use Epoll for linux
v3.1.0,public String uuid;
v3.1.0,TODO:
v3.1.0,part = new ServerPartition();
v3.1.0,TODO:
v3.1.0,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
v3.1.0,this.channelPool = channelPool;
v3.1.0,}
v3.1.0,private final ParameterServer psServer;
v3.1.0,Create and start workers
v3.1.0,Set workers
v3.1.0,Create and start workers
v3.1.0,Set workers
v3.1.0,"If matrix checkpoint path not exist, just return null"
v3.1.0,Return the path with maximum checkpoint id
v3.1.0,Rename temp to item path
v3.1.0,Checkpoint base path = Base dir/matrix name
v3.1.0,Path for this checkpoint
v3.1.0,Generate tmp path
v3.1.0,Delete old checkpoints
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
v3.1.0,and call endWrite/endRead after
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
v3.1.0,and call endWrite/endRead after
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"////// network io method, for model transform"
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,Serailize the head
v3.1.0,Serialize the storage
v3.1.0,Deserailze the head
v3.1.0,Deseralize the storage
v3.1.0,Serailize the head
v3.1.0,Serialize the storage
v3.1.0,Deserailze the head
v3.1.0,Deseralize the storage
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
v3.1.0,and call endWrite/endRead after
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
v3.1.0,and call endWrite/endRead after
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
v3.1.0,and call endWrite/endRead after
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
v3.1.0,and call endWrite/endRead after
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
v3.1.0,and call endWrite/endRead after
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
v3.1.0,and call endWrite/endRead after
v3.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,"Notice: Basic type ServerRow only support Vector as inner storage now, so you can use methods"
v3.1.0,to get inner vector for basic type ServerRow.
v3.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
v3.1.0,Just update the exist element now!!
v3.1.0,Just update the exist element now!!
v3.1.0,TODO: just check the value is 0 or not now
v3.1.0,TODO: just check the value is zero or not now
v3.1.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
v3.1.0,Attention: Only update the exist values for sorted storage method
v3.1.0,Attention: Only update exist element
v3.1.0,Attention: Only update the exist values for sorted storage method
v3.1.0,Attention: Only update exist element
v3.1.0,TODO: just check the value is zero or not now
v3.1.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
v3.1.0,Valid element number
v3.1.0,Element data
v3.1.0,Valid element number
v3.1.0,Deserialize the data
v3.1.0,Element data
v3.1.0,Valid element number
v3.1.0,Element data
v3.1.0,Valid element number
v3.1.0,Deserialize the data
v3.1.0,Attention: Only update the exist values for sorted storage method
v3.1.0,Attention: Only update exist element
v3.1.0,TODO: just check the value is zero or not now
v3.1.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
v3.1.0,TODO: just check the value is 0 or not now
v3.1.0,Just update the exist element now!!
v3.1.0,Just update the exist element now!!
v3.1.0,"Use sparse storage method, as some elements in the array maybe null"
v3.1.0,Array length
v3.1.0,Valid element number
v3.1.0,Element data
v3.1.0,Array len
v3.1.0,Valid element number
v3.1.0,"Use sparse storage method, as some elements in the array maybe null"
v3.1.0,Array length
v3.1.0,Valid element number
v3.1.0,Element data
v3.1.0,Element data
v3.1.0,Array len
v3.1.0,Valid element number
v3.1.0,Attention: Only update the exist values for sorted storage method
v3.1.0,Attention: Only update exist element
v3.1.0,TODO: just check the value is zero or not now
v3.1.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
v3.1.0,Row type
v3.1.0,Storage method
v3.1.0,Key type
v3.1.0,Value type
v3.1.0,Vector dim
v3.1.0,Vector length
v3.1.0,Vector data
v3.1.0,Row type
v3.1.0,Storage method
v3.1.0,Key type
v3.1.0,Value type
v3.1.0,Vector dim
v3.1.0,Vector length
v3.1.0,Init the vector
v3.1.0,Vector data
v3.1.0,"Sparse storage, use the iterator to avoid array copy"
v3.1.0,Get the array pair
v3.1.0,"Sparse storage, use the iterator to avoid array copy"
v3.1.0,Get the array pair
v3.1.0,"Sparse storage, use the iterator to avoid array copy"
v3.1.0,Get the array pair
v3.1.0,"Sparse storage, use the iterator to avoid array copy"
v3.1.0,Get the array pair
v3.1.0,"Sparse storage, use the iterator to avoid array copy"
v3.1.0,Get the array pair
v3.1.0,Impossible now
v3.1.0,"Sparse storage, use the iterator to avoid array copy"
v3.1.0,Get the array pair
v3.1.0,Impossible now
v3.1.0,"Sparse storage, use the iterator to avoid array copy"
v3.1.0,Get the array pair
v3.1.0,Impossible now
v3.1.0,"Sparse storage, use the iterator to avoid array copy"
v3.1.0,Get the array pair
v3.1.0,Impossible now
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,"If use sorted storage, we should get the array pair first"
v3.1.0,Just update the exist element now!!
v3.1.0,Just update the exist element now!!
v3.1.0,TODO: just check the value is 0 or not now
v3.1.0,Just update the exist element now!!
v3.1.0,Just update the exist element now!!
v3.1.0,TODO: just check the value is 0 or not now
v3.1.0,Valid element number
v3.1.0,Element data
v3.1.0,Valid element number
v3.1.0,Deserialize the data
v3.1.0,Element data
v3.1.0,Valid element number
v3.1.0,Element data
v3.1.0,Valid element number
v3.1.0,Deserialize the data
v3.1.0,private final List<PartitionKey> partitionKeys;
v3.1.0,Get server partition class
v3.1.0,"If partition class is not set, just use the default partition class"
v3.1.0,Get server partition storage class type
v3.1.0,Get value class
v3.1.0,"if col == -1, we use the start/end index to calculate range,"
v3.1.0,we use double to store the range value since two long minus might exceed the
v3.1.0,range of long.
v3.1.0,Serialize the head
v3.1.0,Serialize the storage
v3.1.0,Deserialize the head
v3.1.0,Deseralize the storage
v3.1.0,Serialize the head
v3.1.0,Serialize the storage
v3.1.0,Deserialize the head
v3.1.0,Deseralize the storage
v3.1.0,Row base partition
v3.1.0,"If storage class is not set, use default DenseServerRowsStorage"
v3.1.0,Serialize values
v3.1.0,Deserialize values
v3.1.0,Array size
v3.1.0,Actual write size
v3.1.0,Rows data
v3.1.0,Row id
v3.1.0,Row type
v3.1.0,Row data
v3.1.0,Array size
v3.1.0,Actual write row number
v3.1.0,Rows data
v3.1.0,Row id
v3.1.0,Create empty server row
v3.1.0,Row data
v3.1.0,Rows data
v3.1.0,TODO
v3.1.0,Serialize row offsets
v3.1.0,Serialize column offsets
v3.1.0,Deserialize row offset
v3.1.0,Deserialize row offset
v3.1.0,"If storage is set, just get a instance"
v3.1.0,"If storage is not set, use default"
v3.1.0,"If storage is set, just get a instance"
v3.1.0,"If storage is not set, use default"
v3.1.0,Map size
v3.1.0,Actual write size
v3.1.0,Rows data
v3.1.0,Row id
v3.1.0,Row type
v3.1.0,Row data
v3.1.0,Array size
v3.1.0,Actual write row number
v3.1.0,Rows data
v3.1.0,Row id
v3.1.0,Create empty server row
v3.1.0,Row data
v3.1.0,Rows data
v3.1.0,Use Epoll for linux
v3.1.0,find the partition request context from cache
v3.1.0,get a channel to server from pool
v3.1.0,"if channel is not valid, it means maybe the connections to the server are closed"
v3.1.0,channelManager.removeChannelPool(loc);
v3.1.0,Generate seq id
v3.1.0,Create a RecoverPartRequest
v3.1.0,Serialize the request
v3.1.0,Change the seqId for the request
v3.1.0,Serialize the request
v3.1.0,"First check the state of the channels in the pool, if a channel is unused, just return"
v3.1.0,"If all channels are in use, create a new channel or wait"
v3.1.0,Create a new channel
v3.1.0,"add the PSAgentContext,need fix"
v3.1.0,If col == -1 and start/end not set
v3.1.0,start/end set
v3.1.0,"for dense type, we need to set the colNum to set dim for vectors"
v3.1.0,"colNum set, start/end not set"
v3.1.0,Row number must > 0
v3.1.0,"both set, check its valid"
v3.1.0,public static final int T_INT_ARBITRARY_VALUE = 28;
v3.1.0,public static final int T_INVALID_VALUE = 29;
v3.1.0,TODO:add more vector type
v3.1.0,TODO : subDim set
v3.1.0,Sort the parts by partitionId
v3.1.0,Sort partition keys use start column index
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,Sort the parts by partitionId
v3.1.0,Sort partition keys use start column index
v3.1.0,"For each partition, we generate a update split."
v3.1.0,"Although the split is empty for partitions those without any update data,"
v3.1.0,we still need to generate a update split to update the clock info on ps.
v3.1.0,Split updates
v3.1.0,Shuffle update splits
v3.1.0,Generate part update parameters
v3.1.0,"Set split context: partition key, use int key for long key vector or not ect"
v3.1.0,write the max abs
v3.1.0,---------------------------------------------------
v3.1.0,---------------------------------------------------
v3.1.0,---------------------------------------------------------------
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,axis = 0: on rows
v3.1.0,axis = 1: on cols
v3.1.0,1. find the insert point
v3.1.0,2. check the capacity and insert
v3.1.0,3. increase size
v3.1.0,1. find the insert point
v3.1.0,2. check the capacity and insert
v3.1.0,3. increase size
v3.1.0,1. find the insert point
v3.1.0,2. check the capacity and insert
v3.1.0,3. increase size
v3.1.0,1. find the insert point
v3.1.0,2. check the capacity and insert
v3.1.0,3. increase size
v3.1.0,1. find the insert point
v3.1.0,2. check the capacity and insert
v3.1.0,3. increase size
v3.1.0,1. find the insert point
v3.1.0,2. check the capacity and insert
v3.1.0,3. increase size
v3.1.0,1. find the insert point
v3.1.0,2. check the capacity and insert
v3.1.0,3. increase size
v3.1.0,1. find the insert point
v3.1.0,2. check the capacity and insert
v3.1.0,3. increase size
v3.1.0,-----------------
v3.1.0,-----------------
v3.1.0,-----------------
v3.1.0,-----------------
v3.1.0,-----------------
v3.1.0,KeepStorage is guaranteed
v3.1.0,"ignore the isInplace option, since v2 is dense"
v3.1.0,"the value in old storage can be changed safe, so switch a storage"
v3.1.0,"but user required keep storage, we can prevent rehash"
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,KeepStorage is guaranteed
v3.1.0,we gauss dense storage is more efficient
v3.1.0,v1Size < v2Size * Constant.sparseThreshold
v3.1.0,KeepStorage is guaranteed
v3.1.0,"ignore the isInplace option, since v2 is dense"
v3.1.0,"the value in old storage can be changed safe, so switch a storage"
v3.1.0,"but user required keep storage, we can prevent rehash"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,prevent rehash
v3.1.0,KeepStorage is guaranteed
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,dense preferred
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,sorted preferred
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,multi-rehash
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"dense preferred, KeepStorage is guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,"sparse preferred, keep storage guaranteed"
v3.1.0,preferred dense
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,we gauss dense storage is more efficient
v3.1.0,to avoid multi-rehash
v3.1.0,"no rehashor one onle rehash is required, nothing to optimization"
v3.1.0,multi-rehash
v3.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,to avoid multi-rehash
v3.1.0,"Transform mat1, generate a new matrix"
v3.1.0,Split the row indices of mat1Trans
v3.1.0,Parallel execute use fork-join
v3.1.0,"Get the sub-matrix of left matrix, split by row"
v3.1.0,"Transform mat1, generate a new matrix"
v3.1.0,Split the row indices of mat1Trans
v3.1.0,Parallel execute use fork-join
v3.1.0,"Get the sub-matrix of left matrix, split by row"
v3.1.0,"mat1 trans true, mat trans true"
v3.1.0,"mat1 trans true, mat trans false"
v3.1.0,"mat1 trans false, mat trans true, important"
v3.1.0,"mat1 trans false, mat trans false"
v3.1.0,"mat1 trans true, mat trans true"
v3.1.0,"mat1 trans true, mat trans false"
v3.1.0,"mat1 trans false, mat trans true, important"
v3.1.0,"mat1 trans false, mat trans false"
v3.1.0,"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
v3.1.0,"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
v3.1.0,not the first time
v3.1.0,first time and do the sample
v3.1.0,set to zero
v3.1.0,not the first time
v3.1.0,first time and do the sample
v3.1.0,set to zero
v3.1.0,not the first time
v3.1.0,first time and do the sample
v3.1.0,set to zero
v3.1.0,not the first time
v3.1.0,first time and do the sample
v3.1.0,set to zero
v3.1.0,not the first time
v3.1.0,first time and do the sample
v3.1.0,set to zero
v3.1.0,not the first time
v3.1.0,first time and do the sample
v3.1.0,set to zero
v3.1.0,not the first time
v3.1.0,first time and do the sample
v3.1.0,set to zero
v3.1.0,not the first time
v3.1.0,first time and do the sample
v3.1.0,set to zero
v3.1.0,not the first time
v3.1.0,first time and do the sample
v3.1.0,set to zero
v3.1.0,not the first time
v3.1.0,first time and do the sample
v3.1.0,set to zero
v3.1.0,get configuration from envs
v3.1.0,get master location
v3.1.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
v3.1.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
v3.1.0,add dense double matrix
v3.1.0,TODO Auto-generated method stub
v3.1.0,TODO Auto-generated method stub
v3.1.0,TODO Auto-generated method stub
v3.1.0,get configuration from config file
v3.1.0,set localDir with enviroment set by nm.
v3.1.0,get master location
v3.1.0,init task manager and start tasks
v3.1.0,start heartbeat thread
v3.1.0,taskManager.assignTaskIds(response.getTaskidsList());
v3.1.0,todo
v3.1.0,"if worker timeout, it may be knocked off."
v3.1.0,"SUCCESS, do nothing"
v3.1.0,heartbeatFailedTime = 0;
v3.1.0,private KEY currentKey;
v3.1.0,will be created
v3.1.0,TODO Auto-generated method stub
v3.1.0,Bitmap bitmap = new Bitmap();
v3.1.0,int max = indexArray[size - 1];
v3.1.0,byte [] bitIndexArray = new byte[max / 8 + 1];
v3.1.0,for(int i = 0; i < size; i++){
v3.1.0,int bitIndex = indexArray[i] >> 3;
v3.1.0,int bitOffset = indexArray[i] - (bitIndex << 3);
v3.1.0,switch(bitOffset){
v3.1.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
v3.1.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
v3.1.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
v3.1.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
v3.1.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
v3.1.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
v3.1.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
v3.1.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
v3.1.0,}
v3.1.0,}
v3.1.0,"true, false"
v3.1.0,//////////////////////////////
v3.1.0,Application Configs
v3.1.0,//////////////////////////////
v3.1.0,//////////////////////////////
v3.1.0,Master Configs
v3.1.0,//////////////////////////////
v3.1.0,//////////////////////////////
v3.1.0,Worker Configs
v3.1.0,//////////////////////////////
v3.1.0,//////////////////////////////
v3.1.0,Task Configs
v3.1.0,//////////////////////////////
v3.1.0,//////////////////////////////
v3.1.0,ParameterServer Configs
v3.1.0,//////////////////////////////
v3.1.0,//////////////////////////////
v3.1.0,Kubernetes Configs.
v3.1.0,//////////////////////////////
v3.1.0,////////////////// IPC //////////////////////////
v3.1.0,//////////////////////////////
v3.1.0,Matrix transfer Configs.
v3.1.0,//////////////////////////////
v3.1.0,//////////////////////////////
v3.1.0,Matrix transfer Configs.
v3.1.0,//////////////////////////////
v3.1.0,Configs used to ANGEL_PS_PSAGENT running mode future.
v3.1.0,model parse
v3.1.0,Mark whether use pyangel or not.
v3.1.0,private Configuration conf;
v3.1.0,"Configuration that should be used in python environment, there should only be one"
v3.1.0,configuration instance in each Angel context.
v3.1.0,Use private access means jconf should not be changed or modified in this way.
v3.1.0,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
v3.1.0,Do nothing
v3.1.0,To-DO: add other ways to justify different value types
v3.1.0,"This is so ugly, must re-implement by more elegance way"
v3.1.0,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
v3.1.0,and other files submitted by user.
v3.1.0,Launch python process
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
v3.1.0,get a angel client
v3.1.0,add sparse float matrix
v3.1.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
v3.1.0,siMat.setPartitionClass(CSRPartition.class);
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,Init node neighbors
v3.1.0,client.asyncUpdate(new InitNeighborOver(new InitNeighborOverParam(matrixId))).get();
v3.1.0,Sample the neighbors
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
v3.1.0,get a angel client
v3.1.0,add sparse float matrix
v3.1.0,siMat.setValidIndexNum(100);
v3.1.0,siMat.setColNum(10000000000L);
v3.1.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
v3.1.0,siMat.setPartitionClass(CSRPartition.class);
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,Init node neighbors and feats
v3.1.0,Sample the neighbors
v3.1.0,TODO Auto-generated constructor stub
v3.1.0,set basic configuration keys
v3.1.0,use local deploy mode and dummy dataspliter
v3.1.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
v3.1.0,get a angel client
v3.1.0,add sparse float matrix
v3.1.0,Start PS
v3.1.0,Start to run application
v3.1.0,Init node neighbors
v3.1.0,Sample the neighbors
v3.1.0,sample continuously beginning from a random index
v3.1.0,Get node neighbor number
v3.1.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
v3.1.0,Get node neighbor number
v3.1.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
v3.1.0,"ServerLongAnyRow row = (ServerLongAnyRow) psContext.getMatrixStorageManager().getRow(pparam.getPartKey(), 0);"
v3.1.0,ObjectIterator<Long2ObjectMap.Entry<IElement>> iter = row.iterator();
v3.1.0,while (iter.hasNext()) {
v3.1.0,Long2ObjectMap.Entry<IElement> entry = iter.next();
v3.1.0,long key = entry.getLongKey() + pparam.getPartKey().getStartCol();
v3.1.0,WalkPath value = (WalkPath) entry.getValue();
v3.1.0,
v3.1.0,if (workerPartitionId == value.getNextPartitionIdx()) {
v3.1.0,"result.put(key, value.getTail2());"
v3.1.0,}
v3.1.0,}
v3.1.0,"int matrixId, PartitionKey partKey, long[] keyIds, int startIdx, int endIdx"
v3.1.0,"System.out.println(""PathQueue: put data to queue"");"
v3.1.0,"System.out.println(""queue.size: "" + queue.size());"
v3.1.0,"System.out.println(""CurrPathIdx of "" + wPath.getHead() + "" is "" + wPath.getCurrPathIdx());"
v3.1.0,if (numRetry == retry) {
v3.1.0,"System.out.println(""retried 3 time, got : "" + result.size());"
v3.1.0,}
v3.1.0,"System.out.println(""popBatch: "" + result.size() +"" | ""+ count);"
v3.1.0,"getRow(partKey.getMatrixId(), rowId, partKey.getPartitionId())"
v3.1.0,StringBuilder sb = new StringBuilder();
v3.1.0,"sb.append(key).append("" -> {"");"
v3.1.0,for (long n: neighbor) {
v3.1.0,"sb.append(n).append("", "");"
v3.1.0,}
v3.1.0,"sb.append(""} : "").append(neigh);"
v3.1.0,System.out.println(sb.toString());
v3.1.0,"System.out.println(""pushed size: "" + pathTail.size());"
v3.1.0,List<LinkedBlockingQueue<WalkPath>> queueList = PathQueue.getQueueList(partKey.getPartitionId());
v3.1.0,int p = 0;
v3.1.0,for (LinkedBlockingQueue<WalkPath> queue: queueList) {
v3.1.0,"System.out.println(""partition "" + p + "", size1 = ""+ pathTail.size() +  "" size2 = "" + queue.size());"
v3.1.0,p++;
v3.1.0,}
v3.1.0,"System.out.println(""pushed batch finished!"");"
v3.1.0,Get node neighbor number
v3.1.0,"If the neighbor number is 0, just return a int[0]"
v3.1.0,"If count <= 0 or the neighbor number is less or equal then count, just copy all neighbors to the result array"
v3.1.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
v3.1.0,Store the total neighbor number of all nodes in rowOffsets
v3.1.0,"Put the node ids, node neighbor number, node neighbors to the cache"
v3.1.0,No data in this partition
v3.1.0,Get total neighbor number
v3.1.0,Final matrix column indices: neighbors node ids
v3.1.0,Write positions in cloumnIndices for nodes
v3.1.0,Copy all cached sub column indices to final column indices
v3.1.0,Read position for a sub column indices
v3.1.0,Copy column indices for a node to final column indices
v3.1.0,Update write position for this node in final column indices
v3.1.0,Update the read position in sub column indices
v3.1.0,Clear all temp data
v3.1.0,Get node neighbor number
v3.1.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
v3.1.0,sample happens here to avoid memory copy on servers
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set trainning data path
v3.1.0,Set predict result path
v3.1.0,Set actionType prediction
v3.1.0,Data format
v3.1.0,Feature number of train data
v3.1.0,Tree number
v3.1.0,Tree depth
v3.1.0,Split number
v3.1.0,Feature sample ratio
v3.1.0,Ratio of validation
v3.1.0,Learning rate
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,Set data format
v3.1.0,"Set angel resource, #worker, #task, #PS"
v3.1.0,Set GBDT algorithm parameters
v3.1.0,Set training data path
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set predict data path
v3.1.0,Set load model path
v3.1.0,Set predict result path
v3.1.0,Set log path
v3.1.0,Set actionType prediction
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,Train batch number per epoch.
v3.1.0,Batch number
v3.1.0,Model type
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,Set data format
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,set sgd LR algorithm parameters #feature #epoch
v3.1.0,Set trainning data path
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set trainning data path
v3.1.0,Set load model path
v3.1.0,Set predict result path
v3.1.0,Set actionType prediction
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,Train batch number per epoch.
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,Set data format
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,set sgd FM algorithm parameters #feature #epoch
v3.1.0,Set trainning data path
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set trainning data path
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set trainning data path
v3.1.0,Set load model path
v3.1.0,Set predict result path
v3.1.0,Set actionType prediction
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,Model type
v3.1.0,String modelType = String.valueOf(RowType.T_FLOAT_DENSE);
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,set sgd LR algorithm parameters #feature #epoch
v3.1.0,"conf.set(MLConf.ML_MODEL_TYPE(), modelType);"
v3.1.0,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
v3.1.0,predictTest();
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,Set data format
v3.1.0,Set trainning data path
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set data format
v3.1.0,Set trainning data path
v3.1.0,Set load model path
v3.1.0,Set predict result path
v3.1.0,Set actionType prediction
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,class number
v3.1.0,Model type
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,Set data format
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,set sgd LR algorithm parameters #feature #epoch
v3.1.0,Set log path
v3.1.0,Set trainning data path
v3.1.0,Set save model path
v3.1.0,Set actionType train
v3.1.0,Set trainning data path
v3.1.0,Set load model path
v3.1.0,Set save model path
v3.1.0,Set actionType incremental train
v3.1.0,Set log path
v3.1.0,Set trainning data path
v3.1.0,Set load model path
v3.1.0,Set predict result path
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set training data path
v3.1.0,Set predict result path
v3.1.0,Set actionType prediction
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set actionType train
v3.1.0,Set trainning data path
v3.1.0,Set predict result path
v3.1.0,Set actionType prediction
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set trainning data path
v3.1.0,Set predict result path
v3.1.0,Set actionType prediction
v3.1.0,Cluster center number
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Sample ratio per mini-batch
v3.1.0,C
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,set Kmeans algorithm parameters #cluster #feature #epoch
v3.1.0,Set data format
v3.1.0,Set trainning data path
v3.1.0,Set save model path
v3.1.0,Set log save path
v3.1.0,Set actionType train
v3.1.0,Set trainning data path
v3.1.0,Set load model path
v3.1.0,Set save model path
v3.1.0,Set actionType incremental train
v3.1.0,Set log path
v3.1.0,Set testing data path
v3.1.0,Set load model path
v3.1.0,Set predict result path
v3.1.0,Set actionType prediction
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set trainning data path
v3.1.0,Set predict result path
v3.1.0,Set actionType prediction
v3.1.0,Feature number of train data
v3.1.0,Total iteration number
v3.1.0,Validation sample Ratio
v3.1.0,"Data format, libsvm or dummy"
v3.1.0,Model type
v3.1.0,Learning rate
v3.1.0,Decay of learning rate
v3.1.0,Regularization coefficient
v3.1.0,Set local deploy mode
v3.1.0,Set basic configuration keys
v3.1.0,Set data format
v3.1.0,"set angel resource parameters #worker, #task, #PS"
v3.1.0,set sgd LR algorithm parameters #feature #epoch
v3.1.0,Set trainning data path
v3.1.0,Set save model path
v3.1.0,Set log path
v3.1.0,Set actionType train
v3.1.0,Set trainning data path
v3.1.0,Set load model path
v3.1.0,Set predict result path
v3.1.0,TODO: optimize int key indices
v3.1.0,"System.out.println(""deserialize cols.length="" + nCols);"
v3.1.0,"System.out.print(""deserialize "");"
v3.1.0,"System.out.print(cols[c] + "" "");"
v3.1.0,System.out.println();
v3.1.0,TODO Auto-generated method stub
v3.1.0,"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
v3.1.0,"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
v3.1.0,"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));"
v3.1.0,"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));"
v3.1.0,"ground truth: positive, precision: positive"
v3.1.0,start row index for words
v3.1.0,start row index for docs
v3.1.0,doc ids
v3.1.0,topic assignments
v3.1.0,word to docs reverse index
v3.1.0,count word
v3.1.0,build word start index
v3.1.0,build word to doc reverse idx
v3.1.0,build dks
v3.1.0,dks = new TraverseHashMap[n_docs];
v3.1.0,for (int d = 0; d < n_docs; d++) {
v3.1.0,if (K < Short.MAX_VALUE) {
v3.1.0,if (docs.get(d).len < Byte.MAX_VALUE)
v3.1.0,dks[d] = new S2BTraverseMap(docs.get(d).len);
v3.1.0,if (docs.get(d).len < Short.MAX_VALUE)
v3.1.0,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
v3.1.0,else
v3.1.0,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
v3.1.0,} else {
v3.1.0,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
v3.1.0,}
v3.1.0,}
v3.1.0,build dks
v3.1.0,allocate update maps
v3.1.0,Skip if no token for this word
v3.1.0,Check whether error when fetching word-topic
v3.1.0,Build FTree for current word
v3.1.0,current doc
v3.1.0,old topic assignment
v3.1.0,"Check if error happens. if this happen, it's probably that failures happen to servers."
v3.1.0,We need to adjust the memory settings or network fetching parameters.
v3.1.0,Update statistics if needed
v3.1.0,Calculate psum and sample new topic
v3.1.0,Update statistics if needed
v3.1.0,Assign new topic
v3.1.0,Skip if no token for this word
v3.1.0,if (u >= p[end]) {
v3.1.0,"System.out.println(""u="" + u + "" p[end]="" + p[end] + "" start="" + start + "" end="" + end);"
v3.1.0,return end;
v3.1.0,}
v3.1.0,
v3.1.0,if (u < p[start]) {
v3.1.0,"System.out.println(""u="" + u + "" p[start]="" + p[start] + "" start="" + start + "" end="" + end);"
v3.1.0,return start;
v3.1.0,}
v3.1.0,The starting point
v3.1.0,There's always an unused entry.
v3.1.0,The starting point
v3.1.0,There's always an unused entry.
v3.1.0,The starting point
v3.1.0,There's always an unused entry.
v3.1.0,The starting point
v3.1.0,There's always an unused entry.
v3.1.0,The starting point
v3.1.0,There's always an unused entry.
v3.1.0,The starting point
v3.1.0,There's always an unused entry.
v3.1.0,print();
v3.1.0,The starting point
v3.1.0,There's always an unused entry.
v3.1.0,The starting point
v3.1.0,There's always an unused entry.
v3.1.0,The starting point
v3.1.0,There's always an unused entry.
v3.1.0,The starting point
v3.1.0,There's always an unused entry.
v3.1.0,print();
v3.1.0,Write #rows
v3.1.0,Write each row
v3.1.0,dense
v3.1.0,sparse
v3.1.0,LOG.info(buf.refCnt());
v3.1.0,dense
v3.1.0,sparse
v3.1.0,calculate columns
v3.1.0,reset(row);
v3.1.0,loss function
v3.1.0,gradient and hessian
v3.1.0,"categorical feature set, null: none, empty: all, else: partial"
v3.1.0,"node's end index in instancePos, instances in [start, end] belong to a tree node"
v3.1.0,initialize the phase
v3.1.0,current tree and depth
v3.1.0,create loss function
v3.1.0,calculate grad info of each instance
v3.1.0,"create data sketch, push candidate split value to PS"
v3.1.0,1. calculate candidate split value
v3.1.0,categorical features
v3.1.0,2. push local sketch to PS
v3.1.0,the leader worker
v3.1.0,merge categorical features
v3.1.0,create updates
v3.1.0,"pull the global sketch from PS, only called once by each worker"
v3.1.0,number of categorical feature
v3.1.0,sample feature
v3.1.0,push sampled feature set to the current tree
v3.1.0,create new tree
v3.1.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
v3.1.0,calculate gradient
v3.1.0,"1. create new tree, initialize tree nodes and node stats"
v3.1.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
v3.1.0,2.1. pull the sampled features of the current tree
v3.1.0,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
v3.1.0,"2.2. if use all the features, only called one"
v3.1.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
v3.1.0,4. set root node to active
v3.1.0,"5. reset instance position, set the root node's span"
v3.1.0,6. calculate gradient
v3.1.0,1. decide nodes that should be calculated
v3.1.0,2. decide calculated and subtracted tree nodes
v3.1.0,3. calculate threads
v3.1.0,wait until all threads finish
v3.1.0,4. subtract threads
v3.1.0,wait until all threads finish
v3.1.0,5. send histograms to PS
v3.1.0,6. update histogram cache
v3.1.0,clock
v3.1.0,find split
v3.1.0,"1. find responsible tree node, using RR scheme"
v3.1.0,2. pull gradient histogram
v3.1.0,2.1. get the name of this node's gradient histogram on PS
v3.1.0,2.2. pull the histogram
v3.1.0,2.3. find best split result of this tree node
v3.1.0,2.3.1 using server split
v3.1.0,"update the grad stats of the root node on PS, only called once by leader worker"
v3.1.0,update the grad stats of children node
v3.1.0,update the left child
v3.1.0,update the right child
v3.1.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
v3.1.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
v3.1.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
v3.1.0,2.3.5 reset this tree node's gradient histogram to 0
v3.1.0,3. push split feature to PS
v3.1.0,4. push split value to PS
v3.1.0,5. push split gain to PS
v3.1.0,6. set phase to AFTER_SPLIT
v3.1.0,this.phase = GBDTPhase.AFTER_SPLIT;
v3.1.0,clock
v3.1.0,1. get split feature
v3.1.0,2. get split value
v3.1.0,3. get split gain
v3.1.0,4. get node weight
v3.1.0,5. split node
v3.1.0,update local replica
v3.1.0,create AfterSplit task
v3.1.0,"2. check thread stats, if all threads finish, return"
v3.1.0,6. clock
v3.1.0,"split the span of one node, reset the instance position"
v3.1.0,in case this worker has no instance on this node
v3.1.0,set the span of left child
v3.1.0,set the span of right child
v3.1.0,"1. left to right, find the first instance that should be in the right child"
v3.1.0,"2. right to left, find the first instance that should be in the left child"
v3.1.0,3. swap two instances
v3.1.0,4. find the cut pos
v3.1.0,5. set the span of left child
v3.1.0,6. set the span of right child
v3.1.0,set tree node to active
v3.1.0,set node to leaf
v3.1.0,set node to inactive
v3.1.0,finish current depth
v3.1.0,finish current tree
v3.1.0,set the tree phase
v3.1.0,check if there is active node
v3.1.0,check if finish all the tree
v3.1.0,update node's grad stats on PS
v3.1.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
v3.1.0,the root node's stats is updated by leader worker
v3.1.0,1. create the update
v3.1.0,2. push the update to PS
v3.1.0,1. update predictions of training data
v3.1.0,2. update predictions of validation data
v3.1.0,the leader task adds node prediction to flush list
v3.1.0,1. name of this node's grad histogram on PS
v3.1.0,2. build the grad histogram of this node
v3.1.0,3. push the histograms to PS
v3.1.0,4. reset thread stats to finished
v3.1.0,5.1. set the children nodes of this node
v3.1.0,5.2. set split info and grad stats to this node
v3.1.0,5.2. create children nodes
v3.1.0,"5.3. create node stats for children nodes, and add them to the tree"
v3.1.0,5.4. reset instance position
v3.1.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
v3.1.0,5.6. set children nodes to leaf nodes
v3.1.0,5.7. set nid to leaf node
v3.1.0,5.8. deactivate active node
v3.1.0,"get feature type, 0:empty 1:all equal 2:real"
v3.1.0,"if not -1, sufficient space will be allocated at once"
v3.1.0,copy the highest levels
v3.1.0,copy baseBuffer
v3.1.0,merge two non-empty quantile sketches
v3.1.0,left child <= split value; right child > split value
v3.1.0,"the first: minimal, the last: maximal"
v3.1.0,categorical features
v3.1.0,continuous features
v3.1.0,left child <= split value; right child > split value
v3.1.0,feature index used to split
v3.1.0,feature value used to split
v3.1.0,loss change after split this node
v3.1.0,grad stats of the left child
v3.1.0,grad stats of the right child
v3.1.0,"LOG.info(""Constructor with fid = -1"");"
v3.1.0,fid = -1: no split currently
v3.1.0,the minimal split value is the minimal value of feature
v3.1.0,the splits do not include the maximal value of feature
v3.1.0,"1. the average distance, (maxValue - minValue) / splitNum"
v3.1.0,2. calculate the candidate split value
v3.1.0,1. new feature's histogram (grad + hess)
v3.1.0,size: sampled_featureNum * (2 * splitNum)
v3.1.0,"in other words, concatenate each feature's histogram"
v3.1.0,2. get the span of this node
v3.1.0,------ 3. using sparse-aware method to build histogram ---
v3.1.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
v3.1.0,3.1. get the instance index
v3.1.0,3.2. get the grad and hess of the instance
v3.1.0,3.3. add to the sum
v3.1.0,3.4. loop the non-zero entries
v3.1.0,3.4.1. get feature value
v3.1.0,3.4.2. current feature's position in the sampled feature set
v3.1.0,"int fPos = findFidPlace(this.controller.fSet, fid);"
v3.1.0,3.4.3. find the position of feature value in a histogram
v3.1.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
v3.1.0,3.4.4. add the grad and hess to the corresponding bin
v3.1.0,3.4.5. add the reverse to the bin that contains 0.0f
v3.1.0,4. add the grad and hess sum to the zero bin of all features
v3.1.0,find the best split result of the histogram of a tree node
v3.1.0,1. calculate the gradStats of the root node
v3.1.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
v3.1.0,2. loop over features
v3.1.0,2.1. get the ture feature id in the sampled feature set
v3.1.0,2.2. get the indexes of histogram of this feature
v3.1.0,2.3. find the best split of current feature
v3.1.0,2.4. update the best split result if possible
v3.1.0,"update the grad stats of the root node on PS, only called once by leader worker"
v3.1.0,3. update the grad stats of children node
v3.1.0,3.1. update the left child
v3.1.0,3.2. update the right child
v3.1.0,find the best split result of one feature
v3.1.0,1. set the feature id
v3.1.0,2. create the best left stats and right stats
v3.1.0,3. the gain of the root node
v3.1.0,4. create the temp left and right grad stats
v3.1.0,5. loop over all the data in histogram
v3.1.0,5.1. get the grad and hess of current hist bin
v3.1.0,5.2. check whether we can split with current left hessian
v3.1.0,right = root - left
v3.1.0,5.3. check whether we can split with current right hessian
v3.1.0,5.4. calculate the current loss gain
v3.1.0,5.5. check whether we should update the split result with current loss gain
v3.1.0,split value = sketches[splitIdx]
v3.1.0,"5.6. if should update, also update the best left and right grad stats"
v3.1.0,6. set the best left and right grad stats
v3.1.0,partition number
v3.1.0,cols of each partition
v3.1.0,1. calculate the total grad sum and hess sum
v3.1.0,2. create the grad stats of the node
v3.1.0,1. calculate the total grad sum and hess sum
v3.1.0,2. create the grad stats of the node
v3.1.0,1. calculate the total grad sum and hess sum
v3.1.0,2. create the grad stats of the node
v3.1.0,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
v3.1.0,assert fvalue >= sketch[start] && fvalue <= sketch[end];
v3.1.0,if (left > end) return end - start;
v3.1.0,find the best split result of the histogram of a tree node
v3.1.0,2.2. get the indexes of histogram of this feature
v3.1.0,2.3. find the best split of current feature
v3.1.0,2.4. update the best split result if possible
v3.1.0,find the best split result of one feature
v3.1.0,1. set the feature id
v3.1.0,splitEntry.setFid(fid);
v3.1.0,2. create the best left stats and right stats
v3.1.0,3. the gain of the root node
v3.1.0,4. create the temp left and right grad stats
v3.1.0,5. loop over all the data in histogram
v3.1.0,5.1. get the grad and hess of current hist bin
v3.1.0,5.2. check whether we can split with current left hessian
v3.1.0,right = root - left
v3.1.0,5.3. check whether we can split with current right hessian
v3.1.0,5.4. calculate the current loss gain
v3.1.0,5.5. check whether we should update the split result with current loss gain
v3.1.0,"5.6. if should update, also update the best left and right grad stats"
v3.1.0,6. set the best left and right grad stats
v3.1.0,find the best split result of a serve row on the PS
v3.1.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v3.1.0,2.2. get the start index in histogram of this feature
v3.1.0,2.3. find the best split of current feature
v3.1.0,2.4. update the best split result if possible
v3.1.0,"find the best split result of one feature from a server row, used by the PS"
v3.1.0,1. set the feature id
v3.1.0,2. create the best left stats and right stats
v3.1.0,3. the gain of the root node
v3.1.0,4. create the temp left and right grad stats
v3.1.0,5. loop over all the data in histogram
v3.1.0,5.1. get the grad and hess of current hist bin
v3.1.0,5.2. check whether we can split with current left hessian
v3.1.0,right = root - left
v3.1.0,5.3. check whether we can split with current right hessian
v3.1.0,5.4. calculate the current loss gain
v3.1.0,5.5. check whether we should update the split result with current loss gain
v3.1.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
v3.1.0,the task use index to find fvalue
v3.1.0,"5.6. if should update, also update the best left and right grad stats"
v3.1.0,6. set the best left and right grad stats
v3.1.0,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
v3.1.0,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
v3.1.0,max and min of each feature
v3.1.0,clear all the information
v3.1.0,calculate the sum of gradient and hess
v3.1.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v3.1.0,ridx)
v3.1.0,check if necessary information is ready
v3.1.0,"same as add, reduce is used in All Reduce"
v3.1.0,"features used in this tree, if equals null, means use all the features without sampling"
v3.1.0,node in the tree
v3.1.0,the gradient info of each instances
v3.1.0,initialize nodes
v3.1.0,gradient
v3.1.0,second order gradient
v3.1.0,int sendStartCol = (int) row.getStartCol();
v3.1.0,logistic loss for binary classification task.
v3.1.0,"logistic loss, but predict un-transformed margin"
v3.1.0,check if label in range
v3.1.0,return the default evaluation metric for the objective
v3.1.0,"task type: classification, regression, or ranking"
v3.1.0,"quantile sketch, size = featureNum * splitNum"
v3.1.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
v3.1.0,"active tree nodes, size = pow(2, treeDepth) -1"
v3.1.0,sampled features. size = treeNum * sampleRatio * featureNum
v3.1.0,categorical feature. size = workerNum * cateFeatNum * splitNum
v3.1.0,"split features, size = treeNum * treeNodeNum"
v3.1.0,"split values, size = treeNum * treeNodeNum"
v3.1.0,"split gains, size = treeNum * treeNodeNum"
v3.1.0,"node weights, size = treeNum * treeNodeNum"
v3.1.0,"node preds, size = treeNum * treeNodeNum"
v3.1.0,if using PS to perform split
v3.1.0,step size for a tree
v3.1.0,number of class
v3.1.0,minimum loss change required for a split
v3.1.0,maximum depth of a tree
v3.1.0,number of features
v3.1.0,number of nonzero
v3.1.0,number of candidates split value
v3.1.0,----- the rest parameters are less important ----
v3.1.0,base instance weight
v3.1.0,minimum amount of hessian(weight) allowed in a child
v3.1.0,L2 regularization factor
v3.1.0,L1 regularization factor
v3.1.0,default direction choice
v3.1.0,maximum delta update we can add in weight estimation
v3.1.0,this parameter can be used to stabilize update
v3.1.0,default=0 means no constraint on weight delta
v3.1.0,whether we want to do subsample for row
v3.1.0,whether to subsample columns for each tree
v3.1.0,accuracy of sketch
v3.1.0,accuracy of sketch
v3.1.0,leaf vector size
v3.1.0,option for parallelization
v3.1.0,option to open cacheline optimization
v3.1.0,whether to not print info during training.
v3.1.0,maximum depth of the tree
v3.1.0,number of features used for tree construction
v3.1.0,"minimum loss change required for a split, otherwise stop split"
v3.1.0,----- the rest parameters are less important ----
v3.1.0,default direction choice
v3.1.0,whether we want to do sample data
v3.1.0,whether to sample columns during tree construction
v3.1.0,whether to use histogram for split
v3.1.0,number of histogram units
v3.1.0,whether to print info during training.
v3.1.0,----- the rest parameters are obtained after training ----
v3.1.0,total number of nodes
v3.1.0,number of deleted nodes */
Release-2.4.0,@maxIndex: this variable contains the max index of node/word
Release-2.4.0,values[b + offset] = (random.nextFloat() - 0.5f) / dimension;
Release-2.4.0,some params
Release-2.4.0,max index for node/word
Release-2.4.0,compute number of nodes for one row
Release-2.4.0,check the length of dot values
Release-2.4.0,merge dot values from all partitions
Release-2.4.0,Skip-Gram model
Release-2.4.0,Negative sampling
Release-2.4.0,used to accumulate the updates for input vectors
Release-2.4.0,Negative sampling
Release-2.4.0,accumulate for the hidden layer
Release-2.4.0,update output layer
Release-2.4.0,update the hidden layer
Release-2.4.0,update input
Release-2.4.0,Skip-Gram model
Release-2.4.0,Negative sampling
Release-2.4.0,used to accumulate the updates for input vectors
Release-2.4.0,Negative sampling
Release-2.4.0,accumulate for the hidden layer
Release-2.4.0,update output layer
Release-2.4.0,update the hidden layer
Release-2.4.0,update input
Release-2.4.0,update output
Release-2.4.0,Some params
Release-2.4.0,compute number of nodes for one row
Release-2.4.0,window size
Release-2.4.0,Skip-Gram model
Release-2.4.0,Accumulate the input vectors from context
Release-2.4.0,Negative sampling
Release-2.4.0,used to accumulate the updates for input vectors
Release-2.4.0,window size
Release-2.4.0,skip-gram model
Release-2.4.0,Negative sampling
Release-2.4.0,accumulate for the hidden layer
Release-2.4.0,update output layer
Release-2.4.0,update the hidden layer
Release-2.4.0,update input
Release-2.4.0,update output
Release-2.4.0,some params
Release-2.4.0,batch sentences
Release-2.4.0,max index for node/word
Release-2.4.0,compute number of nodes for one row
Release-2.4.0,check the length of dot values
Release-2.4.0,merge dot values from all partitions
Release-2.4.0,locates the input vectors to local array to prevent randomly access
Release-2.4.0,on the large server row.
Release-2.4.0,fill 0 for context vector
Release-2.4.0,window size
Release-2.4.0,Continuous bag-of-words Models
Release-2.4.0,Accumulate the input vectors from context
Release-2.4.0,Calculate the partial dot values
Release-2.4.0,We should guarantee here that the sample would not equal the ``word``
Release-2.4.0,used to accumulate the context input vectors
Release-2.4.0,locates the input vector into local arrays to prevent randomly access for
Release-2.4.0,the large server row.
Release-2.4.0,window size
Release-2.4.0,while true to prevent sampling out a positive target
Release-2.4.0,how to prevent the randomly access to the output vectors??
Release-2.4.0,accumulate gradients for the input vectors
Release-2.4.0,update output vectors
Release-2.4.0,update input
Release-2.4.0,update output
Release-2.4.0,Some params
Release-2.4.0,compute number of nodes for one row
Release-2.4.0,// calculate bias
Release-2.4.0,if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {
Release-2.4.0,"double zVal = VectorUtils.getDouble(z, 0);"
Release-2.4.0,"double nVal = VectorUtils.getDouble(n, 0);"
Release-2.4.0,"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));"
Release-2.4.0,}
Release-2.4.0,Do nothing.
Release-2.4.0,split updates
Release-2.4.0,shuffle update splits
Release-2.4.0,generate part update splits
Release-2.4.0,"set split context: partition key, use int key for long key vector or net"
Release-2.4.0,how to do intersection for two dense vector with a given indices ??
Release-2.4.0,compress the neighbor IDs
Release-2.4.0,write out edges
Release-2.4.0,write out tags
Release-2.4.0,Get node neighbors
Release-2.4.0,current word
Release-2.4.0,neu1 stores the average value of input vectors in the context (CBOW)
Release-2.4.0,Continuous Bag-of-Words Model
Release-2.4.0,Accumulate the input vectors from context
Release-2.4.0,negative sampling
Release-2.4.0,Using the sigmoid value from the pre-computed table
Release-2.4.0,accumulate for the hidden layer
Release-2.4.0,update output layer
Release-2.4.0,add the counter for target
Release-2.4.0,update hidden layer
Release-2.4.0,Update the input vector for each word in the context
Release-2.4.0,add the counter to input
Release-2.4.0,update input layers
Release-2.4.0,update output layers
Release-2.4.0,for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];
Release-2.4.0,copy the highest levels
Release-2.4.0,copy baseBuffer
Release-2.4.0,merge two non-empty quantile sketches
Release-2.4.0,"if not -1, sufficient space will be allocated at once"
Release-2.4.0,InstanceRow ins = instanceRows[insId];
Release-2.4.0,int[] indices = ins.indices();
Release-2.4.0,int[] bins = ins.bins();
Release-2.4.0,int nnz = indices.length;
Release-2.4.0,for (int j = 0; j < nnz; j++) {
Release-2.4.0,int fid = indices[j];
Release-2.4.0,if (isFeatUsed[fid - featLo]) {
Release-2.4.0,"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);"
Release-2.4.0,}
Release-2.4.0,}
Release-2.4.0,1. allocate histogram
Release-2.4.0,"2. loop non-zero instances, accumulate to histogram"
Release-2.4.0,if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature
Release-2.4.0,3. add remaining grad and hess to default bin
Release-2.4.0,"return param.calcWeights(grad, hess);"
Release-2.4.0,"numClass is usually small, so we do not use arraycopy here"
Release-2.4.0,"numClass is usually small, so we do not use arraycopy here"
Release-2.4.0,TODO: use more schema on default bin
Release-2.4.0,1. set default bin to left child
Release-2.4.0,"2. for other bins, find its location"
Release-2.4.0,3. create split set
Release-2.4.0,this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];
Release-2.4.0,predict sparse instance with indices and values
Release-2.4.0,predict libsvm data
Release-2.4.0,"Preconditions.checkArgument(preds.length == labels.length,"
Release-2.4.0,"""LogLossMetric should be used for binary-label classification"");"
Release-2.4.0,double loss = 0.0;
Release-2.4.0,for (int i = 0; i < preds.length; i++) {
Release-2.4.0,"loss += evalOne(preds[i], labels[i]);"
Release-2.4.0,}
Release-2.4.0,return loss / labels.length;
Release-2.4.0,double error = 0.0;
Release-2.4.0,if (preds.length == labels.length) {
Release-2.4.0,for (int i = 0; i < preds.length; i++) {
Release-2.4.0,"error += evalOne(preds[i], labels[i]);"
Release-2.4.0,}
Release-2.4.0,} else {
Release-2.4.0,int numLabel = preds.length / labels.length;
Release-2.4.0,float[] pred = new float[numLabel];
Release-2.4.0,for (int i = 0; i < labels.length; i++) {
Release-2.4.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.4.0,"error += evalOne(pred, labels[i]);"
Release-2.4.0,}
Release-2.4.0,}
Release-2.4.0,return error / labels.length;
Release-2.4.0,Preconditions.checkArgument(preds.length != labels.length
Release-2.4.0,"&& preds.length % labels.length == 0,"
Release-2.4.0,"""CrossEntropyMetric should be used for multi-label classification"");"
Release-2.4.0,double loss = 0.0;
Release-2.4.0,int numLabel = preds.length / labels.length;
Release-2.4.0,float[] pred = new float[numLabel];
Release-2.4.0,for (int i = 0; i < labels.length; i++) {
Release-2.4.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.4.0,"loss += evalOne(pred, labels[i]);"
Release-2.4.0,}
Release-2.4.0,return loss / labels.length;
Release-2.4.0,double correct = 0.0;
Release-2.4.0,if (preds.length == labels.length) {
Release-2.4.0,for (int i = 0; i < preds.length; i++) {
Release-2.4.0,"correct += evalOne(preds[i], labels[i]);"
Release-2.4.0,}
Release-2.4.0,} else {
Release-2.4.0,int numLabel = preds.length / labels.length;
Release-2.4.0,float[] pred = new float[numLabel];
Release-2.4.0,for (int i = 0; i < labels.length; i++) {
Release-2.4.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.4.0,"correct += evalOne(pred, labels[i]);"
Release-2.4.0,}
Release-2.4.0,}
Release-2.4.0,return (float) (correct / labels.length);
Release-2.4.0,double errSum = 0.0f;
Release-2.4.0,if (preds.length == labels.length) {
Release-2.4.0,for (int i = 0; i < preds.length; i++) {
Release-2.4.0,"errSum += evalOne(preds[i], labels[i]);"
Release-2.4.0,}
Release-2.4.0,} else {
Release-2.4.0,int numLabel = preds.length / labels.length;
Release-2.4.0,float[] pred = new float[numLabel];
Release-2.4.0,for (int i = 0; i < labels.length; i++) {
Release-2.4.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.4.0,"errSum += evalOne(pred, labels[i]);"
Release-2.4.0,}
Release-2.4.0,}
Release-2.4.0,return Math.sqrt(errSum / labels.length);
Release-2.4.0,"System.out.println(""----------"");"
Release-2.4.0,"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)"
Release-2.4.0,"+ "", mask = "" + Integer.toBinaryString(readMaskT));"
Release-2.4.0,readMaskT <<= 1;
Release-2.4.0,"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};"
Release-2.4.0,int n = bits.length;
Release-2.4.0,BufferedBitSet writeBitSet = new BufferedBitSet(n);
Release-2.4.0,"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);"
Release-2.4.0,if (bitSet.get(i) != bits[i]) {
Release-2.4.0,"throw new RuntimeException("""" + i);"
Release-2.4.0,}
Release-2.4.0,private final ByteBuffer bytes;
Release-2.4.0,"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {"
Release-2.4.0,int capacity = bytes.capacity() * 8;
Release-2.4.0,readIndexT = bytes.capacity() - 1;
Release-2.4.0,return bytes.get(index);
Release-2.4.0,TODO: use arraycopy to make it faster
Release-2.4.0,assert from >= this.from && to <= this.to;
Release-2.4.0,"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));"
Release-2.4.0,"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));"
Release-2.4.0,return bits.clone();
Release-2.4.0,private final SerializableBuffer bytes;
Release-2.4.0,private final ByteBuffer bytes;
Release-2.4.0,this.bytes = ByteBuffer.allocate(numBytes);
Release-2.4.0,public BufferedBitSetWriter(ByteBuffer bytes) {
Release-2.4.0,this.bytes = bytes;
Release-2.4.0,}
Release-2.4.0,"bytes.put(writeIndex++, (byte) writeBuffer);"
Release-2.4.0,public ByteBuffer getBytes() {
Release-2.4.0,return bytes;
Release-2.4.0,}
Release-2.4.0,ML TreeConf
Release-2.4.0,GBDT TreeConf
Release-2.4.0,"edges=[x,...] firstFlow=1 => go to right if < x and go to left if > x"
Release-2.4.0,"edges=[x,...] firstFlow=0 => go to left if < x and go to right if > x"
Release-2.4.0,"different types of tree node splits, enumerated by their complexity"
Release-2.4.0,"in order to reduce model size, we give priority to split point"
Release-2.4.0,"comparison between two split points, we give priority to lower feature index"
Release-2.4.0,TODO: comparison between two split sets
Release-2.4.0,"public boolean leafwise;  // true if leaf-wise training, false if level-wise training"
Release-2.4.0,TODO: regularization
Release-2.4.0,TODO: regularization
Release-2.4.0,public float insSampleRatio;  // subsample ratio for instances
Release-2.4.0,Use by line with weight
Release-2.4.0,evict entry with the smallest degree
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy data spliter
Release-2.4.0,get a angel client
Release-2.4.0,add matrix
Release-2.4.0,TODO Auto-generated constructor stub
Release-2.4.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighbor;
Release-2.4.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborParam;
Release-2.4.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborResult;
Release-2.4.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighbor;
Release-2.4.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighborParam;
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add matrix
Release-2.4.0,@Test
Release-2.4.0,"public void testInitAndGet() throws ExecutionException, InterruptedException {"
Release-2.4.0,Worker worker = LocalClusterContext.get().getWorker(worker0Attempt0Id).getWorker();
Release-2.4.0,"MatrixClient client1 = worker.getPSAgent().getMatrixClient(""w2"", 0);"
Release-2.4.0,int matrixW1Id = client1.getMatrixId();
Release-2.4.0,// Generate graph data
Release-2.4.0,"Map<Integer, int []> adjMap = generateAdjTable(nodeNum, maxNeighborNum);"
Release-2.4.0,
Release-2.4.0,// Init graph adj table
Release-2.4.0,"InitNeighbor func = new InitNeighbor(new InitNeighborParam(matrixW1Id, adjMap));"
Release-2.4.0,client1.update(func);
Release-2.4.0,
Release-2.4.0,int [] nodeIds = new int[adjMap.size()];
Release-2.4.0,int i = 0;
Release-2.4.0,for(int nodeId : adjMap.keySet()) {
Release-2.4.0,nodeIds[i++] = nodeId;
Release-2.4.0,}
Release-2.4.0,
Release-2.4.0,// Get graph adj table from PS
Release-2.4.0,"GetNeighbor getFunc = new GetNeighbor(new GetNeighborParam(matrixW1Id, nodeIds, maxNeighborNum));"
Release-2.4.0,"Map<Integer, int[]> getResults = ((GetNeighborResult) (client1.get(getFunc)))"
Release-2.4.0,.getNodeIdToNeighborIndices();
Release-2.4.0,
Release-2.4.0,// Check the result
Release-2.4.0,"for(Entry<Integer, int[]> entry : getResults.entrySet()) {"
Release-2.4.0,"Assert.assertArrayEquals(entry.getValue(), adjMap.get(entry.getKey()));"
Release-2.4.0,}
Release-2.4.0,}
Release-2.4.0,row 0 is a random uniform
Release-2.4.0,row 1 is a random normal
Release-2.4.0,row 2 is filled with 1.0
Release-2.4.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
Release-2.4.0,"paras[1] = ""abc"";"
Release-2.4.0,"paras[2] = ""123"";"
Release-2.4.0,Add standard Hadoop classes
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,set sgd LR algorithm parameters #feature #epoch
Release-2.4.0,Set input data path
Release-2.4.0,Set save model path
Release-2.4.0,Set actionType train
Release-2.4.0,QSLRRunner runner = new QSLRRunner();
Release-2.4.0,runner.train(conf);
Release-2.4.0,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
Release-2.4.0,Dataset
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,class number
Release-2.4.0,Model type
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,Train batch number per epoch.
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set file system
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Use local deploy mode and data format
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set Softmax algorithm parameters
Release-2.4.0,Set model class
Release-2.4.0,Dataset
Release-2.4.0,Data format
Release-2.4.0,Feature number of train data
Release-2.4.0,Tree number
Release-2.4.0,Tree depth
Release-2.4.0,Split number
Release-2.4.0,Feature sample ratio
Release-2.4.0,Ratio of validation
Release-2.4.0,Learning rate
Release-2.4.0,Set file system
Release-2.4.0,Use local deploy mode and data format
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource, #worker, #task, #PS"
Release-2.4.0,Set GBDT algorithm parameters
Release-2.4.0,Dataset
Release-2.4.0,Set file system
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Use local deploy mode
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set DeepFM algorithm parameters
Release-2.4.0,Set model class
Release-2.4.0,Dataset
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,Model type
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set file system
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Use local deploy mode and data format
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set LR algorithm parameters
Release-2.4.0,Set model class
Release-2.4.0,Dataset
Release-2.4.0,Data format
Release-2.4.0,Model type
Release-2.4.0,Cluster center number
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Sample ratio per mini-batch
Release-2.4.0,C
Release-2.4.0,Set file system
Release-2.4.0,Use local deploy mode and data format
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource, #worker, #task, #PS"
Release-2.4.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.4.0,Dataset
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,Model type
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set file system
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Use local deploy mode and data format
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set FM algorithm parameters
Release-2.4.0,Set model class
Release-2.4.0,Dataset
Release-2.4.0,Set file system
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Use local deploy mode
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set WideAndDeep algorithm parameters
Release-2.4.0,Set model class
Release-2.4.0,Dataset
Release-2.4.0,Data format
Release-2.4.0,"Set LDA parameters #V, #K"
Release-2.4.0,Set file system
Release-2.4.0,Use local deploy mode and data format
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource, #worker, #task, #PS"
Release-2.4.0,Set LDA algorithm parameters
Release-2.4.0,Dataset
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,Model type
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set file system
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Use local deploy mode and data format
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set SVM algorithm parameters
Release-2.4.0,Set model class
Release-2.4.0,Dataset
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,Model type
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,Model is classification
Release-2.4.0,Train batch number per epoch.
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set file system
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Use local deploy mode and data format
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set LR algorithm parameters
Release-2.4.0,Set model class
Release-2.4.0,Dataset
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,Model type
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,Model is classification
Release-2.4.0,Train batch number per epoch.
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set file system
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Use local deploy mode and data format
Release-2.4.0,Set data path
Release-2.4.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set LR algorithm parameters
Release-2.4.0,Set model class
Release-2.4.0,Load model meta
Release-2.4.0,Convert model
Release-2.4.0,"Get input path, output path"
Release-2.4.0,Init serde
Release-2.4.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.4.0,Load model meta
Release-2.4.0,Convert model
Release-2.4.0,load hadoop configuration
Release-2.4.0,"Get input path, output path"
Release-2.4.0,Init serde
Release-2.4.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.4.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.4.0,input.seek(rowOffset.getOffset());
Release-2.4.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.4.0,input.seek(rowOffset.getOffset());
Release-2.4.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.4.0,input.seek(rowOffset.getOffset());
Release-2.4.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.4.0,input.seek(rowOffset.getOffset());
Release-2.4.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.4.0,input.seek(rowOffset.getOffset());
Release-2.4.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.4.0,input.seek(rowOffset.getOffset());
Release-2.4.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.4.0,input.seek(rowOffset.getOffset());
Release-2.4.0,Load model meta
Release-2.4.0,Check row type
Release-2.4.0,Load model
Release-2.4.0,Load model meta
Release-2.4.0,Check row type
Release-2.4.0,Load model
Release-2.4.0,Load model meta
Release-2.4.0,Check row type
Release-2.4.0,Load model
Release-2.4.0,Load model meta
Release-2.4.0,Check row type
Release-2.4.0,Load model
Release-2.4.0,Load model meta
Release-2.4.0,Check row type
Release-2.4.0,Load model
Release-2.4.0,Load model meta
Release-2.4.0,Check row type
Release-2.4.0,Load model
Release-2.4.0,Load model meta
Release-2.4.0,Check row type
Release-2.4.0,Load model
Release-2.4.0,Load model
Release-2.4.0,load hadoop configuration
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add matrix
Release-2.4.0,worker register
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add matrix
Release-2.4.0,add matrix
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add matrix
Release-2.4.0,attempt 0
Release-2.4.0,attempt1
Release-2.4.0,attempt1
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add matrix
Release-2.4.0,TODO Auto-generated constructor stub
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add matrix
Release-2.4.0,set basic configuration keys
Release-2.4.0,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add matrix
Release-2.4.0,Thread.sleep(5000);
Release-2.4.0,"response = master.getJobReport(null, request);"
Release-2.4.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
Release-2.4.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
Release-2.4.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add dense double matrix
Release-2.4.0,add comp dense double matrix
Release-2.4.0,add sparse double matrix
Release-2.4.0,add component sparse double matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense long matrix
Release-2.4.0,add comp dense long matrix
Release-2.4.0,add sparse long matrix
Release-2.4.0,add component sparse long matrix
Release-2.4.0,add comp dense long double matrix
Release-2.4.0,add sparse long-key double matrix
Release-2.4.0,add component long-key sparse double matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add sparse long-key float matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add sparse long-key int matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,add sparse long-key long matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add dense double matrix
Release-2.4.0,add comp dense double matrix
Release-2.4.0,add sparse double matrix
Release-2.4.0,add component sparse double matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense long matrix
Release-2.4.0,add comp dense long matrix
Release-2.4.0,add sparse long matrix
Release-2.4.0,add component sparse long matrix
Release-2.4.0,add comp dense long double matrix
Release-2.4.0,add sparse long-key double matrix
Release-2.4.0,add component long-key sparse double matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add sparse long-key float matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add sparse long-key int matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,add sparse long-key long matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,testDenseDoubleUDF();
Release-2.4.0,testSparseDoubleUDF();
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add dense double matrix
Release-2.4.0,add comp dense double matrix
Release-2.4.0,add sparse double matrix
Release-2.4.0,add component sparse double matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense long matrix
Release-2.4.0,add comp dense long matrix
Release-2.4.0,add sparse long matrix
Release-2.4.0,add component sparse long matrix
Release-2.4.0,add comp dense long double matrix
Release-2.4.0,add sparse long-key double matrix
Release-2.4.0,add component long-key sparse double matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add sparse long-key float matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add sparse long-key int matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,add sparse long-key long matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,client1.clock().get();
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,client1.clock().get();
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add dense double matrix
Release-2.4.0,add comp dense double matrix
Release-2.4.0,add sparse double matrix
Release-2.4.0,add component sparse double matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense long matrix
Release-2.4.0,add comp dense long matrix
Release-2.4.0,add sparse long matrix
Release-2.4.0,add component sparse long matrix
Release-2.4.0,add comp dense long double matrix
Release-2.4.0,add sparse long-key double matrix
Release-2.4.0,add component long-key sparse double matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add sparse long-key float matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add sparse long-key int matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,add sparse long-key long matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,testDenseDoubleUDF();
Release-2.4.0,testSparseDoubleUDF();
Release-2.4.0,client1.clock().get();
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,client1.clock().get();
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,TODO Auto-generated constructor stub
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add dense double matrix
Release-2.4.0,add sparse double matrix
Release-2.4.0,add comp dense double matrix
Release-2.4.0,add component sparse double matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense long matrix
Release-2.4.0,add comp dense long matrix
Release-2.4.0,add sparse long matrix
Release-2.4.0,add component sparse long matrix
Release-2.4.0,add comp dense long double matrix
Release-2.4.0,add sparse long-key double matrix
Release-2.4.0,add component long-key sparse double matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add sparse long-key float matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add sparse long-key int matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,add sparse long-key long matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,testDenseDoubleUDF();
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,Assert.assertTrue(index.length == row.size());
Release-2.4.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.4.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.4.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"LOG.info(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.4.0,IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,for (int i = 0; i < feaNum; i++) {
Release-2.4.0,"deltaVec.set(i, i);"
Release-2.4.0,}
Release-2.4.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.4.0,IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add sparse float matrix
Release-2.4.0,siMat.setPartitionClass(CSRPartition.class);
Release-2.4.0,siMat.setPartitionStorageClass(IntCSRStorage.class);
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add dense double matrix
Release-2.4.0,add comp dense double matrix
Release-2.4.0,add sparse double matrix
Release-2.4.0,add component sparse double matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense long matrix
Release-2.4.0,add comp dense long matrix
Release-2.4.0,add sparse long matrix
Release-2.4.0,add component sparse long matrix
Release-2.4.0,add comp dense long double matrix
Release-2.4.0,add sparse long-key double matrix
Release-2.4.0,add component long-key sparse double matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add sparse long-key float matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add sparse long-key int matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,add sparse long-key long matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,get a angel client
Release-2.4.0,add dense double matrix
Release-2.4.0,add comp dense double matrix
Release-2.4.0,add sparse double matrix
Release-2.4.0,add component sparse double matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense float matrix
Release-2.4.0,add comp dense float matrix
Release-2.4.0,add sparse float matrix
Release-2.4.0,add component sparse float matrix
Release-2.4.0,add dense long matrix
Release-2.4.0,add comp dense long matrix
Release-2.4.0,add sparse long matrix
Release-2.4.0,add component sparse long matrix
Release-2.4.0,add comp dense long double matrix
Release-2.4.0,add sparse long-key double matrix
Release-2.4.0,add component long-key sparse double matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add sparse long-key float matrix
Release-2.4.0,add component long-key sparse float matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add sparse long-key int matrix
Release-2.4.0,add component long-key sparse int matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,add sparse long-key long matrix
Release-2.4.0,add component long-key sparse long matrix
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-2.4.0,get a angel client
Release-2.4.0,add sparse float matrix
Release-2.4.0,MatrixContext siMat = new MatrixContext();
Release-2.4.0,siMat.setName(SPARSE_INT_MAT);
Release-2.4.0,siMat.setRowType(RowType.T_ANY_INTKEY_SPARSE);
Release-2.4.0,siMat.setRowNum(1);
Release-2.4.0,siMat.setValidIndexNum(100);
Release-2.4.0,siMat.setColNum(10000000000L);
Release-2.4.0,siMat.setValueType(Node.class);
Release-2.4.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-2.4.0,siMat.setPartitionClass(CSRPartition.class);
Release-2.4.0,angelClient.addMatrix(siMat);
Release-2.4.0,add sparse long-key double matrix
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,client1.clock().get();
Release-2.4.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.4.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
Release-2.4.0,@RunWith(MockitoJUnitRunner.class)
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
Release-2.4.0,get a angel client
Release-2.4.0,add matrix
Release-2.4.0,psAgent.initAndStart();
Release-2.4.0,test conf
Release-2.4.0,test master location
Release-2.4.0,test app id
Release-2.4.0,test user
Release-2.4.0,test ps agent attempt id
Release-2.4.0,test connection
Release-2.4.0,test master client
Release-2.4.0,test ip
Release-2.4.0,test loc
Release-2.4.0,test master location
Release-2.4.0,test ps location
Release-2.4.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
Release-2.4.0,test all ps ids
Release-2.4.0,test all matrix ids
Release-2.4.0,test all matrix names
Release-2.4.0,test matrix attribute
Release-2.4.0,test matrix meta
Release-2.4.0,test ps location
Release-2.4.0,test partitions
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,System.out.println(content);
Release-2.4.0,https://blog.csdn.net/cocoonyang/article/details/63068108
Release-2.4.0,v1[i] = v1[i] + da * v2[i];
Release-2.4.0,"dgemm(String transa, String transb,"
Release-2.4.0,"int m, int n, int k,"
Release-2.4.0,"double alpha,"
Release-2.4.0,"double[] a, int lda,"
Release-2.4.0,"double[] b, int ldb,"
Release-2.4.0,"double beta,"
Release-2.4.0,"double[] c, int ldc);"
Release-2.4.0,C := alpha*op( A )*op( B ) + beta*C
Release-2.4.0,v1[i] = v1[i] + da * v2[i];
Release-2.4.0,y := alpha*A*x + beta*y
Release-2.4.0,y := alpha*A*x + beta*y
Release-2.4.0,y := alpha*A*x + beta*y
Release-2.4.0,"dgemm(String transa, String transb,"
Release-2.4.0,"int m, int n, int k,"
Release-2.4.0,"double alpha,"
Release-2.4.0,"double[] a, int lda,"
Release-2.4.0,"double[] b, int ldb,"
Release-2.4.0,"double beta,"
Release-2.4.0,"double[] c, int ldc);"
Release-2.4.0,C := alpha*op( A )*op( B ) + beta*C
Release-2.4.0,Default does nothing.
Release-2.4.0,The app injection is optional
Release-2.4.0,"renderText(""hello world"");"
Release-2.4.0,"user choose a workerGroupID from the workergroups page,"
Release-2.4.0,now we should change the AngelApp params and render the workergroup page;
Release-2.4.0,"static final String WORKER_ID = ""worker.id"";"
Release-2.4.0,"div(""#logo"")."
Release-2.4.0,"img(""/static/hadoop-st.png"")._()."
Release-2.4.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
Release-2.4.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
Release-2.4.0,JQueryUI.jsnotice(html);
Release-2.4.0,import org.apache.hadoop.conf.Configuration;
Release-2.4.0,import java.lang.reflect.Field;
Release-2.4.0,all the files in input set
Release-2.4.0,Shuffle the file
Release-2.4.0,Get the blocks for all files
Release-2.4.0,Adjust the maxSize to make the split more balanced
Release-2.4.0,Handle the splittable files
Release-2.4.0,Handle the unsplittable files
Release-2.4.0,Split the blocks
Release-2.4.0,"If the remaining size of the current block is smaller than the required size,"
Release-2.4.0,the remaining blocks are divided into the current split
Release-2.4.0,Update current split length and move to next block
Release-2.4.0,Clear the current block offset
Release-2.4.0,"Current split length is > maxSize, split the block and generate a new split"
Release-2.4.0,Clear blocks list for next split
Release-2.4.0,Clear the current split length
Release-2.4.0,"If splitBlocks is not empty, just genetate a split for it"
Release-2.4.0,get block locations from file system
Release-2.4.0,create an input split
Release-2.4.0,get block locations from file system
Release-2.4.0,create a list of all block and their locations
Release-2.4.0,"if the file is not splitable, just create the one block with"
Release-2.4.0,full file length
Release-2.4.0,each split can be a maximum of maxSize
Release-2.4.0,if remainder is between max and 2*max - then
Release-2.4.0,"instead of creating splits of size max, left-max we"
Release-2.4.0,create splits of size left/2 and left/2. This is
Release-2.4.0,a heuristic to avoid creating really really small
Release-2.4.0,splits.
Release-2.4.0,add this block to the block --> node locations map
Release-2.4.0,"For blocks that do not have host/rack information,"
Release-2.4.0,assign to default  rack.
Release-2.4.0,add this block to the rack --> block map
Release-2.4.0,Add this host to rackToNodes map
Release-2.4.0,add this block to the node --> block map
Release-2.4.0,"if the file system does not have any rack information, then"
Release-2.4.0,use dummy rack location.
Release-2.4.0,The topology paths have the host name included as the last
Release-2.4.0,component. Strip it.
Release-2.4.0,get tokens for all the required FileSystems..
Release-2.4.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
Release-2.4.0,job.getConfiguration());
Release-2.4.0,Whether we need to recursive look into the directory structure
Release-2.4.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.4.0,user provided one (if any).
Release-2.4.0,all the files in input set
Release-2.4.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
Release-2.4.0,process all nodes and create splits that are local to a node. Generate
Release-2.4.0,"one split per node iteration, and walk over nodes multiple times to"
Release-2.4.0,distribute the splits across nodes.
Release-2.4.0,Skip the node if it has previously been marked as completed.
Release-2.4.0,"for each block, copy it into validBlocks. Delete it from"
Release-2.4.0,blockToNodes so that the same block does not appear in
Release-2.4.0,two different splits.
Release-2.4.0,Remove all blocks which may already have been assigned to other
Release-2.4.0,splits.
Release-2.4.0,"if the accumulated split size exceeds the maximum, then"
Release-2.4.0,create this split.
Release-2.4.0,create an input split and add it to the splits array
Release-2.4.0,Remove entries from blocksInNode so that we don't walk these
Release-2.4.0,again.
Release-2.4.0,Done creating a single split for this node. Move on to the next
Release-2.4.0,node so that splits are distributed across nodes.
Release-2.4.0,This implies that the last few blocks (or all in case maxSize=0)
Release-2.4.0,were not part of a split. The node is complete.
Release-2.4.0,if there were any blocks left over and their combined size is
Release-2.4.0,"larger than minSplitNode, then combine them into one split."
Release-2.4.0,Otherwise add them back to the unprocessed pool. It is likely
Release-2.4.0,that they will be combined with other blocks from the
Release-2.4.0,same rack later on.
Release-2.4.0,This condition also kicks in when max split size is not set. All
Release-2.4.0,blocks on a node will be grouped together into a single split.
Release-2.4.0,haven't created any split on this machine. so its ok to add a
Release-2.4.0,smaller one for parallelism. Otherwise group it in the rack for
Release-2.4.0,balanced size create an input split and add it to the splits
Release-2.4.0,array
Release-2.4.0,Remove entries from blocksInNode so that we don't walk this again.
Release-2.4.0,The node is done. This was the last set of blocks for this node.
Release-2.4.0,Put the unplaced blocks back into the pool for later rack-allocation.
Release-2.4.0,Node is done. All blocks were fit into node-local splits.
Release-2.4.0,Check if node-local assignments are complete.
Release-2.4.0,All nodes have been walked over and marked as completed or all blocks
Release-2.4.0,have been assigned. The rest should be handled via rackLock assignment.
Release-2.4.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
Release-2.4.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
Release-2.4.0,"if blocks in a rack are below the specified minimum size, then keep them"
Release-2.4.0,"in 'overflow'. After the processing of all racks is complete, these"
Release-2.4.0,overflow blocks will be combined into splits.
Release-2.4.0,Process all racks over and over again until there is no more work to do.
Release-2.4.0,Create one split for this rack before moving over to the next rack.
Release-2.4.0,Come back to this rack after creating a single split for each of the
Release-2.4.0,remaining racks.
Release-2.4.0,"Process one rack location at a time, Combine all possible blocks that"
Release-2.4.0,reside on this rack as one split. (constrained by minimum and maximum
Release-2.4.0,split size).
Release-2.4.0,iterate over all racks
Release-2.4.0,"for each block, copy it into validBlocks. Delete it from"
Release-2.4.0,blockToNodes so that the same block does not appear in
Release-2.4.0,two different splits.
Release-2.4.0,"if the accumulated split size exceeds the maximum, then"
Release-2.4.0,create this split.
Release-2.4.0,create an input split and add it to the splits array
Release-2.4.0,"if we created a split, then just go to the next rack"
Release-2.4.0,"if there is a minimum size specified, then create a single split"
Release-2.4.0,"otherwise, store these blocks into overflow data structure"
Release-2.4.0,There were a few blocks in this rack that
Release-2.4.0,remained to be processed. Keep them in 'overflow' block list.
Release-2.4.0,These will be combined later.
Release-2.4.0,Process all overflow blocks
Release-2.4.0,"This might cause an exiting rack location to be re-added,"
Release-2.4.0,but it should be ok.
Release-2.4.0,"if the accumulated split size exceeds the maximum, then"
Release-2.4.0,create this split.
Release-2.4.0,create an input split and add it to the splits array
Release-2.4.0,"Process any remaining blocks, if any."
Release-2.4.0,create an input split
Release-2.4.0,add this split to the list that is returned
Release-2.4.0,long num = totLength / maxSize;
Release-2.4.0,all blocks for all the files in input set
Release-2.4.0,mapping from a rack name to the list of blocks it has
Release-2.4.0,mapping from a block to the nodes on which it has replicas
Release-2.4.0,mapping from a node to the list of blocks that it contains
Release-2.4.0,populate all the blocks for all files
Release-2.4.0,stop all services
Release-2.4.0,1.write application state to file so that the client can get the state of the application
Release-2.4.0,if master exit
Release-2.4.0,2.clear tmp and staging directory
Release-2.4.0,waiting for client to get application state
Release-2.4.0,stop the RPC server
Release-2.4.0,"Security framework already loaded the tokens into current UGI, just use"
Release-2.4.0,them
Release-2.4.0,Now remove the AM->RM token so tasks don't have it
Release-2.4.0,add a shutdown hook
Release-2.4.0,init app state storage
Release-2.4.0,init event dispacher
Release-2.4.0,init location manager
Release-2.4.0,init container allocator
Release-2.4.0,init a rpc service
Release-2.4.0,recover matrix meta if needed
Release-2.4.0,recover ps attempt information if need
Release-2.4.0,Init Client manager
Release-2.4.0,Init PS Client manager
Release-2.4.0,init parameter server manager
Release-2.4.0,recover task information if needed
Release-2.4.0,a dummy data spliter is just for test now
Release-2.4.0,recover data splits information if needed
Release-2.4.0,init worker manager and register worker manager event
Release-2.4.0,register slow worker/ps checker
Release-2.4.0,register app manager event and finish event
Release-2.4.0,Init model saver & loader
Release-2.4.0,start a web service if use yarn deploy mode
Release-2.4.0,load from app state storage first if attempt index great than 1(the master is not the first
Release-2.4.0,retry)
Release-2.4.0,"if load failed, just build a new MatrixMetaManager"
Release-2.4.0,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-2.4.0,is not the first retry)
Release-2.4.0,load task information from app state storage first if attempt index great than 1(the master
Release-2.4.0,is not the first retry)
Release-2.4.0,"if load failed, just build a new AMTaskManager"
Release-2.4.0,load data splits information from app state storage first if attempt index great than 1(the
Release-2.4.0,master is not the first retry)
Release-2.4.0,"if load failed, we need to recalculate the data splits"
Release-2.4.0,Check Workers
Release-2.4.0,Check PSS
Release-2.4.0,Check Clients
Release-2.4.0,Check PS Clients
Release-2.4.0,stop all services
Release-2.4.0,1.write application state to file so that the client can get the state of the application
Release-2.4.0,if master exit
Release-2.4.0,2.clear tmp and staging directory
Release-2.4.0,waiting for client to get application state
Release-2.4.0,stop the RPC server
Release-2.4.0,add a shutdown hook
Release-2.4.0,init app state storage
Release-2.4.0,init event dispacher
Release-2.4.0,init location manager
Release-2.4.0,init a rpc service
Release-2.4.0,recover matrix meta if needed
Release-2.4.0,recover ps attempt information if need
Release-2.4.0,Init Client manager
Release-2.4.0,Init PS Client manager
Release-2.4.0,init parameter server manager
Release-2.4.0,recover task information if needed
Release-2.4.0,a dummy data spliter is just for test now
Release-2.4.0,recover data splits information if needed
Release-2.4.0,init worker manager and register worker manager event
Release-2.4.0,register slow worker/ps checker
Release-2.4.0,register app manager event and finish event
Release-2.4.0,Init model saver & loader
Release-2.4.0,k8sClusterManager = new KubernetesClusterManager(appContext);
Release-2.4.0,load from app state storage first if attempt index great than 1(the master is not the first
Release-2.4.0,retry)
Release-2.4.0,"if load failed, just build a new MatrixMetaManager"
Release-2.4.0,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-2.4.0,is not the first retry)
Release-2.4.0,load task information from app state storage first if attempt index great than 1(the master
Release-2.4.0,is not the first retry)
Release-2.4.0,"if load failed, just build a new AMTaskManager"
Release-2.4.0,load data splits information from app state storage first if attempt index great than 1(the
Release-2.4.0,master is not the first retry)
Release-2.4.0,"if load failed, we need to recalculate the data splits"
Release-2.4.0,parse parameter server counters
Release-2.4.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.4.0,refresh last heartbeat timestamp
Release-2.4.0,send a state update event to the specific PSAttempt
Release-2.4.0,Check is there save request
Release-2.4.0,"LOG.info(""PS "" + psAttemptId + "" need save "" + subSaveContext);"
Release-2.4.0,Check is there load request
Release-2.4.0,"LOG.info(""PS "" + psAttemptId + "" need load "" + subLoadContext);"
Release-2.4.0,check matrix metadata inconsistencies between master and parameter server.
Release-2.4.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
Release-2.4.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
Release-2.4.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.4.0,choose a unused port
Release-2.4.0,start RPC server
Release-2.4.0,remove this parameter server attempt from monitor set
Release-2.4.0,remove this parameter server attempt from monitor set
Release-2.4.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.4.0,find workergroup in worker manager
Release-2.4.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
Release-2.4.0,"if this worker group run over, just return WORKERGROUP_EXITED"
Release-2.4.0,"if this worker group is running now, return tasks, workers, data splits for it"
Release-2.4.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.4.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.4.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.4.0,"in ANGEL_PS mode, task id may can not know advance"
Release-2.4.0,update the clock for this matrix
Release-2.4.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.4.0,"in ANGEL_PS mode, task id may can not know advance"
Release-2.4.0,update task iteration
Release-2.4.0,"LOG.info(""error happened in psAttempt "" + psAttemptId + "" error msg="" + request.getMsg());"
Release-2.4.0,remove this parameter server attempt from monitor set
Release-2.4.0,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
Release-2.4.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
Release-2.4.0,the number of splits equal to the number of tasks
Release-2.4.0,split data
Release-2.4.0,dispatch the splits to workergroups
Release-2.4.0,split data
Release-2.4.0,dispatch the splits to workergroups
Release-2.4.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.4.0,"first, then divided by expected split number"
Release-2.4.0,get input format class from configuration and then instantiation a input format object
Release-2.4.0,split data
Release-2.4.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.4.0,"first, then divided by expected split number"
Release-2.4.0,get input format class from configuration and then instantiation a input format object
Release-2.4.0,split data
Release-2.4.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.4.0,need to fine tune the number of workergroup and task based on the actual split number
Release-2.4.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.4.0,Record the location information for the splits in order to data localized schedule
Release-2.4.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.4.0,need to fine tune the number of workergroup and task based on the actual split number
Release-2.4.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.4.0,Record the location information for the splits in order to data localized schedule
Release-2.4.0,write meta data to a temporary file
Release-2.4.0,rename the temporary file to final file
Release-2.4.0,"if the file exists, read from file and deserialize it"
Release-2.4.0,write task meta
Release-2.4.0,write ps meta
Release-2.4.0,generate a temporary file
Release-2.4.0,write task meta to the temporary file first
Release-2.4.0,rename the temporary file to the final file
Release-2.4.0,"if last final task file exist, remove it"
Release-2.4.0,find task meta file which has max timestamp
Release-2.4.0,"if the file does not exist, just return null"
Release-2.4.0,read task meta from file and deserialize it
Release-2.4.0,generate a temporary file
Release-2.4.0,write ps meta to the temporary file first.
Release-2.4.0,rename the temporary file to the final file
Release-2.4.0,"if the old final file exist, just remove it"
Release-2.4.0,find ps meta file
Release-2.4.0,"if ps meta file does not exist, just return null"
Release-2.4.0,read ps meta from file and deserialize it
Release-2.4.0,"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),"
Release-2.4.0,String.valueOf(requestId));
Release-2.4.0,Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));
Release-2.4.0,saveContext.setTmpSavePath(tmpPath.toString());
Release-2.4.0,Filter old epoch trigger first
Release-2.4.0,Split the user request to sub-requests to pss
Release-2.4.0,Init matrix files meta
Release-2.4.0,Move output files
Release-2.4.0,Write the meta file
Release-2.4.0,Split the user request to sub-requests to pss
Release-2.4.0,check whether psagent heartbeat timeout
Release-2.4.0,Set up the launch command
Release-2.4.0,Duplicate the ByteBuffers for access by multiple containers.
Release-2.4.0,Construct the actual Container
Release-2.4.0,Application resources
Release-2.4.0,Application environment
Release-2.4.0,Service data
Release-2.4.0,Tokens
Release-2.4.0,Set up JobConf to be localized properly on the remote NM.
Release-2.4.0,Setup DistributedCache
Release-2.4.0,Setup up task credentials buffer
Release-2.4.0,LocalStorageToken is needed irrespective of whether security is enabled
Release-2.4.0,or not.
Release-2.4.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
Release-2.4.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
Release-2.4.0,Construct the actual Container
Release-2.4.0,The null fields are per-container and will be constructed for each
Release-2.4.0,container separately.
Release-2.4.0,Set up the launch command
Release-2.4.0,Duplicate the ByteBuffers for access by multiple containers.
Release-2.4.0,Construct the actual Container
Release-2.4.0,"a * in the classpath will only find a .jar, so we need to filter out"
Release-2.4.0,all .jars and add everything else
Release-2.4.0,Propagate the system classpath when using the mini cluster
Release-2.4.0,Add standard Hadoop classes
Release-2.4.0,Add mr
Release-2.4.0,Cache archives
Release-2.4.0,Cache files
Release-2.4.0,Sanity check
Release-2.4.0,Add URI fragment or just the filename
Release-2.4.0,Add the env variables passed by the user
Release-2.4.0,Set logging level in the environment.
Release-2.4.0,Old parameter name
Release-2.4.0,Parallel GC parameters
Release-2.4.0,G1 params
Release-2.4.0,Parallel Scavenge + Parallel Old
Release-2.4.0,G1
Release-2.4.0,".append("" -XX:G1NewSizePercent="").append(minNewRatio)"
Release-2.4.0,".append("" -XX:G1MaxNewSizePercent="").append(maxNewRatio)"
Release-2.4.0,CMS
Release-2.4.0,Setup the log4j prop
Release-2.4.0,Add main class and its arguments
Release-2.4.0,Finally add the jvmID
Release-2.4.0,vargs.add(String.valueOf(jvmID.getId()));
Release-2.4.0,Final commmand
Release-2.4.0,G1 params
Release-2.4.0,Add the env variables passed by the user
Release-2.4.0,Set logging level in the environment.
Release-2.4.0,Setup the log4j prop
Release-2.4.0,Add main class and its arguments
Release-2.4.0,Final commmand
Release-2.4.0,"if amTask is not null, we should clone task state from it"
Release-2.4.0,"if all parameter server complete commit, master can commit now"
Release-2.4.0,restartPS(psLoc);
Release-2.4.0,check whether parameter server heartbeat timeout
Release-2.4.0,Transitions from the NEW state.
Release-2.4.0,Transitions from the UNASSIGNED state.
Release-2.4.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
Release-2.4.0,Transitions from the ASSIGNED state.
Release-2.4.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
Release-2.4.0,PA_CONTAINER_LAUNCHED event
Release-2.4.0,Transitions from the PSAttemptStateInternal.RUNNING state.
Release-2.4.0,Transitions from the PSAttemptStateInternal.COMMITTING state
Release-2.4.0,Transitions from the PSAttemptStateInternal.KILLED state
Release-2.4.0,Transitions from the PSAttemptStateInternal.FAILED state
Release-2.4.0,create the topology tables
Release-2.4.0,reqeuest resource:send a resource request to the resource allocator
Release-2.4.0,"Once the resource is applied, build and send the launch request to the container launcher"
Release-2.4.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
Release-2.4.0,resource allocator
Release-2.4.0,set the launch time
Release-2.4.0,add the ps attempt to the heartbeat timeout monitoring list
Release-2.4.0,parse ps attempt location and put it to location manager
Release-2.4.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
Release-2.4.0,or failed
Release-2.4.0,remove ps attempt id from heartbeat timeout monitor list
Release-2.4.0,release container:send a release request to container launcher
Release-2.4.0,set the finish time only if launch time is set
Release-2.4.0,private long scheduledTime;
Release-2.4.0,Transitions from the NEW state.
Release-2.4.0,Transitions from the SCHEDULED state.
Release-2.4.0,Transitions from the RUNNING state.
Release-2.4.0,"another attempt launched,"
Release-2.4.0,Transitions from the SUCCEEDED state
Release-2.4.0,Transitions from the KILLED state
Release-2.4.0,Transitions from the FAILED state
Release-2.4.0,add diagnostic
Release-2.4.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.4.0,Refresh ps location & matrix meta
Release-2.4.0,start a new attempt for this ps
Release-2.4.0,notify ps manager
Release-2.4.0,"getContext().getLocationManager().setPsLocation(id, null);"
Release-2.4.0,add diagnostic
Release-2.4.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.4.0,start a new attempt for this ps
Release-2.4.0,notify ps manager
Release-2.4.0,notify the event handler of state change
Release-2.4.0,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
Release-2.4.0,"if forcedState is set, just return"
Release-2.4.0,else get state from state machine
Release-2.4.0,add this worker group to the success set
Release-2.4.0,check if all worker group run or run over
Release-2.4.0,add this worker group to the success set
Release-2.4.0,check if all worker group run over
Release-2.4.0,add this worker group to the failed set
Release-2.4.0,check if too many worker groups are failed or killed
Release-2.4.0,notify a run failed event
Release-2.4.0,add this worker group to the failed set
Release-2.4.0,check if too many worker groups are failed or killed
Release-2.4.0,notify a run failed event
Release-2.4.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
Release-2.4.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
Release-2.4.0,just return the total task number now
Release-2.4.0,TODO
Release-2.4.0,check whether worker heartbeat timeout
Release-2.4.0,"if workerAttempt is not null, we should clone task state from it"
Release-2.4.0,from NEW state
Release-2.4.0,from SCHEDULED state
Release-2.4.0,get data splits location for data locality
Release-2.4.0,reqeuest resource:send a resource request to the resource allocator
Release-2.4.0,"once the resource is applied, build and send the launch request to the container launcher"
Release-2.4.0,notify failed message to the worker
Release-2.4.0,notify killed message to the worker
Release-2.4.0,release the allocated container
Release-2.4.0,notify failed message to the worker
Release-2.4.0,remove the worker attempt from heartbeat timeout listen list
Release-2.4.0,release the allocated container
Release-2.4.0,notify killed message to the worker
Release-2.4.0,remove the worker attempt from heartbeat timeout listen list
Release-2.4.0,clean the container
Release-2.4.0,notify failed message to the worker
Release-2.4.0,remove the worker attempt from heartbeat timeout listen list
Release-2.4.0,record the finish time
Release-2.4.0,clean the container
Release-2.4.0,notify killed message to the worker
Release-2.4.0,remove the worker attempt from heartbeat timeout listening list
Release-2.4.0,record the finish time
Release-2.4.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
Release-2.4.0,set worker attempt location
Release-2.4.0,notify the register message to the worker
Release-2.4.0,record the launch time
Release-2.4.0,update worker attempt metrics
Release-2.4.0,update tasks metrics
Release-2.4.0,clean the container
Release-2.4.0,notify the worker attempt run successfully message to the worker
Release-2.4.0,record the finish time
Release-2.4.0,todo
Release-2.4.0,init a worker attempt for the worker
Release-2.4.0,schedule the worker attempt
Release-2.4.0,add diagnostic
Release-2.4.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.4.0,init and start a new attempt for this ps
Release-2.4.0,notify worker manager
Release-2.4.0,add diagnostic
Release-2.4.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.4.0,init and start a new attempt for this ps
Release-2.4.0,notify worker manager
Release-2.4.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
Release-2.4.0,register to Yarn RM
Release-2.4.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
Release-2.4.0,"catch YarnException or YarnRuntimeException, we should exit and need not retry"
Release-2.4.0,build heartbeat request
Release-2.4.0,send heartbeat request to rm
Release-2.4.0,"This can happen if the RM has been restarted. If it is in that state,"
Release-2.4.0,this application must clean itself up.
Release-2.4.0,Setting NMTokens
Release-2.4.0,assgin containers
Release-2.4.0,"if some container is not assigned, release them"
Release-2.4.0,handle finish containers
Release-2.4.0,dispatch container exit message to corresponding components
Release-2.4.0,killed by framework
Release-2.4.0,killed by framework
Release-2.4.0,get application finish state
Release-2.4.0,build application diagnostics
Release-2.4.0,TODO:add a job history for angel
Release-2.4.0,build unregister request
Release-2.4.0,send unregister request to rm
Release-2.4.0,Note this down for next interaction with ResourceManager
Release-2.4.0,based on blacklisting comments above we can end up decrementing more
Release-2.4.0,than requested. so guard for that.
Release-2.4.0,send the updated resource request to RM
Release-2.4.0,send 0 container count requests also to cancel previous requests
Release-2.4.0,Update resource requests
Release-2.4.0,try to assign to all nodes first to match node local
Release-2.4.0,try to match all rack local
Release-2.4.0,assign remaining
Release-2.4.0,Update resource requests
Release-2.4.0,send the container-assigned event to task attempt
Release-2.4.0,build the start container request use launch context
Release-2.4.0,send the start request to Yarn nm
Release-2.4.0,send the message that the container starts successfully to the corresponding component
Release-2.4.0,"after launching, send launched event to task attempt to move"
Release-2.4.0,it from ASSIGNED to RUNNING state
Release-2.4.0,send the message that the container starts failed to the corresponding component
Release-2.4.0,kill the remote container if already launched
Release-2.4.0,start a thread pool to startup the container
Release-2.4.0,See if we need up the pool size only if haven't reached the
Release-2.4.0,maximum limit yet.
Release-2.4.0,nodes where containers will run at *this* point of time. This is
Release-2.4.0,*not* the cluster size and doesn't need to be.
Release-2.4.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
Release-2.4.0,later is just a buffer so we are not always increasing the
Release-2.4.0,pool-size
Release-2.4.0,the events from the queue are handled in parallel
Release-2.4.0,using a thread pool
Release-2.4.0,return if already stopped
Release-2.4.0,shutdown any containers that might be left running
Release-2.4.0,Add one sync matrix
Release-2.4.0,addSyncMatrix();
Release-2.4.0,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
Release-2.4.0,"matrixContext.set(MatrixConf.MATRIX_LOAD_PATH, """");"
Release-2.4.0,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
Release-2.4.0,"LOG.info(""ps id = "" + psEntry.getKey());"
Release-2.4.0,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
Release-2.4.0,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
Release-2.4.0,"LOG.info(""matrix id = "" + metaEntry.getKey());"
Release-2.4.0,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
Release-2.4.0,}
Release-2.4.0,}
Release-2.4.0,get matrix ids in the parameter server report
Release-2.4.0,get the matrices parameter server need to create and delete
Release-2.4.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
Release-2.4.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
Release-2.4.0,Init control connection manager
Release-2.4.0,Get ps locations from master and put them to the location cache.
Release-2.4.0,Build and initialize rpc client to master
Release-2.4.0,Get psagent id
Release-2.4.0,Build PS control rpc client manager
Release-2.4.0,Build local location
Release-2.4.0,Initialize matrix meta information
Release-2.4.0,Start all services
Release-2.4.0,Stop all modules
Release-2.4.0,Stop all modules
Release-2.4.0,clock first
Release-2.4.0,wait
Release-2.4.0,Update generic resource counters
Release-2.4.0,Updating resources specified in ResourceCalculatorProcessTree
Release-2.4.0,Remove the CPU time consumed previously by JVM reuse
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,/ Plus a vector/matrix to the matrix stored in pss
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,/ Update a vector/matrix to the matrix stored in pss
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,/ Get values from pss use row/column indices
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"/ PSF get/update, use can implement their own psf"
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,/ Get a row or a batch of rows
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,Just return
Release-2.4.0,Just return
Release-2.4.0,Just return
Release-2.4.0,Just return
Release-2.4.0,Return a empty vector
Release-2.4.0,Return a empty vector
Release-2.4.0,Return a empty vector
Release-2.4.0,Return a empty vector
Release-2.4.0,Return a empty vector
Release-2.4.0,Return a empty vector
Release-2.4.0,Return a empty vector
Release-2.4.0,Return a empty vector
Release-2.4.0,"checkNotNull(func, ""func"");"
Release-2.4.0,Return a empty vector
Release-2.4.0,"checkNotNull(func, ""func"");"
Release-2.4.0,Return a empty vector
Release-2.4.0,"checkNotNull(func, ""func"");"
Release-2.4.0,Return a empty vector
Release-2.4.0,"checkNotNull(func, ""func"");"
Release-2.4.0,Return a empty vector
Release-2.4.0,"checkNotNull(func, ""func"");"
Release-2.4.0,Return a empty vector
Release-2.4.0,"checkNotNull(func, ""func"");"
Release-2.4.0,Return a empty vector
Release-2.4.0,"checkNotNull(func, ""func"");"
Release-2.4.0,Return a empty vector
Release-2.4.0,"checkNotNull(func, ""func"");"
Release-2.4.0,Return a empty vector
Release-2.4.0,Sort the partitions by start column index
Release-2.4.0,Generate a flush request and put it to request queue
Release-2.4.0,Generate a clock request and put it to request queue
Release-2.4.0,Generate a merge request and put it to request queue
Release-2.4.0,Generate a merge request and put it to request queue
Release-2.4.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
Release-2.4.0,matrix
Release-2.4.0,and add it to cache maps
Release-2.4.0,Add the message to the tree map
Release-2.4.0,"If there are flush / clock requests blocked, we need to put this merge request into"
Release-2.4.0,the waiting queue
Release-2.4.0,Launch a merge worker to merge the update to matrix op log cache
Release-2.4.0,Remove the message from the tree map
Release-2.4.0,Wake up blocked flush/clock request
Release-2.4.0,Add flush/clock request to listener list to waiting for all the existing
Release-2.4.0,updates are merged
Release-2.4.0,Wake up blocked flush/clock request
Release-2.4.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
Release-2.4.0,blocked.
Release-2.4.0,Get next merge message sequence id
Release-2.4.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
Release-2.4.0,position
Release-2.4.0,Wake up blocked merge requests
Release-2.4.0,Get minimal sequence id from listeners
Release-2.4.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
Release-2.4.0,should flush updates to local matrix storage
Release-2.4.0,Doing average or not
Release-2.4.0,Filter un-important update
Release-2.4.0,Split this row according the matrix partitions
Release-2.4.0,Set split context
Release-2.4.0,Remove the row from matrix
Release-2.4.0,buf.writeDouble(0.0);
Release-2.4.0,TODO:
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
Release-2.4.0,"LOG.error(""put response message queue failed "", e);"
Release-2.4.0,Use Epoll for linux
Release-2.4.0,Update location table
Release-2.4.0,Remove the server from failed list
Release-2.4.0,Notify refresh success message to request dispatcher
Release-2.4.0,Check PS exist or not
Release-2.4.0,Check heartbeat timeout
Release-2.4.0,getPSState(entry.getKey());
Release-2.4.0,Check PS restart or not
Release-2.4.0,private final HashSet<ParameterServerId> refreshingServerSet;
Release-2.4.0,Add it to failed rpc list
Release-2.4.0,Add the server to gray server list
Release-2.4.0,Add it to failed rpc list
Release-2.4.0,Add the server to gray server list
Release-2.4.0,Move from gray server list to failed server list
Release-2.4.0,Handle the RPCS to this server
Release-2.4.0,Submit the schedulable failed get RPCS
Release-2.4.0,Submit new get RPCS
Release-2.4.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.4.0,"If the queue is empty, just return 0"
Release-2.4.0,"If request is not over limit, just submit it"
Release-2.4.0,Submit the schedulable failed get RPCS
Release-2.4.0,Submit new put RPCS
Release-2.4.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.4.0,"LOG.info(""choose put server "" + psIds[index]);"
Release-2.4.0,Check all pending RPCS
Release-2.4.0,Check get channel context
Release-2.4.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.4.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.4.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.4.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.4.0,channelManager.printPools();
Release-2.4.0,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
Release-2.4.0,if(ts - entry.getValue() > requestTimeOut * 2)  {
Release-2.4.0,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
Release-2.4.0,"+ "" milliseconds, close all channels to it"");"
Release-2.4.0,closeChannels(entry.getKey());
Release-2.4.0,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
Release-2.4.0,}
Release-2.4.0,}
Release-2.4.0,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
Release-2.4.0,Remove all pending RPCS
Release-2.4.0,Close all channel to this PS
Release-2.4.0,private Channel getChannel(Location loc) throws Exception {
Release-2.4.0,"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));"
Release-2.4.0,}
Release-2.4.0,private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {
Release-2.4.0,"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext"
Release-2.4.0,.get()
Release-2.4.0,.getConf()
Release-2.4.0,".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,"
Release-2.4.0,AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));
Release-2.4.0,}
Release-2.4.0,Get server id and location for this request
Release-2.4.0,"If location is null, means that the server is not ready"
Release-2.4.0,Get the channel for the location
Release-2.4.0,Check if need get token first
Release-2.4.0,Serialize the request
Release-2.4.0,Send the request
Release-2.4.0,get a channel to server from pool
Release-2.4.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.4.0,request.getContext().setChannelPool(pool);
Release-2.4.0,Allocate the bytebuf and serialize the request
Release-2.4.0,find the partition request context from cache
Release-2.4.0,"updateMatrixCache(request.getPartKey(), response.getPartition());"
Release-2.4.0,"updateMatrixCache(request.getPartKey(), response.getRowsSplit());"
Release-2.4.0,TODO
Release-2.4.0,ServerRow rowSplit = PSAgentContext.get().getMatricesCache()
Release-2.4.0,".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),"
Release-2.4.0,request.getRowIndex());
Release-2.4.0,response.setRowSplit(rowSplit);
Release-2.4.0,"updateMatrixCache(request.getPartKey(), response.getRowSplit());"
Release-2.4.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.4.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.4.0,TODO
Release-2.4.0,"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {"
Release-2.4.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);"
Release-2.4.0,}
Release-2.4.0,"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {"
Release-2.4.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);"
Release-2.4.0,}
Release-2.4.0,"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {"
Release-2.4.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);"
Release-2.4.0,}
Release-2.4.0,Get partitions for this row
Release-2.4.0,Distinct get row requests
Release-2.4.0,Get row splits of this row from the matrix cache first
Release-2.4.0,responseCache.addSubResponse(rowSplit);
Release-2.4.0,"If the row split does not exist in cache, get it from parameter server"
Release-2.4.0,Split the param use matrix partitions
Release-2.4.0,Send request to PSS
Release-2.4.0,Split the matrix oplog according to the matrix partitions
Release-2.4.0,"If need update clock, we should send requests to all partitions"
Release-2.4.0,Send request to PSS
Release-2.4.0,Filter the rowIds which are fetching now
Release-2.4.0,Send the rowIndex to rpc dispatcher and return immediately
Release-2.4.0,"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));"
Release-2.4.0,"LOG.info(""start to request "" + requestId);"
Release-2.4.0,"LOG.info(""start to request "" + requestId);"
Release-2.4.0,Split param use matrix partitons
Release-2.4.0,"If all sub-results are received, just remove request and result cache"
Release-2.4.0,Split this row according the matrix partitions
Release-2.4.0,Set split context
Release-2.4.0,Split this row according the matrix partitions
Release-2.4.0,Set split context
Release-2.4.0,long startTs = System.currentTimeMillis();
Release-2.4.0,"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.4.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
Release-2.4.0,Generate dispatch items and add them to the corresponding queues
Release-2.4.0,Filter the rowIds which are fetching now
Release-2.4.0,Sort the parts by partitionId
Release-2.4.0,Sort partition keys use start column index
Release-2.4.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.4.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.4.0,});
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,Sort the parts by partitionId
Release-2.4.0,Sort partition keys use start column index
Release-2.4.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.4.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.4.0,});
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,Put the row split to the cache(row index to row splits map)
Release-2.4.0,"If all splits of the row are received, means this row can be merged"
Release-2.4.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-2.4.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-2.4.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-2.4.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-2.4.0,TODO
Release-2.4.0,TODO
Release-2.4.0,/////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,TODO
Release-2.4.0,buf.writeDouble(0);
Release-2.4.0,TODO
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,Now we just support pipelined row splits merging for dense type row
Release-2.4.0,Pre-fetching is disable default
Release-2.4.0,matrix id to clock map
Release-2.4.0,"task index, it must be unique for whole application"
Release-2.4.0,Deserialize data splits meta
Release-2.4.0,Get workers
Release-2.4.0,Send request to every ps
Release-2.4.0,Wait the responses
Release-2.4.0,Update clock cache
Release-2.4.0,if(syncNum % 1024 == 0) {
Release-2.4.0,}
Release-2.4.0,"Use simple flow, do not use any cache"
Release-2.4.0,Get row from cache.
Release-2.4.0,"if row clock is satisfy ssp staleness limit, just return."
Release-2.4.0,Get row from ps.
Release-2.4.0,Wait until the clock value of this row is greater than or equal to the value
Release-2.4.0,"For ASYNC mode, just get from pss."
Release-2.4.0,"For BSP/SSP, get rows from storage/cache first"
Release-2.4.0,Get from ps.
Release-2.4.0,Wait until the clock value of this row is greater than or equal to the value
Release-2.4.0,"For ASYNC, just get rows from pss."
Release-2.4.0,no more retries.
Release-2.4.0,calculate sleep time and return.
Release-2.4.0,parse the i-th sleep-time
Release-2.4.0,parse the i-th number-of-retries
Release-2.4.0,calculateSleepTime may overflow.
Release-2.4.0,"A few common retry policies, with no delays."
Release-2.4.0,Read matrix meta from meta file
Release-2.4.0,Save partitions to files use fork-join
Release-2.4.0,Write the ps matrix meta to the meta file
Release-2.4.0,matrix.startServering();
Release-2.4.0,return;
Release-2.4.0,Read matrix meta from meta file
Release-2.4.0,Load partitions from file use fork-join
Release-2.4.0,Read matrix meta from meta file
Release-2.4.0,Sort partitions
Release-2.4.0,TODO:
Release-2.4.0,int size = rows.length;
Release-2.4.0,int size = rows.length;
Release-2.4.0,int size = rows.size();
Release-2.4.0,int size = rows.size();
Release-2.4.0,int size = rows.size();
Release-2.4.0,int size = rows.size();
Release-2.4.0,int size = rows.size();
Release-2.4.0,int size = rows.size();
Release-2.4.0,close is a local operation and should finish within milliseconds; timeout just to be safe
Release-2.4.0,response will be null for one way messages.
Release-2.4.0,maxFrameLength = 2G
Release-2.4.0,lengthFieldOffset = 0
Release-2.4.0,lengthFieldLength = 8
Release-2.4.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
Release-2.4.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
Release-2.4.0,"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())"
Release-2.4.0,.toString();
Release-2.4.0,indicates whether this connection's life cycle is managed
Release-2.4.0,See if we already have a connection (common case)
Release-2.4.0,create a unique lock for this RS + protocol (if necessary)
Release-2.4.0,get the RS lock
Release-2.4.0,do one more lookup in case we were stalled above
Release-2.4.0,Only create isa when we need to.
Release-2.4.0,definitely a cache miss. establish an RPC for
Release-2.4.0,this RS
Release-2.4.0,Throw what the RemoteException was carrying.
Release-2.4.0,check
Release-2.4.0,every
Release-2.4.0,minutes
Release-2.4.0,TODO
Release-2.4.0,创建failoverHandler
Release-2.4.0,"The number of times this invocation handler has ever been failed over,"
Release-2.4.0,before this method invocation attempt. Used to prevent concurrent
Release-2.4.0,failed method invocations from triggering multiple failover attempts.
Release-2.4.0,Make sure that concurrent failed method invocations
Release-2.4.0,only cause a
Release-2.4.0,single actual fail over.
Release-2.4.0,RpcController + Message in the method args
Release-2.4.0,(generated code from RPC bits in .proto files have
Release-2.4.0,RpcController)
Release-2.4.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
Release-2.4.0,+ (System.currentTimeMillis() - beforeConstructTs));
Release-2.4.0,get an instance of the method arg type
Release-2.4.0,RpcController + Message in the method args
Release-2.4.0,(generated code from RPC bits in .proto files have
Release-2.4.0,RpcController)
Release-2.4.0,Message (hand written code usually has only a single
Release-2.4.0,argument)
Release-2.4.0,log any RPC responses that are slower than the configured
Release-2.4.0,warn
Release-2.4.0,response time or larger than configured warning size
Release-2.4.0,"when tagging, we let TooLarge trump TooSmall to keep"
Release-2.4.0,output simple
Release-2.4.0,note that large responses will often also be slow.
Release-2.4.0,provides a count of log-reported slow responses
Release-2.4.0,RpcController + Message in the method args
Release-2.4.0,(generated code from RPC bits in .proto files have
Release-2.4.0,RpcController)
Release-2.4.0,unexpected
Release-2.4.0,"in the protobuf methods, args[1] is the only significant argument"
Release-2.4.0,for JSON encoding
Release-2.4.0,base information that is reported regardless of type of call
Release-2.4.0,Disable Nagle's Algorithm since we don't want packets to wait
Release-2.4.0,Configure the event pipeline factory.
Release-2.4.0,Make a new connection.
Release-2.4.0,Remove all pending requests (will be canceled after relinquishing
Release-2.4.0,write lock).
Release-2.4.0,Cancel any pending requests by sending errors to the callbacks:
Release-2.4.0,Close the channel:
Release-2.4.0,Close the connection:
Release-2.4.0,Shut down all thread pools to exit.
Release-2.4.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
Release-2.4.0,See NettyServer.prepareResponse for where we write out the response.
Release-2.4.0,"It writes the call.id (int), a boolean signifying any error (and if"
Release-2.4.0,"so the exception name/trace), and the response bytes"
Release-2.4.0,Read the call id.
Release-2.4.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
Release-2.4.0,"instead, it returns a null message object."
Release-2.4.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
Release-2.4.0,System.currentTimeMillis());
Release-2.4.0,"It would be good widen this to just Throwable, but IOException is what we"
Release-2.4.0,allow now
Release-2.4.0,not implemented
Release-2.4.0,not implemented
Release-2.4.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
Release-2.4.0,cache of RpcEngines by protocol
Release-2.4.0,return the RpcEngine configured to handle a protocol
Release-2.4.0,We only handle the ConnectException.
Release-2.4.0,This is the exception we can't handle.
Release-2.4.0,check if timed out
Release-2.4.0,wait for retry
Release-2.4.0,IGNORE
Release-2.4.0,return the RpcEngine that handles a proxy object
Release-2.4.0,The default implementation works synchronously
Release-2.4.0,punt: allocate a new buffer & copy into it
Release-2.4.0,Parse cmd parameters
Release-2.4.0,load hadoop configuration
Release-2.4.0,load angel system configuration
Release-2.4.0,load user configuration:
Release-2.4.0,load user config file
Release-2.4.0,load command line parameters
Release-2.4.0,load user job resource files
Release-2.4.0,load ml conf file for graph based algorithm
Release-2.4.0,load user job jar if it exist
Release-2.4.0,Expand the environment variable
Release-2.4.0,Add default fs(local fs) for lib jars.
Release-2.4.0,"LOG.info(System.getProperty(""user.dir""));"
Release-2.4.0,get tokens for all the required FileSystems..
Release-2.4.0,Whether we need to recursive look into the directory structure
Release-2.4.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.4.0,user provided one (if any).
Release-2.4.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.4.0,get tokens for all the required FileSystems..
Release-2.4.0,Whether we need to recursive look into the directory structure
Release-2.4.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.4.0,user provided one (if any).
Release-2.4.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.4.0,a simple hdfs copy function assume src path and dest path are in same hdfs
Release-2.4.0,and FileSystem object has same schema
Release-2.4.0,"If out path exist , just remove it first"
Release-2.4.0,Create parent directory if not exist
Release-2.4.0,Rename
Release-2.4.0,"LOG.warn(""interrupted while sleeping"", ie);"
Release-2.4.0,public static String getHostname() {
Release-2.4.0,try {
Release-2.4.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
Release-2.4.0,} catch (UnknownHostException uhe) {
Release-2.4.0,}
Release-2.4.0,"return new StringBuilder().append("""").append(uhe).toString();"
Release-2.4.0,}
Release-2.4.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
Release-2.4.0,String hostname = getHostname();
Release-2.4.0,String classname = clazz.getSimpleName();
Release-2.4.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
Release-2.4.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
Release-2.4.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
Release-2.4.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
Release-2.4.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
Release-2.4.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
Release-2.4.0,
Release-2.4.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
Release-2.4.0,public void run() {
Release-2.4.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
Release-2.4.0,"this.val$classname + "" at "" + this.val$hostname}));"
Release-2.4.0,}
Release-2.4.0,});
Release-2.4.0,}
Release-2.4.0,"We we interrupted because we're meant to stop? If not, just"
Release-2.4.0,continue ignoring the interruption
Release-2.4.0,Recalculate waitTime.
Release-2.4.0,// Begin delegation to Thread
Release-2.4.0,// End delegation to Thread
Release-2.4.0,instance submitter class
Release-2.4.0,Obtain filename from path
Release-2.4.0,Split filename to prexif and suffix (extension)
Release-2.4.0,Check if the filename is okay
Release-2.4.0,Prepare temporary file
Release-2.4.0,Prepare buffer for data copying
Release-2.4.0,Open and check input stream
Release-2.4.0,Open output stream and copy data between source file in JAR and the temporary file
Release-2.4.0,"If read/write fails, close streams safely before throwing an exception"
Release-2.4.0,"Finally, load the library"
Release-2.4.0,little endian load order
Release-2.4.0,tail
Release-2.4.0,fallthrough
Release-2.4.0,fallthrough
Release-2.4.0,finalization
Release-2.4.0,fmix(h1);
Release-2.4.0,----------
Release-2.4.0,body
Release-2.4.0,----------
Release-2.4.0,tail
Release-2.4.0,----------
Release-2.4.0,finalization
Release-2.4.0,----------
Release-2.4.0,body
Release-2.4.0,----------
Release-2.4.0,tail
Release-2.4.0,----------
Release-2.4.0,finalization
Release-2.4.0,throw new AngelException(e);
Release-2.4.0,JobStateProto jobState = report.getJobState();
Release-2.4.0,Check need load matrices
Release-2.4.0,Used for java code to get a AngelClient instance
Release-2.4.0,Used for python code to get a AngelClient instance
Release-2.4.0,load user job resource files
Release-2.4.0,setLocalAddr();
Release-2.4.0,2.get job id
Release-2.4.0,5.write configuration to a xml file
Release-2.4.0,8.get app master client
Release-2.4.0,Write job file to JobTracker's fs
Release-2.4.0,the leaf level file should be readable by others
Release-2.4.0,the subdirs in the path should have execute permissions for
Release-2.4.0,others
Release-2.4.0,2.get job id
Release-2.4.0,Credentials credentials = new Credentials();
Release-2.4.0,4.copy resource files to hdfs
Release-2.4.0,5.write configuration to a xml file
Release-2.4.0,6.create am container context
Release-2.4.0,7.Submit to ResourceManager
Release-2.4.0,8.get app master client
Release-2.4.0,Create a number of filenames in the JobTracker's fs namespace
Release-2.4.0,add all the command line files/ jars and archive
Release-2.4.0,first copy them to jobtrackers filesystem
Release-2.4.0,should not throw a uri exception
Release-2.4.0,should not throw an uri excpetion
Release-2.4.0,set the timestamps of the archives and files
Release-2.4.0,set the public/private visibility of the archives and files
Release-2.4.0,get DelegationToken for each cached file
Release-2.4.0,check if we do not need to copy the files
Release-2.4.0,is jt using the same file system.
Release-2.4.0,just checking for uri strings... doing no dns lookups
Release-2.4.0,to see if the filesystems are the same. This is not optimal.
Release-2.4.0,but avoids name resolution.
Release-2.4.0,this might have name collisions. copy will throw an exception
Release-2.4.0,parse the original path to create new path
Release-2.4.0,check for ports
Release-2.4.0,Write job file to JobTracker's fs
Release-2.4.0,Setup resource requirements
Release-2.4.0,Setup LocalResources
Release-2.4.0,Setup security tokens
Release-2.4.0,Setup the command to run the AM
Release-2.4.0,Add AM user command opts
Release-2.4.0,Final command
Release-2.4.0,Setup the CLASSPATH in environment
Release-2.4.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
Release-2.4.0,Setup the environment variables for Admin first
Release-2.4.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
Release-2.4.0,Parse distributed cache
Release-2.4.0,Setup ContainerLaunchContext for AM container
Release-2.4.0,Set up the ApplicationSubmissionContext
Release-2.4.0,private volatile PS2PSPusherImpl ps2PSPusher;
Release-2.4.0,TODO
Release-2.4.0,Add tokens to new user so that it may execute its task correctly.
Release-2.4.0,TODO
Release-2.4.0,to exit
Release-2.4.0,TODO
Release-2.4.0,TODO
Release-2.4.0,ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));
Release-2.4.0,context.getSnapshotManager().processRecovery();
Release-2.4.0,Recover PS from snapshot or load path
Release-2.4.0,1. First check old snapshot
Release-2.4.0,2. Check new checkpoints
Release-2.4.0,3. Check load path setting and old save result
Release-2.4.0,Just init it again
Release-2.4.0,TODO
Release-2.4.0,if(ps2PSPusher != null) {
Release-2.4.0,ps2PSPusher.start();
Release-2.4.0,}
Release-2.4.0,public PS2PSPusherImpl getPs2PSPusher() {
Release-2.4.0,return ps2PSPusher;
Release-2.4.0,}
Release-2.4.0,Filter the head
Release-2.4.0,Get the RPC destination
Release-2.4.0,Get and init the queue
Release-2.4.0,"If the queue is empty, activate the processor"
Release-2.4.0,Just put it to the rpc queue
Release-2.4.0,if(useInDepWorkers) {
Release-2.4.0,Use independent rpc workers
Release-2.4.0,if (method == TransportMethod.GET_CLOCKS || method == TransportMethod.UPDATE_CLOCK) {
Release-2.4.0,"Small rpc request, use sync to avoid thread switch"
Release-2.4.0,return false;
Release-2.4.0,}
Release-2.4.0,return true;
Release-2.4.0,} else {
Release-2.4.0,return false;
Release-2.4.0,}
Release-2.4.0,if (!useSync && useAyncHandler) {
Release-2.4.0,"senderPool.execute(new Sender(clientId, seqId, method, ctx, result));"
Release-2.4.0,} else {
Release-2.4.0,"send(clientId, seqId, method, ctx, result);"
Release-2.4.0,}
Release-2.4.0,Release the input buffer
Release-2.4.0,Release the input buffer
Release-2.4.0,"1. handle the rpc, get the response"
Release-2.4.0,Release the input buffer
Release-2.4.0,2. Serialize the response
Release-2.4.0,Send the serialized response
Release-2.4.0,Exception happened
Release-2.4.0,write seq id
Release-2.4.0,Just serialize the head
Release-2.4.0,Exception happened
Release-2.4.0,Allocate result buffer
Release-2.4.0,Exception happened
Release-2.4.0,Just serialize the head
Release-2.4.0,Exception happened
Release-2.4.0,runningContext.printToken();
Release-2.4.0,Reset the response and allocate buffer again
Release-2.4.0,Get partition and check the partition state
Release-2.4.0,Get the stored pss for this partition
Release-2.4.0,"Check this ps is the master ps for this location, only master ps can accept the update"
Release-2.4.0,Check the partition state again
Release-2.4.0,Start to put the update to the slave pss
Release-2.4.0,TODO
Release-2.4.0,"context.getPS2PSPusher().put(request, in, partLoc);"
Release-2.4.0,Get partition and check the partition state
Release-2.4.0,Get the stored pss for this partition
Release-2.4.0,"Check this ps is the master ps for this partition, if not, just return failed"
Release-2.4.0,Start to put the update to the slave pss
Release-2.4.0,TODO
Release-2.4.0,"int maxRPCCounter = Math.max(estSize, (int) (workerNum * factor));"
Release-2.4.0,"for (Map.Entry<Integer, ClientRunningContext> clientEntry : clientRPCCounters.entrySet()) {"
Release-2.4.0,"LOG.info(""client "" + clientEntry.getKey() + "" running context:"");"
Release-2.4.0,clientEntry.getValue().printToken();
Release-2.4.0,}
Release-2.4.0,return ServerState.GENERAL;
Release-2.4.0,Use Epoll for linux
Release-2.4.0,public String uuid;
Release-2.4.0,TODO:
Release-2.4.0,part = new ServerPartition();
Release-2.4.0,TODO:
Release-2.4.0,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
Release-2.4.0,this.channelPool = channelPool;
Release-2.4.0,}
Release-2.4.0,private final ParameterServer psServer;
Release-2.4.0,Create and start workers
Release-2.4.0,Set workers
Release-2.4.0,Create and start workers
Release-2.4.0,Set workers
Release-2.4.0,"If matrix checkpoint path not exist, just return null"
Release-2.4.0,Return the path with maximum checkpoint id
Release-2.4.0,Rename temp to item path
Release-2.4.0,Checkpoint base path = Base dir/matrix name
Release-2.4.0,Path for this checkpoint
Release-2.4.0,Generate tmp path
Release-2.4.0,Delete old checkpoints
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.4.0,and call endWrite/endRead after
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.4.0,and call endWrite/endRead after
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"////// network io method, for model transform"
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,Serailize the head
Release-2.4.0,Serialize the storage
Release-2.4.0,Deserailze the head
Release-2.4.0,Deseralize the storage
Release-2.4.0,Serailize the head
Release-2.4.0,Serialize the storage
Release-2.4.0,Deserailze the head
Release-2.4.0,Deseralize the storage
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.4.0,and call endWrite/endRead after
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.4.0,and call endWrite/endRead after
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.4.0,and call endWrite/endRead after
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.4.0,and call endWrite/endRead after
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.4.0,and call endWrite/endRead after
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.4.0,and call endWrite/endRead after
Release-2.4.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,"Notice: Basic type ServerRow only support Vector as inner storage now, so you can use methods"
Release-2.4.0,to get inner vector for basic type ServerRow.
Release-2.4.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.4.0,Just update the exist element now!!
Release-2.4.0,Just update the exist element now!!
Release-2.4.0,TODO: just check the value is 0 or not now
Release-2.4.0,TODO: just check the value is zero or not now
Release-2.4.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-2.4.0,Attention: Only update the exist values for sorted storage method
Release-2.4.0,Attention: Only update exist element
Release-2.4.0,Attention: Only update the exist values for sorted storage method
Release-2.4.0,Attention: Only update exist element
Release-2.4.0,TODO: just check the value is zero or not now
Release-2.4.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-2.4.0,Valid element number
Release-2.4.0,Element data
Release-2.4.0,Valid element number
Release-2.4.0,Deserialize the data
Release-2.4.0,Element data
Release-2.4.0,Valid element number
Release-2.4.0,Element data
Release-2.4.0,Valid element number
Release-2.4.0,Deserialize the data
Release-2.4.0,Attention: Only update the exist values for sorted storage method
Release-2.4.0,Attention: Only update exist element
Release-2.4.0,TODO: just check the value is zero or not now
Release-2.4.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-2.4.0,TODO: just check the value is 0 or not now
Release-2.4.0,Just update the exist element now!!
Release-2.4.0,Just update the exist element now!!
Release-2.4.0,"Use sparse storage method, as some elements in the array maybe null"
Release-2.4.0,Array length
Release-2.4.0,Valid element number
Release-2.4.0,Element data
Release-2.4.0,Array len
Release-2.4.0,Valid element number
Release-2.4.0,"Use sparse storage method, as some elements in the array maybe null"
Release-2.4.0,Array length
Release-2.4.0,Valid element number
Release-2.4.0,Element data
Release-2.4.0,Element data
Release-2.4.0,Array len
Release-2.4.0,Valid element number
Release-2.4.0,Attention: Only update the exist values for sorted storage method
Release-2.4.0,Attention: Only update exist element
Release-2.4.0,TODO: just check the value is zero or not now
Release-2.4.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-2.4.0,Row type
Release-2.4.0,Storage method
Release-2.4.0,Key type
Release-2.4.0,Value type
Release-2.4.0,Vector dim
Release-2.4.0,Vector length
Release-2.4.0,Vector data
Release-2.4.0,Row type
Release-2.4.0,Storage method
Release-2.4.0,Key type
Release-2.4.0,Value type
Release-2.4.0,Vector dim
Release-2.4.0,Vector length
Release-2.4.0,Init the vector
Release-2.4.0,Vector data
Release-2.4.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.4.0,Get the array pair
Release-2.4.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.4.0,Get the array pair
Release-2.4.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.4.0,Get the array pair
Release-2.4.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.4.0,Get the array pair
Release-2.4.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.4.0,Get the array pair
Release-2.4.0,Impossible now
Release-2.4.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.4.0,Get the array pair
Release-2.4.0,Impossible now
Release-2.4.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.4.0,Get the array pair
Release-2.4.0,Impossible now
Release-2.4.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.4.0,Get the array pair
Release-2.4.0,Impossible now
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,"If use sorted storage, we should get the array pair first"
Release-2.4.0,Just update the exist element now!!
Release-2.4.0,Just update the exist element now!!
Release-2.4.0,TODO: just check the value is 0 or not now
Release-2.4.0,Just update the exist element now!!
Release-2.4.0,Just update the exist element now!!
Release-2.4.0,TODO: just check the value is 0 or not now
Release-2.4.0,Valid element number
Release-2.4.0,Element data
Release-2.4.0,Valid element number
Release-2.4.0,Deserialize the data
Release-2.4.0,Element data
Release-2.4.0,Valid element number
Release-2.4.0,Element data
Release-2.4.0,Valid element number
Release-2.4.0,Deserialize the data
Release-2.4.0,private final List<PartitionKey> partitionKeys;
Release-2.4.0,Get server partition class
Release-2.4.0,"If partition class is not set, just use the default partition class"
Release-2.4.0,Get server partition storage class type
Release-2.4.0,Get value class
Release-2.4.0,"if col == -1, we use the start/end index to calculate range,"
Release-2.4.0,we use double to store the range value since two long minus might exceed the
Release-2.4.0,range of long.
Release-2.4.0,Serialize the head
Release-2.4.0,Serialize the storage
Release-2.4.0,Deserialize the head
Release-2.4.0,Deseralize the storage
Release-2.4.0,Serialize the head
Release-2.4.0,Serialize the storage
Release-2.4.0,Deserialize the head
Release-2.4.0,Deseralize the storage
Release-2.4.0,Row base partition
Release-2.4.0,"If storage class is not set, use default DenseServerRowsStorage"
Release-2.4.0,Serialize values
Release-2.4.0,Deserialize values
Release-2.4.0,Array size
Release-2.4.0,Actual write size
Release-2.4.0,Rows data
Release-2.4.0,Row id
Release-2.4.0,Row type
Release-2.4.0,Row data
Release-2.4.0,Array size
Release-2.4.0,Actual write row number
Release-2.4.0,Rows data
Release-2.4.0,Row id
Release-2.4.0,Create empty server row
Release-2.4.0,Row data
Release-2.4.0,Rows data
Release-2.4.0,TODO
Release-2.4.0,Serialize row offsets
Release-2.4.0,Serialize column offsets
Release-2.4.0,Deserialize row offset
Release-2.4.0,Deserialize row offset
Release-2.4.0,"If storage is set, just get a instance"
Release-2.4.0,"If storage is not set, use default"
Release-2.4.0,"If storage is set, just get a instance"
Release-2.4.0,"If storage is not set, use default"
Release-2.4.0,Map size
Release-2.4.0,Actual write size
Release-2.4.0,Rows data
Release-2.4.0,Row id
Release-2.4.0,Row type
Release-2.4.0,Row data
Release-2.4.0,Array size
Release-2.4.0,Actual write row number
Release-2.4.0,Rows data
Release-2.4.0,Row id
Release-2.4.0,Create empty server row
Release-2.4.0,Row data
Release-2.4.0,Rows data
Release-2.4.0,Use Epoll for linux
Release-2.4.0,find the partition request context from cache
Release-2.4.0,get a channel to server from pool
Release-2.4.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.4.0,channelManager.removeChannelPool(loc);
Release-2.4.0,Generate seq id
Release-2.4.0,Create a RecoverPartRequest
Release-2.4.0,Serialize the request
Release-2.4.0,Change the seqId for the request
Release-2.4.0,Serialize the request
Release-2.4.0,"First check the state of the channels in the pool, if a channel is unused, just return"
Release-2.4.0,"If all channels are in use, create a new channel or wait"
Release-2.4.0,Create a new channel
Release-2.4.0,"add the PSAgentContext,need fix"
Release-2.4.0,If col == -1 and start/end not set
Release-2.4.0,start/end set
Release-2.4.0,"for dense type, we need to set the colNum to set dim for vectors"
Release-2.4.0,"colNum set, start/end not set"
Release-2.4.0,Row number must > 0
Release-2.4.0,"both set, check its valid"
Release-2.4.0,public static final int T_INT_ARBITRARY_VALUE = 28;
Release-2.4.0,public static final int T_INVALID_VALUE = 29;
Release-2.4.0,TODO:add more vector type
Release-2.4.0,TODO : subDim set
Release-2.4.0,Sort the parts by partitionId
Release-2.4.0,Sort partition keys use start column index
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,Sort the parts by partitionId
Release-2.4.0,Sort partition keys use start column index
Release-2.4.0,"For each partition, we generate a update split."
Release-2.4.0,"Although the split is empty for partitions those without any update data,"
Release-2.4.0,we still need to generate a update split to update the clock info on ps.
Release-2.4.0,Split updates
Release-2.4.0,Shuffle update splits
Release-2.4.0,Generate part update parameters
Release-2.4.0,"Set split context: partition key, use int key for long key vector or not ect"
Release-2.4.0,write the max abs
Release-2.4.0,---------------------------------------------------
Release-2.4.0,---------------------------------------------------
Release-2.4.0,---------------------------------------------------------------
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,axis = 0: on rows
Release-2.4.0,axis = 1: on cols
Release-2.4.0,1. find the insert point
Release-2.4.0,2. check the capacity and insert
Release-2.4.0,3. increase size
Release-2.4.0,1. find the insert point
Release-2.4.0,2. check the capacity and insert
Release-2.4.0,3. increase size
Release-2.4.0,1. find the insert point
Release-2.4.0,2. check the capacity and insert
Release-2.4.0,3. increase size
Release-2.4.0,1. find the insert point
Release-2.4.0,2. check the capacity and insert
Release-2.4.0,3. increase size
Release-2.4.0,1. find the insert point
Release-2.4.0,2. check the capacity and insert
Release-2.4.0,3. increase size
Release-2.4.0,1. find the insert point
Release-2.4.0,2. check the capacity and insert
Release-2.4.0,3. increase size
Release-2.4.0,1. find the insert point
Release-2.4.0,2. check the capacity and insert
Release-2.4.0,3. increase size
Release-2.4.0,1. find the insert point
Release-2.4.0,2. check the capacity and insert
Release-2.4.0,3. increase size
Release-2.4.0,-----------------
Release-2.4.0,-----------------
Release-2.4.0,-----------------
Release-2.4.0,-----------------
Release-2.4.0,-----------------
Release-2.4.0,KeepStorage is guaranteed
Release-2.4.0,"ignore the isInplace option, since v2 is dense"
Release-2.4.0,"the value in old storage can be changed safe, so switch a storage"
Release-2.4.0,"but user required keep storage, we can prevent rehash"
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,KeepStorage is guaranteed
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,v1Size < v2Size * Constant.sparseThreshold
Release-2.4.0,KeepStorage is guaranteed
Release-2.4.0,"ignore the isInplace option, since v2 is dense"
Release-2.4.0,"the value in old storage can be changed safe, so switch a storage"
Release-2.4.0,"but user required keep storage, we can prevent rehash"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,prevent rehash
Release-2.4.0,KeepStorage is guaranteed
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,dense preferred
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,sorted preferred
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,multi-rehash
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"dense preferred, KeepStorage is guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,"sparse preferred, keep storage guaranteed"
Release-2.4.0,preferred dense
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,we gauss dense storage is more efficient
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.4.0,multi-rehash
Release-2.4.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,to avoid multi-rehash
Release-2.4.0,"Transform mat1, generate a new matrix"
Release-2.4.0,Split the row indices of mat1Trans
Release-2.4.0,Parallel execute use fork-join
Release-2.4.0,"Get the sub-matrix of left matrix, split by row"
Release-2.4.0,"Transform mat1, generate a new matrix"
Release-2.4.0,Split the row indices of mat1Trans
Release-2.4.0,Parallel execute use fork-join
Release-2.4.0,"Get the sub-matrix of left matrix, split by row"
Release-2.4.0,"mat1 trans true, mat trans true"
Release-2.4.0,"mat1 trans true, mat trans false"
Release-2.4.0,"mat1 trans false, mat trans true, important"
Release-2.4.0,"mat1 trans false, mat trans false"
Release-2.4.0,"mat1 trans true, mat trans true"
Release-2.4.0,"mat1 trans true, mat trans false"
Release-2.4.0,"mat1 trans false, mat trans true, important"
Release-2.4.0,"mat1 trans false, mat trans false"
Release-2.4.0,"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.4.0,"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.4.0,not the first time
Release-2.4.0,first time and do the sample
Release-2.4.0,set to zero
Release-2.4.0,not the first time
Release-2.4.0,first time and do the sample
Release-2.4.0,set to zero
Release-2.4.0,not the first time
Release-2.4.0,first time and do the sample
Release-2.4.0,set to zero
Release-2.4.0,not the first time
Release-2.4.0,first time and do the sample
Release-2.4.0,set to zero
Release-2.4.0,not the first time
Release-2.4.0,first time and do the sample
Release-2.4.0,set to zero
Release-2.4.0,not the first time
Release-2.4.0,first time and do the sample
Release-2.4.0,set to zero
Release-2.4.0,not the first time
Release-2.4.0,first time and do the sample
Release-2.4.0,set to zero
Release-2.4.0,not the first time
Release-2.4.0,first time and do the sample
Release-2.4.0,set to zero
Release-2.4.0,not the first time
Release-2.4.0,first time and do the sample
Release-2.4.0,set to zero
Release-2.4.0,not the first time
Release-2.4.0,first time and do the sample
Release-2.4.0,set to zero
Release-2.4.0,get configuration from envs
Release-2.4.0,get master location
Release-2.4.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.4.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.4.0,add dense double matrix
Release-2.4.0,TODO Auto-generated method stub
Release-2.4.0,TODO Auto-generated method stub
Release-2.4.0,TODO Auto-generated method stub
Release-2.4.0,get configuration from config file
Release-2.4.0,set localDir with enviroment set by nm.
Release-2.4.0,get master location
Release-2.4.0,init task manager and start tasks
Release-2.4.0,start heartbeat thread
Release-2.4.0,taskManager.assignTaskIds(response.getTaskidsList());
Release-2.4.0,todo
Release-2.4.0,"if worker timeout, it may be knocked off."
Release-2.4.0,"SUCCESS, do nothing"
Release-2.4.0,heartbeatFailedTime = 0;
Release-2.4.0,private KEY currentKey;
Release-2.4.0,will be created
Release-2.4.0,TODO Auto-generated method stub
Release-2.4.0,Bitmap bitmap = new Bitmap();
Release-2.4.0,int max = indexArray[size - 1];
Release-2.4.0,byte [] bitIndexArray = new byte[max / 8 + 1];
Release-2.4.0,for(int i = 0; i < size; i++){
Release-2.4.0,int bitIndex = indexArray[i] >> 3;
Release-2.4.0,int bitOffset = indexArray[i] - (bitIndex << 3);
Release-2.4.0,switch(bitOffset){
Release-2.4.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
Release-2.4.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
Release-2.4.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
Release-2.4.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
Release-2.4.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
Release-2.4.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
Release-2.4.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
Release-2.4.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
Release-2.4.0,}
Release-2.4.0,}
Release-2.4.0,"true, false"
Release-2.4.0,//////////////////////////////
Release-2.4.0,Application Configs
Release-2.4.0,//////////////////////////////
Release-2.4.0,//////////////////////////////
Release-2.4.0,Master Configs
Release-2.4.0,//////////////////////////////
Release-2.4.0,//////////////////////////////
Release-2.4.0,Worker Configs
Release-2.4.0,//////////////////////////////
Release-2.4.0,//////////////////////////////
Release-2.4.0,Task Configs
Release-2.4.0,//////////////////////////////
Release-2.4.0,//////////////////////////////
Release-2.4.0,ParameterServer Configs
Release-2.4.0,//////////////////////////////
Release-2.4.0,//////////////////////////////
Release-2.4.0,Kubernetes Configs.
Release-2.4.0,//////////////////////////////
Release-2.4.0,////////////////// IPC //////////////////////////
Release-2.4.0,//////////////////////////////
Release-2.4.0,Matrix transfer Configs.
Release-2.4.0,//////////////////////////////
Release-2.4.0,//////////////////////////////
Release-2.4.0,Matrix transfer Configs.
Release-2.4.0,//////////////////////////////
Release-2.4.0,Configs used to ANGEL_PS_PSAGENT running mode future.
Release-2.4.0,model parse
Release-2.4.0,Mark whether use pyangel or not.
Release-2.4.0,private Configuration conf;
Release-2.4.0,"Configuration that should be used in python environment, there should only be one"
Release-2.4.0,configuration instance in each Angel context.
Release-2.4.0,Use private access means jconf should not be changed or modified in this way.
Release-2.4.0,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
Release-2.4.0,Do nothing
Release-2.4.0,To-DO: add other ways to justify different value types
Release-2.4.0,"This is so ugly, must re-implement by more elegance way"
Release-2.4.0,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
Release-2.4.0,and other files submitted by user.
Release-2.4.0,Launch python process
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-2.4.0,get a angel client
Release-2.4.0,add sparse float matrix
Release-2.4.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-2.4.0,siMat.setPartitionClass(CSRPartition.class);
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,Init node neighbors
Release-2.4.0,client.asyncUpdate(new InitNeighborOver(new InitNeighborOverParam(matrixId))).get();
Release-2.4.0,Sample the neighbors
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-2.4.0,get a angel client
Release-2.4.0,add sparse float matrix
Release-2.4.0,siMat.setValidIndexNum(100);
Release-2.4.0,siMat.setColNum(10000000000L);
Release-2.4.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-2.4.0,siMat.setPartitionClass(CSRPartition.class);
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,Init node neighbors and feats
Release-2.4.0,Sample the neighbors
Release-2.4.0,TODO Auto-generated constructor stub
Release-2.4.0,set basic configuration keys
Release-2.4.0,use local deploy mode and dummy dataspliter
Release-2.4.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-2.4.0,get a angel client
Release-2.4.0,add sparse float matrix
Release-2.4.0,Start PS
Release-2.4.0,Start to run application
Release-2.4.0,Init node neighbors
Release-2.4.0,Sample the neighbors
Release-2.4.0,sample continuously beginning from a random index
Release-2.4.0,Get node neighbor number
Release-2.4.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-2.4.0,Get node neighbor number
Release-2.4.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-2.4.0,"ServerLongAnyRow row = (ServerLongAnyRow) psContext.getMatrixStorageManager().getRow(pparam.getPartKey(), 0);"
Release-2.4.0,ObjectIterator<Long2ObjectMap.Entry<IElement>> iter = row.iterator();
Release-2.4.0,while (iter.hasNext()) {
Release-2.4.0,Long2ObjectMap.Entry<IElement> entry = iter.next();
Release-2.4.0,long key = entry.getLongKey() + pparam.getPartKey().getStartCol();
Release-2.4.0,WalkPath value = (WalkPath) entry.getValue();
Release-2.4.0,
Release-2.4.0,if (workerPartitionId == value.getNextPartitionIdx()) {
Release-2.4.0,"result.put(key, value.getTail2());"
Release-2.4.0,}
Release-2.4.0,}
Release-2.4.0,"int matrixId, PartitionKey partKey, long[] keyIds, int startIdx, int endIdx"
Release-2.4.0,"System.out.println(""PathQueue: put data to queue"");"
Release-2.4.0,"System.out.println(""queue.size: "" + queue.size());"
Release-2.4.0,"System.out.println(""CurrPathIdx of "" + wPath.getHead() + "" is "" + wPath.getCurrPathIdx());"
Release-2.4.0,if (numRetry == retry) {
Release-2.4.0,"System.out.println(""retried 3 time, got : "" + result.size());"
Release-2.4.0,}
Release-2.4.0,"System.out.println(""popBatch: "" + result.size() +"" | ""+ count);"
Release-2.4.0,"getRow(partKey.getMatrixId(), rowId, partKey.getPartitionId())"
Release-2.4.0,StringBuilder sb = new StringBuilder();
Release-2.4.0,"sb.append(key).append("" -> {"");"
Release-2.4.0,for (long n: neighbor) {
Release-2.4.0,"sb.append(n).append("", "");"
Release-2.4.0,}
Release-2.4.0,"sb.append(""} : "").append(neigh);"
Release-2.4.0,System.out.println(sb.toString());
Release-2.4.0,"System.out.println(""pushed size: "" + pathTail.size());"
Release-2.4.0,List<LinkedBlockingQueue<WalkPath>> queueList = PathQueue.getQueueList(partKey.getPartitionId());
Release-2.4.0,int p = 0;
Release-2.4.0,for (LinkedBlockingQueue<WalkPath> queue: queueList) {
Release-2.4.0,"System.out.println(""partition "" + p + "", size1 = ""+ pathTail.size() +  "" size2 = "" + queue.size());"
Release-2.4.0,p++;
Release-2.4.0,}
Release-2.4.0,"System.out.println(""pushed batch finished!"");"
Release-2.4.0,Get node neighbor number
Release-2.4.0,"If the neighbor number is 0, just return a int[0]"
Release-2.4.0,"If count <= 0 or the neighbor number is less or equal then count, just copy all neighbors to the result array"
Release-2.4.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-2.4.0,Store the total neighbor number of all nodes in rowOffsets
Release-2.4.0,"Put the node ids, node neighbor number, node neighbors to the cache"
Release-2.4.0,No data in this partition
Release-2.4.0,Get total neighbor number
Release-2.4.0,Final matrix column indices: neighbors node ids
Release-2.4.0,Write positions in cloumnIndices for nodes
Release-2.4.0,Copy all cached sub column indices to final column indices
Release-2.4.0,Read position for a sub column indices
Release-2.4.0,Copy column indices for a node to final column indices
Release-2.4.0,Update write position for this node in final column indices
Release-2.4.0,Update the read position in sub column indices
Release-2.4.0,Clear all temp data
Release-2.4.0,Get node neighbor number
Release-2.4.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-2.4.0,sample happens here to avoid memory copy on servers
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set trainning data path
Release-2.4.0,Set predict result path
Release-2.4.0,Set actionType prediction
Release-2.4.0,Data format
Release-2.4.0,Feature number of train data
Release-2.4.0,Tree number
Release-2.4.0,Tree depth
Release-2.4.0,Split number
Release-2.4.0,Feature sample ratio
Release-2.4.0,Ratio of validation
Release-2.4.0,Learning rate
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Set data format
Release-2.4.0,"Set angel resource, #worker, #task, #PS"
Release-2.4.0,Set GBDT algorithm parameters
Release-2.4.0,Set training data path
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set predict data path
Release-2.4.0,Set load model path
Release-2.4.0,Set predict result path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType prediction
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,Train batch number per epoch.
Release-2.4.0,Batch number
Release-2.4.0,Model type
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Set data format
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,set sgd LR algorithm parameters #feature #epoch
Release-2.4.0,Set trainning data path
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set trainning data path
Release-2.4.0,Set load model path
Release-2.4.0,Set predict result path
Release-2.4.0,Set actionType prediction
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,Train batch number per epoch.
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Set data format
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,set sgd FM algorithm parameters #feature #epoch
Release-2.4.0,Set trainning data path
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set trainning data path
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set trainning data path
Release-2.4.0,Set load model path
Release-2.4.0,Set predict result path
Release-2.4.0,Set actionType prediction
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,Model type
Release-2.4.0,String modelType = String.valueOf(RowType.T_FLOAT_DENSE);
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,set sgd LR algorithm parameters #feature #epoch
Release-2.4.0,"conf.set(MLConf.ML_MODEL_TYPE(), modelType);"
Release-2.4.0,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
Release-2.4.0,predictTest();
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,Set data format
Release-2.4.0,Set trainning data path
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set data format
Release-2.4.0,Set trainning data path
Release-2.4.0,Set load model path
Release-2.4.0,Set predict result path
Release-2.4.0,Set actionType prediction
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,class number
Release-2.4.0,Model type
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Set data format
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,set sgd LR algorithm parameters #feature #epoch
Release-2.4.0,Set log path
Release-2.4.0,Set trainning data path
Release-2.4.0,Set save model path
Release-2.4.0,Set actionType train
Release-2.4.0,Set trainning data path
Release-2.4.0,Set load model path
Release-2.4.0,Set save model path
Release-2.4.0,Set actionType incremental train
Release-2.4.0,Set log path
Release-2.4.0,Set trainning data path
Release-2.4.0,Set load model path
Release-2.4.0,Set predict result path
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set training data path
Release-2.4.0,Set predict result path
Release-2.4.0,Set actionType prediction
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set actionType train
Release-2.4.0,Set trainning data path
Release-2.4.0,Set predict result path
Release-2.4.0,Set actionType prediction
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set trainning data path
Release-2.4.0,Set predict result path
Release-2.4.0,Set actionType prediction
Release-2.4.0,Cluster center number
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Sample ratio per mini-batch
Release-2.4.0,C
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.4.0,Set data format
Release-2.4.0,Set trainning data path
Release-2.4.0,Set save model path
Release-2.4.0,Set log save path
Release-2.4.0,Set actionType train
Release-2.4.0,Set trainning data path
Release-2.4.0,Set load model path
Release-2.4.0,Set save model path
Release-2.4.0,Set actionType incremental train
Release-2.4.0,Set log path
Release-2.4.0,Set testing data path
Release-2.4.0,Set load model path
Release-2.4.0,Set predict result path
Release-2.4.0,Set actionType prediction
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set trainning data path
Release-2.4.0,Set predict result path
Release-2.4.0,Set actionType prediction
Release-2.4.0,Feature number of train data
Release-2.4.0,Total iteration number
Release-2.4.0,Validation sample Ratio
Release-2.4.0,"Data format, libsvm or dummy"
Release-2.4.0,Model type
Release-2.4.0,Learning rate
Release-2.4.0,Decay of learning rate
Release-2.4.0,Regularization coefficient
Release-2.4.0,Set local deploy mode
Release-2.4.0,Set basic configuration keys
Release-2.4.0,Set data format
Release-2.4.0,"set angel resource parameters #worker, #task, #PS"
Release-2.4.0,set sgd LR algorithm parameters #feature #epoch
Release-2.4.0,Set trainning data path
Release-2.4.0,Set save model path
Release-2.4.0,Set log path
Release-2.4.0,Set actionType train
Release-2.4.0,Set trainning data path
Release-2.4.0,Set load model path
Release-2.4.0,Set predict result path
Release-2.4.0,TODO: optimize int key indices
Release-2.4.0,"System.out.println(""deserialize cols.length="" + nCols);"
Release-2.4.0,"System.out.print(""deserialize "");"
Release-2.4.0,"System.out.print(cols[c] + "" "");"
Release-2.4.0,System.out.println();
Release-2.4.0,TODO Auto-generated method stub
Release-2.4.0,"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.4.0,"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.4.0,"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));"
Release-2.4.0,"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));"
Release-2.4.0,"ground truth: positive, precision: positive"
Release-2.4.0,start row index for words
Release-2.4.0,start row index for docs
Release-2.4.0,doc ids
Release-2.4.0,topic assignments
Release-2.4.0,word to docs reverse index
Release-2.4.0,count word
Release-2.4.0,build word start index
Release-2.4.0,build word to doc reverse idx
Release-2.4.0,build dks
Release-2.4.0,dks = new TraverseHashMap[n_docs];
Release-2.4.0,for (int d = 0; d < n_docs; d++) {
Release-2.4.0,if (K < Short.MAX_VALUE) {
Release-2.4.0,if (docs.get(d).len < Byte.MAX_VALUE)
Release-2.4.0,dks[d] = new S2BTraverseMap(docs.get(d).len);
Release-2.4.0,if (docs.get(d).len < Short.MAX_VALUE)
Release-2.4.0,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
Release-2.4.0,else
Release-2.4.0,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
Release-2.4.0,} else {
Release-2.4.0,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
Release-2.4.0,}
Release-2.4.0,}
Release-2.4.0,build dks
Release-2.4.0,allocate update maps
Release-2.4.0,Skip if no token for this word
Release-2.4.0,Check whether error when fetching word-topic
Release-2.4.0,Build FTree for current word
Release-2.4.0,current doc
Release-2.4.0,old topic assignment
Release-2.4.0,"Check if error happens. if this happen, it's probably that failures happen to servers."
Release-2.4.0,We need to adjust the memory settings or network fetching parameters.
Release-2.4.0,Update statistics if needed
Release-2.4.0,Calculate psum and sample new topic
Release-2.4.0,Update statistics if needed
Release-2.4.0,Assign new topic
Release-2.4.0,Skip if no token for this word
Release-2.4.0,if (u >= p[end]) {
Release-2.4.0,"System.out.println(""u="" + u + "" p[end]="" + p[end] + "" start="" + start + "" end="" + end);"
Release-2.4.0,return end;
Release-2.4.0,}
Release-2.4.0,
Release-2.4.0,if (u < p[start]) {
Release-2.4.0,"System.out.println(""u="" + u + "" p[start]="" + p[start] + "" start="" + start + "" end="" + end);"
Release-2.4.0,return start;
Release-2.4.0,}
Release-2.4.0,The starting point
Release-2.4.0,There's always an unused entry.
Release-2.4.0,The starting point
Release-2.4.0,There's always an unused entry.
Release-2.4.0,The starting point
Release-2.4.0,There's always an unused entry.
Release-2.4.0,The starting point
Release-2.4.0,There's always an unused entry.
Release-2.4.0,The starting point
Release-2.4.0,There's always an unused entry.
Release-2.4.0,The starting point
Release-2.4.0,There's always an unused entry.
Release-2.4.0,print();
Release-2.4.0,The starting point
Release-2.4.0,There's always an unused entry.
Release-2.4.0,The starting point
Release-2.4.0,There's always an unused entry.
Release-2.4.0,The starting point
Release-2.4.0,There's always an unused entry.
Release-2.4.0,The starting point
Release-2.4.0,There's always an unused entry.
Release-2.4.0,print();
Release-2.4.0,Write #rows
Release-2.4.0,Write each row
Release-2.4.0,dense
Release-2.4.0,sparse
Release-2.4.0,LOG.info(buf.refCnt());
Release-2.4.0,dense
Release-2.4.0,sparse
Release-2.4.0,calculate columns
Release-2.4.0,reset(row);
Release-2.4.0,loss function
Release-2.4.0,gradient and hessian
Release-2.4.0,"categorical feature set, null: none, empty: all, else: partial"
Release-2.4.0,"node's end index in instancePos, instances in [start, end] belong to a tree node"
Release-2.4.0,initialize the phase
Release-2.4.0,current tree and depth
Release-2.4.0,create loss function
Release-2.4.0,calculate grad info of each instance
Release-2.4.0,"create data sketch, push candidate split value to PS"
Release-2.4.0,1. calculate candidate split value
Release-2.4.0,categorical features
Release-2.4.0,2. push local sketch to PS
Release-2.4.0,the leader worker
Release-2.4.0,merge categorical features
Release-2.4.0,create updates
Release-2.4.0,"pull the global sketch from PS, only called once by each worker"
Release-2.4.0,number of categorical feature
Release-2.4.0,sample feature
Release-2.4.0,push sampled feature set to the current tree
Release-2.4.0,create new tree
Release-2.4.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
Release-2.4.0,calculate gradient
Release-2.4.0,"1. create new tree, initialize tree nodes and node stats"
Release-2.4.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
Release-2.4.0,2.1. pull the sampled features of the current tree
Release-2.4.0,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
Release-2.4.0,"2.2. if use all the features, only called one"
Release-2.4.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
Release-2.4.0,4. set root node to active
Release-2.4.0,"5. reset instance position, set the root node's span"
Release-2.4.0,6. calculate gradient
Release-2.4.0,1. decide nodes that should be calculated
Release-2.4.0,2. decide calculated and subtracted tree nodes
Release-2.4.0,3. calculate threads
Release-2.4.0,wait until all threads finish
Release-2.4.0,4. subtract threads
Release-2.4.0,wait until all threads finish
Release-2.4.0,5. send histograms to PS
Release-2.4.0,6. update histogram cache
Release-2.4.0,clock
Release-2.4.0,find split
Release-2.4.0,"1. find responsible tree node, using RR scheme"
Release-2.4.0,2. pull gradient histogram
Release-2.4.0,2.1. get the name of this node's gradient histogram on PS
Release-2.4.0,2.2. pull the histogram
Release-2.4.0,2.3. find best split result of this tree node
Release-2.4.0,2.3.1 using server split
Release-2.4.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.4.0,update the grad stats of children node
Release-2.4.0,update the left child
Release-2.4.0,update the right child
Release-2.4.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.4.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
Release-2.4.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.4.0,2.3.5 reset this tree node's gradient histogram to 0
Release-2.4.0,3. push split feature to PS
Release-2.4.0,4. push split value to PS
Release-2.4.0,5. push split gain to PS
Release-2.4.0,6. set phase to AFTER_SPLIT
Release-2.4.0,this.phase = GBDTPhase.AFTER_SPLIT;
Release-2.4.0,clock
Release-2.4.0,1. get split feature
Release-2.4.0,2. get split value
Release-2.4.0,3. get split gain
Release-2.4.0,4. get node weight
Release-2.4.0,5. split node
Release-2.4.0,update local replica
Release-2.4.0,create AfterSplit task
Release-2.4.0,"2. check thread stats, if all threads finish, return"
Release-2.4.0,6. clock
Release-2.4.0,"split the span of one node, reset the instance position"
Release-2.4.0,in case this worker has no instance on this node
Release-2.4.0,set the span of left child
Release-2.4.0,set the span of right child
Release-2.4.0,"1. left to right, find the first instance that should be in the right child"
Release-2.4.0,"2. right to left, find the first instance that should be in the left child"
Release-2.4.0,3. swap two instances
Release-2.4.0,4. find the cut pos
Release-2.4.0,5. set the span of left child
Release-2.4.0,6. set the span of right child
Release-2.4.0,set tree node to active
Release-2.4.0,set node to leaf
Release-2.4.0,set node to inactive
Release-2.4.0,finish current depth
Release-2.4.0,finish current tree
Release-2.4.0,set the tree phase
Release-2.4.0,check if there is active node
Release-2.4.0,check if finish all the tree
Release-2.4.0,update node's grad stats on PS
Release-2.4.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
Release-2.4.0,the root node's stats is updated by leader worker
Release-2.4.0,1. create the update
Release-2.4.0,2. push the update to PS
Release-2.4.0,1. update predictions of training data
Release-2.4.0,2. update predictions of validation data
Release-2.4.0,the leader task adds node prediction to flush list
Release-2.4.0,1. name of this node's grad histogram on PS
Release-2.4.0,2. build the grad histogram of this node
Release-2.4.0,3. push the histograms to PS
Release-2.4.0,4. reset thread stats to finished
Release-2.4.0,5.1. set the children nodes of this node
Release-2.4.0,5.2. set split info and grad stats to this node
Release-2.4.0,5.2. create children nodes
Release-2.4.0,"5.3. create node stats for children nodes, and add them to the tree"
Release-2.4.0,5.4. reset instance position
Release-2.4.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
Release-2.4.0,5.6. set children nodes to leaf nodes
Release-2.4.0,5.7. set nid to leaf node
Release-2.4.0,5.8. deactivate active node
Release-2.4.0,"get feature type, 0:empty 1:all equal 2:real"
Release-2.4.0,"if not -1, sufficient space will be allocated at once"
Release-2.4.0,copy the highest levels
Release-2.4.0,copy baseBuffer
Release-2.4.0,merge two non-empty quantile sketches
Release-2.4.0,left child <= split value; right child > split value
Release-2.4.0,"the first: minimal, the last: maximal"
Release-2.4.0,categorical features
Release-2.4.0,continuous features
Release-2.4.0,left child <= split value; right child > split value
Release-2.4.0,feature index used to split
Release-2.4.0,feature value used to split
Release-2.4.0,loss change after split this node
Release-2.4.0,grad stats of the left child
Release-2.4.0,grad stats of the right child
Release-2.4.0,"LOG.info(""Constructor with fid = -1"");"
Release-2.4.0,fid = -1: no split currently
Release-2.4.0,the minimal split value is the minimal value of feature
Release-2.4.0,the splits do not include the maximal value of feature
Release-2.4.0,"1. the average distance, (maxValue - minValue) / splitNum"
Release-2.4.0,2. calculate the candidate split value
Release-2.4.0,1. new feature's histogram (grad + hess)
Release-2.4.0,size: sampled_featureNum * (2 * splitNum)
Release-2.4.0,"in other words, concatenate each feature's histogram"
Release-2.4.0,2. get the span of this node
Release-2.4.0,------ 3. using sparse-aware method to build histogram ---
Release-2.4.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
Release-2.4.0,3.1. get the instance index
Release-2.4.0,3.2. get the grad and hess of the instance
Release-2.4.0,3.3. add to the sum
Release-2.4.0,3.4. loop the non-zero entries
Release-2.4.0,3.4.1. get feature value
Release-2.4.0,3.4.2. current feature's position in the sampled feature set
Release-2.4.0,"int fPos = findFidPlace(this.controller.fSet, fid);"
Release-2.4.0,3.4.3. find the position of feature value in a histogram
Release-2.4.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
Release-2.4.0,3.4.4. add the grad and hess to the corresponding bin
Release-2.4.0,3.4.5. add the reverse to the bin that contains 0.0f
Release-2.4.0,4. add the grad and hess sum to the zero bin of all features
Release-2.4.0,find the best split result of the histogram of a tree node
Release-2.4.0,1. calculate the gradStats of the root node
Release-2.4.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
Release-2.4.0,2. loop over features
Release-2.4.0,2.1. get the ture feature id in the sampled feature set
Release-2.4.0,2.2. get the indexes of histogram of this feature
Release-2.4.0,2.3. find the best split of current feature
Release-2.4.0,2.4. update the best split result if possible
Release-2.4.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.4.0,3. update the grad stats of children node
Release-2.4.0,3.1. update the left child
Release-2.4.0,3.2. update the right child
Release-2.4.0,find the best split result of one feature
Release-2.4.0,1. set the feature id
Release-2.4.0,2. create the best left stats and right stats
Release-2.4.0,3. the gain of the root node
Release-2.4.0,4. create the temp left and right grad stats
Release-2.4.0,5. loop over all the data in histogram
Release-2.4.0,5.1. get the grad and hess of current hist bin
Release-2.4.0,5.2. check whether we can split with current left hessian
Release-2.4.0,right = root - left
Release-2.4.0,5.3. check whether we can split with current right hessian
Release-2.4.0,5.4. calculate the current loss gain
Release-2.4.0,5.5. check whether we should update the split result with current loss gain
Release-2.4.0,split value = sketches[splitIdx]
Release-2.4.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.4.0,6. set the best left and right grad stats
Release-2.4.0,partition number
Release-2.4.0,cols of each partition
Release-2.4.0,1. calculate the total grad sum and hess sum
Release-2.4.0,2. create the grad stats of the node
Release-2.4.0,1. calculate the total grad sum and hess sum
Release-2.4.0,2. create the grad stats of the node
Release-2.4.0,1. calculate the total grad sum and hess sum
Release-2.4.0,2. create the grad stats of the node
Release-2.4.0,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
Release-2.4.0,assert fvalue >= sketch[start] && fvalue <= sketch[end];
Release-2.4.0,if (left > end) return end - start;
Release-2.4.0,find the best split result of the histogram of a tree node
Release-2.4.0,2.2. get the indexes of histogram of this feature
Release-2.4.0,2.3. find the best split of current feature
Release-2.4.0,2.4. update the best split result if possible
Release-2.4.0,find the best split result of one feature
Release-2.4.0,1. set the feature id
Release-2.4.0,splitEntry.setFid(fid);
Release-2.4.0,2. create the best left stats and right stats
Release-2.4.0,3. the gain of the root node
Release-2.4.0,4. create the temp left and right grad stats
Release-2.4.0,5. loop over all the data in histogram
Release-2.4.0,5.1. get the grad and hess of current hist bin
Release-2.4.0,5.2. check whether we can split with current left hessian
Release-2.4.0,right = root - left
Release-2.4.0,5.3. check whether we can split with current right hessian
Release-2.4.0,5.4. calculate the current loss gain
Release-2.4.0,5.5. check whether we should update the split result with current loss gain
Release-2.4.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.4.0,6. set the best left and right grad stats
Release-2.4.0,find the best split result of a serve row on the PS
Release-2.4.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
Release-2.4.0,2.2. get the start index in histogram of this feature
Release-2.4.0,2.3. find the best split of current feature
Release-2.4.0,2.4. update the best split result if possible
Release-2.4.0,"find the best split result of one feature from a server row, used by the PS"
Release-2.4.0,1. set the feature id
Release-2.4.0,2. create the best left stats and right stats
Release-2.4.0,3. the gain of the root node
Release-2.4.0,4. create the temp left and right grad stats
Release-2.4.0,5. loop over all the data in histogram
Release-2.4.0,5.1. get the grad and hess of current hist bin
Release-2.4.0,5.2. check whether we can split with current left hessian
Release-2.4.0,right = root - left
Release-2.4.0,5.3. check whether we can split with current right hessian
Release-2.4.0,5.4. calculate the current loss gain
Release-2.4.0,5.5. check whether we should update the split result with current loss gain
Release-2.4.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
Release-2.4.0,the task use index to find fvalue
Release-2.4.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.4.0,6. set the best left and right grad stats
Release-2.4.0,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
Release-2.4.0,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
Release-2.4.0,max and min of each feature
Release-2.4.0,clear all the information
Release-2.4.0,calculate the sum of gradient and hess
Release-2.4.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
Release-2.4.0,ridx)
Release-2.4.0,check if necessary information is ready
Release-2.4.0,"same as add, reduce is used in All Reduce"
Release-2.4.0,"features used in this tree, if equals null, means use all the features without sampling"
Release-2.4.0,node in the tree
Release-2.4.0,the gradient info of each instances
Release-2.4.0,initialize nodes
Release-2.4.0,gradient
Release-2.4.0,second order gradient
Release-2.4.0,int sendStartCol = (int) row.getStartCol();
Release-2.4.0,logistic loss for binary classification task.
Release-2.4.0,"logistic loss, but predict un-transformed margin"
Release-2.4.0,check if label in range
Release-2.4.0,return the default evaluation metric for the objective
Release-2.4.0,"task type: classification, regression, or ranking"
Release-2.4.0,"quantile sketch, size = featureNum * splitNum"
Release-2.4.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
Release-2.4.0,"active tree nodes, size = pow(2, treeDepth) -1"
Release-2.4.0,sampled features. size = treeNum * sampleRatio * featureNum
Release-2.4.0,categorical feature. size = workerNum * cateFeatNum * splitNum
Release-2.4.0,"split features, size = treeNum * treeNodeNum"
Release-2.4.0,"split values, size = treeNum * treeNodeNum"
Release-2.4.0,"split gains, size = treeNum * treeNodeNum"
Release-2.4.0,"node weights, size = treeNum * treeNodeNum"
Release-2.4.0,"node preds, size = treeNum * treeNodeNum"
Release-2.4.0,if using PS to perform split
Release-2.4.0,step size for a tree
Release-2.4.0,number of class
Release-2.4.0,minimum loss change required for a split
Release-2.4.0,maximum depth of a tree
Release-2.4.0,number of features
Release-2.4.0,number of nonzero
Release-2.4.0,number of candidates split value
Release-2.4.0,----- the rest parameters are less important ----
Release-2.4.0,base instance weight
Release-2.4.0,minimum amount of hessian(weight) allowed in a child
Release-2.4.0,L2 regularization factor
Release-2.4.0,L1 regularization factor
Release-2.4.0,default direction choice
Release-2.4.0,maximum delta update we can add in weight estimation
Release-2.4.0,this parameter can be used to stabilize update
Release-2.4.0,default=0 means no constraint on weight delta
Release-2.4.0,whether we want to do subsample for row
Release-2.4.0,whether to subsample columns for each tree
Release-2.4.0,accuracy of sketch
Release-2.4.0,accuracy of sketch
Release-2.4.0,leaf vector size
Release-2.4.0,option for parallelization
Release-2.4.0,option to open cacheline optimization
Release-2.4.0,whether to not print info during training.
Release-2.4.0,maximum depth of the tree
Release-2.4.0,number of features used for tree construction
Release-2.4.0,"minimum loss change required for a split, otherwise stop split"
Release-2.4.0,----- the rest parameters are less important ----
Release-2.4.0,default direction choice
Release-2.4.0,whether we want to do sample data
Release-2.4.0,whether to sample columns during tree construction
Release-2.4.0,whether to use histogram for split
Release-2.4.0,number of histogram units
Release-2.4.0,whether to print info during training.
Release-2.4.0,----- the rest parameters are obtained after training ----
Release-2.4.0,total number of nodes
Release-2.4.0,number of deleted nodes */
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy data spliter
Release-3.0.0,get a angel client
Release-3.0.0,add matrix
Release-3.0.0,TODO Auto-generated constructor stub
Release-3.0.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighbor;
Release-3.0.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborParam;
Release-3.0.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborResult;
Release-3.0.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighbor;
Release-3.0.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighborParam;
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add matrix
Release-3.0.0,@Test
Release-3.0.0,"public void testInitAndGet() throws ExecutionException, InterruptedException {"
Release-3.0.0,Worker worker = LocalClusterContext.get().getWorker(worker0Attempt0Id).getWorker();
Release-3.0.0,"MatrixClient client1 = worker.getPSAgent().getMatrixClient(""w2"", 0);"
Release-3.0.0,int matrixW1Id = client1.getMatrixId();
Release-3.0.0,// Generate graph data
Release-3.0.0,"Map<Integer, int []> adjMap = generateAdjTable(nodeNum, maxNeighborNum);"
Release-3.0.0,
Release-3.0.0,// Init graph adj table
Release-3.0.0,"InitNeighbor func = new InitNeighbor(new InitNeighborParam(matrixW1Id, adjMap));"
Release-3.0.0,client1.update(func);
Release-3.0.0,
Release-3.0.0,int [] nodeIds = new int[adjMap.size()];
Release-3.0.0,int i = 0;
Release-3.0.0,for(int nodeId : adjMap.keySet()) {
Release-3.0.0,nodeIds[i++] = nodeId;
Release-3.0.0,}
Release-3.0.0,
Release-3.0.0,// Get graph adj table from PS
Release-3.0.0,"GetNeighbor getFunc = new GetNeighbor(new GetNeighborParam(matrixW1Id, nodeIds, maxNeighborNum));"
Release-3.0.0,"Map<Integer, int[]> getResults = ((GetNeighborResult) (client1.get(getFunc)))"
Release-3.0.0,.getNodeIdToNeighborIndices();
Release-3.0.0,
Release-3.0.0,// Check the result
Release-3.0.0,"for(Entry<Integer, int[]> entry : getResults.entrySet()) {"
Release-3.0.0,"Assert.assertArrayEquals(entry.getValue(), adjMap.get(entry.getKey()));"
Release-3.0.0,}
Release-3.0.0,}
Release-3.0.0,row 0 is a random uniform
Release-3.0.0,row 1 is a random normal
Release-3.0.0,row 2 is filled with 1.0
Release-3.0.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
Release-3.0.0,"paras[1] = ""abc"";"
Release-3.0.0,"paras[2] = ""123"";"
Release-3.0.0,Add standard Hadoop classes
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,set sgd LR algorithm parameters #feature #epoch
Release-3.0.0,Set input data path
Release-3.0.0,Set save model path
Release-3.0.0,Set actionType train
Release-3.0.0,QSLRRunner runner = new QSLRRunner();
Release-3.0.0,runner.train(conf);
Release-3.0.0,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
Release-3.0.0,Dataset
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,class number
Release-3.0.0,Model type
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,Train batch number per epoch.
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set file system
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Use local deploy mode and data format
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set Softmax algorithm parameters
Release-3.0.0,Set model class
Release-3.0.0,Dataset
Release-3.0.0,Set file system
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Use local deploy mode
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set AFM algorithm parameters
Release-3.0.0,Set model class
Release-3.0.0,Dataset
Release-3.0.0,Data format
Release-3.0.0,Feature number of train data
Release-3.0.0,Tree number
Release-3.0.0,Tree depth
Release-3.0.0,Split number
Release-3.0.0,Feature sample ratio
Release-3.0.0,Ratio of validation
Release-3.0.0,Learning rate
Release-3.0.0,Set file system
Release-3.0.0,Use local deploy mode and data format
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource, #worker, #task, #PS"
Release-3.0.0,Set GBDT algorithm parameters
Release-3.0.0,Dataset
Release-3.0.0,Set file system
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Use local deploy mode
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set DeepFM algorithm parameters
Release-3.0.0,Set model class
Release-3.0.0,Dataset
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Model type
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set file system
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Use local deploy mode and data format
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set LR algorithm parameters
Release-3.0.0,Set model class
Release-3.0.0,Dataset
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Model type
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set file system
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Use local deploy mode and data format
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set FM algorithm parameters
Release-3.0.0,Set model class
Release-3.0.0,Dataset
Release-3.0.0,Set file system
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Use local deploy mode
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set WideAndDeep algorithm parameters
Release-3.0.0,Set model class
Release-3.0.0,Dataset
Release-3.0.0,Set file system
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Use local deploy mode
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set DCN algorithm parameters
Release-3.0.0,Set model class
Release-3.0.0,Dataset
Release-3.0.0,Data format
Release-3.0.0,"Set LDA parameters #V, #K"
Release-3.0.0,Set file system
Release-3.0.0,Use local deploy mode and data format
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource, #worker, #task, #PS"
Release-3.0.0,Set LDA algorithm parameters
Release-3.0.0,Dataset
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Model type
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set file system
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Use local deploy mode and data format
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set SVM algorithm parameters
Release-3.0.0,Set model class
Release-3.0.0,Dataset
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Model type
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,Model is classification
Release-3.0.0,Train batch number per epoch.
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set file system
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Use local deploy mode and data format
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set LR algorithm parameters
Release-3.0.0,Set model class
Release-3.0.0,Dataset
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Model type
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,Model is classification
Release-3.0.0,Train batch number per epoch.
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set file system
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Use local deploy mode and data format
Release-3.0.0,Set data path
Release-3.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set LR algorithm parameters
Release-3.0.0,Set model class
Release-3.0.0,Load model meta
Release-3.0.0,Convert model
Release-3.0.0,"Get input path, output path"
Release-3.0.0,Init serde
Release-3.0.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-3.0.0,Load model meta
Release-3.0.0,Convert model
Release-3.0.0,load hadoop configuration
Release-3.0.0,"Get input path, output path"
Release-3.0.0,Init serde
Release-3.0.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-3.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.0.0,input.seek(rowOffset.getOffset());
Release-3.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.0.0,input.seek(rowOffset.getOffset());
Release-3.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.0.0,input.seek(rowOffset.getOffset());
Release-3.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.0.0,input.seek(rowOffset.getOffset());
Release-3.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.0.0,input.seek(rowOffset.getOffset());
Release-3.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.0.0,input.seek(rowOffset.getOffset());
Release-3.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-3.0.0,input.seek(rowOffset.getOffset());
Release-3.0.0,Load model meta
Release-3.0.0,Check row type
Release-3.0.0,Load model
Release-3.0.0,Load model meta
Release-3.0.0,Check row type
Release-3.0.0,Load model
Release-3.0.0,Load model meta
Release-3.0.0,Check row type
Release-3.0.0,Load model
Release-3.0.0,Load model meta
Release-3.0.0,Check row type
Release-3.0.0,Load model
Release-3.0.0,Load model meta
Release-3.0.0,Check row type
Release-3.0.0,Load model
Release-3.0.0,Load model meta
Release-3.0.0,Check row type
Release-3.0.0,Load model
Release-3.0.0,Load model meta
Release-3.0.0,Check row type
Release-3.0.0,Load model
Release-3.0.0,Load model
Release-3.0.0,load hadoop configuration
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add matrix
Release-3.0.0,add matrix
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add matrix
Release-3.0.0,attempt 0
Release-3.0.0,attempt1
Release-3.0.0,attempt1
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add matrix
Release-3.0.0,TODO Auto-generated constructor stub
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add matrix
Release-3.0.0,set basic configuration keys
Release-3.0.0,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add matrix
Release-3.0.0,Thread.sleep(5000);
Release-3.0.0,"response = master.getJobReport(null, request);"
Release-3.0.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
Release-3.0.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
Release-3.0.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add dense double matrix
Release-3.0.0,add comp dense double matrix
Release-3.0.0,add sparse double matrix
Release-3.0.0,add component sparse double matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense long matrix
Release-3.0.0,add comp dense long matrix
Release-3.0.0,add sparse long matrix
Release-3.0.0,add component sparse long matrix
Release-3.0.0,add comp dense long double matrix
Release-3.0.0,add sparse long-key double matrix
Release-3.0.0,add component long-key sparse double matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add sparse long-key float matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add sparse long-key int matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,add sparse long-key long matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add dense double matrix
Release-3.0.0,add comp dense double matrix
Release-3.0.0,add sparse double matrix
Release-3.0.0,add component sparse double matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense long matrix
Release-3.0.0,add comp dense long matrix
Release-3.0.0,add sparse long matrix
Release-3.0.0,add component sparse long matrix
Release-3.0.0,add comp dense long double matrix
Release-3.0.0,add sparse long-key double matrix
Release-3.0.0,add component long-key sparse double matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add sparse long-key float matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add sparse long-key int matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,add sparse long-key long matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,testDenseDoubleUDF();
Release-3.0.0,testSparseDoubleUDF();
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add dense double matrix
Release-3.0.0,add comp dense double matrix
Release-3.0.0,add sparse double matrix
Release-3.0.0,add component sparse double matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense long matrix
Release-3.0.0,add comp dense long matrix
Release-3.0.0,add sparse long matrix
Release-3.0.0,add component sparse long matrix
Release-3.0.0,add comp dense long double matrix
Release-3.0.0,add sparse long-key double matrix
Release-3.0.0,add component long-key sparse double matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add sparse long-key float matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add sparse long-key int matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,add sparse long-key long matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,client1.clock().get();
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,client1.clock().get();
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add dense double matrix
Release-3.0.0,add comp dense double matrix
Release-3.0.0,add sparse double matrix
Release-3.0.0,add component sparse double matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense long matrix
Release-3.0.0,add comp dense long matrix
Release-3.0.0,add sparse long matrix
Release-3.0.0,add component sparse long matrix
Release-3.0.0,add comp dense long double matrix
Release-3.0.0,add sparse long-key double matrix
Release-3.0.0,add component long-key sparse double matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add sparse long-key float matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add sparse long-key int matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,add sparse long-key long matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,testDenseDoubleUDF();
Release-3.0.0,testSparseDoubleUDF();
Release-3.0.0,client1.clock().get();
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,client1.clock().get();
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,TODO Auto-generated constructor stub
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add dense double matrix
Release-3.0.0,add sparse double matrix
Release-3.0.0,add comp dense double matrix
Release-3.0.0,add component sparse double matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense long matrix
Release-3.0.0,add comp dense long matrix
Release-3.0.0,add sparse long matrix
Release-3.0.0,add component sparse long matrix
Release-3.0.0,add comp dense long double matrix
Release-3.0.0,add sparse long-key double matrix
Release-3.0.0,add component long-key sparse double matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add sparse long-key float matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add sparse long-key int matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,add sparse long-key long matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,testDenseDoubleUDF();
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,Assert.assertTrue(index.length == row.size());
Release-3.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.0.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.0.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"LOG.info(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.0.0,IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,for (int i = 0; i < feaNum; i++) {
Release-3.0.0,"deltaVec.set(i, i);"
Release-3.0.0,}
Release-3.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-3.0.0,IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add sparse float matrix
Release-3.0.0,siMat.setPartitionClass(CSRPartition.class);
Release-3.0.0,siMat.setPartitionStorageClass(IntCSRStorage.class);
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add dense double matrix
Release-3.0.0,add comp dense double matrix
Release-3.0.0,add sparse double matrix
Release-3.0.0,add component sparse double matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense long matrix
Release-3.0.0,add comp dense long matrix
Release-3.0.0,add sparse long matrix
Release-3.0.0,add component sparse long matrix
Release-3.0.0,add comp dense long double matrix
Release-3.0.0,add sparse long-key double matrix
Release-3.0.0,add component long-key sparse double matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add sparse long-key float matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add sparse long-key int matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,add sparse long-key long matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,get a angel client
Release-3.0.0,add dense double matrix
Release-3.0.0,add comp dense double matrix
Release-3.0.0,add sparse double matrix
Release-3.0.0,add component sparse double matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense float matrix
Release-3.0.0,add comp dense float matrix
Release-3.0.0,add sparse float matrix
Release-3.0.0,add component sparse float matrix
Release-3.0.0,add dense long matrix
Release-3.0.0,add comp dense long matrix
Release-3.0.0,add sparse long matrix
Release-3.0.0,add component sparse long matrix
Release-3.0.0,add comp dense long double matrix
Release-3.0.0,add sparse long-key double matrix
Release-3.0.0,add component long-key sparse double matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add sparse long-key float matrix
Release-3.0.0,add component long-key sparse float matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add sparse long-key int matrix
Release-3.0.0,add component long-key sparse int matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,add sparse long-key long matrix
Release-3.0.0,add component long-key sparse long matrix
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-3.0.0,get a angel client
Release-3.0.0,add sparse float matrix
Release-3.0.0,MatrixContext siMat = new MatrixContext();
Release-3.0.0,siMat.setName(SPARSE_INT_MAT);
Release-3.0.0,siMat.setRowType(RowType.T_ANY_INTKEY_SPARSE);
Release-3.0.0,siMat.setRowNum(1);
Release-3.0.0,siMat.setValidIndexNum(100);
Release-3.0.0,siMat.setColNum(10000000000L);
Release-3.0.0,siMat.setValueType(Node.class);
Release-3.0.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-3.0.0,siMat.setPartitionClass(CSRPartition.class);
Release-3.0.0,angelClient.addMatrix(siMat);
Release-3.0.0,add sparse long-key double matrix
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,client1.clock().get();
Release-3.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-3.0.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
Release-3.0.0,@RunWith(MockitoJUnitRunner.class)
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
Release-3.0.0,get a angel client
Release-3.0.0,add matrix
Release-3.0.0,psAgent.initAndStart();
Release-3.0.0,test conf
Release-3.0.0,test master location
Release-3.0.0,test app id
Release-3.0.0,test user
Release-3.0.0,test ps agent attempt id
Release-3.0.0,test connection
Release-3.0.0,test master client
Release-3.0.0,test ip
Release-3.0.0,test loc
Release-3.0.0,test master location
Release-3.0.0,test ps location
Release-3.0.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
Release-3.0.0,test all ps ids
Release-3.0.0,test all matrix ids
Release-3.0.0,test all matrix names
Release-3.0.0,test matrix attribute
Release-3.0.0,test matrix meta
Release-3.0.0,test ps location
Release-3.0.0,test partitions
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,System.out.println(content);
Release-3.0.0,https://blog.csdn.net/cocoonyang/article/details/63068108
Release-3.0.0,v1[i] = v1[i] + da * v2[i];
Release-3.0.0,"dgemm(String transa, String transb,"
Release-3.0.0,"int m, int n, int k,"
Release-3.0.0,"double alpha,"
Release-3.0.0,"double[] a, int lda,"
Release-3.0.0,"double[] b, int ldb,"
Release-3.0.0,"double beta,"
Release-3.0.0,"double[] c, int ldc);"
Release-3.0.0,C := alpha*op( A )*op( B ) + beta*C
Release-3.0.0,v1[i] = v1[i] + da * v2[i];
Release-3.0.0,y := alpha*A*x + beta*y
Release-3.0.0,y := alpha*A*x + beta*y
Release-3.0.0,y := alpha*A*x + beta*y
Release-3.0.0,"dgemm(String transa, String transb,"
Release-3.0.0,"int m, int n, int k,"
Release-3.0.0,"double alpha,"
Release-3.0.0,"double[] a, int lda,"
Release-3.0.0,"double[] b, int ldb,"
Release-3.0.0,"double beta,"
Release-3.0.0,"double[] c, int ldc);"
Release-3.0.0,C := alpha*op( A )*op( B ) + beta*C
Release-3.0.0,Default does nothing.
Release-3.0.0,The app injection is optional
Release-3.0.0,"renderText(""hello world"");"
Release-3.0.0,"user choose a workerGroupID from the workergroups page,"
Release-3.0.0,now we should change the AngelApp params and render the workergroup page;
Release-3.0.0,"static final String WORKER_ID = ""worker.id"";"
Release-3.0.0,"div(""#logo"")."
Release-3.0.0,"img(""/static/hadoop-st.png"")._()."
Release-3.0.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
Release-3.0.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
Release-3.0.0,JQueryUI.jsnotice(html);
Release-3.0.0,import org.apache.hadoop.conf.Configuration;
Release-3.0.0,import java.lang.reflect.Field;
Release-3.0.0,all the files in input set
Release-3.0.0,Shuffle the file
Release-3.0.0,Get the blocks for all files
Release-3.0.0,Adjust the maxSize to make the split more balanced
Release-3.0.0,Handle the splittable files
Release-3.0.0,Handle the unsplittable files
Release-3.0.0,Split the blocks
Release-3.0.0,"If the remaining size of the current block is smaller than the required size,"
Release-3.0.0,the remaining blocks are divided into the current split
Release-3.0.0,Update current split length and move to next block
Release-3.0.0,Clear the current block offset
Release-3.0.0,"Current split length is > maxSize, split the block and generate a new split"
Release-3.0.0,Clear blocks list for next split
Release-3.0.0,Clear the current split length
Release-3.0.0,"If splitBlocks is not empty, just genetate a split for it"
Release-3.0.0,get block locations from file system
Release-3.0.0,create an input split
Release-3.0.0,get block locations from file system
Release-3.0.0,create a list of all block and their locations
Release-3.0.0,"if the file is not splitable, just create the one block with"
Release-3.0.0,full file length
Release-3.0.0,each split can be a maximum of maxSize
Release-3.0.0,if remainder is between max and 2*max - then
Release-3.0.0,"instead of creating splits of size max, left-max we"
Release-3.0.0,create splits of size left/2 and left/2. This is
Release-3.0.0,a heuristic to avoid creating really really small
Release-3.0.0,splits.
Release-3.0.0,add this block to the block --> node locations map
Release-3.0.0,"For blocks that do not have host/rack information,"
Release-3.0.0,assign to default  rack.
Release-3.0.0,add this block to the rack --> block map
Release-3.0.0,Add this host to rackToNodes map
Release-3.0.0,add this block to the node --> block map
Release-3.0.0,"if the file system does not have any rack information, then"
Release-3.0.0,use dummy rack location.
Release-3.0.0,The topology paths have the host name included as the last
Release-3.0.0,component. Strip it.
Release-3.0.0,get tokens for all the required FileSystems..
Release-3.0.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
Release-3.0.0,job.getConfiguration());
Release-3.0.0,Whether we need to recursive look into the directory structure
Release-3.0.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-3.0.0,user provided one (if any).
Release-3.0.0,all the files in input set
Release-3.0.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
Release-3.0.0,process all nodes and create splits that are local to a node. Generate
Release-3.0.0,"one split per node iteration, and walk over nodes multiple times to"
Release-3.0.0,distribute the splits across nodes.
Release-3.0.0,Skip the node if it has previously been marked as completed.
Release-3.0.0,"for each block, copy it into validBlocks. Delete it from"
Release-3.0.0,blockToNodes so that the same block does not appear in
Release-3.0.0,two different splits.
Release-3.0.0,Remove all blocks which may already have been assigned to other
Release-3.0.0,splits.
Release-3.0.0,"if the accumulated split size exceeds the maximum, then"
Release-3.0.0,create this split.
Release-3.0.0,create an input split and add it to the splits array
Release-3.0.0,Remove entries from blocksInNode so that we don't walk these
Release-3.0.0,again.
Release-3.0.0,Done creating a single split for this node. Move on to the next
Release-3.0.0,node so that splits are distributed across nodes.
Release-3.0.0,This implies that the last few blocks (or all in case maxSize=0)
Release-3.0.0,were not part of a split. The node is complete.
Release-3.0.0,if there were any blocks left over and their combined size is
Release-3.0.0,"larger than minSplitNode, then combine them into one split."
Release-3.0.0,Otherwise add them back to the unprocessed pool. It is likely
Release-3.0.0,that they will be combined with other blocks from the
Release-3.0.0,same rack later on.
Release-3.0.0,This condition also kicks in when max split size is not set. All
Release-3.0.0,blocks on a node will be grouped together into a single split.
Release-3.0.0,haven't created any split on this machine. so its ok to add a
Release-3.0.0,smaller one for parallelism. Otherwise group it in the rack for
Release-3.0.0,balanced size create an input split and add it to the splits
Release-3.0.0,array
Release-3.0.0,Remove entries from blocksInNode so that we don't walk this again.
Release-3.0.0,The node is done. This was the last set of blocks for this node.
Release-3.0.0,Put the unplaced blocks back into the pool for later rack-allocation.
Release-3.0.0,Node is done. All blocks were fit into node-local splits.
Release-3.0.0,Check if node-local assignments are complete.
Release-3.0.0,All nodes have been walked over and marked as completed or all blocks
Release-3.0.0,have been assigned. The rest should be handled via rackLock assignment.
Release-3.0.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
Release-3.0.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
Release-3.0.0,"if blocks in a rack are below the specified minimum size, then keep them"
Release-3.0.0,"in 'overflow'. After the processing of all racks is complete, these"
Release-3.0.0,overflow blocks will be combined into splits.
Release-3.0.0,Process all racks over and over again until there is no more work to do.
Release-3.0.0,Create one split for this rack before moving over to the next rack.
Release-3.0.0,Come back to this rack after creating a single split for each of the
Release-3.0.0,remaining racks.
Release-3.0.0,"Process one rack location at a time, Combine all possible blocks that"
Release-3.0.0,reside on this rack as one split. (constrained by minimum and maximum
Release-3.0.0,split size).
Release-3.0.0,iterate over all racks
Release-3.0.0,"for each block, copy it into validBlocks. Delete it from"
Release-3.0.0,blockToNodes so that the same block does not appear in
Release-3.0.0,two different splits.
Release-3.0.0,"if the accumulated split size exceeds the maximum, then"
Release-3.0.0,create this split.
Release-3.0.0,create an input split and add it to the splits array
Release-3.0.0,"if we created a split, then just go to the next rack"
Release-3.0.0,"if there is a minimum size specified, then create a single split"
Release-3.0.0,"otherwise, store these blocks into overflow data structure"
Release-3.0.0,There were a few blocks in this rack that
Release-3.0.0,remained to be processed. Keep them in 'overflow' block list.
Release-3.0.0,These will be combined later.
Release-3.0.0,Process all overflow blocks
Release-3.0.0,"This might cause an exiting rack location to be re-added,"
Release-3.0.0,but it should be ok.
Release-3.0.0,"if the accumulated split size exceeds the maximum, then"
Release-3.0.0,create this split.
Release-3.0.0,create an input split and add it to the splits array
Release-3.0.0,"Process any remaining blocks, if any."
Release-3.0.0,create an input split
Release-3.0.0,add this split to the list that is returned
Release-3.0.0,long num = totLength / maxSize;
Release-3.0.0,all blocks for all the files in input set
Release-3.0.0,mapping from a rack name to the list of blocks it has
Release-3.0.0,mapping from a block to the nodes on which it has replicas
Release-3.0.0,mapping from a node to the list of blocks that it contains
Release-3.0.0,populate all the blocks for all files
Release-3.0.0,stop all services
Release-3.0.0,1.write application state to file so that the client can get the state of the application
Release-3.0.0,if master exit
Release-3.0.0,2.clear tmp and staging directory
Release-3.0.0,waiting for client to get application state
Release-3.0.0,stop the RPC server
Release-3.0.0,"Security framework already loaded the tokens into current UGI, just use"
Release-3.0.0,them
Release-3.0.0,Now remove the AM->RM token so tasks don't have it
Release-3.0.0,add a shutdown hook
Release-3.0.0,init app state storage
Release-3.0.0,init event dispacher
Release-3.0.0,init location manager
Release-3.0.0,init container allocator
Release-3.0.0,init a rpc service
Release-3.0.0,recover matrix meta if needed
Release-3.0.0,recover ps attempt information if need
Release-3.0.0,Init Client manager
Release-3.0.0,Init PS Client manager
Release-3.0.0,init parameter server manager
Release-3.0.0,recover task information if needed
Release-3.0.0,a dummy data spliter is just for test now
Release-3.0.0,recover data splits information if needed
Release-3.0.0,init worker manager and register worker manager event
Release-3.0.0,register slow worker/ps checker
Release-3.0.0,register app manager event and finish event
Release-3.0.0,Init model saver & loader
Release-3.0.0,start a web service if use yarn deploy mode
Release-3.0.0,load from app state storage first if attempt index great than 1(the master is not the first
Release-3.0.0,retry)
Release-3.0.0,"if load failed, just build a new MatrixMetaManager"
Release-3.0.0,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-3.0.0,is not the first retry)
Release-3.0.0,load task information from app state storage first if attempt index great than 1(the master
Release-3.0.0,is not the first retry)
Release-3.0.0,"if load failed, just build a new AMTaskManager"
Release-3.0.0,load data splits information from app state storage first if attempt index great than 1(the
Release-3.0.0,master is not the first retry)
Release-3.0.0,"if load failed, we need to recalculate the data splits"
Release-3.0.0,Check Workers
Release-3.0.0,Check PSS
Release-3.0.0,Check Clients
Release-3.0.0,Check PS Clients
Release-3.0.0,stop all services
Release-3.0.0,1.write application state to file so that the client can get the state of the application
Release-3.0.0,if master exit
Release-3.0.0,2.clear tmp and staging directory
Release-3.0.0,waiting for client to get application state
Release-3.0.0,stop the RPC server
Release-3.0.0,add a shutdown hook
Release-3.0.0,init app state storage
Release-3.0.0,init event dispacher
Release-3.0.0,init location manager
Release-3.0.0,init a rpc service
Release-3.0.0,recover matrix meta if needed
Release-3.0.0,recover ps attempt information if need
Release-3.0.0,Init Client manager
Release-3.0.0,Init PS Client manager
Release-3.0.0,init parameter server manager
Release-3.0.0,recover task information if needed
Release-3.0.0,a dummy data spliter is just for test now
Release-3.0.0,recover data splits information if needed
Release-3.0.0,init worker manager and register worker manager event
Release-3.0.0,register slow worker/ps checker
Release-3.0.0,register app manager event and finish event
Release-3.0.0,Init model saver & loader
Release-3.0.0,k8sClusterManager = new KubernetesClusterManager(appContext);
Release-3.0.0,load from app state storage first if attempt index great than 1(the master is not the first
Release-3.0.0,retry)
Release-3.0.0,"if load failed, just build a new MatrixMetaManager"
Release-3.0.0,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-3.0.0,is not the first retry)
Release-3.0.0,load task information from app state storage first if attempt index great than 1(the master
Release-3.0.0,is not the first retry)
Release-3.0.0,"if load failed, just build a new AMTaskManager"
Release-3.0.0,load data splits information from app state storage first if attempt index great than 1(the
Release-3.0.0,master is not the first retry)
Release-3.0.0,"if load failed, we need to recalculate the data splits"
Release-3.0.0,parse parameter server counters
Release-3.0.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-3.0.0,refresh last heartbeat timestamp
Release-3.0.0,send a state update event to the specific PSAttempt
Release-3.0.0,Check is there save request
Release-3.0.0,"LOG.info(""PS "" + psAttemptId + "" need save "" + subSaveContext);"
Release-3.0.0,Check is there load request
Release-3.0.0,"LOG.info(""PS "" + psAttemptId + "" need load "" + subLoadContext);"
Release-3.0.0,check matrix metadata inconsistencies between master and parameter server.
Release-3.0.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
Release-3.0.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
Release-3.0.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-3.0.0,choose a unused port
Release-3.0.0,start RPC server
Release-3.0.0,remove this parameter server attempt from monitor set
Release-3.0.0,remove this parameter server attempt from monitor set
Release-3.0.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-3.0.0,find workergroup in worker manager
Release-3.0.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
Release-3.0.0,"if this worker group run over, just return WORKERGROUP_EXITED"
Release-3.0.0,"if this worker group is running now, return tasks, workers, data splits for it"
Release-3.0.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-3.0.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-3.0.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-3.0.0,"in ANGEL_PS mode, task id may can not know advance"
Release-3.0.0,update the clock for this matrix
Release-3.0.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-3.0.0,"in ANGEL_PS mode, task id may can not know advance"
Release-3.0.0,update task iteration
Release-3.0.0,"LOG.info(""error happened in psAttempt "" + psAttemptId + "" error msg="" + request.getMsg());"
Release-3.0.0,remove this parameter server attempt from monitor set
Release-3.0.0,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
Release-3.0.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
Release-3.0.0,the number of splits equal to the number of tasks
Release-3.0.0,split data
Release-3.0.0,dispatch the splits to workergroups
Release-3.0.0,split data
Release-3.0.0,dispatch the splits to workergroups
Release-3.0.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-3.0.0,"first, then divided by expected split number"
Release-3.0.0,get input format class from configuration and then instantiation a input format object
Release-3.0.0,split data
Release-3.0.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-3.0.0,"first, then divided by expected split number"
Release-3.0.0,get input format class from configuration and then instantiation a input format object
Release-3.0.0,split data
Release-3.0.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-3.0.0,need to fine tune the number of workergroup and task based on the actual split number
Release-3.0.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-3.0.0,Record the location information for the splits in order to data localized schedule
Release-3.0.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-3.0.0,need to fine tune the number of workergroup and task based on the actual split number
Release-3.0.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-3.0.0,Record the location information for the splits in order to data localized schedule
Release-3.0.0,write meta data to a temporary file
Release-3.0.0,rename the temporary file to final file
Release-3.0.0,"if the file exists, read from file and deserialize it"
Release-3.0.0,write task meta
Release-3.0.0,write ps meta
Release-3.0.0,generate a temporary file
Release-3.0.0,write task meta to the temporary file first
Release-3.0.0,rename the temporary file to the final file
Release-3.0.0,"if last final task file exist, remove it"
Release-3.0.0,find task meta file which has max timestamp
Release-3.0.0,"if the file does not exist, just return null"
Release-3.0.0,read task meta from file and deserialize it
Release-3.0.0,generate a temporary file
Release-3.0.0,write ps meta to the temporary file first.
Release-3.0.0,rename the temporary file to the final file
Release-3.0.0,"if the old final file exist, just remove it"
Release-3.0.0,find ps meta file
Release-3.0.0,"if ps meta file does not exist, just return null"
Release-3.0.0,read ps meta from file and deserialize it
Release-3.0.0,"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),"
Release-3.0.0,String.valueOf(requestId));
Release-3.0.0,Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));
Release-3.0.0,saveContext.setTmpSavePath(tmpPath.toString());
Release-3.0.0,Filter old epoch trigger first
Release-3.0.0,Split the user request to sub-requests to pss
Release-3.0.0,Init matrix files meta
Release-3.0.0,Move output files
Release-3.0.0,Write the meta file
Release-3.0.0,Split the user request to sub-requests to pss
Release-3.0.0,check whether psagent heartbeat timeout
Release-3.0.0,Set up the launch command
Release-3.0.0,Duplicate the ByteBuffers for access by multiple containers.
Release-3.0.0,Construct the actual Container
Release-3.0.0,Application resources
Release-3.0.0,Application environment
Release-3.0.0,Service data
Release-3.0.0,Tokens
Release-3.0.0,Set up JobConf to be localized properly on the remote NM.
Release-3.0.0,Setup DistributedCache
Release-3.0.0,Setup up task credentials buffer
Release-3.0.0,LocalStorageToken is needed irrespective of whether security is enabled
Release-3.0.0,or not.
Release-3.0.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
Release-3.0.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
Release-3.0.0,Construct the actual Container
Release-3.0.0,The null fields are per-container and will be constructed for each
Release-3.0.0,container separately.
Release-3.0.0,Set up the launch command
Release-3.0.0,Duplicate the ByteBuffers for access by multiple containers.
Release-3.0.0,Construct the actual Container
Release-3.0.0,"a * in the classpath will only find a .jar, so we need to filter out"
Release-3.0.0,all .jars and add everything else
Release-3.0.0,Propagate the system classpath when using the mini cluster
Release-3.0.0,Add standard Hadoop classes
Release-3.0.0,Add mr
Release-3.0.0,Cache archives
Release-3.0.0,Cache files
Release-3.0.0,Sanity check
Release-3.0.0,Add URI fragment or just the filename
Release-3.0.0,Add the env variables passed by the user
Release-3.0.0,Set logging level in the environment.
Release-3.0.0,G1 params
Release-3.0.0,".append("" -XX:G1NewSizePercent="").append(minNewRatio)"
Release-3.0.0,".append("" -XX:G1MaxNewSizePercent="").append(maxNewRatio)"
Release-3.0.0,Setup the log4j prop
Release-3.0.0,Add main class and its arguments
Release-3.0.0,Finally add the jvmID
Release-3.0.0,vargs.add(String.valueOf(jvmID.getId()));
Release-3.0.0,Final commmand
Release-3.0.0,G1 params
Release-3.0.0,Add the env variables passed by the user
Release-3.0.0,Set logging level in the environment.
Release-3.0.0,Setup the log4j prop
Release-3.0.0,Add main class and its arguments
Release-3.0.0,Final commmand
Release-3.0.0,"if amTask is not null, we should clone task state from it"
Release-3.0.0,"if all parameter server complete commit, master can commit now"
Release-3.0.0,restartPS(psLoc);
Release-3.0.0,check whether parameter server heartbeat timeout
Release-3.0.0,Transitions from the NEW state.
Release-3.0.0,Transitions from the UNASSIGNED state.
Release-3.0.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
Release-3.0.0,Transitions from the ASSIGNED state.
Release-3.0.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
Release-3.0.0,PA_CONTAINER_LAUNCHED event
Release-3.0.0,Transitions from the PSAttemptStateInternal.RUNNING state.
Release-3.0.0,Transitions from the PSAttemptStateInternal.COMMITTING state
Release-3.0.0,Transitions from the PSAttemptStateInternal.KILLED state
Release-3.0.0,Transitions from the PSAttemptStateInternal.FAILED state
Release-3.0.0,create the topology tables
Release-3.0.0,reqeuest resource:send a resource request to the resource allocator
Release-3.0.0,"Once the resource is applied, build and send the launch request to the container launcher"
Release-3.0.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
Release-3.0.0,resource allocator
Release-3.0.0,set the launch time
Release-3.0.0,add the ps attempt to the heartbeat timeout monitoring list
Release-3.0.0,parse ps attempt location and put it to location manager
Release-3.0.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
Release-3.0.0,or failed
Release-3.0.0,remove ps attempt id from heartbeat timeout monitor list
Release-3.0.0,release container:send a release request to container launcher
Release-3.0.0,TODO: 2019/5/5
Release-3.0.0,set the finish time only if launch time is set
Release-3.0.0,private long scheduledTime;
Release-3.0.0,Transitions from the NEW state.
Release-3.0.0,Transitions from the SCHEDULED state.
Release-3.0.0,Transitions from the RUNNING state.
Release-3.0.0,"another attempt launched,"
Release-3.0.0,Transitions from the SUCCEEDED state
Release-3.0.0,Transitions from the KILLED state
Release-3.0.0,Transitions from the FAILED state
Release-3.0.0,add diagnostic
Release-3.0.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-3.0.0,Refresh ps location & matrix meta
Release-3.0.0,start a new attempt for this ps
Release-3.0.0,notify ps manager
Release-3.0.0,"getContext().getLocationManager().setPsLocation(id, null);"
Release-3.0.0,add diagnostic
Release-3.0.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-3.0.0,start a new attempt for this ps
Release-3.0.0,notify ps manager
Release-3.0.0,notify the event handler of state change
Release-3.0.0,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
Release-3.0.0,"if forcedState is set, just return"
Release-3.0.0,else get state from state machine
Release-3.0.0,add this worker group to the success set
Release-3.0.0,check if all worker group run or run over
Release-3.0.0,add this worker group to the success set
Release-3.0.0,check if all worker group run over
Release-3.0.0,add this worker group to the failed set
Release-3.0.0,check if too many worker groups are failed or killed
Release-3.0.0,notify a run failed event
Release-3.0.0,add this worker group to the failed set
Release-3.0.0,check if too many worker groups are failed or killed
Release-3.0.0,notify a run failed event
Release-3.0.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
Release-3.0.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
Release-3.0.0,just return the total task number now
Release-3.0.0,TODO
Release-3.0.0,check whether worker heartbeat timeout
Release-3.0.0,"if workerAttempt is not null, we should clone task state from it"
Release-3.0.0,from NEW state
Release-3.0.0,from SCHEDULED state
Release-3.0.0,get data splits location for data locality
Release-3.0.0,reqeuest resource:send a resource request to the resource allocator
Release-3.0.0,"once the resource is applied, build and send the launch request to the container launcher"
Release-3.0.0,notify failed message to the worker
Release-3.0.0,notify killed message to the worker
Release-3.0.0,release the allocated container
Release-3.0.0,notify failed message to the worker
Release-3.0.0,remove the worker attempt from heartbeat timeout listen list
Release-3.0.0,release the allocated container
Release-3.0.0,notify killed message to the worker
Release-3.0.0,remove the worker attempt from heartbeat timeout listen list
Release-3.0.0,clean the container
Release-3.0.0,notify failed message to the worker
Release-3.0.0,remove the worker attempt from heartbeat timeout listen list
Release-3.0.0,record the finish time
Release-3.0.0,clean the container
Release-3.0.0,notify killed message to the worker
Release-3.0.0,remove the worker attempt from heartbeat timeout listening list
Release-3.0.0,record the finish time
Release-3.0.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
Release-3.0.0,set worker attempt location
Release-3.0.0,notify the register message to the worker
Release-3.0.0,record the launch time
Release-3.0.0,update worker attempt metrics
Release-3.0.0,update tasks metrics
Release-3.0.0,clean the container
Release-3.0.0,notify the worker attempt run successfully message to the worker
Release-3.0.0,record the finish time
Release-3.0.0,todo
Release-3.0.0,init a worker attempt for the worker
Release-3.0.0,schedule the worker attempt
Release-3.0.0,add diagnostic
Release-3.0.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-3.0.0,init and start a new attempt for this ps
Release-3.0.0,notify worker manager
Release-3.0.0,add diagnostic
Release-3.0.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-3.0.0,init and start a new attempt for this ps
Release-3.0.0,notify worker manager
Release-3.0.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
Release-3.0.0,register to Yarn RM
Release-3.0.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
Release-3.0.0,"catch YarnException or YarnRuntimeException, we should exit and need not retry"
Release-3.0.0,build heartbeat request
Release-3.0.0,send heartbeat request to rm
Release-3.0.0,"This can happen if the RM has been restarted. If it is in that state,"
Release-3.0.0,this application must clean itself up.
Release-3.0.0,Setting NMTokens
Release-3.0.0,assgin containers
Release-3.0.0,"if some container is not assigned, release them"
Release-3.0.0,handle finish containers
Release-3.0.0,dispatch container exit message to corresponding components
Release-3.0.0,killed by framework
Release-3.0.0,killed by framework
Release-3.0.0,get application finish state
Release-3.0.0,build application diagnostics
Release-3.0.0,TODO:add a job history for angel
Release-3.0.0,build unregister request
Release-3.0.0,send unregister request to rm
Release-3.0.0,Note this down for next interaction with ResourceManager
Release-3.0.0,based on blacklisting comments above we can end up decrementing more
Release-3.0.0,than requested. so guard for that.
Release-3.0.0,send the updated resource request to RM
Release-3.0.0,send 0 container count requests also to cancel previous requests
Release-3.0.0,Update resource requests
Release-3.0.0,try to assign to all nodes first to match node local
Release-3.0.0,try to match all rack local
Release-3.0.0,assign remaining
Release-3.0.0,Update resource requests
Release-3.0.0,send the container-assigned event to task attempt
Release-3.0.0,build the start container request use launch context
Release-3.0.0,send the start request to Yarn nm
Release-3.0.0,send the message that the container starts successfully to the corresponding component
Release-3.0.0,"after launching, send launched event to task attempt to move"
Release-3.0.0,it from ASSIGNED to RUNNING state
Release-3.0.0,send the message that the container starts failed to the corresponding component
Release-3.0.0,kill the remote container if already launched
Release-3.0.0,start a thread pool to startup the container
Release-3.0.0,See if we need up the pool size only if haven't reached the
Release-3.0.0,maximum limit yet.
Release-3.0.0,nodes where containers will run at *this* point of time. This is
Release-3.0.0,*not* the cluster size and doesn't need to be.
Release-3.0.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
Release-3.0.0,later is just a buffer so we are not always increasing the
Release-3.0.0,pool-size
Release-3.0.0,the events from the queue are handled in parallel
Release-3.0.0,using a thread pool
Release-3.0.0,return if already stopped
Release-3.0.0,shutdown any containers that might be left running
Release-3.0.0,Add one sync matrix
Release-3.0.0,addSyncMatrix();
Release-3.0.0,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
Release-3.0.0,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
Release-3.0.0,"LOG.info(""ps id = "" + psEntry.getKey());"
Release-3.0.0,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
Release-3.0.0,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
Release-3.0.0,"LOG.info(""matrix id = "" + metaEntry.getKey());"
Release-3.0.0,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
Release-3.0.0,}
Release-3.0.0,}
Release-3.0.0,get matrix ids in the parameter server report
Release-3.0.0,get the matrices parameter server need to create and delete
Release-3.0.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
Release-3.0.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
Release-3.0.0,Init control connection manager
Release-3.0.0,Get ps locations from master and put them to the location cache.
Release-3.0.0,Build and initialize rpc client to master
Release-3.0.0,Get psagent id
Release-3.0.0,Build PS control rpc client manager
Release-3.0.0,Build local location
Release-3.0.0,Initialize matrix meta information
Release-3.0.0,Start all services
Release-3.0.0,Stop all modules
Release-3.0.0,Stop all modules
Release-3.0.0,clock first
Release-3.0.0,wait
Release-3.0.0,Update generic resource counters
Release-3.0.0,Updating resources specified in ResourceCalculatorProcessTree
Release-3.0.0,Remove the CPU time consumed previously by JVM reuse
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,/ Plus a vector/matrix to the matrix stored in pss
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,/ Update a vector/matrix to the matrix stored in pss
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,/ Get values from pss use row/column indices
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"/ PSF get/update, use can implement their own psf"
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,/ Get a row or a batch of rows
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,Just return
Release-3.0.0,Just return
Release-3.0.0,Just return
Release-3.0.0,Just return
Release-3.0.0,Return a empty vector
Release-3.0.0,Return a empty vector
Release-3.0.0,Return a empty vector
Release-3.0.0,Return a empty vector
Release-3.0.0,Return a empty vector
Release-3.0.0,Return a empty vector
Release-3.0.0,Return a empty vector
Release-3.0.0,Return a empty vector
Release-3.0.0,"checkNotNull(func, ""func"");"
Release-3.0.0,Return a empty vector
Release-3.0.0,"checkNotNull(func, ""func"");"
Release-3.0.0,Return a empty vector
Release-3.0.0,"checkNotNull(func, ""func"");"
Release-3.0.0,Return a empty vector
Release-3.0.0,"checkNotNull(func, ""func"");"
Release-3.0.0,Return a empty vector
Release-3.0.0,"checkNotNull(func, ""func"");"
Release-3.0.0,Return a empty vector
Release-3.0.0,"checkNotNull(func, ""func"");"
Release-3.0.0,Return a empty vector
Release-3.0.0,"checkNotNull(func, ""func"");"
Release-3.0.0,Return a empty vector
Release-3.0.0,"checkNotNull(func, ""func"");"
Release-3.0.0,Return a empty vector
Release-3.0.0,Sort the partitions by start column index
Release-3.0.0,Generate a flush request and put it to request queue
Release-3.0.0,Generate a clock request and put it to request queue
Release-3.0.0,Generate a merge request and put it to request queue
Release-3.0.0,Generate a merge request and put it to request queue
Release-3.0.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
Release-3.0.0,matrix
Release-3.0.0,and add it to cache maps
Release-3.0.0,Add the message to the tree map
Release-3.0.0,"If there are flush / clock requests blocked, we need to put this merge request into"
Release-3.0.0,the waiting queue
Release-3.0.0,Launch a merge worker to merge the update to matrix op log cache
Release-3.0.0,Remove the message from the tree map
Release-3.0.0,Wake up blocked flush/clock request
Release-3.0.0,Add flush/clock request to listener list to waiting for all the existing
Release-3.0.0,updates are merged
Release-3.0.0,Wake up blocked flush/clock request
Release-3.0.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
Release-3.0.0,blocked.
Release-3.0.0,Get next merge message sequence id
Release-3.0.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
Release-3.0.0,position
Release-3.0.0,Wake up blocked merge requests
Release-3.0.0,Get minimal sequence id from listeners
Release-3.0.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
Release-3.0.0,should flush updates to local matrix storage
Release-3.0.0,Doing average or not
Release-3.0.0,Filter un-important update
Release-3.0.0,Split this row according the matrix partitions
Release-3.0.0,Set split context
Release-3.0.0,Remove the row from matrix
Release-3.0.0,buf.writeDouble(0.0);
Release-3.0.0,TODO:
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,"LOG.error(""channel "" + ctx.channel() + "" inactive"");"
Release-3.0.0,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
Release-3.0.0,"LOG.error(""put response message queue failed "", e);"
Release-3.0.0,Use Epoll for linux
Release-3.0.0,Update location table
Release-3.0.0,Remove the server from failed list
Release-3.0.0,Notify refresh success message to request dispatcher
Release-3.0.0,Check PS exist or not
Release-3.0.0,Check heartbeat timeout
Release-3.0.0,getPSState(entry.getKey());
Release-3.0.0,Check PS restart or not
Release-3.0.0,private final HashSet<ParameterServerId> refreshingServerSet;
Release-3.0.0,Add it to failed rpc list
Release-3.0.0,Add the server to gray server list
Release-3.0.0,Add it to failed rpc list
Release-3.0.0,Add the server to gray server list
Release-3.0.0,Move from gray server list to failed server list
Release-3.0.0,Handle the RPCS to this server
Release-3.0.0,Submit the schedulable failed get RPCS
Release-3.0.0,Submit new get RPCS
Release-3.0.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-3.0.0,"If the queue is empty, just return 0"
Release-3.0.0,"If request is not over limit, just submit it"
Release-3.0.0,Submit the schedulable failed get RPCS
Release-3.0.0,Submit new put RPCS
Release-3.0.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-3.0.0,"LOG.info(""choose put server "" + psIds[index]);"
Release-3.0.0,Check all pending RPCS
Release-3.0.0,Check get channel context
Release-3.0.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-3.0.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-3.0.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-3.0.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-3.0.0,channelManager.printPools();
Release-3.0.0,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
Release-3.0.0,if(ts - entry.getValue() > requestTimeOut * 2)  {
Release-3.0.0,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
Release-3.0.0,"+ "" milliseconds, close all channels to it"");"
Release-3.0.0,closeChannels(entry.getKey());
Release-3.0.0,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
Release-3.0.0,}
Release-3.0.0,}
Release-3.0.0,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
Release-3.0.0,Remove all pending RPCS
Release-3.0.0,Close all channel to this PS
Release-3.0.0,private Channel getChannel(Location loc) throws Exception {
Release-3.0.0,"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));"
Release-3.0.0,}
Release-3.0.0,private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {
Release-3.0.0,"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext"
Release-3.0.0,.get()
Release-3.0.0,.getConf()
Release-3.0.0,".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,"
Release-3.0.0,AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));
Release-3.0.0,}
Release-3.0.0,Get server id and location for this request
Release-3.0.0,"If location is null, means that the server is not ready"
Release-3.0.0,Get the channel for the location
Release-3.0.0,Check if need get token first
Release-3.0.0,Serialize the request
Release-3.0.0,Send the request
Release-3.0.0,get a channel to server from pool
Release-3.0.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-3.0.0,request.getContext().setChannelPool(pool);
Release-3.0.0,Allocate the bytebuf and serialize the request
Release-3.0.0,find the partition request context from cache
Release-3.0.0,"updateMatrixCache(request.getPartKey(), response.getPartition());"
Release-3.0.0,"updateMatrixCache(request.getPartKey(), response.getRowsSplit());"
Release-3.0.0,TODO
Release-3.0.0,ServerRow rowSplit = PSAgentContext.get().getMatricesCache()
Release-3.0.0,".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),"
Release-3.0.0,request.getRowIndex());
Release-3.0.0,response.setRowSplit(rowSplit);
Release-3.0.0,"updateMatrixCache(request.getPartKey(), response.getRowSplit());"
Release-3.0.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-3.0.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-3.0.0,TODO
Release-3.0.0,"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {"
Release-3.0.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);"
Release-3.0.0,}
Release-3.0.0,"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {"
Release-3.0.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);"
Release-3.0.0,}
Release-3.0.0,"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {"
Release-3.0.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);"
Release-3.0.0,}
Release-3.0.0,Get partitions for this row
Release-3.0.0,Distinct get row requests
Release-3.0.0,Get row splits of this row from the matrix cache first
Release-3.0.0,responseCache.addSubResponse(rowSplit);
Release-3.0.0,"If the row split does not exist in cache, get it from parameter server"
Release-3.0.0,Split the param use matrix partitions
Release-3.0.0,Send request to PSS
Release-3.0.0,Split the matrix oplog according to the matrix partitions
Release-3.0.0,"If need update clock, we should send requests to all partitions"
Release-3.0.0,Send request to PSS
Release-3.0.0,Filter the rowIds which are fetching now
Release-3.0.0,Send the rowIndex to rpc dispatcher and return immediately
Release-3.0.0,"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));"
Release-3.0.0,"LOG.info(""start to request "" + requestId);"
Release-3.0.0,"LOG.info(""start to request "" + requestId);"
Release-3.0.0,Split param use matrix partitons
Release-3.0.0,"If all sub-results are received, just remove request and result cache"
Release-3.0.0,Split this row according the matrix partitions
Release-3.0.0,Set split context
Release-3.0.0,Split this row according the matrix partitions
Release-3.0.0,Set split context
Release-3.0.0,long startTs = System.currentTimeMillis();
Release-3.0.0,"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));"
Release-3.0.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
Release-3.0.0,Generate dispatch items and add them to the corresponding queues
Release-3.0.0,Filter the rowIds which are fetching now
Release-3.0.0,Sort the parts by partitionId
Release-3.0.0,Sort partition keys use start column index
Release-3.0.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-3.0.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-3.0.0,});
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,Sort the parts by partitionId
Release-3.0.0,Sort partition keys use start column index
Release-3.0.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-3.0.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-3.0.0,});
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,Put the row split to the cache(row index to row splits map)
Release-3.0.0,"If all splits of the row are received, means this row can be merged"
Release-3.0.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-3.0.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-3.0.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-3.0.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-3.0.0,TODO
Release-3.0.0,TODO
Release-3.0.0,/////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,TODO
Release-3.0.0,buf.writeDouble(0);
Release-3.0.0,TODO
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,Now we just support pipelined row splits merging for dense type row
Release-3.0.0,Pre-fetching is disable default
Release-3.0.0,matrix id to clock map
Release-3.0.0,"task index, it must be unique for whole application"
Release-3.0.0,Deserialize data splits meta
Release-3.0.0,Get workers
Release-3.0.0,Send request to every ps
Release-3.0.0,Wait the responses
Release-3.0.0,Update clock cache
Release-3.0.0,if(syncNum % 1024 == 0) {
Release-3.0.0,}
Release-3.0.0,"Use simple flow, do not use any cache"
Release-3.0.0,Get row from cache.
Release-3.0.0,"if row clock is satisfy ssp staleness limit, just return."
Release-3.0.0,Get row from ps.
Release-3.0.0,Wait until the clock value of this row is greater than or equal to the value
Release-3.0.0,"For ASYNC mode, just get from pss."
Release-3.0.0,"For BSP/SSP, get rows from storage/cache first"
Release-3.0.0,Get from ps.
Release-3.0.0,Wait until the clock value of this row is greater than or equal to the value
Release-3.0.0,"For ASYNC, just get rows from pss."
Release-3.0.0,no more retries.
Release-3.0.0,calculate sleep time and return.
Release-3.0.0,parse the i-th sleep-time
Release-3.0.0,parse the i-th number-of-retries
Release-3.0.0,calculateSleepTime may overflow.
Release-3.0.0,"A few common retry policies, with no delays."
Release-3.0.0,int size = rows.length;
Release-3.0.0,int size = rows.length;
Release-3.0.0,int size = rows.size();
Release-3.0.0,int size = rows.size();
Release-3.0.0,int size = rows.size();
Release-3.0.0,int size = rows.size();
Release-3.0.0,int size = rows.size();
Release-3.0.0,int size = rows.size();
Release-3.0.0,row.getStorage().serialize(dataOutputStream);
Release-3.0.0,row.getStorage().deserialize(input);
Release-3.0.0,Save partitions to files use fork-join
Release-3.0.0,Write the ps matrix meta to the meta file
Release-3.0.0,matrix.startServering();
Release-3.0.0,return;
Release-3.0.0,Read matrix meta from meta file
Release-3.0.0,Load partitions from file use fork-join
Release-3.0.0,close is a local operation and should finish within milliseconds; timeout just to be safe
Release-3.0.0,response will be null for one way messages.
Release-3.0.0,maxFrameLength = 2G
Release-3.0.0,lengthFieldOffset = 0
Release-3.0.0,lengthFieldLength = 8
Release-3.0.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
Release-3.0.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
Release-3.0.0,"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())"
Release-3.0.0,.toString();
Release-3.0.0,indicates whether this connection's life cycle is managed
Release-3.0.0,See if we already have a connection (common case)
Release-3.0.0,create a unique lock for this RS + protocol (if necessary)
Release-3.0.0,get the RS lock
Release-3.0.0,do one more lookup in case we were stalled above
Release-3.0.0,Only create isa when we need to.
Release-3.0.0,definitely a cache miss. establish an RPC for
Release-3.0.0,this RS
Release-3.0.0,Throw what the RemoteException was carrying.
Release-3.0.0,check
Release-3.0.0,every
Release-3.0.0,minutes
Release-3.0.0,TODO
Release-3.0.0,创建failoverHandler
Release-3.0.0,"The number of times this invocation handler has ever been failed over,"
Release-3.0.0,before this method invocation attempt. Used to prevent concurrent
Release-3.0.0,failed method invocations from triggering multiple failover attempts.
Release-3.0.0,Make sure that concurrent failed method invocations
Release-3.0.0,only cause a
Release-3.0.0,single actual fail over.
Release-3.0.0,RpcController + Message in the method args
Release-3.0.0,(generated code from RPC bits in .proto files have
Release-3.0.0,RpcController)
Release-3.0.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
Release-3.0.0,+ (System.currentTimeMillis() - beforeConstructTs));
Release-3.0.0,get an instance of the method arg type
Release-3.0.0,RpcController + Message in the method args
Release-3.0.0,(generated code from RPC bits in .proto files have
Release-3.0.0,RpcController)
Release-3.0.0,Message (hand written code usually has only a single
Release-3.0.0,argument)
Release-3.0.0,log any RPC responses that are slower than the configured
Release-3.0.0,warn
Release-3.0.0,response time or larger than configured warning size
Release-3.0.0,"when tagging, we let TooLarge trump TooSmall to keep"
Release-3.0.0,output simple
Release-3.0.0,note that large responses will often also be slow.
Release-3.0.0,provides a count of log-reported slow responses
Release-3.0.0,RpcController + Message in the method args
Release-3.0.0,(generated code from RPC bits in .proto files have
Release-3.0.0,RpcController)
Release-3.0.0,unexpected
Release-3.0.0,"in the protobuf methods, args[1] is the only significant argument"
Release-3.0.0,for JSON encoding
Release-3.0.0,base information that is reported regardless of type of call
Release-3.0.0,Disable Nagle's Algorithm since we don't want packets to wait
Release-3.0.0,Configure the event pipeline factory.
Release-3.0.0,Make a new connection.
Release-3.0.0,Remove all pending requests (will be canceled after relinquishing
Release-3.0.0,write lock).
Release-3.0.0,Cancel any pending requests by sending errors to the callbacks:
Release-3.0.0,Close the channel:
Release-3.0.0,Close the connection:
Release-3.0.0,Shut down all thread pools to exit.
Release-3.0.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
Release-3.0.0,See NettyServer.prepareResponse for where we write out the response.
Release-3.0.0,"It writes the call.id (int), a boolean signifying any error (and if"
Release-3.0.0,"so the exception name/trace), and the response bytes"
Release-3.0.0,Read the call id.
Release-3.0.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
Release-3.0.0,"instead, it returns a null message object."
Release-3.0.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
Release-3.0.0,System.currentTimeMillis());
Release-3.0.0,"It would be good widen this to just Throwable, but IOException is what we"
Release-3.0.0,allow now
Release-3.0.0,not implemented
Release-3.0.0,not implemented
Release-3.0.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
Release-3.0.0,cache of RpcEngines by protocol
Release-3.0.0,return the RpcEngine configured to handle a protocol
Release-3.0.0,We only handle the ConnectException.
Release-3.0.0,This is the exception we can't handle.
Release-3.0.0,check if timed out
Release-3.0.0,wait for retry
Release-3.0.0,IGNORE
Release-3.0.0,return the RpcEngine that handles a proxy object
Release-3.0.0,The default implementation works synchronously
Release-3.0.0,punt: allocate a new buffer & copy into it
Release-3.0.0,Parse cmd parameters
Release-3.0.0,load hadoop configuration
Release-3.0.0,load angel system configuration
Release-3.0.0,load user configuration:
Release-3.0.0,load user config file
Release-3.0.0,load command line parameters
Release-3.0.0,load user job resource files
Release-3.0.0,load ml conf file for graph based algorithm
Release-3.0.0,load user job jar if it exist
Release-3.0.0,Expand the environment variable
Release-3.0.0,Add default fs(local fs) for lib jars.
Release-3.0.0,"LOG.info(System.getProperty(""user.dir""));"
Release-3.0.0,get tokens for all the required FileSystems..
Release-3.0.0,Whether we need to recursive look into the directory structure
Release-3.0.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-3.0.0,user provided one (if any).
Release-3.0.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-3.0.0,get tokens for all the required FileSystems..
Release-3.0.0,Whether we need to recursive look into the directory structure
Release-3.0.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-3.0.0,user provided one (if any).
Release-3.0.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-3.0.0,a simple hdfs copy function assume src path and dest path are in same hdfs
Release-3.0.0,and FileSystem object has same schema
Release-3.0.0,"If out path exist , just remove it first"
Release-3.0.0,Create parent directory if not exist
Release-3.0.0,Rename
Release-3.0.0,"LOG.warn(""interrupted while sleeping"", ie);"
Release-3.0.0,public static String getHostname() {
Release-3.0.0,try {
Release-3.0.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
Release-3.0.0,} catch (UnknownHostException uhe) {
Release-3.0.0,}
Release-3.0.0,"return new StringBuilder().append("""").append(uhe).toString();"
Release-3.0.0,}
Release-3.0.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
Release-3.0.0,String hostname = getHostname();
Release-3.0.0,String classname = clazz.getSimpleName();
Release-3.0.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
Release-3.0.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
Release-3.0.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
Release-3.0.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
Release-3.0.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
Release-3.0.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
Release-3.0.0,
Release-3.0.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
Release-3.0.0,public void run() {
Release-3.0.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
Release-3.0.0,"this.val$classname + "" at "" + this.val$hostname}));"
Release-3.0.0,}
Release-3.0.0,});
Release-3.0.0,}
Release-3.0.0,"We we interrupted because we're meant to stop? If not, just"
Release-3.0.0,continue ignoring the interruption
Release-3.0.0,Recalculate waitTime.
Release-3.0.0,// Begin delegation to Thread
Release-3.0.0,// End delegation to Thread
Release-3.0.0,instance submitter class
Release-3.0.0,Obtain filename from path
Release-3.0.0,Split filename to prexif and suffix (extension)
Release-3.0.0,Check if the filename is okay
Release-3.0.0,Prepare temporary file
Release-3.0.0,Prepare buffer for data copying
Release-3.0.0,Open and check input stream
Release-3.0.0,Open output stream and copy data between source file in JAR and the temporary file
Release-3.0.0,"If read/write fails, close streams safely before throwing an exception"
Release-3.0.0,"Finally, load the library"
Release-3.0.0,little endian load order
Release-3.0.0,tail
Release-3.0.0,fallthrough
Release-3.0.0,fallthrough
Release-3.0.0,finalization
Release-3.0.0,fmix(h1);
Release-3.0.0,----------
Release-3.0.0,body
Release-3.0.0,----------
Release-3.0.0,tail
Release-3.0.0,----------
Release-3.0.0,finalization
Release-3.0.0,----------
Release-3.0.0,body
Release-3.0.0,----------
Release-3.0.0,tail
Release-3.0.0,----------
Release-3.0.0,finalization
Release-3.0.0,throw new AngelException(e);
Release-3.0.0,JobStateProto jobState = report.getJobState();
Release-3.0.0,Check need load matrices
Release-3.0.0,Used for java code to get a AngelClient instance
Release-3.0.0,Used for python code to get a AngelClient instance
Release-3.0.0,load user job resource files
Release-3.0.0,setLocalAddr();
Release-3.0.0,2.get job id
Release-3.0.0,5.write configuration to a xml file
Release-3.0.0,8.get app master client
Release-3.0.0,Write job file to JobTracker's fs
Release-3.0.0,the leaf level file should be readable by others
Release-3.0.0,the subdirs in the path should have execute permissions for
Release-3.0.0,others
Release-3.0.0,2.get job id
Release-3.0.0,Credentials credentials = new Credentials();
Release-3.0.0,4.copy resource files to hdfs
Release-3.0.0,5.write configuration to a xml file
Release-3.0.0,6.create am container context
Release-3.0.0,7.Submit to ResourceManager
Release-3.0.0,8.get app master client
Release-3.0.0,Create a number of filenames in the JobTracker's fs namespace
Release-3.0.0,add all the command line files/ jars and archive
Release-3.0.0,first copy them to jobtrackers filesystem
Release-3.0.0,should not throw a uri exception
Release-3.0.0,should not throw an uri excpetion
Release-3.0.0,set the timestamps of the archives and files
Release-3.0.0,set the public/private visibility of the archives and files
Release-3.0.0,get DelegationToken for each cached file
Release-3.0.0,check if we do not need to copy the files
Release-3.0.0,is jt using the same file system.
Release-3.0.0,just checking for uri strings... doing no dns lookups
Release-3.0.0,to see if the filesystems are the same. This is not optimal.
Release-3.0.0,but avoids name resolution.
Release-3.0.0,this might have name collisions. copy will throw an exception
Release-3.0.0,parse the original path to create new path
Release-3.0.0,check for ports
Release-3.0.0,Write job file to JobTracker's fs
Release-3.0.0,Setup resource requirements
Release-3.0.0,Setup LocalResources
Release-3.0.0,Setup security tokens
Release-3.0.0,Setup the command to run the AM
Release-3.0.0,Add AM user command opts
Release-3.0.0,Final command
Release-3.0.0,Setup the CLASSPATH in environment
Release-3.0.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
Release-3.0.0,Setup the environment variables for Admin first
Release-3.0.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
Release-3.0.0,Parse distributed cache
Release-3.0.0,Setup ContainerLaunchContext for AM container
Release-3.0.0,Set up the ApplicationSubmissionContext
Release-3.0.0,private volatile PS2PSPusherImpl ps2PSPusher;
Release-3.0.0,TODO
Release-3.0.0,Add tokens to new user so that it may execute its task correctly.
Release-3.0.0,TODO
Release-3.0.0,to exit
Release-3.0.0,TODO
Release-3.0.0,TODO
Release-3.0.0,ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));
Release-3.0.0,context.getSnapshotManager().processRecovery();
Release-3.0.0,Recover PS from snapshot or load path
Release-3.0.0,1. First check old snapshot
Release-3.0.0,2. Check new checkpoints
Release-3.0.0,3. Check load path setting and old save result
Release-3.0.0,Just init it again
Release-3.0.0,TODO
Release-3.0.0,if(ps2PSPusher != null) {
Release-3.0.0,ps2PSPusher.start();
Release-3.0.0,}
Release-3.0.0,public PS2PSPusherImpl getPs2PSPusher() {
Release-3.0.0,return ps2PSPusher;
Release-3.0.0,}
Release-3.0.0,"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");"
Release-3.0.0,"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);"
Release-3.0.0,Release the input buffer
Release-3.0.0,Release the input buffer
Release-3.0.0,"1. handle the rpc, get the response"
Release-3.0.0,Release the input buffer
Release-3.0.0,2. Serialize the response
Release-3.0.0,Send the serialized response
Release-3.0.0,Exception happened
Release-3.0.0,write seq id
Release-3.0.0,Just serialize the head
Release-3.0.0,Exception happened
Release-3.0.0,Allocate result buffer
Release-3.0.0,Exception happened
Release-3.0.0,Just serialize the head
Release-3.0.0,Exception happened
Release-3.0.0,runningContext.printToken();
Release-3.0.0,Reset the response and allocate buffer again
Release-3.0.0,Get partition and check the partition state
Release-3.0.0,Get the stored pss for this partition
Release-3.0.0,"Check this ps is the master ps for this location, only master ps can accept the update"
Release-3.0.0,Check the partition state again
Release-3.0.0,Start to put the update to the slave pss
Release-3.0.0,TODO
Release-3.0.0,"context.getPS2PSPusher().put(request, in, partLoc);"
Release-3.0.0,Get partition and check the partition state
Release-3.0.0,Get the stored pss for this partition
Release-3.0.0,"Check this ps is the master ps for this partition, if not, just return failed"
Release-3.0.0,Start to put the update to the slave pss
Release-3.0.0,TODO
Release-3.0.0,return ServerState.GENERAL;
Release-3.0.0,Use Epoll for linux
Release-3.0.0,public String uuid;
Release-3.0.0,TODO:
Release-3.0.0,part = new ServerPartition();
Release-3.0.0,TODO:
Release-3.0.0,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
Release-3.0.0,this.channelPool = channelPool;
Release-3.0.0,}
Release-3.0.0,private final ParameterServer psServer;
Release-3.0.0,Create and start workers
Release-3.0.0,Set workers
Release-3.0.0,Create and start workers
Release-3.0.0,Set workers
Release-3.0.0,"If matrix checkpoint path not exist, just return null"
Release-3.0.0,Return the path with maximum checkpoint id
Release-3.0.0,Rename temp to item path
Release-3.0.0,Checkpoint base path = Base dir/matrix name
Release-3.0.0,Path for this checkpoint
Release-3.0.0,Generate tmp path
Release-3.0.0,Delete old checkpoints
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.0.0,and call endWrite/endRead after
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.0.0,and call endWrite/endRead after
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"////// network io method, for model transform"
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,Serailize the head
Release-3.0.0,Serialize the storage
Release-3.0.0,Deserailze the head
Release-3.0.0,Deseralize the storage
Release-3.0.0,Serailize the head
Release-3.0.0,Serialize the storage
Release-3.0.0,Deserailze the head
Release-3.0.0,Deseralize the storage
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.0.0,and call endWrite/endRead after
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.0.0,and call endWrite/endRead after
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.0.0,and call endWrite/endRead after
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.0.0,and call endWrite/endRead after
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.0.0,and call endWrite/endRead after
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-3.0.0,and call endWrite/endRead after
Release-3.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,"Notice: Basic type ServerRow only support Vector as inner storage now, so you can use methods"
Release-3.0.0,to get inner vector for basic type ServerRow.
Release-3.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-3.0.0,Just update the exist element now!!
Release-3.0.0,Just update the exist element now!!
Release-3.0.0,TODO: just check the value is 0 or not now
Release-3.0.0,TODO: just check the value is zero or not now
Release-3.0.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-3.0.0,Attention: Only update the exist values for sorted storage method
Release-3.0.0,Attention: Only update exist element
Release-3.0.0,Attention: Only update the exist values for sorted storage method
Release-3.0.0,Attention: Only update exist element
Release-3.0.0,TODO: just check the value is zero or not now
Release-3.0.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-3.0.0,Valid element number
Release-3.0.0,Element data
Release-3.0.0,Valid element number
Release-3.0.0,Deserialize the data
Release-3.0.0,Element data
Release-3.0.0,Valid element number
Release-3.0.0,Element data
Release-3.0.0,Valid element number
Release-3.0.0,Deserialize the data
Release-3.0.0,Attention: Only update the exist values for sorted storage method
Release-3.0.0,Attention: Only update exist element
Release-3.0.0,TODO: just check the value is zero or not now
Release-3.0.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-3.0.0,TODO: just check the value is 0 or not now
Release-3.0.0,Just update the exist element now!!
Release-3.0.0,Just update the exist element now!!
Release-3.0.0,"Use sparse storage method, as some elements in the array maybe null"
Release-3.0.0,Array length
Release-3.0.0,Valid element number
Release-3.0.0,Element data
Release-3.0.0,Array len
Release-3.0.0,Valid element number
Release-3.0.0,"Use sparse storage method, as some elements in the array maybe null"
Release-3.0.0,Array length
Release-3.0.0,Valid element number
Release-3.0.0,Element data
Release-3.0.0,Element data
Release-3.0.0,Array len
Release-3.0.0,Valid element number
Release-3.0.0,Attention: Only update the exist values for sorted storage method
Release-3.0.0,Attention: Only update exist element
Release-3.0.0,TODO: just check the value is zero or not now
Release-3.0.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-3.0.0,Row type
Release-3.0.0,Storage method
Release-3.0.0,Key type
Release-3.0.0,Value type
Release-3.0.0,Vector dim
Release-3.0.0,Vector length
Release-3.0.0,Vector data
Release-3.0.0,Row type
Release-3.0.0,Storage method
Release-3.0.0,Key type
Release-3.0.0,Value type
Release-3.0.0,Vector dim
Release-3.0.0,Vector length
Release-3.0.0,Init the vector
Release-3.0.0,Vector data
Release-3.0.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.0.0,Get the array pair
Release-3.0.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.0.0,Get the array pair
Release-3.0.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.0.0,Get the array pair
Release-3.0.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.0.0,Get the array pair
Release-3.0.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.0.0,Get the array pair
Release-3.0.0,Impossible now
Release-3.0.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.0.0,Get the array pair
Release-3.0.0,Impossible now
Release-3.0.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.0.0,Get the array pair
Release-3.0.0,Impossible now
Release-3.0.0,"Sparse storage, use the iterator to avoid array copy"
Release-3.0.0,Get the array pair
Release-3.0.0,Impossible now
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,"If use sorted storage, we should get the array pair first"
Release-3.0.0,Just update the exist element now!!
Release-3.0.0,Just update the exist element now!!
Release-3.0.0,TODO: just check the value is 0 or not now
Release-3.0.0,Just update the exist element now!!
Release-3.0.0,Just update the exist element now!!
Release-3.0.0,TODO: just check the value is 0 or not now
Release-3.0.0,Valid element number
Release-3.0.0,Element data
Release-3.0.0,Valid element number
Release-3.0.0,Deserialize the data
Release-3.0.0,Element data
Release-3.0.0,Valid element number
Release-3.0.0,Element data
Release-3.0.0,Valid element number
Release-3.0.0,Deserialize the data
Release-3.0.0,private final List<PartitionKey> partitionKeys;
Release-3.0.0,Get server partition class
Release-3.0.0,"If partition class is not set, just use the default partition class"
Release-3.0.0,Get server partition storage class type
Release-3.0.0,Get value class
Release-3.0.0,"if col == -1, we use the start/end index to calculate range,"
Release-3.0.0,we use double to store the range value since two long minus might exceed the
Release-3.0.0,range of long.
Release-3.0.0,Serialize the head
Release-3.0.0,Serialize the storage
Release-3.0.0,Deserialize the head
Release-3.0.0,Deseralize the storage
Release-3.0.0,Row base partition
Release-3.0.0,"If storage class is not set, use default DenseServerRowsStorage"
Release-3.0.0,Serialize values
Release-3.0.0,Deserialize values
Release-3.0.0,Array size
Release-3.0.0,Actual write size
Release-3.0.0,Rows data
Release-3.0.0,Row id
Release-3.0.0,Row type
Release-3.0.0,Row data
Release-3.0.0,Array size
Release-3.0.0,Actual write row number
Release-3.0.0,Rows data
Release-3.0.0,Row id
Release-3.0.0,Create empty server row
Release-3.0.0,Row data
Release-3.0.0,Rows data
Release-3.0.0,TODO
Release-3.0.0,Serialize row offsets
Release-3.0.0,Serialize column offsets
Release-3.0.0,Deserialize row offset
Release-3.0.0,Deserialize row offset
Release-3.0.0,"If storage is set, just get a instance"
Release-3.0.0,"If storage is not set, use default"
Release-3.0.0,"If storage is set, just get a instance"
Release-3.0.0,"If storage is not set, use default"
Release-3.0.0,Map size
Release-3.0.0,Actual write size
Release-3.0.0,Rows data
Release-3.0.0,Row id
Release-3.0.0,Row type
Release-3.0.0,Row data
Release-3.0.0,Array size
Release-3.0.0,Actual write row number
Release-3.0.0,Rows data
Release-3.0.0,Row id
Release-3.0.0,Create empty server row
Release-3.0.0,Row data
Release-3.0.0,Rows data
Release-3.0.0,Use Epoll for linux
Release-3.0.0,find the partition request context from cache
Release-3.0.0,get a channel to server from pool
Release-3.0.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-3.0.0,channelManager.removeChannelPool(loc);
Release-3.0.0,Generate seq id
Release-3.0.0,Create a RecoverPartRequest
Release-3.0.0,Serialize the request
Release-3.0.0,Change the seqId for the request
Release-3.0.0,Serialize the request
Release-3.0.0,"First check the state of the channels in the pool, if a channel is unused, just return"
Release-3.0.0,"If all channels are in use, create a new channel or wait"
Release-3.0.0,Create a new channel
Release-3.0.0,"add the PSAgentContext,need fix"
Release-3.0.0,If col == -1 and start/end not set
Release-3.0.0,start/end set
Release-3.0.0,"for dense type, we need to set the colNum to set dim for vectors"
Release-3.0.0,"colNum set, start/end not set"
Release-3.0.0,Row number must > 0
Release-3.0.0,"both set, check its valid"
Release-3.0.0,TODO:add more vector type
Release-3.0.0,TODO : subDim set
Release-3.0.0,Sort the parts by partitionId
Release-3.0.0,Sort partition keys use start column index
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,Sort the parts by partitionId
Release-3.0.0,Sort partition keys use start column index
Release-3.0.0,"For each partition, we generate a update split."
Release-3.0.0,"Although the split is empty for partitions those without any update data,"
Release-3.0.0,we still need to generate a update split to update the clock info on ps.
Release-3.0.0,Split updates
Release-3.0.0,Shuffle update splits
Release-3.0.0,Generate part update parameters
Release-3.0.0,"Set split context: partition key, use int key for long key vector or not ect"
Release-3.0.0,write the max abs
Release-3.0.0,get configuration from envs
Release-3.0.0,get master location
Release-3.0.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-3.0.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-3.0.0,add dense double matrix
Release-3.0.0,TODO Auto-generated method stub
Release-3.0.0,TODO Auto-generated method stub
Release-3.0.0,TODO Auto-generated method stub
Release-3.0.0,get configuration from config file
Release-3.0.0,set localDir with enviroment set by nm.
Release-3.0.0,get master location
Release-3.0.0,init task manager and start tasks
Release-3.0.0,start heartbeat thread
Release-3.0.0,taskManager.assignTaskIds(response.getTaskidsList());
Release-3.0.0,todo
Release-3.0.0,"if worker timeout, it may be knocked off."
Release-3.0.0,"SUCCESS, do nothing"
Release-3.0.0,heartbeatFailedTime = 0;
Release-3.0.0,private KEY currentKey;
Release-3.0.0,will be created
Release-3.0.0,TODO Auto-generated method stub
Release-3.0.0,Bitmap bitmap = new Bitmap();
Release-3.0.0,int max = indexArray[size - 1];
Release-3.0.0,byte [] bitIndexArray = new byte[max / 8 + 1];
Release-3.0.0,for(int i = 0; i < size; i++){
Release-3.0.0,int bitIndex = indexArray[i] >> 3;
Release-3.0.0,int bitOffset = indexArray[i] - (bitIndex << 3);
Release-3.0.0,switch(bitOffset){
Release-3.0.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
Release-3.0.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
Release-3.0.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
Release-3.0.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
Release-3.0.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
Release-3.0.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
Release-3.0.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
Release-3.0.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
Release-3.0.0,}
Release-3.0.0,}
Release-3.0.0,"true, false"
Release-3.0.0,//////////////////////////////
Release-3.0.0,Application Configs
Release-3.0.0,//////////////////////////////
Release-3.0.0,//////////////////////////////
Release-3.0.0,Master Configs
Release-3.0.0,//////////////////////////////
Release-3.0.0,//////////////////////////////
Release-3.0.0,Worker Configs
Release-3.0.0,//////////////////////////////
Release-3.0.0,//////////////////////////////
Release-3.0.0,Task Configs
Release-3.0.0,//////////////////////////////
Release-3.0.0,//////////////////////////////
Release-3.0.0,ParameterServer Configs
Release-3.0.0,//////////////////////////////
Release-3.0.0,//////////////////////////////
Release-3.0.0,Kubernetes Configs.
Release-3.0.0,//////////////////////////////
Release-3.0.0,////////////////// IPC //////////////////////////
Release-3.0.0,//////////////////////////////
Release-3.0.0,Matrix transfer Configs.
Release-3.0.0,//////////////////////////////
Release-3.0.0,//////////////////////////////
Release-3.0.0,Matrix transfer Configs.
Release-3.0.0,//////////////////////////////
Release-3.0.0,Configs used to ANGEL_PS_PSAGENT running mode future.
Release-3.0.0,model parse
Release-3.0.0,Mark whether use pyangel or not.
Release-3.0.0,private Configuration conf;
Release-3.0.0,"Configuration that should be used in python environment, there should only be one"
Release-3.0.0,configuration instance in each Angel context.
Release-3.0.0,Use private access means jconf should not be changed or modified in this way.
Release-3.0.0,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
Release-3.0.0,Do nothing
Release-3.0.0,To-DO: add other ways to justify different value types
Release-3.0.0,"This is so ugly, must re-implement by more elegance way"
Release-3.0.0,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
Release-3.0.0,and other files submitted by user.
Release-3.0.0,Launch python process
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-3.0.0,get a angel client
Release-3.0.0,add sparse float matrix
Release-3.0.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-3.0.0,siMat.setPartitionClass(CSRPartition.class);
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,Init node neighbors
Release-3.0.0,client.asyncUpdate(new InitNeighborOver(new InitNeighborOverParam(matrixId))).get();
Release-3.0.0,Sample the neighbors
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-3.0.0,get a angel client
Release-3.0.0,add sparse float matrix
Release-3.0.0,siMat.setValidIndexNum(100);
Release-3.0.0,siMat.setColNum(10000000000L);
Release-3.0.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-3.0.0,siMat.setPartitionClass(CSRPartition.class);
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,Init node neighbors and feats
Release-3.0.0,Sample the neighbors
Release-3.0.0,TODO Auto-generated constructor stub
Release-3.0.0,set basic configuration keys
Release-3.0.0,use local deploy mode and dummy dataspliter
Release-3.0.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-3.0.0,get a angel client
Release-3.0.0,add sparse float matrix
Release-3.0.0,Start PS
Release-3.0.0,Start to run application
Release-3.0.0,Init node neighbors
Release-3.0.0,Sample the neighbors
Release-3.0.0,sample continuously beginning from a random index
Release-3.0.0,Get node neighbor number
Release-3.0.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-3.0.0,Get node neighbor number
Release-3.0.0,"If the neighbor number is 0, just return a int[0]"
Release-3.0.0,"If count <= 0 or the neighbor number is less or equal then count, just copy all neighbors to the result array"
Release-3.0.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-3.0.0,Store the total neighbor number of all nodes in rowOffsets
Release-3.0.0,"Put the node ids, node neighbor number, node neighbors to the cache"
Release-3.0.0,No data in this partition
Release-3.0.0,Get total neighbor number
Release-3.0.0,Final matrix column indices: neighbors node ids
Release-3.0.0,Write positions in cloumnIndices for nodes
Release-3.0.0,Copy all cached sub column indices to final column indices
Release-3.0.0,Read position for a sub column indices
Release-3.0.0,Copy column indices for a node to final column indices
Release-3.0.0,Update write position for this node in final column indices
Release-3.0.0,Update the read position in sub column indices
Release-3.0.0,Clear all temp data
Release-3.0.0,Get node neighbor number
Release-3.0.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Data format
Release-3.0.0,Feature number of train data
Release-3.0.0,Tree number
Release-3.0.0,Tree depth
Release-3.0.0,Split number
Release-3.0.0,Feature sample ratio
Release-3.0.0,Ratio of validation
Release-3.0.0,Learning rate
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Set data format
Release-3.0.0,"Set angel resource, #worker, #task, #PS"
Release-3.0.0,Set GBDT algorithm parameters
Release-3.0.0,Set training data path
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set predict data path
Release-3.0.0,Set load model path
Release-3.0.0,Set predict result path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Train batch number per epoch.
Release-3.0.0,Batch number
Release-3.0.0,Model type
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Set data format
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,set sgd LR algorithm parameters #feature #epoch
Release-3.0.0,Set trainning data path
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Train batch number per epoch.
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Set data format
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,set sgd FM algorithm parameters #feature #epoch
Release-3.0.0,Set trainning data path
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,Model type
Release-3.0.0,String modelType = String.valueOf(RowType.T_FLOAT_DENSE);
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,set sgd LR algorithm parameters #feature #epoch
Release-3.0.0,"conf.set(AngelMLConf.ML_MODEL_TYPE(), modelType);"
Release-3.0.0,"conf.setDouble(AngelMLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Set data format
Release-3.0.0,Set trainning data path
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set data format
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,class number
Release-3.0.0,Model type
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Set data format
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,set sgd LR algorithm parameters #feature #epoch
Release-3.0.0,"conf.setStrings(AngelConf.ANGEL_ML_CONF, jsonFile);"
Release-3.0.0,Set log path
Release-3.0.0,Set trainning data path
Release-3.0.0,Set save model path
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set save model path
Release-3.0.0,Set actionType incremental train
Release-3.0.0,Set log path
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set predict result path
Release-3.0.0,Set log path
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,"String savePath = LOCAL_FS + TMP_PATH + ""/model/wideDeep"";"
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,predictTest();
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set training data path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,"String savePath = LOCAL_FS + TMP_PATH + ""/model/wideDeep"";"
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,predictTest();
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,"String savePath = LOCAL_FS + TMP_PATH + ""/model/wideDeep"";"
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,predictTest();
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Cluster center number
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Sample ratio per mini-batch
Release-3.0.0,C
Release-3.0.0,Model type
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-3.0.0,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
Release-3.0.0,"conf.setStrings(AngelConf.ANGEL_ML_CONF, jsonFile);"
Release-3.0.0,Set data format
Release-3.0.0,Set trainning data path
Release-3.0.0,Set save model path
Release-3.0.0,Set log save path
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set save model path
Release-3.0.0,Set actionType incremental train
Release-3.0.0,Set log path
Release-3.0.0,Set testing data path
Release-3.0.0,Set load model path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,incTrain();
Release-3.0.0,predictTest();
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Model type
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Set data format
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,set sgd LR algorithm parameters #feature #epoch
Release-3.0.0,Set trainning data path
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set predict result path
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Data is classification
Release-3.0.0,Model is classification
Release-3.0.0,Train batch number per epoch.
Release-3.0.0,loss delta
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Set data format
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,set sgd LR algorithm parameters #feature #epoch
Release-3.0.0,Set trainning data path
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set save model path
Release-3.0.0,Set actionType incremental train
Release-3.0.0,Set log path
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,Feature number of train data
Release-3.0.0,Total iteration number
Release-3.0.0,Validation sample Ratio
Release-3.0.0,"Data format, libsvm or dummy"
Release-3.0.0,Data is classification
Release-3.0.0,Model is classification
Release-3.0.0,Train batch number per epoch.
Release-3.0.0,Learning rate
Release-3.0.0,Decay of learning rate
Release-3.0.0,Regularization coefficient
Release-3.0.0,Model type
Release-3.0.0,Set local deploy mode
Release-3.0.0,Set basic configuration keys
Release-3.0.0,Set data format
Release-3.0.0,"set angel resource parameters #worker, #task, #PS"
Release-3.0.0,set sgd LR algorithm parameters #feature #epoch
Release-3.0.0,"conf.setStrings(AngelConf.ANGEL_ML_CONF, jsonFile);"
Release-3.0.0,Set trainning data path
Release-3.0.0,Set save model path
Release-3.0.0,Set log path
Release-3.0.0,Set actionType train
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set save model path
Release-3.0.0,Set actionType incremental train
Release-3.0.0,Set log path
Release-3.0.0,Set trainning data path
Release-3.0.0,Set load model path
Release-3.0.0,Set predict result path
Release-3.0.0,Set actionType prediction
Release-3.0.0,TODO: optimize int key indices
Release-3.0.0,"System.out.println(""deserialize cols.length="" + nCols);"
Release-3.0.0,"System.out.print(""deserialize "");"
Release-3.0.0,"System.out.print(cols[c] + "" "");"
Release-3.0.0,System.out.println();
Release-3.0.0,TODO Auto-generated method stub
Release-3.0.0,"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-3.0.0,"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-3.0.0,"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));"
Release-3.0.0,"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));"
Release-3.0.0,start row index for words
Release-3.0.0,start row index for docs
Release-3.0.0,doc ids
Release-3.0.0,topic assignments
Release-3.0.0,word to docs reverse index
Release-3.0.0,count word
Release-3.0.0,build word start index
Release-3.0.0,build word to doc reverse idx
Release-3.0.0,build dks
Release-3.0.0,dks = new TraverseHashMap[n_docs];
Release-3.0.0,for (int d = 0; d < n_docs; d++) {
Release-3.0.0,if (K < Short.MAX_VALUE) {
Release-3.0.0,if (docs.get(d).len < Byte.MAX_VALUE)
Release-3.0.0,dks[d] = new S2BTraverseMap(docs.get(d).len);
Release-3.0.0,if (docs.get(d).len < Short.MAX_VALUE)
Release-3.0.0,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
Release-3.0.0,else
Release-3.0.0,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
Release-3.0.0,} else {
Release-3.0.0,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
Release-3.0.0,}
Release-3.0.0,}
Release-3.0.0,build dks
Release-3.0.0,allocate update maps
Release-3.0.0,Skip if no token for this word
Release-3.0.0,Check whether error when fetching word-topic
Release-3.0.0,Build FTree for current word
Release-3.0.0,current doc
Release-3.0.0,old topic assignment
Release-3.0.0,"Check if error happens. if this happen, it's probably that failures happen to servers."
Release-3.0.0,We need to adjust the memory settings or network fetching parameters.
Release-3.0.0,Update statistics if needed
Release-3.0.0,Calculate psum and sample new topic
Release-3.0.0,Update statistics if needed
Release-3.0.0,Assign new topic
Release-3.0.0,Skip if no token for this word
Release-3.0.0,The starting point
Release-3.0.0,There's always an unused entry.
Release-3.0.0,The starting point
Release-3.0.0,There's always an unused entry.
Release-3.0.0,The starting point
Release-3.0.0,There's always an unused entry.
Release-3.0.0,The starting point
Release-3.0.0,There's always an unused entry.
Release-3.0.0,The starting point
Release-3.0.0,There's always an unused entry.
Release-3.0.0,The starting point
Release-3.0.0,There's always an unused entry.
Release-3.0.0,print();
Release-3.0.0,The starting point
Release-3.0.0,There's always an unused entry.
Release-3.0.0,The starting point
Release-3.0.0,There's always an unused entry.
Release-3.0.0,The starting point
Release-3.0.0,There's always an unused entry.
Release-3.0.0,The starting point
Release-3.0.0,There's always an unused entry.
Release-3.0.0,print();
Release-3.0.0,Write #rows
Release-3.0.0,Write each row
Release-3.0.0,dense
Release-3.0.0,sparse
Release-3.0.0,LOG.info(buf.refCnt());
Release-3.0.0,dense
Release-3.0.0,sparse
Release-3.0.0,calculate columns
Release-3.0.0,loss function
Release-3.0.0,gradient and hessian
Release-3.0.0,"categorical feature set, null: none, empty: all, else: partial"
Release-3.0.0,"node's end index in instancePos, instances in [start, end] belong to a tree node"
Release-3.0.0,initialize the phase
Release-3.0.0,current tree and depth
Release-3.0.0,create loss function
Release-3.0.0,calculate grad info of each instance
Release-3.0.0,"create data sketch, push candidate split value to PS"
Release-3.0.0,1. calculate candidate split value
Release-3.0.0,categorical features
Release-3.0.0,2. push local sketch to PS
Release-3.0.0,the leader worker
Release-3.0.0,merge categorical features
Release-3.0.0,create updates
Release-3.0.0,"pull the global sketch from PS, only called once by each worker"
Release-3.0.0,number of categorical feature
Release-3.0.0,sample feature
Release-3.0.0,push sampled feature set to the current tree
Release-3.0.0,create new tree
Release-3.0.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
Release-3.0.0,calculate gradient
Release-3.0.0,"1. create new tree, initialize tree nodes and node stats"
Release-3.0.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
Release-3.0.0,2.1. pull the sampled features of the current tree
Release-3.0.0,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
Release-3.0.0,"2.2. if use all the features, only called one"
Release-3.0.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
Release-3.0.0,4. set root node to active
Release-3.0.0,"5. reset instance position, set the root node's span"
Release-3.0.0,6. calculate gradient
Release-3.0.0,1. decide nodes that should be calculated
Release-3.0.0,2. decide calculated and subtracted tree nodes
Release-3.0.0,3. calculate threads
Release-3.0.0,wait until all threads finish
Release-3.0.0,4. subtract threads
Release-3.0.0,wait until all threads finish
Release-3.0.0,5. send histograms to PS
Release-3.0.0,6. update histogram cache
Release-3.0.0,clock
Release-3.0.0,find split
Release-3.0.0,"1. find responsible tree node, using RR scheme"
Release-3.0.0,2. pull gradient histogram
Release-3.0.0,2.1. get the name of this node's gradient histogram on PS
Release-3.0.0,2.2. pull the histogram
Release-3.0.0,2.3. find best split result of this tree node
Release-3.0.0,2.3.1 using server split
Release-3.0.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-3.0.0,update the grad stats of children node
Release-3.0.0,update the left child
Release-3.0.0,update the right child
Release-3.0.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
Release-3.0.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
Release-3.0.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
Release-3.0.0,2.3.5 reset this tree node's gradient histogram to 0
Release-3.0.0,3. push split feature to PS
Release-3.0.0,4. push split value to PS
Release-3.0.0,5. push split gain to PS
Release-3.0.0,6. set phase to AFTER_SPLIT
Release-3.0.0,this.phase = GBDTPhase.AFTER_SPLIT;
Release-3.0.0,clock
Release-3.0.0,1. get split feature
Release-3.0.0,2. get split value
Release-3.0.0,3. get split gain
Release-3.0.0,4. get node weight
Release-3.0.0,5. split node
Release-3.0.0,update local replica
Release-3.0.0,create AfterSplit task
Release-3.0.0,"2. check thread stats, if all threads finish, return"
Release-3.0.0,6. clock
Release-3.0.0,"split the span of one node, reset the instance position"
Release-3.0.0,in case this worker has no instance on this node
Release-3.0.0,set the span of left child
Release-3.0.0,set the span of right child
Release-3.0.0,"1. left to right, find the first instance that should be in the right child"
Release-3.0.0,"2. right to left, find the first instance that should be in the left child"
Release-3.0.0,3. swap two instances
Release-3.0.0,4. find the cut pos
Release-3.0.0,5. set the span of left child
Release-3.0.0,6. set the span of right child
Release-3.0.0,set tree node to active
Release-3.0.0,set node to leaf
Release-3.0.0,set node to inactive
Release-3.0.0,finish current depth
Release-3.0.0,finish current tree
Release-3.0.0,set the tree phase
Release-3.0.0,check if there is active node
Release-3.0.0,check if finish all the tree
Release-3.0.0,update node's grad stats on PS
Release-3.0.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
Release-3.0.0,the root node's stats is updated by leader worker
Release-3.0.0,1. create the update
Release-3.0.0,2. push the update to PS
Release-3.0.0,1. update predictions of training data
Release-3.0.0,2. update predictions of validation data
Release-3.0.0,the leader task adds node prediction to flush list
Release-3.0.0,1. name of this node's grad histogram on PS
Release-3.0.0,2. build the grad histogram of this node
Release-3.0.0,3. push the histograms to PS
Release-3.0.0,4. reset thread stats to finished
Release-3.0.0,5.1. set the children nodes of this node
Release-3.0.0,5.2. set split info and grad stats to this node
Release-3.0.0,5.2. create children nodes
Release-3.0.0,"5.3. create node stats for children nodes, and add them to the tree"
Release-3.0.0,5.4. reset instance position
Release-3.0.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
Release-3.0.0,5.6. set children nodes to leaf nodes
Release-3.0.0,5.7. set nid to leaf node
Release-3.0.0,5.8. deactivate active node
Release-3.0.0,"get feature type, 0:empty 1:all equal 2:real"
Release-3.0.0,"if not -1, sufficient space will be allocated at once"
Release-3.0.0,copy the highest levels
Release-3.0.0,copy baseBuffer
Release-3.0.0,merge two non-empty quantile sketches
Release-3.0.0,left child <= split value; right child > split value
Release-3.0.0,"the first: minimal, the last: maximal"
Release-3.0.0,categorical features
Release-3.0.0,continuous features
Release-3.0.0,left child <= split value; right child > split value
Release-3.0.0,feature index used to split
Release-3.0.0,feature value used to split
Release-3.0.0,loss change after split this node
Release-3.0.0,grad stats of the left child
Release-3.0.0,grad stats of the right child
Release-3.0.0,"LOG.info(""Constructor with fid = -1"");"
Release-3.0.0,fid = -1: no split currently
Release-3.0.0,the minimal split value is the minimal value of feature
Release-3.0.0,the splits do not include the maximal value of feature
Release-3.0.0,"1. the average distance, (maxValue - minValue) / splitNum"
Release-3.0.0,2. calculate the candidate split value
Release-3.0.0,1. new feature's histogram (grad + hess)
Release-3.0.0,size: sampled_featureNum * (2 * splitNum)
Release-3.0.0,"in other words, concatenate each feature's histogram"
Release-3.0.0,2. get the span of this node
Release-3.0.0,------ 3. using sparse-aware method to build histogram ---
Release-3.0.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
Release-3.0.0,3.1. get the instance index
Release-3.0.0,3.2. get the grad and hess of the instance
Release-3.0.0,3.3. add to the sum
Release-3.0.0,3.4. loop the non-zero entries
Release-3.0.0,3.4.1. get feature value
Release-3.0.0,3.4.2. current feature's position in the sampled feature set
Release-3.0.0,"int fPos = findFidPlace(this.controller.fSet, fid);"
Release-3.0.0,3.4.3. find the position of feature value in a histogram
Release-3.0.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
Release-3.0.0,3.4.4. add the grad and hess to the corresponding bin
Release-3.0.0,3.4.5. add the reverse to the bin that contains 0.0f
Release-3.0.0,4. add the grad and hess sum to the zero bin of all features
Release-3.0.0,find the best split result of the histogram of a tree node
Release-3.0.0,1. calculate the gradStats of the root node
Release-3.0.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
Release-3.0.0,2. loop over features
Release-3.0.0,2.1. get the ture feature id in the sampled feature set
Release-3.0.0,2.2. get the indexes of histogram of this feature
Release-3.0.0,2.3. find the best split of current feature
Release-3.0.0,2.4. update the best split result if possible
Release-3.0.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-3.0.0,3. update the grad stats of children node
Release-3.0.0,3.1. update the left child
Release-3.0.0,3.2. update the right child
Release-3.0.0,find the best split result of one feature
Release-3.0.0,1. set the feature id
Release-3.0.0,2. create the best left stats and right stats
Release-3.0.0,3. the gain of the root node
Release-3.0.0,4. create the temp left and right grad stats
Release-3.0.0,5. loop over all the data in histogram
Release-3.0.0,5.1. get the grad and hess of current hist bin
Release-3.0.0,5.2. check whether we can split with current left hessian
Release-3.0.0,right = root - left
Release-3.0.0,5.3. check whether we can split with current right hessian
Release-3.0.0,5.4. calculate the current loss gain
Release-3.0.0,5.5. check whether we should update the split result with current loss gain
Release-3.0.0,split value = sketches[splitIdx]
Release-3.0.0,"5.6. if should update, also update the best left and right grad stats"
Release-3.0.0,6. set the best left and right grad stats
Release-3.0.0,partition number
Release-3.0.0,cols of each partition
Release-3.0.0,1. calculate the total grad sum and hess sum
Release-3.0.0,2. create the grad stats of the node
Release-3.0.0,1. calculate the total grad sum and hess sum
Release-3.0.0,2. create the grad stats of the node
Release-3.0.0,1. calculate the total grad sum and hess sum
Release-3.0.0,2. create the grad stats of the node
Release-3.0.0,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
Release-3.0.0,assert fvalue >= sketch[start] && fvalue <= sketch[end];
Release-3.0.0,if (left > end) return end - start;
Release-3.0.0,find the best split result of the histogram of a tree node
Release-3.0.0,2.2. get the indexes of histogram of this feature
Release-3.0.0,2.3. find the best split of current feature
Release-3.0.0,2.4. update the best split result if possible
Release-3.0.0,find the best split result of one feature
Release-3.0.0,1. set the feature id
Release-3.0.0,splitEntry.setFid(fid);
Release-3.0.0,2. create the best left stats and right stats
Release-3.0.0,3. the gain of the root node
Release-3.0.0,4. create the temp left and right grad stats
Release-3.0.0,5. loop over all the data in histogram
Release-3.0.0,5.1. get the grad and hess of current hist bin
Release-3.0.0,5.2. check whether we can split with current left hessian
Release-3.0.0,right = root - left
Release-3.0.0,5.3. check whether we can split with current right hessian
Release-3.0.0,5.4. calculate the current loss gain
Release-3.0.0,5.5. check whether we should update the split result with current loss gain
Release-3.0.0,"5.6. if should update, also update the best left and right grad stats"
Release-3.0.0,6. set the best left and right grad stats
Release-3.0.0,find the best split result of a serve row on the PS
Release-3.0.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
Release-3.0.0,2.2. get the start index in histogram of this feature
Release-3.0.0,2.3. find the best split of current feature
Release-3.0.0,2.4. update the best split result if possible
Release-3.0.0,"find the best split result of one feature from a server row, used by the PS"
Release-3.0.0,1. set the feature id
Release-3.0.0,2. create the best left stats and right stats
Release-3.0.0,3. the gain of the root node
Release-3.0.0,4. create the temp left and right grad stats
Release-3.0.0,5. loop over all the data in histogram
Release-3.0.0,5.1. get the grad and hess of current hist bin
Release-3.0.0,5.2. check whether we can split with current left hessian
Release-3.0.0,right = root - left
Release-3.0.0,5.3. check whether we can split with current right hessian
Release-3.0.0,5.4. calculate the current loss gain
Release-3.0.0,5.5. check whether we should update the split result with current loss gain
Release-3.0.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
Release-3.0.0,the task use index to find fvalue
Release-3.0.0,"5.6. if should update, also update the best left and right grad stats"
Release-3.0.0,6. set the best left and right grad stats
Release-3.0.0,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
Release-3.0.0,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
Release-3.0.0,max and min of each feature
Release-3.0.0,clear all the information
Release-3.0.0,calculate the sum of gradient and hess
Release-3.0.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
Release-3.0.0,ridx)
Release-3.0.0,check if necessary information is ready
Release-3.0.0,"same as add, reduce is used in All Reduce"
Release-3.0.0,"features used in this tree, if equals null, means use all the features without sampling"
Release-3.0.0,node in the tree
Release-3.0.0,the gradient info of each instances
Release-3.0.0,initialize nodes
Release-3.0.0,gradient
Release-3.0.0,second order gradient
Release-3.0.0,int sendStartCol = (int) row.getStartCol();
Release-3.0.0,logistic loss for binary classification task.
Release-3.0.0,"logistic loss, but predict un-transformed margin"
Release-3.0.0,check if label in range
Release-3.0.0,return the default evaluation metric for the objective
Release-3.0.0,"task type: classification, regression, or ranking"
Release-3.0.0,"quantile sketch, size = featureNum * splitNum"
Release-3.0.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
Release-3.0.0,"active tree nodes, size = pow(2, treeDepth) -1"
Release-3.0.0,sampled features. size = treeNum * sampleRatio * featureNum
Release-3.0.0,categorical feature. size = workerNum * cateFeatNum * splitNum
Release-3.0.0,"split features, size = treeNum * treeNodeNum"
Release-3.0.0,"split values, size = treeNum * treeNodeNum"
Release-3.0.0,"split gains, size = treeNum * treeNodeNum"
Release-3.0.0,"node weights, size = treeNum * treeNodeNum"
Release-3.0.0,"node preds, size = treeNum * treeNodeNum"
Release-3.0.0,if using PS to perform split
Release-3.0.0,step size for a tree
Release-3.0.0,number of class
Release-3.0.0,minimum loss change required for a split
Release-3.0.0,maximum depth of a tree
Release-3.0.0,number of features
Release-3.0.0,number of nonzero
Release-3.0.0,number of candidates split value
Release-3.0.0,----- the rest parameters are less important ----
Release-3.0.0,base instance weight
Release-3.0.0,minimum amount of hessian(weight) allowed in a child
Release-3.0.0,L2 regularization factor
Release-3.0.0,L1 regularization factor
Release-3.0.0,default direction choice
Release-3.0.0,maximum delta update we can add in weight estimation
Release-3.0.0,this parameter can be used to stabilize update
Release-3.0.0,default=0 means no constraint on weight delta
Release-3.0.0,whether we want to do subsample for row
Release-3.0.0,whether to subsample columns for each tree
Release-3.0.0,accuracy of sketch
Release-3.0.0,accuracy of sketch
Release-3.0.0,leaf vector size
Release-3.0.0,option for parallelization
Release-3.0.0,option to open cacheline optimization
Release-3.0.0,whether to not print info during training.
Release-3.0.0,maximum depth of the tree
Release-3.0.0,number of features used for tree construction
Release-3.0.0,"minimum loss change required for a split, otherwise stop split"
Release-3.0.0,----- the rest parameters are less important ----
Release-3.0.0,default direction choice
Release-3.0.0,whether we want to do sample data
Release-3.0.0,whether to sample columns during tree construction
Release-3.0.0,whether to use histogram for split
Release-3.0.0,number of histogram units
Release-3.0.0,whether to print info during training.
Release-3.0.0,----- the rest parameters are obtained after training ----
Release-3.0.0,total number of nodes
Release-3.0.0,number of deleted nodes */
Release-2.3.0,@maxIndex: this variable contains the max index of node/word
Release-2.3.0,values[b + offset] = (random.nextFloat() - 0.5f) / dimension;
Release-2.3.0,some params
Release-2.3.0,max index for node/word
Release-2.3.0,compute number of nodes for one row
Release-2.3.0,check the length of dot values
Release-2.3.0,merge dot values from all partitions
Release-2.3.0,Skip-Gram model
Release-2.3.0,Negative sampling
Release-2.3.0,used to accumulate the updates for input vectors
Release-2.3.0,Negative sampling
Release-2.3.0,accumulate for the hidden layer
Release-2.3.0,update output layer
Release-2.3.0,update the hidden layer
Release-2.3.0,update input
Release-2.3.0,Skip-Gram model
Release-2.3.0,Negative sampling
Release-2.3.0,used to accumulate the updates for input vectors
Release-2.3.0,Negative sampling
Release-2.3.0,accumulate for the hidden layer
Release-2.3.0,update output layer
Release-2.3.0,update the hidden layer
Release-2.3.0,update input
Release-2.3.0,update output
Release-2.3.0,Some params
Release-2.3.0,compute number of nodes for one row
Release-2.3.0,window size
Release-2.3.0,Skip-Gram model
Release-2.3.0,Accumulate the input vectors from context
Release-2.3.0,Negative sampling
Release-2.3.0,used to accumulate the updates for input vectors
Release-2.3.0,window size
Release-2.3.0,skip-gram model
Release-2.3.0,Negative sampling
Release-2.3.0,accumulate for the hidden layer
Release-2.3.0,update output layer
Release-2.3.0,update the hidden layer
Release-2.3.0,update input
Release-2.3.0,update output
Release-2.3.0,some params
Release-2.3.0,batch sentences
Release-2.3.0,max index for node/word
Release-2.3.0,compute number of nodes for one row
Release-2.3.0,check the length of dot values
Release-2.3.0,merge dot values from all partitions
Release-2.3.0,locates the input vectors to local array to prevent randomly access
Release-2.3.0,on the large server row.
Release-2.3.0,fill 0 for context vector
Release-2.3.0,window size
Release-2.3.0,Continuous bag-of-words Models
Release-2.3.0,Accumulate the input vectors from context
Release-2.3.0,Calculate the partial dot values
Release-2.3.0,We should guarantee here that the sample would not equal the ``word``
Release-2.3.0,used to accumulate the context input vectors
Release-2.3.0,locates the input vector into local arrays to prevent randomly access for
Release-2.3.0,the large server row.
Release-2.3.0,window size
Release-2.3.0,while true to prevent sampling out a positive target
Release-2.3.0,how to prevent the randomly access to the output vectors??
Release-2.3.0,accumulate gradients for the input vectors
Release-2.3.0,update output vectors
Release-2.3.0,update input
Release-2.3.0,update output
Release-2.3.0,Some params
Release-2.3.0,compute number of nodes for one row
Release-2.3.0,// calculate bias
Release-2.3.0,if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {
Release-2.3.0,"double zVal = VectorUtils.getDouble(z, 0);"
Release-2.3.0,"double nVal = VectorUtils.getDouble(n, 0);"
Release-2.3.0,"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));"
Release-2.3.0,}
Release-2.3.0,Do nothing.
Release-2.3.0,split updates
Release-2.3.0,shuffle update splits
Release-2.3.0,generate part update splits
Release-2.3.0,"set split context: partition key, use int key for long key vector or net"
Release-2.3.0,how to do intersection for two dense vector with a given indices ??
Release-2.3.0,current word
Release-2.3.0,neu1 stores the average value of input vectors in the context (CBOW)
Release-2.3.0,Continuous Bag-of-Words Model
Release-2.3.0,Accumulate the input vectors from context
Release-2.3.0,negative sampling
Release-2.3.0,Using the sigmoid value from the pre-computed table
Release-2.3.0,accumulate for the hidden layer
Release-2.3.0,update output layer
Release-2.3.0,add the counter for target
Release-2.3.0,update hidden layer
Release-2.3.0,Update the input vector for each word in the context
Release-2.3.0,add the counter to input
Release-2.3.0,update input layers
Release-2.3.0,update output layers
Release-2.3.0,for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];
Release-2.3.0,copy the highest levels
Release-2.3.0,copy baseBuffer
Release-2.3.0,merge two non-empty quantile sketches
Release-2.3.0,"if not -1, sufficient space will be allocated at once"
Release-2.3.0,InstanceRow ins = instanceRows[insId];
Release-2.3.0,int[] indices = ins.indices();
Release-2.3.0,int[] bins = ins.bins();
Release-2.3.0,int nnz = indices.length;
Release-2.3.0,for (int j = 0; j < nnz; j++) {
Release-2.3.0,int fid = indices[j];
Release-2.3.0,if (isFeatUsed[fid - featLo]) {
Release-2.3.0,"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);"
Release-2.3.0,}
Release-2.3.0,}
Release-2.3.0,1. allocate histogram
Release-2.3.0,"2. loop non-zero instances, accumulate to histogram"
Release-2.3.0,if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature
Release-2.3.0,3. add remaining grad and hess to default bin
Release-2.3.0,"return param.calcWeights(grad, hess);"
Release-2.3.0,"numClass is usually small, so we do not use arraycopy here"
Release-2.3.0,"numClass is usually small, so we do not use arraycopy here"
Release-2.3.0,TODO: use more schema on default bin
Release-2.3.0,1. set default bin to left child
Release-2.3.0,"2. for other bins, find its location"
Release-2.3.0,3. create split set
Release-2.3.0,this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];
Release-2.3.0,predict sparse instance with indices and values
Release-2.3.0,predict libsvm data
Release-2.3.0,"Preconditions.checkArgument(preds.length == labels.length,"
Release-2.3.0,"""LogLossMetric should be used for binary-label classification"");"
Release-2.3.0,double loss = 0.0;
Release-2.3.0,for (int i = 0; i < preds.length; i++) {
Release-2.3.0,"loss += evalOne(preds[i], labels[i]);"
Release-2.3.0,}
Release-2.3.0,return loss / labels.length;
Release-2.3.0,double error = 0.0;
Release-2.3.0,if (preds.length == labels.length) {
Release-2.3.0,for (int i = 0; i < preds.length; i++) {
Release-2.3.0,"error += evalOne(preds[i], labels[i]);"
Release-2.3.0,}
Release-2.3.0,} else {
Release-2.3.0,int numLabel = preds.length / labels.length;
Release-2.3.0,float[] pred = new float[numLabel];
Release-2.3.0,for (int i = 0; i < labels.length; i++) {
Release-2.3.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.3.0,"error += evalOne(pred, labels[i]);"
Release-2.3.0,}
Release-2.3.0,}
Release-2.3.0,return error / labels.length;
Release-2.3.0,Preconditions.checkArgument(preds.length != labels.length
Release-2.3.0,"&& preds.length % labels.length == 0,"
Release-2.3.0,"""CrossEntropyMetric should be used for multi-label classification"");"
Release-2.3.0,double loss = 0.0;
Release-2.3.0,int numLabel = preds.length / labels.length;
Release-2.3.0,float[] pred = new float[numLabel];
Release-2.3.0,for (int i = 0; i < labels.length; i++) {
Release-2.3.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.3.0,"loss += evalOne(pred, labels[i]);"
Release-2.3.0,}
Release-2.3.0,return loss / labels.length;
Release-2.3.0,double correct = 0.0;
Release-2.3.0,if (preds.length == labels.length) {
Release-2.3.0,for (int i = 0; i < preds.length; i++) {
Release-2.3.0,"correct += evalOne(preds[i], labels[i]);"
Release-2.3.0,}
Release-2.3.0,} else {
Release-2.3.0,int numLabel = preds.length / labels.length;
Release-2.3.0,float[] pred = new float[numLabel];
Release-2.3.0,for (int i = 0; i < labels.length; i++) {
Release-2.3.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.3.0,"correct += evalOne(pred, labels[i]);"
Release-2.3.0,}
Release-2.3.0,}
Release-2.3.0,return (float) (correct / labels.length);
Release-2.3.0,double errSum = 0.0f;
Release-2.3.0,if (preds.length == labels.length) {
Release-2.3.0,for (int i = 0; i < preds.length; i++) {
Release-2.3.0,"errSum += evalOne(preds[i], labels[i]);"
Release-2.3.0,}
Release-2.3.0,} else {
Release-2.3.0,int numLabel = preds.length / labels.length;
Release-2.3.0,float[] pred = new float[numLabel];
Release-2.3.0,for (int i = 0; i < labels.length; i++) {
Release-2.3.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.3.0,"errSum += evalOne(pred, labels[i]);"
Release-2.3.0,}
Release-2.3.0,}
Release-2.3.0,return Math.sqrt(errSum / labels.length);
Release-2.3.0,"System.out.println(""----------"");"
Release-2.3.0,"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)"
Release-2.3.0,"+ "", mask = "" + Integer.toBinaryString(readMaskT));"
Release-2.3.0,readMaskT <<= 1;
Release-2.3.0,"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};"
Release-2.3.0,int n = bits.length;
Release-2.3.0,BufferedBitSet writeBitSet = new BufferedBitSet(n);
Release-2.3.0,"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);"
Release-2.3.0,if (bitSet.get(i) != bits[i]) {
Release-2.3.0,"throw new RuntimeException("""" + i);"
Release-2.3.0,}
Release-2.3.0,private final ByteBuffer bytes;
Release-2.3.0,"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {"
Release-2.3.0,int capacity = bytes.capacity() * 8;
Release-2.3.0,readIndexT = bytes.capacity() - 1;
Release-2.3.0,return bytes.get(index);
Release-2.3.0,TODO: use arraycopy to make it faster
Release-2.3.0,assert from >= this.from && to <= this.to;
Release-2.3.0,"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));"
Release-2.3.0,"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));"
Release-2.3.0,return bits.clone();
Release-2.3.0,private final SerializableBuffer bytes;
Release-2.3.0,private final ByteBuffer bytes;
Release-2.3.0,this.bytes = ByteBuffer.allocate(numBytes);
Release-2.3.0,public BufferedBitSetWriter(ByteBuffer bytes) {
Release-2.3.0,this.bytes = bytes;
Release-2.3.0,}
Release-2.3.0,"bytes.put(writeIndex++, (byte) writeBuffer);"
Release-2.3.0,public ByteBuffer getBytes() {
Release-2.3.0,return bytes;
Release-2.3.0,}
Release-2.3.0,ML TreeConf
Release-2.3.0,GBDT TreeConf
Release-2.3.0,"edges=[x,...] firstFlow=1 => go to right if < x and go to left if > x"
Release-2.3.0,"edges=[x,...] firstFlow=0 => go to left if < x and go to right if > x"
Release-2.3.0,"different types of tree node splits, enumerated by their complexity"
Release-2.3.0,"in order to reduce model size, we give priority to split point"
Release-2.3.0,"comparison between two split points, we give priority to lower feature index"
Release-2.3.0,TODO: comparison between two split sets
Release-2.3.0,"public boolean leafwise;  // true if leaf-wise training, false if level-wise training"
Release-2.3.0,TODO: regularization
Release-2.3.0,TODO: regularization
Release-2.3.0,public float insSampleRatio;  // subsample ratio for instances
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy data spliter
Release-2.3.0,get a angel client
Release-2.3.0,add matrix
Release-2.3.0,TODO Auto-generated constructor stub
Release-2.3.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighbor;
Release-2.3.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborParam;
Release-2.3.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.getneighbor.GetNeighborResult;
Release-2.3.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighbor;
Release-2.3.0,import com.tencent.angel.ml.matrix.psf.graph.adjacency.initneighbor.InitNeighborParam;
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add matrix
Release-2.3.0,@Test
Release-2.3.0,"public void testInitAndGet() throws ExecutionException, InterruptedException {"
Release-2.3.0,Worker worker = LocalClusterContext.get().getWorker(worker0Attempt0Id).getWorker();
Release-2.3.0,"MatrixClient client1 = worker.getPSAgent().getMatrixClient(""w2"", 0);"
Release-2.3.0,int matrixW1Id = client1.getMatrixId();
Release-2.3.0,// Generate graph data
Release-2.3.0,"Map<Integer, int []> adjMap = generateAdjTable(nodeNum, maxNeighborNum);"
Release-2.3.0,
Release-2.3.0,// Init graph adj table
Release-2.3.0,"InitNeighbor func = new InitNeighbor(new InitNeighborParam(matrixW1Id, adjMap));"
Release-2.3.0,client1.update(func);
Release-2.3.0,
Release-2.3.0,int [] nodeIds = new int[adjMap.size()];
Release-2.3.0,int i = 0;
Release-2.3.0,for(int nodeId : adjMap.keySet()) {
Release-2.3.0,nodeIds[i++] = nodeId;
Release-2.3.0,}
Release-2.3.0,
Release-2.3.0,// Get graph adj table from PS
Release-2.3.0,"GetNeighbor getFunc = new GetNeighbor(new GetNeighborParam(matrixW1Id, nodeIds, maxNeighborNum));"
Release-2.3.0,"Map<Integer, int[]> getResults = ((GetNeighborResult) (client1.get(getFunc)))"
Release-2.3.0,.getNodeIdToNeighborIndices();
Release-2.3.0,
Release-2.3.0,// Check the result
Release-2.3.0,"for(Entry<Integer, int[]> entry : getResults.entrySet()) {"
Release-2.3.0,"Assert.assertArrayEquals(entry.getValue(), adjMap.get(entry.getKey()));"
Release-2.3.0,}
Release-2.3.0,}
Release-2.3.0,row 0 is a random uniform
Release-2.3.0,row 1 is a random normal
Release-2.3.0,row 2 is filled with 1.0
Release-2.3.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
Release-2.3.0,"paras[1] = ""abc"";"
Release-2.3.0,"paras[2] = ""123"";"
Release-2.3.0,Add standard Hadoop classes
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,set sgd LR algorithm parameters #feature #epoch
Release-2.3.0,Set input data path
Release-2.3.0,Set save model path
Release-2.3.0,Set actionType train
Release-2.3.0,QSLRRunner runner = new QSLRRunner();
Release-2.3.0,runner.train(conf);
Release-2.3.0,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
Release-2.3.0,Dataset
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,class number
Release-2.3.0,Model type
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,Train batch number per epoch.
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set file system
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Use local deploy mode and data format
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set Softmax algorithm parameters
Release-2.3.0,Set model class
Release-2.3.0,Dataset
Release-2.3.0,Data format
Release-2.3.0,Feature number of train data
Release-2.3.0,Tree number
Release-2.3.0,Tree depth
Release-2.3.0,Split number
Release-2.3.0,Feature sample ratio
Release-2.3.0,Ratio of validation
Release-2.3.0,Learning rate
Release-2.3.0,Set file system
Release-2.3.0,Use local deploy mode and data format
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource, #worker, #task, #PS"
Release-2.3.0,Set GBDT algorithm parameters
Release-2.3.0,Dataset
Release-2.3.0,Set file system
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Use local deploy mode
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set DeepFM algorithm parameters
Release-2.3.0,Set model class
Release-2.3.0,Dataset
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Model type
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set file system
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Use local deploy mode and data format
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set LR algorithm parameters
Release-2.3.0,Set model class
Release-2.3.0,Dataset
Release-2.3.0,Data format
Release-2.3.0,Model type
Release-2.3.0,Cluster center number
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Sample ratio per mini-batch
Release-2.3.0,C
Release-2.3.0,Set file system
Release-2.3.0,Use local deploy mode and data format
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource, #worker, #task, #PS"
Release-2.3.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.3.0,Dataset
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Model type
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set file system
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Use local deploy mode and data format
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set FM algorithm parameters
Release-2.3.0,Set model class
Release-2.3.0,Dataset
Release-2.3.0,Set file system
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Use local deploy mode
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set WideAndDeep algorithm parameters
Release-2.3.0,Set model class
Release-2.3.0,Dataset
Release-2.3.0,Data format
Release-2.3.0,"Set LDA parameters #V, #K"
Release-2.3.0,Set file system
Release-2.3.0,Use local deploy mode and data format
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource, #worker, #task, #PS"
Release-2.3.0,Set LDA algorithm parameters
Release-2.3.0,Dataset
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Model type
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set file system
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Use local deploy mode and data format
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set SVM algorithm parameters
Release-2.3.0,Set model class
Release-2.3.0,Dataset
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Model type
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,Model is classification
Release-2.3.0,Train batch number per epoch.
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set file system
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Use local deploy mode and data format
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set LR algorithm parameters
Release-2.3.0,Set model class
Release-2.3.0,Dataset
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Model type
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,Model is classification
Release-2.3.0,Train batch number per epoch.
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set file system
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Use local deploy mode and data format
Release-2.3.0,Set data path
Release-2.3.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set LR algorithm parameters
Release-2.3.0,Set model class
Release-2.3.0,Load model meta
Release-2.3.0,Convert model
Release-2.3.0,"Get input path, output path"
Release-2.3.0,Init serde
Release-2.3.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.3.0,Load model meta
Release-2.3.0,Convert model
Release-2.3.0,load hadoop configuration
Release-2.3.0,"Get input path, output path"
Release-2.3.0,Init serde
Release-2.3.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.3.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.3.0,input.seek(rowOffset.getOffset());
Release-2.3.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.3.0,input.seek(rowOffset.getOffset());
Release-2.3.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.3.0,input.seek(rowOffset.getOffset());
Release-2.3.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.3.0,input.seek(rowOffset.getOffset());
Release-2.3.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.3.0,input.seek(rowOffset.getOffset());
Release-2.3.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.3.0,input.seek(rowOffset.getOffset());
Release-2.3.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.3.0,input.seek(rowOffset.getOffset());
Release-2.3.0,Load model meta
Release-2.3.0,Check row type
Release-2.3.0,Load model
Release-2.3.0,Load model meta
Release-2.3.0,Check row type
Release-2.3.0,Load model
Release-2.3.0,Load model meta
Release-2.3.0,Check row type
Release-2.3.0,Load model
Release-2.3.0,Load model meta
Release-2.3.0,Check row type
Release-2.3.0,Load model
Release-2.3.0,Load model meta
Release-2.3.0,Check row type
Release-2.3.0,Load model
Release-2.3.0,Load model meta
Release-2.3.0,Check row type
Release-2.3.0,Load model
Release-2.3.0,Load model meta
Release-2.3.0,Check row type
Release-2.3.0,Load model
Release-2.3.0,Load model
Release-2.3.0,load hadoop configuration
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add matrix
Release-2.3.0,worker register
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add matrix
Release-2.3.0,add matrix
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add matrix
Release-2.3.0,attempt 0
Release-2.3.0,attempt1
Release-2.3.0,attempt1
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add matrix
Release-2.3.0,TODO Auto-generated constructor stub
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add matrix
Release-2.3.0,set basic configuration keys
Release-2.3.0,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add matrix
Release-2.3.0,Thread.sleep(5000);
Release-2.3.0,"response = master.getJobReport(null, request);"
Release-2.3.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
Release-2.3.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
Release-2.3.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add dense double matrix
Release-2.3.0,add comp dense double matrix
Release-2.3.0,add sparse double matrix
Release-2.3.0,add component sparse double matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense long matrix
Release-2.3.0,add comp dense long matrix
Release-2.3.0,add sparse long matrix
Release-2.3.0,add component sparse long matrix
Release-2.3.0,add comp dense long double matrix
Release-2.3.0,add sparse long-key double matrix
Release-2.3.0,add component long-key sparse double matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add sparse long-key float matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add sparse long-key int matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,add sparse long-key long matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add dense double matrix
Release-2.3.0,add comp dense double matrix
Release-2.3.0,add sparse double matrix
Release-2.3.0,add component sparse double matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense long matrix
Release-2.3.0,add comp dense long matrix
Release-2.3.0,add sparse long matrix
Release-2.3.0,add component sparse long matrix
Release-2.3.0,add comp dense long double matrix
Release-2.3.0,add sparse long-key double matrix
Release-2.3.0,add component long-key sparse double matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add sparse long-key float matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add sparse long-key int matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,add sparse long-key long matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,testDenseDoubleUDF();
Release-2.3.0,testSparseDoubleUDF();
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add dense double matrix
Release-2.3.0,add comp dense double matrix
Release-2.3.0,add sparse double matrix
Release-2.3.0,add component sparse double matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense long matrix
Release-2.3.0,add comp dense long matrix
Release-2.3.0,add sparse long matrix
Release-2.3.0,add component sparse long matrix
Release-2.3.0,add comp dense long double matrix
Release-2.3.0,add sparse long-key double matrix
Release-2.3.0,add component long-key sparse double matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add sparse long-key float matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add sparse long-key int matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,add sparse long-key long matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,client1.clock().get();
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,client1.clock().get();
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add dense double matrix
Release-2.3.0,add comp dense double matrix
Release-2.3.0,add sparse double matrix
Release-2.3.0,add component sparse double matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense long matrix
Release-2.3.0,add comp dense long matrix
Release-2.3.0,add sparse long matrix
Release-2.3.0,add component sparse long matrix
Release-2.3.0,add comp dense long double matrix
Release-2.3.0,add sparse long-key double matrix
Release-2.3.0,add component long-key sparse double matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add sparse long-key float matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add sparse long-key int matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,add sparse long-key long matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,testDenseDoubleUDF();
Release-2.3.0,testSparseDoubleUDF();
Release-2.3.0,client1.clock().get();
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,client1.clock().get();
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,TODO Auto-generated constructor stub
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add dense double matrix
Release-2.3.0,add sparse double matrix
Release-2.3.0,add comp dense double matrix
Release-2.3.0,add component sparse double matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense long matrix
Release-2.3.0,add comp dense long matrix
Release-2.3.0,add sparse long matrix
Release-2.3.0,add component sparse long matrix
Release-2.3.0,add comp dense long double matrix
Release-2.3.0,add sparse long-key double matrix
Release-2.3.0,add component long-key sparse double matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add sparse long-key float matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add sparse long-key int matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,add sparse long-key long matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,testDenseDoubleUDF();
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,Assert.assertTrue(index.length == row.size());
Release-2.3.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.3.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.3.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"LOG.info(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.3.0,IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,for (int i = 0; i < feaNum; i++) {
Release-2.3.0,"deltaVec.set(i, i);"
Release-2.3.0,}
Release-2.3.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.3.0,IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add sparse float matrix
Release-2.3.0,siMat.setPartitionClass(CSRPartition.class);
Release-2.3.0,siMat.setPartitionStorageClass(IntCSRStorage.class);
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add dense double matrix
Release-2.3.0,add comp dense double matrix
Release-2.3.0,add sparse double matrix
Release-2.3.0,add component sparse double matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense long matrix
Release-2.3.0,add comp dense long matrix
Release-2.3.0,add sparse long matrix
Release-2.3.0,add component sparse long matrix
Release-2.3.0,add comp dense long double matrix
Release-2.3.0,add sparse long-key double matrix
Release-2.3.0,add component long-key sparse double matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add sparse long-key float matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add sparse long-key int matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,add sparse long-key long matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,get a angel client
Release-2.3.0,add dense double matrix
Release-2.3.0,add comp dense double matrix
Release-2.3.0,add sparse double matrix
Release-2.3.0,add component sparse double matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense float matrix
Release-2.3.0,add comp dense float matrix
Release-2.3.0,add sparse float matrix
Release-2.3.0,add component sparse float matrix
Release-2.3.0,add dense long matrix
Release-2.3.0,add comp dense long matrix
Release-2.3.0,add sparse long matrix
Release-2.3.0,add component sparse long matrix
Release-2.3.0,add comp dense long double matrix
Release-2.3.0,add sparse long-key double matrix
Release-2.3.0,add component long-key sparse double matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add sparse long-key float matrix
Release-2.3.0,add component long-key sparse float matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add sparse long-key int matrix
Release-2.3.0,add component long-key sparse int matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,add sparse long-key long matrix
Release-2.3.0,add component long-key sparse long matrix
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-2.3.0,get a angel client
Release-2.3.0,add sparse float matrix
Release-2.3.0,MatrixContext siMat = new MatrixContext();
Release-2.3.0,siMat.setName(SPARSE_INT_MAT);
Release-2.3.0,siMat.setRowType(RowType.T_ANY_INTKEY_SPARSE);
Release-2.3.0,siMat.setRowNum(1);
Release-2.3.0,siMat.setValidIndexNum(100);
Release-2.3.0,siMat.setColNum(10000000000L);
Release-2.3.0,siMat.setValueType(Node.class);
Release-2.3.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-2.3.0,siMat.setPartitionClass(CSRPartition.class);
Release-2.3.0,angelClient.addMatrix(siMat);
Release-2.3.0,add sparse long-key double matrix
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,client1.clock().get();
Release-2.3.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.3.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
Release-2.3.0,@RunWith(MockitoJUnitRunner.class)
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
Release-2.3.0,get a angel client
Release-2.3.0,add matrix
Release-2.3.0,psAgent.initAndStart();
Release-2.3.0,test conf
Release-2.3.0,test master location
Release-2.3.0,test app id
Release-2.3.0,test user
Release-2.3.0,test ps agent attempt id
Release-2.3.0,test connection
Release-2.3.0,test master client
Release-2.3.0,test ip
Release-2.3.0,test loc
Release-2.3.0,test master location
Release-2.3.0,test ps location
Release-2.3.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
Release-2.3.0,test all ps ids
Release-2.3.0,test all matrix ids
Release-2.3.0,test all matrix names
Release-2.3.0,test matrix attribute
Release-2.3.0,test matrix meta
Release-2.3.0,test ps location
Release-2.3.0,test partitions
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,System.out.println(content);
Release-2.3.0,https://blog.csdn.net/cocoonyang/article/details/63068108
Release-2.3.0,v1[i] = v1[i] + da * v2[i];
Release-2.3.0,"dgemm(String transa, String transb,"
Release-2.3.0,"int m, int n, int k,"
Release-2.3.0,"double alpha,"
Release-2.3.0,"double[] a, int lda,"
Release-2.3.0,"double[] b, int ldb,"
Release-2.3.0,"double beta,"
Release-2.3.0,"double[] c, int ldc);"
Release-2.3.0,C := alpha*op( A )*op( B ) + beta*C
Release-2.3.0,v1[i] = v1[i] + da * v2[i];
Release-2.3.0,y := alpha*A*x + beta*y
Release-2.3.0,y := alpha*A*x + beta*y
Release-2.3.0,y := alpha*A*x + beta*y
Release-2.3.0,"dgemm(String transa, String transb,"
Release-2.3.0,"int m, int n, int k,"
Release-2.3.0,"double alpha,"
Release-2.3.0,"double[] a, int lda,"
Release-2.3.0,"double[] b, int ldb,"
Release-2.3.0,"double beta,"
Release-2.3.0,"double[] c, int ldc);"
Release-2.3.0,C := alpha*op( A )*op( B ) + beta*C
Release-2.3.0,Default does nothing.
Release-2.3.0,The app injection is optional
Release-2.3.0,"renderText(""hello world"");"
Release-2.3.0,"user choose a workerGroupID from the workergroups page,"
Release-2.3.0,now we should change the AngelApp params and render the workergroup page;
Release-2.3.0,"static final String WORKER_ID = ""worker.id"";"
Release-2.3.0,"div(""#logo"")."
Release-2.3.0,"img(""/static/hadoop-st.png"")._()."
Release-2.3.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
Release-2.3.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
Release-2.3.0,JQueryUI.jsnotice(html);
Release-2.3.0,import org.apache.hadoop.conf.Configuration;
Release-2.3.0,import java.lang.reflect.Field;
Release-2.3.0,all the files in input set
Release-2.3.0,Shuffle the file
Release-2.3.0,Get the blocks for all files
Release-2.3.0,Adjust the maxSize to make the split more balanced
Release-2.3.0,Handle the splittable files
Release-2.3.0,Handle the unsplittable files
Release-2.3.0,Split the blocks
Release-2.3.0,"If the remaining size of the current block is smaller than the required size,"
Release-2.3.0,the remaining blocks are divided into the current split
Release-2.3.0,Update current split length and move to next block
Release-2.3.0,Clear the current block offset
Release-2.3.0,"Current split length is > maxSize, split the block and generate a new split"
Release-2.3.0,Clear blocks list for next split
Release-2.3.0,Clear the current split length
Release-2.3.0,"If splitBlocks is not empty, just genetate a split for it"
Release-2.3.0,get block locations from file system
Release-2.3.0,create an input split
Release-2.3.0,get block locations from file system
Release-2.3.0,create a list of all block and their locations
Release-2.3.0,"if the file is not splitable, just create the one block with"
Release-2.3.0,full file length
Release-2.3.0,each split can be a maximum of maxSize
Release-2.3.0,if remainder is between max and 2*max - then
Release-2.3.0,"instead of creating splits of size max, left-max we"
Release-2.3.0,create splits of size left/2 and left/2. This is
Release-2.3.0,a heuristic to avoid creating really really small
Release-2.3.0,splits.
Release-2.3.0,add this block to the block --> node locations map
Release-2.3.0,"For blocks that do not have host/rack information,"
Release-2.3.0,assign to default  rack.
Release-2.3.0,add this block to the rack --> block map
Release-2.3.0,Add this host to rackToNodes map
Release-2.3.0,add this block to the node --> block map
Release-2.3.0,"if the file system does not have any rack information, then"
Release-2.3.0,use dummy rack location.
Release-2.3.0,The topology paths have the host name included as the last
Release-2.3.0,component. Strip it.
Release-2.3.0,get tokens for all the required FileSystems..
Release-2.3.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
Release-2.3.0,job.getConfiguration());
Release-2.3.0,Whether we need to recursive look into the directory structure
Release-2.3.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.3.0,user provided one (if any).
Release-2.3.0,all the files in input set
Release-2.3.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
Release-2.3.0,process all nodes and create splits that are local to a node. Generate
Release-2.3.0,"one split per node iteration, and walk over nodes multiple times to"
Release-2.3.0,distribute the splits across nodes.
Release-2.3.0,Skip the node if it has previously been marked as completed.
Release-2.3.0,"for each block, copy it into validBlocks. Delete it from"
Release-2.3.0,blockToNodes so that the same block does not appear in
Release-2.3.0,two different splits.
Release-2.3.0,Remove all blocks which may already have been assigned to other
Release-2.3.0,splits.
Release-2.3.0,"if the accumulated split size exceeds the maximum, then"
Release-2.3.0,create this split.
Release-2.3.0,create an input split and add it to the splits array
Release-2.3.0,Remove entries from blocksInNode so that we don't walk these
Release-2.3.0,again.
Release-2.3.0,Done creating a single split for this node. Move on to the next
Release-2.3.0,node so that splits are distributed across nodes.
Release-2.3.0,This implies that the last few blocks (or all in case maxSize=0)
Release-2.3.0,were not part of a split. The node is complete.
Release-2.3.0,if there were any blocks left over and their combined size is
Release-2.3.0,"larger than minSplitNode, then combine them into one split."
Release-2.3.0,Otherwise add them back to the unprocessed pool. It is likely
Release-2.3.0,that they will be combined with other blocks from the
Release-2.3.0,same rack later on.
Release-2.3.0,This condition also kicks in when max split size is not set. All
Release-2.3.0,blocks on a node will be grouped together into a single split.
Release-2.3.0,haven't created any split on this machine. so its ok to add a
Release-2.3.0,smaller one for parallelism. Otherwise group it in the rack for
Release-2.3.0,balanced size create an input split and add it to the splits
Release-2.3.0,array
Release-2.3.0,Remove entries from blocksInNode so that we don't walk this again.
Release-2.3.0,The node is done. This was the last set of blocks for this node.
Release-2.3.0,Put the unplaced blocks back into the pool for later rack-allocation.
Release-2.3.0,Node is done. All blocks were fit into node-local splits.
Release-2.3.0,Check if node-local assignments are complete.
Release-2.3.0,All nodes have been walked over and marked as completed or all blocks
Release-2.3.0,have been assigned. The rest should be handled via rackLock assignment.
Release-2.3.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
Release-2.3.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
Release-2.3.0,"if blocks in a rack are below the specified minimum size, then keep them"
Release-2.3.0,"in 'overflow'. After the processing of all racks is complete, these"
Release-2.3.0,overflow blocks will be combined into splits.
Release-2.3.0,Process all racks over and over again until there is no more work to do.
Release-2.3.0,Create one split for this rack before moving over to the next rack.
Release-2.3.0,Come back to this rack after creating a single split for each of the
Release-2.3.0,remaining racks.
Release-2.3.0,"Process one rack location at a time, Combine all possible blocks that"
Release-2.3.0,reside on this rack as one split. (constrained by minimum and maximum
Release-2.3.0,split size).
Release-2.3.0,iterate over all racks
Release-2.3.0,"for each block, copy it into validBlocks. Delete it from"
Release-2.3.0,blockToNodes so that the same block does not appear in
Release-2.3.0,two different splits.
Release-2.3.0,"if the accumulated split size exceeds the maximum, then"
Release-2.3.0,create this split.
Release-2.3.0,create an input split and add it to the splits array
Release-2.3.0,"if we created a split, then just go to the next rack"
Release-2.3.0,"if there is a minimum size specified, then create a single split"
Release-2.3.0,"otherwise, store these blocks into overflow data structure"
Release-2.3.0,There were a few blocks in this rack that
Release-2.3.0,remained to be processed. Keep them in 'overflow' block list.
Release-2.3.0,These will be combined later.
Release-2.3.0,Process all overflow blocks
Release-2.3.0,"This might cause an exiting rack location to be re-added,"
Release-2.3.0,but it should be ok.
Release-2.3.0,"if the accumulated split size exceeds the maximum, then"
Release-2.3.0,create this split.
Release-2.3.0,create an input split and add it to the splits array
Release-2.3.0,"Process any remaining blocks, if any."
Release-2.3.0,create an input split
Release-2.3.0,add this split to the list that is returned
Release-2.3.0,long num = totLength / maxSize;
Release-2.3.0,all blocks for all the files in input set
Release-2.3.0,mapping from a rack name to the list of blocks it has
Release-2.3.0,mapping from a block to the nodes on which it has replicas
Release-2.3.0,mapping from a node to the list of blocks that it contains
Release-2.3.0,populate all the blocks for all files
Release-2.3.0,stop all services
Release-2.3.0,1.write application state to file so that the client can get the state of the application
Release-2.3.0,if master exit
Release-2.3.0,2.clear tmp and staging directory
Release-2.3.0,waiting for client to get application state
Release-2.3.0,stop the RPC server
Release-2.3.0,"Security framework already loaded the tokens into current UGI, just use"
Release-2.3.0,them
Release-2.3.0,Now remove the AM->RM token so tasks don't have it
Release-2.3.0,add a shutdown hook
Release-2.3.0,init app state storage
Release-2.3.0,init event dispacher
Release-2.3.0,init location manager
Release-2.3.0,init container allocator
Release-2.3.0,init a rpc service
Release-2.3.0,recover matrix meta if needed
Release-2.3.0,recover ps attempt information if need
Release-2.3.0,Init Client manager
Release-2.3.0,Init PS Client manager
Release-2.3.0,init parameter server manager
Release-2.3.0,recover task information if needed
Release-2.3.0,a dummy data spliter is just for test now
Release-2.3.0,recover data splits information if needed
Release-2.3.0,init worker manager and register worker manager event
Release-2.3.0,register slow worker/ps checker
Release-2.3.0,register app manager event and finish event
Release-2.3.0,Init model saver & loader
Release-2.3.0,start a web service if use yarn deploy mode
Release-2.3.0,load from app state storage first if attempt index great than 1(the master is not the first
Release-2.3.0,retry)
Release-2.3.0,"if load failed, just build a new MatrixMetaManager"
Release-2.3.0,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-2.3.0,is not the first retry)
Release-2.3.0,load task information from app state storage first if attempt index great than 1(the master
Release-2.3.0,is not the first retry)
Release-2.3.0,"if load failed, just build a new AMTaskManager"
Release-2.3.0,load data splits information from app state storage first if attempt index great than 1(the
Release-2.3.0,master is not the first retry)
Release-2.3.0,"if load failed, we need to recalculate the data splits"
Release-2.3.0,Check Workers
Release-2.3.0,Check PSS
Release-2.3.0,Check Clients
Release-2.3.0,Check PS Clients
Release-2.3.0,parse parameter server counters
Release-2.3.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.3.0,refresh last heartbeat timestamp
Release-2.3.0,send a state update event to the specific PSAttempt
Release-2.3.0,Check is there save request
Release-2.3.0,"LOG.info(""PS "" + psAttemptId + "" need save "" + subSaveContext);"
Release-2.3.0,Check is there load request
Release-2.3.0,"LOG.info(""PS "" + psAttemptId + "" need load "" + subLoadContext);"
Release-2.3.0,check matrix metadata inconsistencies between master and parameter server.
Release-2.3.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
Release-2.3.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
Release-2.3.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.3.0,choose a unused port
Release-2.3.0,start RPC server
Release-2.3.0,remove this parameter server attempt from monitor set
Release-2.3.0,remove this parameter server attempt from monitor set
Release-2.3.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.3.0,find workergroup in worker manager
Release-2.3.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
Release-2.3.0,"if this worker group run over, just return WORKERGROUP_EXITED"
Release-2.3.0,"if this worker group is running now, return tasks, workers, data splits for it"
Release-2.3.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.3.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.3.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.3.0,"in ANGEL_PS mode, task id may can not know advance"
Release-2.3.0,update the clock for this matrix
Release-2.3.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.3.0,"in ANGEL_PS mode, task id may can not know advance"
Release-2.3.0,update task iteration
Release-2.3.0,"LOG.info(""error happened in psAttempt "" + psAttemptId + "" error msg="" + request.getMsg());"
Release-2.3.0,remove this parameter server attempt from monitor set
Release-2.3.0,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
Release-2.3.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
Release-2.3.0,the number of splits equal to the number of tasks
Release-2.3.0,split data
Release-2.3.0,dispatch the splits to workergroups
Release-2.3.0,split data
Release-2.3.0,dispatch the splits to workergroups
Release-2.3.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.3.0,"first, then divided by expected split number"
Release-2.3.0,get input format class from configuration and then instantiation a input format object
Release-2.3.0,split data
Release-2.3.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.3.0,"first, then divided by expected split number"
Release-2.3.0,get input format class from configuration and then instantiation a input format object
Release-2.3.0,split data
Release-2.3.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.3.0,need to fine tune the number of workergroup and task based on the actual split number
Release-2.3.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.3.0,Record the location information for the splits in order to data localized schedule
Release-2.3.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.3.0,need to fine tune the number of workergroup and task based on the actual split number
Release-2.3.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.3.0,Record the location information for the splits in order to data localized schedule
Release-2.3.0,write meta data to a temporary file
Release-2.3.0,rename the temporary file to final file
Release-2.3.0,"if the file exists, read from file and deserialize it"
Release-2.3.0,write task meta
Release-2.3.0,write ps meta
Release-2.3.0,generate a temporary file
Release-2.3.0,write task meta to the temporary file first
Release-2.3.0,rename the temporary file to the final file
Release-2.3.0,"if last final task file exist, remove it"
Release-2.3.0,find task meta file which has max timestamp
Release-2.3.0,"if the file does not exist, just return null"
Release-2.3.0,read task meta from file and deserialize it
Release-2.3.0,generate a temporary file
Release-2.3.0,write ps meta to the temporary file first.
Release-2.3.0,rename the temporary file to the final file
Release-2.3.0,"if the old final file exist, just remove it"
Release-2.3.0,find ps meta file
Release-2.3.0,"if ps meta file does not exist, just return null"
Release-2.3.0,read ps meta from file and deserialize it
Release-2.3.0,"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),"
Release-2.3.0,String.valueOf(requestId));
Release-2.3.0,Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));
Release-2.3.0,saveContext.setTmpSavePath(tmpPath.toString());
Release-2.3.0,Filter old epoch trigger first
Release-2.3.0,Split the user request to sub-requests to pss
Release-2.3.0,Init matrix files meta
Release-2.3.0,Move output files
Release-2.3.0,Write the meta file
Release-2.3.0,Split the user request to sub-requests to pss
Release-2.3.0,check whether psagent heartbeat timeout
Release-2.3.0,Set up the launch command
Release-2.3.0,Duplicate the ByteBuffers for access by multiple containers.
Release-2.3.0,Construct the actual Container
Release-2.3.0,Application resources
Release-2.3.0,Application environment
Release-2.3.0,Service data
Release-2.3.0,Tokens
Release-2.3.0,Set up JobConf to be localized properly on the remote NM.
Release-2.3.0,Setup DistributedCache
Release-2.3.0,Setup up task credentials buffer
Release-2.3.0,LocalStorageToken is needed irrespective of whether security is enabled
Release-2.3.0,or not.
Release-2.3.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
Release-2.3.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
Release-2.3.0,Construct the actual Container
Release-2.3.0,The null fields are per-container and will be constructed for each
Release-2.3.0,container separately.
Release-2.3.0,Set up the launch command
Release-2.3.0,Duplicate the ByteBuffers for access by multiple containers.
Release-2.3.0,Construct the actual Container
Release-2.3.0,"a * in the classpath will only find a .jar, so we need to filter out"
Release-2.3.0,all .jars and add everything else
Release-2.3.0,Propagate the system classpath when using the mini cluster
Release-2.3.0,Add standard Hadoop classes
Release-2.3.0,Add mr
Release-2.3.0,Cache archives
Release-2.3.0,Cache files
Release-2.3.0,Sanity check
Release-2.3.0,Add URI fragment or just the filename
Release-2.3.0,Add the env variables passed by the user
Release-2.3.0,Set logging level in the environment.
Release-2.3.0,G1 params
Release-2.3.0,".append("" -XX:G1NewSizePercent="").append(minNewRatio)"
Release-2.3.0,".append("" -XX:G1MaxNewSizePercent="").append(maxNewRatio)"
Release-2.3.0,Setup the log4j prop
Release-2.3.0,Add main class and its arguments
Release-2.3.0,Finally add the jvmID
Release-2.3.0,vargs.add(String.valueOf(jvmID.getId()));
Release-2.3.0,Final commmand
Release-2.3.0,G1 params
Release-2.3.0,Add the env variables passed by the user
Release-2.3.0,Set logging level in the environment.
Release-2.3.0,Setup the log4j prop
Release-2.3.0,Add main class and its arguments
Release-2.3.0,Final commmand
Release-2.3.0,"if amTask is not null, we should clone task state from it"
Release-2.3.0,"if all parameter server complete commit, master can commit now"
Release-2.3.0,restartPS(psLoc);
Release-2.3.0,check whether parameter server heartbeat timeout
Release-2.3.0,Transitions from the NEW state.
Release-2.3.0,Transitions from the UNASSIGNED state.
Release-2.3.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
Release-2.3.0,Transitions from the ASSIGNED state.
Release-2.3.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
Release-2.3.0,PA_CONTAINER_LAUNCHED event
Release-2.3.0,Transitions from the PSAttemptStateInternal.RUNNING state.
Release-2.3.0,Transitions from the PSAttemptStateInternal.COMMITTING state
Release-2.3.0,Transitions from the PSAttemptStateInternal.KILLED state
Release-2.3.0,Transitions from the PSAttemptStateInternal.FAILED state
Release-2.3.0,create the topology tables
Release-2.3.0,reqeuest resource:send a resource request to the resource allocator
Release-2.3.0,"Once the resource is applied, build and send the launch request to the container launcher"
Release-2.3.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
Release-2.3.0,resource allocator
Release-2.3.0,set the launch time
Release-2.3.0,add the ps attempt to the heartbeat timeout monitoring list
Release-2.3.0,parse ps attempt location and put it to location manager
Release-2.3.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
Release-2.3.0,or failed
Release-2.3.0,remove ps attempt id from heartbeat timeout monitor list
Release-2.3.0,release container:send a release request to container launcher
Release-2.3.0,set the finish time only if launch time is set
Release-2.3.0,private long scheduledTime;
Release-2.3.0,Transitions from the NEW state.
Release-2.3.0,Transitions from the SCHEDULED state.
Release-2.3.0,Transitions from the RUNNING state.
Release-2.3.0,"another attempt launched,"
Release-2.3.0,Transitions from the SUCCEEDED state
Release-2.3.0,Transitions from the KILLED state
Release-2.3.0,Transitions from the FAILED state
Release-2.3.0,add diagnostic
Release-2.3.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.3.0,Refresh ps location & matrix meta
Release-2.3.0,start a new attempt for this ps
Release-2.3.0,notify ps manager
Release-2.3.0,"getContext().getLocationManager().setPsLocation(id, null);"
Release-2.3.0,add diagnostic
Release-2.3.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.3.0,start a new attempt for this ps
Release-2.3.0,notify ps manager
Release-2.3.0,notify the event handler of state change
Release-2.3.0,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
Release-2.3.0,"if forcedState is set, just return"
Release-2.3.0,else get state from state machine
Release-2.3.0,add this worker group to the success set
Release-2.3.0,check if all worker group run or run over
Release-2.3.0,add this worker group to the success set
Release-2.3.0,check if all worker group run over
Release-2.3.0,add this worker group to the failed set
Release-2.3.0,check if too many worker groups are failed or killed
Release-2.3.0,notify a run failed event
Release-2.3.0,add this worker group to the failed set
Release-2.3.0,check if too many worker groups are failed or killed
Release-2.3.0,notify a run failed event
Release-2.3.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
Release-2.3.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
Release-2.3.0,just return the total task number now
Release-2.3.0,TODO
Release-2.3.0,check whether worker heartbeat timeout
Release-2.3.0,"if workerAttempt is not null, we should clone task state from it"
Release-2.3.0,from NEW state
Release-2.3.0,from SCHEDULED state
Release-2.3.0,get data splits location for data locality
Release-2.3.0,reqeuest resource:send a resource request to the resource allocator
Release-2.3.0,"once the resource is applied, build and send the launch request to the container launcher"
Release-2.3.0,notify failed message to the worker
Release-2.3.0,notify killed message to the worker
Release-2.3.0,release the allocated container
Release-2.3.0,notify failed message to the worker
Release-2.3.0,remove the worker attempt from heartbeat timeout listen list
Release-2.3.0,release the allocated container
Release-2.3.0,notify killed message to the worker
Release-2.3.0,remove the worker attempt from heartbeat timeout listen list
Release-2.3.0,clean the container
Release-2.3.0,notify failed message to the worker
Release-2.3.0,remove the worker attempt from heartbeat timeout listen list
Release-2.3.0,record the finish time
Release-2.3.0,clean the container
Release-2.3.0,notify killed message to the worker
Release-2.3.0,remove the worker attempt from heartbeat timeout listening list
Release-2.3.0,record the finish time
Release-2.3.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
Release-2.3.0,set worker attempt location
Release-2.3.0,notify the register message to the worker
Release-2.3.0,record the launch time
Release-2.3.0,update worker attempt metrics
Release-2.3.0,update tasks metrics
Release-2.3.0,clean the container
Release-2.3.0,notify the worker attempt run successfully message to the worker
Release-2.3.0,record the finish time
Release-2.3.0,init a worker attempt for the worker
Release-2.3.0,schedule the worker attempt
Release-2.3.0,add diagnostic
Release-2.3.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.3.0,init and start a new attempt for this ps
Release-2.3.0,notify worker manager
Release-2.3.0,add diagnostic
Release-2.3.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.3.0,init and start a new attempt for this ps
Release-2.3.0,notify worker manager
Release-2.3.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
Release-2.3.0,register to Yarn RM
Release-2.3.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
Release-2.3.0,"catch YarnException or YarnRuntimeException, we should exit and need not retry"
Release-2.3.0,build heartbeat request
Release-2.3.0,send heartbeat request to rm
Release-2.3.0,"This can happen if the RM has been restarted. If it is in that state,"
Release-2.3.0,this application must clean itself up.
Release-2.3.0,Setting NMTokens
Release-2.3.0,assgin containers
Release-2.3.0,"if some container is not assigned, release them"
Release-2.3.0,handle finish containers
Release-2.3.0,dispatch container exit message to corresponding components
Release-2.3.0,killed by framework
Release-2.3.0,killed by framework
Release-2.3.0,get application finish state
Release-2.3.0,build application diagnostics
Release-2.3.0,TODO:add a job history for angel
Release-2.3.0,build unregister request
Release-2.3.0,send unregister request to rm
Release-2.3.0,Note this down for next interaction with ResourceManager
Release-2.3.0,based on blacklisting comments above we can end up decrementing more
Release-2.3.0,than requested. so guard for that.
Release-2.3.0,send the updated resource request to RM
Release-2.3.0,send 0 container count requests also to cancel previous requests
Release-2.3.0,Update resource requests
Release-2.3.0,try to assign to all nodes first to match node local
Release-2.3.0,try to match all rack local
Release-2.3.0,assign remaining
Release-2.3.0,Update resource requests
Release-2.3.0,send the container-assigned event to task attempt
Release-2.3.0,build the start container request use launch context
Release-2.3.0,send the start request to Yarn nm
Release-2.3.0,send the message that the container starts successfully to the corresponding component
Release-2.3.0,"after launching, send launched event to task attempt to move"
Release-2.3.0,it from ASSIGNED to RUNNING state
Release-2.3.0,send the message that the container starts failed to the corresponding component
Release-2.3.0,kill the remote container if already launched
Release-2.3.0,start a thread pool to startup the container
Release-2.3.0,See if we need up the pool size only if haven't reached the
Release-2.3.0,maximum limit yet.
Release-2.3.0,nodes where containers will run at *this* point of time. This is
Release-2.3.0,*not* the cluster size and doesn't need to be.
Release-2.3.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
Release-2.3.0,later is just a buffer so we are not always increasing the
Release-2.3.0,pool-size
Release-2.3.0,the events from the queue are handled in parallel
Release-2.3.0,using a thread pool
Release-2.3.0,return if already stopped
Release-2.3.0,shutdown any containers that might be left running
Release-2.3.0,Add one sync matrix
Release-2.3.0,addSyncMatrix();
Release-2.3.0,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
Release-2.3.0,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
Release-2.3.0,"LOG.info(""ps id = "" + psEntry.getKey());"
Release-2.3.0,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
Release-2.3.0,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
Release-2.3.0,"LOG.info(""matrix id = "" + metaEntry.getKey());"
Release-2.3.0,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
Release-2.3.0,}
Release-2.3.0,}
Release-2.3.0,get matrix ids in the parameter server report
Release-2.3.0,get the matrices parameter server need to create and delete
Release-2.3.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
Release-2.3.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
Release-2.3.0,Init control connection manager
Release-2.3.0,Get ps locations from master and put them to the location cache.
Release-2.3.0,Build and initialize rpc client to master
Release-2.3.0,Get psagent id
Release-2.3.0,Build PS control rpc client manager
Release-2.3.0,Build local location
Release-2.3.0,Initialize matrix meta information
Release-2.3.0,Start all services
Release-2.3.0,Stop all modules
Release-2.3.0,Stop all modules
Release-2.3.0,clock first
Release-2.3.0,wait
Release-2.3.0,Update generic resource counters
Release-2.3.0,Updating resources specified in ResourceCalculatorProcessTree
Release-2.3.0,Remove the CPU time consumed previously by JVM reuse
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,/ Plus a vector/matrix to the matrix stored in pss
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,/ Update a vector/matrix to the matrix stored in pss
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,/ Get values from pss use row/column indices
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"/ PSF get/update, use can implement their own psf"
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,/ Get a row or a batch of rows
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,Just return
Release-2.3.0,Just return
Release-2.3.0,Just return
Release-2.3.0,Just return
Release-2.3.0,Return a empty vector
Release-2.3.0,Return a empty vector
Release-2.3.0,Return a empty vector
Release-2.3.0,Return a empty vector
Release-2.3.0,Return a empty vector
Release-2.3.0,Return a empty vector
Release-2.3.0,Return a empty vector
Release-2.3.0,Return a empty vector
Release-2.3.0,"checkNotNull(func, ""func"");"
Release-2.3.0,Return a empty vector
Release-2.3.0,"checkNotNull(func, ""func"");"
Release-2.3.0,Return a empty vector
Release-2.3.0,"checkNotNull(func, ""func"");"
Release-2.3.0,Return a empty vector
Release-2.3.0,"checkNotNull(func, ""func"");"
Release-2.3.0,Return a empty vector
Release-2.3.0,"checkNotNull(func, ""func"");"
Release-2.3.0,Return a empty vector
Release-2.3.0,"checkNotNull(func, ""func"");"
Release-2.3.0,Return a empty vector
Release-2.3.0,"checkNotNull(func, ""func"");"
Release-2.3.0,Return a empty vector
Release-2.3.0,"checkNotNull(func, ""func"");"
Release-2.3.0,Return a empty vector
Release-2.3.0,Sort the partitions by start column index
Release-2.3.0,Generate a flush request and put it to request queue
Release-2.3.0,Generate a clock request and put it to request queue
Release-2.3.0,Generate a merge request and put it to request queue
Release-2.3.0,Generate a merge request and put it to request queue
Release-2.3.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
Release-2.3.0,matrix
Release-2.3.0,and add it to cache maps
Release-2.3.0,Add the message to the tree map
Release-2.3.0,"If there are flush / clock requests blocked, we need to put this merge request into"
Release-2.3.0,the waiting queue
Release-2.3.0,Launch a merge worker to merge the update to matrix op log cache
Release-2.3.0,Remove the message from the tree map
Release-2.3.0,Wake up blocked flush/clock request
Release-2.3.0,Add flush/clock request to listener list to waiting for all the existing
Release-2.3.0,updates are merged
Release-2.3.0,Wake up blocked flush/clock request
Release-2.3.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
Release-2.3.0,blocked.
Release-2.3.0,Get next merge message sequence id
Release-2.3.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
Release-2.3.0,position
Release-2.3.0,Wake up blocked merge requests
Release-2.3.0,Get minimal sequence id from listeners
Release-2.3.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
Release-2.3.0,should flush updates to local matrix storage
Release-2.3.0,Doing average or not
Release-2.3.0,Filter un-important update
Release-2.3.0,Split this row according the matrix partitions
Release-2.3.0,Set split context
Release-2.3.0,Remove the row from matrix
Release-2.3.0,buf.writeDouble(0.0);
Release-2.3.0,TODO:
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,"LOG.error(""channel "" + ctx.channel() + "" inactive"");"
Release-2.3.0,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
Release-2.3.0,"LOG.error(""put response message queue failed "", e);"
Release-2.3.0,Use Epoll for linux
Release-2.3.0,Update location table
Release-2.3.0,Remove the server from failed list
Release-2.3.0,Notify refresh success message to request dispatcher
Release-2.3.0,Check PS exist or not
Release-2.3.0,Check heartbeat timeout
Release-2.3.0,getPSState(entry.getKey());
Release-2.3.0,Check PS restart or not
Release-2.3.0,private final HashSet<ParameterServerId> refreshingServerSet;
Release-2.3.0,Add it to failed rpc list
Release-2.3.0,Add the server to gray server list
Release-2.3.0,Add it to failed rpc list
Release-2.3.0,Add the server to gray server list
Release-2.3.0,Move from gray server list to failed server list
Release-2.3.0,Handle the RPCS to this server
Release-2.3.0,Submit the schedulable failed get RPCS
Release-2.3.0,Submit new get RPCS
Release-2.3.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.3.0,"If the queue is empty, just return 0"
Release-2.3.0,"If request is not over limit, just submit it"
Release-2.3.0,Submit the schedulable failed get RPCS
Release-2.3.0,Submit new put RPCS
Release-2.3.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.3.0,"LOG.info(""choose put server "" + psIds[index]);"
Release-2.3.0,Check all pending RPCS
Release-2.3.0,Check get channel context
Release-2.3.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.3.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.3.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.3.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.3.0,channelManager.printPools();
Release-2.3.0,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
Release-2.3.0,if(ts - entry.getValue() > requestTimeOut * 2)  {
Release-2.3.0,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
Release-2.3.0,"+ "" milliseconds, close all channels to it"");"
Release-2.3.0,closeChannels(entry.getKey());
Release-2.3.0,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
Release-2.3.0,}
Release-2.3.0,}
Release-2.3.0,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
Release-2.3.0,Remove all pending RPCS
Release-2.3.0,Close all channel to this PS
Release-2.3.0,private Channel getChannel(Location loc) throws Exception {
Release-2.3.0,"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));"
Release-2.3.0,}
Release-2.3.0,private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {
Release-2.3.0,"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext"
Release-2.3.0,.get()
Release-2.3.0,.getConf()
Release-2.3.0,".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,"
Release-2.3.0,AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));
Release-2.3.0,}
Release-2.3.0,Get server id and location for this request
Release-2.3.0,"If location is null, means that the server is not ready"
Release-2.3.0,Get the channel for the location
Release-2.3.0,Check if need get token first
Release-2.3.0,Serialize the request
Release-2.3.0,Send the request
Release-2.3.0,get a channel to server from pool
Release-2.3.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.3.0,request.getContext().setChannelPool(pool);
Release-2.3.0,Allocate the bytebuf and serialize the request
Release-2.3.0,find the partition request context from cache
Release-2.3.0,"updateMatrixCache(request.getPartKey(), response.getPartition());"
Release-2.3.0,"updateMatrixCache(request.getPartKey(), response.getRowsSplit());"
Release-2.3.0,TODO
Release-2.3.0,ServerRow rowSplit = PSAgentContext.get().getMatricesCache()
Release-2.3.0,".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),"
Release-2.3.0,request.getRowIndex());
Release-2.3.0,response.setRowSplit(rowSplit);
Release-2.3.0,"updateMatrixCache(request.getPartKey(), response.getRowSplit());"
Release-2.3.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.3.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.3.0,TODO
Release-2.3.0,"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {"
Release-2.3.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);"
Release-2.3.0,}
Release-2.3.0,"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {"
Release-2.3.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);"
Release-2.3.0,}
Release-2.3.0,"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {"
Release-2.3.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);"
Release-2.3.0,}
Release-2.3.0,Get partitions for this row
Release-2.3.0,Distinct get row requests
Release-2.3.0,Get row splits of this row from the matrix cache first
Release-2.3.0,responseCache.addSubResponse(rowSplit);
Release-2.3.0,"If the row split does not exist in cache, get it from parameter server"
Release-2.3.0,Split the param use matrix partitions
Release-2.3.0,Send request to PSS
Release-2.3.0,Split the matrix oplog according to the matrix partitions
Release-2.3.0,"If need update clock, we should send requests to all partitions"
Release-2.3.0,Send request to PSS
Release-2.3.0,Filter the rowIds which are fetching now
Release-2.3.0,Send the rowIndex to rpc dispatcher and return immediately
Release-2.3.0,"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));"
Release-2.3.0,"LOG.info(""start to request "" + requestId);"
Release-2.3.0,"LOG.info(""start to request "" + requestId);"
Release-2.3.0,Split param use matrix partitons
Release-2.3.0,"If all sub-results are received, just remove request and result cache"
Release-2.3.0,Split this row according the matrix partitions
Release-2.3.0,Set split context
Release-2.3.0,Split this row according the matrix partitions
Release-2.3.0,Set split context
Release-2.3.0,long startTs = System.currentTimeMillis();
Release-2.3.0,"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.3.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
Release-2.3.0,Generate dispatch items and add them to the corresponding queues
Release-2.3.0,Filter the rowIds which are fetching now
Release-2.3.0,Sort the parts by partitionId
Release-2.3.0,Sort partition keys use start column index
Release-2.3.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.3.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.3.0,});
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,Sort the parts by partitionId
Release-2.3.0,Sort partition keys use start column index
Release-2.3.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.3.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.3.0,});
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,Put the row split to the cache(row index to row splits map)
Release-2.3.0,"If all splits of the row are received, means this row can be merged"
Release-2.3.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-2.3.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-2.3.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-2.3.0,TODO: ServerLongDoubleRow maybe use IntDoubleVector as inner storage
Release-2.3.0,TODO
Release-2.3.0,TODO
Release-2.3.0,/////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,TODO
Release-2.3.0,buf.writeDouble(0);
Release-2.3.0,TODO
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,Now we just support pipelined row splits merging for dense type row
Release-2.3.0,Pre-fetching is disable default
Release-2.3.0,matrix id to clock map
Release-2.3.0,"task index, it must be unique for whole application"
Release-2.3.0,Deserialize data splits meta
Release-2.3.0,Get workers
Release-2.3.0,Send request to every ps
Release-2.3.0,Wait the responses
Release-2.3.0,Update clock cache
Release-2.3.0,if(syncNum % 1024 == 0) {
Release-2.3.0,}
Release-2.3.0,"Use simple flow, do not use any cache"
Release-2.3.0,Get row from cache.
Release-2.3.0,"if row clock is satisfy ssp staleness limit, just return."
Release-2.3.0,Get row from ps.
Release-2.3.0,Wait until the clock value of this row is greater than or equal to the value
Release-2.3.0,"For ASYNC mode, just get from pss."
Release-2.3.0,"For BSP/SSP, get rows from storage/cache first"
Release-2.3.0,Get from ps.
Release-2.3.0,Wait until the clock value of this row is greater than or equal to the value
Release-2.3.0,"For ASYNC, just get rows from pss."
Release-2.3.0,no more retries.
Release-2.3.0,calculate sleep time and return.
Release-2.3.0,parse the i-th sleep-time
Release-2.3.0,parse the i-th number-of-retries
Release-2.3.0,calculateSleepTime may overflow.
Release-2.3.0,"A few common retry policies, with no delays."
Release-2.3.0,Read matrix meta from meta file
Release-2.3.0,Save partitions to files use fork-join
Release-2.3.0,Write the ps matrix meta to the meta file
Release-2.3.0,matrix.startServering();
Release-2.3.0,return;
Release-2.3.0,Read matrix meta from meta file
Release-2.3.0,Load partitions from file use fork-join
Release-2.3.0,Read matrix meta from meta file
Release-2.3.0,Sort partitions
Release-2.3.0,TODO:
Release-2.3.0,int size = rows.length;
Release-2.3.0,int size = rows.length;
Release-2.3.0,int size = rows.size();
Release-2.3.0,int size = rows.size();
Release-2.3.0,int size = rows.size();
Release-2.3.0,int size = rows.size();
Release-2.3.0,int size = rows.size();
Release-2.3.0,int size = rows.size();
Release-2.3.0,close is a local operation and should finish within milliseconds; timeout just to be safe
Release-2.3.0,response will be null for one way messages.
Release-2.3.0,maxFrameLength = 2G
Release-2.3.0,lengthFieldOffset = 0
Release-2.3.0,lengthFieldLength = 8
Release-2.3.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
Release-2.3.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
Release-2.3.0,"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())"
Release-2.3.0,.toString();
Release-2.3.0,indicates whether this connection's life cycle is managed
Release-2.3.0,See if we already have a connection (common case)
Release-2.3.0,create a unique lock for this RS + protocol (if necessary)
Release-2.3.0,get the RS lock
Release-2.3.0,do one more lookup in case we were stalled above
Release-2.3.0,Only create isa when we need to.
Release-2.3.0,definitely a cache miss. establish an RPC for
Release-2.3.0,this RS
Release-2.3.0,Throw what the RemoteException was carrying.
Release-2.3.0,check
Release-2.3.0,every
Release-2.3.0,minutes
Release-2.3.0,TODO
Release-2.3.0,创建failoverHandler
Release-2.3.0,"The number of times this invocation handler has ever been failed over,"
Release-2.3.0,before this method invocation attempt. Used to prevent concurrent
Release-2.3.0,failed method invocations from triggering multiple failover attempts.
Release-2.3.0,Make sure that concurrent failed method invocations
Release-2.3.0,only cause a
Release-2.3.0,single actual fail over.
Release-2.3.0,RpcController + Message in the method args
Release-2.3.0,(generated code from RPC bits in .proto files have
Release-2.3.0,RpcController)
Release-2.3.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
Release-2.3.0,+ (System.currentTimeMillis() - beforeConstructTs));
Release-2.3.0,get an instance of the method arg type
Release-2.3.0,RpcController + Message in the method args
Release-2.3.0,(generated code from RPC bits in .proto files have
Release-2.3.0,RpcController)
Release-2.3.0,Message (hand written code usually has only a single
Release-2.3.0,argument)
Release-2.3.0,log any RPC responses that are slower than the configured
Release-2.3.0,warn
Release-2.3.0,response time or larger than configured warning size
Release-2.3.0,"when tagging, we let TooLarge trump TooSmall to keep"
Release-2.3.0,output simple
Release-2.3.0,note that large responses will often also be slow.
Release-2.3.0,provides a count of log-reported slow responses
Release-2.3.0,RpcController + Message in the method args
Release-2.3.0,(generated code from RPC bits in .proto files have
Release-2.3.0,RpcController)
Release-2.3.0,unexpected
Release-2.3.0,"in the protobuf methods, args[1] is the only significant argument"
Release-2.3.0,for JSON encoding
Release-2.3.0,base information that is reported regardless of type of call
Release-2.3.0,Disable Nagle's Algorithm since we don't want packets to wait
Release-2.3.0,Configure the event pipeline factory.
Release-2.3.0,Make a new connection.
Release-2.3.0,Remove all pending requests (will be canceled after relinquishing
Release-2.3.0,write lock).
Release-2.3.0,Cancel any pending requests by sending errors to the callbacks:
Release-2.3.0,Close the channel:
Release-2.3.0,Close the connection:
Release-2.3.0,Shut down all thread pools to exit.
Release-2.3.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
Release-2.3.0,See NettyServer.prepareResponse for where we write out the response.
Release-2.3.0,"It writes the call.id (int), a boolean signifying any error (and if"
Release-2.3.0,"so the exception name/trace), and the response bytes"
Release-2.3.0,Read the call id.
Release-2.3.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
Release-2.3.0,"instead, it returns a null message object."
Release-2.3.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
Release-2.3.0,System.currentTimeMillis());
Release-2.3.0,"It would be good widen this to just Throwable, but IOException is what we"
Release-2.3.0,allow now
Release-2.3.0,not implemented
Release-2.3.0,not implemented
Release-2.3.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
Release-2.3.0,cache of RpcEngines by protocol
Release-2.3.0,return the RpcEngine configured to handle a protocol
Release-2.3.0,We only handle the ConnectException.
Release-2.3.0,This is the exception we can't handle.
Release-2.3.0,check if timed out
Release-2.3.0,wait for retry
Release-2.3.0,IGNORE
Release-2.3.0,return the RpcEngine that handles a proxy object
Release-2.3.0,The default implementation works synchronously
Release-2.3.0,punt: allocate a new buffer & copy into it
Release-2.3.0,Parse cmd parameters
Release-2.3.0,load hadoop configuration
Release-2.3.0,load angel system configuration
Release-2.3.0,load user configuration:
Release-2.3.0,load user config file
Release-2.3.0,load command line parameters
Release-2.3.0,load user job resource files
Release-2.3.0,load ml conf file for graph based algorithm
Release-2.3.0,load user job jar if it exist
Release-2.3.0,Expand the environment variable
Release-2.3.0,Add default fs(local fs) for lib jars.
Release-2.3.0,"LOG.info(System.getProperty(""user.dir""));"
Release-2.3.0,get tokens for all the required FileSystems..
Release-2.3.0,Whether we need to recursive look into the directory structure
Release-2.3.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.3.0,user provided one (if any).
Release-2.3.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.3.0,get tokens for all the required FileSystems..
Release-2.3.0,Whether we need to recursive look into the directory structure
Release-2.3.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.3.0,user provided one (if any).
Release-2.3.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.3.0,a simple hdfs copy function assume src path and dest path are in same hdfs
Release-2.3.0,and FileSystem object has same schema
Release-2.3.0,"If out path exist , just remove it first"
Release-2.3.0,Create parent directory if not exist
Release-2.3.0,Rename
Release-2.3.0,"LOG.warn(""interrupted while sleeping"", ie);"
Release-2.3.0,public static String getHostname() {
Release-2.3.0,try {
Release-2.3.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
Release-2.3.0,} catch (UnknownHostException uhe) {
Release-2.3.0,}
Release-2.3.0,"return new StringBuilder().append("""").append(uhe).toString();"
Release-2.3.0,}
Release-2.3.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
Release-2.3.0,String hostname = getHostname();
Release-2.3.0,String classname = clazz.getSimpleName();
Release-2.3.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
Release-2.3.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
Release-2.3.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
Release-2.3.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
Release-2.3.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
Release-2.3.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
Release-2.3.0,
Release-2.3.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
Release-2.3.0,public void run() {
Release-2.3.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
Release-2.3.0,"this.val$classname + "" at "" + this.val$hostname}));"
Release-2.3.0,}
Release-2.3.0,});
Release-2.3.0,}
Release-2.3.0,"We we interrupted because we're meant to stop? If not, just"
Release-2.3.0,continue ignoring the interruption
Release-2.3.0,Recalculate waitTime.
Release-2.3.0,// Begin delegation to Thread
Release-2.3.0,// End delegation to Thread
Release-2.3.0,instance submitter class
Release-2.3.0,Obtain filename from path
Release-2.3.0,Split filename to prexif and suffix (extension)
Release-2.3.0,Check if the filename is okay
Release-2.3.0,Prepare temporary file
Release-2.3.0,Prepare buffer for data copying
Release-2.3.0,Open and check input stream
Release-2.3.0,Open output stream and copy data between source file in JAR and the temporary file
Release-2.3.0,"If read/write fails, close streams safely before throwing an exception"
Release-2.3.0,"Finally, load the library"
Release-2.3.0,little endian load order
Release-2.3.0,tail
Release-2.3.0,fallthrough
Release-2.3.0,fallthrough
Release-2.3.0,finalization
Release-2.3.0,fmix(h1);
Release-2.3.0,----------
Release-2.3.0,body
Release-2.3.0,----------
Release-2.3.0,tail
Release-2.3.0,----------
Release-2.3.0,finalization
Release-2.3.0,----------
Release-2.3.0,body
Release-2.3.0,----------
Release-2.3.0,tail
Release-2.3.0,----------
Release-2.3.0,finalization
Release-2.3.0,throw new AngelException(e);
Release-2.3.0,JobStateProto jobState = report.getJobState();
Release-2.3.0,Check need load matrices
Release-2.3.0,Used for java code to get a AngelClient instance
Release-2.3.0,Used for python code to get a AngelClient instance
Release-2.3.0,load user job resource files
Release-2.3.0,the leaf level file should be readable by others
Release-2.3.0,the subdirs in the path should have execute permissions for
Release-2.3.0,others
Release-2.3.0,2.get job id
Release-2.3.0,Credentials credentials = new Credentials();
Release-2.3.0,4.copy resource files to hdfs
Release-2.3.0,5.write configuration to a xml file
Release-2.3.0,6.create am container context
Release-2.3.0,7.Submit to ResourceManager
Release-2.3.0,8.get app master client
Release-2.3.0,Create a number of filenames in the JobTracker's fs namespace
Release-2.3.0,add all the command line files/ jars and archive
Release-2.3.0,first copy them to jobtrackers filesystem
Release-2.3.0,should not throw a uri exception
Release-2.3.0,should not throw an uri excpetion
Release-2.3.0,set the timestamps of the archives and files
Release-2.3.0,set the public/private visibility of the archives and files
Release-2.3.0,get DelegationToken for each cached file
Release-2.3.0,check if we do not need to copy the files
Release-2.3.0,is jt using the same file system.
Release-2.3.0,just checking for uri strings... doing no dns lookups
Release-2.3.0,to see if the filesystems are the same. This is not optimal.
Release-2.3.0,but avoids name resolution.
Release-2.3.0,this might have name collisions. copy will throw an exception
Release-2.3.0,parse the original path to create new path
Release-2.3.0,check for ports
Release-2.3.0,Write job file to JobTracker's fs
Release-2.3.0,Setup resource requirements
Release-2.3.0,Setup LocalResources
Release-2.3.0,Setup security tokens
Release-2.3.0,Setup the command to run the AM
Release-2.3.0,Add AM user command opts
Release-2.3.0,Final command
Release-2.3.0,Setup the CLASSPATH in environment
Release-2.3.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
Release-2.3.0,Setup the environment variables for Admin first
Release-2.3.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
Release-2.3.0,Parse distributed cache
Release-2.3.0,Setup ContainerLaunchContext for AM container
Release-2.3.0,Set up the ApplicationSubmissionContext
Release-2.3.0,private volatile PS2PSPusherImpl ps2PSPusher;
Release-2.3.0,TODO
Release-2.3.0,Add tokens to new user so that it may execute its task correctly.
Release-2.3.0,TODO
Release-2.3.0,to exit
Release-2.3.0,TODO
Release-2.3.0,TODO
Release-2.3.0,ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));
Release-2.3.0,context.getSnapshotManager().processRecovery();
Release-2.3.0,Recover PS from snapshot or load path
Release-2.3.0,1. First check old snapshot
Release-2.3.0,2. Check new checkpoints
Release-2.3.0,3. Check load path setting and old save result
Release-2.3.0,Just init it again
Release-2.3.0,TODO
Release-2.3.0,if(ps2PSPusher != null) {
Release-2.3.0,ps2PSPusher.start();
Release-2.3.0,}
Release-2.3.0,public PS2PSPusherImpl getPs2PSPusher() {
Release-2.3.0,return ps2PSPusher;
Release-2.3.0,}
Release-2.3.0,"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");"
Release-2.3.0,"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);"
Release-2.3.0,Release the input buffer
Release-2.3.0,Release the input buffer
Release-2.3.0,"1. handle the rpc, get the response"
Release-2.3.0,Release the input buffer
Release-2.3.0,2. Serialize the response
Release-2.3.0,Send the serialized response
Release-2.3.0,Exception happened
Release-2.3.0,write seq id
Release-2.3.0,Just serialize the head
Release-2.3.0,Exception happened
Release-2.3.0,Allocate result buffer
Release-2.3.0,Exception happened
Release-2.3.0,Just serialize the head
Release-2.3.0,Exception happened
Release-2.3.0,runningContext.printToken();
Release-2.3.0,Reset the response and allocate buffer again
Release-2.3.0,Get partition and check the partition state
Release-2.3.0,Get the stored pss for this partition
Release-2.3.0,"Check this ps is the master ps for this location, only master ps can accept the update"
Release-2.3.0,Check the partition state again
Release-2.3.0,Start to put the update to the slave pss
Release-2.3.0,TODO
Release-2.3.0,"context.getPS2PSPusher().put(request, in, partLoc);"
Release-2.3.0,Get partition and check the partition state
Release-2.3.0,Get the stored pss for this partition
Release-2.3.0,"Check this ps is the master ps for this partition, if not, just return failed"
Release-2.3.0,Start to put the update to the slave pss
Release-2.3.0,TODO
Release-2.3.0,return ServerState.GENERAL;
Release-2.3.0,Use Epoll for linux
Release-2.3.0,public String uuid;
Release-2.3.0,TODO:
Release-2.3.0,part = new ServerPartition();
Release-2.3.0,TODO:
Release-2.3.0,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
Release-2.3.0,this.channelPool = channelPool;
Release-2.3.0,}
Release-2.3.0,private final ParameterServer psServer;
Release-2.3.0,Create and start workers
Release-2.3.0,Set workers
Release-2.3.0,Create and start workers
Release-2.3.0,Set workers
Release-2.3.0,"If matrix checkpoint path not exist, just return null"
Release-2.3.0,Return the path with maximum checkpoint id
Release-2.3.0,Rename temp to item path
Release-2.3.0,Checkpoint base path = Base dir/matrix name
Release-2.3.0,Path for this checkpoint
Release-2.3.0,Generate tmp path
Release-2.3.0,Delete old checkpoints
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.3.0,and call endWrite/endRead after
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.3.0,and call endWrite/endRead after
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"////// network io method, for model transform"
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,Serailize the head
Release-2.3.0,Serialize the storage
Release-2.3.0,Deserailze the head
Release-2.3.0,Deseralize the storage
Release-2.3.0,Serailize the head
Release-2.3.0,Serialize the storage
Release-2.3.0,Deserailze the head
Release-2.3.0,Deseralize the storage
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.3.0,and call endWrite/endRead after
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.3.0,and call endWrite/endRead after
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.3.0,and call endWrite/endRead after
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.3.0,and call endWrite/endRead after
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.3.0,and call endWrite/endRead after
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.3.0,and call endWrite/endRead after
Release-2.3.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,"Notice: Basic type ServerRow only support Vector as inner storage now, so you can use methods"
Release-2.3.0,to get inner vector for basic type ServerRow.
Release-2.3.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.3.0,Just update the exist element now!!
Release-2.3.0,Just update the exist element now!!
Release-2.3.0,TODO: just check the value is 0 or not now
Release-2.3.0,TODO: just check the value is zero or not now
Release-2.3.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-2.3.0,Attention: Only update the exist values for sorted storage method
Release-2.3.0,Attention: Only update exist element
Release-2.3.0,Attention: Only update the exist values for sorted storage method
Release-2.3.0,Attention: Only update exist element
Release-2.3.0,TODO: just check the value is zero or not now
Release-2.3.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-2.3.0,Valid element number
Release-2.3.0,Element data
Release-2.3.0,Valid element number
Release-2.3.0,Deserialize the data
Release-2.3.0,Element data
Release-2.3.0,Valid element number
Release-2.3.0,Element data
Release-2.3.0,Valid element number
Release-2.3.0,Deserialize the data
Release-2.3.0,Attention: Only update the exist values for sorted storage method
Release-2.3.0,Attention: Only update exist element
Release-2.3.0,TODO: just check the value is zero or not now
Release-2.3.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-2.3.0,TODO: just check the value is 0 or not now
Release-2.3.0,Just update the exist element now!!
Release-2.3.0,Just update the exist element now!!
Release-2.3.0,"Use sparse storage method, as some elements in the array maybe null"
Release-2.3.0,Array length
Release-2.3.0,Valid element number
Release-2.3.0,Element data
Release-2.3.0,Array len
Release-2.3.0,Valid element number
Release-2.3.0,"Use sparse storage method, as some elements in the array maybe null"
Release-2.3.0,Array length
Release-2.3.0,Valid element number
Release-2.3.0,Element data
Release-2.3.0,Element data
Release-2.3.0,Array len
Release-2.3.0,Valid element number
Release-2.3.0,Attention: Only update the exist values for sorted storage method
Release-2.3.0,Attention: Only update exist element
Release-2.3.0,TODO: just check the value is zero or not now
Release-2.3.0,"SPARSE and SORT, check index exist or not, When using SORT mode storage, the search efficiency is very low."
Release-2.3.0,Row type
Release-2.3.0,Storage method
Release-2.3.0,Key type
Release-2.3.0,Value type
Release-2.3.0,Vector dim
Release-2.3.0,Vector length
Release-2.3.0,Vector data
Release-2.3.0,Row type
Release-2.3.0,Storage method
Release-2.3.0,Key type
Release-2.3.0,Value type
Release-2.3.0,Vector dim
Release-2.3.0,Vector length
Release-2.3.0,Init the vector
Release-2.3.0,Vector data
Release-2.3.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.3.0,Get the array pair
Release-2.3.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.3.0,Get the array pair
Release-2.3.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.3.0,Get the array pair
Release-2.3.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.3.0,Get the array pair
Release-2.3.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.3.0,Get the array pair
Release-2.3.0,Impossible now
Release-2.3.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.3.0,Get the array pair
Release-2.3.0,Impossible now
Release-2.3.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.3.0,Get the array pair
Release-2.3.0,Impossible now
Release-2.3.0,"Sparse storage, use the iterator to avoid array copy"
Release-2.3.0,Get the array pair
Release-2.3.0,Impossible now
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,"If use sorted storage, we should get the array pair first"
Release-2.3.0,Just update the exist element now!!
Release-2.3.0,Just update the exist element now!!
Release-2.3.0,TODO: just check the value is 0 or not now
Release-2.3.0,Just update the exist element now!!
Release-2.3.0,Just update the exist element now!!
Release-2.3.0,TODO: just check the value is 0 or not now
Release-2.3.0,Valid element number
Release-2.3.0,Element data
Release-2.3.0,Valid element number
Release-2.3.0,Deserialize the data
Release-2.3.0,Element data
Release-2.3.0,Valid element number
Release-2.3.0,Element data
Release-2.3.0,Valid element number
Release-2.3.0,Deserialize the data
Release-2.3.0,private final List<PartitionKey> partitionKeys;
Release-2.3.0,Get server partition class
Release-2.3.0,"If partition class is not set, just use the default partition class"
Release-2.3.0,Get server partition storage class type
Release-2.3.0,Get value class
Release-2.3.0,"if col == -1, we use the start/end index to calculate range,"
Release-2.3.0,we use double to store the range value since two long minus might exceed the
Release-2.3.0,range of long.
Release-2.3.0,Serialize the head
Release-2.3.0,Serialize the storage
Release-2.3.0,Deserialize the head
Release-2.3.0,Deseralize the storage
Release-2.3.0,Row base partition
Release-2.3.0,"If storage class is not set, use default DenseServerRowsStorage"
Release-2.3.0,Serialize values
Release-2.3.0,Deserialize values
Release-2.3.0,Array size
Release-2.3.0,Actual write size
Release-2.3.0,Rows data
Release-2.3.0,Row id
Release-2.3.0,Row type
Release-2.3.0,Row data
Release-2.3.0,Array size
Release-2.3.0,Actual write row number
Release-2.3.0,Rows data
Release-2.3.0,Row id
Release-2.3.0,Create empty server row
Release-2.3.0,Row data
Release-2.3.0,Rows data
Release-2.3.0,TODO
Release-2.3.0,Serialize row offsets
Release-2.3.0,Serialize column offsets
Release-2.3.0,Deserialize row offset
Release-2.3.0,Deserialize row offset
Release-2.3.0,"If storage is set, just get a instance"
Release-2.3.0,"If storage is not set, use default"
Release-2.3.0,"If storage is set, just get a instance"
Release-2.3.0,"If storage is not set, use default"
Release-2.3.0,Map size
Release-2.3.0,Actual write size
Release-2.3.0,Rows data
Release-2.3.0,Row id
Release-2.3.0,Row type
Release-2.3.0,Row data
Release-2.3.0,Array size
Release-2.3.0,Actual write row number
Release-2.3.0,Rows data
Release-2.3.0,Row id
Release-2.3.0,Create empty server row
Release-2.3.0,Row data
Release-2.3.0,Rows data
Release-2.3.0,Use Epoll for linux
Release-2.3.0,find the partition request context from cache
Release-2.3.0,get a channel to server from pool
Release-2.3.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.3.0,channelManager.removeChannelPool(loc);
Release-2.3.0,Generate seq id
Release-2.3.0,Create a RecoverPartRequest
Release-2.3.0,Serialize the request
Release-2.3.0,Change the seqId for the request
Release-2.3.0,Serialize the request
Release-2.3.0,"First check the state of the channels in the pool, if a channel is unused, just return"
Release-2.3.0,"If all channels are in use, create a new channel or wait"
Release-2.3.0,Create a new channel
Release-2.3.0,"add the PSAgentContext,need fix"
Release-2.3.0,If col == -1 and start/end not set
Release-2.3.0,start/end set
Release-2.3.0,"for dense type, we need to set the colNum to set dim for vectors"
Release-2.3.0,"colNum set, start/end not set"
Release-2.3.0,Row number must > 0
Release-2.3.0,"both set, check its valid"
Release-2.3.0,public static final int T_INT_ARBITRARY_VALUE = 28;
Release-2.3.0,public static final int T_INVALID_VALUE = 29;
Release-2.3.0,TODO:add more vector type
Release-2.3.0,TODO : subDim set
Release-2.3.0,Sort the parts by partitionId
Release-2.3.0,Sort partition keys use start column index
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,Sort the parts by partitionId
Release-2.3.0,Sort partition keys use start column index
Release-2.3.0,"For each partition, we generate a update split."
Release-2.3.0,"Although the split is empty for partitions those without any update data,"
Release-2.3.0,we still need to generate a update split to update the clock info on ps.
Release-2.3.0,Split updates
Release-2.3.0,Shuffle update splits
Release-2.3.0,Generate part update parameters
Release-2.3.0,"Set split context: partition key, use int key for long key vector or not ect"
Release-2.3.0,write the max abs
Release-2.3.0,---------------------------------------------------
Release-2.3.0,---------------------------------------------------
Release-2.3.0,---------------------------------------------------------------
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,axis = 0: on rows
Release-2.3.0,axis = 1: on cols
Release-2.3.0,1. find the insert point
Release-2.3.0,2. check the capacity and insert
Release-2.3.0,3. increase size
Release-2.3.0,1. find the insert point
Release-2.3.0,2. check the capacity and insert
Release-2.3.0,3. increase size
Release-2.3.0,1. find the insert point
Release-2.3.0,2. check the capacity and insert
Release-2.3.0,3. increase size
Release-2.3.0,1. find the insert point
Release-2.3.0,2. check the capacity and insert
Release-2.3.0,3. increase size
Release-2.3.0,1. find the insert point
Release-2.3.0,2. check the capacity and insert
Release-2.3.0,3. increase size
Release-2.3.0,1. find the insert point
Release-2.3.0,2. check the capacity and insert
Release-2.3.0,3. increase size
Release-2.3.0,1. find the insert point
Release-2.3.0,2. check the capacity and insert
Release-2.3.0,3. increase size
Release-2.3.0,1. find the insert point
Release-2.3.0,2. check the capacity and insert
Release-2.3.0,3. increase size
Release-2.3.0,-----------------
Release-2.3.0,-----------------
Release-2.3.0,-----------------
Release-2.3.0,-----------------
Release-2.3.0,-----------------
Release-2.3.0,KeepStorage is guaranteed
Release-2.3.0,"ignore the isInplace option, since v2 is dense"
Release-2.3.0,"the value in old storage can be changed safe, so switch a storage"
Release-2.3.0,"but user required keep storage, we can prevent rehash"
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,KeepStorage is guaranteed
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,v1Size < v2Size * Constant.sparseThreshold
Release-2.3.0,KeepStorage is guaranteed
Release-2.3.0,"ignore the isInplace option, since v2 is dense"
Release-2.3.0,"the value in old storage can be changed safe, so switch a storage"
Release-2.3.0,"but user required keep storage, we can prevent rehash"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,prevent rehash
Release-2.3.0,KeepStorage is guaranteed
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,dense preferred
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,sorted preferred
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,multi-rehash
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"dense preferred, KeepStorage is guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,"sparse preferred, keep storage guaranteed"
Release-2.3.0,preferred dense
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,we gauss dense storage is more efficient
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.3.0,multi-rehash
Release-2.3.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,to avoid multi-rehash
Release-2.3.0,"Transform mat1, generate a new matrix"
Release-2.3.0,Split the row indices of mat1Trans
Release-2.3.0,Parallel execute use fork-join
Release-2.3.0,"Get the sub-matrix of left matrix, split by row"
Release-2.3.0,"Transform mat1, generate a new matrix"
Release-2.3.0,Split the row indices of mat1Trans
Release-2.3.0,Parallel execute use fork-join
Release-2.3.0,"Get the sub-matrix of left matrix, split by row"
Release-2.3.0,"mat1 trans true, mat trans true"
Release-2.3.0,"mat1 trans true, mat trans false"
Release-2.3.0,"mat1 trans false, mat trans true, important"
Release-2.3.0,"mat1 trans false, mat trans false"
Release-2.3.0,"mat1 trans true, mat trans true"
Release-2.3.0,"mat1 trans true, mat trans false"
Release-2.3.0,"mat1 trans false, mat trans true, important"
Release-2.3.0,"mat1 trans false, mat trans false"
Release-2.3.0,"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.3.0,"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.3.0,not the first time
Release-2.3.0,first time and do the sample
Release-2.3.0,set to zero
Release-2.3.0,not the first time
Release-2.3.0,first time and do the sample
Release-2.3.0,set to zero
Release-2.3.0,not the first time
Release-2.3.0,first time and do the sample
Release-2.3.0,set to zero
Release-2.3.0,not the first time
Release-2.3.0,first time and do the sample
Release-2.3.0,set to zero
Release-2.3.0,not the first time
Release-2.3.0,first time and do the sample
Release-2.3.0,set to zero
Release-2.3.0,not the first time
Release-2.3.0,first time and do the sample
Release-2.3.0,set to zero
Release-2.3.0,not the first time
Release-2.3.0,first time and do the sample
Release-2.3.0,set to zero
Release-2.3.0,not the first time
Release-2.3.0,first time and do the sample
Release-2.3.0,set to zero
Release-2.3.0,not the first time
Release-2.3.0,first time and do the sample
Release-2.3.0,set to zero
Release-2.3.0,not the first time
Release-2.3.0,first time and do the sample
Release-2.3.0,set to zero
Release-2.3.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.3.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.3.0,add dense double matrix
Release-2.3.0,TODO Auto-generated method stub
Release-2.3.0,TODO Auto-generated method stub
Release-2.3.0,TODO Auto-generated method stub
Release-2.3.0,get configuration from config file
Release-2.3.0,set localDir with enviroment set by nm.
Release-2.3.0,get master location
Release-2.3.0,init task manager and start tasks
Release-2.3.0,start heartbeat thread
Release-2.3.0,taskManager.assignTaskIds(response.getTaskidsList());
Release-2.3.0,todo
Release-2.3.0,"if worker timeout, it may be knocked off."
Release-2.3.0,"SUCCESS, do nothing"
Release-2.3.0,heartbeatFailedTime = 0;
Release-2.3.0,private KEY currentKey;
Release-2.3.0,will be created
Release-2.3.0,TODO Auto-generated method stub
Release-2.3.0,Bitmap bitmap = new Bitmap();
Release-2.3.0,int max = indexArray[size - 1];
Release-2.3.0,byte [] bitIndexArray = new byte[max / 8 + 1];
Release-2.3.0,for(int i = 0; i < size; i++){
Release-2.3.0,int bitIndex = indexArray[i] >> 3;
Release-2.3.0,int bitOffset = indexArray[i] - (bitIndex << 3);
Release-2.3.0,switch(bitOffset){
Release-2.3.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
Release-2.3.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
Release-2.3.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
Release-2.3.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
Release-2.3.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
Release-2.3.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
Release-2.3.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
Release-2.3.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
Release-2.3.0,}
Release-2.3.0,}
Release-2.3.0,"true, false"
Release-2.3.0,//////////////////////////////
Release-2.3.0,Application Configs
Release-2.3.0,//////////////////////////////
Release-2.3.0,//////////////////////////////
Release-2.3.0,Master Configs
Release-2.3.0,//////////////////////////////
Release-2.3.0,//////////////////////////////
Release-2.3.0,Worker Configs
Release-2.3.0,//////////////////////////////
Release-2.3.0,//////////////////////////////
Release-2.3.0,Task Configs
Release-2.3.0,//////////////////////////////
Release-2.3.0,//////////////////////////////
Release-2.3.0,ParameterServer Configs
Release-2.3.0,//////////////////////////////
Release-2.3.0,////////////////// IPC //////////////////////////
Release-2.3.0,//////////////////////////////
Release-2.3.0,Matrix transfer Configs.
Release-2.3.0,//////////////////////////////
Release-2.3.0,//////////////////////////////
Release-2.3.0,Matrix transfer Configs.
Release-2.3.0,//////////////////////////////
Release-2.3.0,Configs used to ANGEL_PS_PSAGENT running mode future.
Release-2.3.0,model parse
Release-2.3.0,Mark whether use pyangel or not.
Release-2.3.0,private Configuration conf;
Release-2.3.0,"Configuration that should be used in python environment, there should only be one"
Release-2.3.0,configuration instance in each Angel context.
Release-2.3.0,Use private access means jconf should not be changed or modified in this way.
Release-2.3.0,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
Release-2.3.0,Do nothing
Release-2.3.0,To-DO: add other ways to justify different value types
Release-2.3.0,"This is so ugly, must re-implement by more elegance way"
Release-2.3.0,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
Release-2.3.0,and other files submitted by user.
Release-2.3.0,Launch python process
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-2.3.0,get a angel client
Release-2.3.0,add sparse float matrix
Release-2.3.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-2.3.0,siMat.setPartitionClass(CSRPartition.class);
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,Init node neighbors
Release-2.3.0,client.asyncUpdate(new InitNeighborOver(new InitNeighborOverParam(matrixId))).get();
Release-2.3.0,Sample the neighbors
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-2.3.0,get a angel client
Release-2.3.0,add sparse float matrix
Release-2.3.0,siMat.setValidIndexNum(100);
Release-2.3.0,siMat.setColNum(10000000000L);
Release-2.3.0,siMat.setPartitionStorageClass(LongElementMapStorage.class);
Release-2.3.0,siMat.setPartitionClass(CSRPartition.class);
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,Init node neighbors and feats
Release-2.3.0,Sample the neighbors
Release-2.3.0,TODO Auto-generated constructor stub
Release-2.3.0,set basic configuration keys
Release-2.3.0,use local deploy mode and dummy dataspliter
Release-2.3.0,"conf.setInt(AngelConf.ANGEL_MODEL_PARTITIONER_PARTITION_SIZE, 1000);"
Release-2.3.0,get a angel client
Release-2.3.0,add sparse float matrix
Release-2.3.0,Start PS
Release-2.3.0,Start to run application
Release-2.3.0,Init node neighbors
Release-2.3.0,Sample the neighbors
Release-2.3.0,sample continuously beginning from a random index
Release-2.3.0,Get node neighbor number
Release-2.3.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-2.3.0,Get node neighbor number
Release-2.3.0,"If the neighbor number is 0, just return a int[0]"
Release-2.3.0,"If count <= 0 or the neighbor number is less or equal then count, just copy all neighbors to the result array"
Release-2.3.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-2.3.0,"System.out.println(""serialize size="" + (endIndex - startIndex));"
Release-2.3.0,"System.out.println(""deserialize size="" + nodesToNeighbors.size());"
Release-2.3.0,Store the total neighbor number of all nodes in rowOffsets
Release-2.3.0,"Put the node ids, node neighbor number, node neighbors to the cache"
Release-2.3.0,No data in this partition
Release-2.3.0,Get total neighbor number
Release-2.3.0,Final matrix column indices: neighbors node ids
Release-2.3.0,Write positions in cloumnIndices for nodes
Release-2.3.0,Copy all cached sub column indices to final column indices
Release-2.3.0,Read position for a sub column indices
Release-2.3.0,Copy column indices for a node to final column indices
Release-2.3.0,Update write position for this node in final column indices
Release-2.3.0,Update the read position in sub column indices
Release-2.3.0,Clear all temp data
Release-2.3.0,Get node neighbor number
Release-2.3.0,"If the neighbor number > count, just copy a range of neighbors to the result array, the copy position is random"
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Data format
Release-2.3.0,Feature number of train data
Release-2.3.0,Tree number
Release-2.3.0,Tree depth
Release-2.3.0,Split number
Release-2.3.0,Feature sample ratio
Release-2.3.0,Ratio of validation
Release-2.3.0,Learning rate
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Set data format
Release-2.3.0,"Set angel resource, #worker, #task, #PS"
Release-2.3.0,Set GBDT algorithm parameters
Release-2.3.0,Set training data path
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set predict data path
Release-2.3.0,Set load model path
Release-2.3.0,Set predict result path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Train batch number per epoch.
Release-2.3.0,Batch number
Release-2.3.0,Model type
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Set data format
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,set sgd LR algorithm parameters #feature #epoch
Release-2.3.0,Set trainning data path
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Train batch number per epoch.
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Set data format
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,set sgd FM algorithm parameters #feature #epoch
Release-2.3.0,Set trainning data path
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,Model type
Release-2.3.0,String modelType = String.valueOf(RowType.T_FLOAT_DENSE);
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,set sgd LR algorithm parameters #feature #epoch
Release-2.3.0,"conf.set(MLConf.ML_MODEL_TYPE(), modelType);"
Release-2.3.0,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
Release-2.3.0,predictTest();
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Set data format
Release-2.3.0,Set trainning data path
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set data format
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,class number
Release-2.3.0,Model type
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Set data format
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,set sgd LR algorithm parameters #feature #epoch
Release-2.3.0,Set log path
Release-2.3.0,Set trainning data path
Release-2.3.0,Set save model path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set save model path
Release-2.3.0,Set actionType incremental train
Release-2.3.0,Set log path
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set predict result path
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set training data path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Cluster center number
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Sample ratio per mini-batch
Release-2.3.0,C
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.3.0,Set data format
Release-2.3.0,Set trainning data path
Release-2.3.0,Set save model path
Release-2.3.0,Set log save path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set save model path
Release-2.3.0,Set actionType incremental train
Release-2.3.0,Set log path
Release-2.3.0,Set testing data path
Release-2.3.0,Set load model path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Model type
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Set data format
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,set sgd LR algorithm parameters #feature #epoch
Release-2.3.0,Set trainning data path
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set predict result path
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Data is classification
Release-2.3.0,Model is classification
Release-2.3.0,Train batch number per epoch.
Release-2.3.0,loss delta
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Set data format
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,set sgd LR algorithm parameters #feature #epoch
Release-2.3.0,Set trainning data path
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set save model path
Release-2.3.0,Set actionType incremental train
Release-2.3.0,Set log path
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,Feature number of train data
Release-2.3.0,Total iteration number
Release-2.3.0,Validation sample Ratio
Release-2.3.0,"Data format, libsvm or dummy"
Release-2.3.0,Data is classification
Release-2.3.0,Model is classification
Release-2.3.0,Train batch number per epoch.
Release-2.3.0,Learning rate
Release-2.3.0,Decay of learning rate
Release-2.3.0,Regularization coefficient
Release-2.3.0,Set local deploy mode
Release-2.3.0,Set basic configuration keys
Release-2.3.0,Set data format
Release-2.3.0,"set angel resource parameters #worker, #task, #PS"
Release-2.3.0,set sgd LR algorithm parameters #feature #epoch
Release-2.3.0,Set trainning data path
Release-2.3.0,Set save model path
Release-2.3.0,Set log path
Release-2.3.0,Set actionType train
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set save model path
Release-2.3.0,Set actionType incremental train
Release-2.3.0,Set log path
Release-2.3.0,Set trainning data path
Release-2.3.0,Set load model path
Release-2.3.0,Set predict result path
Release-2.3.0,Set actionType prediction
Release-2.3.0,TODO: optimize int key indices
Release-2.3.0,"System.out.println(""deserialize cols.length="" + nCols);"
Release-2.3.0,"System.out.print(""deserialize "");"
Release-2.3.0,"System.out.print(cols[c] + "" "");"
Release-2.3.0,System.out.println();
Release-2.3.0,TODO Auto-generated method stub
Release-2.3.0,"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.3.0,"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.3.0,"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));"
Release-2.3.0,"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));"
Release-2.3.0,"ground truth: positive, precision: positive"
Release-2.3.0,start row index for words
Release-2.3.0,start row index for docs
Release-2.3.0,doc ids
Release-2.3.0,topic assignments
Release-2.3.0,word to docs reverse index
Release-2.3.0,count word
Release-2.3.0,build word start index
Release-2.3.0,build word to doc reverse idx
Release-2.3.0,build dks
Release-2.3.0,dks = new TraverseHashMap[n_docs];
Release-2.3.0,for (int d = 0; d < n_docs; d++) {
Release-2.3.0,if (K < Short.MAX_VALUE) {
Release-2.3.0,if (docs.get(d).len < Byte.MAX_VALUE)
Release-2.3.0,dks[d] = new S2BTraverseMap(docs.get(d).len);
Release-2.3.0,if (docs.get(d).len < Short.MAX_VALUE)
Release-2.3.0,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
Release-2.3.0,else
Release-2.3.0,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
Release-2.3.0,} else {
Release-2.3.0,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
Release-2.3.0,}
Release-2.3.0,}
Release-2.3.0,build dks
Release-2.3.0,allocate update maps
Release-2.3.0,Skip if no token for this word
Release-2.3.0,Check whether error when fetching word-topic
Release-2.3.0,Build FTree for current word
Release-2.3.0,current doc
Release-2.3.0,old topic assignment
Release-2.3.0,"Check if error happens. if this happen, it's probably that failures happen to servers."
Release-2.3.0,We need to adjust the memory settings or network fetching parameters.
Release-2.3.0,Update statistics if needed
Release-2.3.0,Calculate psum and sample new topic
Release-2.3.0,Update statistics if needed
Release-2.3.0,Assign new topic
Release-2.3.0,Skip if no token for this word
Release-2.3.0,if (u >= p[end]) {
Release-2.3.0,"System.out.println(""u="" + u + "" p[end]="" + p[end] + "" start="" + start + "" end="" + end);"
Release-2.3.0,return end;
Release-2.3.0,}
Release-2.3.0,
Release-2.3.0,if (u < p[start]) {
Release-2.3.0,"System.out.println(""u="" + u + "" p[start]="" + p[start] + "" start="" + start + "" end="" + end);"
Release-2.3.0,return start;
Release-2.3.0,}
Release-2.3.0,The starting point
Release-2.3.0,There's always an unused entry.
Release-2.3.0,The starting point
Release-2.3.0,There's always an unused entry.
Release-2.3.0,The starting point
Release-2.3.0,There's always an unused entry.
Release-2.3.0,The starting point
Release-2.3.0,There's always an unused entry.
Release-2.3.0,The starting point
Release-2.3.0,There's always an unused entry.
Release-2.3.0,The starting point
Release-2.3.0,There's always an unused entry.
Release-2.3.0,print();
Release-2.3.0,The starting point
Release-2.3.0,There's always an unused entry.
Release-2.3.0,The starting point
Release-2.3.0,There's always an unused entry.
Release-2.3.0,The starting point
Release-2.3.0,There's always an unused entry.
Release-2.3.0,The starting point
Release-2.3.0,There's always an unused entry.
Release-2.3.0,print();
Release-2.3.0,Write #rows
Release-2.3.0,Write each row
Release-2.3.0,dense
Release-2.3.0,sparse
Release-2.3.0,LOG.info(buf.refCnt());
Release-2.3.0,dense
Release-2.3.0,sparse
Release-2.3.0,calculate columns
Release-2.3.0,reset(row);
Release-2.3.0,loss function
Release-2.3.0,gradient and hessian
Release-2.3.0,"categorical feature set, null: none, empty: all, else: partial"
Release-2.3.0,"node's end index in instancePos, instances in [start, end] belong to a tree node"
Release-2.3.0,initialize the phase
Release-2.3.0,current tree and depth
Release-2.3.0,create loss function
Release-2.3.0,calculate grad info of each instance
Release-2.3.0,"create data sketch, push candidate split value to PS"
Release-2.3.0,1. calculate candidate split value
Release-2.3.0,categorical features
Release-2.3.0,2. push local sketch to PS
Release-2.3.0,the leader worker
Release-2.3.0,merge categorical features
Release-2.3.0,create updates
Release-2.3.0,"pull the global sketch from PS, only called once by each worker"
Release-2.3.0,number of categorical feature
Release-2.3.0,sample feature
Release-2.3.0,push sampled feature set to the current tree
Release-2.3.0,create new tree
Release-2.3.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
Release-2.3.0,calculate gradient
Release-2.3.0,"1. create new tree, initialize tree nodes and node stats"
Release-2.3.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
Release-2.3.0,2.1. pull the sampled features of the current tree
Release-2.3.0,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
Release-2.3.0,"2.2. if use all the features, only called one"
Release-2.3.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
Release-2.3.0,4. set root node to active
Release-2.3.0,"5. reset instance position, set the root node's span"
Release-2.3.0,6. calculate gradient
Release-2.3.0,1. decide nodes that should be calculated
Release-2.3.0,2. decide calculated and subtracted tree nodes
Release-2.3.0,3. calculate threads
Release-2.3.0,wait until all threads finish
Release-2.3.0,4. subtract threads
Release-2.3.0,wait until all threads finish
Release-2.3.0,5. send histograms to PS
Release-2.3.0,6. update histogram cache
Release-2.3.0,clock
Release-2.3.0,find split
Release-2.3.0,"1. find responsible tree node, using RR scheme"
Release-2.3.0,2. pull gradient histogram
Release-2.3.0,2.1. get the name of this node's gradient histogram on PS
Release-2.3.0,2.2. pull the histogram
Release-2.3.0,2.3. find best split result of this tree node
Release-2.3.0,2.3.1 using server split
Release-2.3.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.3.0,update the grad stats of children node
Release-2.3.0,update the left child
Release-2.3.0,update the right child
Release-2.3.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.3.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
Release-2.3.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.3.0,2.3.5 reset this tree node's gradient histogram to 0
Release-2.3.0,3. push split feature to PS
Release-2.3.0,4. push split value to PS
Release-2.3.0,5. push split gain to PS
Release-2.3.0,6. set phase to AFTER_SPLIT
Release-2.3.0,this.phase = GBDTPhase.AFTER_SPLIT;
Release-2.3.0,clock
Release-2.3.0,1. get split feature
Release-2.3.0,2. get split value
Release-2.3.0,3. get split gain
Release-2.3.0,4. get node weight
Release-2.3.0,5. split node
Release-2.3.0,update local replica
Release-2.3.0,create AfterSplit task
Release-2.3.0,"2. check thread stats, if all threads finish, return"
Release-2.3.0,6. clock
Release-2.3.0,"split the span of one node, reset the instance position"
Release-2.3.0,in case this worker has no instance on this node
Release-2.3.0,set the span of left child
Release-2.3.0,set the span of right child
Release-2.3.0,"1. left to right, find the first instance that should be in the right child"
Release-2.3.0,"2. right to left, find the first instance that should be in the left child"
Release-2.3.0,3. swap two instances
Release-2.3.0,4. find the cut pos
Release-2.3.0,5. set the span of left child
Release-2.3.0,6. set the span of right child
Release-2.3.0,set tree node to active
Release-2.3.0,set node to leaf
Release-2.3.0,set node to inactive
Release-2.3.0,finish current depth
Release-2.3.0,finish current tree
Release-2.3.0,set the tree phase
Release-2.3.0,check if there is active node
Release-2.3.0,check if finish all the tree
Release-2.3.0,update node's grad stats on PS
Release-2.3.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
Release-2.3.0,the root node's stats is updated by leader worker
Release-2.3.0,1. create the update
Release-2.3.0,2. push the update to PS
Release-2.3.0,1. update predictions of training data
Release-2.3.0,2. update predictions of validation data
Release-2.3.0,the leader task adds node prediction to flush list
Release-2.3.0,1. name of this node's grad histogram on PS
Release-2.3.0,2. build the grad histogram of this node
Release-2.3.0,3. push the histograms to PS
Release-2.3.0,4. reset thread stats to finished
Release-2.3.0,5.1. set the children nodes of this node
Release-2.3.0,5.2. set split info and grad stats to this node
Release-2.3.0,5.2. create children nodes
Release-2.3.0,"5.3. create node stats for children nodes, and add them to the tree"
Release-2.3.0,5.4. reset instance position
Release-2.3.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
Release-2.3.0,5.6. set children nodes to leaf nodes
Release-2.3.0,5.7. set nid to leaf node
Release-2.3.0,5.8. deactivate active node
Release-2.3.0,"get feature type, 0:empty 1:all equal 2:real"
Release-2.3.0,"if not -1, sufficient space will be allocated at once"
Release-2.3.0,copy the highest levels
Release-2.3.0,copy baseBuffer
Release-2.3.0,merge two non-empty quantile sketches
Release-2.3.0,left child <= split value; right child > split value
Release-2.3.0,"the first: minimal, the last: maximal"
Release-2.3.0,categorical features
Release-2.3.0,continuous features
Release-2.3.0,left child <= split value; right child > split value
Release-2.3.0,feature index used to split
Release-2.3.0,feature value used to split
Release-2.3.0,loss change after split this node
Release-2.3.0,grad stats of the left child
Release-2.3.0,grad stats of the right child
Release-2.3.0,"LOG.info(""Constructor with fid = -1"");"
Release-2.3.0,fid = -1: no split currently
Release-2.3.0,the minimal split value is the minimal value of feature
Release-2.3.0,the splits do not include the maximal value of feature
Release-2.3.0,"1. the average distance, (maxValue - minValue) / splitNum"
Release-2.3.0,2. calculate the candidate split value
Release-2.3.0,1. new feature's histogram (grad + hess)
Release-2.3.0,size: sampled_featureNum * (2 * splitNum)
Release-2.3.0,"in other words, concatenate each feature's histogram"
Release-2.3.0,2. get the span of this node
Release-2.3.0,------ 3. using sparse-aware method to build histogram ---
Release-2.3.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
Release-2.3.0,3.1. get the instance index
Release-2.3.0,3.2. get the grad and hess of the instance
Release-2.3.0,3.3. add to the sum
Release-2.3.0,3.4. loop the non-zero entries
Release-2.3.0,3.4.1. get feature value
Release-2.3.0,3.4.2. current feature's position in the sampled feature set
Release-2.3.0,"int fPos = findFidPlace(this.controller.fSet, fid);"
Release-2.3.0,3.4.3. find the position of feature value in a histogram
Release-2.3.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
Release-2.3.0,3.4.4. add the grad and hess to the corresponding bin
Release-2.3.0,3.4.5. add the reverse to the bin that contains 0.0f
Release-2.3.0,4. add the grad and hess sum to the zero bin of all features
Release-2.3.0,find the best split result of the histogram of a tree node
Release-2.3.0,1. calculate the gradStats of the root node
Release-2.3.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
Release-2.3.0,2. loop over features
Release-2.3.0,2.1. get the ture feature id in the sampled feature set
Release-2.3.0,2.2. get the indexes of histogram of this feature
Release-2.3.0,2.3. find the best split of current feature
Release-2.3.0,2.4. update the best split result if possible
Release-2.3.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.3.0,3. update the grad stats of children node
Release-2.3.0,3.1. update the left child
Release-2.3.0,3.2. update the right child
Release-2.3.0,find the best split result of one feature
Release-2.3.0,1. set the feature id
Release-2.3.0,2. create the best left stats and right stats
Release-2.3.0,3. the gain of the root node
Release-2.3.0,4. create the temp left and right grad stats
Release-2.3.0,5. loop over all the data in histogram
Release-2.3.0,5.1. get the grad and hess of current hist bin
Release-2.3.0,5.2. check whether we can split with current left hessian
Release-2.3.0,right = root - left
Release-2.3.0,5.3. check whether we can split with current right hessian
Release-2.3.0,5.4. calculate the current loss gain
Release-2.3.0,5.5. check whether we should update the split result with current loss gain
Release-2.3.0,split value = sketches[splitIdx]
Release-2.3.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.3.0,6. set the best left and right grad stats
Release-2.3.0,partition number
Release-2.3.0,cols of each partition
Release-2.3.0,1. calculate the total grad sum and hess sum
Release-2.3.0,2. create the grad stats of the node
Release-2.3.0,1. calculate the total grad sum and hess sum
Release-2.3.0,2. create the grad stats of the node
Release-2.3.0,1. calculate the total grad sum and hess sum
Release-2.3.0,2. create the grad stats of the node
Release-2.3.0,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
Release-2.3.0,assert fvalue >= sketch[start] && fvalue <= sketch[end];
Release-2.3.0,if (left > end) return end - start;
Release-2.3.0,find the best split result of the histogram of a tree node
Release-2.3.0,2.2. get the indexes of histogram of this feature
Release-2.3.0,2.3. find the best split of current feature
Release-2.3.0,2.4. update the best split result if possible
Release-2.3.0,find the best split result of one feature
Release-2.3.0,1. set the feature id
Release-2.3.0,splitEntry.setFid(fid);
Release-2.3.0,2. create the best left stats and right stats
Release-2.3.0,3. the gain of the root node
Release-2.3.0,4. create the temp left and right grad stats
Release-2.3.0,5. loop over all the data in histogram
Release-2.3.0,5.1. get the grad and hess of current hist bin
Release-2.3.0,5.2. check whether we can split with current left hessian
Release-2.3.0,right = root - left
Release-2.3.0,5.3. check whether we can split with current right hessian
Release-2.3.0,5.4. calculate the current loss gain
Release-2.3.0,5.5. check whether we should update the split result with current loss gain
Release-2.3.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.3.0,6. set the best left and right grad stats
Release-2.3.0,find the best split result of a serve row on the PS
Release-2.3.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
Release-2.3.0,2.2. get the start index in histogram of this feature
Release-2.3.0,2.3. find the best split of current feature
Release-2.3.0,2.4. update the best split result if possible
Release-2.3.0,"find the best split result of one feature from a server row, used by the PS"
Release-2.3.0,1. set the feature id
Release-2.3.0,2. create the best left stats and right stats
Release-2.3.0,3. the gain of the root node
Release-2.3.0,4. create the temp left and right grad stats
Release-2.3.0,5. loop over all the data in histogram
Release-2.3.0,5.1. get the grad and hess of current hist bin
Release-2.3.0,5.2. check whether we can split with current left hessian
Release-2.3.0,right = root - left
Release-2.3.0,5.3. check whether we can split with current right hessian
Release-2.3.0,5.4. calculate the current loss gain
Release-2.3.0,5.5. check whether we should update the split result with current loss gain
Release-2.3.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
Release-2.3.0,the task use index to find fvalue
Release-2.3.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.3.0,6. set the best left and right grad stats
Release-2.3.0,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
Release-2.3.0,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
Release-2.3.0,max and min of each feature
Release-2.3.0,clear all the information
Release-2.3.0,calculate the sum of gradient and hess
Release-2.3.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
Release-2.3.0,ridx)
Release-2.3.0,check if necessary information is ready
Release-2.3.0,"same as add, reduce is used in All Reduce"
Release-2.3.0,"features used in this tree, if equals null, means use all the features without sampling"
Release-2.3.0,node in the tree
Release-2.3.0,the gradient info of each instances
Release-2.3.0,initialize nodes
Release-2.3.0,gradient
Release-2.3.0,second order gradient
Release-2.3.0,int sendStartCol = (int) row.getStartCol();
Release-2.3.0,logistic loss for binary classification task.
Release-2.3.0,"logistic loss, but predict un-transformed margin"
Release-2.3.0,check if label in range
Release-2.3.0,return the default evaluation metric for the objective
Release-2.3.0,"task type: classification, regression, or ranking"
Release-2.3.0,"quantile sketch, size = featureNum * splitNum"
Release-2.3.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
Release-2.3.0,"active tree nodes, size = pow(2, treeDepth) -1"
Release-2.3.0,sampled features. size = treeNum * sampleRatio * featureNum
Release-2.3.0,categorical feature. size = workerNum * cateFeatNum * splitNum
Release-2.3.0,"split features, size = treeNum * treeNodeNum"
Release-2.3.0,"split values, size = treeNum * treeNodeNum"
Release-2.3.0,"split gains, size = treeNum * treeNodeNum"
Release-2.3.0,"node weights, size = treeNum * treeNodeNum"
Release-2.3.0,"node preds, size = treeNum * treeNodeNum"
Release-2.3.0,if using PS to perform split
Release-2.3.0,step size for a tree
Release-2.3.0,number of class
Release-2.3.0,minimum loss change required for a split
Release-2.3.0,maximum depth of a tree
Release-2.3.0,number of features
Release-2.3.0,number of nonzero
Release-2.3.0,number of candidates split value
Release-2.3.0,----- the rest parameters are less important ----
Release-2.3.0,base instance weight
Release-2.3.0,minimum amount of hessian(weight) allowed in a child
Release-2.3.0,L2 regularization factor
Release-2.3.0,L1 regularization factor
Release-2.3.0,default direction choice
Release-2.3.0,maximum delta update we can add in weight estimation
Release-2.3.0,this parameter can be used to stabilize update
Release-2.3.0,default=0 means no constraint on weight delta
Release-2.3.0,whether we want to do subsample for row
Release-2.3.0,whether to subsample columns for each tree
Release-2.3.0,accuracy of sketch
Release-2.3.0,accuracy of sketch
Release-2.3.0,leaf vector size
Release-2.3.0,option for parallelization
Release-2.3.0,option to open cacheline optimization
Release-2.3.0,whether to not print info during training.
Release-2.3.0,maximum depth of the tree
Release-2.3.0,number of features used for tree construction
Release-2.3.0,"minimum loss change required for a split, otherwise stop split"
Release-2.3.0,----- the rest parameters are less important ----
Release-2.3.0,default direction choice
Release-2.3.0,whether we want to do sample data
Release-2.3.0,whether to sample columns during tree construction
Release-2.3.0,whether to use histogram for split
Release-2.3.0,number of histogram units
Release-2.3.0,whether to print info during training.
Release-2.3.0,----- the rest parameters are obtained after training ----
Release-2.3.0,total number of nodes
Release-2.3.0,number of deleted nodes */
Release-2.2.0,@maxIndex: this variable contains the max index of node/word
Release-2.2.0,values[b + offset] = (random.nextFloat() - 0.5f) / dimension;
Release-2.2.0,some params
Release-2.2.0,max index for node/word
Release-2.2.0,compute number of nodes for one row
Release-2.2.0,check the length of dot values
Release-2.2.0,merge dot values from all partitions
Release-2.2.0,Skip-Gram model
Release-2.2.0,Negative sampling
Release-2.2.0,used to accumulate the updates for input vectors
Release-2.2.0,Negative sampling
Release-2.2.0,accumulate for the hidden layer
Release-2.2.0,update output layer
Release-2.2.0,update the hidden layer
Release-2.2.0,update input
Release-2.2.0,Skip-Gram model
Release-2.2.0,Negative sampling
Release-2.2.0,used to accumulate the updates for input vectors
Release-2.2.0,Negative sampling
Release-2.2.0,accumulate for the hidden layer
Release-2.2.0,update output layer
Release-2.2.0,update the hidden layer
Release-2.2.0,update input
Release-2.2.0,update output
Release-2.2.0,Some params
Release-2.2.0,compute number of nodes for one row
Release-2.2.0,window size
Release-2.2.0,Skip-Gram model
Release-2.2.0,Accumulate the input vectors from context
Release-2.2.0,Negative sampling
Release-2.2.0,used to accumulate the updates for input vectors
Release-2.2.0,window size
Release-2.2.0,skip-gram model
Release-2.2.0,Negative sampling
Release-2.2.0,accumulate for the hidden layer
Release-2.2.0,update output layer
Release-2.2.0,update the hidden layer
Release-2.2.0,update input
Release-2.2.0,update output
Release-2.2.0,some params
Release-2.2.0,batch sentences
Release-2.2.0,max index for node/word
Release-2.2.0,compute number of nodes for one row
Release-2.2.0,check the length of dot values
Release-2.2.0,merge dot values from all partitions
Release-2.2.0,locates the input vectors to local array to prevent randomly access
Release-2.2.0,on the large server row.
Release-2.2.0,fill 0 for context vector
Release-2.2.0,window size
Release-2.2.0,Continuous bag-of-words Models
Release-2.2.0,Accumulate the input vectors from context
Release-2.2.0,Calculate the partial dot values
Release-2.2.0,We should guarantee here that the sample would not equal the ``word``
Release-2.2.0,used to accumulate the context input vectors
Release-2.2.0,locates the input vector into local arrays to prevent randomly access for
Release-2.2.0,the large server row.
Release-2.2.0,window size
Release-2.2.0,while true to prevent sampling out a positive target
Release-2.2.0,how to prevent the randomly access to the output vectors??
Release-2.2.0,accumulate gradients for the input vectors
Release-2.2.0,update output vectors
Release-2.2.0,update input
Release-2.2.0,update output
Release-2.2.0,Some params
Release-2.2.0,compute number of nodes for one row
Release-2.2.0,// calculate bias
Release-2.2.0,if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {
Release-2.2.0,"double zVal = VectorUtils.getDouble(z, 0);"
Release-2.2.0,"double nVal = VectorUtils.getDouble(n, 0);"
Release-2.2.0,"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));"
Release-2.2.0,}
Release-2.2.0,Do nothing.
Release-2.2.0,current word
Release-2.2.0,neu1 stores the average value of input vectors in the context (CBOW)
Release-2.2.0,Continuous Bag-of-Words Model
Release-2.2.0,Accumulate the input vectors from context
Release-2.2.0,negative sampling
Release-2.2.0,Using the sigmoid value from the pre-computed table
Release-2.2.0,accumulate for the hidden layer
Release-2.2.0,update output layer
Release-2.2.0,add the counter for target
Release-2.2.0,update hidden layer
Release-2.2.0,Update the input vector for each word in the context
Release-2.2.0,add the counter to input
Release-2.2.0,update input layers
Release-2.2.0,update output layers
Release-2.2.0,for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];
Release-2.2.0,copy the highest levels
Release-2.2.0,copy baseBuffer
Release-2.2.0,merge two non-empty quantile sketches
Release-2.2.0,"if not -1, sufficient space will be allocated at once"
Release-2.2.0,InstanceRow ins = instanceRows[insId];
Release-2.2.0,int[] indices = ins.indices();
Release-2.2.0,int[] bins = ins.bins();
Release-2.2.0,int nnz = indices.length;
Release-2.2.0,for (int j = 0; j < nnz; j++) {
Release-2.2.0,int fid = indices[j];
Release-2.2.0,if (isFeatUsed[fid - featLo]) {
Release-2.2.0,"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);"
Release-2.2.0,}
Release-2.2.0,}
Release-2.2.0,1. allocate histogram
Release-2.2.0,"2. loop non-zero instances, accumulate to histogram"
Release-2.2.0,if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature
Release-2.2.0,3. add remaining grad and hess to default bin
Release-2.2.0,"return param.calcWeights(grad, hess);"
Release-2.2.0,"numClass is usually small, so we do not use arraycopy here"
Release-2.2.0,"numClass is usually small, so we do not use arraycopy here"
Release-2.2.0,TODO: use more schema on default bin
Release-2.2.0,1. set default bin to left child
Release-2.2.0,"2. for other bins, find its location"
Release-2.2.0,3. create split set
Release-2.2.0,this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];
Release-2.2.0,predict sparse instance with indices and values
Release-2.2.0,predict libsvm data
Release-2.2.0,"Preconditions.checkArgument(preds.length == labels.length,"
Release-2.2.0,"""LogLossMetric should be used for binary-label classification"");"
Release-2.2.0,double loss = 0.0;
Release-2.2.0,for (int i = 0; i < preds.length; i++) {
Release-2.2.0,"loss += evalOne(preds[i], labels[i]);"
Release-2.2.0,}
Release-2.2.0,return loss / labels.length;
Release-2.2.0,double error = 0.0;
Release-2.2.0,if (preds.length == labels.length) {
Release-2.2.0,for (int i = 0; i < preds.length; i++) {
Release-2.2.0,"error += evalOne(preds[i], labels[i]);"
Release-2.2.0,}
Release-2.2.0,} else {
Release-2.2.0,int numLabel = preds.length / labels.length;
Release-2.2.0,float[] pred = new float[numLabel];
Release-2.2.0,for (int i = 0; i < labels.length; i++) {
Release-2.2.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.2.0,"error += evalOne(pred, labels[i]);"
Release-2.2.0,}
Release-2.2.0,}
Release-2.2.0,return error / labels.length;
Release-2.2.0,Preconditions.checkArgument(preds.length != labels.length
Release-2.2.0,"&& preds.length % labels.length == 0,"
Release-2.2.0,"""CrossEntropyMetric should be used for multi-label classification"");"
Release-2.2.0,double loss = 0.0;
Release-2.2.0,int numLabel = preds.length / labels.length;
Release-2.2.0,float[] pred = new float[numLabel];
Release-2.2.0,for (int i = 0; i < labels.length; i++) {
Release-2.2.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.2.0,"loss += evalOne(pred, labels[i]);"
Release-2.2.0,}
Release-2.2.0,return loss / labels.length;
Release-2.2.0,double correct = 0.0;
Release-2.2.0,if (preds.length == labels.length) {
Release-2.2.0,for (int i = 0; i < preds.length; i++) {
Release-2.2.0,"correct += evalOne(preds[i], labels[i]);"
Release-2.2.0,}
Release-2.2.0,} else {
Release-2.2.0,int numLabel = preds.length / labels.length;
Release-2.2.0,float[] pred = new float[numLabel];
Release-2.2.0,for (int i = 0; i < labels.length; i++) {
Release-2.2.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.2.0,"correct += evalOne(pred, labels[i]);"
Release-2.2.0,}
Release-2.2.0,}
Release-2.2.0,return (float) (correct / labels.length);
Release-2.2.0,double errSum = 0.0f;
Release-2.2.0,if (preds.length == labels.length) {
Release-2.2.0,for (int i = 0; i < preds.length; i++) {
Release-2.2.0,"errSum += evalOne(preds[i], labels[i]);"
Release-2.2.0,}
Release-2.2.0,} else {
Release-2.2.0,int numLabel = preds.length / labels.length;
Release-2.2.0,float[] pred = new float[numLabel];
Release-2.2.0,for (int i = 0; i < labels.length; i++) {
Release-2.2.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.2.0,"errSum += evalOne(pred, labels[i]);"
Release-2.2.0,}
Release-2.2.0,}
Release-2.2.0,return Math.sqrt(errSum / labels.length);
Release-2.2.0,"System.out.println(""----------"");"
Release-2.2.0,"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)"
Release-2.2.0,"+ "", mask = "" + Integer.toBinaryString(readMaskT));"
Release-2.2.0,readMaskT <<= 1;
Release-2.2.0,"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};"
Release-2.2.0,int n = bits.length;
Release-2.2.0,BufferedBitSet writeBitSet = new BufferedBitSet(n);
Release-2.2.0,"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);"
Release-2.2.0,if (bitSet.get(i) != bits[i]) {
Release-2.2.0,"throw new RuntimeException("""" + i);"
Release-2.2.0,}
Release-2.2.0,private final ByteBuffer bytes;
Release-2.2.0,"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {"
Release-2.2.0,int capacity = bytes.capacity() * 8;
Release-2.2.0,readIndexT = bytes.capacity() - 1;
Release-2.2.0,return bytes.get(index);
Release-2.2.0,TODO: use arraycopy to make it faster
Release-2.2.0,assert from >= this.from && to <= this.to;
Release-2.2.0,"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));"
Release-2.2.0,"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));"
Release-2.2.0,return bits.clone();
Release-2.2.0,private final SerializableBuffer bytes;
Release-2.2.0,private final ByteBuffer bytes;
Release-2.2.0,this.bytes = ByteBuffer.allocate(numBytes);
Release-2.2.0,public BufferedBitSetWriter(ByteBuffer bytes) {
Release-2.2.0,this.bytes = bytes;
Release-2.2.0,}
Release-2.2.0,"bytes.put(writeIndex++, (byte) writeBuffer);"
Release-2.2.0,public ByteBuffer getBytes() {
Release-2.2.0,return bytes;
Release-2.2.0,}
Release-2.2.0,ML TreeConf
Release-2.2.0,GBDT TreeConf
Release-2.2.0,"different types of tree node splits, enumerated by their complexity"
Release-2.2.0,"in order to reduce model size, we give priority to split point"
Release-2.2.0,"comparison between two split points, we give priority to lower feature index"
Release-2.2.0,TODO: comparison between two split sets
Release-2.2.0,"public boolean leafwise;  // true if leaf-wise training, false if level-wise training"
Release-2.2.0,TODO: regularization
Release-2.2.0,TODO: regularization
Release-2.2.0,public float insSampleRatio;  // subsample ratio for instances
Release-2.2.0,set basic configuration keys
Release-2.2.0,use local deploy mode and dummy data spliter
Release-2.2.0,get a angel client
Release-2.2.0,add matrix
Release-2.2.0,TODO Auto-generated constructor stub
Release-2.2.0,row 0 is a random uniform
Release-2.2.0,row 1 is a random normal
Release-2.2.0,row 2 is filled with 1.0
Release-2.2.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
Release-2.2.0,"paras[1] = ""abc"";"
Release-2.2.0,"paras[2] = ""123"";"
Release-2.2.0,Add standard Hadoop classes
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,set sgd LR algorithm parameters #feature #epoch
Release-2.2.0,Set input data path
Release-2.2.0,Set save model path
Release-2.2.0,Set actionType train
Release-2.2.0,QSLRRunner runner = new QSLRRunner();
Release-2.2.0,runner.train(conf);
Release-2.2.0,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
Release-2.2.0,Dataset
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,class number
Release-2.2.0,Model type
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,Train batch number per epoch.
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set file system
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Use local deploy mode and data format
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set Softmax algorithm parameters
Release-2.2.0,Set model class
Release-2.2.0,Dataset
Release-2.2.0,Data format
Release-2.2.0,Feature number of train data
Release-2.2.0,Tree number
Release-2.2.0,Tree depth
Release-2.2.0,Split number
Release-2.2.0,Feature sample ratio
Release-2.2.0,Ratio of validation
Release-2.2.0,Learning rate
Release-2.2.0,Set file system
Release-2.2.0,Use local deploy mode and data format
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource, #worker, #task, #PS"
Release-2.2.0,Set GBDT algorithm parameters
Release-2.2.0,Dataset
Release-2.2.0,Set file system
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Use local deploy mode
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set DeepFM algorithm parameters
Release-2.2.0,Set model class
Release-2.2.0,Dataset
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Model type
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set file system
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Use local deploy mode and data format
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set LR algorithm parameters
Release-2.2.0,Set model class
Release-2.2.0,Dataset
Release-2.2.0,Data format
Release-2.2.0,Model type
Release-2.2.0,Cluster center number
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Sample ratio per mini-batch
Release-2.2.0,C
Release-2.2.0,Set file system
Release-2.2.0,Use local deploy mode and data format
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource, #worker, #task, #PS"
Release-2.2.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.2.0,Dataset
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Model type
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set file system
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Use local deploy mode and data format
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set FM algorithm parameters
Release-2.2.0,Set model class
Release-2.2.0,Dataset
Release-2.2.0,Set file system
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Use local deploy mode
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set WideAndDeep algorithm parameters
Release-2.2.0,Set model class
Release-2.2.0,Dataset
Release-2.2.0,Data format
Release-2.2.0,"Set LDA parameters #V, #K"
Release-2.2.0,Set file system
Release-2.2.0,Use local deploy mode and data format
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource, #worker, #task, #PS"
Release-2.2.0,Set LDA algorithm parameters
Release-2.2.0,Dataset
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Model type
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set file system
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Use local deploy mode and data format
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set SVM algorithm parameters
Release-2.2.0,Set model class
Release-2.2.0,Dataset
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Model type
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,Model is classification
Release-2.2.0,Train batch number per epoch.
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set file system
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Use local deploy mode and data format
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set LR algorithm parameters
Release-2.2.0,Set model class
Release-2.2.0,Dataset
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Model type
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,Model is classification
Release-2.2.0,Train batch number per epoch.
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set file system
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Use local deploy mode and data format
Release-2.2.0,Set data path
Release-2.2.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set LR algorithm parameters
Release-2.2.0,Set model class
Release-2.2.0,Load model meta
Release-2.2.0,Convert model
Release-2.2.0,"Get input path, output path"
Release-2.2.0,Init serde
Release-2.2.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.2.0,Load model meta
Release-2.2.0,Convert model
Release-2.2.0,load hadoop configuration
Release-2.2.0,"Get input path, output path"
Release-2.2.0,Init serde
Release-2.2.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.2.0,input.seek(rowOffset.getOffset());
Release-2.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.2.0,input.seek(rowOffset.getOffset());
Release-2.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.2.0,input.seek(rowOffset.getOffset());
Release-2.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.2.0,input.seek(rowOffset.getOffset());
Release-2.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.2.0,input.seek(rowOffset.getOffset());
Release-2.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.2.0,input.seek(rowOffset.getOffset());
Release-2.2.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.2.0,input.seek(rowOffset.getOffset());
Release-2.2.0,Load model meta
Release-2.2.0,Check row type
Release-2.2.0,Load model
Release-2.2.0,Load model meta
Release-2.2.0,Check row type
Release-2.2.0,Load model
Release-2.2.0,Load model meta
Release-2.2.0,Check row type
Release-2.2.0,Load model
Release-2.2.0,Load model meta
Release-2.2.0,Check row type
Release-2.2.0,Load model
Release-2.2.0,Load model meta
Release-2.2.0,Check row type
Release-2.2.0,Load model
Release-2.2.0,Load model meta
Release-2.2.0,Check row type
Release-2.2.0,Load model
Release-2.2.0,Load model meta
Release-2.2.0,Check row type
Release-2.2.0,Load model
Release-2.2.0,Load model
Release-2.2.0,load hadoop configuration
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,System.out.println(content);
Release-2.2.0,https://blog.csdn.net/cocoonyang/article/details/63068108
Release-2.2.0,v1[i] = v1[i] + da * v2[i];
Release-2.2.0,"dgemm(String transa, String transb,"
Release-2.2.0,"int m, int n, int k,"
Release-2.2.0,"double alpha,"
Release-2.2.0,"double[] a, int lda,"
Release-2.2.0,"double[] b, int ldb,"
Release-2.2.0,"double beta,"
Release-2.2.0,"double[] c, int ldc);"
Release-2.2.0,C := alpha*op( A )*op( B ) + beta*C
Release-2.2.0,v1[i] = v1[i] + da * v2[i];
Release-2.2.0,y := alpha*A*x + beta*y
Release-2.2.0,y := alpha*A*x + beta*y
Release-2.2.0,y := alpha*A*x + beta*y
Release-2.2.0,"dgemm(String transa, String transb,"
Release-2.2.0,"int m, int n, int k,"
Release-2.2.0,"double alpha,"
Release-2.2.0,"double[] a, int lda,"
Release-2.2.0,"double[] b, int ldb,"
Release-2.2.0,"double beta,"
Release-2.2.0,"double[] c, int ldc);"
Release-2.2.0,C := alpha*op( A )*op( B ) + beta*C
Release-2.2.0,Default does nothing.
Release-2.2.0,The app injection is optional
Release-2.2.0,"renderText(""hello world"");"
Release-2.2.0,"user choose a workerGroupID from the workergroups page,"
Release-2.2.0,now we should change the AngelApp params and render the workergroup page;
Release-2.2.0,"static final String WORKER_ID = ""worker.id"";"
Release-2.2.0,"div(""#logo"")."
Release-2.2.0,"img(""/static/hadoop-st.png"")._()."
Release-2.2.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
Release-2.2.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
Release-2.2.0,JQueryUI.jsnotice(html);
Release-2.2.0,import org.apache.hadoop.conf.Configuration;
Release-2.2.0,import java.lang.reflect.Field;
Release-2.2.0,all the files in input set
Release-2.2.0,Shuffle the file
Release-2.2.0,Get the blocks for all files
Release-2.2.0,Adjust the maxSize to make the split more balanced
Release-2.2.0,Handle the splittable files
Release-2.2.0,Handle the unsplittable files
Release-2.2.0,Split the blocks
Release-2.2.0,"If the remaining size of the current block is smaller than the required size,"
Release-2.2.0,the remaining blocks are divided into the current split
Release-2.2.0,Update current split length and move to next block
Release-2.2.0,Clear the current block offset
Release-2.2.0,"Current split length is > maxSize, split the block and generate a new split"
Release-2.2.0,Clear blocks list for next split
Release-2.2.0,Clear the current split length
Release-2.2.0,"If splitBlocks is not empty, just genetate a split for it"
Release-2.2.0,get block locations from file system
Release-2.2.0,create an input split
Release-2.2.0,get block locations from file system
Release-2.2.0,create a list of all block and their locations
Release-2.2.0,"if the file is not splitable, just create the one block with"
Release-2.2.0,full file length
Release-2.2.0,each split can be a maximum of maxSize
Release-2.2.0,if remainder is between max and 2*max - then
Release-2.2.0,"instead of creating splits of size max, left-max we"
Release-2.2.0,create splits of size left/2 and left/2. This is
Release-2.2.0,a heuristic to avoid creating really really small
Release-2.2.0,splits.
Release-2.2.0,add this block to the block --> node locations map
Release-2.2.0,"For blocks that do not have host/rack information,"
Release-2.2.0,assign to default  rack.
Release-2.2.0,add this block to the rack --> block map
Release-2.2.0,Add this host to rackToNodes map
Release-2.2.0,add this block to the node --> block map
Release-2.2.0,"if the file system does not have any rack information, then"
Release-2.2.0,use dummy rack location.
Release-2.2.0,The topology paths have the host name included as the last
Release-2.2.0,component. Strip it.
Release-2.2.0,get tokens for all the required FileSystems..
Release-2.2.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
Release-2.2.0,job.getConfiguration());
Release-2.2.0,Whether we need to recursive look into the directory structure
Release-2.2.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.2.0,user provided one (if any).
Release-2.2.0,all the files in input set
Release-2.2.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
Release-2.2.0,process all nodes and create splits that are local to a node. Generate
Release-2.2.0,"one split per node iteration, and walk over nodes multiple times to"
Release-2.2.0,distribute the splits across nodes.
Release-2.2.0,Skip the node if it has previously been marked as completed.
Release-2.2.0,"for each block, copy it into validBlocks. Delete it from"
Release-2.2.0,blockToNodes so that the same block does not appear in
Release-2.2.0,two different splits.
Release-2.2.0,Remove all blocks which may already have been assigned to other
Release-2.2.0,splits.
Release-2.2.0,"if the accumulated split size exceeds the maximum, then"
Release-2.2.0,create this split.
Release-2.2.0,create an input split and add it to the splits array
Release-2.2.0,Remove entries from blocksInNode so that we don't walk these
Release-2.2.0,again.
Release-2.2.0,Done creating a single split for this node. Move on to the next
Release-2.2.0,node so that splits are distributed across nodes.
Release-2.2.0,This implies that the last few blocks (or all in case maxSize=0)
Release-2.2.0,were not part of a split. The node is complete.
Release-2.2.0,if there were any blocks left over and their combined size is
Release-2.2.0,"larger than minSplitNode, then combine them into one split."
Release-2.2.0,Otherwise add them back to the unprocessed pool. It is likely
Release-2.2.0,that they will be combined with other blocks from the
Release-2.2.0,same rack later on.
Release-2.2.0,This condition also kicks in when max split size is not set. All
Release-2.2.0,blocks on a node will be grouped together into a single split.
Release-2.2.0,haven't created any split on this machine. so its ok to add a
Release-2.2.0,smaller one for parallelism. Otherwise group it in the rack for
Release-2.2.0,balanced size create an input split and add it to the splits
Release-2.2.0,array
Release-2.2.0,Remove entries from blocksInNode so that we don't walk this again.
Release-2.2.0,The node is done. This was the last set of blocks for this node.
Release-2.2.0,Put the unplaced blocks back into the pool for later rack-allocation.
Release-2.2.0,Node is done. All blocks were fit into node-local splits.
Release-2.2.0,Check if node-local assignments are complete.
Release-2.2.0,All nodes have been walked over and marked as completed or all blocks
Release-2.2.0,have been assigned. The rest should be handled via rackLock assignment.
Release-2.2.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
Release-2.2.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
Release-2.2.0,"if blocks in a rack are below the specified minimum size, then keep them"
Release-2.2.0,"in 'overflow'. After the processing of all racks is complete, these"
Release-2.2.0,overflow blocks will be combined into splits.
Release-2.2.0,Process all racks over and over again until there is no more work to do.
Release-2.2.0,Create one split for this rack before moving over to the next rack.
Release-2.2.0,Come back to this rack after creating a single split for each of the
Release-2.2.0,remaining racks.
Release-2.2.0,"Process one rack location at a time, Combine all possible blocks that"
Release-2.2.0,reside on this rack as one split. (constrained by minimum and maximum
Release-2.2.0,split size).
Release-2.2.0,iterate over all racks
Release-2.2.0,"for each block, copy it into validBlocks. Delete it from"
Release-2.2.0,blockToNodes so that the same block does not appear in
Release-2.2.0,two different splits.
Release-2.2.0,"if the accumulated split size exceeds the maximum, then"
Release-2.2.0,create this split.
Release-2.2.0,create an input split and add it to the splits array
Release-2.2.0,"if we created a split, then just go to the next rack"
Release-2.2.0,"if there is a minimum size specified, then create a single split"
Release-2.2.0,"otherwise, store these blocks into overflow data structure"
Release-2.2.0,There were a few blocks in this rack that
Release-2.2.0,remained to be processed. Keep them in 'overflow' block list.
Release-2.2.0,These will be combined later.
Release-2.2.0,Process all overflow blocks
Release-2.2.0,"This might cause an exiting rack location to be re-added,"
Release-2.2.0,but it should be ok.
Release-2.2.0,"if the accumulated split size exceeds the maximum, then"
Release-2.2.0,create this split.
Release-2.2.0,create an input split and add it to the splits array
Release-2.2.0,"Process any remaining blocks, if any."
Release-2.2.0,create an input split
Release-2.2.0,add this split to the list that is returned
Release-2.2.0,long num = totLength / maxSize;
Release-2.2.0,all blocks for all the files in input set
Release-2.2.0,mapping from a rack name to the list of blocks it has
Release-2.2.0,mapping from a block to the nodes on which it has replicas
Release-2.2.0,mapping from a node to the list of blocks that it contains
Release-2.2.0,populate all the blocks for all files
Release-2.2.0,stop all services
Release-2.2.0,1.write application state to file so that the client can get the state of the application
Release-2.2.0,if master exit
Release-2.2.0,2.clear tmp and staging directory
Release-2.2.0,waiting for client to get application state
Release-2.2.0,stop the RPC server
Release-2.2.0,"Security framework already loaded the tokens into current UGI, just use"
Release-2.2.0,them
Release-2.2.0,Now remove the AM->RM token so tasks don't have it
Release-2.2.0,add a shutdown hook
Release-2.2.0,init app state storage
Release-2.2.0,init event dispacher
Release-2.2.0,init location manager
Release-2.2.0,init container allocator
Release-2.2.0,init a rpc service
Release-2.2.0,recover matrix meta if needed
Release-2.2.0,recover ps attempt information if need
Release-2.2.0,Init Client manager
Release-2.2.0,Init PS Client manager
Release-2.2.0,init parameter server manager
Release-2.2.0,recover task information if needed
Release-2.2.0,a dummy data spliter is just for test now
Release-2.2.0,recover data splits information if needed
Release-2.2.0,init worker manager and register worker manager event
Release-2.2.0,register slow worker/ps checker
Release-2.2.0,register app manager event and finish event
Release-2.2.0,Init model saver & loader
Release-2.2.0,start a web service if use yarn deploy mode
Release-2.2.0,load from app state storage first if attempt index great than 1(the master is not the first
Release-2.2.0,retry)
Release-2.2.0,"if load failed, just build a new MatrixMetaManager"
Release-2.2.0,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-2.2.0,is not the first retry)
Release-2.2.0,load task information from app state storage first if attempt index great than 1(the master
Release-2.2.0,is not the first retry)
Release-2.2.0,"if load failed, just build a new AMTaskManager"
Release-2.2.0,load data splits information from app state storage first if attempt index great than 1(the
Release-2.2.0,master is not the first retry)
Release-2.2.0,"if load failed, we need to recalculate the data splits"
Release-2.2.0,Check Workers
Release-2.2.0,Check PSS
Release-2.2.0,Check Clients
Release-2.2.0,Check PS Clients
Release-2.2.0,parse parameter server counters
Release-2.2.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.2.0,refresh last heartbeat timestamp
Release-2.2.0,send a state update event to the specific PSAttempt
Release-2.2.0,Check is there save request
Release-2.2.0,Check is there load request
Release-2.2.0,check matrix metadata inconsistencies between master and parameter server.
Release-2.2.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
Release-2.2.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
Release-2.2.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.2.0,choose a unused port
Release-2.2.0,start RPC server
Release-2.2.0,remove this parameter server attempt from monitor set
Release-2.2.0,remove this parameter server attempt from monitor set
Release-2.2.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.2.0,find workergroup in worker manager
Release-2.2.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
Release-2.2.0,"if this worker group run over, just return WORKERGROUP_EXITED"
Release-2.2.0,"if this worker group is running now, return tasks, workers, data splits for it"
Release-2.2.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.2.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.2.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.2.0,"in ANGEL_PS mode, task id may can not know advance"
Release-2.2.0,update the clock for this matrix
Release-2.2.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.2.0,"in ANGEL_PS mode, task id may can not know advance"
Release-2.2.0,update task iteration
Release-2.2.0,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
Release-2.2.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
Release-2.2.0,the number of splits equal to the number of tasks
Release-2.2.0,split data
Release-2.2.0,dispatch the splits to workergroups
Release-2.2.0,split data
Release-2.2.0,dispatch the splits to workergroups
Release-2.2.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.2.0,"first, then divided by expected split number"
Release-2.2.0,get input format class from configuration and then instantiation a input format object
Release-2.2.0,split data
Release-2.2.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.2.0,"first, then divided by expected split number"
Release-2.2.0,get input format class from configuration and then instantiation a input format object
Release-2.2.0,split data
Release-2.2.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.2.0,need to fine tune the number of workergroup and task based on the actual split number
Release-2.2.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.2.0,Record the location information for the splits in order to data localized schedule
Release-2.2.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.2.0,need to fine tune the number of workergroup and task based on the actual split number
Release-2.2.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.2.0,Record the location information for the splits in order to data localized schedule
Release-2.2.0,write meta data to a temporary file
Release-2.2.0,rename the temporary file to final file
Release-2.2.0,"if the file exists, read from file and deserialize it"
Release-2.2.0,write task meta
Release-2.2.0,write ps meta
Release-2.2.0,generate a temporary file
Release-2.2.0,write task meta to the temporary file first
Release-2.2.0,rename the temporary file to the final file
Release-2.2.0,"if last final task file exist, remove it"
Release-2.2.0,find task meta file which has max timestamp
Release-2.2.0,"if the file does not exist, just return null"
Release-2.2.0,read task meta from file and deserialize it
Release-2.2.0,generate a temporary file
Release-2.2.0,write ps meta to the temporary file first.
Release-2.2.0,rename the temporary file to the final file
Release-2.2.0,"if the old final file exist, just remove it"
Release-2.2.0,find ps meta file
Release-2.2.0,"if ps meta file does not exist, just return null"
Release-2.2.0,read ps meta from file and deserialize it
Release-2.2.0,"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),"
Release-2.2.0,String.valueOf(requestId));
Release-2.2.0,Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));
Release-2.2.0,saveContext.setTmpSavePath(tmpPath.toString());
Release-2.2.0,Filter old epoch trigger first
Release-2.2.0,Split the user request to sub-requests to pss
Release-2.2.0,Init matrix files meta
Release-2.2.0,Move output files
Release-2.2.0,Write the meta file
Release-2.2.0,Split the user request to sub-requests to pss
Release-2.2.0,check whether psagent heartbeat timeout
Release-2.2.0,Set up the launch command
Release-2.2.0,Duplicate the ByteBuffers for access by multiple containers.
Release-2.2.0,Construct the actual Container
Release-2.2.0,Application resources
Release-2.2.0,Application environment
Release-2.2.0,Service data
Release-2.2.0,Tokens
Release-2.2.0,Set up JobConf to be localized properly on the remote NM.
Release-2.2.0,Setup DistributedCache
Release-2.2.0,Setup up task credentials buffer
Release-2.2.0,LocalStorageToken is needed irrespective of whether security is enabled
Release-2.2.0,or not.
Release-2.2.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
Release-2.2.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
Release-2.2.0,Construct the actual Container
Release-2.2.0,The null fields are per-container and will be constructed for each
Release-2.2.0,container separately.
Release-2.2.0,Set up the launch command
Release-2.2.0,Duplicate the ByteBuffers for access by multiple containers.
Release-2.2.0,Construct the actual Container
Release-2.2.0,"a * in the classpath will only find a .jar, so we need to filter out"
Release-2.2.0,all .jars and add everything else
Release-2.2.0,Propagate the system classpath when using the mini cluster
Release-2.2.0,Add standard Hadoop classes
Release-2.2.0,Add mr
Release-2.2.0,Cache archives
Release-2.2.0,Cache files
Release-2.2.0,Sanity check
Release-2.2.0,Add URI fragment or just the filename
Release-2.2.0,Add the env variables passed by the user
Release-2.2.0,Set logging level in the environment.
Release-2.2.0,Setup the log4j prop
Release-2.2.0,Add main class and its arguments
Release-2.2.0,Finally add the jvmID
Release-2.2.0,vargs.add(String.valueOf(jvmID.getId()));
Release-2.2.0,Final commmand
Release-2.2.0,Add the env variables passed by the user
Release-2.2.0,Set logging level in the environment.
Release-2.2.0,Setup the log4j prop
Release-2.2.0,Add main class and its arguments
Release-2.2.0,Final commmand
Release-2.2.0,"if amTask is not null, we should clone task state from it"
Release-2.2.0,"if all parameter server complete commit, master can commit now"
Release-2.2.0,check whether parameter server heartbeat timeout
Release-2.2.0,Transitions from the NEW state.
Release-2.2.0,Transitions from the UNASSIGNED state.
Release-2.2.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
Release-2.2.0,Transitions from the ASSIGNED state.
Release-2.2.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
Release-2.2.0,PA_CONTAINER_LAUNCHED event
Release-2.2.0,Transitions from the PSAttemptStateInternal.RUNNING state.
Release-2.2.0,Transitions from the PSAttemptStateInternal.COMMITTING state
Release-2.2.0,Transitions from the PSAttemptStateInternal.KILLED state
Release-2.2.0,Transitions from the PSAttemptStateInternal.FAILED state
Release-2.2.0,create the topology tables
Release-2.2.0,reqeuest resource:send a resource request to the resource allocator
Release-2.2.0,"Once the resource is applied, build and send the launch request to the container launcher"
Release-2.2.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
Release-2.2.0,resource allocator
Release-2.2.0,set the launch time
Release-2.2.0,add the ps attempt to the heartbeat timeout monitoring list
Release-2.2.0,parse ps attempt location and put it to location manager
Release-2.2.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
Release-2.2.0,or failed
Release-2.2.0,remove ps attempt id from heartbeat timeout monitor list
Release-2.2.0,release container:send a release request to container launcher
Release-2.2.0,set the finish time only if launch time is set
Release-2.2.0,private long scheduledTime;
Release-2.2.0,Transitions from the NEW state.
Release-2.2.0,Transitions from the SCHEDULED state.
Release-2.2.0,Transitions from the RUNNING state.
Release-2.2.0,"another attempt launched,"
Release-2.2.0,Transitions from the SUCCEEDED state
Release-2.2.0,Transitions from the KILLED state
Release-2.2.0,Transitions from the FAILED state
Release-2.2.0,add diagnostic
Release-2.2.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.2.0,Refresh ps location & matrix meta
Release-2.2.0,start a new attempt for this ps
Release-2.2.0,notify ps manager
Release-2.2.0,"getContext().getLocationManager().setPsLocation(id, null);"
Release-2.2.0,add diagnostic
Release-2.2.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.2.0,start a new attempt for this ps
Release-2.2.0,notify ps manager
Release-2.2.0,notify the event handler of state change
Release-2.2.0,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
Release-2.2.0,"if forcedState is set, just return"
Release-2.2.0,else get state from state machine
Release-2.2.0,add this worker group to the success set
Release-2.2.0,check if all worker group run or run over
Release-2.2.0,add this worker group to the success set
Release-2.2.0,check if all worker group run over
Release-2.2.0,add this worker group to the failed set
Release-2.2.0,check if too many worker groups are failed or killed
Release-2.2.0,notify a run failed event
Release-2.2.0,add this worker group to the failed set
Release-2.2.0,check if too many worker groups are failed or killed
Release-2.2.0,notify a run failed event
Release-2.2.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
Release-2.2.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
Release-2.2.0,just return the total task number now
Release-2.2.0,TODO
Release-2.2.0,check whether worker heartbeat timeout
Release-2.2.0,"if workerAttempt is not null, we should clone task state from it"
Release-2.2.0,from NEW state
Release-2.2.0,from SCHEDULED state
Release-2.2.0,get data splits location for data locality
Release-2.2.0,reqeuest resource:send a resource request to the resource allocator
Release-2.2.0,"once the resource is applied, build and send the launch request to the container launcher"
Release-2.2.0,notify failed message to the worker
Release-2.2.0,notify killed message to the worker
Release-2.2.0,release the allocated container
Release-2.2.0,notify failed message to the worker
Release-2.2.0,remove the worker attempt from heartbeat timeout listen list
Release-2.2.0,release the allocated container
Release-2.2.0,notify killed message to the worker
Release-2.2.0,remove the worker attempt from heartbeat timeout listen list
Release-2.2.0,clean the container
Release-2.2.0,notify failed message to the worker
Release-2.2.0,remove the worker attempt from heartbeat timeout listen list
Release-2.2.0,record the finish time
Release-2.2.0,clean the container
Release-2.2.0,notify killed message to the worker
Release-2.2.0,remove the worker attempt from heartbeat timeout listening list
Release-2.2.0,record the finish time
Release-2.2.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
Release-2.2.0,set worker attempt location
Release-2.2.0,notify the register message to the worker
Release-2.2.0,record the launch time
Release-2.2.0,update worker attempt metrics
Release-2.2.0,update tasks metrics
Release-2.2.0,clean the container
Release-2.2.0,notify the worker attempt run successfully message to the worker
Release-2.2.0,record the finish time
Release-2.2.0,init a worker attempt for the worker
Release-2.2.0,schedule the worker attempt
Release-2.2.0,add diagnostic
Release-2.2.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.2.0,init and start a new attempt for this ps
Release-2.2.0,notify worker manager
Release-2.2.0,add diagnostic
Release-2.2.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.2.0,init and start a new attempt for this ps
Release-2.2.0,notify worker manager
Release-2.2.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
Release-2.2.0,register to Yarn RM
Release-2.2.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
Release-2.2.0,"catch YarnRuntimeException, we should exit and need not retry"
Release-2.2.0,build heartbeat request
Release-2.2.0,send heartbeat request to rm
Release-2.2.0,"This can happen if the RM has been restarted. If it is in that state,"
Release-2.2.0,this application must clean itself up.
Release-2.2.0,Setting NMTokens
Release-2.2.0,assgin containers
Release-2.2.0,"if some container is not assigned, release them"
Release-2.2.0,handle finish containers
Release-2.2.0,dispatch container exit message to corresponding components
Release-2.2.0,killed by framework
Release-2.2.0,killed by framework
Release-2.2.0,get application finish state
Release-2.2.0,build application diagnostics
Release-2.2.0,TODO:add a job history for angel
Release-2.2.0,build unregister request
Release-2.2.0,send unregister request to rm
Release-2.2.0,Note this down for next interaction with ResourceManager
Release-2.2.0,based on blacklisting comments above we can end up decrementing more
Release-2.2.0,than requested. so guard for that.
Release-2.2.0,send the updated resource request to RM
Release-2.2.0,send 0 container count requests also to cancel previous requests
Release-2.2.0,Update resource requests
Release-2.2.0,try to assign to all nodes first to match node local
Release-2.2.0,try to match all rack local
Release-2.2.0,assign remaining
Release-2.2.0,Update resource requests
Release-2.2.0,send the container-assigned event to task attempt
Release-2.2.0,build the start container request use launch context
Release-2.2.0,send the start request to Yarn nm
Release-2.2.0,send the message that the container starts successfully to the corresponding component
Release-2.2.0,"after launching, send launched event to task attempt to move"
Release-2.2.0,it from ASSIGNED to RUNNING state
Release-2.2.0,send the message that the container starts failed to the corresponding component
Release-2.2.0,kill the remote container if already launched
Release-2.2.0,start a thread pool to startup the container
Release-2.2.0,See if we need up the pool size only if haven't reached the
Release-2.2.0,maximum limit yet.
Release-2.2.0,nodes where containers will run at *this* point of time. This is
Release-2.2.0,*not* the cluster size and doesn't need to be.
Release-2.2.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
Release-2.2.0,later is just a buffer so we are not always increasing the
Release-2.2.0,pool-size
Release-2.2.0,the events from the queue are handled in parallel
Release-2.2.0,using a thread pool
Release-2.2.0,return if already stopped
Release-2.2.0,shutdown any containers that might be left running
Release-2.2.0,Add one sync matrix
Release-2.2.0,addSyncMatrix();
Release-2.2.0,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
Release-2.2.0,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
Release-2.2.0,"LOG.info(""ps id = "" + psEntry.getKey());"
Release-2.2.0,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
Release-2.2.0,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
Release-2.2.0,"LOG.info(""matrix id = "" + metaEntry.getKey());"
Release-2.2.0,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
Release-2.2.0,}
Release-2.2.0,}
Release-2.2.0,get matrix ids in the parameter server report
Release-2.2.0,get the matrices parameter server need to create and delete
Release-2.2.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
Release-2.2.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
Release-2.2.0,Init control connection manager
Release-2.2.0,Get ps locations from master and put them to the location cache.
Release-2.2.0,Build and initialize rpc client to master
Release-2.2.0,Get psagent id
Release-2.2.0,Build PS control rpc client manager
Release-2.2.0,Build local location
Release-2.2.0,Initialize matrix meta information
Release-2.2.0,Start all services
Release-2.2.0,Stop all modules
Release-2.2.0,Stop all modules
Release-2.2.0,clock first
Release-2.2.0,wait
Release-2.2.0,Update generic resource counters
Release-2.2.0,Updating resources specified in ResourceCalculatorProcessTree
Release-2.2.0,Remove the CPU time consumed previously by JVM reuse
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,/ Plus a vector/matrix to the matrix stored in pss
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,/ Update a vector/matrix to the matrix stored in pss
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,/ Get values from pss use row/column indices
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"/ PSF get/update, use can implement their own psf"
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,/ Get a row or a batch of rows
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,Just return
Release-2.2.0,Just return
Release-2.2.0,Return a empty vector
Release-2.2.0,Return a empty vector
Release-2.2.0,Return a empty vector
Release-2.2.0,Return a empty vector
Release-2.2.0,"checkNotNull(func, ""func"");"
Release-2.2.0,Return a empty vector
Release-2.2.0,"checkNotNull(func, ""func"");"
Release-2.2.0,Return a empty vector
Release-2.2.0,"checkNotNull(func, ""func"");"
Release-2.2.0,Return a empty vector
Release-2.2.0,"checkNotNull(func, ""func"");"
Release-2.2.0,Return a empty vector
Release-2.2.0,TODO:
Release-2.2.0,Generate a flush request and put it to request queue
Release-2.2.0,Generate a clock request and put it to request queue
Release-2.2.0,Generate a merge request and put it to request queue
Release-2.2.0,Generate a merge request and put it to request queue
Release-2.2.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
Release-2.2.0,matrix
Release-2.2.0,and add it to cache maps
Release-2.2.0,Add the message to the tree map
Release-2.2.0,"If there are flush / clock requests blocked, we need to put this merge request into"
Release-2.2.0,the waiting queue
Release-2.2.0,Launch a merge worker to merge the update to matrix op log cache
Release-2.2.0,Remove the message from the tree map
Release-2.2.0,Wake up blocked flush/clock request
Release-2.2.0,Add flush/clock request to listener list to waiting for all the existing
Release-2.2.0,updates are merged
Release-2.2.0,Wake up blocked flush/clock request
Release-2.2.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
Release-2.2.0,blocked.
Release-2.2.0,Get next merge message sequence id
Release-2.2.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
Release-2.2.0,position
Release-2.2.0,Wake up blocked merge requests
Release-2.2.0,Get minimal sequence id from listeners
Release-2.2.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
Release-2.2.0,should flush updates to local matrix storage
Release-2.2.0,unused now
Release-2.2.0,TODO:
Release-2.2.0,Doing average or not
Release-2.2.0,Filter un-important update
Release-2.2.0,Split this row according the matrix partitions
Release-2.2.0,Set split context
Release-2.2.0,Remove the row from matrix
Release-2.2.0,buf.writeDouble(0.0);
Release-2.2.0,TODO
Release-2.2.0,TODO: write map default value
Release-2.2.0,buf.writeDouble(0);
Release-2.2.0,TODO:
Release-2.2.0,TODO:
Release-2.2.0,TODO:
Release-2.2.0,"For each partition, we generate a update split."
Release-2.2.0,"Although the split is empty for partitions those without any update data,"
Release-2.2.0,we still need to generate a update split to update the clock info on ps.
Release-2.2.0,"For each partition, we generate a update split."
Release-2.2.0,"Although the split is empty for partitions those without any update data,"
Release-2.2.0,we still need to generate a update split to update the clock info on ps.
Release-2.2.0,"For each partition, we generate a update split."
Release-2.2.0,"Although the split is empty for partitions those without any update data,"
Release-2.2.0,we still need to generate a update split to update the clock info on ps.
Release-2.2.0,"For each partition, we generate a update split."
Release-2.2.0,"Although the split is empty for partitions those without any update data,"
Release-2.2.0,we still need to generate a update split to update the clock info on ps.
Release-2.2.0,"For each partition, we generate a update split."
Release-2.2.0,"Although the split is empty for partitions those without any update data,"
Release-2.2.0,we still need to generate a update split to update the clock info on ps.
Release-2.2.0,"For each partition, we generate a update split."
Release-2.2.0,"Although the split is empty for partitions those without any update data,"
Release-2.2.0,we still need to generate a update split to update the clock info on ps.
Release-2.2.0,"LOG.error(""channel "" + ctx.channel() + "" inactive"");"
Release-2.2.0,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
Release-2.2.0,"LOG.error(""put response message queue failed "", e);"
Release-2.2.0,Use Epoll for linux
Release-2.2.0,Update location table
Release-2.2.0,Remove the server from failed list
Release-2.2.0,Notify refresh success message to request dispatcher
Release-2.2.0,Check PS exist or not
Release-2.2.0,Check heartbeat timeout
Release-2.2.0,Check PS restart or not
Release-2.2.0,private final HashSet<ParameterServerId> refreshingServerSet;
Release-2.2.0,Add it to failed rpc list
Release-2.2.0,Add the server to gray server list
Release-2.2.0,Add it to failed rpc list
Release-2.2.0,Add the server to gray server list
Release-2.2.0,Move from gray server list to failed server list
Release-2.2.0,Handle the RPCS to this server
Release-2.2.0,Submit the schedulable failed get RPCS
Release-2.2.0,Submit new get RPCS
Release-2.2.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.2.0,"If the queue is empty, just return 0"
Release-2.2.0,"If request is not over limit, just submit it"
Release-2.2.0,Submit the schedulable failed get RPCS
Release-2.2.0,Submit new put RPCS
Release-2.2.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.2.0,"LOG.info(""choose put server "" + psIds[index]);"
Release-2.2.0,Check all pending RPCS
Release-2.2.0,Check get channel context
Release-2.2.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.2.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.2.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.2.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.2.0,channelManager.printPools();
Release-2.2.0,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
Release-2.2.0,if(ts - entry.getValue() > requestTimeOut * 2)  {
Release-2.2.0,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
Release-2.2.0,"+ "" milliseconds, close all channels to it"");"
Release-2.2.0,closeChannels(entry.getKey());
Release-2.2.0,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
Release-2.2.0,}
Release-2.2.0,}
Release-2.2.0,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
Release-2.2.0,Remove all pending RPCS
Release-2.2.0,Close all channel to this PS
Release-2.2.0,private Channel getChannel(Location loc) throws Exception {
Release-2.2.0,"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));"
Release-2.2.0,}
Release-2.2.0,private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {
Release-2.2.0,"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext"
Release-2.2.0,.get()
Release-2.2.0,.getConf()
Release-2.2.0,".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,"
Release-2.2.0,AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));
Release-2.2.0,}
Release-2.2.0,"LOG.error(""send request "" + request + "" is interrupted"");"
Release-2.2.0,"LOG.error(""send request "" + request + "" failed, "", e);"
Release-2.2.0,Get server id and location for this request
Release-2.2.0,"If location is null, means that the server is not ready"
Release-2.2.0,Get the channel for the location
Release-2.2.0,Check if need get token first
Release-2.2.0,Serialize the request
Release-2.2.0,Send the request
Release-2.2.0,get a channel to server from pool
Release-2.2.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.2.0,request.getContext().setChannelPool(pool);
Release-2.2.0,Allocate the bytebuf and serialize the request
Release-2.2.0,find the partition request context from cache
Release-2.2.0,"updateMatrixCache(request.getPartKey(), response.getPartition());"
Release-2.2.0,"updateMatrixCache(request.getPartKey(), response.getRowsSplit());"
Release-2.2.0,TODO
Release-2.2.0,ServerRow rowSplit = PSAgentContext.get().getMatricesCache()
Release-2.2.0,".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),"
Release-2.2.0,request.getRowIndex());
Release-2.2.0,response.setRowSplit(rowSplit);
Release-2.2.0,"updateMatrixCache(request.getPartKey(), response.getRowSplit());"
Release-2.2.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.2.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.2.0,TODO
Release-2.2.0,"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {"
Release-2.2.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);"
Release-2.2.0,}
Release-2.2.0,"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {"
Release-2.2.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);"
Release-2.2.0,}
Release-2.2.0,"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {"
Release-2.2.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);"
Release-2.2.0,}
Release-2.2.0,Get partitions for this row
Release-2.2.0,Distinct get row requests
Release-2.2.0,Need get from ps or storage/cache
Release-2.2.0,"Switch to new request id, send a new request"
Release-2.2.0,First get this row from matrix storage
Release-2.2.0,MatrixStorage matrixStorage =
Release-2.2.0,PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);
Release-2.2.0,TVector row = matrixStorage.getRow(rowIndex);
Release-2.2.0,if (row != null && row.getClock() >= clock) {
Release-2.2.0,result.set(row);
Release-2.2.0,return row;
Release-2.2.0,}
Release-2.2.0,Get row splits of this row from the matrix cache first
Release-2.2.0,responseCache.addSubResponse(rowSplit);
Release-2.2.0,"If the row split does not exist in cache, get it from parameter server"
Release-2.2.0,Wait the final result
Release-2.2.0,Put it to the matrix cache
Release-2.2.0,"matrixStorage.addRow(rowIndex, row);"
Release-2.2.0,Just wait result
Release-2.2.0,Split the param use matrix partitions
Release-2.2.0,Send request to PSS
Release-2.2.0,Split the matrix oplog according to the matrix partitions
Release-2.2.0,"If need update clock, we should send requests to all partitions"
Release-2.2.0,Send request to PSS
Release-2.2.0,Filter the rowIds which are fetching now
Release-2.2.0,Send the rowIndex to rpc dispatcher and return immediately
Release-2.2.0,"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));"
Release-2.2.0,"LOG.info(""start to request "" + requestId);"
Release-2.2.0,"LOG.info(""start to request "" + requestId);"
Release-2.2.0,Split param use matrix partitons
Release-2.2.0,"If all sub-results are received, just remove request and result cache"
Release-2.2.0,"LOG.info(""request = "" + request + "", cache = "" + cache);"
Release-2.2.0,"LOG.info(""start to merge "" + cache + "" for request "" + request);"
Release-2.2.0,"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.2.0,Split this row according the matrix partitions
Release-2.2.0,Set split context
Release-2.2.0,Split this row according the matrix partitions
Release-2.2.0,Set split context
Release-2.2.0,long startTs = System.currentTimeMillis();
Release-2.2.0,"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.2.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
Release-2.2.0,Generate dispatch items and add them to the corresponding queues
Release-2.2.0,Filter the rowIds which are fetching now
Release-2.2.0,Sort the parts by partitionId
Release-2.2.0,Sort partition keys use start column index
Release-2.2.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.2.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.2.0,});
Release-2.2.0,"For each partition, we generate a update split."
Release-2.2.0,"Although the split is empty for partitions those without any update data,"
Release-2.2.0,we still need to generate a update split to update the clock info on ps.
Release-2.2.0,Sort the parts by partitionId
Release-2.2.0,Sort partition keys use start column index
Release-2.2.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.2.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.2.0,});
Release-2.2.0,"For each partition, we generate a update split."
Release-2.2.0,"Although the split is empty for partitions those without any update data,"
Release-2.2.0,we still need to generate a update split to update the clock info on ps.
Release-2.2.0,Put the row split to the cache(row index to row splits map)
Release-2.2.0,"If all splits of the row are received, means this row can be merged"
Release-2.2.0,TODO
Release-2.2.0,TODO
Release-2.2.0,/////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,TODO
Release-2.2.0,buf.writeDouble(0);
Release-2.2.0,TODO
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,Now we just support pipelined row splits merging for dense type row
Release-2.2.0,Pre-fetching is disable default
Release-2.2.0,matrix id to clock map
Release-2.2.0,"task index, it must be unique for whole application"
Release-2.2.0,Deserialize data splits meta
Release-2.2.0,Get workers
Release-2.2.0,Send request to every ps
Release-2.2.0,Wait the responses
Release-2.2.0,Update clock cache
Release-2.2.0,if(syncNum % 1024 == 0) {
Release-2.2.0,}
Release-2.2.0,"Use simple flow, do not use any cache"
Release-2.2.0,Get row from cache.
Release-2.2.0,"if row clock is satisfy ssp staleness limit, just return."
Release-2.2.0,Get row from ps.
Release-2.2.0,Wait until the clock value of this row is greater than or equal to the value
Release-2.2.0,"For ASYNC mode, just get from pss."
Release-2.2.0,"For BSP/SSP, get rows from storage/cache first"
Release-2.2.0,Get from ps.
Release-2.2.0,Wait until the clock value of this row is greater than or equal to the value
Release-2.2.0,"For ASYNC, just get rows from pss."
Release-2.2.0,no more retries.
Release-2.2.0,calculate sleep time and return.
Release-2.2.0,parse the i-th sleep-time
Release-2.2.0,parse the i-th number-of-retries
Release-2.2.0,calculateSleepTime may overflow.
Release-2.2.0,"A few common retry policies, with no delays."
Release-2.2.0,Read matrix meta from meta file
Release-2.2.0,Save partitions to files use fork-join
Release-2.2.0,Write the ps matrix meta to the meta file
Release-2.2.0,matrix.startServering();
Release-2.2.0,return;
Release-2.2.0,Read matrix meta from meta file
Release-2.2.0,Load partitions from file use fork-join
Release-2.2.0,Read matrix meta from meta file
Release-2.2.0,Sort partitions
Release-2.2.0,int size = rows.length;
Release-2.2.0,int size = rows.length;
Release-2.2.0,int size = rows.size();
Release-2.2.0,int size = rows.size();
Release-2.2.0,int size = rows.size();
Release-2.2.0,int size = rows.size();
Release-2.2.0,int size = rows.size();
Release-2.2.0,int size = rows.size();
Release-2.2.0,close is a local operation and should finish within milliseconds; timeout just to be safe
Release-2.2.0,response will be null for one way messages.
Release-2.2.0,maxFrameLength = 2G
Release-2.2.0,lengthFieldOffset = 0
Release-2.2.0,lengthFieldLength = 8
Release-2.2.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
Release-2.2.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
Release-2.2.0,"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())"
Release-2.2.0,.toString();
Release-2.2.0,indicates whether this connection's life cycle is managed
Release-2.2.0,See if we already have a connection (common case)
Release-2.2.0,create a unique lock for this RS + protocol (if necessary)
Release-2.2.0,get the RS lock
Release-2.2.0,do one more lookup in case we were stalled above
Release-2.2.0,Only create isa when we need to.
Release-2.2.0,definitely a cache miss. establish an RPC for
Release-2.2.0,this RS
Release-2.2.0,Throw what the RemoteException was carrying.
Release-2.2.0,check
Release-2.2.0,every
Release-2.2.0,minutes
Release-2.2.0,TODO
Release-2.2.0,创建failoverHandler
Release-2.2.0,"The number of times this invocation handler has ever been failed over,"
Release-2.2.0,before this method invocation attempt. Used to prevent concurrent
Release-2.2.0,failed method invocations from triggering multiple failover attempts.
Release-2.2.0,Make sure that concurrent failed method invocations
Release-2.2.0,only cause a
Release-2.2.0,single actual fail over.
Release-2.2.0,RpcController + Message in the method args
Release-2.2.0,(generated code from RPC bits in .proto files have
Release-2.2.0,RpcController)
Release-2.2.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
Release-2.2.0,+ (System.currentTimeMillis() - beforeConstructTs));
Release-2.2.0,get an instance of the method arg type
Release-2.2.0,RpcController + Message in the method args
Release-2.2.0,(generated code from RPC bits in .proto files have
Release-2.2.0,RpcController)
Release-2.2.0,Message (hand written code usually has only a single
Release-2.2.0,argument)
Release-2.2.0,log any RPC responses that are slower than the configured
Release-2.2.0,warn
Release-2.2.0,response time or larger than configured warning size
Release-2.2.0,"when tagging, we let TooLarge trump TooSmall to keep"
Release-2.2.0,output simple
Release-2.2.0,note that large responses will often also be slow.
Release-2.2.0,provides a count of log-reported slow responses
Release-2.2.0,RpcController + Message in the method args
Release-2.2.0,(generated code from RPC bits in .proto files have
Release-2.2.0,RpcController)
Release-2.2.0,unexpected
Release-2.2.0,"in the protobuf methods, args[1] is the only significant argument"
Release-2.2.0,for JSON encoding
Release-2.2.0,base information that is reported regardless of type of call
Release-2.2.0,Disable Nagle's Algorithm since we don't want packets to wait
Release-2.2.0,Configure the event pipeline factory.
Release-2.2.0,Make a new connection.
Release-2.2.0,Remove all pending requests (will be canceled after relinquishing
Release-2.2.0,write lock).
Release-2.2.0,Cancel any pending requests by sending errors to the callbacks:
Release-2.2.0,Close the channel:
Release-2.2.0,Close the connection:
Release-2.2.0,Shut down all thread pools to exit.
Release-2.2.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
Release-2.2.0,See NettyServer.prepareResponse for where we write out the response.
Release-2.2.0,"It writes the call.id (int), a boolean signifying any error (and if"
Release-2.2.0,"so the exception name/trace), and the response bytes"
Release-2.2.0,Read the call id.
Release-2.2.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
Release-2.2.0,"instead, it returns a null message object."
Release-2.2.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
Release-2.2.0,System.currentTimeMillis());
Release-2.2.0,"It would be good widen this to just Throwable, but IOException is what we"
Release-2.2.0,allow now
Release-2.2.0,not implemented
Release-2.2.0,not implemented
Release-2.2.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
Release-2.2.0,cache of RpcEngines by protocol
Release-2.2.0,return the RpcEngine configured to handle a protocol
Release-2.2.0,We only handle the ConnectException.
Release-2.2.0,This is the exception we can't handle.
Release-2.2.0,check if timed out
Release-2.2.0,wait for retry
Release-2.2.0,IGNORE
Release-2.2.0,return the RpcEngine that handles a proxy object
Release-2.2.0,The default implementation works synchronously
Release-2.2.0,punt: allocate a new buffer & copy into it
Release-2.2.0,Parse cmd parameters
Release-2.2.0,load hadoop configuration
Release-2.2.0,load angel system configuration
Release-2.2.0,load user configuration:
Release-2.2.0,load user config file
Release-2.2.0,load command line parameters
Release-2.2.0,load user job resource files
Release-2.2.0,load ml conf file for graph based algorithm
Release-2.2.0,load user job jar if it exist
Release-2.2.0,Expand the environment variable
Release-2.2.0,Add default fs(local fs) for lib jars.
Release-2.2.0,"LOG.info(System.getProperty(""user.dir""));"
Release-2.2.0,get tokens for all the required FileSystems..
Release-2.2.0,Whether we need to recursive look into the directory structure
Release-2.2.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.2.0,user provided one (if any).
Release-2.2.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.2.0,get tokens for all the required FileSystems..
Release-2.2.0,Whether we need to recursive look into the directory structure
Release-2.2.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.2.0,user provided one (if any).
Release-2.2.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.2.0,a simple hdfs copy function assume src path and dest path are in same hdfs
Release-2.2.0,and FileSystem object has same schema
Release-2.2.0,"If out path exist , just remove it first"
Release-2.2.0,Create parent directory if not exist
Release-2.2.0,Rename
Release-2.2.0,"LOG.warn(""interrupted while sleeping"", ie);"
Release-2.2.0,public static String getHostname() {
Release-2.2.0,try {
Release-2.2.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
Release-2.2.0,} catch (UnknownHostException uhe) {
Release-2.2.0,}
Release-2.2.0,"return new StringBuilder().append("""").append(uhe).toString();"
Release-2.2.0,}
Release-2.2.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
Release-2.2.0,String hostname = getHostname();
Release-2.2.0,String classname = clazz.getSimpleName();
Release-2.2.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
Release-2.2.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
Release-2.2.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
Release-2.2.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
Release-2.2.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
Release-2.2.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
Release-2.2.0,
Release-2.2.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
Release-2.2.0,public void run() {
Release-2.2.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
Release-2.2.0,"this.val$classname + "" at "" + this.val$hostname}));"
Release-2.2.0,}
Release-2.2.0,});
Release-2.2.0,}
Release-2.2.0,"We we interrupted because we're meant to stop? If not, just"
Release-2.2.0,continue ignoring the interruption
Release-2.2.0,Recalculate waitTime.
Release-2.2.0,// Begin delegation to Thread
Release-2.2.0,// End delegation to Thread
Release-2.2.0,instance submitter class
Release-2.2.0,Obtain filename from path
Release-2.2.0,Split filename to prexif and suffix (extension)
Release-2.2.0,Check if the filename is okay
Release-2.2.0,Prepare temporary file
Release-2.2.0,Prepare buffer for data copying
Release-2.2.0,Open and check input stream
Release-2.2.0,Open output stream and copy data between source file in JAR and the temporary file
Release-2.2.0,"If read/write fails, close streams safely before throwing an exception"
Release-2.2.0,"Finally, load the library"
Release-2.2.0,little endian load order
Release-2.2.0,tail
Release-2.2.0,fallthrough
Release-2.2.0,fallthrough
Release-2.2.0,finalization
Release-2.2.0,fmix(h1);
Release-2.2.0,----------
Release-2.2.0,body
Release-2.2.0,----------
Release-2.2.0,tail
Release-2.2.0,----------
Release-2.2.0,finalization
Release-2.2.0,----------
Release-2.2.0,body
Release-2.2.0,----------
Release-2.2.0,tail
Release-2.2.0,----------
Release-2.2.0,finalization
Release-2.2.0,throw new AngelException(e);
Release-2.2.0,JobStateProto jobState = report.getJobState();
Release-2.2.0,Check need load matrices
Release-2.2.0,Used for java code to get a AngelClient instance
Release-2.2.0,Used for python code to get a AngelClient instance
Release-2.2.0,load user job resource files
Release-2.2.0,the leaf level file should be readable by others
Release-2.2.0,the subdirs in the path should have execute permissions for
Release-2.2.0,others
Release-2.2.0,2.get job id
Release-2.2.0,Credentials credentials = new Credentials();
Release-2.2.0,4.copy resource files to hdfs
Release-2.2.0,5.write configuration to a xml file
Release-2.2.0,6.create am container context
Release-2.2.0,7.Submit to ResourceManager
Release-2.2.0,8.get app master client
Release-2.2.0,Create a number of filenames in the JobTracker's fs namespace
Release-2.2.0,add all the command line files/ jars and archive
Release-2.2.0,first copy them to jobtrackers filesystem
Release-2.2.0,should not throw a uri exception
Release-2.2.0,should not throw an uri excpetion
Release-2.2.0,set the timestamps of the archives and files
Release-2.2.0,set the public/private visibility of the archives and files
Release-2.2.0,get DelegationToken for each cached file
Release-2.2.0,check if we do not need to copy the files
Release-2.2.0,is jt using the same file system.
Release-2.2.0,just checking for uri strings... doing no dns lookups
Release-2.2.0,to see if the filesystems are the same. This is not optimal.
Release-2.2.0,but avoids name resolution.
Release-2.2.0,this might have name collisions. copy will throw an exception
Release-2.2.0,parse the original path to create new path
Release-2.2.0,check for ports
Release-2.2.0,Write job file to JobTracker's fs
Release-2.2.0,Setup resource requirements
Release-2.2.0,Setup LocalResources
Release-2.2.0,Setup security tokens
Release-2.2.0,Setup the command to run the AM
Release-2.2.0,Add AM user command opts
Release-2.2.0,Final command
Release-2.2.0,Setup the CLASSPATH in environment
Release-2.2.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
Release-2.2.0,Setup the environment variables for Admin first
Release-2.2.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
Release-2.2.0,Parse distributed cache
Release-2.2.0,Setup ContainerLaunchContext for AM container
Release-2.2.0,Set up the ApplicationSubmissionContext
Release-2.2.0,private volatile PS2PSPusherImpl ps2PSPusher;
Release-2.2.0,TODO
Release-2.2.0,Add tokens to new user so that it may execute its task correctly.
Release-2.2.0,TODO
Release-2.2.0,to exit
Release-2.2.0,TODO
Release-2.2.0,TODO
Release-2.2.0,ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));
Release-2.2.0,context.getSnapshotManager().processRecovery();
Release-2.2.0,Recover PS from snapshot or load path
Release-2.2.0,First check snapshot
Release-2.2.0,Check load path setting
Release-2.2.0,TODO
Release-2.2.0,if(ps2PSPusher != null) {
Release-2.2.0,ps2PSPusher.start();
Release-2.2.0,}
Release-2.2.0,public PS2PSPusherImpl getPs2PSPusher() {
Release-2.2.0,return ps2PSPusher;
Release-2.2.0,}
Release-2.2.0,"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");"
Release-2.2.0,"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);"
Release-2.2.0,Release the input buffer
Release-2.2.0,Release the input buffer
Release-2.2.0,"1. handle the rpc, get the response"
Release-2.2.0,Release the input buffer
Release-2.2.0,2. Serialize the response
Release-2.2.0,Send the serialized response
Release-2.2.0,Exception happened
Release-2.2.0,write seq id
Release-2.2.0,Just serialize the head
Release-2.2.0,Exception happened
Release-2.2.0,Allocate result buffer
Release-2.2.0,Exception happened
Release-2.2.0,Just serialize the head
Release-2.2.0,Exception happened
Release-2.2.0,Reset the response and allocate buffer again
Release-2.2.0,Get partition and check the partition state
Release-2.2.0,Get the stored pss for this partition
Release-2.2.0,"Check this ps is the master ps for this location, only master ps can accept the update"
Release-2.2.0,Check the partition state again
Release-2.2.0,Start to put the update to the slave pss
Release-2.2.0,TODO
Release-2.2.0,"context.getPS2PSPusher().put(request, in, partLoc);"
Release-2.2.0,Get partition and check the partition state
Release-2.2.0,Get the stored pss for this partition
Release-2.2.0,"Check this ps is the master ps for this partition, if not, just return failed"
Release-2.2.0,Start to put the update to the slave pss
Release-2.2.0,TODO
Release-2.2.0,return ServerState.GENERAL;
Release-2.2.0,Use Epoll for linux
Release-2.2.0,public String uuid;
Release-2.2.0,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
Release-2.2.0,this.channelPool = channelPool;
Release-2.2.0,}
Release-2.2.0,private final ParameterServer psServer;
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO:default value
Release-2.2.0,buf.readDouble();
Release-2.2.0,TODO
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"////// network io method, for model transform"
Release-2.2.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,public ObjectIterator<Long2FloatMap.Entry> getIter() {
Release-2.2.0,return ((LongFloatVector) row).getStorage().entryIterator();
Release-2.2.0,}
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,public ObjectIterator<Long2LongMap.Entry> getIter() {
Release-2.2.0,return ((LongLongVector) row).getStorage().entryIterator();
Release-2.2.0,}
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.2.0,and call endWrite/endRead after
Release-2.2.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.2.0,TODO: dynamic add/delete row
Release-2.2.0,private final List<PartitionKey> partitionKeys;
Release-2.2.0,"if col == -1, we use the start/end index to calculate range,"
Release-2.2.0,we use double to store the range value since two long minus might exceed the
Release-2.2.0,range of long.
Release-2.2.0,Use Epoll for linux
Release-2.2.0,find the partition request context from cache
Release-2.2.0,get a channel to server from pool
Release-2.2.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.2.0,channelManager.removeChannelPool(loc);
Release-2.2.0,Generate seq id
Release-2.2.0,Create a RecoverPartRequest
Release-2.2.0,Serialize the request
Release-2.2.0,Change the seqId for the request
Release-2.2.0,Serialize the request
Release-2.2.0,"First check the state of the channels in the pool, if a channel is unused, just return"
Release-2.2.0,"If all channels are in use, create a new channel or wait"
Release-2.2.0,Create a new channel
Release-2.2.0,"add the PSAgentContext,need fix"
Release-2.2.0,If col == -1 and start/end not set
Release-2.2.0,start/end set
Release-2.2.0,"for dense type, we need to set the colNum to set dim for vectors"
Release-2.2.0,"colNum set, start/end not set"
Release-2.2.0,Row number must > 0
Release-2.2.0,"both set, check its valid"
Release-2.2.0,TODO:add more vector type
Release-2.2.0,TODO : subDim set
Release-2.2.0,Sort the parts by partitionId
Release-2.2.0,Sort partition keys use start column index
Release-2.2.0,"For each partition, we generate a update split."
Release-2.2.0,"Although the split is empty for partitions those without any update data,"
Release-2.2.0,we still need to generate a update split to update the clock info on ps.
Release-2.2.0,Sort the parts by partitionId
Release-2.2.0,Sort partition keys use start column index
Release-2.2.0,"For each partition, we generate a update split."
Release-2.2.0,"Although the split is empty for partitions those without any update data,"
Release-2.2.0,we still need to generate a update split to update the clock info on ps.
Release-2.2.0,write the max abs
Release-2.2.0,---------------------------------------------------
Release-2.2.0,---------------------------------------------------
Release-2.2.0,---------------------------------------------------------------
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,axis = 0: on rows
Release-2.2.0,axis = 1: on cols
Release-2.2.0,1. find the insert point
Release-2.2.0,2. check the capacity and insert
Release-2.2.0,3. increase size
Release-2.2.0,1. find the insert point
Release-2.2.0,2. check the capacity and insert
Release-2.2.0,3. increase size
Release-2.2.0,1. find the insert point
Release-2.2.0,2. check the capacity and insert
Release-2.2.0,3. increase size
Release-2.2.0,1. find the insert point
Release-2.2.0,2. check the capacity and insert
Release-2.2.0,3. increase size
Release-2.2.0,1. find the insert point
Release-2.2.0,2. check the capacity and insert
Release-2.2.0,3. increase size
Release-2.2.0,1. find the insert point
Release-2.2.0,2. check the capacity and insert
Release-2.2.0,3. increase size
Release-2.2.0,1. find the insert point
Release-2.2.0,2. check the capacity and insert
Release-2.2.0,3. increase size
Release-2.2.0,1. find the insert point
Release-2.2.0,2. check the capacity and insert
Release-2.2.0,3. increase size
Release-2.2.0,-----------------
Release-2.2.0,-----------------
Release-2.2.0,-----------------
Release-2.2.0,-----------------
Release-2.2.0,-----------------
Release-2.2.0,KeepStorage is guaranteed
Release-2.2.0,"ignore the isInplace option, since v2 is dense"
Release-2.2.0,"the value in old storage can be changed safe, so switch a storage"
Release-2.2.0,"but user required keep storage, we can prevent rehash"
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,KeepStorage is guaranteed
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,v1Size < v2Size * Constant.sparseThreshold
Release-2.2.0,KeepStorage is guaranteed
Release-2.2.0,"ignore the isInplace option, since v2 is dense"
Release-2.2.0,"the value in old storage can be changed safe, so switch a storage"
Release-2.2.0,"but user required keep storage, we can prevent rehash"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,prevent rehash
Release-2.2.0,KeepStorage is guaranteed
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,dense preferred
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,sorted preferred
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,multi-rehash
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"slower but memory efficient, for small vector only"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"slower but memory efficient, for small vector only"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"slower but memory efficient, for small vector only"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"slower but memory efficient, for small vector only"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"slower but memory efficient, for small vector only"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"slower but memory efficient, for small vector only"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"slower but memory efficient, for small vector only"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"slower but memory efficient, for small vector only"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"slower but memory efficient, for small vector only"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"slower but memory efficient, for small vector only"
Release-2.2.0,"dense preferred, KeepStorage is guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,"sparse preferred, keep storage guaranteed"
Release-2.2.0,preferred dense
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,we gauss dense storage is more efficient
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.2.0,multi-rehash
Release-2.2.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,to avoid multi-rehash
Release-2.2.0,"Transform mat1, generate a new matrix"
Release-2.2.0,Split the row indices of mat1Trans
Release-2.2.0,Parallel execute use fork-join
Release-2.2.0,"Get the sub-matrix of left matrix, split by row"
Release-2.2.0,"Transform mat1, generate a new matrix"
Release-2.2.0,Split the row indices of mat1Trans
Release-2.2.0,Parallel execute use fork-join
Release-2.2.0,"Get the sub-matrix of left matrix, split by row"
Release-2.2.0,"mat1 trans true, mat trans true"
Release-2.2.0,"mat1 trans true, mat trans false"
Release-2.2.0,"mat1 trans false, mat trans true, important"
Release-2.2.0,"mat1 trans false, mat trans false"
Release-2.2.0,"mat1 trans true, mat trans true"
Release-2.2.0,"mat1 trans true, mat trans false"
Release-2.2.0,"mat1 trans false, mat trans true, important"
Release-2.2.0,"mat1 trans false, mat trans false"
Release-2.2.0,"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.2.0,"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.2.0,not the first time
Release-2.2.0,first time and do the sample
Release-2.2.0,set to zero
Release-2.2.0,not the first time
Release-2.2.0,first time and do the sample
Release-2.2.0,set to zero
Release-2.2.0,not the first time
Release-2.2.0,first time and do the sample
Release-2.2.0,set to zero
Release-2.2.0,not the first time
Release-2.2.0,first time and do the sample
Release-2.2.0,set to zero
Release-2.2.0,not the first time
Release-2.2.0,first time and do the sample
Release-2.2.0,set to zero
Release-2.2.0,not the first time
Release-2.2.0,first time and do the sample
Release-2.2.0,set to zero
Release-2.2.0,not the first time
Release-2.2.0,first time and do the sample
Release-2.2.0,set to zero
Release-2.2.0,not the first time
Release-2.2.0,first time and do the sample
Release-2.2.0,set to zero
Release-2.2.0,not the first time
Release-2.2.0,first time and do the sample
Release-2.2.0,set to zero
Release-2.2.0,not the first time
Release-2.2.0,first time and do the sample
Release-2.2.0,set to zero
Release-2.2.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.2.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.2.0,add dense double matrix
Release-2.2.0,TODO Auto-generated method stub
Release-2.2.0,TODO Auto-generated method stub
Release-2.2.0,TODO Auto-generated method stub
Release-2.2.0,get configuration from config file
Release-2.2.0,set localDir with enviroment set by nm.
Release-2.2.0,get master location
Release-2.2.0,init task manager and start tasks
Release-2.2.0,start heartbeat thread
Release-2.2.0,taskManager.assignTaskIds(response.getTaskidsList());
Release-2.2.0,todo
Release-2.2.0,"if worker timeout, it may be knocked off."
Release-2.2.0,"SUCCESS, do nothing"
Release-2.2.0,heartbeatFailedTime = 0;
Release-2.2.0,private KEY currentKey;
Release-2.2.0,will be created
Release-2.2.0,TODO Auto-generated method stub
Release-2.2.0,Bitmap bitmap = new Bitmap();
Release-2.2.0,int max = indexArray[size - 1];
Release-2.2.0,byte [] bitIndexArray = new byte[max / 8 + 1];
Release-2.2.0,for(int i = 0; i < size; i++){
Release-2.2.0,int bitIndex = indexArray[i] >> 3;
Release-2.2.0,int bitOffset = indexArray[i] - (bitIndex << 3);
Release-2.2.0,switch(bitOffset){
Release-2.2.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
Release-2.2.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
Release-2.2.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
Release-2.2.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
Release-2.2.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
Release-2.2.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
Release-2.2.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
Release-2.2.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
Release-2.2.0,}
Release-2.2.0,}
Release-2.2.0,"true, false"
Release-2.2.0,//////////////////////////////
Release-2.2.0,Application Configs
Release-2.2.0,//////////////////////////////
Release-2.2.0,//////////////////////////////
Release-2.2.0,Master Configs
Release-2.2.0,//////////////////////////////
Release-2.2.0,//////////////////////////////
Release-2.2.0,Worker Configs
Release-2.2.0,//////////////////////////////
Release-2.2.0,//////////////////////////////
Release-2.2.0,Task Configs
Release-2.2.0,//////////////////////////////
Release-2.2.0,//////////////////////////////
Release-2.2.0,ParameterServer Configs
Release-2.2.0,//////////////////////////////
Release-2.2.0,////////////////// IPC //////////////////////////
Release-2.2.0,//////////////////////////////
Release-2.2.0,Matrix transfer Configs.
Release-2.2.0,//////////////////////////////
Release-2.2.0,//////////////////////////////
Release-2.2.0,Matrix transfer Configs.
Release-2.2.0,//////////////////////////////
Release-2.2.0,Configs used to ANGEL_PS_PSAGENT running mode future.
Release-2.2.0,model parse
Release-2.2.0,Mark whether use pyangel or not.
Release-2.2.0,private Configuration conf;
Release-2.2.0,"Configuration that should be used in python environment, there should only be one"
Release-2.2.0,configuration instance in each Angel context.
Release-2.2.0,Use private access means jconf should not be changed or modified in this way.
Release-2.2.0,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
Release-2.2.0,Do nothing
Release-2.2.0,To-DO: add other ways to justify different value types
Release-2.2.0,"This is so ugly, must re-implement by more elegance way"
Release-2.2.0,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
Release-2.2.0,and other files submitted by user.
Release-2.2.0,Launch python process
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Data format
Release-2.2.0,Feature number of train data
Release-2.2.0,Tree number
Release-2.2.0,Tree depth
Release-2.2.0,Split number
Release-2.2.0,Feature sample ratio
Release-2.2.0,Ratio of validation
Release-2.2.0,Learning rate
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Set data format
Release-2.2.0,"Set angel resource, #worker, #task, #PS"
Release-2.2.0,Set GBDT algorithm parameters
Release-2.2.0,Set training data path
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set predict data path
Release-2.2.0,Set load model path
Release-2.2.0,Set predict result path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Train batch number per epoch.
Release-2.2.0,Batch number
Release-2.2.0,Model type
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Set data format
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,set sgd LR algorithm parameters #feature #epoch
Release-2.2.0,Set trainning data path
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Train batch number per epoch.
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Set data format
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,set sgd FM algorithm parameters #feature #epoch
Release-2.2.0,Set trainning data path
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,Model type
Release-2.2.0,String modelType = String.valueOf(RowType.T_FLOAT_DENSE);
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,set sgd LR algorithm parameters #feature #epoch
Release-2.2.0,"conf.set(MLConf.ML_MODEL_TYPE(), modelType);"
Release-2.2.0,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
Release-2.2.0,predictTest();
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Set data format
Release-2.2.0,Set trainning data path
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set data format
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,class number
Release-2.2.0,Model type
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Set data format
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,set sgd LR algorithm parameters #feature #epoch
Release-2.2.0,Set log path
Release-2.2.0,Set trainning data path
Release-2.2.0,Set save model path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set save model path
Release-2.2.0,Set actionType incremental train
Release-2.2.0,Set log path
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set predict result path
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set training data path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Cluster center number
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Sample ratio per mini-batch
Release-2.2.0,C
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.2.0,Set data format
Release-2.2.0,Set trainning data path
Release-2.2.0,Set save model path
Release-2.2.0,Set log save path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set save model path
Release-2.2.0,Set actionType incremental train
Release-2.2.0,Set log path
Release-2.2.0,Set testing data path
Release-2.2.0,Set load model path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Model type
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Set data format
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,set sgd LR algorithm parameters #feature #epoch
Release-2.2.0,Set trainning data path
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set predict result path
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Data is classification
Release-2.2.0,Model is classification
Release-2.2.0,Train batch number per epoch.
Release-2.2.0,loss delta
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Set data format
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,set sgd LR algorithm parameters #feature #epoch
Release-2.2.0,Set trainning data path
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set save model path
Release-2.2.0,Set actionType incremental train
Release-2.2.0,Set log path
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,Feature number of train data
Release-2.2.0,Total iteration number
Release-2.2.0,Validation sample Ratio
Release-2.2.0,"Data format, libsvm or dummy"
Release-2.2.0,Data is classification
Release-2.2.0,Model is classification
Release-2.2.0,Train batch number per epoch.
Release-2.2.0,Learning rate
Release-2.2.0,Decay of learning rate
Release-2.2.0,Regularization coefficient
Release-2.2.0,Set local deploy mode
Release-2.2.0,Set basic configuration keys
Release-2.2.0,Set data format
Release-2.2.0,"set angel resource parameters #worker, #task, #PS"
Release-2.2.0,set sgd LR algorithm parameters #feature #epoch
Release-2.2.0,Set trainning data path
Release-2.2.0,Set save model path
Release-2.2.0,Set log path
Release-2.2.0,Set actionType train
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set save model path
Release-2.2.0,Set actionType incremental train
Release-2.2.0,Set log path
Release-2.2.0,Set trainning data path
Release-2.2.0,Set load model path
Release-2.2.0,Set predict result path
Release-2.2.0,Set actionType prediction
Release-2.2.0,TODO: optimize int key indices
Release-2.2.0,"System.out.println(""deserialize cols.length="" + nCols);"
Release-2.2.0,"System.out.print(""deserialize "");"
Release-2.2.0,"System.out.print(cols[c] + "" "");"
Release-2.2.0,System.out.println();
Release-2.2.0,TODO Auto-generated method stub
Release-2.2.0,"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.2.0,"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.2.0,"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));"
Release-2.2.0,"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));"
Release-2.2.0,"ground truth: positive, precision: positive"
Release-2.2.0,start row index for words
Release-2.2.0,start row index for docs
Release-2.2.0,doc ids
Release-2.2.0,topic assignments
Release-2.2.0,word to docs reverse index
Release-2.2.0,count word
Release-2.2.0,build word start index
Release-2.2.0,build word to doc reverse idx
Release-2.2.0,build dks
Release-2.2.0,dks = new TraverseHashMap[n_docs];
Release-2.2.0,for (int d = 0; d < n_docs; d++) {
Release-2.2.0,if (K < Short.MAX_VALUE) {
Release-2.2.0,if (docs.get(d).len < Byte.MAX_VALUE)
Release-2.2.0,dks[d] = new S2BTraverseMap(docs.get(d).len);
Release-2.2.0,if (docs.get(d).len < Short.MAX_VALUE)
Release-2.2.0,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
Release-2.2.0,else
Release-2.2.0,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
Release-2.2.0,} else {
Release-2.2.0,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
Release-2.2.0,}
Release-2.2.0,}
Release-2.2.0,build dks
Release-2.2.0,allocate update maps
Release-2.2.0,Skip if no token for this word
Release-2.2.0,Check whether error when fetching word-topic
Release-2.2.0,Build FTree for current word
Release-2.2.0,current doc
Release-2.2.0,old topic assignment
Release-2.2.0,"Check if error happens. if this happen, it's probably that failures happen to servers."
Release-2.2.0,We need to adjust the memory settings or network fetching parameters.
Release-2.2.0,Update statistics if needed
Release-2.2.0,Calculate psum and sample new topic
Release-2.2.0,Update statistics if needed
Release-2.2.0,Assign new topic
Release-2.2.0,Skip if no token for this word
Release-2.2.0,The starting point
Release-2.2.0,There's always an unused entry.
Release-2.2.0,The starting point
Release-2.2.0,There's always an unused entry.
Release-2.2.0,The starting point
Release-2.2.0,There's always an unused entry.
Release-2.2.0,The starting point
Release-2.2.0,There's always an unused entry.
Release-2.2.0,The starting point
Release-2.2.0,There's always an unused entry.
Release-2.2.0,The starting point
Release-2.2.0,There's always an unused entry.
Release-2.2.0,print();
Release-2.2.0,The starting point
Release-2.2.0,There's always an unused entry.
Release-2.2.0,The starting point
Release-2.2.0,There's always an unused entry.
Release-2.2.0,The starting point
Release-2.2.0,There's always an unused entry.
Release-2.2.0,The starting point
Release-2.2.0,There's always an unused entry.
Release-2.2.0,print();
Release-2.2.0,Write #rows
Release-2.2.0,Write each row
Release-2.2.0,dense
Release-2.2.0,sparse
Release-2.2.0,LOG.info(buf.refCnt());
Release-2.2.0,dense
Release-2.2.0,sparse
Release-2.2.0,calculate columns
Release-2.2.0,loss function
Release-2.2.0,gradient and hessian
Release-2.2.0,"categorical feature set, null: none, empty: all, else: partial"
Release-2.2.0,"node's end index in instancePos, instances in [start, end] belong to a tree node"
Release-2.2.0,initialize the phase
Release-2.2.0,current tree and depth
Release-2.2.0,create loss function
Release-2.2.0,calculate grad info of each instance
Release-2.2.0,"create data sketch, push candidate split value to PS"
Release-2.2.0,1. calculate candidate split value
Release-2.2.0,categorical features
Release-2.2.0,2. push local sketch to PS
Release-2.2.0,the leader worker
Release-2.2.0,merge categorical features
Release-2.2.0,create updates
Release-2.2.0,"pull the global sketch from PS, only called once by each worker"
Release-2.2.0,number of categorical feature
Release-2.2.0,sample feature
Release-2.2.0,push sampled feature set to the current tree
Release-2.2.0,create new tree
Release-2.2.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
Release-2.2.0,calculate gradient
Release-2.2.0,"1. create new tree, initialize tree nodes and node stats"
Release-2.2.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
Release-2.2.0,2.1. pull the sampled features of the current tree
Release-2.2.0,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
Release-2.2.0,"2.2. if use all the features, only called one"
Release-2.2.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
Release-2.2.0,4. set root node to active
Release-2.2.0,"5. reset instance position, set the root node's span"
Release-2.2.0,6. calculate gradient
Release-2.2.0,1. decide nodes that should be calculated
Release-2.2.0,2. decide calculated and subtracted tree nodes
Release-2.2.0,3. calculate threads
Release-2.2.0,wait until all threads finish
Release-2.2.0,4. subtract threads
Release-2.2.0,wait until all threads finish
Release-2.2.0,5. send histograms to PS
Release-2.2.0,6. update histogram cache
Release-2.2.0,clock
Release-2.2.0,find split
Release-2.2.0,"1. find responsible tree node, using RR scheme"
Release-2.2.0,2. pull gradient histogram
Release-2.2.0,2.1. get the name of this node's gradient histogram on PS
Release-2.2.0,2.2. pull the histogram
Release-2.2.0,2.3. find best split result of this tree node
Release-2.2.0,2.3.1 using server split
Release-2.2.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.2.0,update the grad stats of children node
Release-2.2.0,update the left child
Release-2.2.0,update the right child
Release-2.2.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.2.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
Release-2.2.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.2.0,2.3.5 reset this tree node's gradient histogram to 0
Release-2.2.0,3. push split feature to PS
Release-2.2.0,4. push split value to PS
Release-2.2.0,5. push split gain to PS
Release-2.2.0,6. set phase to AFTER_SPLIT
Release-2.2.0,this.phase = GBDTPhase.AFTER_SPLIT;
Release-2.2.0,clock
Release-2.2.0,1. get split feature
Release-2.2.0,2. get split value
Release-2.2.0,3. get split gain
Release-2.2.0,4. get node weight
Release-2.2.0,5. split node
Release-2.2.0,update local replica
Release-2.2.0,create AfterSplit task
Release-2.2.0,"2. check thread stats, if all threads finish, return"
Release-2.2.0,6. clock
Release-2.2.0,"split the span of one node, reset the instance position"
Release-2.2.0,in case this worker has no instance on this node
Release-2.2.0,set the span of left child
Release-2.2.0,set the span of right child
Release-2.2.0,"1. left to right, find the first instance that should be in the right child"
Release-2.2.0,"2. right to left, find the first instance that should be in the left child"
Release-2.2.0,3. swap two instances
Release-2.2.0,4. find the cut pos
Release-2.2.0,5. set the span of left child
Release-2.2.0,6. set the span of right child
Release-2.2.0,set tree node to active
Release-2.2.0,set node to leaf
Release-2.2.0,set node to inactive
Release-2.2.0,finish current depth
Release-2.2.0,finish current tree
Release-2.2.0,set the tree phase
Release-2.2.0,check if there is active node
Release-2.2.0,check if finish all the tree
Release-2.2.0,update node's grad stats on PS
Release-2.2.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
Release-2.2.0,the root node's stats is updated by leader worker
Release-2.2.0,1. create the update
Release-2.2.0,2. push the update to PS
Release-2.2.0,1. update predictions of training data
Release-2.2.0,2. update predictions of validation data
Release-2.2.0,the leader task adds node prediction to flush list
Release-2.2.0,1. name of this node's grad histogram on PS
Release-2.2.0,2. build the grad histogram of this node
Release-2.2.0,3. push the histograms to PS
Release-2.2.0,4. reset thread stats to finished
Release-2.2.0,5.1. set the children nodes of this node
Release-2.2.0,5.2. set split info and grad stats to this node
Release-2.2.0,5.2. create children nodes
Release-2.2.0,"5.3. create node stats for children nodes, and add them to the tree"
Release-2.2.0,5.4. reset instance position
Release-2.2.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
Release-2.2.0,5.6. set children nodes to leaf nodes
Release-2.2.0,5.7. set nid to leaf node
Release-2.2.0,5.8. deactivate active node
Release-2.2.0,"get feature type, 0:empty 1:all equal 2:real"
Release-2.2.0,"if not -1, sufficient space will be allocated at once"
Release-2.2.0,copy the highest levels
Release-2.2.0,copy baseBuffer
Release-2.2.0,merge two non-empty quantile sketches
Release-2.2.0,left child <= split value; right child > split value
Release-2.2.0,"the first: minimal, the last: maximal"
Release-2.2.0,categorical features
Release-2.2.0,continuous features
Release-2.2.0,left child <= split value; right child > split value
Release-2.2.0,feature index used to split
Release-2.2.0,feature value used to split
Release-2.2.0,loss change after split this node
Release-2.2.0,grad stats of the left child
Release-2.2.0,grad stats of the right child
Release-2.2.0,"LOG.info(""Constructor with fid = -1"");"
Release-2.2.0,fid = -1: no split currently
Release-2.2.0,the minimal split value is the minimal value of feature
Release-2.2.0,the splits do not include the maximal value of feature
Release-2.2.0,"1. the average distance, (maxValue - minValue) / splitNum"
Release-2.2.0,2. calculate the candidate split value
Release-2.2.0,1. new feature's histogram (grad + hess)
Release-2.2.0,size: sampled_featureNum * (2 * splitNum)
Release-2.2.0,"in other words, concatenate each feature's histogram"
Release-2.2.0,2. get the span of this node
Release-2.2.0,------ 3. using sparse-aware method to build histogram ---
Release-2.2.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
Release-2.2.0,3.1. get the instance index
Release-2.2.0,3.2. get the grad and hess of the instance
Release-2.2.0,3.3. add to the sum
Release-2.2.0,3.4. loop the non-zero entries
Release-2.2.0,3.4.1. get feature value
Release-2.2.0,3.4.2. current feature's position in the sampled feature set
Release-2.2.0,"int fPos = findFidPlace(this.controller.fSet, fid);"
Release-2.2.0,3.4.3. find the position of feature value in a histogram
Release-2.2.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
Release-2.2.0,3.4.4. add the grad and hess to the corresponding bin
Release-2.2.0,3.4.5. add the reverse to the bin that contains 0.0f
Release-2.2.0,4. add the grad and hess sum to the zero bin of all features
Release-2.2.0,find the best split result of the histogram of a tree node
Release-2.2.0,1. calculate the gradStats of the root node
Release-2.2.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
Release-2.2.0,2. loop over features
Release-2.2.0,2.1. get the ture feature id in the sampled feature set
Release-2.2.0,2.2. get the indexes of histogram of this feature
Release-2.2.0,2.3. find the best split of current feature
Release-2.2.0,2.4. update the best split result if possible
Release-2.2.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.2.0,3. update the grad stats of children node
Release-2.2.0,3.1. update the left child
Release-2.2.0,3.2. update the right child
Release-2.2.0,find the best split result of one feature
Release-2.2.0,1. set the feature id
Release-2.2.0,2. create the best left stats and right stats
Release-2.2.0,3. the gain of the root node
Release-2.2.0,4. create the temp left and right grad stats
Release-2.2.0,5. loop over all the data in histogram
Release-2.2.0,5.1. get the grad and hess of current hist bin
Release-2.2.0,5.2. check whether we can split with current left hessian
Release-2.2.0,right = root - left
Release-2.2.0,5.3. check whether we can split with current right hessian
Release-2.2.0,5.4. calculate the current loss gain
Release-2.2.0,5.5. check whether we should update the split result with current loss gain
Release-2.2.0,split value = sketches[splitIdx]
Release-2.2.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.2.0,6. set the best left and right grad stats
Release-2.2.0,partition number
Release-2.2.0,cols of each partition
Release-2.2.0,1. calculate the total grad sum and hess sum
Release-2.2.0,2. create the grad stats of the node
Release-2.2.0,1. calculate the total grad sum and hess sum
Release-2.2.0,2. create the grad stats of the node
Release-2.2.0,1. calculate the total grad sum and hess sum
Release-2.2.0,2. create the grad stats of the node
Release-2.2.0,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
Release-2.2.0,assert fvalue >= sketch[start] && fvalue <= sketch[end];
Release-2.2.0,if (left > end) return end - start;
Release-2.2.0,find the best split result of the histogram of a tree node
Release-2.2.0,2.2. get the indexes of histogram of this feature
Release-2.2.0,2.3. find the best split of current feature
Release-2.2.0,2.4. update the best split result if possible
Release-2.2.0,find the best split result of one feature
Release-2.2.0,1. set the feature id
Release-2.2.0,splitEntry.setFid(fid);
Release-2.2.0,2. create the best left stats and right stats
Release-2.2.0,3. the gain of the root node
Release-2.2.0,4. create the temp left and right grad stats
Release-2.2.0,5. loop over all the data in histogram
Release-2.2.0,5.1. get the grad and hess of current hist bin
Release-2.2.0,5.2. check whether we can split with current left hessian
Release-2.2.0,right = root - left
Release-2.2.0,5.3. check whether we can split with current right hessian
Release-2.2.0,5.4. calculate the current loss gain
Release-2.2.0,5.5. check whether we should update the split result with current loss gain
Release-2.2.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.2.0,6. set the best left and right grad stats
Release-2.2.0,find the best split result of a serve row on the PS
Release-2.2.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
Release-2.2.0,2.2. get the start index in histogram of this feature
Release-2.2.0,2.3. find the best split of current feature
Release-2.2.0,2.4. update the best split result if possible
Release-2.2.0,"find the best split result of one feature from a server row, used by the PS"
Release-2.2.0,1. set the feature id
Release-2.2.0,2. create the best left stats and right stats
Release-2.2.0,3. the gain of the root node
Release-2.2.0,4. create the temp left and right grad stats
Release-2.2.0,5. loop over all the data in histogram
Release-2.2.0,5.1. get the grad and hess of current hist bin
Release-2.2.0,5.2. check whether we can split with current left hessian
Release-2.2.0,right = root - left
Release-2.2.0,5.3. check whether we can split with current right hessian
Release-2.2.0,5.4. calculate the current loss gain
Release-2.2.0,5.5. check whether we should update the split result with current loss gain
Release-2.2.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
Release-2.2.0,the task use index to find fvalue
Release-2.2.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.2.0,6. set the best left and right grad stats
Release-2.2.0,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
Release-2.2.0,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
Release-2.2.0,max and min of each feature
Release-2.2.0,clear all the information
Release-2.2.0,calculate the sum of gradient and hess
Release-2.2.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
Release-2.2.0,ridx)
Release-2.2.0,check if necessary information is ready
Release-2.2.0,"same as add, reduce is used in All Reduce"
Release-2.2.0,"features used in this tree, if equals null, means use all the features without sampling"
Release-2.2.0,node in the tree
Release-2.2.0,the gradient info of each instances
Release-2.2.0,initialize nodes
Release-2.2.0,gradient
Release-2.2.0,second order gradient
Release-2.2.0,int sendStartCol = (int) row.getStartCol();
Release-2.2.0,logistic loss for binary classification task.
Release-2.2.0,"logistic loss, but predict un-transformed margin"
Release-2.2.0,check if label in range
Release-2.2.0,return the default evaluation metric for the objective
Release-2.2.0,"task type: classification, regression, or ranking"
Release-2.2.0,"quantile sketch, size = featureNum * splitNum"
Release-2.2.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
Release-2.2.0,"active tree nodes, size = pow(2, treeDepth) -1"
Release-2.2.0,sampled features. size = treeNum * sampleRatio * featureNum
Release-2.2.0,categorical feature. size = workerNum * cateFeatNum * splitNum
Release-2.2.0,"split features, size = treeNum * treeNodeNum"
Release-2.2.0,"split values, size = treeNum * treeNodeNum"
Release-2.2.0,"split gains, size = treeNum * treeNodeNum"
Release-2.2.0,"node weights, size = treeNum * treeNodeNum"
Release-2.2.0,"node preds, size = treeNum * treeNodeNum"
Release-2.2.0,if using PS to perform split
Release-2.2.0,step size for a tree
Release-2.2.0,number of class
Release-2.2.0,minimum loss change required for a split
Release-2.2.0,maximum depth of a tree
Release-2.2.0,number of features
Release-2.2.0,number of nonzero
Release-2.2.0,number of candidates split value
Release-2.2.0,----- the rest parameters are less important ----
Release-2.2.0,base instance weight
Release-2.2.0,minimum amount of hessian(weight) allowed in a child
Release-2.2.0,L2 regularization factor
Release-2.2.0,L1 regularization factor
Release-2.2.0,default direction choice
Release-2.2.0,maximum delta update we can add in weight estimation
Release-2.2.0,this parameter can be used to stabilize update
Release-2.2.0,default=0 means no constraint on weight delta
Release-2.2.0,whether we want to do subsample for row
Release-2.2.0,whether to subsample columns for each tree
Release-2.2.0,accuracy of sketch
Release-2.2.0,accuracy of sketch
Release-2.2.0,leaf vector size
Release-2.2.0,option for parallelization
Release-2.2.0,option to open cacheline optimization
Release-2.2.0,whether to not print info during training.
Release-2.2.0,maximum depth of the tree
Release-2.2.0,number of features used for tree construction
Release-2.2.0,"minimum loss change required for a split, otherwise stop split"
Release-2.2.0,----- the rest parameters are less important ----
Release-2.2.0,default direction choice
Release-2.2.0,whether we want to do sample data
Release-2.2.0,whether to sample columns during tree construction
Release-2.2.0,whether to use histogram for split
Release-2.2.0,number of histogram units
Release-2.2.0,whether to print info during training.
Release-2.2.0,----- the rest parameters are obtained after training ----
Release-2.2.0,total number of nodes
Release-2.2.0,number of deleted nodes */
Release-2.1.0,@maxIndex: this variable contains the max index of node/word
Release-2.1.0,values[b + offset] = (random.nextFloat() - 0.5f) / dimension;
Release-2.1.0,some params
Release-2.1.0,max index for node/word
Release-2.1.0,compute number of nodes for one row
Release-2.1.0,check the length of dot values
Release-2.1.0,merge dot values from all partitions
Release-2.1.0,Skip-Gram model
Release-2.1.0,Negative sampling
Release-2.1.0,used to accumulate the updates for input vectors
Release-2.1.0,Negative sampling
Release-2.1.0,accumulate for the hidden layer
Release-2.1.0,update output layer
Release-2.1.0,update the hidden layer
Release-2.1.0,update input
Release-2.1.0,Skip-Gram model
Release-2.1.0,Negative sampling
Release-2.1.0,used to accumulate the updates for input vectors
Release-2.1.0,Negative sampling
Release-2.1.0,accumulate for the hidden layer
Release-2.1.0,update output layer
Release-2.1.0,update the hidden layer
Release-2.1.0,update input
Release-2.1.0,update output
Release-2.1.0,Some params
Release-2.1.0,compute number of nodes for one row
Release-2.1.0,window size
Release-2.1.0,Skip-Gram model
Release-2.1.0,Accumulate the input vectors from context
Release-2.1.0,Negative sampling
Release-2.1.0,used to accumulate the updates for input vectors
Release-2.1.0,window size
Release-2.1.0,skip-gram model
Release-2.1.0,Negative sampling
Release-2.1.0,accumulate for the hidden layer
Release-2.1.0,update output layer
Release-2.1.0,update the hidden layer
Release-2.1.0,update input
Release-2.1.0,update output
Release-2.1.0,some params
Release-2.1.0,batch sentences
Release-2.1.0,max index for node/word
Release-2.1.0,compute number of nodes for one row
Release-2.1.0,check the length of dot values
Release-2.1.0,merge dot values from all partitions
Release-2.1.0,locates the input vectors to local array to prevent randomly access
Release-2.1.0,on the large server row.
Release-2.1.0,fill 0 for context vector
Release-2.1.0,window size
Release-2.1.0,Continuous bag-of-words Models
Release-2.1.0,Accumulate the input vectors from context
Release-2.1.0,Calculate the partial dot values
Release-2.1.0,We should guarantee here that the sample would not equal the ``word``
Release-2.1.0,used to accumulate the context input vectors
Release-2.1.0,locates the input vector into local arrays to prevent randomly access for
Release-2.1.0,the large server row.
Release-2.1.0,window size
Release-2.1.0,while true to prevent sampling out a positive target
Release-2.1.0,how to prevent the randomly access to the output vectors??
Release-2.1.0,accumulate gradients for the input vectors
Release-2.1.0,update output vectors
Release-2.1.0,update input
Release-2.1.0,update output
Release-2.1.0,Some params
Release-2.1.0,compute number of nodes for one row
Release-2.1.0,// calculate bias
Release-2.1.0,if (param.getPartKey().getStartCol() <= 0 && param.getPartKey().getEndCol() > 0) {
Release-2.1.0,"double zVal = VectorUtils.getDouble(z, 0);"
Release-2.1.0,"double nVal = VectorUtils.getDouble(n, 0);"
Release-2.1.0,"VectorUtils.setFloat(w, 0, (float) (-1.0 * alpha * zVal / (beta + Math.sqrt(nVal))));"
Release-2.1.0,}
Release-2.1.0,Do nothing.
Release-2.1.0,current word
Release-2.1.0,neu1 stores the average value of input vectors in the context (CBOW)
Release-2.1.0,Continuous Bag-of-Words Model
Release-2.1.0,Accumulate the input vectors from context
Release-2.1.0,negative sampling
Release-2.1.0,Using the sigmoid value from the pre-computed table
Release-2.1.0,accumulate for the hidden layer
Release-2.1.0,update output layer
Release-2.1.0,add the counter for target
Release-2.1.0,update hidden layer
Release-2.1.0,Update the input vector for each word in the context
Release-2.1.0,add the counter to input
Release-2.1.0,update input layers
Release-2.1.0,update output layers
Release-2.1.0,for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];
Release-2.1.0,copy the highest levels
Release-2.1.0,copy baseBuffer
Release-2.1.0,merge two non-empty quantile sketches
Release-2.1.0,"if not -1, sufficient space will be allocated at once"
Release-2.1.0,InstanceRow ins = instanceRows[insId];
Release-2.1.0,int[] indices = ins.indices();
Release-2.1.0,int[] bins = ins.bins();
Release-2.1.0,int nnz = indices.length;
Release-2.1.0,for (int j = 0; j < nnz; j++) {
Release-2.1.0,int fid = indices[j];
Release-2.1.0,if (isFeatUsed[fid - featLo]) {
Release-2.1.0,"histograms[fid - featLo].accumulate(bins[j], gradPairs[insId]);"
Release-2.1.0,}
Release-2.1.0,}
Release-2.1.0,1. allocate histogram
Release-2.1.0,"2. loop non-zero instances, accumulate to histogram"
Release-2.1.0,if (nnz <= nodeEnd - nodeStart + 1) { // loop all nnz of current feature
Release-2.1.0,3. add remaining grad and hess to default bin
Release-2.1.0,"return param.calcWeights(grad, hess);"
Release-2.1.0,"numClass is usually small, so we do not use arraycopy here"
Release-2.1.0,"numClass is usually small, so we do not use arraycopy here"
Release-2.1.0,TODO: use more schema on default bin
Release-2.1.0,1. set default bin to left child
Release-2.1.0,"2. for other bins, find its location"
Release-2.1.0,3. create split set
Release-2.1.0,this.nodeStats = new GBTNodeStat[numClass == 2 ? 1 : numClass];
Release-2.1.0,predict sparse instance with indices and values
Release-2.1.0,predict libsvm data
Release-2.1.0,"different types of tree node splits, enumerated by their complexity"
Release-2.1.0,"in order to reduce model size, we give priority to split point"
Release-2.1.0,"comparison between two split points, we give priority to lower feature index"
Release-2.1.0,TODO: comparison between two split sets
Release-2.1.0,"public boolean leafwise;  // true if leaf-wise training, false if level-wise training"
Release-2.1.0,TODO: regularization
Release-2.1.0,TODO: regularization
Release-2.1.0,public float insSampleRatio;  // subsample ratio for instances
Release-2.1.0,"Preconditions.checkArgument(preds.length == labels.length,"
Release-2.1.0,"""LogLossMetric should be used for binary-label classification"");"
Release-2.1.0,double loss = 0.0;
Release-2.1.0,for (int i = 0; i < preds.length; i++) {
Release-2.1.0,"loss += evalOne(preds[i], labels[i]);"
Release-2.1.0,}
Release-2.1.0,return loss / labels.length;
Release-2.1.0,double error = 0.0;
Release-2.1.0,if (preds.length == labels.length) {
Release-2.1.0,for (int i = 0; i < preds.length; i++) {
Release-2.1.0,"error += evalOne(preds[i], labels[i]);"
Release-2.1.0,}
Release-2.1.0,} else {
Release-2.1.0,int numLabel = preds.length / labels.length;
Release-2.1.0,float[] pred = new float[numLabel];
Release-2.1.0,for (int i = 0; i < labels.length; i++) {
Release-2.1.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.1.0,"error += evalOne(pred, labels[i]);"
Release-2.1.0,}
Release-2.1.0,}
Release-2.1.0,return error / labels.length;
Release-2.1.0,Preconditions.checkArgument(preds.length != labels.length
Release-2.1.0,"&& preds.length % labels.length == 0,"
Release-2.1.0,"""CrossEntropyMetric should be used for multi-label classification"");"
Release-2.1.0,double loss = 0.0;
Release-2.1.0,int numLabel = preds.length / labels.length;
Release-2.1.0,float[] pred = new float[numLabel];
Release-2.1.0,for (int i = 0; i < labels.length; i++) {
Release-2.1.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.1.0,"loss += evalOne(pred, labels[i]);"
Release-2.1.0,}
Release-2.1.0,return loss / labels.length;
Release-2.1.0,double correct = 0.0;
Release-2.1.0,if (preds.length == labels.length) {
Release-2.1.0,for (int i = 0; i < preds.length; i++) {
Release-2.1.0,"correct += evalOne(preds[i], labels[i]);"
Release-2.1.0,}
Release-2.1.0,} else {
Release-2.1.0,int numLabel = preds.length / labels.length;
Release-2.1.0,float[] pred = new float[numLabel];
Release-2.1.0,for (int i = 0; i < labels.length; i++) {
Release-2.1.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.1.0,"correct += evalOne(pred, labels[i]);"
Release-2.1.0,}
Release-2.1.0,}
Release-2.1.0,return (float) (correct / labels.length);
Release-2.1.0,double errSum = 0.0f;
Release-2.1.0,if (preds.length == labels.length) {
Release-2.1.0,for (int i = 0; i < preds.length; i++) {
Release-2.1.0,"errSum += evalOne(preds[i], labels[i]);"
Release-2.1.0,}
Release-2.1.0,} else {
Release-2.1.0,int numLabel = preds.length / labels.length;
Release-2.1.0,float[] pred = new float[numLabel];
Release-2.1.0,for (int i = 0; i < labels.length; i++) {
Release-2.1.0,"System.arraycopy(preds, i * numLabel, pred, 0, numLabel);"
Release-2.1.0,"errSum += evalOne(pred, labels[i]);"
Release-2.1.0,}
Release-2.1.0,}
Release-2.1.0,return Math.sqrt(errSum / labels.length);
Release-2.1.0,"System.out.println(""----------"");"
Release-2.1.0,"System.out.println(""read buffer = "" + Integer.toBinaryString(readBufferT & 0b11111111)"
Release-2.1.0,"+ "", mask = "" + Integer.toBinaryString(readMaskT));"
Release-2.1.0,readMaskT <<= 1;
Release-2.1.0,"boolean[] bits = new boolean[]{true, true, false, false, true, false, false, false, true, true, false, true};"
Release-2.1.0,int n = bits.length;
Release-2.1.0,BufferedBitSet writeBitSet = new BufferedBitSet(n);
Release-2.1.0,"BufferedBitSet readBitSet = new BufferedBitSet(writeBitSet.getBytes(), n);"
Release-2.1.0,if (bitSet.get(i) != bits[i]) {
Release-2.1.0,"throw new RuntimeException("""" + i);"
Release-2.1.0,}
Release-2.1.0,private final ByteBuffer bytes;
Release-2.1.0,"public BufferedBitSetReader(ByteBuffer bytes, int numBits) {"
Release-2.1.0,int capacity = bytes.capacity() * 8;
Release-2.1.0,readIndexT = bytes.capacity() - 1;
Release-2.1.0,return bytes.get(index);
Release-2.1.0,TODO: use arraycopy to make it faster
Release-2.1.0,assert from >= this.from && to <= this.to;
Release-2.1.0,"LOG.debug(String.format(""Create subset: [%d-%d]"", newFrom, newTo));"
Release-2.1.0,"LOG.debug(String.format(""Get overlap: [%d-%d]"", newFrom, newTo));"
Release-2.1.0,return bits.clone();
Release-2.1.0,private final SerializableBuffer bytes;
Release-2.1.0,private final ByteBuffer bytes;
Release-2.1.0,this.bytes = ByteBuffer.allocate(numBytes);
Release-2.1.0,public BufferedBitSetWriter(ByteBuffer bytes) {
Release-2.1.0,this.bytes = bytes;
Release-2.1.0,}
Release-2.1.0,"bytes.put(writeIndex++, (byte) writeBuffer);"
Release-2.1.0,public ByteBuffer getBytes() {
Release-2.1.0,return bytes;
Release-2.1.0,}
Release-2.1.0,ML TreeConf
Release-2.1.0,GBDT TreeConf
Release-2.1.0,set basic configuration keys
Release-2.1.0,use local deploy mode and dummy data spliter
Release-2.1.0,get a angel client
Release-2.1.0,add matrix
Release-2.1.0,TODO Auto-generated constructor stub
Release-2.1.0,row 0 is a random uniform
Release-2.1.0,row 1 is a random normal
Release-2.1.0,row 2 is filled with 1.0
Release-2.1.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
Release-2.1.0,"paras[1] = ""abc"";"
Release-2.1.0,"paras[2] = ""123"";"
Release-2.1.0,Add standard Hadoop classes
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,set sgd LR algorithm parameters #feature #epoch
Release-2.1.0,Set input data path
Release-2.1.0,Set save model path
Release-2.1.0,Set actionType train
Release-2.1.0,QSLRRunner runner = new QSLRRunner();
Release-2.1.0,runner.train(conf);
Release-2.1.0,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
Release-2.1.0,Dataset
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,class number
Release-2.1.0,Model type
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,Train batch number per epoch.
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set file system
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Use local deploy mode and data format
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set Softmax algorithm parameters
Release-2.1.0,Set model class
Release-2.1.0,Dataset
Release-2.1.0,Data format
Release-2.1.0,Feature number of train data
Release-2.1.0,Tree number
Release-2.1.0,Tree depth
Release-2.1.0,Split number
Release-2.1.0,Feature sample ratio
Release-2.1.0,Ratio of validation
Release-2.1.0,Learning rate
Release-2.1.0,Set file system
Release-2.1.0,Use local deploy mode and data format
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource, #worker, #task, #PS"
Release-2.1.0,Set GBDT algorithm parameters
Release-2.1.0,Dataset
Release-2.1.0,Set file system
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Use local deploy mode
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set DeepFM algorithm parameters
Release-2.1.0,Set model class
Release-2.1.0,Dataset
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Model type
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set file system
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Use local deploy mode and data format
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set LR algorithm parameters
Release-2.1.0,Set model class
Release-2.1.0,Dataset
Release-2.1.0,Data format
Release-2.1.0,Model type
Release-2.1.0,Cluster center number
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Sample ratio per mini-batch
Release-2.1.0,C
Release-2.1.0,Set file system
Release-2.1.0,Use local deploy mode and data format
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource, #worker, #task, #PS"
Release-2.1.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.1.0,Dataset
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Model type
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set file system
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Use local deploy mode and data format
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set FM algorithm parameters
Release-2.1.0,Set model class
Release-2.1.0,Dataset
Release-2.1.0,Set file system
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Use local deploy mode
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set WideAndDeep algorithm parameters
Release-2.1.0,Set model class
Release-2.1.0,Dataset
Release-2.1.0,Data format
Release-2.1.0,"Set LDA parameters #V, #K"
Release-2.1.0,Set file system
Release-2.1.0,Use local deploy mode and data format
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource, #worker, #task, #PS"
Release-2.1.0,Set LDA algorithm parameters
Release-2.1.0,Dataset
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Model type
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set file system
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Use local deploy mode and data format
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set SVM algorithm parameters
Release-2.1.0,Set model class
Release-2.1.0,Dataset
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Model type
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,Model is classification
Release-2.1.0,Train batch number per epoch.
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set file system
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Use local deploy mode and data format
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set LR algorithm parameters
Release-2.1.0,Set model class
Release-2.1.0,Dataset
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Model type
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,Model is classification
Release-2.1.0,Train batch number per epoch.
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set file system
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Use local deploy mode and data format
Release-2.1.0,Set data path
Release-2.1.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set LR algorithm parameters
Release-2.1.0,Set model class
Release-2.1.0,Load model meta
Release-2.1.0,Convert model
Release-2.1.0,"Get input path, output path"
Release-2.1.0,Init serde
Release-2.1.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.1.0,Load model meta
Release-2.1.0,Convert model
Release-2.1.0,load hadoop configuration
Release-2.1.0,"Get input path, output path"
Release-2.1.0,Init serde
Release-2.1.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.1.0,input.seek(rowOffset.getOffset());
Release-2.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.1.0,input.seek(rowOffset.getOffset());
Release-2.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.1.0,input.seek(rowOffset.getOffset());
Release-2.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.1.0,input.seek(rowOffset.getOffset());
Release-2.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.1.0,input.seek(rowOffset.getOffset());
Release-2.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.1.0,input.seek(rowOffset.getOffset());
Release-2.1.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.1.0,input.seek(rowOffset.getOffset());
Release-2.1.0,Load model meta
Release-2.1.0,Check row type
Release-2.1.0,Load model
Release-2.1.0,Load model meta
Release-2.1.0,Check row type
Release-2.1.0,Load model
Release-2.1.0,Load model meta
Release-2.1.0,Check row type
Release-2.1.0,Load model
Release-2.1.0,Load model meta
Release-2.1.0,Check row type
Release-2.1.0,Load model
Release-2.1.0,Load model meta
Release-2.1.0,Check row type
Release-2.1.0,Load model
Release-2.1.0,Load model meta
Release-2.1.0,Check row type
Release-2.1.0,Load model
Release-2.1.0,Load model meta
Release-2.1.0,Check row type
Release-2.1.0,Load model
Release-2.1.0,Load model
Release-2.1.0,load hadoop configuration
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,System.out.println(content);
Release-2.1.0,https://blog.csdn.net/cocoonyang/article/details/63068108
Release-2.1.0,v1[i] = v1[i] + da * v2[i];
Release-2.1.0,"dgemm(String transa, String transb,"
Release-2.1.0,"int m, int n, int k,"
Release-2.1.0,"double alpha,"
Release-2.1.0,"double[] a, int lda,"
Release-2.1.0,"double[] b, int ldb,"
Release-2.1.0,"double beta,"
Release-2.1.0,"double[] c, int ldc);"
Release-2.1.0,C := alpha*op( A )*op( B ) + beta*C
Release-2.1.0,v1[i] = v1[i] + da * v2[i];
Release-2.1.0,y := alpha*A*x + beta*y
Release-2.1.0,y := alpha*A*x + beta*y
Release-2.1.0,y := alpha*A*x + beta*y
Release-2.1.0,"dgemm(String transa, String transb,"
Release-2.1.0,"int m, int n, int k,"
Release-2.1.0,"double alpha,"
Release-2.1.0,"double[] a, int lda,"
Release-2.1.0,"double[] b, int ldb,"
Release-2.1.0,"double beta,"
Release-2.1.0,"double[] c, int ldc);"
Release-2.1.0,C := alpha*op( A )*op( B ) + beta*C
Release-2.1.0,Default does nothing.
Release-2.1.0,The app injection is optional
Release-2.1.0,"renderText(""hello world"");"
Release-2.1.0,"user choose a workerGroupID from the workergroups page,"
Release-2.1.0,now we should change the AngelApp params and render the workergroup page;
Release-2.1.0,"static final String WORKER_ID = ""worker.id"";"
Release-2.1.0,"div(""#logo"")."
Release-2.1.0,"img(""/static/hadoop-st.png"")._()."
Release-2.1.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
Release-2.1.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
Release-2.1.0,JQueryUI.jsnotice(html);
Release-2.1.0,import org.apache.hadoop.conf.Configuration;
Release-2.1.0,import java.lang.reflect.Field;
Release-2.1.0,get block locations from file system
Release-2.1.0,create a list of all block and their locations
Release-2.1.0,"if the file is not splitable, just create the one block with"
Release-2.1.0,full file length
Release-2.1.0,each split can be a maximum of maxSize
Release-2.1.0,if remainder is between max and 2*max - then
Release-2.1.0,"instead of creating splits of size max, left-max we"
Release-2.1.0,create splits of size left/2 and left/2. This is
Release-2.1.0,a heuristic to avoid creating really really small
Release-2.1.0,splits.
Release-2.1.0,add this block to the block --> node locations map
Release-2.1.0,"For blocks that do not have host/rack information,"
Release-2.1.0,assign to default  rack.
Release-2.1.0,add this block to the rack --> block map
Release-2.1.0,Add this host to rackToNodes map
Release-2.1.0,add this block to the node --> block map
Release-2.1.0,"if the file system does not have any rack information, then"
Release-2.1.0,use dummy rack location.
Release-2.1.0,The topology paths have the host name included as the last
Release-2.1.0,component. Strip it.
Release-2.1.0,get tokens for all the required FileSystems..
Release-2.1.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
Release-2.1.0,job.getConfiguration());
Release-2.1.0,Whether we need to recursive look into the directory structure
Release-2.1.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.1.0,user provided one (if any).
Release-2.1.0,all the files in input set
Release-2.1.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
Release-2.1.0,process all nodes and create splits that are local to a node. Generate
Release-2.1.0,"one split per node iteration, and walk over nodes multiple times to"
Release-2.1.0,distribute the splits across nodes.
Release-2.1.0,Skip the node if it has previously been marked as completed.
Release-2.1.0,"for each block, copy it into validBlocks. Delete it from"
Release-2.1.0,blockToNodes so that the same block does not appear in
Release-2.1.0,two different splits.
Release-2.1.0,Remove all blocks which may already have been assigned to other
Release-2.1.0,splits.
Release-2.1.0,"if the accumulated split size exceeds the maximum, then"
Release-2.1.0,create this split.
Release-2.1.0,create an input split and add it to the splits array
Release-2.1.0,Remove entries from blocksInNode so that we don't walk these
Release-2.1.0,again.
Release-2.1.0,Done creating a single split for this node. Move on to the next
Release-2.1.0,node so that splits are distributed across nodes.
Release-2.1.0,This implies that the last few blocks (or all in case maxSize=0)
Release-2.1.0,were not part of a split. The node is complete.
Release-2.1.0,if there were any blocks left over and their combined size is
Release-2.1.0,"larger than minSplitNode, then combine them into one split."
Release-2.1.0,Otherwise add them back to the unprocessed pool. It is likely
Release-2.1.0,that they will be combined with other blocks from the
Release-2.1.0,same rack later on.
Release-2.1.0,This condition also kicks in when max split size is not set. All
Release-2.1.0,blocks on a node will be grouped together into a single split.
Release-2.1.0,haven't created any split on this machine. so its ok to add a
Release-2.1.0,smaller one for parallelism. Otherwise group it in the rack for
Release-2.1.0,balanced size create an input split and add it to the splits
Release-2.1.0,array
Release-2.1.0,Remove entries from blocksInNode so that we don't walk this again.
Release-2.1.0,The node is done. This was the last set of blocks for this node.
Release-2.1.0,Put the unplaced blocks back into the pool for later rack-allocation.
Release-2.1.0,Node is done. All blocks were fit into node-local splits.
Release-2.1.0,Check if node-local assignments are complete.
Release-2.1.0,All nodes have been walked over and marked as completed or all blocks
Release-2.1.0,have been assigned. The rest should be handled via rackLock assignment.
Release-2.1.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
Release-2.1.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
Release-2.1.0,"if blocks in a rack are below the specified minimum size, then keep them"
Release-2.1.0,"in 'overflow'. After the processing of all racks is complete, these"
Release-2.1.0,overflow blocks will be combined into splits.
Release-2.1.0,Process all racks over and over again until there is no more work to do.
Release-2.1.0,Create one split for this rack before moving over to the next rack.
Release-2.1.0,Come back to this rack after creating a single split for each of the
Release-2.1.0,remaining racks.
Release-2.1.0,"Process one rack location at a time, Combine all possible blocks that"
Release-2.1.0,reside on this rack as one split. (constrained by minimum and maximum
Release-2.1.0,split size).
Release-2.1.0,iterate over all racks
Release-2.1.0,"for each block, copy it into validBlocks. Delete it from"
Release-2.1.0,blockToNodes so that the same block does not appear in
Release-2.1.0,two different splits.
Release-2.1.0,"if the accumulated split size exceeds the maximum, then"
Release-2.1.0,create this split.
Release-2.1.0,create an input split and add it to the splits array
Release-2.1.0,"if we created a split, then just go to the next rack"
Release-2.1.0,"if there is a minimum size specified, then create a single split"
Release-2.1.0,"otherwise, store these blocks into overflow data structure"
Release-2.1.0,There were a few blocks in this rack that
Release-2.1.0,remained to be processed. Keep them in 'overflow' block list.
Release-2.1.0,These will be combined later.
Release-2.1.0,Process all overflow blocks
Release-2.1.0,"This might cause an exiting rack location to be re-added,"
Release-2.1.0,but it should be ok.
Release-2.1.0,"if the accumulated split size exceeds the maximum, then"
Release-2.1.0,create this split.
Release-2.1.0,create an input split and add it to the splits array
Release-2.1.0,"Process any remaining blocks, if any."
Release-2.1.0,create an input split
Release-2.1.0,add this split to the list that is returned
Release-2.1.0,long num = totLength / maxSize;
Release-2.1.0,all blocks for all the files in input set
Release-2.1.0,mapping from a rack name to the list of blocks it has
Release-2.1.0,mapping from a block to the nodes on which it has replicas
Release-2.1.0,mapping from a node to the list of blocks that it contains
Release-2.1.0,populate all the blocks for all files
Release-2.1.0,stop all services
Release-2.1.0,1.write application state to file so that the client can get the state of the application
Release-2.1.0,if master exit
Release-2.1.0,2.clear tmp and staging directory
Release-2.1.0,waiting for client to get application state
Release-2.1.0,stop the RPC server
Release-2.1.0,"Security framework already loaded the tokens into current UGI, just use"
Release-2.1.0,them
Release-2.1.0,Now remove the AM->RM token so tasks don't have it
Release-2.1.0,add a shutdown hook
Release-2.1.0,init app state storage
Release-2.1.0,init event dispacher
Release-2.1.0,init location manager
Release-2.1.0,init container allocator
Release-2.1.0,init a rpc service
Release-2.1.0,recover matrix meta if needed
Release-2.1.0,recover ps attempt information if need
Release-2.1.0,Init Client manager
Release-2.1.0,Init PS Client manager
Release-2.1.0,init parameter server manager
Release-2.1.0,recover task information if needed
Release-2.1.0,a dummy data spliter is just for test now
Release-2.1.0,recover data splits information if needed
Release-2.1.0,init worker manager and register worker manager event
Release-2.1.0,register slow worker/ps checker
Release-2.1.0,register app manager event and finish event
Release-2.1.0,Init model saver & loader
Release-2.1.0,start a web service if use yarn deploy mode
Release-2.1.0,load from app state storage first if attempt index great than 1(the master is not the first
Release-2.1.0,retry)
Release-2.1.0,"if load failed, just build a new MatrixMetaManager"
Release-2.1.0,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-2.1.0,is not the first retry)
Release-2.1.0,load task information from app state storage first if attempt index great than 1(the master
Release-2.1.0,is not the first retry)
Release-2.1.0,"if load failed, just build a new AMTaskManager"
Release-2.1.0,load data splits information from app state storage first if attempt index great than 1(the
Release-2.1.0,master is not the first retry)
Release-2.1.0,"if load failed, we need to recalculate the data splits"
Release-2.1.0,Check Workers
Release-2.1.0,Check PSS
Release-2.1.0,Check Clients
Release-2.1.0,Check PS Clients
Release-2.1.0,parse parameter server counters
Release-2.1.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.1.0,refresh last heartbeat timestamp
Release-2.1.0,send a state update event to the specific PSAttempt
Release-2.1.0,Check is there save request
Release-2.1.0,Check is there load request
Release-2.1.0,check matrix metadata inconsistencies between master and parameter server.
Release-2.1.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
Release-2.1.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
Release-2.1.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.1.0,choose a unused port
Release-2.1.0,start RPC server
Release-2.1.0,remove this parameter server attempt from monitor set
Release-2.1.0,remove this parameter server attempt from monitor set
Release-2.1.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.1.0,find workergroup in worker manager
Release-2.1.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
Release-2.1.0,"if this worker group run over, just return WORKERGROUP_EXITED"
Release-2.1.0,"if this worker group is running now, return tasks, workers, data splits for it"
Release-2.1.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.1.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.1.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.1.0,"in ANGEL_PS mode, task id may can not know advance"
Release-2.1.0,update the clock for this matrix
Release-2.1.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.1.0,"in ANGEL_PS mode, task id may can not know advance"
Release-2.1.0,update task iteration
Release-2.1.0,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
Release-2.1.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
Release-2.1.0,the number of splits equal to the number of tasks
Release-2.1.0,split data
Release-2.1.0,dispatch the splits to workergroups
Release-2.1.0,split data
Release-2.1.0,dispatch the splits to workergroups
Release-2.1.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.1.0,"first, then divided by expected split number"
Release-2.1.0,get input format class from configuration and then instantiation a input format object
Release-2.1.0,split data
Release-2.1.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.1.0,"first, then divided by expected split number"
Release-2.1.0,get input format class from configuration and then instantiation a input format object
Release-2.1.0,split data
Release-2.1.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.1.0,need to fine tune the number of workergroup and task based on the actual split number
Release-2.1.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.1.0,Record the location information for the splits in order to data localized schedule
Release-2.1.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.1.0,need to fine tune the number of workergroup and task based on the actual split number
Release-2.1.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.1.0,Record the location information for the splits in order to data localized schedule
Release-2.1.0,write meta data to a temporary file
Release-2.1.0,rename the temporary file to final file
Release-2.1.0,"if the file exists, read from file and deserialize it"
Release-2.1.0,write task meta
Release-2.1.0,write ps meta
Release-2.1.0,generate a temporary file
Release-2.1.0,write task meta to the temporary file first
Release-2.1.0,rename the temporary file to the final file
Release-2.1.0,"if last final task file exist, remove it"
Release-2.1.0,find task meta file which has max timestamp
Release-2.1.0,"if the file does not exist, just return null"
Release-2.1.0,read task meta from file and deserialize it
Release-2.1.0,generate a temporary file
Release-2.1.0,write ps meta to the temporary file first.
Release-2.1.0,rename the temporary file to the final file
Release-2.1.0,"if the old final file exist, just remove it"
Release-2.1.0,find ps meta file
Release-2.1.0,"if ps meta file does not exist, just return null"
Release-2.1.0,read ps meta from file and deserialize it
Release-2.1.0,"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),"
Release-2.1.0,String.valueOf(requestId));
Release-2.1.0,Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));
Release-2.1.0,saveContext.setTmpSavePath(tmpPath.toString());
Release-2.1.0,Filter old epoch trigger first
Release-2.1.0,Split the user request to sub-requests to pss
Release-2.1.0,Init matrix files meta
Release-2.1.0,Move output files
Release-2.1.0,Write the meta file
Release-2.1.0,Split the user request to sub-requests to pss
Release-2.1.0,check whether psagent heartbeat timeout
Release-2.1.0,Set up the launch command
Release-2.1.0,Duplicate the ByteBuffers for access by multiple containers.
Release-2.1.0,Construct the actual Container
Release-2.1.0,Application resources
Release-2.1.0,Application environment
Release-2.1.0,Service data
Release-2.1.0,Tokens
Release-2.1.0,Set up JobConf to be localized properly on the remote NM.
Release-2.1.0,Setup DistributedCache
Release-2.1.0,Setup up task credentials buffer
Release-2.1.0,LocalStorageToken is needed irrespective of whether security is enabled
Release-2.1.0,or not.
Release-2.1.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
Release-2.1.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
Release-2.1.0,Construct the actual Container
Release-2.1.0,The null fields are per-container and will be constructed for each
Release-2.1.0,container separately.
Release-2.1.0,Set up the launch command
Release-2.1.0,Duplicate the ByteBuffers for access by multiple containers.
Release-2.1.0,Construct the actual Container
Release-2.1.0,"a * in the classpath will only find a .jar, so we need to filter out"
Release-2.1.0,all .jars and add everything else
Release-2.1.0,Propagate the system classpath when using the mini cluster
Release-2.1.0,Add standard Hadoop classes
Release-2.1.0,Add mr
Release-2.1.0,Cache archives
Release-2.1.0,Cache files
Release-2.1.0,Sanity check
Release-2.1.0,Add URI fragment or just the filename
Release-2.1.0,Add the env variables passed by the user
Release-2.1.0,Set logging level in the environment.
Release-2.1.0,Setup the log4j prop
Release-2.1.0,Add main class and its arguments
Release-2.1.0,Finally add the jvmID
Release-2.1.0,vargs.add(String.valueOf(jvmID.getId()));
Release-2.1.0,Final commmand
Release-2.1.0,Add the env variables passed by the user
Release-2.1.0,Set logging level in the environment.
Release-2.1.0,Setup the log4j prop
Release-2.1.0,Add main class and its arguments
Release-2.1.0,Final commmand
Release-2.1.0,"if amTask is not null, we should clone task state from it"
Release-2.1.0,"if all parameter server complete commit, master can commit now"
Release-2.1.0,check whether parameter server heartbeat timeout
Release-2.1.0,Transitions from the NEW state.
Release-2.1.0,Transitions from the UNASSIGNED state.
Release-2.1.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
Release-2.1.0,Transitions from the ASSIGNED state.
Release-2.1.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
Release-2.1.0,PA_CONTAINER_LAUNCHED event
Release-2.1.0,Transitions from the PSAttemptStateInternal.RUNNING state.
Release-2.1.0,Transitions from the PSAttemptStateInternal.COMMITTING state
Release-2.1.0,Transitions from the PSAttemptStateInternal.KILLED state
Release-2.1.0,Transitions from the PSAttemptStateInternal.FAILED state
Release-2.1.0,create the topology tables
Release-2.1.0,reqeuest resource:send a resource request to the resource allocator
Release-2.1.0,"Once the resource is applied, build and send the launch request to the container launcher"
Release-2.1.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
Release-2.1.0,resource allocator
Release-2.1.0,set the launch time
Release-2.1.0,add the ps attempt to the heartbeat timeout monitoring list
Release-2.1.0,parse ps attempt location and put it to location manager
Release-2.1.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
Release-2.1.0,or failed
Release-2.1.0,remove ps attempt id from heartbeat timeout monitor list
Release-2.1.0,release container:send a release request to container launcher
Release-2.1.0,set the finish time only if launch time is set
Release-2.1.0,private long scheduledTime;
Release-2.1.0,Transitions from the NEW state.
Release-2.1.0,Transitions from the SCHEDULED state.
Release-2.1.0,Transitions from the RUNNING state.
Release-2.1.0,"another attempt launched,"
Release-2.1.0,Transitions from the SUCCEEDED state
Release-2.1.0,Transitions from the KILLED state
Release-2.1.0,Transitions from the FAILED state
Release-2.1.0,add diagnostic
Release-2.1.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.1.0,Refresh ps location & matrix meta
Release-2.1.0,start a new attempt for this ps
Release-2.1.0,notify ps manager
Release-2.1.0,"getContext().getLocationManager().setPsLocation(id, null);"
Release-2.1.0,add diagnostic
Release-2.1.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.1.0,start a new attempt for this ps
Release-2.1.0,notify ps manager
Release-2.1.0,notify the event handler of state change
Release-2.1.0,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
Release-2.1.0,"if forcedState is set, just return"
Release-2.1.0,else get state from state machine
Release-2.1.0,add this worker group to the success set
Release-2.1.0,check if all worker group run over
Release-2.1.0,add this worker group to the failed set
Release-2.1.0,check if too many worker groups are failed or killed
Release-2.1.0,notify a run failed event
Release-2.1.0,add this worker group to the failed set
Release-2.1.0,check if too many worker groups are failed or killed
Release-2.1.0,notify a run failed event
Release-2.1.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
Release-2.1.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
Release-2.1.0,just return the total task number now
Release-2.1.0,TODO
Release-2.1.0,check whether worker heartbeat timeout
Release-2.1.0,"if workerAttempt is not null, we should clone task state from it"
Release-2.1.0,from NEW state
Release-2.1.0,from SCHEDULED state
Release-2.1.0,get data splits location for data locality
Release-2.1.0,reqeuest resource:send a resource request to the resource allocator
Release-2.1.0,"once the resource is applied, build and send the launch request to the container launcher"
Release-2.1.0,notify failed message to the worker
Release-2.1.0,notify killed message to the worker
Release-2.1.0,release the allocated container
Release-2.1.0,notify failed message to the worker
Release-2.1.0,remove the worker attempt from heartbeat timeout listen list
Release-2.1.0,release the allocated container
Release-2.1.0,notify killed message to the worker
Release-2.1.0,remove the worker attempt from heartbeat timeout listen list
Release-2.1.0,clean the container
Release-2.1.0,notify failed message to the worker
Release-2.1.0,remove the worker attempt from heartbeat timeout listen list
Release-2.1.0,record the finish time
Release-2.1.0,clean the container
Release-2.1.0,notify killed message to the worker
Release-2.1.0,remove the worker attempt from heartbeat timeout listening list
Release-2.1.0,record the finish time
Release-2.1.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
Release-2.1.0,set worker attempt location
Release-2.1.0,notify the register message to the worker
Release-2.1.0,record the launch time
Release-2.1.0,update worker attempt metrics
Release-2.1.0,update tasks metrics
Release-2.1.0,clean the container
Release-2.1.0,notify the worker attempt run successfully message to the worker
Release-2.1.0,record the finish time
Release-2.1.0,init a worker attempt for the worker
Release-2.1.0,schedule the worker attempt
Release-2.1.0,add diagnostic
Release-2.1.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.1.0,init and start a new attempt for this ps
Release-2.1.0,notify worker manager
Release-2.1.0,add diagnostic
Release-2.1.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.1.0,init and start a new attempt for this ps
Release-2.1.0,notify worker manager
Release-2.1.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
Release-2.1.0,register to Yarn RM
Release-2.1.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
Release-2.1.0,"catch YarnRuntimeException, we should exit and need not retry"
Release-2.1.0,build heartbeat request
Release-2.1.0,send heartbeat request to rm
Release-2.1.0,"This can happen if the RM has been restarted. If it is in that state,"
Release-2.1.0,this application must clean itself up.
Release-2.1.0,Setting NMTokens
Release-2.1.0,assgin containers
Release-2.1.0,"if some container is not assigned, release them"
Release-2.1.0,handle finish containers
Release-2.1.0,dispatch container exit message to corresponding components
Release-2.1.0,killed by framework
Release-2.1.0,killed by framework
Release-2.1.0,get application finish state
Release-2.1.0,build application diagnostics
Release-2.1.0,TODO:add a job history for angel
Release-2.1.0,build unregister request
Release-2.1.0,send unregister request to rm
Release-2.1.0,Note this down for next interaction with ResourceManager
Release-2.1.0,based on blacklisting comments above we can end up decrementing more
Release-2.1.0,than requested. so guard for that.
Release-2.1.0,send the updated resource request to RM
Release-2.1.0,send 0 container count requests also to cancel previous requests
Release-2.1.0,Update resource requests
Release-2.1.0,try to assign to all nodes first to match node local
Release-2.1.0,try to match all rack local
Release-2.1.0,assign remaining
Release-2.1.0,Update resource requests
Release-2.1.0,send the container-assigned event to task attempt
Release-2.1.0,build the start container request use launch context
Release-2.1.0,send the start request to Yarn nm
Release-2.1.0,send the message that the container starts successfully to the corresponding component
Release-2.1.0,"after launching, send launched event to task attempt to move"
Release-2.1.0,it from ASSIGNED to RUNNING state
Release-2.1.0,send the message that the container starts failed to the corresponding component
Release-2.1.0,kill the remote container if already launched
Release-2.1.0,start a thread pool to startup the container
Release-2.1.0,See if we need up the pool size only if haven't reached the
Release-2.1.0,maximum limit yet.
Release-2.1.0,nodes where containers will run at *this* point of time. This is
Release-2.1.0,*not* the cluster size and doesn't need to be.
Release-2.1.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
Release-2.1.0,later is just a buffer so we are not always increasing the
Release-2.1.0,pool-size
Release-2.1.0,the events from the queue are handled in parallel
Release-2.1.0,using a thread pool
Release-2.1.0,return if already stopped
Release-2.1.0,shutdown any containers that might be left running
Release-2.1.0,Add one sync matrix
Release-2.1.0,addSyncMatrix();
Release-2.1.0,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
Release-2.1.0,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
Release-2.1.0,"LOG.info(""ps id = "" + psEntry.getKey());"
Release-2.1.0,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
Release-2.1.0,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
Release-2.1.0,"LOG.info(""matrix id = "" + metaEntry.getKey());"
Release-2.1.0,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
Release-2.1.0,}
Release-2.1.0,}
Release-2.1.0,get matrix ids in the parameter server report
Release-2.1.0,get the matrices parameter server need to create and delete
Release-2.1.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
Release-2.1.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
Release-2.1.0,Init control connection manager
Release-2.1.0,Get ps locations from master and put them to the location cache.
Release-2.1.0,Build and initialize rpc client to master
Release-2.1.0,Get psagent id
Release-2.1.0,Build PS control rpc client manager
Release-2.1.0,Build local location
Release-2.1.0,Initialize matrix meta information
Release-2.1.0,Start all services
Release-2.1.0,Stop all modules
Release-2.1.0,Stop all modules
Release-2.1.0,clock first
Release-2.1.0,wait
Release-2.1.0,Update generic resource counters
Release-2.1.0,Updating resources specified in ResourceCalculatorProcessTree
Release-2.1.0,Remove the CPU time consumed previously by JVM reuse
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,/ Plus a vector/matrix to the matrix stored in pss
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,/ Update a vector/matrix to the matrix stored in pss
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,/ Get values from pss use row/column indices
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"/ PSF get/update, use can implement their own psf"
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,/ Get a row or a batch of rows
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,TODO:
Release-2.1.0,Generate a flush request and put it to request queue
Release-2.1.0,Generate a clock request and put it to request queue
Release-2.1.0,Generate a merge request and put it to request queue
Release-2.1.0,Generate a merge request and put it to request queue
Release-2.1.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
Release-2.1.0,matrix
Release-2.1.0,and add it to cache maps
Release-2.1.0,Add the message to the tree map
Release-2.1.0,"If there are flush / clock requests blocked, we need to put this merge request into"
Release-2.1.0,the waiting queue
Release-2.1.0,Launch a merge worker to merge the update to matrix op log cache
Release-2.1.0,Remove the message from the tree map
Release-2.1.0,Wake up blocked flush/clock request
Release-2.1.0,Add flush/clock request to listener list to waiting for all the existing
Release-2.1.0,updates are merged
Release-2.1.0,Wake up blocked flush/clock request
Release-2.1.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
Release-2.1.0,blocked.
Release-2.1.0,Get next merge message sequence id
Release-2.1.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
Release-2.1.0,position
Release-2.1.0,Wake up blocked merge requests
Release-2.1.0,Get minimal sequence id from listeners
Release-2.1.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
Release-2.1.0,should flush updates to local matrix storage
Release-2.1.0,unused now
Release-2.1.0,TODO:
Release-2.1.0,Doing average or not
Release-2.1.0,Filter un-important update
Release-2.1.0,Split this row according the matrix partitions
Release-2.1.0,Set split context
Release-2.1.0,Remove the row from matrix
Release-2.1.0,buf.writeDouble(0.0);
Release-2.1.0,TODO
Release-2.1.0,TODO: write map default value
Release-2.1.0,buf.writeDouble(0);
Release-2.1.0,TODO:
Release-2.1.0,TODO:
Release-2.1.0,TODO:
Release-2.1.0,"For each partition, we generate a update split."
Release-2.1.0,"Although the split is empty for partitions those without any update data,"
Release-2.1.0,we still need to generate a update split to update the clock info on ps.
Release-2.1.0,"For each partition, we generate a update split."
Release-2.1.0,"Although the split is empty for partitions those without any update data,"
Release-2.1.0,we still need to generate a update split to update the clock info on ps.
Release-2.1.0,"For each partition, we generate a update split."
Release-2.1.0,"Although the split is empty for partitions those without any update data,"
Release-2.1.0,we still need to generate a update split to update the clock info on ps.
Release-2.1.0,"For each partition, we generate a update split."
Release-2.1.0,"Although the split is empty for partitions those without any update data,"
Release-2.1.0,we still need to generate a update split to update the clock info on ps.
Release-2.1.0,"For each partition, we generate a update split."
Release-2.1.0,"Although the split is empty for partitions those without any update data,"
Release-2.1.0,we still need to generate a update split to update the clock info on ps.
Release-2.1.0,"For each partition, we generate a update split."
Release-2.1.0,"Although the split is empty for partitions those without any update data,"
Release-2.1.0,we still need to generate a update split to update the clock info on ps.
Release-2.1.0,"LOG.error(""channel "" + ctx.channel() + "" inactive"");"
Release-2.1.0,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
Release-2.1.0,"LOG.error(""put response message queue failed "", e);"
Release-2.1.0,Use Epoll for linux
Release-2.1.0,Update location table
Release-2.1.0,Remove the server from failed list
Release-2.1.0,Notify refresh success message to request dispatcher
Release-2.1.0,Check PS exist or not
Release-2.1.0,Check heartbeat timeout
Release-2.1.0,Check PS restart or not
Release-2.1.0,private final HashSet<ParameterServerId> refreshingServerSet;
Release-2.1.0,Add it to failed rpc list
Release-2.1.0,Add the server to gray server list
Release-2.1.0,Add it to failed rpc list
Release-2.1.0,Add the server to gray server list
Release-2.1.0,Move from gray server list to failed server list
Release-2.1.0,Handle the RPCS to this server
Release-2.1.0,Submit the schedulable failed get RPCS
Release-2.1.0,Submit new get RPCS
Release-2.1.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.1.0,"If the queue is empty, just return 0"
Release-2.1.0,"If request is not over limit, just submit it"
Release-2.1.0,Submit the schedulable failed get RPCS
Release-2.1.0,Submit new put RPCS
Release-2.1.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.1.0,"LOG.info(""choose put server "" + psIds[index]);"
Release-2.1.0,Check all pending RPCS
Release-2.1.0,Check get channel context
Release-2.1.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.1.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.1.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.1.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.1.0,channelManager.printPools();
Release-2.1.0,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
Release-2.1.0,if(ts - entry.getValue() > requestTimeOut * 2)  {
Release-2.1.0,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
Release-2.1.0,"+ "" milliseconds, close all channels to it"");"
Release-2.1.0,closeChannels(entry.getKey());
Release-2.1.0,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
Release-2.1.0,}
Release-2.1.0,}
Release-2.1.0,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
Release-2.1.0,Remove all pending RPCS
Release-2.1.0,Close all channel to this PS
Release-2.1.0,private Channel getChannel(Location loc) throws Exception {
Release-2.1.0,"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));"
Release-2.1.0,}
Release-2.1.0,private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {
Release-2.1.0,"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext"
Release-2.1.0,.get()
Release-2.1.0,.getConf()
Release-2.1.0,".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,"
Release-2.1.0,AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));
Release-2.1.0,}
Release-2.1.0,"LOG.error(""send request "" + request + "" is interrupted"");"
Release-2.1.0,"LOG.error(""send request "" + request + "" failed, "", e);"
Release-2.1.0,Get server id and location for this request
Release-2.1.0,"If location is null, means that the server is not ready"
Release-2.1.0,Get the channel for the location
Release-2.1.0,Check if need get token first
Release-2.1.0,Serialize the request
Release-2.1.0,Send the request
Release-2.1.0,get a channel to server from pool
Release-2.1.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.1.0,request.getContext().setChannelPool(pool);
Release-2.1.0,Allocate the bytebuf and serialize the request
Release-2.1.0,find the partition request context from cache
Release-2.1.0,"updateMatrixCache(request.getPartKey(), response.getPartition());"
Release-2.1.0,"updateMatrixCache(request.getPartKey(), response.getRowsSplit());"
Release-2.1.0,TODO
Release-2.1.0,ServerRow rowSplit = PSAgentContext.get().getMatricesCache()
Release-2.1.0,".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),"
Release-2.1.0,request.getRowIndex());
Release-2.1.0,response.setRowSplit(rowSplit);
Release-2.1.0,"updateMatrixCache(request.getPartKey(), response.getRowSplit());"
Release-2.1.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.1.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.1.0,TODO
Release-2.1.0,"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {"
Release-2.1.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);"
Release-2.1.0,}
Release-2.1.0,"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {"
Release-2.1.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);"
Release-2.1.0,}
Release-2.1.0,"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {"
Release-2.1.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);"
Release-2.1.0,}
Release-2.1.0,Get partitions for this row
Release-2.1.0,Distinct get row requests
Release-2.1.0,Need get from ps or storage/cache
Release-2.1.0,"Switch to new request id, send a new request"
Release-2.1.0,First get this row from matrix storage
Release-2.1.0,MatrixStorage matrixStorage =
Release-2.1.0,PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);
Release-2.1.0,TVector row = matrixStorage.getRow(rowIndex);
Release-2.1.0,if (row != null && row.getClock() >= clock) {
Release-2.1.0,result.set(row);
Release-2.1.0,return row;
Release-2.1.0,}
Release-2.1.0,Get row splits of this row from the matrix cache first
Release-2.1.0,responseCache.addSubResponse(rowSplit);
Release-2.1.0,"If the row split does not exist in cache, get it from parameter server"
Release-2.1.0,Wait the final result
Release-2.1.0,Put it to the matrix cache
Release-2.1.0,"matrixStorage.addRow(rowIndex, row);"
Release-2.1.0,Just wait result
Release-2.1.0,Split the param use matrix partitions
Release-2.1.0,Send request to PSS
Release-2.1.0,Split the matrix oplog according to the matrix partitions
Release-2.1.0,"If need update clock, we should send requests to all partitions"
Release-2.1.0,Send request to PSS
Release-2.1.0,Filter the rowIds which are fetching now
Release-2.1.0,Send the rowIndex to rpc dispatcher and return immediately
Release-2.1.0,"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));"
Release-2.1.0,"LOG.info(""start to request "" + requestId);"
Release-2.1.0,"LOG.info(""start to request "" + requestId);"
Release-2.1.0,Split param use matrix partitons
Release-2.1.0,"If all sub-results are received, just remove request and result cache"
Release-2.1.0,"LOG.info(""request = "" + request + "", cache = "" + cache);"
Release-2.1.0,"LOG.info(""start to merge "" + cache + "" for request "" + request);"
Release-2.1.0,"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.1.0,Split this row according the matrix partitions
Release-2.1.0,Set split context
Release-2.1.0,Split this row according the matrix partitions
Release-2.1.0,Set split context
Release-2.1.0,long startTs = System.currentTimeMillis();
Release-2.1.0,"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.1.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
Release-2.1.0,Generate dispatch items and add them to the corresponding queues
Release-2.1.0,Filter the rowIds which are fetching now
Release-2.1.0,Sort the parts by partitionId
Release-2.1.0,Sort partition keys use start column index
Release-2.1.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.1.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.1.0,});
Release-2.1.0,"For each partition, we generate a update split."
Release-2.1.0,"Although the split is empty for partitions those without any update data,"
Release-2.1.0,we still need to generate a update split to update the clock info on ps.
Release-2.1.0,Sort the parts by partitionId
Release-2.1.0,Sort partition keys use start column index
Release-2.1.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.1.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.1.0,});
Release-2.1.0,"For each partition, we generate a update split."
Release-2.1.0,"Although the split is empty for partitions those without any update data,"
Release-2.1.0,we still need to generate a update split to update the clock info on ps.
Release-2.1.0,Put the row split to the cache(row index to row splits map)
Release-2.1.0,"If all splits of the row are received, means this row can be merged"
Release-2.1.0,TODO
Release-2.1.0,TODO
Release-2.1.0,/////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,TODO
Release-2.1.0,buf.writeDouble(0);
Release-2.1.0,TODO
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,Now we just support pipelined row splits merging for dense type row
Release-2.1.0,Pre-fetching is disable default
Release-2.1.0,matrix id to clock map
Release-2.1.0,"task index, it must be unique for whole application"
Release-2.1.0,Deserialize data splits meta
Release-2.1.0,Get workers
Release-2.1.0,Send request to every ps
Release-2.1.0,Wait the responses
Release-2.1.0,Update clock cache
Release-2.1.0,if(syncNum % 1024 == 0) {
Release-2.1.0,}
Release-2.1.0,"Use simple flow, do not use any cache"
Release-2.1.0,Get row from cache.
Release-2.1.0,"if row clock is satisfy ssp staleness limit, just return."
Release-2.1.0,Get row from ps.
Release-2.1.0,Wait until the clock value of this row is greater than or equal to the value
Release-2.1.0,"For ASYNC mode, just get from pss."
Release-2.1.0,"For BSP/SSP, get rows from storage/cache first"
Release-2.1.0,Get from ps.
Release-2.1.0,Wait until the clock value of this row is greater than or equal to the value
Release-2.1.0,"For ASYNC, just get rows from pss."
Release-2.1.0,no more retries.
Release-2.1.0,calculate sleep time and return.
Release-2.1.0,parse the i-th sleep-time
Release-2.1.0,parse the i-th number-of-retries
Release-2.1.0,calculateSleepTime may overflow.
Release-2.1.0,"A few common retry policies, with no delays."
Release-2.1.0,Read matrix meta from meta file
Release-2.1.0,Save partitions to files use fork-join
Release-2.1.0,Write the ps matrix meta to the meta file
Release-2.1.0,matrix.startServering();
Release-2.1.0,return;
Release-2.1.0,Read matrix meta from meta file
Release-2.1.0,Load partitions from file use fork-join
Release-2.1.0,Read matrix meta from meta file
Release-2.1.0,Sort partitions
Release-2.1.0,int size = rows.length;
Release-2.1.0,int size = rows.length;
Release-2.1.0,int size = rows.size();
Release-2.1.0,int size = rows.size();
Release-2.1.0,int size = rows.size();
Release-2.1.0,int size = rows.size();
Release-2.1.0,int size = rows.size();
Release-2.1.0,int size = rows.size();
Release-2.1.0,close is a local operation and should finish within milliseconds; timeout just to be safe
Release-2.1.0,response will be null for one way messages.
Release-2.1.0,maxFrameLength = 2G
Release-2.1.0,lengthFieldOffset = 0
Release-2.1.0,lengthFieldLength = 8
Release-2.1.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
Release-2.1.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
Release-2.1.0,"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())"
Release-2.1.0,.toString();
Release-2.1.0,indicates whether this connection's life cycle is managed
Release-2.1.0,See if we already have a connection (common case)
Release-2.1.0,create a unique lock for this RS + protocol (if necessary)
Release-2.1.0,get the RS lock
Release-2.1.0,do one more lookup in case we were stalled above
Release-2.1.0,Only create isa when we need to.
Release-2.1.0,definitely a cache miss. establish an RPC for
Release-2.1.0,this RS
Release-2.1.0,Throw what the RemoteException was carrying.
Release-2.1.0,check
Release-2.1.0,every
Release-2.1.0,minutes
Release-2.1.0,TODO
Release-2.1.0,创建failoverHandler
Release-2.1.0,"The number of times this invocation handler has ever been failed over,"
Release-2.1.0,before this method invocation attempt. Used to prevent concurrent
Release-2.1.0,failed method invocations from triggering multiple failover attempts.
Release-2.1.0,Make sure that concurrent failed method invocations
Release-2.1.0,only cause a
Release-2.1.0,single actual fail over.
Release-2.1.0,RpcController + Message in the method args
Release-2.1.0,(generated code from RPC bits in .proto files have
Release-2.1.0,RpcController)
Release-2.1.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
Release-2.1.0,+ (System.currentTimeMillis() - beforeConstructTs));
Release-2.1.0,get an instance of the method arg type
Release-2.1.0,RpcController + Message in the method args
Release-2.1.0,(generated code from RPC bits in .proto files have
Release-2.1.0,RpcController)
Release-2.1.0,Message (hand written code usually has only a single
Release-2.1.0,argument)
Release-2.1.0,log any RPC responses that are slower than the configured
Release-2.1.0,warn
Release-2.1.0,response time or larger than configured warning size
Release-2.1.0,"when tagging, we let TooLarge trump TooSmall to keep"
Release-2.1.0,output simple
Release-2.1.0,note that large responses will often also be slow.
Release-2.1.0,provides a count of log-reported slow responses
Release-2.1.0,RpcController + Message in the method args
Release-2.1.0,(generated code from RPC bits in .proto files have
Release-2.1.0,RpcController)
Release-2.1.0,unexpected
Release-2.1.0,"in the protobuf methods, args[1] is the only significant argument"
Release-2.1.0,for JSON encoding
Release-2.1.0,base information that is reported regardless of type of call
Release-2.1.0,Disable Nagle's Algorithm since we don't want packets to wait
Release-2.1.0,Configure the event pipeline factory.
Release-2.1.0,Make a new connection.
Release-2.1.0,Remove all pending requests (will be canceled after relinquishing
Release-2.1.0,write lock).
Release-2.1.0,Cancel any pending requests by sending errors to the callbacks:
Release-2.1.0,Close the channel:
Release-2.1.0,Close the connection:
Release-2.1.0,Shut down all thread pools to exit.
Release-2.1.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
Release-2.1.0,See NettyServer.prepareResponse for where we write out the response.
Release-2.1.0,"It writes the call.id (int), a boolean signifying any error (and if"
Release-2.1.0,"so the exception name/trace), and the response bytes"
Release-2.1.0,Read the call id.
Release-2.1.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
Release-2.1.0,"instead, it returns a null message object."
Release-2.1.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
Release-2.1.0,System.currentTimeMillis());
Release-2.1.0,"It would be good widen this to just Throwable, but IOException is what we"
Release-2.1.0,allow now
Release-2.1.0,not implemented
Release-2.1.0,not implemented
Release-2.1.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
Release-2.1.0,cache of RpcEngines by protocol
Release-2.1.0,return the RpcEngine configured to handle a protocol
Release-2.1.0,We only handle the ConnectException.
Release-2.1.0,This is the exception we can't handle.
Release-2.1.0,check if timed out
Release-2.1.0,wait for retry
Release-2.1.0,IGNORE
Release-2.1.0,return the RpcEngine that handles a proxy object
Release-2.1.0,The default implementation works synchronously
Release-2.1.0,punt: allocate a new buffer & copy into it
Release-2.1.0,Parse cmd parameters
Release-2.1.0,load hadoop configuration
Release-2.1.0,load angel system configuration
Release-2.1.0,load user configuration:
Release-2.1.0,load user config file
Release-2.1.0,load command line parameters
Release-2.1.0,load user job resource files
Release-2.1.0,load ml conf file for graph based algorithm
Release-2.1.0,load user job jar if it exist
Release-2.1.0,Expand the environment variable
Release-2.1.0,Add default fs(local fs) for lib jars.
Release-2.1.0,"LOG.info(System.getProperty(""user.dir""));"
Release-2.1.0,get tokens for all the required FileSystems..
Release-2.1.0,Whether we need to recursive look into the directory structure
Release-2.1.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.1.0,user provided one (if any).
Release-2.1.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.1.0,get tokens for all the required FileSystems..
Release-2.1.0,Whether we need to recursive look into the directory structure
Release-2.1.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.1.0,user provided one (if any).
Release-2.1.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.1.0,a simple hdfs copy function assume src path and dest path are in same hdfs
Release-2.1.0,and FileSystem object has same schema
Release-2.1.0,"LOG.warn(""interrupted while sleeping"", ie);"
Release-2.1.0,public static String getHostname() {
Release-2.1.0,try {
Release-2.1.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
Release-2.1.0,} catch (UnknownHostException uhe) {
Release-2.1.0,}
Release-2.1.0,"return new StringBuilder().append("""").append(uhe).toString();"
Release-2.1.0,}
Release-2.1.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
Release-2.1.0,String hostname = getHostname();
Release-2.1.0,String classname = clazz.getSimpleName();
Release-2.1.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
Release-2.1.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
Release-2.1.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
Release-2.1.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
Release-2.1.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
Release-2.1.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
Release-2.1.0,
Release-2.1.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
Release-2.1.0,public void run() {
Release-2.1.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
Release-2.1.0,"this.val$classname + "" at "" + this.val$hostname}));"
Release-2.1.0,}
Release-2.1.0,});
Release-2.1.0,}
Release-2.1.0,"We we interrupted because we're meant to stop? If not, just"
Release-2.1.0,continue ignoring the interruption
Release-2.1.0,Recalculate waitTime.
Release-2.1.0,// Begin delegation to Thread
Release-2.1.0,// End delegation to Thread
Release-2.1.0,instance submitter class
Release-2.1.0,Obtain filename from path
Release-2.1.0,Split filename to prexif and suffix (extension)
Release-2.1.0,Check if the filename is okay
Release-2.1.0,Prepare temporary file
Release-2.1.0,Prepare buffer for data copying
Release-2.1.0,Open and check input stream
Release-2.1.0,Open output stream and copy data between source file in JAR and the temporary file
Release-2.1.0,"If read/write fails, close streams safely before throwing an exception"
Release-2.1.0,"Finally, load the library"
Release-2.1.0,little endian load order
Release-2.1.0,tail
Release-2.1.0,fallthrough
Release-2.1.0,fallthrough
Release-2.1.0,finalization
Release-2.1.0,fmix(h1);
Release-2.1.0,----------
Release-2.1.0,body
Release-2.1.0,----------
Release-2.1.0,tail
Release-2.1.0,----------
Release-2.1.0,finalization
Release-2.1.0,----------
Release-2.1.0,body
Release-2.1.0,----------
Release-2.1.0,tail
Release-2.1.0,----------
Release-2.1.0,finalization
Release-2.1.0,throw new AngelException(e);
Release-2.1.0,JobStateProto jobState = report.getJobState();
Release-2.1.0,Check need load matrices
Release-2.1.0,Used for java code to get a AngelClient instance
Release-2.1.0,Used for python code to get a AngelClient instance
Release-2.1.0,load user job resource files
Release-2.1.0,the leaf level file should be readable by others
Release-2.1.0,the subdirs in the path should have execute permissions for
Release-2.1.0,others
Release-2.1.0,2.get job id
Release-2.1.0,Credentials credentials = new Credentials();
Release-2.1.0,4.copy resource files to hdfs
Release-2.1.0,5.write configuration to a xml file
Release-2.1.0,6.create am container context
Release-2.1.0,7.Submit to ResourceManager
Release-2.1.0,8.get app master client
Release-2.1.0,Create a number of filenames in the JobTracker's fs namespace
Release-2.1.0,add all the command line files/ jars and archive
Release-2.1.0,first copy them to jobtrackers filesystem
Release-2.1.0,should not throw a uri exception
Release-2.1.0,should not throw an uri excpetion
Release-2.1.0,set the timestamps of the archives and files
Release-2.1.0,set the public/private visibility of the archives and files
Release-2.1.0,get DelegationToken for each cached file
Release-2.1.0,check if we do not need to copy the files
Release-2.1.0,is jt using the same file system.
Release-2.1.0,just checking for uri strings... doing no dns lookups
Release-2.1.0,to see if the filesystems are the same. This is not optimal.
Release-2.1.0,but avoids name resolution.
Release-2.1.0,this might have name collisions. copy will throw an exception
Release-2.1.0,parse the original path to create new path
Release-2.1.0,check for ports
Release-2.1.0,Write job file to JobTracker's fs
Release-2.1.0,Setup resource requirements
Release-2.1.0,Setup LocalResources
Release-2.1.0,Setup security tokens
Release-2.1.0,Setup the command to run the AM
Release-2.1.0,Add AM user command opts
Release-2.1.0,Final command
Release-2.1.0,Setup the CLASSPATH in environment
Release-2.1.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
Release-2.1.0,Setup the environment variables for Admin first
Release-2.1.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
Release-2.1.0,Parse distributed cache
Release-2.1.0,Setup ContainerLaunchContext for AM container
Release-2.1.0,Set up the ApplicationSubmissionContext
Release-2.1.0,private volatile PS2PSPusherImpl ps2PSPusher;
Release-2.1.0,TODO
Release-2.1.0,Add tokens to new user so that it may execute its task correctly.
Release-2.1.0,TODO
Release-2.1.0,to exit
Release-2.1.0,TODO
Release-2.1.0,TODO
Release-2.1.0,ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));
Release-2.1.0,context.getSnapshotManager().processRecovery();
Release-2.1.0,Recover PS from snapshot or load path
Release-2.1.0,First check snapshot
Release-2.1.0,Check load path setting
Release-2.1.0,TODO
Release-2.1.0,if(ps2PSPusher != null) {
Release-2.1.0,ps2PSPusher.start();
Release-2.1.0,}
Release-2.1.0,public PS2PSPusherImpl getPs2PSPusher() {
Release-2.1.0,return ps2PSPusher;
Release-2.1.0,}
Release-2.1.0,"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");"
Release-2.1.0,"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);"
Release-2.1.0,Release the input buffer
Release-2.1.0,Release the input buffer
Release-2.1.0,"1. handle the rpc, get the response"
Release-2.1.0,Release the input buffer
Release-2.1.0,2. Serialize the response
Release-2.1.0,Send the serialized response
Release-2.1.0,Exception happened
Release-2.1.0,write seq id
Release-2.1.0,Just serialize the head
Release-2.1.0,Exception happened
Release-2.1.0,Allocate result buffer
Release-2.1.0,Exception happened
Release-2.1.0,Just serialize the head
Release-2.1.0,Exception happened
Release-2.1.0,Reset the response and allocate buffer again
Release-2.1.0,Get partition and check the partition state
Release-2.1.0,Get the stored pss for this partition
Release-2.1.0,"Check this ps is the master ps for this location, only master ps can accept the update"
Release-2.1.0,Check the partition state again
Release-2.1.0,Start to put the update to the slave pss
Release-2.1.0,TODO
Release-2.1.0,"context.getPS2PSPusher().put(request, in, partLoc);"
Release-2.1.0,Get partition and check the partition state
Release-2.1.0,Get the stored pss for this partition
Release-2.1.0,"Check this ps is the master ps for this partition, if not, just return failed"
Release-2.1.0,Start to put the update to the slave pss
Release-2.1.0,TODO
Release-2.1.0,return ServerState.GENERAL;
Release-2.1.0,Use Epoll for linux
Release-2.1.0,public String uuid;
Release-2.1.0,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
Release-2.1.0,this.channelPool = channelPool;
Release-2.1.0,}
Release-2.1.0,private final ParameterServer psServer;
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO:default value
Release-2.1.0,buf.readDouble();
Release-2.1.0,TODO
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"////// network io method, for model transform"
Release-2.1.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,public ObjectIterator<Long2FloatMap.Entry> getIter() {
Release-2.1.0,return ((LongFloatVector) row).getStorage().entryIterator();
Release-2.1.0,}
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,public ObjectIterator<Long2LongMap.Entry> getIter() {
Release-2.1.0,return ((LongLongVector) row).getStorage().entryIterator();
Release-2.1.0,}
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.1.0,and call endWrite/endRead after
Release-2.1.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.1.0,TODO: dynamic add/delete row
Release-2.1.0,private final List<PartitionKey> partitionKeys;
Release-2.1.0,"if col == -1, we use the start/end index to calculate range,"
Release-2.1.0,we use double to store the range value since two long minus might exceed the
Release-2.1.0,range of long.
Release-2.1.0,Use Epoll for linux
Release-2.1.0,find the partition request context from cache
Release-2.1.0,get a channel to server from pool
Release-2.1.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.1.0,channelManager.removeChannelPool(loc);
Release-2.1.0,Generate seq id
Release-2.1.0,Create a RecoverPartRequest
Release-2.1.0,Serialize the request
Release-2.1.0,Change the seqId for the request
Release-2.1.0,Serialize the request
Release-2.1.0,"First check the state of the channels in the pool, if a channel is unused, just return"
Release-2.1.0,"If all channels are in use, create a new channel or wait"
Release-2.1.0,Create a new channel
Release-2.1.0,"add the PSAgentContext,need fix"
Release-2.1.0,If col == -1 and start/end not set
Release-2.1.0,start/end set
Release-2.1.0,"for dense type, we need to set the colNum to set dim for vectors"
Release-2.1.0,"colNum set, start/end not set"
Release-2.1.0,Row number must > 0
Release-2.1.0,"both set, check its valid"
Release-2.1.0,TODO:add more vector type
Release-2.1.0,TODO : subDim set
Release-2.1.0,Sort the parts by partitionId
Release-2.1.0,Sort partition keys use start column index
Release-2.1.0,"For each partition, we generate a update split."
Release-2.1.0,"Although the split is empty for partitions those without any update data,"
Release-2.1.0,we still need to generate a update split to update the clock info on ps.
Release-2.1.0,Sort the parts by partitionId
Release-2.1.0,Sort partition keys use start column index
Release-2.1.0,"For each partition, we generate a update split."
Release-2.1.0,"Although the split is empty for partitions those without any update data,"
Release-2.1.0,we still need to generate a update split to update the clock info on ps.
Release-2.1.0,write the max abs
Release-2.1.0,---------------------------------------------------
Release-2.1.0,---------------------------------------------------
Release-2.1.0,---------------------------------------------------------------
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,axis = 0: on rows
Release-2.1.0,axis = 1: on cols
Release-2.1.0,1. find the insert point
Release-2.1.0,2. check the capacity and insert
Release-2.1.0,3. increase size
Release-2.1.0,1. find the insert point
Release-2.1.0,2. check the capacity and insert
Release-2.1.0,3. increase size
Release-2.1.0,1. find the insert point
Release-2.1.0,2. check the capacity and insert
Release-2.1.0,3. increase size
Release-2.1.0,1. find the insert point
Release-2.1.0,2. check the capacity and insert
Release-2.1.0,3. increase size
Release-2.1.0,1. find the insert point
Release-2.1.0,2. check the capacity and insert
Release-2.1.0,3. increase size
Release-2.1.0,1. find the insert point
Release-2.1.0,2. check the capacity and insert
Release-2.1.0,3. increase size
Release-2.1.0,1. find the insert point
Release-2.1.0,2. check the capacity and insert
Release-2.1.0,3. increase size
Release-2.1.0,1. find the insert point
Release-2.1.0,2. check the capacity and insert
Release-2.1.0,3. increase size
Release-2.1.0,-----------------
Release-2.1.0,-----------------
Release-2.1.0,-----------------
Release-2.1.0,-----------------
Release-2.1.0,-----------------
Release-2.1.0,KeepStorage is guaranteed
Release-2.1.0,"ignore the isInplace option, since v2 is dense"
Release-2.1.0,"the value in old storage can be changed safe, so switch a storage"
Release-2.1.0,"but user required keep storage, we can prevent rehash"
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,KeepStorage is guaranteed
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,v1Size < v2Size * Constant.sparseThreshold
Release-2.1.0,KeepStorage is guaranteed
Release-2.1.0,"ignore the isInplace option, since v2 is dense"
Release-2.1.0,"the value in old storage can be changed safe, so switch a storage"
Release-2.1.0,"but user required keep storage, we can prevent rehash"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,prevent rehash
Release-2.1.0,KeepStorage is guaranteed
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,dense preferred
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,sorted preferred
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,multi-rehash
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"slower but memory efficient, for small vector only"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"slower but memory efficient, for small vector only"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"slower but memory efficient, for small vector only"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"slower but memory efficient, for small vector only"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"slower but memory efficient, for small vector only"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"slower but memory efficient, for small vector only"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"slower but memory efficient, for small vector only"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"slower but memory efficient, for small vector only"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"slower but memory efficient, for small vector only"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"slower but memory efficient, for small vector only"
Release-2.1.0,"dense preferred, KeepStorage is guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,"sparse preferred, keep storage guaranteed"
Release-2.1.0,preferred dense
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,we gauss dense storage is more efficient
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.1.0,multi-rehash
Release-2.1.0,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,to avoid multi-rehash
Release-2.1.0,"Transform mat1, generate a new matrix"
Release-2.1.0,Split the row indices of mat1Trans
Release-2.1.0,Parallel execute use fork-join
Release-2.1.0,"Get the sub-matrix of left matrix, split by row"
Release-2.1.0,"Transform mat1, generate a new matrix"
Release-2.1.0,Split the row indices of mat1Trans
Release-2.1.0,Parallel execute use fork-join
Release-2.1.0,"Get the sub-matrix of left matrix, split by row"
Release-2.1.0,"mat1 trans true, mat trans true"
Release-2.1.0,"mat1 trans true, mat trans false"
Release-2.1.0,"mat1 trans false, mat trans true, important"
Release-2.1.0,"mat1 trans false, mat trans false"
Release-2.1.0,"mat1 trans true, mat trans true"
Release-2.1.0,"mat1 trans true, mat trans false"
Release-2.1.0,"mat1 trans false, mat trans true, important"
Release-2.1.0,"mat1 trans false, mat trans false"
Release-2.1.0,"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.1.0,"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.1.0,not the first time
Release-2.1.0,first time and do the sample
Release-2.1.0,set to zero
Release-2.1.0,not the first time
Release-2.1.0,first time and do the sample
Release-2.1.0,set to zero
Release-2.1.0,not the first time
Release-2.1.0,first time and do the sample
Release-2.1.0,set to zero
Release-2.1.0,not the first time
Release-2.1.0,first time and do the sample
Release-2.1.0,set to zero
Release-2.1.0,not the first time
Release-2.1.0,first time and do the sample
Release-2.1.0,set to zero
Release-2.1.0,not the first time
Release-2.1.0,first time and do the sample
Release-2.1.0,set to zero
Release-2.1.0,not the first time
Release-2.1.0,first time and do the sample
Release-2.1.0,set to zero
Release-2.1.0,not the first time
Release-2.1.0,first time and do the sample
Release-2.1.0,set to zero
Release-2.1.0,not the first time
Release-2.1.0,first time and do the sample
Release-2.1.0,set to zero
Release-2.1.0,not the first time
Release-2.1.0,first time and do the sample
Release-2.1.0,set to zero
Release-2.1.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.1.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.1.0,add dense double matrix
Release-2.1.0,TODO Auto-generated method stub
Release-2.1.0,TODO Auto-generated method stub
Release-2.1.0,TODO Auto-generated method stub
Release-2.1.0,get configuration from config file
Release-2.1.0,set localDir with enviroment set by nm.
Release-2.1.0,get master location
Release-2.1.0,init task manager and start tasks
Release-2.1.0,start heartbeat thread
Release-2.1.0,taskManager.assignTaskIds(response.getTaskidsList());
Release-2.1.0,todo
Release-2.1.0,"if worker timeout, it may be knocked off."
Release-2.1.0,"SUCCESS, do nothing"
Release-2.1.0,heartbeatFailedTime = 0;
Release-2.1.0,private KEY currentKey;
Release-2.1.0,will be created
Release-2.1.0,TODO Auto-generated method stub
Release-2.1.0,Bitmap bitmap = new Bitmap();
Release-2.1.0,int max = indexArray[size - 1];
Release-2.1.0,byte [] bitIndexArray = new byte[max / 8 + 1];
Release-2.1.0,for(int i = 0; i < size; i++){
Release-2.1.0,int bitIndex = indexArray[i] >> 3;
Release-2.1.0,int bitOffset = indexArray[i] - (bitIndex << 3);
Release-2.1.0,switch(bitOffset){
Release-2.1.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
Release-2.1.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
Release-2.1.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
Release-2.1.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
Release-2.1.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
Release-2.1.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
Release-2.1.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
Release-2.1.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
Release-2.1.0,}
Release-2.1.0,}
Release-2.1.0,"true, false"
Release-2.1.0,//////////////////////////////
Release-2.1.0,Application Configs
Release-2.1.0,//////////////////////////////
Release-2.1.0,//////////////////////////////
Release-2.1.0,Master Configs
Release-2.1.0,//////////////////////////////
Release-2.1.0,//////////////////////////////
Release-2.1.0,Worker Configs
Release-2.1.0,//////////////////////////////
Release-2.1.0,//////////////////////////////
Release-2.1.0,Task Configs
Release-2.1.0,//////////////////////////////
Release-2.1.0,//////////////////////////////
Release-2.1.0,ParameterServer Configs
Release-2.1.0,//////////////////////////////
Release-2.1.0,////////////////// IPC //////////////////////////
Release-2.1.0,//////////////////////////////
Release-2.1.0,Matrix transfer Configs.
Release-2.1.0,//////////////////////////////
Release-2.1.0,//////////////////////////////
Release-2.1.0,Matrix transfer Configs.
Release-2.1.0,//////////////////////////////
Release-2.1.0,Configs used to ANGEL_PS_PSAGENT running mode future.
Release-2.1.0,model parse
Release-2.1.0,Mark whether use pyangel or not.
Release-2.1.0,private Configuration conf;
Release-2.1.0,"Configuration that should be used in python environment, there should only be one"
Release-2.1.0,configuration instance in each Angel context.
Release-2.1.0,Use private access means jconf should not be changed or modified in this way.
Release-2.1.0,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
Release-2.1.0,Do nothing
Release-2.1.0,To-DO: add other ways to justify different value types
Release-2.1.0,"This is so ugly, must re-implement by more elegance way"
Release-2.1.0,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
Release-2.1.0,and other files submitted by user.
Release-2.1.0,Launch python process
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Data format
Release-2.1.0,Feature number of train data
Release-2.1.0,Tree number
Release-2.1.0,Tree depth
Release-2.1.0,Split number
Release-2.1.0,Feature sample ratio
Release-2.1.0,Ratio of validation
Release-2.1.0,Learning rate
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Set data format
Release-2.1.0,"Set angel resource, #worker, #task, #PS"
Release-2.1.0,Set GBDT algorithm parameters
Release-2.1.0,Set training data path
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set predict data path
Release-2.1.0,Set load model path
Release-2.1.0,Set predict result path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Train batch number per epoch.
Release-2.1.0,Batch number
Release-2.1.0,Model type
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Set data format
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,set sgd LR algorithm parameters #feature #epoch
Release-2.1.0,Set trainning data path
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Train batch number per epoch.
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Set data format
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,set sgd FM algorithm parameters #feature #epoch
Release-2.1.0,Set trainning data path
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,Model type
Release-2.1.0,String modelType = String.valueOf(RowType.T_FLOAT_DENSE);
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,set sgd LR algorithm parameters #feature #epoch
Release-2.1.0,"conf.set(MLConf.ML_MODEL_TYPE(), modelType);"
Release-2.1.0,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
Release-2.1.0,predictTest();
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Set data format
Release-2.1.0,Set trainning data path
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set data format
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,class number
Release-2.1.0,Model type
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Set data format
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,set sgd LR algorithm parameters #feature #epoch
Release-2.1.0,Set log path
Release-2.1.0,Set trainning data path
Release-2.1.0,Set save model path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set save model path
Release-2.1.0,Set actionType incremental train
Release-2.1.0,Set log path
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set predict result path
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set training data path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Cluster center number
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Sample ratio per mini-batch
Release-2.1.0,C
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.1.0,Set data format
Release-2.1.0,Set trainning data path
Release-2.1.0,Set save model path
Release-2.1.0,Set log save path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set save model path
Release-2.1.0,Set actionType incremental train
Release-2.1.0,Set log path
Release-2.1.0,Set testing data path
Release-2.1.0,Set load model path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Model type
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Set data format
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,set sgd LR algorithm parameters #feature #epoch
Release-2.1.0,Set trainning data path
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set predict result path
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Data is classification
Release-2.1.0,Model is classification
Release-2.1.0,Train batch number per epoch.
Release-2.1.0,loss delta
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Set data format
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,set sgd LR algorithm parameters #feature #epoch
Release-2.1.0,Set trainning data path
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set save model path
Release-2.1.0,Set actionType incremental train
Release-2.1.0,Set log path
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,Feature number of train data
Release-2.1.0,Total iteration number
Release-2.1.0,Validation sample Ratio
Release-2.1.0,"Data format, libsvm or dummy"
Release-2.1.0,Data is classification
Release-2.1.0,Model is classification
Release-2.1.0,Train batch number per epoch.
Release-2.1.0,Learning rate
Release-2.1.0,Decay of learning rate
Release-2.1.0,Regularization coefficient
Release-2.1.0,Set local deploy mode
Release-2.1.0,Set basic configuration keys
Release-2.1.0,Set data format
Release-2.1.0,"set angel resource parameters #worker, #task, #PS"
Release-2.1.0,set sgd LR algorithm parameters #feature #epoch
Release-2.1.0,Set trainning data path
Release-2.1.0,Set save model path
Release-2.1.0,Set log path
Release-2.1.0,Set actionType train
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set save model path
Release-2.1.0,Set actionType incremental train
Release-2.1.0,Set log path
Release-2.1.0,Set trainning data path
Release-2.1.0,Set load model path
Release-2.1.0,Set predict result path
Release-2.1.0,Set actionType prediction
Release-2.1.0,TODO: optimize int key indices
Release-2.1.0,"System.out.println(""deserialize cols.length="" + nCols);"
Release-2.1.0,"System.out.print(""deserialize "");"
Release-2.1.0,"System.out.print(cols[c] + "" "");"
Release-2.1.0,System.out.println();
Release-2.1.0,TODO Auto-generated method stub
Release-2.1.0,"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.1.0,"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.1.0,"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));"
Release-2.1.0,"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));"
Release-2.1.0,"ground truth: positive, precision: positive"
Release-2.1.0,start row index for words
Release-2.1.0,start row index for docs
Release-2.1.0,doc ids
Release-2.1.0,topic assignments
Release-2.1.0,word to docs reverse index
Release-2.1.0,count word
Release-2.1.0,build word start index
Release-2.1.0,build word to doc reverse idx
Release-2.1.0,build dks
Release-2.1.0,dks = new TraverseHashMap[n_docs];
Release-2.1.0,for (int d = 0; d < n_docs; d++) {
Release-2.1.0,if (K < Short.MAX_VALUE) {
Release-2.1.0,if (docs.get(d).len < Byte.MAX_VALUE)
Release-2.1.0,dks[d] = new S2BTraverseMap(docs.get(d).len);
Release-2.1.0,if (docs.get(d).len < Short.MAX_VALUE)
Release-2.1.0,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
Release-2.1.0,else
Release-2.1.0,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
Release-2.1.0,} else {
Release-2.1.0,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
Release-2.1.0,}
Release-2.1.0,}
Release-2.1.0,build dks
Release-2.1.0,allocate update maps
Release-2.1.0,Skip if no token for this word
Release-2.1.0,Check whether error when fetching word-topic
Release-2.1.0,Build FTree for current word
Release-2.1.0,current doc
Release-2.1.0,old topic assignment
Release-2.1.0,"Check if error happens. if this happen, it's probably that failures happen to servers."
Release-2.1.0,We need to adjust the memory settings or network fetching parameters.
Release-2.1.0,Update statistics if needed
Release-2.1.0,Calculate psum and sample new topic
Release-2.1.0,Update statistics if needed
Release-2.1.0,Assign new topic
Release-2.1.0,Skip if no token for this word
Release-2.1.0,The starting point
Release-2.1.0,There's always an unused entry.
Release-2.1.0,The starting point
Release-2.1.0,There's always an unused entry.
Release-2.1.0,The starting point
Release-2.1.0,There's always an unused entry.
Release-2.1.0,The starting point
Release-2.1.0,There's always an unused entry.
Release-2.1.0,The starting point
Release-2.1.0,There's always an unused entry.
Release-2.1.0,The starting point
Release-2.1.0,There's always an unused entry.
Release-2.1.0,print();
Release-2.1.0,The starting point
Release-2.1.0,There's always an unused entry.
Release-2.1.0,The starting point
Release-2.1.0,There's always an unused entry.
Release-2.1.0,The starting point
Release-2.1.0,There's always an unused entry.
Release-2.1.0,The starting point
Release-2.1.0,There's always an unused entry.
Release-2.1.0,print();
Release-2.1.0,Write #rows
Release-2.1.0,Write each row
Release-2.1.0,dense
Release-2.1.0,sparse
Release-2.1.0,LOG.info(buf.refCnt());
Release-2.1.0,dense
Release-2.1.0,sparse
Release-2.1.0,calculate columns
Release-2.1.0,loss function
Release-2.1.0,gradient and hessian
Release-2.1.0,"categorical feature set, null: none, empty: all, else: partial"
Release-2.1.0,"node's end index in instancePos, instances in [start, end] belong to a tree node"
Release-2.1.0,initialize the phase
Release-2.1.0,current tree and depth
Release-2.1.0,create loss function
Release-2.1.0,calculate grad info of each instance
Release-2.1.0,"create data sketch, push candidate split value to PS"
Release-2.1.0,1. calculate candidate split value
Release-2.1.0,categorical features
Release-2.1.0,2. push local sketch to PS
Release-2.1.0,the leader worker
Release-2.1.0,merge categorical features
Release-2.1.0,create updates
Release-2.1.0,"pull the global sketch from PS, only called once by each worker"
Release-2.1.0,number of categorical feature
Release-2.1.0,sample feature
Release-2.1.0,push sampled feature set to the current tree
Release-2.1.0,create new tree
Release-2.1.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
Release-2.1.0,calculate gradient
Release-2.1.0,"1. create new tree, initialize tree nodes and node stats"
Release-2.1.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
Release-2.1.0,2.1. pull the sampled features of the current tree
Release-2.1.0,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
Release-2.1.0,"2.2. if use all the features, only called one"
Release-2.1.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
Release-2.1.0,4. set root node to active
Release-2.1.0,"5. reset instance position, set the root node's span"
Release-2.1.0,6. calculate gradient
Release-2.1.0,1. decide nodes that should be calculated
Release-2.1.0,2. decide calculated and subtracted tree nodes
Release-2.1.0,3. calculate threads
Release-2.1.0,wait until all threads finish
Release-2.1.0,4. subtract threads
Release-2.1.0,wait until all threads finish
Release-2.1.0,5. send histograms to PS
Release-2.1.0,6. update histogram cache
Release-2.1.0,clock
Release-2.1.0,find split
Release-2.1.0,"1. find responsible tree node, using RR scheme"
Release-2.1.0,2. pull gradient histogram
Release-2.1.0,2.1. get the name of this node's gradient histogram on PS
Release-2.1.0,2.2. pull the histogram
Release-2.1.0,2.3. find best split result of this tree node
Release-2.1.0,2.3.1 using server split
Release-2.1.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.1.0,update the grad stats of children node
Release-2.1.0,update the left child
Release-2.1.0,update the right child
Release-2.1.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.1.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
Release-2.1.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.1.0,2.3.5 reset this tree node's gradient histogram to 0
Release-2.1.0,3. push split feature to PS
Release-2.1.0,4. push split value to PS
Release-2.1.0,5. push split gain to PS
Release-2.1.0,6. set phase to AFTER_SPLIT
Release-2.1.0,this.phase = GBDTPhase.AFTER_SPLIT;
Release-2.1.0,clock
Release-2.1.0,1. get split feature
Release-2.1.0,2. get split value
Release-2.1.0,3. get split gain
Release-2.1.0,4. get node weight
Release-2.1.0,5. split node
Release-2.1.0,update local replica
Release-2.1.0,create AfterSplit task
Release-2.1.0,"2. check thread stats, if all threads finish, return"
Release-2.1.0,6. clock
Release-2.1.0,"split the span of one node, reset the instance position"
Release-2.1.0,in case this worker has no instance on this node
Release-2.1.0,set the span of left child
Release-2.1.0,set the span of right child
Release-2.1.0,"1. left to right, find the first instance that should be in the right child"
Release-2.1.0,"2. right to left, find the first instance that should be in the left child"
Release-2.1.0,3. swap two instances
Release-2.1.0,4. find the cut pos
Release-2.1.0,5. set the span of left child
Release-2.1.0,6. set the span of right child
Release-2.1.0,set tree node to active
Release-2.1.0,set node to leaf
Release-2.1.0,set node to inactive
Release-2.1.0,finish current depth
Release-2.1.0,finish current tree
Release-2.1.0,set the tree phase
Release-2.1.0,check if there is active node
Release-2.1.0,check if finish all the tree
Release-2.1.0,update node's grad stats on PS
Release-2.1.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
Release-2.1.0,the root node's stats is updated by leader worker
Release-2.1.0,1. create the update
Release-2.1.0,2. push the update to PS
Release-2.1.0,1. update predictions of training data
Release-2.1.0,2. update predictions of validation data
Release-2.1.0,the leader task adds node prediction to flush list
Release-2.1.0,1. name of this node's grad histogram on PS
Release-2.1.0,2. build the grad histogram of this node
Release-2.1.0,3. push the histograms to PS
Release-2.1.0,4. reset thread stats to finished
Release-2.1.0,5.1. set the children nodes of this node
Release-2.1.0,5.2. set split info and grad stats to this node
Release-2.1.0,5.2. create children nodes
Release-2.1.0,"5.3. create node stats for children nodes, and add them to the tree"
Release-2.1.0,5.4. reset instance position
Release-2.1.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
Release-2.1.0,5.6. set children nodes to leaf nodes
Release-2.1.0,5.7. set nid to leaf node
Release-2.1.0,5.8. deactivate active node
Release-2.1.0,"get feature type, 0:empty 1:all equal 2:real"
Release-2.1.0,"if not -1, sufficient space will be allocated at once"
Release-2.1.0,copy the highest levels
Release-2.1.0,copy baseBuffer
Release-2.1.0,merge two non-empty quantile sketches
Release-2.1.0,left child <= split value; right child > split value
Release-2.1.0,"the first: minimal, the last: maximal"
Release-2.1.0,categorical features
Release-2.1.0,continuous features
Release-2.1.0,left child <= split value; right child > split value
Release-2.1.0,feature index used to split
Release-2.1.0,feature value used to split
Release-2.1.0,loss change after split this node
Release-2.1.0,grad stats of the left child
Release-2.1.0,grad stats of the right child
Release-2.1.0,"LOG.info(""Constructor with fid = -1"");"
Release-2.1.0,fid = -1: no split currently
Release-2.1.0,the minimal split value is the minimal value of feature
Release-2.1.0,the splits do not include the maximal value of feature
Release-2.1.0,"1. the average distance, (maxValue - minValue) / splitNum"
Release-2.1.0,2. calculate the candidate split value
Release-2.1.0,1. new feature's histogram (grad + hess)
Release-2.1.0,size: sampled_featureNum * (2 * splitNum)
Release-2.1.0,"in other words, concatenate each feature's histogram"
Release-2.1.0,2. get the span of this node
Release-2.1.0,------ 3. using sparse-aware method to build histogram ---
Release-2.1.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
Release-2.1.0,3.1. get the instance index
Release-2.1.0,3.2. get the grad and hess of the instance
Release-2.1.0,3.3. add to the sum
Release-2.1.0,3.4. loop the non-zero entries
Release-2.1.0,3.4.1. get feature value
Release-2.1.0,3.4.2. current feature's position in the sampled feature set
Release-2.1.0,"int fPos = findFidPlace(this.controller.fSet, fid);"
Release-2.1.0,3.4.3. find the position of feature value in a histogram
Release-2.1.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
Release-2.1.0,3.4.4. add the grad and hess to the corresponding bin
Release-2.1.0,3.4.5. add the reverse to the bin that contains 0.0f
Release-2.1.0,4. add the grad and hess sum to the zero bin of all features
Release-2.1.0,find the best split result of the histogram of a tree node
Release-2.1.0,1. calculate the gradStats of the root node
Release-2.1.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
Release-2.1.0,2. loop over features
Release-2.1.0,2.1. get the ture feature id in the sampled feature set
Release-2.1.0,2.2. get the indexes of histogram of this feature
Release-2.1.0,2.3. find the best split of current feature
Release-2.1.0,2.4. update the best split result if possible
Release-2.1.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.1.0,3. update the grad stats of children node
Release-2.1.0,3.1. update the left child
Release-2.1.0,3.2. update the right child
Release-2.1.0,find the best split result of one feature
Release-2.1.0,1. set the feature id
Release-2.1.0,2. create the best left stats and right stats
Release-2.1.0,3. the gain of the root node
Release-2.1.0,4. create the temp left and right grad stats
Release-2.1.0,5. loop over all the data in histogram
Release-2.1.0,5.1. get the grad and hess of current hist bin
Release-2.1.0,5.2. check whether we can split with current left hessian
Release-2.1.0,right = root - left
Release-2.1.0,5.3. check whether we can split with current right hessian
Release-2.1.0,5.4. calculate the current loss gain
Release-2.1.0,5.5. check whether we should update the split result with current loss gain
Release-2.1.0,split value = sketches[splitIdx]
Release-2.1.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.1.0,6. set the best left and right grad stats
Release-2.1.0,partition number
Release-2.1.0,cols of each partition
Release-2.1.0,1. calculate the total grad sum and hess sum
Release-2.1.0,2. create the grad stats of the node
Release-2.1.0,1. calculate the total grad sum and hess sum
Release-2.1.0,2. create the grad stats of the node
Release-2.1.0,1. calculate the total grad sum and hess sum
Release-2.1.0,2. create the grad stats of the node
Release-2.1.0,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
Release-2.1.0,assert fvalue >= sketch[start] && fvalue <= sketch[end];
Release-2.1.0,if (left > end) return end - start;
Release-2.1.0,find the best split result of the histogram of a tree node
Release-2.1.0,2.2. get the indexes of histogram of this feature
Release-2.1.0,2.3. find the best split of current feature
Release-2.1.0,2.4. update the best split result if possible
Release-2.1.0,find the best split result of one feature
Release-2.1.0,1. set the feature id
Release-2.1.0,splitEntry.setFid(fid);
Release-2.1.0,2. create the best left stats and right stats
Release-2.1.0,3. the gain of the root node
Release-2.1.0,4. create the temp left and right grad stats
Release-2.1.0,5. loop over all the data in histogram
Release-2.1.0,5.1. get the grad and hess of current hist bin
Release-2.1.0,5.2. check whether we can split with current left hessian
Release-2.1.0,right = root - left
Release-2.1.0,5.3. check whether we can split with current right hessian
Release-2.1.0,5.4. calculate the current loss gain
Release-2.1.0,5.5. check whether we should update the split result with current loss gain
Release-2.1.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.1.0,6. set the best left and right grad stats
Release-2.1.0,find the best split result of a serve row on the PS
Release-2.1.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
Release-2.1.0,2.2. get the start index in histogram of this feature
Release-2.1.0,2.3. find the best split of current feature
Release-2.1.0,2.4. update the best split result if possible
Release-2.1.0,"find the best split result of one feature from a server row, used by the PS"
Release-2.1.0,1. set the feature id
Release-2.1.0,2. create the best left stats and right stats
Release-2.1.0,3. the gain of the root node
Release-2.1.0,4. create the temp left and right grad stats
Release-2.1.0,5. loop over all the data in histogram
Release-2.1.0,5.1. get the grad and hess of current hist bin
Release-2.1.0,5.2. check whether we can split with current left hessian
Release-2.1.0,right = root - left
Release-2.1.0,5.3. check whether we can split with current right hessian
Release-2.1.0,5.4. calculate the current loss gain
Release-2.1.0,5.5. check whether we should update the split result with current loss gain
Release-2.1.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
Release-2.1.0,the task use index to find fvalue
Release-2.1.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.1.0,6. set the best left and right grad stats
Release-2.1.0,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
Release-2.1.0,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
Release-2.1.0,max and min of each feature
Release-2.1.0,clear all the information
Release-2.1.0,calculate the sum of gradient and hess
Release-2.1.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
Release-2.1.0,ridx)
Release-2.1.0,check if necessary information is ready
Release-2.1.0,"same as add, reduce is used in All Reduce"
Release-2.1.0,"features used in this tree, if equals null, means use all the features without sampling"
Release-2.1.0,node in the tree
Release-2.1.0,the gradient info of each instances
Release-2.1.0,initialize nodes
Release-2.1.0,gradient
Release-2.1.0,second order gradient
Release-2.1.0,int sendStartCol = (int) row.getStartCol();
Release-2.1.0,logistic loss for binary classification task.
Release-2.1.0,"logistic loss, but predict un-transformed margin"
Release-2.1.0,check if label in range
Release-2.1.0,return the default evaluation metric for the objective
Release-2.1.0,"task type: classification, regression, or ranking"
Release-2.1.0,"quantile sketch, size = featureNum * splitNum"
Release-2.1.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
Release-2.1.0,"active tree nodes, size = pow(2, treeDepth) -1"
Release-2.1.0,sampled features. size = treeNum * sampleRatio * featureNum
Release-2.1.0,categorical feature. size = workerNum * cateFeatNum * splitNum
Release-2.1.0,"split features, size = treeNum * treeNodeNum"
Release-2.1.0,"split values, size = treeNum * treeNodeNum"
Release-2.1.0,"split gains, size = treeNum * treeNodeNum"
Release-2.1.0,"node weights, size = treeNum * treeNodeNum"
Release-2.1.0,"node preds, size = treeNum * treeNodeNum"
Release-2.1.0,if using PS to perform split
Release-2.1.0,step size for a tree
Release-2.1.0,number of class
Release-2.1.0,minimum loss change required for a split
Release-2.1.0,maximum depth of a tree
Release-2.1.0,number of features
Release-2.1.0,number of nonzero
Release-2.1.0,number of candidates split value
Release-2.1.0,----- the rest parameters are less important ----
Release-2.1.0,base instance weight
Release-2.1.0,minimum amount of hessian(weight) allowed in a child
Release-2.1.0,L2 regularization factor
Release-2.1.0,L1 regularization factor
Release-2.1.0,default direction choice
Release-2.1.0,maximum delta update we can add in weight estimation
Release-2.1.0,this parameter can be used to stabilize update
Release-2.1.0,default=0 means no constraint on weight delta
Release-2.1.0,whether we want to do subsample for row
Release-2.1.0,whether to subsample columns for each tree
Release-2.1.0,accuracy of sketch
Release-2.1.0,accuracy of sketch
Release-2.1.0,leaf vector size
Release-2.1.0,option for parallelization
Release-2.1.0,option to open cacheline optimization
Release-2.1.0,whether to not print info during training.
Release-2.1.0,maximum depth of the tree
Release-2.1.0,number of features used for tree construction
Release-2.1.0,"minimum loss change required for a split, otherwise stop split"
Release-2.1.0,----- the rest parameters are less important ----
Release-2.1.0,default direction choice
Release-2.1.0,whether we want to do sample data
Release-2.1.0,whether to sample columns during tree construction
Release-2.1.0,whether to use histogram for split
Release-2.1.0,number of histogram units
Release-2.1.0,whether to print info during training.
Release-2.1.0,----- the rest parameters are obtained after training ----
Release-2.1.0,total number of nodes
Release-2.1.0,number of deleted nodes */
Release-2.0.2,@maxIndex: this variable contains the max index of node/word
Release-2.0.2,values[b + offset] = (random.nextFloat() - 0.5f) / dimension;
Release-2.0.2,some params
Release-2.0.2,max index for node/word
Release-2.0.2,compute number of nodes for one row
Release-2.0.2,check the length of dot values
Release-2.0.2,merge dot values from all partitions
Release-2.0.2,Skip-Gram model
Release-2.0.2,Negative sampling
Release-2.0.2,used to accumulate the updates for input vectors
Release-2.0.2,Negative sampling
Release-2.0.2,accumulate for the hidden layer
Release-2.0.2,update output layer
Release-2.0.2,update the hidden layer
Release-2.0.2,update input
Release-2.0.2,Skip-Gram model
Release-2.0.2,Negative sampling
Release-2.0.2,used to accumulate the updates for input vectors
Release-2.0.2,Negative sampling
Release-2.0.2,accumulate for the hidden layer
Release-2.0.2,update output layer
Release-2.0.2,update the hidden layer
Release-2.0.2,update input
Release-2.0.2,update output
Release-2.0.2,Some params
Release-2.0.2,compute number of nodes for one row
Release-2.0.2,window size
Release-2.0.2,Skip-Gram model
Release-2.0.2,Accumulate the input vectors from context
Release-2.0.2,Negative sampling
Release-2.0.2,used to accumulate the updates for input vectors
Release-2.0.2,window size
Release-2.0.2,skip-gram model
Release-2.0.2,Negative sampling
Release-2.0.2,accumulate for the hidden layer
Release-2.0.2,update output layer
Release-2.0.2,update the hidden layer
Release-2.0.2,update input
Release-2.0.2,update output
Release-2.0.2,some params
Release-2.0.2,batch sentences
Release-2.0.2,max index for node/word
Release-2.0.2,compute number of nodes for one row
Release-2.0.2,check the length of dot values
Release-2.0.2,merge dot values from all partitions
Release-2.0.2,locates the input vectors to local array to prevent randomly access
Release-2.0.2,on the large server row.
Release-2.0.2,fill 0 for context vector
Release-2.0.2,window size
Release-2.0.2,Continuous bag-of-words Models
Release-2.0.2,Accumulate the input vectors from context
Release-2.0.2,Calculate the partial dot values
Release-2.0.2,We should guarantee here that the sample would not equal the ``word``
Release-2.0.2,used to accumulate the context input vectors
Release-2.0.2,locates the input vector into local arrays to prevent randomly access for
Release-2.0.2,the large server row.
Release-2.0.2,window size
Release-2.0.2,while true to prevent sampling out a positive target
Release-2.0.2,how to prevent the randomly access to the output vectors??
Release-2.0.2,accumulate gradients for the input vectors
Release-2.0.2,update output vectors
Release-2.0.2,update input
Release-2.0.2,update output
Release-2.0.2,Some params
Release-2.0.2,compute number of nodes for one row
Release-2.0.2,calculate bias
Release-2.0.2,Do nothing.
Release-2.0.2,current word
Release-2.0.2,neu1 stores the average value of input vectors in the context (CBOW)
Release-2.0.2,Continuous Bag-of-Words Model
Release-2.0.2,Accumulate the input vectors from context
Release-2.0.2,negative sampling
Release-2.0.2,Using the sigmoid value from the pre-computed table
Release-2.0.2,accumulate for the hidden layer
Release-2.0.2,update output layer
Release-2.0.2,add the counter for target
Release-2.0.2,update hidden layer
Release-2.0.2,Update the input vector for each word in the context
Release-2.0.2,add the counter to input
Release-2.0.2,update input layers
Release-2.0.2,update output layers
Release-2.0.2,for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];
Release-2.0.2,set basic configuration keys
Release-2.0.2,use local deploy mode and dummy data spliter
Release-2.0.2,get a angel client
Release-2.0.2,add matrix
Release-2.0.2,TODO Auto-generated constructor stub
Release-2.0.2,row 0 is a random uniform
Release-2.0.2,row 1 is a random normal
Release-2.0.2,row 2 is filled with 1.0
Release-2.0.2,import jdk.nashorn.internal.runtime.regexp.joni.Config;
Release-2.0.2,"paras[1] = ""abc"";"
Release-2.0.2,"paras[2] = ""123"";"
Release-2.0.2,Add standard Hadoop classes
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,set sgd LR algorithm parameters #feature #epoch
Release-2.0.2,Set input data path
Release-2.0.2,Set save model path
Release-2.0.2,Set actionType train
Release-2.0.2,QSLRRunner runner = new QSLRRunner();
Release-2.0.2,runner.train(conf);
Release-2.0.2,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
Release-2.0.2,Dataset
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,class number
Release-2.0.2,Model type
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,Train batch number per epoch.
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set file system
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Use local deploy mode and data format
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set Softmax algorithm parameters
Release-2.0.2,Set model class
Release-2.0.2,Dataset
Release-2.0.2,Data format
Release-2.0.2,Feature number of train data
Release-2.0.2,Tree number
Release-2.0.2,Tree depth
Release-2.0.2,Split number
Release-2.0.2,Feature sample ratio
Release-2.0.2,Ratio of validation
Release-2.0.2,Learning rate
Release-2.0.2,Set file system
Release-2.0.2,Use local deploy mode and data format
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource, #worker, #task, #PS"
Release-2.0.2,Set GBDT algorithm parameters
Release-2.0.2,Dataset
Release-2.0.2,Set file system
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Use local deploy mode
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set DeepFM algorithm parameters
Release-2.0.2,Set model class
Release-2.0.2,Dataset
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Model type
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set file system
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Use local deploy mode and data format
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set LR algorithm parameters
Release-2.0.2,Set model class
Release-2.0.2,Dataset
Release-2.0.2,Data format
Release-2.0.2,Model type
Release-2.0.2,Cluster center number
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Sample ratio per mini-batch
Release-2.0.2,C
Release-2.0.2,Set file system
Release-2.0.2,Use local deploy mode and data format
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource, #worker, #task, #PS"
Release-2.0.2,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.0.2,Dataset
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Model type
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set file system
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Use local deploy mode and data format
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set FM algorithm parameters
Release-2.0.2,Set model class
Release-2.0.2,Dataset
Release-2.0.2,Set file system
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Use local deploy mode
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set WideAndDeep algorithm parameters
Release-2.0.2,Set model class
Release-2.0.2,Dataset
Release-2.0.2,Data format
Release-2.0.2,"Set LDA parameters #V, #K"
Release-2.0.2,Set file system
Release-2.0.2,Use local deploy mode and data format
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource, #worker, #task, #PS"
Release-2.0.2,Set LDA algorithm parameters
Release-2.0.2,Dataset
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Model type
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set file system
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Use local deploy mode and data format
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set SVM algorithm parameters
Release-2.0.2,Set model class
Release-2.0.2,Dataset
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Model type
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,Model is classification
Release-2.0.2,Train batch number per epoch.
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set file system
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Use local deploy mode and data format
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set LR algorithm parameters
Release-2.0.2,Set model class
Release-2.0.2,Dataset
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Model type
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,Model is classification
Release-2.0.2,Train batch number per epoch.
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set file system
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Use local deploy mode and data format
Release-2.0.2,Set data path
Release-2.0.2,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set LR algorithm parameters
Release-2.0.2,Set model class
Release-2.0.2,Load model meta
Release-2.0.2,Convert model
Release-2.0.2,"Get input path, output path"
Release-2.0.2,Init serde
Release-2.0.2,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.0.2,Load model meta
Release-2.0.2,Convert model
Release-2.0.2,load hadoop configuration
Release-2.0.2,"Get input path, output path"
Release-2.0.2,Init serde
Release-2.0.2,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.0.2,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.2,input.seek(rowOffset.getOffset());
Release-2.0.2,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.2,input.seek(rowOffset.getOffset());
Release-2.0.2,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.2,input.seek(rowOffset.getOffset());
Release-2.0.2,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.2,input.seek(rowOffset.getOffset());
Release-2.0.2,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.2,input.seek(rowOffset.getOffset());
Release-2.0.2,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.2,input.seek(rowOffset.getOffset());
Release-2.0.2,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.2,input.seek(rowOffset.getOffset());
Release-2.0.2,Load model meta
Release-2.0.2,Check row type
Release-2.0.2,Load model
Release-2.0.2,Load model meta
Release-2.0.2,Check row type
Release-2.0.2,Load model
Release-2.0.2,Load model meta
Release-2.0.2,Check row type
Release-2.0.2,Load model
Release-2.0.2,Load model meta
Release-2.0.2,Check row type
Release-2.0.2,Load model
Release-2.0.2,Load model meta
Release-2.0.2,Check row type
Release-2.0.2,Load model
Release-2.0.2,Load model meta
Release-2.0.2,Check row type
Release-2.0.2,Load model
Release-2.0.2,Load model meta
Release-2.0.2,Check row type
Release-2.0.2,Load model
Release-2.0.2,Load model
Release-2.0.2,load hadoop configuration
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,System.out.println(content);
Release-2.0.2,https://blog.csdn.net/cocoonyang/article/details/63068108
Release-2.0.2,v1[i] = v1[i] + da * v2[i];
Release-2.0.2,"dgemm(String transa, String transb,"
Release-2.0.2,"int m, int n, int k,"
Release-2.0.2,"double alpha,"
Release-2.0.2,"double[] a, int lda,"
Release-2.0.2,"double[] b, int ldb,"
Release-2.0.2,"double beta,"
Release-2.0.2,"double[] c, int ldc);"
Release-2.0.2,C := alpha*op( A )*op( B ) + beta*C
Release-2.0.2,v1[i] = v1[i] + da * v2[i];
Release-2.0.2,y := alpha*A*x + beta*y
Release-2.0.2,y := alpha*A*x + beta*y
Release-2.0.2,y := alpha*A*x + beta*y
Release-2.0.2,"dgemm(String transa, String transb,"
Release-2.0.2,"int m, int n, int k,"
Release-2.0.2,"double alpha,"
Release-2.0.2,"double[] a, int lda,"
Release-2.0.2,"double[] b, int ldb,"
Release-2.0.2,"double beta,"
Release-2.0.2,"double[] c, int ldc);"
Release-2.0.2,C := alpha*op( A )*op( B ) + beta*C
Release-2.0.2,Default does nothing.
Release-2.0.2,The app injection is optional
Release-2.0.2,"renderText(""hello world"");"
Release-2.0.2,"user choose a workerGroupID from the workergroups page,"
Release-2.0.2,now we should change the AngelApp params and render the workergroup page;
Release-2.0.2,"static final String WORKER_ID = ""worker.id"";"
Release-2.0.2,"div(""#logo"")."
Release-2.0.2,"img(""/static/hadoop-st.png"")._()."
Release-2.0.2,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
Release-2.0.2,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
Release-2.0.2,JQueryUI.jsnotice(html);
Release-2.0.2,import org.apache.hadoop.conf.Configuration;
Release-2.0.2,import java.lang.reflect.Field;
Release-2.0.2,get block locations from file system
Release-2.0.2,create a list of all block and their locations
Release-2.0.2,"if the file is not splitable, just create the one block with"
Release-2.0.2,full file length
Release-2.0.2,each split can be a maximum of maxSize
Release-2.0.2,if remainder is between max and 2*max - then
Release-2.0.2,"instead of creating splits of size max, left-max we"
Release-2.0.2,create splits of size left/2 and left/2. This is
Release-2.0.2,a heuristic to avoid creating really really small
Release-2.0.2,splits.
Release-2.0.2,add this block to the block --> node locations map
Release-2.0.2,"For blocks that do not have host/rack information,"
Release-2.0.2,assign to default  rack.
Release-2.0.2,add this block to the rack --> block map
Release-2.0.2,Add this host to rackToNodes map
Release-2.0.2,add this block to the node --> block map
Release-2.0.2,"if the file system does not have any rack information, then"
Release-2.0.2,use dummy rack location.
Release-2.0.2,The topology paths have the host name included as the last
Release-2.0.2,component. Strip it.
Release-2.0.2,get tokens for all the required FileSystems..
Release-2.0.2,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
Release-2.0.2,job.getConfiguration());
Release-2.0.2,Whether we need to recursive look into the directory structure
Release-2.0.2,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.0.2,user provided one (if any).
Release-2.0.2,all the files in input set
Release-2.0.2,it is allowed for maxSize to be 0. Disable smoothing load for such cases
Release-2.0.2,process all nodes and create splits that are local to a node. Generate
Release-2.0.2,"one split per node iteration, and walk over nodes multiple times to"
Release-2.0.2,distribute the splits across nodes.
Release-2.0.2,Skip the node if it has previously been marked as completed.
Release-2.0.2,"for each block, copy it into validBlocks. Delete it from"
Release-2.0.2,blockToNodes so that the same block does not appear in
Release-2.0.2,two different splits.
Release-2.0.2,Remove all blocks which may already have been assigned to other
Release-2.0.2,splits.
Release-2.0.2,"if the accumulated split size exceeds the maximum, then"
Release-2.0.2,create this split.
Release-2.0.2,create an input split and add it to the splits array
Release-2.0.2,Remove entries from blocksInNode so that we don't walk these
Release-2.0.2,again.
Release-2.0.2,Done creating a single split for this node. Move on to the next
Release-2.0.2,node so that splits are distributed across nodes.
Release-2.0.2,This implies that the last few blocks (or all in case maxSize=0)
Release-2.0.2,were not part of a split. The node is complete.
Release-2.0.2,if there were any blocks left over and their combined size is
Release-2.0.2,"larger than minSplitNode, then combine them into one split."
Release-2.0.2,Otherwise add them back to the unprocessed pool. It is likely
Release-2.0.2,that they will be combined with other blocks from the
Release-2.0.2,same rack later on.
Release-2.0.2,This condition also kicks in when max split size is not set. All
Release-2.0.2,blocks on a node will be grouped together into a single split.
Release-2.0.2,haven't created any split on this machine. so its ok to add a
Release-2.0.2,smaller one for parallelism. Otherwise group it in the rack for
Release-2.0.2,balanced size create an input split and add it to the splits
Release-2.0.2,array
Release-2.0.2,Remove entries from blocksInNode so that we don't walk this again.
Release-2.0.2,The node is done. This was the last set of blocks for this node.
Release-2.0.2,Put the unplaced blocks back into the pool for later rack-allocation.
Release-2.0.2,Node is done. All blocks were fit into node-local splits.
Release-2.0.2,Check if node-local assignments are complete.
Release-2.0.2,All nodes have been walked over and marked as completed or all blocks
Release-2.0.2,have been assigned. The rest should be handled via rackLock assignment.
Release-2.0.2,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
Release-2.0.2,"+ completedNodes.size() + "", size left: "" + totalLength);"
Release-2.0.2,"if blocks in a rack are below the specified minimum size, then keep them"
Release-2.0.2,"in 'overflow'. After the processing of all racks is complete, these"
Release-2.0.2,overflow blocks will be combined into splits.
Release-2.0.2,Process all racks over and over again until there is no more work to do.
Release-2.0.2,Create one split for this rack before moving over to the next rack.
Release-2.0.2,Come back to this rack after creating a single split for each of the
Release-2.0.2,remaining racks.
Release-2.0.2,"Process one rack location at a time, Combine all possible blocks that"
Release-2.0.2,reside on this rack as one split. (constrained by minimum and maximum
Release-2.0.2,split size).
Release-2.0.2,iterate over all racks
Release-2.0.2,"for each block, copy it into validBlocks. Delete it from"
Release-2.0.2,blockToNodes so that the same block does not appear in
Release-2.0.2,two different splits.
Release-2.0.2,"if the accumulated split size exceeds the maximum, then"
Release-2.0.2,create this split.
Release-2.0.2,create an input split and add it to the splits array
Release-2.0.2,"if we created a split, then just go to the next rack"
Release-2.0.2,"if there is a minimum size specified, then create a single split"
Release-2.0.2,"otherwise, store these blocks into overflow data structure"
Release-2.0.2,There were a few blocks in this rack that
Release-2.0.2,remained to be processed. Keep them in 'overflow' block list.
Release-2.0.2,These will be combined later.
Release-2.0.2,Process all overflow blocks
Release-2.0.2,"This might cause an exiting rack location to be re-added,"
Release-2.0.2,but it should be ok.
Release-2.0.2,"if the accumulated split size exceeds the maximum, then"
Release-2.0.2,create this split.
Release-2.0.2,create an input split and add it to the splits array
Release-2.0.2,"Process any remaining blocks, if any."
Release-2.0.2,create an input split
Release-2.0.2,add this split to the list that is returned
Release-2.0.2,long num = totLength / maxSize;
Release-2.0.2,all blocks for all the files in input set
Release-2.0.2,mapping from a rack name to the list of blocks it has
Release-2.0.2,mapping from a block to the nodes on which it has replicas
Release-2.0.2,mapping from a node to the list of blocks that it contains
Release-2.0.2,populate all the blocks for all files
Release-2.0.2,stop all services
Release-2.0.2,1.write application state to file so that the client can get the state of the application
Release-2.0.2,if master exit
Release-2.0.2,2.clear tmp and staging directory
Release-2.0.2,waiting for client to get application state
Release-2.0.2,stop the RPC server
Release-2.0.2,"Security framework already loaded the tokens into current UGI, just use"
Release-2.0.2,them
Release-2.0.2,Now remove the AM->RM token so tasks don't have it
Release-2.0.2,add a shutdown hook
Release-2.0.2,init app state storage
Release-2.0.2,init event dispacher
Release-2.0.2,init location manager
Release-2.0.2,init container allocator
Release-2.0.2,init a rpc service
Release-2.0.2,recover matrix meta if needed
Release-2.0.2,recover ps attempt information if need
Release-2.0.2,Init Client manager
Release-2.0.2,Init PS Client manager
Release-2.0.2,init parameter server manager
Release-2.0.2,recover task information if needed
Release-2.0.2,a dummy data spliter is just for test now
Release-2.0.2,recover data splits information if needed
Release-2.0.2,init worker manager and register worker manager event
Release-2.0.2,register slow worker/ps checker
Release-2.0.2,register app manager event and finish event
Release-2.0.2,Init model saver & loader
Release-2.0.2,start a web service if use yarn deploy mode
Release-2.0.2,load from app state storage first if attempt index great than 1(the master is not the first
Release-2.0.2,retry)
Release-2.0.2,"if load failed, just build a new MatrixMetaManager"
Release-2.0.2,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-2.0.2,is not the first retry)
Release-2.0.2,load task information from app state storage first if attempt index great than 1(the master
Release-2.0.2,is not the first retry)
Release-2.0.2,"if load failed, just build a new AMTaskManager"
Release-2.0.2,load data splits information from app state storage first if attempt index great than 1(the
Release-2.0.2,master is not the first retry)
Release-2.0.2,"if load failed, we need to recalculate the data splits"
Release-2.0.2,Check Workers
Release-2.0.2,Check PSS
Release-2.0.2,Check Clients
Release-2.0.2,Check PS Clients
Release-2.0.2,parse parameter server counters
Release-2.0.2,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.0.2,refresh last heartbeat timestamp
Release-2.0.2,send a state update event to the specific PSAttempt
Release-2.0.2,Check is there save request
Release-2.0.2,Check is there load request
Release-2.0.2,check matrix metadata inconsistencies between master and parameter server.
Release-2.0.2,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
Release-2.0.2,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
Release-2.0.2,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.0.2,choose a unused port
Release-2.0.2,start RPC server
Release-2.0.2,remove this parameter server attempt from monitor set
Release-2.0.2,remove this parameter server attempt from monitor set
Release-2.0.2,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.0.2,find workergroup in worker manager
Release-2.0.2,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
Release-2.0.2,"if this worker group run over, just return WORKERGROUP_EXITED"
Release-2.0.2,"if this worker group is running now, return tasks, workers, data splits for it"
Release-2.0.2,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.0.2,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.0.2,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.0.2,"in ANGEL_PS mode, task id may can not know advance"
Release-2.0.2,update the clock for this matrix
Release-2.0.2,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.0.2,"in ANGEL_PS mode, task id may can not know advance"
Release-2.0.2,update task iteration
Release-2.0.2,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
Release-2.0.2,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
Release-2.0.2,the number of splits equal to the number of tasks
Release-2.0.2,split data
Release-2.0.2,dispatch the splits to workergroups
Release-2.0.2,split data
Release-2.0.2,dispatch the splits to workergroups
Release-2.0.2,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.0.2,"first, then divided by expected split number"
Release-2.0.2,get input format class from configuration and then instantiation a input format object
Release-2.0.2,split data
Release-2.0.2,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.0.2,"first, then divided by expected split number"
Release-2.0.2,get input format class from configuration and then instantiation a input format object
Release-2.0.2,split data
Release-2.0.2,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.0.2,need to fine tune the number of workergroup and task based on the actual split number
Release-2.0.2,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.0.2,Record the location information for the splits in order to data localized schedule
Release-2.0.2,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.0.2,need to fine tune the number of workergroup and task based on the actual split number
Release-2.0.2,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.0.2,Record the location information for the splits in order to data localized schedule
Release-2.0.2,write meta data to a temporary file
Release-2.0.2,rename the temporary file to final file
Release-2.0.2,"if the file exists, read from file and deserialize it"
Release-2.0.2,write task meta
Release-2.0.2,write ps meta
Release-2.0.2,generate a temporary file
Release-2.0.2,write task meta to the temporary file first
Release-2.0.2,rename the temporary file to the final file
Release-2.0.2,"if last final task file exist, remove it"
Release-2.0.2,find task meta file which has max timestamp
Release-2.0.2,"if the file does not exist, just return null"
Release-2.0.2,read task meta from file and deserialize it
Release-2.0.2,generate a temporary file
Release-2.0.2,write ps meta to the temporary file first.
Release-2.0.2,rename the temporary file to the final file
Release-2.0.2,"if the old final file exist, just remove it"
Release-2.0.2,find ps meta file
Release-2.0.2,"if ps meta file does not exist, just return null"
Release-2.0.2,read ps meta from file and deserialize it
Release-2.0.2,"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),"
Release-2.0.2,String.valueOf(requestId));
Release-2.0.2,Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));
Release-2.0.2,saveContext.setTmpSavePath(tmpPath.toString());
Release-2.0.2,Filter old epoch trigger first
Release-2.0.2,Split the user request to sub-requests to pss
Release-2.0.2,Init matrix files meta
Release-2.0.2,Move output files
Release-2.0.2,Write the meta file
Release-2.0.2,Split the user request to sub-requests to pss
Release-2.0.2,check whether psagent heartbeat timeout
Release-2.0.2,Set up the launch command
Release-2.0.2,Duplicate the ByteBuffers for access by multiple containers.
Release-2.0.2,Construct the actual Container
Release-2.0.2,Application resources
Release-2.0.2,Application environment
Release-2.0.2,Service data
Release-2.0.2,Tokens
Release-2.0.2,Set up JobConf to be localized properly on the remote NM.
Release-2.0.2,Setup DistributedCache
Release-2.0.2,Setup up task credentials buffer
Release-2.0.2,LocalStorageToken is needed irrespective of whether security is enabled
Release-2.0.2,or not.
Release-2.0.2,"TokenCache.setJobToken(jobToken, taskCredentials);"
Release-2.0.2,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
Release-2.0.2,Construct the actual Container
Release-2.0.2,The null fields are per-container and will be constructed for each
Release-2.0.2,container separately.
Release-2.0.2,Set up the launch command
Release-2.0.2,Duplicate the ByteBuffers for access by multiple containers.
Release-2.0.2,Construct the actual Container
Release-2.0.2,"a * in the classpath will only find a .jar, so we need to filter out"
Release-2.0.2,all .jars and add everything else
Release-2.0.2,Propagate the system classpath when using the mini cluster
Release-2.0.2,Add standard Hadoop classes
Release-2.0.2,Add mr
Release-2.0.2,Cache archives
Release-2.0.2,Cache files
Release-2.0.2,Sanity check
Release-2.0.2,Add URI fragment or just the filename
Release-2.0.2,Add the env variables passed by the user
Release-2.0.2,Set logging level in the environment.
Release-2.0.2,Setup the log4j prop
Release-2.0.2,Add main class and its arguments
Release-2.0.2,Finally add the jvmID
Release-2.0.2,vargs.add(String.valueOf(jvmID.getId()));
Release-2.0.2,Final commmand
Release-2.0.2,Add the env variables passed by the user
Release-2.0.2,Set logging level in the environment.
Release-2.0.2,Setup the log4j prop
Release-2.0.2,Add main class and its arguments
Release-2.0.2,Final commmand
Release-2.0.2,"if amTask is not null, we should clone task state from it"
Release-2.0.2,"if all parameter server complete commit, master can commit now"
Release-2.0.2,check whether parameter server heartbeat timeout
Release-2.0.2,Transitions from the NEW state.
Release-2.0.2,Transitions from the UNASSIGNED state.
Release-2.0.2,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
Release-2.0.2,Transitions from the ASSIGNED state.
Release-2.0.2,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
Release-2.0.2,PA_CONTAINER_LAUNCHED event
Release-2.0.2,Transitions from the PSAttemptStateInternal.RUNNING state.
Release-2.0.2,Transitions from the PSAttemptStateInternal.COMMITTING state
Release-2.0.2,Transitions from the PSAttemptStateInternal.KILLED state
Release-2.0.2,Transitions from the PSAttemptStateInternal.FAILED state
Release-2.0.2,create the topology tables
Release-2.0.2,reqeuest resource:send a resource request to the resource allocator
Release-2.0.2,"Once the resource is applied, build and send the launch request to the container launcher"
Release-2.0.2,deallocator the resource of the ps attempt:send a resource deallocator request to the
Release-2.0.2,resource allocator
Release-2.0.2,set the launch time
Release-2.0.2,add the ps attempt to the heartbeat timeout monitoring list
Release-2.0.2,parse ps attempt location and put it to location manager
Release-2.0.2,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
Release-2.0.2,or failed
Release-2.0.2,remove ps attempt id from heartbeat timeout monitor list
Release-2.0.2,release container:send a release request to container launcher
Release-2.0.2,set the finish time only if launch time is set
Release-2.0.2,private long scheduledTime;
Release-2.0.2,Transitions from the NEW state.
Release-2.0.2,Transitions from the SCHEDULED state.
Release-2.0.2,Transitions from the RUNNING state.
Release-2.0.2,"another attempt launched,"
Release-2.0.2,Transitions from the SUCCEEDED state
Release-2.0.2,Transitions from the KILLED state
Release-2.0.2,Transitions from the FAILED state
Release-2.0.2,add diagnostic
Release-2.0.2,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.2,Refresh ps location & matrix meta
Release-2.0.2,start a new attempt for this ps
Release-2.0.2,notify ps manager
Release-2.0.2,"getContext().getLocationManager().setPsLocation(id, null);"
Release-2.0.2,add diagnostic
Release-2.0.2,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.2,start a new attempt for this ps
Release-2.0.2,notify ps manager
Release-2.0.2,notify the event handler of state change
Release-2.0.2,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
Release-2.0.2,"if forcedState is set, just return"
Release-2.0.2,else get state from state machine
Release-2.0.2,add this worker group to the success set
Release-2.0.2,check if all worker group run over
Release-2.0.2,add this worker group to the failed set
Release-2.0.2,check if too many worker groups are failed or killed
Release-2.0.2,notify a run failed event
Release-2.0.2,add this worker group to the failed set
Release-2.0.2,check if too many worker groups are failed or killed
Release-2.0.2,notify a run failed event
Release-2.0.2,calculate the actual number of worker groups and the total number of tasks based on the number of data split
Release-2.0.2,"init all tasks , workers and worker groups and put them to the corresponding maps"
Release-2.0.2,just return the total task number now
Release-2.0.2,TODO
Release-2.0.2,check whether worker heartbeat timeout
Release-2.0.2,"if workerAttempt is not null, we should clone task state from it"
Release-2.0.2,from NEW state
Release-2.0.2,from SCHEDULED state
Release-2.0.2,get data splits location for data locality
Release-2.0.2,reqeuest resource:send a resource request to the resource allocator
Release-2.0.2,"once the resource is applied, build and send the launch request to the container launcher"
Release-2.0.2,notify failed message to the worker
Release-2.0.2,notify killed message to the worker
Release-2.0.2,release the allocated container
Release-2.0.2,notify failed message to the worker
Release-2.0.2,remove the worker attempt from heartbeat timeout listen list
Release-2.0.2,release the allocated container
Release-2.0.2,notify killed message to the worker
Release-2.0.2,remove the worker attempt from heartbeat timeout listen list
Release-2.0.2,clean the container
Release-2.0.2,notify failed message to the worker
Release-2.0.2,remove the worker attempt from heartbeat timeout listen list
Release-2.0.2,record the finish time
Release-2.0.2,clean the container
Release-2.0.2,notify killed message to the worker
Release-2.0.2,remove the worker attempt from heartbeat timeout listening list
Release-2.0.2,record the finish time
Release-2.0.2,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
Release-2.0.2,set worker attempt location
Release-2.0.2,notify the register message to the worker
Release-2.0.2,record the launch time
Release-2.0.2,update worker attempt metrics
Release-2.0.2,update tasks metrics
Release-2.0.2,clean the container
Release-2.0.2,notify the worker attempt run successfully message to the worker
Release-2.0.2,record the finish time
Release-2.0.2,init a worker attempt for the worker
Release-2.0.2,schedule the worker attempt
Release-2.0.2,add diagnostic
Release-2.0.2,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.2,init and start a new attempt for this ps
Release-2.0.2,notify worker manager
Release-2.0.2,add diagnostic
Release-2.0.2,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.2,init and start a new attempt for this ps
Release-2.0.2,notify worker manager
Release-2.0.2,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
Release-2.0.2,register to Yarn RM
Release-2.0.2,send heartbeat to Yarn RM every rmPollInterval milliseconds
Release-2.0.2,"catch YarnRuntimeException, we should exit and need not retry"
Release-2.0.2,build heartbeat request
Release-2.0.2,send heartbeat request to rm
Release-2.0.2,"This can happen if the RM has been restarted. If it is in that state,"
Release-2.0.2,this application must clean itself up.
Release-2.0.2,Setting NMTokens
Release-2.0.2,assgin containers
Release-2.0.2,"if some container is not assigned, release them"
Release-2.0.2,handle finish containers
Release-2.0.2,dispatch container exit message to corresponding components
Release-2.0.2,killed by framework
Release-2.0.2,killed by framework
Release-2.0.2,get application finish state
Release-2.0.2,build application diagnostics
Release-2.0.2,TODO:add a job history for angel
Release-2.0.2,build unregister request
Release-2.0.2,send unregister request to rm
Release-2.0.2,Note this down for next interaction with ResourceManager
Release-2.0.2,based on blacklisting comments above we can end up decrementing more
Release-2.0.2,than requested. so guard for that.
Release-2.0.2,send the updated resource request to RM
Release-2.0.2,send 0 container count requests also to cancel previous requests
Release-2.0.2,Update resource requests
Release-2.0.2,try to assign to all nodes first to match node local
Release-2.0.2,try to match all rack local
Release-2.0.2,assign remaining
Release-2.0.2,Update resource requests
Release-2.0.2,send the container-assigned event to task attempt
Release-2.0.2,build the start container request use launch context
Release-2.0.2,send the start request to Yarn nm
Release-2.0.2,send the message that the container starts successfully to the corresponding component
Release-2.0.2,"after launching, send launched event to task attempt to move"
Release-2.0.2,it from ASSIGNED to RUNNING state
Release-2.0.2,send the message that the container starts failed to the corresponding component
Release-2.0.2,kill the remote container if already launched
Release-2.0.2,start a thread pool to startup the container
Release-2.0.2,See if we need up the pool size only if haven't reached the
Release-2.0.2,maximum limit yet.
Release-2.0.2,nodes where containers will run at *this* point of time. This is
Release-2.0.2,*not* the cluster size and doesn't need to be.
Release-2.0.2,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
Release-2.0.2,later is just a buffer so we are not always increasing the
Release-2.0.2,pool-size
Release-2.0.2,the events from the queue are handled in parallel
Release-2.0.2,using a thread pool
Release-2.0.2,return if already stopped
Release-2.0.2,shutdown any containers that might be left running
Release-2.0.2,Add one sync matrix
Release-2.0.2,addSyncMatrix();
Release-2.0.2,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
Release-2.0.2,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
Release-2.0.2,"LOG.info(""ps id = "" + psEntry.getKey());"
Release-2.0.2,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
Release-2.0.2,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
Release-2.0.2,"LOG.info(""matrix id = "" + metaEntry.getKey());"
Release-2.0.2,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
Release-2.0.2,}
Release-2.0.2,}
Release-2.0.2,get matrix ids in the parameter server report
Release-2.0.2,get the matrices parameter server need to create and delete
Release-2.0.2,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
Release-2.0.2,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
Release-2.0.2,Init control connection manager
Release-2.0.2,Get ps locations from master and put them to the location cache.
Release-2.0.2,Build and initialize rpc client to master
Release-2.0.2,Get psagent id
Release-2.0.2,Build PS control rpc client manager
Release-2.0.2,Build local location
Release-2.0.2,Initialize matrix meta information
Release-2.0.2,Start all services
Release-2.0.2,Stop all modules
Release-2.0.2,Stop all modules
Release-2.0.2,clock first
Release-2.0.2,wait
Release-2.0.2,Update generic resource counters
Release-2.0.2,Updating resources specified in ResourceCalculatorProcessTree
Release-2.0.2,Remove the CPU time consumed previously by JVM reuse
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,/ Plus a vector/matrix to the matrix stored in pss
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,/ Update a vector/matrix to the matrix stored in pss
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,/ Get values from pss use row/column indices
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"/ PSF get/update, use can implement their own psf"
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,/ Get a row or a batch of rows
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,TODO:
Release-2.0.2,Generate a flush request and put it to request queue
Release-2.0.2,Generate a clock request and put it to request queue
Release-2.0.2,Generate a merge request and put it to request queue
Release-2.0.2,Generate a merge request and put it to request queue
Release-2.0.2,"If the matrix op log cache does not exist for the matrix, create a new one for the"
Release-2.0.2,matrix
Release-2.0.2,and add it to cache maps
Release-2.0.2,Add the message to the tree map
Release-2.0.2,"If there are flush / clock requests blocked, we need to put this merge request into"
Release-2.0.2,the waiting queue
Release-2.0.2,Launch a merge worker to merge the update to matrix op log cache
Release-2.0.2,Remove the message from the tree map
Release-2.0.2,Wake up blocked flush/clock request
Release-2.0.2,Add flush/clock request to listener list to waiting for all the existing
Release-2.0.2,updates are merged
Release-2.0.2,Wake up blocked flush/clock request
Release-2.0.2,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
Release-2.0.2,blocked.
Release-2.0.2,Get next merge message sequence id
Release-2.0.2,Wake up listeners(flush/clock requests) that have little sequence id than current merge
Release-2.0.2,position
Release-2.0.2,Wake up blocked merge requests
Release-2.0.2,Get minimal sequence id from listeners
Release-2.0.2,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
Release-2.0.2,should flush updates to local matrix storage
Release-2.0.2,unused now
Release-2.0.2,TODO:
Release-2.0.2,Doing average or not
Release-2.0.2,Filter un-important update
Release-2.0.2,Split this row according the matrix partitions
Release-2.0.2,Set split context
Release-2.0.2,Remove the row from matrix
Release-2.0.2,buf.writeDouble(0.0);
Release-2.0.2,TODO
Release-2.0.2,TODO: write map default value
Release-2.0.2,buf.writeDouble(0);
Release-2.0.2,TODO:
Release-2.0.2,TODO:
Release-2.0.2,TODO:
Release-2.0.2,"For each partition, we generate a update split."
Release-2.0.2,"Although the split is empty for partitions those without any update data,"
Release-2.0.2,we still need to generate a update split to update the clock info on ps.
Release-2.0.2,"For each partition, we generate a update split."
Release-2.0.2,"Although the split is empty for partitions those without any update data,"
Release-2.0.2,we still need to generate a update split to update the clock info on ps.
Release-2.0.2,"For each partition, we generate a update split."
Release-2.0.2,"Although the split is empty for partitions those without any update data,"
Release-2.0.2,we still need to generate a update split to update the clock info on ps.
Release-2.0.2,"For each partition, we generate a update split."
Release-2.0.2,"Although the split is empty for partitions those without any update data,"
Release-2.0.2,we still need to generate a update split to update the clock info on ps.
Release-2.0.2,"For each partition, we generate a update split."
Release-2.0.2,"Although the split is empty for partitions those without any update data,"
Release-2.0.2,we still need to generate a update split to update the clock info on ps.
Release-2.0.2,"For each partition, we generate a update split."
Release-2.0.2,"Although the split is empty for partitions those without any update data,"
Release-2.0.2,we still need to generate a update split to update the clock info on ps.
Release-2.0.2,"LOG.error(""channel "" + ctx.channel() + "" inactive"");"
Release-2.0.2,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
Release-2.0.2,"LOG.error(""put response message queue failed "", e);"
Release-2.0.2,Use Epoll for linux
Release-2.0.2,Update location table
Release-2.0.2,Remove the server from failed list
Release-2.0.2,Notify refresh success message to request dispatcher
Release-2.0.2,Check PS exist or not
Release-2.0.2,Check heartbeat timeout
Release-2.0.2,Check PS restart or not
Release-2.0.2,private final HashSet<ParameterServerId> refreshingServerSet;
Release-2.0.2,Add it to failed rpc list
Release-2.0.2,Add the server to gray server list
Release-2.0.2,Add it to failed rpc list
Release-2.0.2,Add the server to gray server list
Release-2.0.2,Move from gray server list to failed server list
Release-2.0.2,Handle the RPCS to this server
Release-2.0.2,Submit the schedulable failed get RPCS
Release-2.0.2,Submit new get RPCS
Release-2.0.2,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.0.2,"If the queue is empty, just return 0"
Release-2.0.2,"If request is not over limit, just submit it"
Release-2.0.2,Submit the schedulable failed get RPCS
Release-2.0.2,Submit new put RPCS
Release-2.0.2,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.0.2,"LOG.info(""choose put server "" + psIds[index]);"
Release-2.0.2,Check all pending RPCS
Release-2.0.2,Check get channel context
Release-2.0.2,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.0.2,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.0.2,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.0.2,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.0.2,channelManager.printPools();
Release-2.0.2,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
Release-2.0.2,if(ts - entry.getValue() > requestTimeOut * 2)  {
Release-2.0.2,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
Release-2.0.2,"+ "" milliseconds, close all channels to it"");"
Release-2.0.2,closeChannels(entry.getKey());
Release-2.0.2,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
Release-2.0.2,}
Release-2.0.2,}
Release-2.0.2,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
Release-2.0.2,Remove all pending RPCS
Release-2.0.2,Close all channel to this PS
Release-2.0.2,private Channel getChannel(Location loc) throws Exception {
Release-2.0.2,"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));"
Release-2.0.2,}
Release-2.0.2,private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {
Release-2.0.2,"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext"
Release-2.0.2,.get()
Release-2.0.2,.getConf()
Release-2.0.2,".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,"
Release-2.0.2,AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));
Release-2.0.2,}
Release-2.0.2,"LOG.error(""send request "" + request + "" is interrupted"");"
Release-2.0.2,"LOG.error(""send request "" + request + "" failed, "", e);"
Release-2.0.2,Get server id and location for this request
Release-2.0.2,"If location is null, means that the server is not ready"
Release-2.0.2,Get the channel for the location
Release-2.0.2,Check if need get token first
Release-2.0.2,Serialize the request
Release-2.0.2,Send the request
Release-2.0.2,get a channel to server from pool
Release-2.0.2,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.0.2,request.getContext().setChannelPool(pool);
Release-2.0.2,Allocate the bytebuf and serialize the request
Release-2.0.2,find the partition request context from cache
Release-2.0.2,"updateMatrixCache(request.getPartKey(), response.getPartition());"
Release-2.0.2,"updateMatrixCache(request.getPartKey(), response.getRowsSplit());"
Release-2.0.2,TODO
Release-2.0.2,ServerRow rowSplit = PSAgentContext.get().getMatricesCache()
Release-2.0.2,".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),"
Release-2.0.2,request.getRowIndex());
Release-2.0.2,response.setRowSplit(rowSplit);
Release-2.0.2,"updateMatrixCache(request.getPartKey(), response.getRowSplit());"
Release-2.0.2,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.0.2,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.0.2,TODO
Release-2.0.2,"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {"
Release-2.0.2,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);"
Release-2.0.2,}
Release-2.0.2,"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {"
Release-2.0.2,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);"
Release-2.0.2,}
Release-2.0.2,"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {"
Release-2.0.2,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);"
Release-2.0.2,}
Release-2.0.2,Get partitions for this row
Release-2.0.2,Distinct get row requests
Release-2.0.2,Need get from ps or storage/cache
Release-2.0.2,"Switch to new request id, send a new request"
Release-2.0.2,First get this row from matrix storage
Release-2.0.2,MatrixStorage matrixStorage =
Release-2.0.2,PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);
Release-2.0.2,TVector row = matrixStorage.getRow(rowIndex);
Release-2.0.2,if (row != null && row.getClock() >= clock) {
Release-2.0.2,result.set(row);
Release-2.0.2,return row;
Release-2.0.2,}
Release-2.0.2,Get row splits of this row from the matrix cache first
Release-2.0.2,responseCache.addSubResponse(rowSplit);
Release-2.0.2,"If the row split does not exist in cache, get it from parameter server"
Release-2.0.2,Wait the final result
Release-2.0.2,Put it to the matrix cache
Release-2.0.2,"matrixStorage.addRow(rowIndex, row);"
Release-2.0.2,Just wait result
Release-2.0.2,Split the param use matrix partitions
Release-2.0.2,Send request to PSS
Release-2.0.2,Split the matrix oplog according to the matrix partitions
Release-2.0.2,"If need update clock, we should send requests to all partitions"
Release-2.0.2,Send request to PSS
Release-2.0.2,Filter the rowIds which are fetching now
Release-2.0.2,Send the rowIndex to rpc dispatcher and return immediately
Release-2.0.2,"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));"
Release-2.0.2,"LOG.info(""start to request "" + requestId);"
Release-2.0.2,"LOG.info(""start to request "" + requestId);"
Release-2.0.2,Split param use matrix partitons
Release-2.0.2,"If all sub-results are received, just remove request and result cache"
Release-2.0.2,"LOG.info(""request = "" + request + "", cache = "" + cache);"
Release-2.0.2,"LOG.info(""start to merge "" + cache + "" for request "" + request);"
Release-2.0.2,"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.0.2,Split this row according the matrix partitions
Release-2.0.2,Set split context
Release-2.0.2,Split this row according the matrix partitions
Release-2.0.2,Set split context
Release-2.0.2,long startTs = System.currentTimeMillis();
Release-2.0.2,"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.0.2,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
Release-2.0.2,Generate dispatch items and add them to the corresponding queues
Release-2.0.2,Filter the rowIds which are fetching now
Release-2.0.2,Sort the parts by partitionId
Release-2.0.2,Sort partition keys use start column index
Release-2.0.2,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.0.2,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.0.2,});
Release-2.0.2,"For each partition, we generate a update split."
Release-2.0.2,"Although the split is empty for partitions those without any update data,"
Release-2.0.2,we still need to generate a update split to update the clock info on ps.
Release-2.0.2,Sort the parts by partitionId
Release-2.0.2,Sort partition keys use start column index
Release-2.0.2,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.0.2,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.0.2,});
Release-2.0.2,"For each partition, we generate a update split."
Release-2.0.2,"Although the split is empty for partitions those without any update data,"
Release-2.0.2,we still need to generate a update split to update the clock info on ps.
Release-2.0.2,Put the row split to the cache(row index to row splits map)
Release-2.0.2,"If all splits of the row are received, means this row can be merged"
Release-2.0.2,TODO
Release-2.0.2,TODO
Release-2.0.2,/////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,TODO
Release-2.0.2,buf.writeDouble(0);
Release-2.0.2,TODO
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,Now we just support pipelined row splits merging for dense type row
Release-2.0.2,Pre-fetching is disable default
Release-2.0.2,matrix id to clock map
Release-2.0.2,"task index, it must be unique for whole application"
Release-2.0.2,Deserialize data splits meta
Release-2.0.2,Get workers
Release-2.0.2,Send request to every ps
Release-2.0.2,Wait the responses
Release-2.0.2,Update clock cache
Release-2.0.2,if(syncNum % 1024 == 0) {
Release-2.0.2,}
Release-2.0.2,"Use simple flow, do not use any cache"
Release-2.0.2,Get row from cache.
Release-2.0.2,"if row clock is satisfy ssp staleness limit, just return."
Release-2.0.2,Get row from ps.
Release-2.0.2,Wait until the clock value of this row is greater than or equal to the value
Release-2.0.2,"For ASYNC mode, just get from pss."
Release-2.0.2,"For BSP/SSP, get rows from storage/cache first"
Release-2.0.2,Get from ps.
Release-2.0.2,Wait until the clock value of this row is greater than or equal to the value
Release-2.0.2,"For ASYNC, just get rows from pss."
Release-2.0.2,no more retries.
Release-2.0.2,calculate sleep time and return.
Release-2.0.2,parse the i-th sleep-time
Release-2.0.2,parse the i-th number-of-retries
Release-2.0.2,calculateSleepTime may overflow.
Release-2.0.2,"A few common retry policies, with no delays."
Release-2.0.2,Read matrix meta from meta file
Release-2.0.2,Save partitions to files use fork-join
Release-2.0.2,Write the ps matrix meta to the meta file
Release-2.0.2,matrix.startServering();
Release-2.0.2,return;
Release-2.0.2,Read matrix meta from meta file
Release-2.0.2,Load partitions from file use fork-join
Release-2.0.2,Read matrix meta from meta file
Release-2.0.2,Sort partitions
Release-2.0.2,int size = rows.length;
Release-2.0.2,int size = rows.length;
Release-2.0.2,int size = rows.size();
Release-2.0.2,int size = rows.size();
Release-2.0.2,int size = rows.size();
Release-2.0.2,int size = rows.size();
Release-2.0.2,int size = rows.size();
Release-2.0.2,int size = rows.size();
Release-2.0.2,close is a local operation and should finish within milliseconds; timeout just to be safe
Release-2.0.2,response will be null for one way messages.
Release-2.0.2,maxFrameLength = 2G
Release-2.0.2,lengthFieldOffset = 0
Release-2.0.2,lengthFieldLength = 8
Release-2.0.2,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
Release-2.0.2,"initialBytesToStrip = 8, i.e. strip out the length field itself"
Release-2.0.2,"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())"
Release-2.0.2,.toString();
Release-2.0.2,indicates whether this connection's life cycle is managed
Release-2.0.2,See if we already have a connection (common case)
Release-2.0.2,create a unique lock for this RS + protocol (if necessary)
Release-2.0.2,get the RS lock
Release-2.0.2,do one more lookup in case we were stalled above
Release-2.0.2,Only create isa when we need to.
Release-2.0.2,definitely a cache miss. establish an RPC for
Release-2.0.2,this RS
Release-2.0.2,Throw what the RemoteException was carrying.
Release-2.0.2,check
Release-2.0.2,every
Release-2.0.2,minutes
Release-2.0.2,TODO
Release-2.0.2,创建failoverHandler
Release-2.0.2,"The number of times this invocation handler has ever been failed over,"
Release-2.0.2,before this method invocation attempt. Used to prevent concurrent
Release-2.0.2,failed method invocations from triggering multiple failover attempts.
Release-2.0.2,Make sure that concurrent failed method invocations
Release-2.0.2,only cause a
Release-2.0.2,single actual fail over.
Release-2.0.2,RpcController + Message in the method args
Release-2.0.2,(generated code from RPC bits in .proto files have
Release-2.0.2,RpcController)
Release-2.0.2,"LOG.info(""method "" + method.getName() + ""construct request time = """
Release-2.0.2,+ (System.currentTimeMillis() - beforeConstructTs));
Release-2.0.2,get an instance of the method arg type
Release-2.0.2,RpcController + Message in the method args
Release-2.0.2,(generated code from RPC bits in .proto files have
Release-2.0.2,RpcController)
Release-2.0.2,Message (hand written code usually has only a single
Release-2.0.2,argument)
Release-2.0.2,log any RPC responses that are slower than the configured
Release-2.0.2,warn
Release-2.0.2,response time or larger than configured warning size
Release-2.0.2,"when tagging, we let TooLarge trump TooSmall to keep"
Release-2.0.2,output simple
Release-2.0.2,note that large responses will often also be slow.
Release-2.0.2,provides a count of log-reported slow responses
Release-2.0.2,RpcController + Message in the method args
Release-2.0.2,(generated code from RPC bits in .proto files have
Release-2.0.2,RpcController)
Release-2.0.2,unexpected
Release-2.0.2,"in the protobuf methods, args[1] is the only significant argument"
Release-2.0.2,for JSON encoding
Release-2.0.2,base information that is reported regardless of type of call
Release-2.0.2,Disable Nagle's Algorithm since we don't want packets to wait
Release-2.0.2,Configure the event pipeline factory.
Release-2.0.2,Make a new connection.
Release-2.0.2,Remove all pending requests (will be canceled after relinquishing
Release-2.0.2,write lock).
Release-2.0.2,Cancel any pending requests by sending errors to the callbacks:
Release-2.0.2,Close the channel:
Release-2.0.2,Close the connection:
Release-2.0.2,Shut down all thread pools to exit.
Release-2.0.2,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
Release-2.0.2,See NettyServer.prepareResponse for where we write out the response.
Release-2.0.2,"It writes the call.id (int), a boolean signifying any error (and if"
Release-2.0.2,"so the exception name/trace), and the response bytes"
Release-2.0.2,Read the call id.
Release-2.0.2,"When the stream is closed, protobuf doesn't raise an EOFException,"
Release-2.0.2,"instead, it returns a null message object."
Release-2.0.2,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
Release-2.0.2,System.currentTimeMillis());
Release-2.0.2,"It would be good widen this to just Throwable, but IOException is what we"
Release-2.0.2,allow now
Release-2.0.2,not implemented
Release-2.0.2,not implemented
Release-2.0.2,"track what RpcEngine is used by a proxy class, for stopProxy()"
Release-2.0.2,cache of RpcEngines by protocol
Release-2.0.2,return the RpcEngine configured to handle a protocol
Release-2.0.2,We only handle the ConnectException.
Release-2.0.2,This is the exception we can't handle.
Release-2.0.2,check if timed out
Release-2.0.2,wait for retry
Release-2.0.2,IGNORE
Release-2.0.2,return the RpcEngine that handles a proxy object
Release-2.0.2,The default implementation works synchronously
Release-2.0.2,punt: allocate a new buffer & copy into it
Release-2.0.2,Parse cmd parameters
Release-2.0.2,load hadoop configuration
Release-2.0.2,load angel system configuration
Release-2.0.2,load user configuration:
Release-2.0.2,load user config file
Release-2.0.2,load command line parameters
Release-2.0.2,load user job resource files
Release-2.0.2,load ml conf file for graph based algorithm
Release-2.0.2,load user job jar if it exist
Release-2.0.2,Expand the environment variable
Release-2.0.2,Add default fs(local fs) for lib jars.
Release-2.0.2,"LOG.info(System.getProperty(""user.dir""));"
Release-2.0.2,get tokens for all the required FileSystems..
Release-2.0.2,Whether we need to recursive look into the directory structure
Release-2.0.2,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.0.2,user provided one (if any).
Release-2.0.2,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.0.2,get tokens for all the required FileSystems..
Release-2.0.2,Whether we need to recursive look into the directory structure
Release-2.0.2,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.0.2,user provided one (if any).
Release-2.0.2,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.0.2,a simple hdfs copy function assume src path and dest path are in same hdfs
Release-2.0.2,and FileSystem object has same schema
Release-2.0.2,"LOG.warn(""interrupted while sleeping"", ie);"
Release-2.0.2,public static String getHostname() {
Release-2.0.2,try {
Release-2.0.2,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
Release-2.0.2,} catch (UnknownHostException uhe) {
Release-2.0.2,}
Release-2.0.2,"return new StringBuilder().append("""").append(uhe).toString();"
Release-2.0.2,}
Release-2.0.2,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
Release-2.0.2,String hostname = getHostname();
Release-2.0.2,String classname = clazz.getSimpleName();
Release-2.0.2,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
Release-2.0.2,"StringBuilder().append(""Starting "").append(classname).toString(), new"
Release-2.0.2,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
Release-2.0.2,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
Release-2.0.2,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
Release-2.0.2,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
Release-2.0.2,
Release-2.0.2,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
Release-2.0.2,public void run() {
Release-2.0.2,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
Release-2.0.2,"this.val$classname + "" at "" + this.val$hostname}));"
Release-2.0.2,}
Release-2.0.2,});
Release-2.0.2,}
Release-2.0.2,"We we interrupted because we're meant to stop? If not, just"
Release-2.0.2,continue ignoring the interruption
Release-2.0.2,Recalculate waitTime.
Release-2.0.2,// Begin delegation to Thread
Release-2.0.2,// End delegation to Thread
Release-2.0.2,instance submitter class
Release-2.0.2,Obtain filename from path
Release-2.0.2,Split filename to prexif and suffix (extension)
Release-2.0.2,Check if the filename is okay
Release-2.0.2,Prepare temporary file
Release-2.0.2,Prepare buffer for data copying
Release-2.0.2,Open and check input stream
Release-2.0.2,Open output stream and copy data between source file in JAR and the temporary file
Release-2.0.2,"If read/write fails, close streams safely before throwing an exception"
Release-2.0.2,"Finally, load the library"
Release-2.0.2,little endian load order
Release-2.0.2,tail
Release-2.0.2,fallthrough
Release-2.0.2,fallthrough
Release-2.0.2,finalization
Release-2.0.2,fmix(h1);
Release-2.0.2,----------
Release-2.0.2,body
Release-2.0.2,----------
Release-2.0.2,tail
Release-2.0.2,----------
Release-2.0.2,finalization
Release-2.0.2,----------
Release-2.0.2,body
Release-2.0.2,----------
Release-2.0.2,tail
Release-2.0.2,----------
Release-2.0.2,finalization
Release-2.0.2,throw new AngelException(e);
Release-2.0.2,JobStateProto jobState = report.getJobState();
Release-2.0.2,Check need load matrices
Release-2.0.2,Used for java code to get a AngelClient instance
Release-2.0.2,Used for python code to get a AngelClient instance
Release-2.0.2,load user job resource files
Release-2.0.2,the leaf level file should be readable by others
Release-2.0.2,the subdirs in the path should have execute permissions for
Release-2.0.2,others
Release-2.0.2,2.get job id
Release-2.0.2,Credentials credentials = new Credentials();
Release-2.0.2,4.copy resource files to hdfs
Release-2.0.2,5.write configuration to a xml file
Release-2.0.2,6.create am container context
Release-2.0.2,7.Submit to ResourceManager
Release-2.0.2,8.get app master client
Release-2.0.2,Create a number of filenames in the JobTracker's fs namespace
Release-2.0.2,add all the command line files/ jars and archive
Release-2.0.2,first copy them to jobtrackers filesystem
Release-2.0.2,should not throw a uri exception
Release-2.0.2,should not throw an uri excpetion
Release-2.0.2,set the timestamps of the archives and files
Release-2.0.2,set the public/private visibility of the archives and files
Release-2.0.2,get DelegationToken for each cached file
Release-2.0.2,check if we do not need to copy the files
Release-2.0.2,is jt using the same file system.
Release-2.0.2,just checking for uri strings... doing no dns lookups
Release-2.0.2,to see if the filesystems are the same. This is not optimal.
Release-2.0.2,but avoids name resolution.
Release-2.0.2,this might have name collisions. copy will throw an exception
Release-2.0.2,parse the original path to create new path
Release-2.0.2,check for ports
Release-2.0.2,Write job file to JobTracker's fs
Release-2.0.2,Setup resource requirements
Release-2.0.2,Setup LocalResources
Release-2.0.2,Setup security tokens
Release-2.0.2,Setup the command to run the AM
Release-2.0.2,Add AM user command opts
Release-2.0.2,Final command
Release-2.0.2,Setup the CLASSPATH in environment
Release-2.0.2,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
Release-2.0.2,Setup the environment variables for Admin first
Release-2.0.2,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
Release-2.0.2,Parse distributed cache
Release-2.0.2,Setup ContainerLaunchContext for AM container
Release-2.0.2,Set up the ApplicationSubmissionContext
Release-2.0.2,private volatile PS2PSPusherImpl ps2PSPusher;
Release-2.0.2,TODO
Release-2.0.2,Add tokens to new user so that it may execute its task correctly.
Release-2.0.2,TODO
Release-2.0.2,to exit
Release-2.0.2,TODO
Release-2.0.2,TODO
Release-2.0.2,ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));
Release-2.0.2,context.getSnapshotManager().processRecovery();
Release-2.0.2,Recover PS from snapshot or load path
Release-2.0.2,First check snapshot
Release-2.0.2,Check load path setting
Release-2.0.2,TODO
Release-2.0.2,if(ps2PSPusher != null) {
Release-2.0.2,ps2PSPusher.start();
Release-2.0.2,}
Release-2.0.2,public PS2PSPusherImpl getPs2PSPusher() {
Release-2.0.2,return ps2PSPusher;
Release-2.0.2,}
Release-2.0.2,"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");"
Release-2.0.2,"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);"
Release-2.0.2,Release the input buffer
Release-2.0.2,Release the input buffer
Release-2.0.2,"1. handle the rpc, get the response"
Release-2.0.2,Release the input buffer
Release-2.0.2,2. Serialize the response
Release-2.0.2,Send the serialized response
Release-2.0.2,Exception happened
Release-2.0.2,write seq id
Release-2.0.2,Just serialize the head
Release-2.0.2,Exception happened
Release-2.0.2,Allocate result buffer
Release-2.0.2,Exception happened
Release-2.0.2,Just serialize the head
Release-2.0.2,Exception happened
Release-2.0.2,Reset the response and allocate buffer again
Release-2.0.2,Get partition and check the partition state
Release-2.0.2,Get the stored pss for this partition
Release-2.0.2,"Check this ps is the master ps for this location, only master ps can accept the update"
Release-2.0.2,Check the partition state again
Release-2.0.2,Start to put the update to the slave pss
Release-2.0.2,TODO
Release-2.0.2,"context.getPS2PSPusher().put(request, in, partLoc);"
Release-2.0.2,Get partition and check the partition state
Release-2.0.2,Get the stored pss for this partition
Release-2.0.2,"Check this ps is the master ps for this partition, if not, just return failed"
Release-2.0.2,Start to put the update to the slave pss
Release-2.0.2,TODO
Release-2.0.2,return ServerState.GENERAL;
Release-2.0.2,Use Epoll for linux
Release-2.0.2,public String uuid;
Release-2.0.2,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
Release-2.0.2,this.channelPool = channelPool;
Release-2.0.2,}
Release-2.0.2,private final ParameterServer psServer;
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO:default value
Release-2.0.2,buf.readDouble();
Release-2.0.2,TODO
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"////// network io method, for model transform"
Release-2.0.2,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,public ObjectIterator<Long2FloatMap.Entry> getIter() {
Release-2.0.2,return ((LongFloatVector) row).getStorage().entryIterator();
Release-2.0.2,}
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,public ObjectIterator<Long2LongMap.Entry> getIter() {
Release-2.0.2,return ((LongLongVector) row).getStorage().entryIterator();
Release-2.0.2,}
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.2,and call endWrite/endRead after
Release-2.0.2,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.2,TODO: dynamic add/delete row
Release-2.0.2,private final List<PartitionKey> partitionKeys;
Release-2.0.2,Use Epoll for linux
Release-2.0.2,find the partition request context from cache
Release-2.0.2,get a channel to server from pool
Release-2.0.2,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.0.2,channelManager.removeChannelPool(loc);
Release-2.0.2,Generate seq id
Release-2.0.2,Create a RecoverPartRequest
Release-2.0.2,Serialize the request
Release-2.0.2,Change the seqId for the request
Release-2.0.2,Serialize the request
Release-2.0.2,"First check the state of the channels in the pool, if a channel is unused, just return"
Release-2.0.2,"If all channels are in use, create a new channel or wait"
Release-2.0.2,Create a new channel
Release-2.0.2,"add the PSAgentContext,need fix"
Release-2.0.2,TODO:add more vector type
Release-2.0.2,TODO : subDim set
Release-2.0.2,Sort the parts by partitionId
Release-2.0.2,Sort partition keys use start column index
Release-2.0.2,"For each partition, we generate a update split."
Release-2.0.2,"Although the split is empty for partitions those without any update data,"
Release-2.0.2,we still need to generate a update split to update the clock info on ps.
Release-2.0.2,Sort the parts by partitionId
Release-2.0.2,Sort partition keys use start column index
Release-2.0.2,"For each partition, we generate a update split."
Release-2.0.2,"Although the split is empty for partitions those without any update data,"
Release-2.0.2,we still need to generate a update split to update the clock info on ps.
Release-2.0.2,write the max abs
Release-2.0.2,---------------------------------------------------
Release-2.0.2,---------------------------------------------------
Release-2.0.2,---------------------------------------------------------------
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,axis = 0: on rows
Release-2.0.2,axis = 1: on cols
Release-2.0.2,1. find the insert point
Release-2.0.2,2. check the capacity and insert
Release-2.0.2,3. increase size
Release-2.0.2,1. find the insert point
Release-2.0.2,2. check the capacity and insert
Release-2.0.2,3. increase size
Release-2.0.2,1. find the insert point
Release-2.0.2,2. check the capacity and insert
Release-2.0.2,3. increase size
Release-2.0.2,1. find the insert point
Release-2.0.2,2. check the capacity and insert
Release-2.0.2,3. increase size
Release-2.0.2,1. find the insert point
Release-2.0.2,2. check the capacity and insert
Release-2.0.2,3. increase size
Release-2.0.2,1. find the insert point
Release-2.0.2,2. check the capacity and insert
Release-2.0.2,3. increase size
Release-2.0.2,1. find the insert point
Release-2.0.2,2. check the capacity and insert
Release-2.0.2,3. increase size
Release-2.0.2,1. find the insert point
Release-2.0.2,2. check the capacity and insert
Release-2.0.2,3. increase size
Release-2.0.2,-----------------
Release-2.0.2,-----------------
Release-2.0.2,-----------------
Release-2.0.2,-----------------
Release-2.0.2,-----------------
Release-2.0.2,KeepStorage is guaranteed
Release-2.0.2,"ignore the isInplace option, since v2 is dense"
Release-2.0.2,"the value in old storage can be changed safe, so switch a storage"
Release-2.0.2,"but user required keep storage, we can prevent rehash"
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,KeepStorage is guaranteed
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,v1Size < v2Size * Constant.sparseThreshold
Release-2.0.2,KeepStorage is guaranteed
Release-2.0.2,"ignore the isInplace option, since v2 is dense"
Release-2.0.2,"the value in old storage can be changed safe, so switch a storage"
Release-2.0.2,"but user required keep storage, we can prevent rehash"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,prevent rehash
Release-2.0.2,KeepStorage is guaranteed
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,dense preferred
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,sorted preferred
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,multi-rehash
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"slower but memory efficient, for small vector only"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"slower but memory efficient, for small vector only"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"slower but memory efficient, for small vector only"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"slower but memory efficient, for small vector only"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"slower but memory efficient, for small vector only"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"slower but memory efficient, for small vector only"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"slower but memory efficient, for small vector only"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"slower but memory efficient, for small vector only"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"slower but memory efficient, for small vector only"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"slower but memory efficient, for small vector only"
Release-2.0.2,"dense preferred, KeepStorage is guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,"sparse preferred, keep storage guaranteed"
Release-2.0.2,preferred dense
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,we gauss dense storage is more efficient
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.2,multi-rehash
Release-2.0.2,"we gauss the indices of v2 maybe is a subset of v1, or overlap is very large"
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,to avoid multi-rehash
Release-2.0.2,"Transform mat1, generate a new matrix"
Release-2.0.2,Split the row indices of mat1Trans
Release-2.0.2,Parallel execute use fork-join
Release-2.0.2,"Get the sub-matrix of left matrix, split by row"
Release-2.0.2,"Transform mat1, generate a new matrix"
Release-2.0.2,Split the row indices of mat1Trans
Release-2.0.2,Parallel execute use fork-join
Release-2.0.2,"Get the sub-matrix of left matrix, split by row"
Release-2.0.2,"mat1 trans true, mat trans true"
Release-2.0.2,"mat1 trans true, mat trans false"
Release-2.0.2,"mat1 trans false, mat trans true, important"
Release-2.0.2,"mat1 trans false, mat trans false"
Release-2.0.2,"mat1 trans true, mat trans true"
Release-2.0.2,"mat1 trans true, mat trans false"
Release-2.0.2,"mat1 trans false, mat trans true, important"
Release-2.0.2,"mat1 trans false, mat trans false"
Release-2.0.2,"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.0.2,"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.0.2,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.0.2,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.0.2,add dense double matrix
Release-2.0.2,TODO Auto-generated method stub
Release-2.0.2,TODO Auto-generated method stub
Release-2.0.2,TODO Auto-generated method stub
Release-2.0.2,get configuration from config file
Release-2.0.2,set localDir with enviroment set by nm.
Release-2.0.2,get master location
Release-2.0.2,init task manager and start tasks
Release-2.0.2,start heartbeat thread
Release-2.0.2,taskManager.assignTaskIds(response.getTaskidsList());
Release-2.0.2,todo
Release-2.0.2,"if worker timeout, it may be knocked off."
Release-2.0.2,"SUCCESS, do nothing"
Release-2.0.2,heartbeatFailedTime = 0;
Release-2.0.2,private KEY currentKey;
Release-2.0.2,will be created
Release-2.0.2,TODO Auto-generated method stub
Release-2.0.2,Bitmap bitmap = new Bitmap();
Release-2.0.2,int max = indexArray[size - 1];
Release-2.0.2,byte [] bitIndexArray = new byte[max / 8 + 1];
Release-2.0.2,for(int i = 0; i < size; i++){
Release-2.0.2,int bitIndex = indexArray[i] >> 3;
Release-2.0.2,int bitOffset = indexArray[i] - (bitIndex << 3);
Release-2.0.2,switch(bitOffset){
Release-2.0.2,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
Release-2.0.2,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
Release-2.0.2,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
Release-2.0.2,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
Release-2.0.2,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
Release-2.0.2,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
Release-2.0.2,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
Release-2.0.2,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
Release-2.0.2,}
Release-2.0.2,}
Release-2.0.2,"true, false"
Release-2.0.2,//////////////////////////////
Release-2.0.2,Application Configs
Release-2.0.2,//////////////////////////////
Release-2.0.2,//////////////////////////////
Release-2.0.2,Master Configs
Release-2.0.2,//////////////////////////////
Release-2.0.2,//////////////////////////////
Release-2.0.2,Worker Configs
Release-2.0.2,//////////////////////////////
Release-2.0.2,//////////////////////////////
Release-2.0.2,Task Configs
Release-2.0.2,//////////////////////////////
Release-2.0.2,//////////////////////////////
Release-2.0.2,ParameterServer Configs
Release-2.0.2,//////////////////////////////
Release-2.0.2,////////////////// IPC //////////////////////////
Release-2.0.2,//////////////////////////////
Release-2.0.2,Matrix transfer Configs.
Release-2.0.2,//////////////////////////////
Release-2.0.2,//////////////////////////////
Release-2.0.2,Matrix transfer Configs.
Release-2.0.2,//////////////////////////////
Release-2.0.2,Configs used to ANGEL_PS_PSAGENT running mode future.
Release-2.0.2,model parse
Release-2.0.2,Mark whether use pyangel or not.
Release-2.0.2,private Configuration conf;
Release-2.0.2,"Configuration that should be used in python environment, there should only be one"
Release-2.0.2,configuration instance in each Angel context.
Release-2.0.2,Use private access means jconf should not be changed or modified in this way.
Release-2.0.2,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
Release-2.0.2,Do nothing
Release-2.0.2,To-DO: add other ways to justify different value types
Release-2.0.2,"This is so ugly, must re-implement by more elegance way"
Release-2.0.2,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
Release-2.0.2,and other files submitted by user.
Release-2.0.2,Launch python process
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Data format
Release-2.0.2,Feature number of train data
Release-2.0.2,Tree number
Release-2.0.2,Tree depth
Release-2.0.2,Split number
Release-2.0.2,Feature sample ratio
Release-2.0.2,Ratio of validation
Release-2.0.2,Learning rate
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Set data format
Release-2.0.2,"Set angel resource, #worker, #task, #PS"
Release-2.0.2,Set GBDT algorithm parameters
Release-2.0.2,Set training data path
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set predict data path
Release-2.0.2,Set load model path
Release-2.0.2,Set predict result path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Train batch number per epoch.
Release-2.0.2,Batch number
Release-2.0.2,Model type
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Set data format
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,set sgd LR algorithm parameters #feature #epoch
Release-2.0.2,Set trainning data path
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Train batch number per epoch.
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Set data format
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,set sgd FM algorithm parameters #feature #epoch
Release-2.0.2,Set trainning data path
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,Model type
Release-2.0.2,String modelType = String.valueOf(RowType.T_FLOAT_DENSE);
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,set sgd LR algorithm parameters #feature #epoch
Release-2.0.2,"conf.set(MLConf.ML_MODEL_TYPE(), modelType);"
Release-2.0.2,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
Release-2.0.2,predictTest();
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Set data format
Release-2.0.2,Set trainning data path
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set data format
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,class number
Release-2.0.2,Model type
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Set data format
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,set sgd LR algorithm parameters #feature #epoch
Release-2.0.2,Set log path
Release-2.0.2,Set trainning data path
Release-2.0.2,Set save model path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set save model path
Release-2.0.2,Set actionType incremental train
Release-2.0.2,Set log path
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set predict result path
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set training data path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Cluster center number
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Sample ratio per mini-batch
Release-2.0.2,C
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.0.2,Set data format
Release-2.0.2,Set trainning data path
Release-2.0.2,Set save model path
Release-2.0.2,Set log save path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set save model path
Release-2.0.2,Set actionType incremental train
Release-2.0.2,Set log path
Release-2.0.2,Set testing data path
Release-2.0.2,Set load model path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Model type
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Set data format
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,set sgd LR algorithm parameters #feature #epoch
Release-2.0.2,Set trainning data path
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set predict result path
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Data is classification
Release-2.0.2,Model is classification
Release-2.0.2,Train batch number per epoch.
Release-2.0.2,loss delta
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Set data format
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,set sgd LR algorithm parameters #feature #epoch
Release-2.0.2,Set trainning data path
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set save model path
Release-2.0.2,Set actionType incremental train
Release-2.0.2,Set log path
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,Feature number of train data
Release-2.0.2,Total iteration number
Release-2.0.2,Validation sample Ratio
Release-2.0.2,"Data format, libsvm or dummy"
Release-2.0.2,Data is classification
Release-2.0.2,Model is classification
Release-2.0.2,Train batch number per epoch.
Release-2.0.2,Learning rate
Release-2.0.2,Decay of learning rate
Release-2.0.2,Regularization coefficient
Release-2.0.2,Set local deploy mode
Release-2.0.2,Set basic configuration keys
Release-2.0.2,Set data format
Release-2.0.2,"set angel resource parameters #worker, #task, #PS"
Release-2.0.2,set sgd LR algorithm parameters #feature #epoch
Release-2.0.2,Set trainning data path
Release-2.0.2,Set save model path
Release-2.0.2,Set log path
Release-2.0.2,Set actionType train
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set save model path
Release-2.0.2,Set actionType incremental train
Release-2.0.2,Set log path
Release-2.0.2,Set trainning data path
Release-2.0.2,Set load model path
Release-2.0.2,Set predict result path
Release-2.0.2,Set actionType prediction
Release-2.0.2,TODO: optimize int key indices
Release-2.0.2,"System.out.println(""deserialize cols.length="" + nCols);"
Release-2.0.2,"System.out.print(""deserialize "");"
Release-2.0.2,"System.out.print(cols[c] + "" "");"
Release-2.0.2,System.out.println();
Release-2.0.2,TODO Auto-generated method stub
Release-2.0.2,"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.0.2,"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.0.2,"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));"
Release-2.0.2,"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));"
Release-2.0.2,"ground truth: positive, precision: positive"
Release-2.0.2,start row index for words
Release-2.0.2,start row index for docs
Release-2.0.2,doc ids
Release-2.0.2,topic assignments
Release-2.0.2,word to docs reverse index
Release-2.0.2,count word
Release-2.0.2,build word start index
Release-2.0.2,build word to doc reverse idx
Release-2.0.2,build dks
Release-2.0.2,dks = new TraverseHashMap[n_docs];
Release-2.0.2,for (int d = 0; d < n_docs; d++) {
Release-2.0.2,if (K < Short.MAX_VALUE) {
Release-2.0.2,if (docs.get(d).len < Byte.MAX_VALUE)
Release-2.0.2,dks[d] = new S2BTraverseMap(docs.get(d).len);
Release-2.0.2,if (docs.get(d).len < Short.MAX_VALUE)
Release-2.0.2,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
Release-2.0.2,else
Release-2.0.2,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
Release-2.0.2,} else {
Release-2.0.2,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
Release-2.0.2,}
Release-2.0.2,}
Release-2.0.2,build dks
Release-2.0.2,allocate update maps
Release-2.0.2,Skip if no token for this word
Release-2.0.2,Check whether error when fetching word-topic
Release-2.0.2,Build FTree for current word
Release-2.0.2,current doc
Release-2.0.2,old topic assignment
Release-2.0.2,"Check if error happens. if this happen, it's probably that failures happen to servers."
Release-2.0.2,We need to adjust the memory settings or network fetching parameters.
Release-2.0.2,Update statistics if needed
Release-2.0.2,Calculate psum and sample new topic
Release-2.0.2,Update statistics if needed
Release-2.0.2,Assign new topic
Release-2.0.2,Skip if no token for this word
Release-2.0.2,The starting point
Release-2.0.2,There's always an unused entry.
Release-2.0.2,The starting point
Release-2.0.2,There's always an unused entry.
Release-2.0.2,The starting point
Release-2.0.2,There's always an unused entry.
Release-2.0.2,The starting point
Release-2.0.2,There's always an unused entry.
Release-2.0.2,The starting point
Release-2.0.2,There's always an unused entry.
Release-2.0.2,The starting point
Release-2.0.2,There's always an unused entry.
Release-2.0.2,print();
Release-2.0.2,The starting point
Release-2.0.2,There's always an unused entry.
Release-2.0.2,The starting point
Release-2.0.2,There's always an unused entry.
Release-2.0.2,The starting point
Release-2.0.2,There's always an unused entry.
Release-2.0.2,The starting point
Release-2.0.2,There's always an unused entry.
Release-2.0.2,print();
Release-2.0.2,Write #rows
Release-2.0.2,Write each row
Release-2.0.2,dense
Release-2.0.2,sparse
Release-2.0.2,LOG.info(buf.refCnt());
Release-2.0.2,dense
Release-2.0.2,sparse
Release-2.0.2,calculate columns
Release-2.0.2,loss function
Release-2.0.2,gradient and hessian
Release-2.0.2,"categorical feature set, null: none, empty: all, else: partial"
Release-2.0.2,"node's end index in instancePos, instances in [start, end] belong to a tree node"
Release-2.0.2,initialize the phase
Release-2.0.2,current tree and depth
Release-2.0.2,create loss function
Release-2.0.2,calculate grad info of each instance
Release-2.0.2,"create data sketch, push candidate split value to PS"
Release-2.0.2,1. calculate candidate split value
Release-2.0.2,categorical features
Release-2.0.2,2. push local sketch to PS
Release-2.0.2,the leader worker
Release-2.0.2,merge categorical features
Release-2.0.2,create updates
Release-2.0.2,"pull the global sketch from PS, only called once by each worker"
Release-2.0.2,number of categorical feature
Release-2.0.2,sample feature
Release-2.0.2,push sampled feature set to the current tree
Release-2.0.2,create new tree
Release-2.0.2,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
Release-2.0.2,calculate gradient
Release-2.0.2,"1. create new tree, initialize tree nodes and node stats"
Release-2.0.2,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
Release-2.0.2,2.1. pull the sampled features of the current tree
Release-2.0.2,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
Release-2.0.2,"2.2. if use all the features, only called one"
Release-2.0.2,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
Release-2.0.2,4. set root node to active
Release-2.0.2,"5. reset instance position, set the root node's span"
Release-2.0.2,6. calculate gradient
Release-2.0.2,1. decide nodes that should be calculated
Release-2.0.2,2. decide calculated and subtracted tree nodes
Release-2.0.2,3. calculate threads
Release-2.0.2,wait until all threads finish
Release-2.0.2,4. subtract threads
Release-2.0.2,wait until all threads finish
Release-2.0.2,5. send histograms to PS
Release-2.0.2,6. update histogram cache
Release-2.0.2,clock
Release-2.0.2,find split
Release-2.0.2,"1. find responsible tree node, using RR scheme"
Release-2.0.2,2. pull gradient histogram
Release-2.0.2,2.1. get the name of this node's gradient histogram on PS
Release-2.0.2,2.2. pull the histogram
Release-2.0.2,2.3. find best split result of this tree node
Release-2.0.2,2.3.1 using server split
Release-2.0.2,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.0.2,update the grad stats of children node
Release-2.0.2,update the left child
Release-2.0.2,update the right child
Release-2.0.2,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.0.2,"2.3.3 otherwise, the returned histogram contains the gradient info"
Release-2.0.2,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.0.2,2.3.5 reset this tree node's gradient histogram to 0
Release-2.0.2,3. push split feature to PS
Release-2.0.2,4. push split value to PS
Release-2.0.2,5. push split gain to PS
Release-2.0.2,6. set phase to AFTER_SPLIT
Release-2.0.2,this.phase = GBDTPhase.AFTER_SPLIT;
Release-2.0.2,clock
Release-2.0.2,1. get split feature
Release-2.0.2,2. get split value
Release-2.0.2,3. get split gain
Release-2.0.2,4. get node weight
Release-2.0.2,5. split node
Release-2.0.2,update local replica
Release-2.0.2,create AfterSplit task
Release-2.0.2,"2. check thread stats, if all threads finish, return"
Release-2.0.2,6. clock
Release-2.0.2,"split the span of one node, reset the instance position"
Release-2.0.2,in case this worker has no instance on this node
Release-2.0.2,set the span of left child
Release-2.0.2,set the span of right child
Release-2.0.2,"1. left to right, find the first instance that should be in the right child"
Release-2.0.2,"2. right to left, find the first instance that should be in the left child"
Release-2.0.2,3. swap two instances
Release-2.0.2,4. find the cut pos
Release-2.0.2,5. set the span of left child
Release-2.0.2,6. set the span of right child
Release-2.0.2,set tree node to active
Release-2.0.2,set node to leaf
Release-2.0.2,set node to inactive
Release-2.0.2,finish current depth
Release-2.0.2,finish current tree
Release-2.0.2,set the tree phase
Release-2.0.2,check if there is active node
Release-2.0.2,check if finish all the tree
Release-2.0.2,update node's grad stats on PS
Release-2.0.2,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
Release-2.0.2,the root node's stats is updated by leader worker
Release-2.0.2,1. create the update
Release-2.0.2,2. push the update to PS
Release-2.0.2,1. update predictions of training data
Release-2.0.2,2. update predictions of validation data
Release-2.0.2,the leader task adds node prediction to flush list
Release-2.0.2,1. name of this node's grad histogram on PS
Release-2.0.2,2. build the grad histogram of this node
Release-2.0.2,3. push the histograms to PS
Release-2.0.2,4. reset thread stats to finished
Release-2.0.2,5.1. set the children nodes of this node
Release-2.0.2,5.2. set split info and grad stats to this node
Release-2.0.2,5.2. create children nodes
Release-2.0.2,"5.3. create node stats for children nodes, and add them to the tree"
Release-2.0.2,5.4. reset instance position
Release-2.0.2,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
Release-2.0.2,5.6. set children nodes to leaf nodes
Release-2.0.2,5.7. set nid to leaf node
Release-2.0.2,5.8. deactivate active node
Release-2.0.2,"get feature type, 0:empty 1:all equal 2:real"
Release-2.0.2,"if not -1, sufficient space will be allocated at once"
Release-2.0.2,copy the highest levels
Release-2.0.2,copy baseBuffer
Release-2.0.2,merge two non-empty quantile sketches
Release-2.0.2,left child <= split value; right child > split value
Release-2.0.2,"the first: minimal, the last: maximal"
Release-2.0.2,categorical features
Release-2.0.2,continuous features
Release-2.0.2,left child <= split value; right child > split value
Release-2.0.2,feature index used to split
Release-2.0.2,feature value used to split
Release-2.0.2,loss change after split this node
Release-2.0.2,grad stats of the left child
Release-2.0.2,grad stats of the right child
Release-2.0.2,"LOG.info(""Constructor with fid = -1"");"
Release-2.0.2,fid = -1: no split currently
Release-2.0.2,the minimal split value is the minimal value of feature
Release-2.0.2,the splits do not include the maximal value of feature
Release-2.0.2,"1. the average distance, (maxValue - minValue) / splitNum"
Release-2.0.2,2. calculate the candidate split value
Release-2.0.2,1. new feature's histogram (grad + hess)
Release-2.0.2,size: sampled_featureNum * (2 * splitNum)
Release-2.0.2,"in other words, concatenate each feature's histogram"
Release-2.0.2,2. get the span of this node
Release-2.0.2,------ 3. using sparse-aware method to build histogram ---
Release-2.0.2,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
Release-2.0.2,3.1. get the instance index
Release-2.0.2,3.2. get the grad and hess of the instance
Release-2.0.2,3.3. add to the sum
Release-2.0.2,3.4. loop the non-zero entries
Release-2.0.2,3.4.1. get feature value
Release-2.0.2,3.4.2. current feature's position in the sampled feature set
Release-2.0.2,"int fPos = findFidPlace(this.controller.fSet, fid);"
Release-2.0.2,3.4.3. find the position of feature value in a histogram
Release-2.0.2,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
Release-2.0.2,3.4.4. add the grad and hess to the corresponding bin
Release-2.0.2,3.4.5. add the reverse to the bin that contains 0.0f
Release-2.0.2,4. add the grad and hess sum to the zero bin of all features
Release-2.0.2,find the best split result of the histogram of a tree node
Release-2.0.2,1. calculate the gradStats of the root node
Release-2.0.2,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
Release-2.0.2,2. loop over features
Release-2.0.2,2.1. get the ture feature id in the sampled feature set
Release-2.0.2,2.2. get the indexes of histogram of this feature
Release-2.0.2,2.3. find the best split of current feature
Release-2.0.2,2.4. update the best split result if possible
Release-2.0.2,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.0.2,3. update the grad stats of children node
Release-2.0.2,3.1. update the left child
Release-2.0.2,3.2. update the right child
Release-2.0.2,find the best split result of one feature
Release-2.0.2,1. set the feature id
Release-2.0.2,2. create the best left stats and right stats
Release-2.0.2,3. the gain of the root node
Release-2.0.2,4. create the temp left and right grad stats
Release-2.0.2,5. loop over all the data in histogram
Release-2.0.2,5.1. get the grad and hess of current hist bin
Release-2.0.2,5.2. check whether we can split with current left hessian
Release-2.0.2,right = root - left
Release-2.0.2,5.3. check whether we can split with current right hessian
Release-2.0.2,5.4. calculate the current loss gain
Release-2.0.2,5.5. check whether we should update the split result with current loss gain
Release-2.0.2,split value = sketches[splitIdx]
Release-2.0.2,"5.6. if should update, also update the best left and right grad stats"
Release-2.0.2,6. set the best left and right grad stats
Release-2.0.2,partition number
Release-2.0.2,cols of each partition
Release-2.0.2,1. calculate the total grad sum and hess sum
Release-2.0.2,2. create the grad stats of the node
Release-2.0.2,1. calculate the total grad sum and hess sum
Release-2.0.2,2. create the grad stats of the node
Release-2.0.2,1. calculate the total grad sum and hess sum
Release-2.0.2,2. create the grad stats of the node
Release-2.0.2,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
Release-2.0.2,assert fvalue >= sketch[start] && fvalue <= sketch[end];
Release-2.0.2,if (left > end) return end - start;
Release-2.0.2,find the best split result of the histogram of a tree node
Release-2.0.2,2.2. get the indexes of histogram of this feature
Release-2.0.2,2.3. find the best split of current feature
Release-2.0.2,2.4. update the best split result if possible
Release-2.0.2,find the best split result of one feature
Release-2.0.2,1. set the feature id
Release-2.0.2,splitEntry.setFid(fid);
Release-2.0.2,2. create the best left stats and right stats
Release-2.0.2,3. the gain of the root node
Release-2.0.2,4. create the temp left and right grad stats
Release-2.0.2,5. loop over all the data in histogram
Release-2.0.2,5.1. get the grad and hess of current hist bin
Release-2.0.2,5.2. check whether we can split with current left hessian
Release-2.0.2,right = root - left
Release-2.0.2,5.3. check whether we can split with current right hessian
Release-2.0.2,5.4. calculate the current loss gain
Release-2.0.2,5.5. check whether we should update the split result with current loss gain
Release-2.0.2,"5.6. if should update, also update the best left and right grad stats"
Release-2.0.2,6. set the best left and right grad stats
Release-2.0.2,find the best split result of a serve row on the PS
Release-2.0.2,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
Release-2.0.2,2.2. get the start index in histogram of this feature
Release-2.0.2,2.3. find the best split of current feature
Release-2.0.2,2.4. update the best split result if possible
Release-2.0.2,"find the best split result of one feature from a server row, used by the PS"
Release-2.0.2,1. set the feature id
Release-2.0.2,2. create the best left stats and right stats
Release-2.0.2,3. the gain of the root node
Release-2.0.2,4. create the temp left and right grad stats
Release-2.0.2,5. loop over all the data in histogram
Release-2.0.2,5.1. get the grad and hess of current hist bin
Release-2.0.2,5.2. check whether we can split with current left hessian
Release-2.0.2,right = root - left
Release-2.0.2,5.3. check whether we can split with current right hessian
Release-2.0.2,5.4. calculate the current loss gain
Release-2.0.2,5.5. check whether we should update the split result with current loss gain
Release-2.0.2,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
Release-2.0.2,the task use index to find fvalue
Release-2.0.2,"5.6. if should update, also update the best left and right grad stats"
Release-2.0.2,6. set the best left and right grad stats
Release-2.0.2,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
Release-2.0.2,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
Release-2.0.2,max and min of each feature
Release-2.0.2,clear all the information
Release-2.0.2,calculate the sum of gradient and hess
Release-2.0.2,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
Release-2.0.2,ridx)
Release-2.0.2,check if necessary information is ready
Release-2.0.2,"same as add, reduce is used in All Reduce"
Release-2.0.2,"features used in this tree, if equals null, means use all the features without sampling"
Release-2.0.2,node in the tree
Release-2.0.2,the gradient info of each instances
Release-2.0.2,initialize nodes
Release-2.0.2,gradient
Release-2.0.2,second order gradient
Release-2.0.2,int sendStartCol = (int) row.getStartCol();
Release-2.0.2,logistic loss for binary classification task.
Release-2.0.2,"logistic loss, but predict un-transformed margin"
Release-2.0.2,check if label in range
Release-2.0.2,return the default evaluation metric for the objective
Release-2.0.2,"task type: classification, regression, or ranking"
Release-2.0.2,"quantile sketch, size = featureNum * splitNum"
Release-2.0.2,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
Release-2.0.2,"active tree nodes, size = pow(2, treeDepth) -1"
Release-2.0.2,sampled features. size = treeNum * sampleRatio * featureNum
Release-2.0.2,categorical feature. size = workerNum * cateFeatNum * splitNum
Release-2.0.2,"split features, size = treeNum * treeNodeNum"
Release-2.0.2,"split values, size = treeNum * treeNodeNum"
Release-2.0.2,"split gains, size = treeNum * treeNodeNum"
Release-2.0.2,"node weights, size = treeNum * treeNodeNum"
Release-2.0.2,"node preds, size = treeNum * treeNodeNum"
Release-2.0.2,if using PS to perform split
Release-2.0.2,step size for a tree
Release-2.0.2,number of class
Release-2.0.2,minimum loss change required for a split
Release-2.0.2,maximum depth of a tree
Release-2.0.2,number of features
Release-2.0.2,number of nonzero
Release-2.0.2,number of candidates split value
Release-2.0.2,----- the rest parameters are less important ----
Release-2.0.2,base instance weight
Release-2.0.2,minimum amount of hessian(weight) allowed in a child
Release-2.0.2,L2 regularization factor
Release-2.0.2,L1 regularization factor
Release-2.0.2,default direction choice
Release-2.0.2,maximum delta update we can add in weight estimation
Release-2.0.2,this parameter can be used to stabilize update
Release-2.0.2,default=0 means no constraint on weight delta
Release-2.0.2,whether we want to do subsample for row
Release-2.0.2,whether to subsample columns for each tree
Release-2.0.2,accuracy of sketch
Release-2.0.2,accuracy of sketch
Release-2.0.2,leaf vector size
Release-2.0.2,option for parallelization
Release-2.0.2,option to open cacheline optimization
Release-2.0.2,whether to not print info during training.
Release-2.0.2,maximum depth of the tree
Release-2.0.2,number of features used for tree construction
Release-2.0.2,"minimum loss change required for a split, otherwise stop split"
Release-2.0.2,----- the rest parameters are less important ----
Release-2.0.2,default direction choice
Release-2.0.2,whether we want to do sample data
Release-2.0.2,whether to sample columns during tree construction
Release-2.0.2,whether to use histogram for split
Release-2.0.2,number of histogram units
Release-2.0.2,whether to print info during training.
Release-2.0.2,----- the rest parameters are obtained after training ----
Release-2.0.2,total number of nodes
Release-2.0.2,number of deleted nodes */
Release-2.0.1,@maxIndex: this variable contains the max index of node/word
Release-2.0.1,values[b + offset] = (random.nextFloat() - 0.5f) / dimension;
Release-2.0.1,some params
Release-2.0.1,max index for node/word
Release-2.0.1,compute number of nodes for one row
Release-2.0.1,check the length of dot values
Release-2.0.1,merge dot values from all partitions
Release-2.0.1,Skip-Gram model
Release-2.0.1,Negative sampling
Release-2.0.1,used to accumulate the updates for input vectors
Release-2.0.1,Negative sampling
Release-2.0.1,accumulate for the hidden layer
Release-2.0.1,update output layer
Release-2.0.1,update the hidden layer
Release-2.0.1,update input
Release-2.0.1,Skip-Gram model
Release-2.0.1,Negative sampling
Release-2.0.1,used to accumulate the updates for input vectors
Release-2.0.1,Negative sampling
Release-2.0.1,accumulate for the hidden layer
Release-2.0.1,update output layer
Release-2.0.1,update the hidden layer
Release-2.0.1,update input
Release-2.0.1,update output
Release-2.0.1,Some params
Release-2.0.1,compute number of nodes for one row
Release-2.0.1,window size
Release-2.0.1,Skip-Gram model
Release-2.0.1,Accumulate the input vectors from context
Release-2.0.1,Negative sampling
Release-2.0.1,used to accumulate the updates for input vectors
Release-2.0.1,window size
Release-2.0.1,skip-gram model
Release-2.0.1,Negative sampling
Release-2.0.1,accumulate for the hidden layer
Release-2.0.1,update output layer
Release-2.0.1,update the hidden layer
Release-2.0.1,update input
Release-2.0.1,update output
Release-2.0.1,some params
Release-2.0.1,batch sentences
Release-2.0.1,max index for node/word
Release-2.0.1,compute number of nodes for one row
Release-2.0.1,check the length of dot values
Release-2.0.1,merge dot values from all partitions
Release-2.0.1,locates the input vectors to local array to prevent randomly access
Release-2.0.1,on the large server row.
Release-2.0.1,fill 0 for context vector
Release-2.0.1,window size
Release-2.0.1,Continuous bag-of-words Models
Release-2.0.1,Accumulate the input vectors from context
Release-2.0.1,Calculate the partial dot values
Release-2.0.1,We should guarantee here that the sample would not equal the ``word``
Release-2.0.1,used to accumulate the context input vectors
Release-2.0.1,locates the input vector into local arrays to prevent randomly access for
Release-2.0.1,the large server row.
Release-2.0.1,window size
Release-2.0.1,while true to prevent sampling out a positive target
Release-2.0.1,how to prevent the randomly access to the output vectors??
Release-2.0.1,accumulate gradients for the input vectors
Release-2.0.1,update output vectors
Release-2.0.1,update input
Release-2.0.1,update output
Release-2.0.1,Some params
Release-2.0.1,compute number of nodes for one row
Release-2.0.1,calculate bias
Release-2.0.1,Do nothing.
Release-2.0.1,current word
Release-2.0.1,neu1 stores the average value of input vectors in the context (CBOW)
Release-2.0.1,Continuous Bag-of-Words Model
Release-2.0.1,Accumulate the input vectors from context
Release-2.0.1,negative sampling
Release-2.0.1,Using the sigmoid value from the pre-computed table
Release-2.0.1,accumulate for the hidden layer
Release-2.0.1,update output layer
Release-2.0.1,add the counter for target
Release-2.0.1,update hidden layer
Release-2.0.1,Update the input vector for each word in the context
Release-2.0.1,add the counter to input
Release-2.0.1,update input layers
Release-2.0.1,update output layers
Release-2.0.1,for (int a = 0; a < layers.length; a++) deltas[a] = layers[a] - deltas[a];
Release-2.0.1,set basic configuration keys
Release-2.0.1,use local deploy mode and dummy data spliter
Release-2.0.1,get a angel client
Release-2.0.1,add matrix
Release-2.0.1,TODO Auto-generated constructor stub
Release-2.0.1,row 0 is a random uniform
Release-2.0.1,row 1 is a random normal
Release-2.0.1,row 2 is filled with 1.0
Release-2.0.1,import jdk.nashorn.internal.runtime.regexp.joni.Config;
Release-2.0.1,"paras[1] = ""abc"";"
Release-2.0.1,"paras[2] = ""123"";"
Release-2.0.1,Add standard Hadoop classes
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,set sgd LR algorithm parameters #feature #epoch
Release-2.0.1,Set input data path
Release-2.0.1,Set save model path
Release-2.0.1,Set actionType train
Release-2.0.1,QSLRRunner runner = new QSLRRunner();
Release-2.0.1,runner.train(conf);
Release-2.0.1,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
Release-2.0.1,Dataset
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,class number
Release-2.0.1,Model type
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,Train batch number per epoch.
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set file system
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Use local deploy mode and data format
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set Softmax algorithm parameters
Release-2.0.1,Set model class
Release-2.0.1,Dataset
Release-2.0.1,Data format
Release-2.0.1,Feature number of train data
Release-2.0.1,Tree number
Release-2.0.1,Tree depth
Release-2.0.1,Split number
Release-2.0.1,Feature sample ratio
Release-2.0.1,Ratio of validation
Release-2.0.1,Learning rate
Release-2.0.1,Set file system
Release-2.0.1,Use local deploy mode and data format
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource, #worker, #task, #PS"
Release-2.0.1,Set GBDT algorithm parameters
Release-2.0.1,Dataset
Release-2.0.1,Set file system
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Use local deploy mode
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set DeepFM algorithm parameters
Release-2.0.1,Set model class
Release-2.0.1,Dataset
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Model type
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set file system
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Use local deploy mode and data format
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set LR algorithm parameters
Release-2.0.1,Set model class
Release-2.0.1,Dataset
Release-2.0.1,Data format
Release-2.0.1,Model type
Release-2.0.1,Cluster center number
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Sample ratio per mini-batch
Release-2.0.1,C
Release-2.0.1,Set file system
Release-2.0.1,Use local deploy mode and data format
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource, #worker, #task, #PS"
Release-2.0.1,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.0.1,Dataset
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Model type
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set file system
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Use local deploy mode and data format
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set FM algorithm parameters
Release-2.0.1,Set model class
Release-2.0.1,Dataset
Release-2.0.1,Set file system
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Use local deploy mode
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set WideAndDeep algorithm parameters
Release-2.0.1,Set model class
Release-2.0.1,Dataset
Release-2.0.1,Data format
Release-2.0.1,"Set LDA parameters #V, #K"
Release-2.0.1,Set file system
Release-2.0.1,Use local deploy mode and data format
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource, #worker, #task, #PS"
Release-2.0.1,Set LDA algorithm parameters
Release-2.0.1,Dataset
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Model type
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set file system
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Use local deploy mode and data format
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set SVM algorithm parameters
Release-2.0.1,Set model class
Release-2.0.1,Dataset
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Model type
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,Model is classification
Release-2.0.1,Train batch number per epoch.
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set file system
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Use local deploy mode and data format
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set LR algorithm parameters
Release-2.0.1,Set model class
Release-2.0.1,Dataset
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Model type
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,Model is classification
Release-2.0.1,Train batch number per epoch.
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set file system
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Use local deploy mode and data format
Release-2.0.1,Set data path
Release-2.0.1,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set LR algorithm parameters
Release-2.0.1,Set model class
Release-2.0.1,Load model meta
Release-2.0.1,Convert model
Release-2.0.1,"Get input path, output path"
Release-2.0.1,Init serde
Release-2.0.1,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.0.1,Load model meta
Release-2.0.1,Convert model
Release-2.0.1,load hadoop configuration
Release-2.0.1,"Get input path, output path"
Release-2.0.1,Init serde
Release-2.0.1,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.0.1,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.1,input.seek(rowOffset.getOffset());
Release-2.0.1,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.1,input.seek(rowOffset.getOffset());
Release-2.0.1,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.1,input.seek(rowOffset.getOffset());
Release-2.0.1,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.1,input.seek(rowOffset.getOffset());
Release-2.0.1,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.1,input.seek(rowOffset.getOffset());
Release-2.0.1,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.1,input.seek(rowOffset.getOffset());
Release-2.0.1,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.1,input.seek(rowOffset.getOffset());
Release-2.0.1,Load model meta
Release-2.0.1,Check row type
Release-2.0.1,Load model
Release-2.0.1,Load model meta
Release-2.0.1,Check row type
Release-2.0.1,Load model
Release-2.0.1,Load model meta
Release-2.0.1,Check row type
Release-2.0.1,Load model
Release-2.0.1,Load model meta
Release-2.0.1,Check row type
Release-2.0.1,Load model
Release-2.0.1,Load model meta
Release-2.0.1,Check row type
Release-2.0.1,Load model
Release-2.0.1,Load model meta
Release-2.0.1,Check row type
Release-2.0.1,Load model
Release-2.0.1,Load model meta
Release-2.0.1,Check row type
Release-2.0.1,Load model
Release-2.0.1,Load model
Release-2.0.1,load hadoop configuration
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,System.out.println(content);
Release-2.0.1,https://blog.csdn.net/cocoonyang/article/details/63068108
Release-2.0.1,v1[i] = v1[i] + da * v2[i];
Release-2.0.1,"dgemm(String transa, String transb,"
Release-2.0.1,"int m, int n, int k,"
Release-2.0.1,"double alpha,"
Release-2.0.1,"double[] a, int lda,"
Release-2.0.1,"double[] b, int ldb,"
Release-2.0.1,"double beta,"
Release-2.0.1,"double[] c, int ldc);"
Release-2.0.1,C := alpha*op( A )*op( B ) + beta*C
Release-2.0.1,v1[i] = v1[i] + da * v2[i];
Release-2.0.1,y := alpha*A*x + beta*y
Release-2.0.1,y := alpha*A*x + beta*y
Release-2.0.1,y := alpha*A*x + beta*y
Release-2.0.1,"dgemm(String transa, String transb,"
Release-2.0.1,"int m, int n, int k,"
Release-2.0.1,"double alpha,"
Release-2.0.1,"double[] a, int lda,"
Release-2.0.1,"double[] b, int ldb,"
Release-2.0.1,"double beta,"
Release-2.0.1,"double[] c, int ldc);"
Release-2.0.1,C := alpha*op( A )*op( B ) + beta*C
Release-2.0.1,Default does nothing.
Release-2.0.1,The app injection is optional
Release-2.0.1,"renderText(""hello world"");"
Release-2.0.1,"user choose a workerGroupID from the workergroups page,"
Release-2.0.1,now we should change the AngelApp params and render the workergroup page;
Release-2.0.1,"static final String WORKER_ID = ""worker.id"";"
Release-2.0.1,"div(""#logo"")."
Release-2.0.1,"img(""/static/hadoop-st.png"")._()."
Release-2.0.1,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
Release-2.0.1,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
Release-2.0.1,JQueryUI.jsnotice(html);
Release-2.0.1,import org.apache.hadoop.conf.Configuration;
Release-2.0.1,import java.lang.reflect.Field;
Release-2.0.1,get block locations from file system
Release-2.0.1,create a list of all block and their locations
Release-2.0.1,"if the file is not splitable, just create the one block with"
Release-2.0.1,full file length
Release-2.0.1,each split can be a maximum of maxSize
Release-2.0.1,if remainder is between max and 2*max - then
Release-2.0.1,"instead of creating splits of size max, left-max we"
Release-2.0.1,create splits of size left/2 and left/2. This is
Release-2.0.1,a heuristic to avoid creating really really small
Release-2.0.1,splits.
Release-2.0.1,add this block to the block --> node locations map
Release-2.0.1,"For blocks that do not have host/rack information,"
Release-2.0.1,assign to default  rack.
Release-2.0.1,add this block to the rack --> block map
Release-2.0.1,Add this host to rackToNodes map
Release-2.0.1,add this block to the node --> block map
Release-2.0.1,"if the file system does not have any rack information, then"
Release-2.0.1,use dummy rack location.
Release-2.0.1,The topology paths have the host name included as the last
Release-2.0.1,component. Strip it.
Release-2.0.1,get tokens for all the required FileSystems..
Release-2.0.1,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
Release-2.0.1,job.getConfiguration());
Release-2.0.1,Whether we need to recursive look into the directory structure
Release-2.0.1,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.0.1,user provided one (if any).
Release-2.0.1,all the files in input set
Release-2.0.1,it is allowed for maxSize to be 0. Disable smoothing load for such cases
Release-2.0.1,process all nodes and create splits that are local to a node. Generate
Release-2.0.1,"one split per node iteration, and walk over nodes multiple times to"
Release-2.0.1,distribute the splits across nodes.
Release-2.0.1,Skip the node if it has previously been marked as completed.
Release-2.0.1,"for each block, copy it into validBlocks. Delete it from"
Release-2.0.1,blockToNodes so that the same block does not appear in
Release-2.0.1,two different splits.
Release-2.0.1,Remove all blocks which may already have been assigned to other
Release-2.0.1,splits.
Release-2.0.1,"if the accumulated split size exceeds the maximum, then"
Release-2.0.1,create this split.
Release-2.0.1,create an input split and add it to the splits array
Release-2.0.1,Remove entries from blocksInNode so that we don't walk these
Release-2.0.1,again.
Release-2.0.1,Done creating a single split for this node. Move on to the next
Release-2.0.1,node so that splits are distributed across nodes.
Release-2.0.1,This implies that the last few blocks (or all in case maxSize=0)
Release-2.0.1,were not part of a split. The node is complete.
Release-2.0.1,if there were any blocks left over and their combined size is
Release-2.0.1,"larger than minSplitNode, then combine them into one split."
Release-2.0.1,Otherwise add them back to the unprocessed pool. It is likely
Release-2.0.1,that they will be combined with other blocks from the
Release-2.0.1,same rack later on.
Release-2.0.1,This condition also kicks in when max split size is not set. All
Release-2.0.1,blocks on a node will be grouped together into a single split.
Release-2.0.1,haven't created any split on this machine. so its ok to add a
Release-2.0.1,smaller one for parallelism. Otherwise group it in the rack for
Release-2.0.1,balanced size create an input split and add it to the splits
Release-2.0.1,array
Release-2.0.1,Remove entries from blocksInNode so that we don't walk this again.
Release-2.0.1,The node is done. This was the last set of blocks for this node.
Release-2.0.1,Put the unplaced blocks back into the pool for later rack-allocation.
Release-2.0.1,Node is done. All blocks were fit into node-local splits.
Release-2.0.1,Check if node-local assignments are complete.
Release-2.0.1,All nodes have been walked over and marked as completed or all blocks
Release-2.0.1,have been assigned. The rest should be handled via rackLock assignment.
Release-2.0.1,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
Release-2.0.1,"+ completedNodes.size() + "", size left: "" + totalLength);"
Release-2.0.1,"if blocks in a rack are below the specified minimum size, then keep them"
Release-2.0.1,"in 'overflow'. After the processing of all racks is complete, these"
Release-2.0.1,overflow blocks will be combined into splits.
Release-2.0.1,Process all racks over and over again until there is no more work to do.
Release-2.0.1,Create one split for this rack before moving over to the next rack.
Release-2.0.1,Come back to this rack after creating a single split for each of the
Release-2.0.1,remaining racks.
Release-2.0.1,"Process one rack location at a time, Combine all possible blocks that"
Release-2.0.1,reside on this rack as one split. (constrained by minimum and maximum
Release-2.0.1,split size).
Release-2.0.1,iterate over all racks
Release-2.0.1,"for each block, copy it into validBlocks. Delete it from"
Release-2.0.1,blockToNodes so that the same block does not appear in
Release-2.0.1,two different splits.
Release-2.0.1,"if the accumulated split size exceeds the maximum, then"
Release-2.0.1,create this split.
Release-2.0.1,create an input split and add it to the splits array
Release-2.0.1,"if we created a split, then just go to the next rack"
Release-2.0.1,"if there is a minimum size specified, then create a single split"
Release-2.0.1,"otherwise, store these blocks into overflow data structure"
Release-2.0.1,There were a few blocks in this rack that
Release-2.0.1,remained to be processed. Keep them in 'overflow' block list.
Release-2.0.1,These will be combined later.
Release-2.0.1,Process all overflow blocks
Release-2.0.1,"This might cause an exiting rack location to be re-added,"
Release-2.0.1,but it should be ok.
Release-2.0.1,"if the accumulated split size exceeds the maximum, then"
Release-2.0.1,create this split.
Release-2.0.1,create an input split and add it to the splits array
Release-2.0.1,"Process any remaining blocks, if any."
Release-2.0.1,create an input split
Release-2.0.1,add this split to the list that is returned
Release-2.0.1,long num = totLength / maxSize;
Release-2.0.1,all blocks for all the files in input set
Release-2.0.1,mapping from a rack name to the list of blocks it has
Release-2.0.1,mapping from a block to the nodes on which it has replicas
Release-2.0.1,mapping from a node to the list of blocks that it contains
Release-2.0.1,populate all the blocks for all files
Release-2.0.1,stop all services
Release-2.0.1,1.write application state to file so that the client can get the state of the application
Release-2.0.1,if master exit
Release-2.0.1,2.clear tmp and staging directory
Release-2.0.1,waiting for client to get application state
Release-2.0.1,stop the RPC server
Release-2.0.1,"Security framework already loaded the tokens into current UGI, just use"
Release-2.0.1,them
Release-2.0.1,Now remove the AM->RM token so tasks don't have it
Release-2.0.1,add a shutdown hook
Release-2.0.1,init app state storage
Release-2.0.1,init event dispacher
Release-2.0.1,init location manager
Release-2.0.1,init container allocator
Release-2.0.1,init a rpc service
Release-2.0.1,recover matrix meta if needed
Release-2.0.1,recover ps attempt information if need
Release-2.0.1,Init Client manager
Release-2.0.1,Init PS Client manager
Release-2.0.1,init parameter server manager
Release-2.0.1,recover task information if needed
Release-2.0.1,a dummy data spliter is just for test now
Release-2.0.1,recover data splits information if needed
Release-2.0.1,init worker manager and register worker manager event
Release-2.0.1,register slow worker/ps checker
Release-2.0.1,register app manager event and finish event
Release-2.0.1,Init model saver & loader
Release-2.0.1,start a web service if use yarn deploy mode
Release-2.0.1,load from app state storage first if attempt index great than 1(the master is not the first
Release-2.0.1,retry)
Release-2.0.1,"if load failed, just build a new MatrixMetaManager"
Release-2.0.1,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-2.0.1,is not the first retry)
Release-2.0.1,load task information from app state storage first if attempt index great than 1(the master
Release-2.0.1,is not the first retry)
Release-2.0.1,"if load failed, just build a new AMTaskManager"
Release-2.0.1,load data splits information from app state storage first if attempt index great than 1(the
Release-2.0.1,master is not the first retry)
Release-2.0.1,"if load failed, we need to recalculate the data splits"
Release-2.0.1,Check Workers
Release-2.0.1,Check PSS
Release-2.0.1,Check Clients
Release-2.0.1,Check PS Clients
Release-2.0.1,parse parameter server counters
Release-2.0.1,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.0.1,refresh last heartbeat timestamp
Release-2.0.1,send a state update event to the specific PSAttempt
Release-2.0.1,Check is there save request
Release-2.0.1,Check is there load request
Release-2.0.1,check matrix metadata inconsistencies between master and parameter server.
Release-2.0.1,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
Release-2.0.1,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
Release-2.0.1,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.0.1,choose a unused port
Release-2.0.1,start RPC server
Release-2.0.1,remove this parameter server attempt from monitor set
Release-2.0.1,remove this parameter server attempt from monitor set
Release-2.0.1,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.0.1,find workergroup in worker manager
Release-2.0.1,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
Release-2.0.1,"if this worker group run over, just return WORKERGROUP_EXITED"
Release-2.0.1,"if this worker group is running now, return tasks, workers, data splits for it"
Release-2.0.1,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.0.1,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.0.1,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.0.1,"in ANGEL_PS mode, task id may can not know advance"
Release-2.0.1,update the clock for this matrix
Release-2.0.1,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.0.1,"in ANGEL_PS mode, task id may can not know advance"
Release-2.0.1,update task iteration
Release-2.0.1,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
Release-2.0.1,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
Release-2.0.1,the number of splits equal to the number of tasks
Release-2.0.1,split data
Release-2.0.1,dispatch the splits to workergroups
Release-2.0.1,split data
Release-2.0.1,dispatch the splits to workergroups
Release-2.0.1,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.0.1,"first, then divided by expected split number"
Release-2.0.1,get input format class from configuration and then instantiation a input format object
Release-2.0.1,split data
Release-2.0.1,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.0.1,"first, then divided by expected split number"
Release-2.0.1,get input format class from configuration and then instantiation a input format object
Release-2.0.1,split data
Release-2.0.1,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.0.1,need to fine tune the number of workergroup and task based on the actual split number
Release-2.0.1,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.0.1,Record the location information for the splits in order to data localized schedule
Release-2.0.1,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.0.1,need to fine tune the number of workergroup and task based on the actual split number
Release-2.0.1,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.0.1,Record the location information for the splits in order to data localized schedule
Release-2.0.1,write meta data to a temporary file
Release-2.0.1,rename the temporary file to final file
Release-2.0.1,"if the file exists, read from file and deserialize it"
Release-2.0.1,write task meta
Release-2.0.1,write ps meta
Release-2.0.1,generate a temporary file
Release-2.0.1,write task meta to the temporary file first
Release-2.0.1,rename the temporary file to the final file
Release-2.0.1,"if last final task file exist, remove it"
Release-2.0.1,find task meta file which has max timestamp
Release-2.0.1,"if the file does not exist, just return null"
Release-2.0.1,read task meta from file and deserialize it
Release-2.0.1,generate a temporary file
Release-2.0.1,write ps meta to the temporary file first.
Release-2.0.1,rename the temporary file to the final file
Release-2.0.1,"if the old final file exist, just remove it"
Release-2.0.1,find ps meta file
Release-2.0.1,"if ps meta file does not exist, just return null"
Release-2.0.1,read ps meta from file and deserialize it
Release-2.0.1,"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),"
Release-2.0.1,String.valueOf(requestId));
Release-2.0.1,Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));
Release-2.0.1,saveContext.setTmpSavePath(tmpPath.toString());
Release-2.0.1,Filter old epoch trigger first
Release-2.0.1,Split the user request to sub-requests to pss
Release-2.0.1,Init matrix files meta
Release-2.0.1,Move output files
Release-2.0.1,Write the meta file
Release-2.0.1,Split the user request to sub-requests to pss
Release-2.0.1,check whether psagent heartbeat timeout
Release-2.0.1,Set up the launch command
Release-2.0.1,Duplicate the ByteBuffers for access by multiple containers.
Release-2.0.1,Construct the actual Container
Release-2.0.1,Application resources
Release-2.0.1,Application environment
Release-2.0.1,Service data
Release-2.0.1,Tokens
Release-2.0.1,Set up JobConf to be localized properly on the remote NM.
Release-2.0.1,Setup DistributedCache
Release-2.0.1,Setup up task credentials buffer
Release-2.0.1,LocalStorageToken is needed irrespective of whether security is enabled
Release-2.0.1,or not.
Release-2.0.1,"TokenCache.setJobToken(jobToken, taskCredentials);"
Release-2.0.1,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
Release-2.0.1,Construct the actual Container
Release-2.0.1,The null fields are per-container and will be constructed for each
Release-2.0.1,container separately.
Release-2.0.1,Set up the launch command
Release-2.0.1,Duplicate the ByteBuffers for access by multiple containers.
Release-2.0.1,Construct the actual Container
Release-2.0.1,"a * in the classpath will only find a .jar, so we need to filter out"
Release-2.0.1,all .jars and add everything else
Release-2.0.1,Propagate the system classpath when using the mini cluster
Release-2.0.1,Add standard Hadoop classes
Release-2.0.1,Add mr
Release-2.0.1,Cache archives
Release-2.0.1,Cache files
Release-2.0.1,Sanity check
Release-2.0.1,Add URI fragment or just the filename
Release-2.0.1,Add the env variables passed by the user
Release-2.0.1,Set logging level in the environment.
Release-2.0.1,Setup the log4j prop
Release-2.0.1,Add main class and its arguments
Release-2.0.1,Finally add the jvmID
Release-2.0.1,vargs.add(String.valueOf(jvmID.getId()));
Release-2.0.1,Final commmand
Release-2.0.1,Add the env variables passed by the user
Release-2.0.1,Set logging level in the environment.
Release-2.0.1,Setup the log4j prop
Release-2.0.1,Add main class and its arguments
Release-2.0.1,Final commmand
Release-2.0.1,"if amTask is not null, we should clone task state from it"
Release-2.0.1,"if all parameter server complete commit, master can commit now"
Release-2.0.1,check whether parameter server heartbeat timeout
Release-2.0.1,Transitions from the NEW state.
Release-2.0.1,Transitions from the UNASSIGNED state.
Release-2.0.1,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
Release-2.0.1,Transitions from the ASSIGNED state.
Release-2.0.1,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
Release-2.0.1,PA_CONTAINER_LAUNCHED event
Release-2.0.1,Transitions from the PSAttemptStateInternal.RUNNING state.
Release-2.0.1,Transitions from the PSAttemptStateInternal.COMMITTING state
Release-2.0.1,Transitions from the PSAttemptStateInternal.KILLED state
Release-2.0.1,Transitions from the PSAttemptStateInternal.FAILED state
Release-2.0.1,create the topology tables
Release-2.0.1,reqeuest resource:send a resource request to the resource allocator
Release-2.0.1,"Once the resource is applied, build and send the launch request to the container launcher"
Release-2.0.1,deallocator the resource of the ps attempt:send a resource deallocator request to the
Release-2.0.1,resource allocator
Release-2.0.1,set the launch time
Release-2.0.1,add the ps attempt to the heartbeat timeout monitoring list
Release-2.0.1,parse ps attempt location and put it to location manager
Release-2.0.1,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
Release-2.0.1,or failed
Release-2.0.1,remove ps attempt id from heartbeat timeout monitor list
Release-2.0.1,release container:send a release request to container launcher
Release-2.0.1,set the finish time only if launch time is set
Release-2.0.1,private long scheduledTime;
Release-2.0.1,Transitions from the NEW state.
Release-2.0.1,Transitions from the SCHEDULED state.
Release-2.0.1,Transitions from the RUNNING state.
Release-2.0.1,"another attempt launched,"
Release-2.0.1,Transitions from the SUCCEEDED state
Release-2.0.1,Transitions from the KILLED state
Release-2.0.1,Transitions from the FAILED state
Release-2.0.1,add diagnostic
Release-2.0.1,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.1,Refresh ps location & matrix meta
Release-2.0.1,start a new attempt for this ps
Release-2.0.1,notify ps manager
Release-2.0.1,"getContext().getLocationManager().setPsLocation(id, null);"
Release-2.0.1,add diagnostic
Release-2.0.1,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.1,start a new attempt for this ps
Release-2.0.1,notify ps manager
Release-2.0.1,notify the event handler of state change
Release-2.0.1,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
Release-2.0.1,"if forcedState is set, just return"
Release-2.0.1,else get state from state machine
Release-2.0.1,add this worker group to the success set
Release-2.0.1,check if all worker group run over
Release-2.0.1,add this worker group to the failed set
Release-2.0.1,check if too many worker groups are failed or killed
Release-2.0.1,notify a run failed event
Release-2.0.1,add this worker group to the failed set
Release-2.0.1,check if too many worker groups are failed or killed
Release-2.0.1,notify a run failed event
Release-2.0.1,calculate the actual number of worker groups and the total number of tasks based on the number of data split
Release-2.0.1,"init all tasks , workers and worker groups and put them to the corresponding maps"
Release-2.0.1,just return the total task number now
Release-2.0.1,TODO
Release-2.0.1,check whether worker heartbeat timeout
Release-2.0.1,"if workerAttempt is not null, we should clone task state from it"
Release-2.0.1,from NEW state
Release-2.0.1,from SCHEDULED state
Release-2.0.1,get data splits location for data locality
Release-2.0.1,reqeuest resource:send a resource request to the resource allocator
Release-2.0.1,"once the resource is applied, build and send the launch request to the container launcher"
Release-2.0.1,notify failed message to the worker
Release-2.0.1,notify killed message to the worker
Release-2.0.1,release the allocated container
Release-2.0.1,notify failed message to the worker
Release-2.0.1,remove the worker attempt from heartbeat timeout listen list
Release-2.0.1,release the allocated container
Release-2.0.1,notify killed message to the worker
Release-2.0.1,remove the worker attempt from heartbeat timeout listen list
Release-2.0.1,clean the container
Release-2.0.1,notify failed message to the worker
Release-2.0.1,remove the worker attempt from heartbeat timeout listen list
Release-2.0.1,record the finish time
Release-2.0.1,clean the container
Release-2.0.1,notify killed message to the worker
Release-2.0.1,remove the worker attempt from heartbeat timeout listening list
Release-2.0.1,record the finish time
Release-2.0.1,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
Release-2.0.1,set worker attempt location
Release-2.0.1,notify the register message to the worker
Release-2.0.1,record the launch time
Release-2.0.1,update worker attempt metrics
Release-2.0.1,update tasks metrics
Release-2.0.1,clean the container
Release-2.0.1,notify the worker attempt run successfully message to the worker
Release-2.0.1,record the finish time
Release-2.0.1,init a worker attempt for the worker
Release-2.0.1,schedule the worker attempt
Release-2.0.1,add diagnostic
Release-2.0.1,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.1,init and start a new attempt for this ps
Release-2.0.1,notify worker manager
Release-2.0.1,add diagnostic
Release-2.0.1,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.1,init and start a new attempt for this ps
Release-2.0.1,notify worker manager
Release-2.0.1,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
Release-2.0.1,register to Yarn RM
Release-2.0.1,send heartbeat to Yarn RM every rmPollInterval milliseconds
Release-2.0.1,"catch YarnRuntimeException, we should exit and need not retry"
Release-2.0.1,build heartbeat request
Release-2.0.1,send heartbeat request to rm
Release-2.0.1,"This can happen if the RM has been restarted. If it is in that state,"
Release-2.0.1,this application must clean itself up.
Release-2.0.1,Setting NMTokens
Release-2.0.1,assgin containers
Release-2.0.1,"if some container is not assigned, release them"
Release-2.0.1,handle finish containers
Release-2.0.1,dispatch container exit message to corresponding components
Release-2.0.1,killed by framework
Release-2.0.1,killed by framework
Release-2.0.1,get application finish state
Release-2.0.1,build application diagnostics
Release-2.0.1,TODO:add a job history for angel
Release-2.0.1,build unregister request
Release-2.0.1,send unregister request to rm
Release-2.0.1,Note this down for next interaction with ResourceManager
Release-2.0.1,based on blacklisting comments above we can end up decrementing more
Release-2.0.1,than requested. so guard for that.
Release-2.0.1,send the updated resource request to RM
Release-2.0.1,send 0 container count requests also to cancel previous requests
Release-2.0.1,Update resource requests
Release-2.0.1,try to assign to all nodes first to match node local
Release-2.0.1,try to match all rack local
Release-2.0.1,assign remaining
Release-2.0.1,Update resource requests
Release-2.0.1,send the container-assigned event to task attempt
Release-2.0.1,build the start container request use launch context
Release-2.0.1,send the start request to Yarn nm
Release-2.0.1,send the message that the container starts successfully to the corresponding component
Release-2.0.1,"after launching, send launched event to task attempt to move"
Release-2.0.1,it from ASSIGNED to RUNNING state
Release-2.0.1,send the message that the container starts failed to the corresponding component
Release-2.0.1,kill the remote container if already launched
Release-2.0.1,start a thread pool to startup the container
Release-2.0.1,See if we need up the pool size only if haven't reached the
Release-2.0.1,maximum limit yet.
Release-2.0.1,nodes where containers will run at *this* point of time. This is
Release-2.0.1,*not* the cluster size and doesn't need to be.
Release-2.0.1,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
Release-2.0.1,later is just a buffer so we are not always increasing the
Release-2.0.1,pool-size
Release-2.0.1,the events from the queue are handled in parallel
Release-2.0.1,using a thread pool
Release-2.0.1,return if already stopped
Release-2.0.1,shutdown any containers that might be left running
Release-2.0.1,Add one sync matrix
Release-2.0.1,addSyncMatrix();
Release-2.0.1,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
Release-2.0.1,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
Release-2.0.1,"LOG.info(""ps id = "" + psEntry.getKey());"
Release-2.0.1,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
Release-2.0.1,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
Release-2.0.1,"LOG.info(""matrix id = "" + metaEntry.getKey());"
Release-2.0.1,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
Release-2.0.1,}
Release-2.0.1,}
Release-2.0.1,get matrix ids in the parameter server report
Release-2.0.1,get the matrices parameter server need to create and delete
Release-2.0.1,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
Release-2.0.1,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
Release-2.0.1,Init control connection manager
Release-2.0.1,Get ps locations from master and put them to the location cache.
Release-2.0.1,Build and initialize rpc client to master
Release-2.0.1,Get psagent id
Release-2.0.1,Build PS control rpc client manager
Release-2.0.1,Build local location
Release-2.0.1,Initialize matrix meta information
Release-2.0.1,Start all services
Release-2.0.1,Stop all modules
Release-2.0.1,Stop all modules
Release-2.0.1,clock first
Release-2.0.1,wait
Release-2.0.1,Update generic resource counters
Release-2.0.1,Updating resources specified in ResourceCalculatorProcessTree
Release-2.0.1,Remove the CPU time consumed previously by JVM reuse
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,/ Plus a vector/matrix to the matrix stored in pss
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,/ Update a vector/matrix to the matrix stored in pss
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,/ Get values from pss use row/column indices
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"/ PSF get/update, use can implement their own psf"
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,/ Get a row or a batch of rows
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,TODO:
Release-2.0.1,Generate a flush request and put it to request queue
Release-2.0.1,Generate a clock request and put it to request queue
Release-2.0.1,Generate a merge request and put it to request queue
Release-2.0.1,Generate a merge request and put it to request queue
Release-2.0.1,"If the matrix op log cache does not exist for the matrix, create a new one for the"
Release-2.0.1,matrix
Release-2.0.1,and add it to cache maps
Release-2.0.1,Add the message to the tree map
Release-2.0.1,"If there are flush / clock requests blocked, we need to put this merge request into"
Release-2.0.1,the waiting queue
Release-2.0.1,Launch a merge worker to merge the update to matrix op log cache
Release-2.0.1,Remove the message from the tree map
Release-2.0.1,Wake up blocked flush/clock request
Release-2.0.1,Add flush/clock request to listener list to waiting for all the existing
Release-2.0.1,updates are merged
Release-2.0.1,Wake up blocked flush/clock request
Release-2.0.1,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
Release-2.0.1,blocked.
Release-2.0.1,Get next merge message sequence id
Release-2.0.1,Wake up listeners(flush/clock requests) that have little sequence id than current merge
Release-2.0.1,position
Release-2.0.1,Wake up blocked merge requests
Release-2.0.1,Get minimal sequence id from listeners
Release-2.0.1,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
Release-2.0.1,should flush updates to local matrix storage
Release-2.0.1,unused now
Release-2.0.1,TODO:
Release-2.0.1,Doing average or not
Release-2.0.1,Filter un-important update
Release-2.0.1,Split this row according the matrix partitions
Release-2.0.1,Set split context
Release-2.0.1,Remove the row from matrix
Release-2.0.1,buf.writeDouble(0.0);
Release-2.0.1,TODO
Release-2.0.1,TODO: write map default value
Release-2.0.1,buf.writeDouble(0);
Release-2.0.1,TODO:
Release-2.0.1,TODO:
Release-2.0.1,TODO:
Release-2.0.1,"For each partition, we generate a update split."
Release-2.0.1,"Although the split is empty for partitions those without any update data,"
Release-2.0.1,we still need to generate a update split to update the clock info on ps.
Release-2.0.1,"For each partition, we generate a update split."
Release-2.0.1,"Although the split is empty for partitions those without any update data,"
Release-2.0.1,we still need to generate a update split to update the clock info on ps.
Release-2.0.1,"For each partition, we generate a update split."
Release-2.0.1,"Although the split is empty for partitions those without any update data,"
Release-2.0.1,we still need to generate a update split to update the clock info on ps.
Release-2.0.1,"For each partition, we generate a update split."
Release-2.0.1,"Although the split is empty for partitions those without any update data,"
Release-2.0.1,we still need to generate a update split to update the clock info on ps.
Release-2.0.1,"For each partition, we generate a update split."
Release-2.0.1,"Although the split is empty for partitions those without any update data,"
Release-2.0.1,we still need to generate a update split to update the clock info on ps.
Release-2.0.1,"For each partition, we generate a update split."
Release-2.0.1,"Although the split is empty for partitions those without any update data,"
Release-2.0.1,we still need to generate a update split to update the clock info on ps.
Release-2.0.1,"LOG.error(""channel "" + ctx.channel() + "" inactive"");"
Release-2.0.1,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
Release-2.0.1,"LOG.error(""put response message queue failed "", e);"
Release-2.0.1,Use Epoll for linux
Release-2.0.1,Update location table
Release-2.0.1,Remove the server from failed list
Release-2.0.1,Notify refresh success message to request dispatcher
Release-2.0.1,Check PS exist or not
Release-2.0.1,Check heartbeat timeout
Release-2.0.1,Check PS restart or not
Release-2.0.1,private final HashSet<ParameterServerId> refreshingServerSet;
Release-2.0.1,Add it to failed rpc list
Release-2.0.1,Add the server to gray server list
Release-2.0.1,Add it to failed rpc list
Release-2.0.1,Add the server to gray server list
Release-2.0.1,Move from gray server list to failed server list
Release-2.0.1,Handle the RPCS to this server
Release-2.0.1,Submit the schedulable failed get RPCS
Release-2.0.1,Submit new get RPCS
Release-2.0.1,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.0.1,"If the queue is empty, just return 0"
Release-2.0.1,"If request is not over limit, just submit it"
Release-2.0.1,Submit the schedulable failed get RPCS
Release-2.0.1,Submit new put RPCS
Release-2.0.1,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.0.1,"LOG.info(""choose put server "" + psIds[index]);"
Release-2.0.1,Check all pending RPCS
Release-2.0.1,Check get channel context
Release-2.0.1,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.0.1,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.0.1,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.0.1,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.0.1,channelManager.printPools();
Release-2.0.1,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
Release-2.0.1,if(ts - entry.getValue() > requestTimeOut * 2)  {
Release-2.0.1,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
Release-2.0.1,"+ "" milliseconds, close all channels to it"");"
Release-2.0.1,closeChannels(entry.getKey());
Release-2.0.1,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
Release-2.0.1,}
Release-2.0.1,}
Release-2.0.1,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
Release-2.0.1,Remove all pending RPCS
Release-2.0.1,Close all channel to this PS
Release-2.0.1,private Channel getChannel(Location loc) throws Exception {
Release-2.0.1,"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));"
Release-2.0.1,}
Release-2.0.1,private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {
Release-2.0.1,"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext"
Release-2.0.1,.get()
Release-2.0.1,.getConf()
Release-2.0.1,".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,"
Release-2.0.1,AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));
Release-2.0.1,}
Release-2.0.1,"LOG.error(""send request "" + request + "" is interrupted"");"
Release-2.0.1,"LOG.error(""send request "" + request + "" failed, "", e);"
Release-2.0.1,Get server id and location for this request
Release-2.0.1,"If location is null, means that the server is not ready"
Release-2.0.1,Get the channel for the location
Release-2.0.1,Check if need get token first
Release-2.0.1,Serialize the request
Release-2.0.1,Send the request
Release-2.0.1,get a channel to server from pool
Release-2.0.1,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.0.1,request.getContext().setChannelPool(pool);
Release-2.0.1,Allocate the bytebuf and serialize the request
Release-2.0.1,find the partition request context from cache
Release-2.0.1,"updateMatrixCache(request.getPartKey(), response.getPartition());"
Release-2.0.1,"updateMatrixCache(request.getPartKey(), response.getRowsSplit());"
Release-2.0.1,TODO
Release-2.0.1,ServerRow rowSplit = PSAgentContext.get().getMatricesCache()
Release-2.0.1,".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),"
Release-2.0.1,request.getRowIndex());
Release-2.0.1,response.setRowSplit(rowSplit);
Release-2.0.1,"updateMatrixCache(request.getPartKey(), response.getRowSplit());"
Release-2.0.1,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.0.1,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.0.1,TODO
Release-2.0.1,"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {"
Release-2.0.1,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);"
Release-2.0.1,}
Release-2.0.1,"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {"
Release-2.0.1,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);"
Release-2.0.1,}
Release-2.0.1,"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {"
Release-2.0.1,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);"
Release-2.0.1,}
Release-2.0.1,Get partitions for this row
Release-2.0.1,Distinct get row requests
Release-2.0.1,Need get from ps or storage/cache
Release-2.0.1,"Switch to new request id, send a new request"
Release-2.0.1,First get this row from matrix storage
Release-2.0.1,MatrixStorage matrixStorage =
Release-2.0.1,PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);
Release-2.0.1,TVector row = matrixStorage.getRow(rowIndex);
Release-2.0.1,if (row != null && row.getClock() >= clock) {
Release-2.0.1,result.set(row);
Release-2.0.1,return row;
Release-2.0.1,}
Release-2.0.1,Get row splits of this row from the matrix cache first
Release-2.0.1,responseCache.addSubResponse(rowSplit);
Release-2.0.1,"If the row split does not exist in cache, get it from parameter server"
Release-2.0.1,Wait the final result
Release-2.0.1,Put it to the matrix cache
Release-2.0.1,"matrixStorage.addRow(rowIndex, row);"
Release-2.0.1,Just wait result
Release-2.0.1,Split the param use matrix partitions
Release-2.0.1,Send request to PSS
Release-2.0.1,Split the matrix oplog according to the matrix partitions
Release-2.0.1,"If need update clock, we should send requests to all partitions"
Release-2.0.1,Send request to PSS
Release-2.0.1,Filter the rowIds which are fetching now
Release-2.0.1,Send the rowIndex to rpc dispatcher and return immediately
Release-2.0.1,"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));"
Release-2.0.1,"LOG.info(""start to request "" + requestId);"
Release-2.0.1,"LOG.info(""start to request "" + requestId);"
Release-2.0.1,Split param use matrix partitons
Release-2.0.1,"If all sub-results are received, just remove request and result cache"
Release-2.0.1,"LOG.info(""request = "" + request + "", cache = "" + cache);"
Release-2.0.1,"LOG.info(""start to merge "" + cache + "" for request "" + request);"
Release-2.0.1,"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.0.1,Split this row according the matrix partitions
Release-2.0.1,Set split context
Release-2.0.1,Split this row according the matrix partitions
Release-2.0.1,Set split context
Release-2.0.1,long startTs = System.currentTimeMillis();
Release-2.0.1,"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.0.1,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
Release-2.0.1,Generate dispatch items and add them to the corresponding queues
Release-2.0.1,Filter the rowIds which are fetching now
Release-2.0.1,Sort the parts by partitionId
Release-2.0.1,Sort partition keys use start column index
Release-2.0.1,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.0.1,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.0.1,});
Release-2.0.1,"For each partition, we generate a update split."
Release-2.0.1,"Although the split is empty for partitions those without any update data,"
Release-2.0.1,we still need to generate a update split to update the clock info on ps.
Release-2.0.1,Sort the parts by partitionId
Release-2.0.1,Sort partition keys use start column index
Release-2.0.1,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.0.1,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.0.1,});
Release-2.0.1,"For each partition, we generate a update split."
Release-2.0.1,"Although the split is empty for partitions those without any update data,"
Release-2.0.1,we still need to generate a update split to update the clock info on ps.
Release-2.0.1,Put the row split to the cache(row index to row splits map)
Release-2.0.1,"If all splits of the row are received, means this row can be merged"
Release-2.0.1,TODO
Release-2.0.1,TODO
Release-2.0.1,/////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,TODO
Release-2.0.1,buf.writeDouble(0);
Release-2.0.1,TODO
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,Now we just support pipelined row splits merging for dense type row
Release-2.0.1,Pre-fetching is disable default
Release-2.0.1,matrix id to clock map
Release-2.0.1,"task index, it must be unique for whole application"
Release-2.0.1,Deserialize data splits meta
Release-2.0.1,Get workers
Release-2.0.1,Send request to every ps
Release-2.0.1,Wait the responses
Release-2.0.1,Update clock cache
Release-2.0.1,if(syncNum % 1024 == 0) {
Release-2.0.1,}
Release-2.0.1,"Use simple flow, do not use any cache"
Release-2.0.1,Get row from cache.
Release-2.0.1,"if row clock is satisfy ssp staleness limit, just return."
Release-2.0.1,Get row from ps.
Release-2.0.1,Wait until the clock value of this row is greater than or equal to the value
Release-2.0.1,"For ASYNC mode, just get from pss."
Release-2.0.1,"For BSP/SSP, get rows from storage/cache first"
Release-2.0.1,Get from ps.
Release-2.0.1,Wait until the clock value of this row is greater than or equal to the value
Release-2.0.1,"For ASYNC, just get rows from pss."
Release-2.0.1,no more retries.
Release-2.0.1,calculate sleep time and return.
Release-2.0.1,parse the i-th sleep-time
Release-2.0.1,parse the i-th number-of-retries
Release-2.0.1,calculateSleepTime may overflow.
Release-2.0.1,"A few common retry policies, with no delays."
Release-2.0.1,Read matrix meta from meta file
Release-2.0.1,Save partitions to files use fork-join
Release-2.0.1,Write the ps matrix meta to the meta file
Release-2.0.1,matrix.startServering();
Release-2.0.1,return;
Release-2.0.1,Read matrix meta from meta file
Release-2.0.1,Load partitions from file use fork-join
Release-2.0.1,Read matrix meta from meta file
Release-2.0.1,Sort partitions
Release-2.0.1,int size = rows.length;
Release-2.0.1,int size = rows.length;
Release-2.0.1,int size = rows.size();
Release-2.0.1,int size = rows.size();
Release-2.0.1,int size = rows.size();
Release-2.0.1,int size = rows.size();
Release-2.0.1,int size = rows.size();
Release-2.0.1,int size = rows.size();
Release-2.0.1,close is a local operation and should finish within milliseconds; timeout just to be safe
Release-2.0.1,response will be null for one way messages.
Release-2.0.1,maxFrameLength = 2G
Release-2.0.1,lengthFieldOffset = 0
Release-2.0.1,lengthFieldLength = 8
Release-2.0.1,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
Release-2.0.1,"initialBytesToStrip = 8, i.e. strip out the length field itself"
Release-2.0.1,"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())"
Release-2.0.1,.toString();
Release-2.0.1,indicates whether this connection's life cycle is managed
Release-2.0.1,See if we already have a connection (common case)
Release-2.0.1,create a unique lock for this RS + protocol (if necessary)
Release-2.0.1,get the RS lock
Release-2.0.1,do one more lookup in case we were stalled above
Release-2.0.1,Only create isa when we need to.
Release-2.0.1,definitely a cache miss. establish an RPC for
Release-2.0.1,this RS
Release-2.0.1,Throw what the RemoteException was carrying.
Release-2.0.1,check
Release-2.0.1,every
Release-2.0.1,minutes
Release-2.0.1,TODO
Release-2.0.1,创建failoverHandler
Release-2.0.1,"The number of times this invocation handler has ever been failed over,"
Release-2.0.1,before this method invocation attempt. Used to prevent concurrent
Release-2.0.1,failed method invocations from triggering multiple failover attempts.
Release-2.0.1,Make sure that concurrent failed method invocations
Release-2.0.1,only cause a
Release-2.0.1,single actual fail over.
Release-2.0.1,RpcController + Message in the method args
Release-2.0.1,(generated code from RPC bits in .proto files have
Release-2.0.1,RpcController)
Release-2.0.1,"LOG.info(""method "" + method.getName() + ""construct request time = """
Release-2.0.1,+ (System.currentTimeMillis() - beforeConstructTs));
Release-2.0.1,get an instance of the method arg type
Release-2.0.1,RpcController + Message in the method args
Release-2.0.1,(generated code from RPC bits in .proto files have
Release-2.0.1,RpcController)
Release-2.0.1,Message (hand written code usually has only a single
Release-2.0.1,argument)
Release-2.0.1,log any RPC responses that are slower than the configured
Release-2.0.1,warn
Release-2.0.1,response time or larger than configured warning size
Release-2.0.1,"when tagging, we let TooLarge trump TooSmall to keep"
Release-2.0.1,output simple
Release-2.0.1,note that large responses will often also be slow.
Release-2.0.1,provides a count of log-reported slow responses
Release-2.0.1,RpcController + Message in the method args
Release-2.0.1,(generated code from RPC bits in .proto files have
Release-2.0.1,RpcController)
Release-2.0.1,unexpected
Release-2.0.1,"in the protobuf methods, args[1] is the only significant argument"
Release-2.0.1,for JSON encoding
Release-2.0.1,base information that is reported regardless of type of call
Release-2.0.1,Disable Nagle's Algorithm since we don't want packets to wait
Release-2.0.1,Configure the event pipeline factory.
Release-2.0.1,Make a new connection.
Release-2.0.1,Remove all pending requests (will be canceled after relinquishing
Release-2.0.1,write lock).
Release-2.0.1,Cancel any pending requests by sending errors to the callbacks:
Release-2.0.1,Close the channel:
Release-2.0.1,Close the connection:
Release-2.0.1,Shut down all thread pools to exit.
Release-2.0.1,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
Release-2.0.1,See NettyServer.prepareResponse for where we write out the response.
Release-2.0.1,"It writes the call.id (int), a boolean signifying any error (and if"
Release-2.0.1,"so the exception name/trace), and the response bytes"
Release-2.0.1,Read the call id.
Release-2.0.1,"When the stream is closed, protobuf doesn't raise an EOFException,"
Release-2.0.1,"instead, it returns a null message object."
Release-2.0.1,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
Release-2.0.1,System.currentTimeMillis());
Release-2.0.1,"It would be good widen this to just Throwable, but IOException is what we"
Release-2.0.1,allow now
Release-2.0.1,not implemented
Release-2.0.1,not implemented
Release-2.0.1,"track what RpcEngine is used by a proxy class, for stopProxy()"
Release-2.0.1,cache of RpcEngines by protocol
Release-2.0.1,return the RpcEngine configured to handle a protocol
Release-2.0.1,We only handle the ConnectException.
Release-2.0.1,This is the exception we can't handle.
Release-2.0.1,check if timed out
Release-2.0.1,wait for retry
Release-2.0.1,IGNORE
Release-2.0.1,return the RpcEngine that handles a proxy object
Release-2.0.1,The default implementation works synchronously
Release-2.0.1,punt: allocate a new buffer & copy into it
Release-2.0.1,Parse cmd parameters
Release-2.0.1,load hadoop configuration
Release-2.0.1,load angel system configuration
Release-2.0.1,load user configuration:
Release-2.0.1,load user config file
Release-2.0.1,load command line parameters
Release-2.0.1,load user job resource files
Release-2.0.1,load ml conf file for graph based algorithm
Release-2.0.1,load user job jar if it exist
Release-2.0.1,Expand the environment variable
Release-2.0.1,Add default fs(local fs) for lib jars.
Release-2.0.1,"LOG.info(System.getProperty(""user.dir""));"
Release-2.0.1,get tokens for all the required FileSystems..
Release-2.0.1,Whether we need to recursive look into the directory structure
Release-2.0.1,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.0.1,user provided one (if any).
Release-2.0.1,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.0.1,get tokens for all the required FileSystems..
Release-2.0.1,Whether we need to recursive look into the directory structure
Release-2.0.1,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.0.1,user provided one (if any).
Release-2.0.1,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.0.1,a simple hdfs copy function assume src path and dest path are in same hdfs
Release-2.0.1,and FileSystem object has same schema
Release-2.0.1,"LOG.warn(""interrupted while sleeping"", ie);"
Release-2.0.1,public static String getHostname() {
Release-2.0.1,try {
Release-2.0.1,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
Release-2.0.1,} catch (UnknownHostException uhe) {
Release-2.0.1,}
Release-2.0.1,"return new StringBuilder().append("""").append(uhe).toString();"
Release-2.0.1,}
Release-2.0.1,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
Release-2.0.1,String hostname = getHostname();
Release-2.0.1,String classname = clazz.getSimpleName();
Release-2.0.1,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
Release-2.0.1,"StringBuilder().append(""Starting "").append(classname).toString(), new"
Release-2.0.1,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
Release-2.0.1,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
Release-2.0.1,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
Release-2.0.1,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
Release-2.0.1,
Release-2.0.1,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
Release-2.0.1,public void run() {
Release-2.0.1,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
Release-2.0.1,"this.val$classname + "" at "" + this.val$hostname}));"
Release-2.0.1,}
Release-2.0.1,});
Release-2.0.1,}
Release-2.0.1,"We we interrupted because we're meant to stop? If not, just"
Release-2.0.1,continue ignoring the interruption
Release-2.0.1,Recalculate waitTime.
Release-2.0.1,// Begin delegation to Thread
Release-2.0.1,// End delegation to Thread
Release-2.0.1,instance submitter class
Release-2.0.1,Obtain filename from path
Release-2.0.1,Split filename to prexif and suffix (extension)
Release-2.0.1,Check if the filename is okay
Release-2.0.1,Prepare temporary file
Release-2.0.1,Prepare buffer for data copying
Release-2.0.1,Open and check input stream
Release-2.0.1,Open output stream and copy data between source file in JAR and the temporary file
Release-2.0.1,"If read/write fails, close streams safely before throwing an exception"
Release-2.0.1,"Finally, load the library"
Release-2.0.1,little endian load order
Release-2.0.1,tail
Release-2.0.1,fallthrough
Release-2.0.1,fallthrough
Release-2.0.1,finalization
Release-2.0.1,fmix(h1);
Release-2.0.1,----------
Release-2.0.1,body
Release-2.0.1,----------
Release-2.0.1,tail
Release-2.0.1,----------
Release-2.0.1,finalization
Release-2.0.1,----------
Release-2.0.1,body
Release-2.0.1,----------
Release-2.0.1,tail
Release-2.0.1,----------
Release-2.0.1,finalization
Release-2.0.1,throw new AngelException(e);
Release-2.0.1,JobStateProto jobState = report.getJobState();
Release-2.0.1,Check need load matrices
Release-2.0.1,Used for java code to get a AngelClient instance
Release-2.0.1,Used for python code to get a AngelClient instance
Release-2.0.1,load user job resource files
Release-2.0.1,the leaf level file should be readable by others
Release-2.0.1,the subdirs in the path should have execute permissions for
Release-2.0.1,others
Release-2.0.1,2.get job id
Release-2.0.1,Credentials credentials = new Credentials();
Release-2.0.1,4.copy resource files to hdfs
Release-2.0.1,5.write configuration to a xml file
Release-2.0.1,6.create am container context
Release-2.0.1,7.Submit to ResourceManager
Release-2.0.1,8.get app master client
Release-2.0.1,Create a number of filenames in the JobTracker's fs namespace
Release-2.0.1,add all the command line files/ jars and archive
Release-2.0.1,first copy them to jobtrackers filesystem
Release-2.0.1,should not throw a uri exception
Release-2.0.1,should not throw an uri excpetion
Release-2.0.1,set the timestamps of the archives and files
Release-2.0.1,set the public/private visibility of the archives and files
Release-2.0.1,get DelegationToken for each cached file
Release-2.0.1,check if we do not need to copy the files
Release-2.0.1,is jt using the same file system.
Release-2.0.1,just checking for uri strings... doing no dns lookups
Release-2.0.1,to see if the filesystems are the same. This is not optimal.
Release-2.0.1,but avoids name resolution.
Release-2.0.1,this might have name collisions. copy will throw an exception
Release-2.0.1,parse the original path to create new path
Release-2.0.1,check for ports
Release-2.0.1,Write job file to JobTracker's fs
Release-2.0.1,Setup resource requirements
Release-2.0.1,Setup LocalResources
Release-2.0.1,Setup security tokens
Release-2.0.1,Setup the command to run the AM
Release-2.0.1,Add AM user command opts
Release-2.0.1,Final command
Release-2.0.1,Setup the CLASSPATH in environment
Release-2.0.1,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
Release-2.0.1,Setup the environment variables for Admin first
Release-2.0.1,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
Release-2.0.1,Parse distributed cache
Release-2.0.1,Setup ContainerLaunchContext for AM container
Release-2.0.1,Set up the ApplicationSubmissionContext
Release-2.0.1,private volatile PS2PSPusherImpl ps2PSPusher;
Release-2.0.1,TODO
Release-2.0.1,Add tokens to new user so that it may execute its task correctly.
Release-2.0.1,TODO
Release-2.0.1,to exit
Release-2.0.1,TODO
Release-2.0.1,TODO
Release-2.0.1,ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));
Release-2.0.1,context.getSnapshotManager().processRecovery();
Release-2.0.1,Recover PS from snapshot or load path
Release-2.0.1,First check snapshot
Release-2.0.1,Check load path setting
Release-2.0.1,TODO
Release-2.0.1,if(ps2PSPusher != null) {
Release-2.0.1,ps2PSPusher.start();
Release-2.0.1,}
Release-2.0.1,public PS2PSPusherImpl getPs2PSPusher() {
Release-2.0.1,return ps2PSPusher;
Release-2.0.1,}
Release-2.0.1,"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");"
Release-2.0.1,"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);"
Release-2.0.1,Release the input buffer
Release-2.0.1,Release the input buffer
Release-2.0.1,"1. handle the rpc, get the response"
Release-2.0.1,Release the input buffer
Release-2.0.1,2. Serialize the response
Release-2.0.1,Send the serialized response
Release-2.0.1,Exception happened
Release-2.0.1,write seq id
Release-2.0.1,Just serialize the head
Release-2.0.1,Exception happened
Release-2.0.1,Allocate result buffer
Release-2.0.1,Exception happened
Release-2.0.1,Just serialize the head
Release-2.0.1,Exception happened
Release-2.0.1,Reset the response and allocate buffer again
Release-2.0.1,Get partition and check the partition state
Release-2.0.1,Get the stored pss for this partition
Release-2.0.1,"Check this ps is the master ps for this location, only master ps can accept the update"
Release-2.0.1,Check the partition state again
Release-2.0.1,Start to put the update to the slave pss
Release-2.0.1,TODO
Release-2.0.1,"context.getPS2PSPusher().put(request, in, partLoc);"
Release-2.0.1,Get partition and check the partition state
Release-2.0.1,Get the stored pss for this partition
Release-2.0.1,"Check this ps is the master ps for this partition, if not, just return failed"
Release-2.0.1,Start to put the update to the slave pss
Release-2.0.1,TODO
Release-2.0.1,return ServerState.GENERAL;
Release-2.0.1,Use Epoll for linux
Release-2.0.1,public String uuid;
Release-2.0.1,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
Release-2.0.1,this.channelPool = channelPool;
Release-2.0.1,}
Release-2.0.1,private final ParameterServer psServer;
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO:default value
Release-2.0.1,buf.readDouble();
Release-2.0.1,TODO
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"////// network io method, for model transform"
Release-2.0.1,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,public ObjectIterator<Long2FloatMap.Entry> getIter() {
Release-2.0.1,return ((LongFloatVector) row).getStorage().entryIterator();
Release-2.0.1,}
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,public ObjectIterator<Long2LongMap.Entry> getIter() {
Release-2.0.1,return ((LongLongVector) row).getStorage().entryIterator();
Release-2.0.1,}
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.1,and call endWrite/endRead after
Release-2.0.1,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.1,TODO: dynamic add/delete row
Release-2.0.1,private final List<PartitionKey> partitionKeys;
Release-2.0.1,Use Epoll for linux
Release-2.0.1,find the partition request context from cache
Release-2.0.1,get a channel to server from pool
Release-2.0.1,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.0.1,channelManager.removeChannelPool(loc);
Release-2.0.1,Generate seq id
Release-2.0.1,Create a RecoverPartRequest
Release-2.0.1,Serialize the request
Release-2.0.1,Change the seqId for the request
Release-2.0.1,Serialize the request
Release-2.0.1,"First check the state of the channels in the pool, if a channel is unused, just return"
Release-2.0.1,"If all channels are in use, create a new channel or wait"
Release-2.0.1,Create a new channel
Release-2.0.1,"add the PSAgentContext,need fix"
Release-2.0.1,TODO:add more vector type
Release-2.0.1,TODO : subDim set
Release-2.0.1,Sort the parts by partitionId
Release-2.0.1,Sort partition keys use start column index
Release-2.0.1,"For each partition, we generate a update split."
Release-2.0.1,"Although the split is empty for partitions those without any update data,"
Release-2.0.1,we still need to generate a update split to update the clock info on ps.
Release-2.0.1,Sort the parts by partitionId
Release-2.0.1,Sort partition keys use start column index
Release-2.0.1,"For each partition, we generate a update split."
Release-2.0.1,"Although the split is empty for partitions those without any update data,"
Release-2.0.1,we still need to generate a update split to update the clock info on ps.
Release-2.0.1,write the max abs
Release-2.0.1,---------------------------------------------------
Release-2.0.1,---------------------------------------------------
Release-2.0.1,---------------------------------------------------------------
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,axis = 0: on rows
Release-2.0.1,axis = 1: on cols
Release-2.0.1,1. find the insert point
Release-2.0.1,2. check the capacity and insert
Release-2.0.1,3. increase size
Release-2.0.1,1. find the insert point
Release-2.0.1,2. check the capacity and insert
Release-2.0.1,3. increase size
Release-2.0.1,1. find the insert point
Release-2.0.1,2. check the capacity and insert
Release-2.0.1,3. increase size
Release-2.0.1,1. find the insert point
Release-2.0.1,2. check the capacity and insert
Release-2.0.1,3. increase size
Release-2.0.1,1. find the insert point
Release-2.0.1,2. check the capacity and insert
Release-2.0.1,3. increase size
Release-2.0.1,1. find the insert point
Release-2.0.1,2. check the capacity and insert
Release-2.0.1,3. increase size
Release-2.0.1,1. find the insert point
Release-2.0.1,2. check the capacity and insert
Release-2.0.1,3. increase size
Release-2.0.1,1. find the insert point
Release-2.0.1,2. check the capacity and insert
Release-2.0.1,3. increase size
Release-2.0.1,-----------------
Release-2.0.1,-----------------
Release-2.0.1,-----------------
Release-2.0.1,-----------------
Release-2.0.1,-----------------
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,multi-rehash
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,"slower but memory efficient, for small vector only"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,to avoid multi-rehash
Release-2.0.1,"Transform mat1, generate a new matrix"
Release-2.0.1,Split the row indices of mat1Trans
Release-2.0.1,Parallel execute use fork-join
Release-2.0.1,"Transform mat1, generate a new matrix"
Release-2.0.1,Split the row indices of mat1Trans
Release-2.0.1,Parallel execute use fork-join
Release-2.0.1,"Get the sub-matrix of left matrix, split by row"
Release-2.0.1,"Get the sub-matrix of left matrix, split by row"
Release-2.0.1,"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.0.1,"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.0.1,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.0.1,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.0.1,add dense double matrix
Release-2.0.1,TODO Auto-generated method stub
Release-2.0.1,TODO Auto-generated method stub
Release-2.0.1,TODO Auto-generated method stub
Release-2.0.1,get configuration from config file
Release-2.0.1,set localDir with enviroment set by nm.
Release-2.0.1,get master location
Release-2.0.1,init task manager and start tasks
Release-2.0.1,start heartbeat thread
Release-2.0.1,taskManager.assignTaskIds(response.getTaskidsList());
Release-2.0.1,todo
Release-2.0.1,"if worker timeout, it may be knocked off."
Release-2.0.1,"SUCCESS, do nothing"
Release-2.0.1,heartbeatFailedTime = 0;
Release-2.0.1,private KEY currentKey;
Release-2.0.1,will be created
Release-2.0.1,TODO Auto-generated method stub
Release-2.0.1,Bitmap bitmap = new Bitmap();
Release-2.0.1,int max = indexArray[size - 1];
Release-2.0.1,byte [] bitIndexArray = new byte[max / 8 + 1];
Release-2.0.1,for(int i = 0; i < size; i++){
Release-2.0.1,int bitIndex = indexArray[i] >> 3;
Release-2.0.1,int bitOffset = indexArray[i] - (bitIndex << 3);
Release-2.0.1,switch(bitOffset){
Release-2.0.1,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
Release-2.0.1,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
Release-2.0.1,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
Release-2.0.1,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
Release-2.0.1,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
Release-2.0.1,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
Release-2.0.1,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
Release-2.0.1,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
Release-2.0.1,}
Release-2.0.1,}
Release-2.0.1,"true, false"
Release-2.0.1,//////////////////////////////
Release-2.0.1,Application Configs
Release-2.0.1,//////////////////////////////
Release-2.0.1,//////////////////////////////
Release-2.0.1,Master Configs
Release-2.0.1,//////////////////////////////
Release-2.0.1,//////////////////////////////
Release-2.0.1,Worker Configs
Release-2.0.1,//////////////////////////////
Release-2.0.1,//////////////////////////////
Release-2.0.1,Task Configs
Release-2.0.1,//////////////////////////////
Release-2.0.1,//////////////////////////////
Release-2.0.1,ParameterServer Configs
Release-2.0.1,//////////////////////////////
Release-2.0.1,////////////////// IPC //////////////////////////
Release-2.0.1,//////////////////////////////
Release-2.0.1,Matrix transfer Configs.
Release-2.0.1,//////////////////////////////
Release-2.0.1,//////////////////////////////
Release-2.0.1,Matrix transfer Configs.
Release-2.0.1,//////////////////////////////
Release-2.0.1,Configs used to ANGEL_PS_PSAGENT running mode future.
Release-2.0.1,model parse
Release-2.0.1,Mark whether use pyangel or not.
Release-2.0.1,private Configuration conf;
Release-2.0.1,"Configuration that should be used in python environment, there should only be one"
Release-2.0.1,configuration instance in each Angel context.
Release-2.0.1,Use private access means jconf should not be changed or modified in this way.
Release-2.0.1,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
Release-2.0.1,Do nothing
Release-2.0.1,To-DO: add other ways to justify different value types
Release-2.0.1,"This is so ugly, must re-implement by more elegance way"
Release-2.0.1,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
Release-2.0.1,and other files submitted by user.
Release-2.0.1,Launch python process
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Data format
Release-2.0.1,Feature number of train data
Release-2.0.1,Tree number
Release-2.0.1,Tree depth
Release-2.0.1,Split number
Release-2.0.1,Feature sample ratio
Release-2.0.1,Ratio of validation
Release-2.0.1,Learning rate
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Set data format
Release-2.0.1,"Set angel resource, #worker, #task, #PS"
Release-2.0.1,Set GBDT algorithm parameters
Release-2.0.1,Set training data path
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set predict data path
Release-2.0.1,Set load model path
Release-2.0.1,Set predict result path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Train batch number per epoch.
Release-2.0.1,Batch number
Release-2.0.1,Model type
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Set data format
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,set sgd LR algorithm parameters #feature #epoch
Release-2.0.1,Set trainning data path
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Train batch number per epoch.
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Set data format
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,set sgd FM algorithm parameters #feature #epoch
Release-2.0.1,Set trainning data path
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,Model type
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,set sgd LR algorithm parameters #feature #epoch
Release-2.0.1,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
Release-2.0.1,predictTest();
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Set data format
Release-2.0.1,Set trainning data path
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set data format
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,class number
Release-2.0.1,Model type
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Set data format
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,set sgd LR algorithm parameters #feature #epoch
Release-2.0.1,Set log path
Release-2.0.1,Set trainning data path
Release-2.0.1,Set save model path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set save model path
Release-2.0.1,Set actionType incremental train
Release-2.0.1,Set log path
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set predict result path
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set training data path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Cluster center number
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Sample ratio per mini-batch
Release-2.0.1,C
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.0.1,Set data format
Release-2.0.1,Set trainning data path
Release-2.0.1,Set save model path
Release-2.0.1,Set log save path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set save model path
Release-2.0.1,Set actionType incremental train
Release-2.0.1,Set log path
Release-2.0.1,Set testing data path
Release-2.0.1,Set load model path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Model type
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Set data format
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,set sgd LR algorithm parameters #feature #epoch
Release-2.0.1,Set trainning data path
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set predict result path
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Data is classification
Release-2.0.1,Model is classification
Release-2.0.1,Train batch number per epoch.
Release-2.0.1,loss delta
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Set data format
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,set sgd LR algorithm parameters #feature #epoch
Release-2.0.1,Set trainning data path
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set save model path
Release-2.0.1,Set actionType incremental train
Release-2.0.1,Set log path
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,Feature number of train data
Release-2.0.1,Total iteration number
Release-2.0.1,Validation sample Ratio
Release-2.0.1,"Data format, libsvm or dummy"
Release-2.0.1,Data is classification
Release-2.0.1,Model is classification
Release-2.0.1,Train batch number per epoch.
Release-2.0.1,Learning rate
Release-2.0.1,Decay of learning rate
Release-2.0.1,Regularization coefficient
Release-2.0.1,Set local deploy mode
Release-2.0.1,Set basic configuration keys
Release-2.0.1,Set data format
Release-2.0.1,"set angel resource parameters #worker, #task, #PS"
Release-2.0.1,set sgd LR algorithm parameters #feature #epoch
Release-2.0.1,Set trainning data path
Release-2.0.1,Set save model path
Release-2.0.1,Set log path
Release-2.0.1,Set actionType train
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set save model path
Release-2.0.1,Set actionType incremental train
Release-2.0.1,Set log path
Release-2.0.1,Set trainning data path
Release-2.0.1,Set load model path
Release-2.0.1,Set predict result path
Release-2.0.1,Set actionType prediction
Release-2.0.1,TODO: optimize int key indices
Release-2.0.1,"System.out.println(""deserialize cols.length="" + nCols);"
Release-2.0.1,"System.out.print(""deserialize "");"
Release-2.0.1,"System.out.print(cols[c] + "" "");"
Release-2.0.1,System.out.println();
Release-2.0.1,TODO Auto-generated method stub
Release-2.0.1,"LOG.info(""original float length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.0.1,"LOG.info(""original double length: "" + len + Arrays.toString(Arrays.copyOfRange(arr, start, end)));"
Release-2.0.1,"LOG.info(""parsed float length: "" + length + Arrays.toString(arr));"
Release-2.0.1,"LOG.info(""parsed double length: "" + length + Arrays.toString(arr));"
Release-2.0.1,"ground truth: positive, precision: positive"
Release-2.0.1,start row index for words
Release-2.0.1,start row index for docs
Release-2.0.1,doc ids
Release-2.0.1,topic assignments
Release-2.0.1,word to docs reverse index
Release-2.0.1,count word
Release-2.0.1,build word start index
Release-2.0.1,build word to doc reverse idx
Release-2.0.1,build dks
Release-2.0.1,dks = new TraverseHashMap[n_docs];
Release-2.0.1,for (int d = 0; d < n_docs; d++) {
Release-2.0.1,if (K < Short.MAX_VALUE) {
Release-2.0.1,if (docs.get(d).len < Byte.MAX_VALUE)
Release-2.0.1,dks[d] = new S2BTraverseMap(docs.get(d).len);
Release-2.0.1,if (docs.get(d).len < Short.MAX_VALUE)
Release-2.0.1,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
Release-2.0.1,else
Release-2.0.1,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
Release-2.0.1,} else {
Release-2.0.1,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
Release-2.0.1,}
Release-2.0.1,}
Release-2.0.1,build dks
Release-2.0.1,allocate update maps
Release-2.0.1,Skip if no token for this word
Release-2.0.1,Check whether error when fetching word-topic
Release-2.0.1,Build FTree for current word
Release-2.0.1,current doc
Release-2.0.1,old topic assignment
Release-2.0.1,"Check if error happens. if this happen, it's probably that failures happen to servers."
Release-2.0.1,We need to adjust the memory settings or network fetching parameters.
Release-2.0.1,Update statistics if needed
Release-2.0.1,Calculate psum and sample new topic
Release-2.0.1,Update statistics if needed
Release-2.0.1,Assign new topic
Release-2.0.1,Skip if no token for this word
Release-2.0.1,The starting point
Release-2.0.1,There's always an unused entry.
Release-2.0.1,The starting point
Release-2.0.1,There's always an unused entry.
Release-2.0.1,The starting point
Release-2.0.1,There's always an unused entry.
Release-2.0.1,The starting point
Release-2.0.1,There's always an unused entry.
Release-2.0.1,The starting point
Release-2.0.1,There's always an unused entry.
Release-2.0.1,The starting point
Release-2.0.1,There's always an unused entry.
Release-2.0.1,print();
Release-2.0.1,The starting point
Release-2.0.1,There's always an unused entry.
Release-2.0.1,The starting point
Release-2.0.1,There's always an unused entry.
Release-2.0.1,The starting point
Release-2.0.1,There's always an unused entry.
Release-2.0.1,The starting point
Release-2.0.1,There's always an unused entry.
Release-2.0.1,print();
Release-2.0.1,Write #rows
Release-2.0.1,Write each row
Release-2.0.1,dense
Release-2.0.1,sparse
Release-2.0.1,LOG.info(buf.refCnt());
Release-2.0.1,dense
Release-2.0.1,sparse
Release-2.0.1,calculate columns
Release-2.0.1,loss function
Release-2.0.1,gradient and hessian
Release-2.0.1,"categorical feature set, null: none, empty: all, else: partial"
Release-2.0.1,"node's end index in instancePos, instances in [start, end] belong to a tree node"
Release-2.0.1,initialize the phase
Release-2.0.1,current tree and depth
Release-2.0.1,create loss function
Release-2.0.1,calculate grad info of each instance
Release-2.0.1,"create data sketch, push candidate split value to PS"
Release-2.0.1,1. calculate candidate split value
Release-2.0.1,categorical features
Release-2.0.1,2. push local sketch to PS
Release-2.0.1,the leader worker
Release-2.0.1,merge categorical features
Release-2.0.1,create updates
Release-2.0.1,"pull the global sketch from PS, only called once by each worker"
Release-2.0.1,number of categorical feature
Release-2.0.1,sample feature
Release-2.0.1,push sampled feature set to the current tree
Release-2.0.1,create new tree
Release-2.0.1,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
Release-2.0.1,calculate gradient
Release-2.0.1,"1. create new tree, initialize tree nodes and node stats"
Release-2.0.1,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
Release-2.0.1,2.1. pull the sampled features of the current tree
Release-2.0.1,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
Release-2.0.1,"2.2. if use all the features, only called one"
Release-2.0.1,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
Release-2.0.1,4. set root node to active
Release-2.0.1,"5. reset instance position, set the root node's span"
Release-2.0.1,6. calculate gradient
Release-2.0.1,1. decide nodes that should be calculated
Release-2.0.1,2. decide calculated and subtracted tree nodes
Release-2.0.1,3. calculate threads
Release-2.0.1,wait until all threads finish
Release-2.0.1,4. subtract threads
Release-2.0.1,wait until all threads finish
Release-2.0.1,5. send histograms to PS
Release-2.0.1,6. update histogram cache
Release-2.0.1,clock
Release-2.0.1,find split
Release-2.0.1,"1. find responsible tree node, using RR scheme"
Release-2.0.1,2. pull gradient histogram
Release-2.0.1,2.1. get the name of this node's gradient histogram on PS
Release-2.0.1,2.2. pull the histogram
Release-2.0.1,2.3. find best split result of this tree node
Release-2.0.1,2.3.1 using server split
Release-2.0.1,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.0.1,update the grad stats of children node
Release-2.0.1,update the left child
Release-2.0.1,update the right child
Release-2.0.1,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.0.1,"2.3.3 otherwise, the returned histogram contains the gradient info"
Release-2.0.1,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.0.1,2.3.5 reset this tree node's gradient histogram to 0
Release-2.0.1,3. push split feature to PS
Release-2.0.1,4. push split value to PS
Release-2.0.1,5. push split gain to PS
Release-2.0.1,6. set phase to AFTER_SPLIT
Release-2.0.1,this.phase = GBDTPhase.AFTER_SPLIT;
Release-2.0.1,clock
Release-2.0.1,1. get split feature
Release-2.0.1,2. get split value
Release-2.0.1,3. get split gain
Release-2.0.1,4. get node weight
Release-2.0.1,5. split node
Release-2.0.1,update local replica
Release-2.0.1,create AfterSplit task
Release-2.0.1,"2. check thread stats, if all threads finish, return"
Release-2.0.1,6. clock
Release-2.0.1,"split the span of one node, reset the instance position"
Release-2.0.1,in case this worker has no instance on this node
Release-2.0.1,set the span of left child
Release-2.0.1,set the span of right child
Release-2.0.1,"1. left to right, find the first instance that should be in the right child"
Release-2.0.1,"2. right to left, find the first instance that should be in the left child"
Release-2.0.1,3. swap two instances
Release-2.0.1,4. find the cut pos
Release-2.0.1,5. set the span of left child
Release-2.0.1,6. set the span of right child
Release-2.0.1,set tree node to active
Release-2.0.1,set node to leaf
Release-2.0.1,set node to inactive
Release-2.0.1,finish current depth
Release-2.0.1,finish current tree
Release-2.0.1,set the tree phase
Release-2.0.1,check if there is active node
Release-2.0.1,check if finish all the tree
Release-2.0.1,update node's grad stats on PS
Release-2.0.1,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
Release-2.0.1,the root node's stats is updated by leader worker
Release-2.0.1,1. create the update
Release-2.0.1,2. push the update to PS
Release-2.0.1,1. update predictions of training data
Release-2.0.1,2. update predictions of validation data
Release-2.0.1,the leader task adds node prediction to flush list
Release-2.0.1,1. name of this node's grad histogram on PS
Release-2.0.1,2. build the grad histogram of this node
Release-2.0.1,3. push the histograms to PS
Release-2.0.1,4. reset thread stats to finished
Release-2.0.1,5.1. set the children nodes of this node
Release-2.0.1,5.2. set split info and grad stats to this node
Release-2.0.1,5.2. create children nodes
Release-2.0.1,"5.3. create node stats for children nodes, and add them to the tree"
Release-2.0.1,5.4. reset instance position
Release-2.0.1,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
Release-2.0.1,5.6. set children nodes to leaf nodes
Release-2.0.1,5.7. set nid to leaf node
Release-2.0.1,5.8. deactivate active node
Release-2.0.1,"get feature type, 0:empty 1:all equal 2:real"
Release-2.0.1,"if not -1, sufficient space will be allocated at once"
Release-2.0.1,copy the highest levels
Release-2.0.1,copy baseBuffer
Release-2.0.1,merge two non-empty quantile sketches
Release-2.0.1,left child <= split value; right child > split value
Release-2.0.1,"the first: minimal, the last: maximal"
Release-2.0.1,categorical features
Release-2.0.1,continuous features
Release-2.0.1,left child <= split value; right child > split value
Release-2.0.1,feature index used to split
Release-2.0.1,feature value used to split
Release-2.0.1,loss change after split this node
Release-2.0.1,grad stats of the left child
Release-2.0.1,grad stats of the right child
Release-2.0.1,"LOG.info(""Constructor with fid = -1"");"
Release-2.0.1,fid = -1: no split currently
Release-2.0.1,the minimal split value is the minimal value of feature
Release-2.0.1,the splits do not include the maximal value of feature
Release-2.0.1,"1. the average distance, (maxValue - minValue) / splitNum"
Release-2.0.1,2. calculate the candidate split value
Release-2.0.1,1. new feature's histogram (grad + hess)
Release-2.0.1,size: sampled_featureNum * (2 * splitNum)
Release-2.0.1,"in other words, concatenate each feature's histogram"
Release-2.0.1,2. get the span of this node
Release-2.0.1,------ 3. using sparse-aware method to build histogram ---
Release-2.0.1,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
Release-2.0.1,3.1. get the instance index
Release-2.0.1,3.2. get the grad and hess of the instance
Release-2.0.1,3.3. add to the sum
Release-2.0.1,3.4. loop the non-zero entries
Release-2.0.1,3.4.1. get feature value
Release-2.0.1,3.4.2. current feature's position in the sampled feature set
Release-2.0.1,"int fPos = findFidPlace(this.controller.fSet, fid);"
Release-2.0.1,3.4.3. find the position of feature value in a histogram
Release-2.0.1,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
Release-2.0.1,3.4.4. add the grad and hess to the corresponding bin
Release-2.0.1,3.4.5. add the reverse to the bin that contains 0.0f
Release-2.0.1,4. add the grad and hess sum to the zero bin of all features
Release-2.0.1,find the best split result of the histogram of a tree node
Release-2.0.1,1. calculate the gradStats of the root node
Release-2.0.1,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
Release-2.0.1,2. loop over features
Release-2.0.1,2.1. get the ture feature id in the sampled feature set
Release-2.0.1,2.2. get the indexes of histogram of this feature
Release-2.0.1,2.3. find the best split of current feature
Release-2.0.1,2.4. update the best split result if possible
Release-2.0.1,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.0.1,3. update the grad stats of children node
Release-2.0.1,3.1. update the left child
Release-2.0.1,3.2. update the right child
Release-2.0.1,find the best split result of one feature
Release-2.0.1,1. set the feature id
Release-2.0.1,2. create the best left stats and right stats
Release-2.0.1,3. the gain of the root node
Release-2.0.1,4. create the temp left and right grad stats
Release-2.0.1,5. loop over all the data in histogram
Release-2.0.1,5.1. get the grad and hess of current hist bin
Release-2.0.1,5.2. check whether we can split with current left hessian
Release-2.0.1,right = root - left
Release-2.0.1,5.3. check whether we can split with current right hessian
Release-2.0.1,5.4. calculate the current loss gain
Release-2.0.1,5.5. check whether we should update the split result with current loss gain
Release-2.0.1,split value = sketches[splitIdx]
Release-2.0.1,"5.6. if should update, also update the best left and right grad stats"
Release-2.0.1,6. set the best left and right grad stats
Release-2.0.1,partition number
Release-2.0.1,cols of each partition
Release-2.0.1,1. calculate the total grad sum and hess sum
Release-2.0.1,2. create the grad stats of the node
Release-2.0.1,1. calculate the total grad sum and hess sum
Release-2.0.1,2. create the grad stats of the node
Release-2.0.1,1. calculate the total grad sum and hess sum
Release-2.0.1,2. create the grad stats of the node
Release-2.0.1,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
Release-2.0.1,assert fvalue >= sketch[start] && fvalue <= sketch[end];
Release-2.0.1,if (left > end) return end - start;
Release-2.0.1,find the best split result of the histogram of a tree node
Release-2.0.1,2.2. get the indexes of histogram of this feature
Release-2.0.1,2.3. find the best split of current feature
Release-2.0.1,2.4. update the best split result if possible
Release-2.0.1,find the best split result of one feature
Release-2.0.1,1. set the feature id
Release-2.0.1,splitEntry.setFid(fid);
Release-2.0.1,2. create the best left stats and right stats
Release-2.0.1,3. the gain of the root node
Release-2.0.1,4. create the temp left and right grad stats
Release-2.0.1,5. loop over all the data in histogram
Release-2.0.1,5.1. get the grad and hess of current hist bin
Release-2.0.1,5.2. check whether we can split with current left hessian
Release-2.0.1,right = root - left
Release-2.0.1,5.3. check whether we can split with current right hessian
Release-2.0.1,5.4. calculate the current loss gain
Release-2.0.1,5.5. check whether we should update the split result with current loss gain
Release-2.0.1,"5.6. if should update, also update the best left and right grad stats"
Release-2.0.1,6. set the best left and right grad stats
Release-2.0.1,find the best split result of a serve row on the PS
Release-2.0.1,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
Release-2.0.1,2.2. get the start index in histogram of this feature
Release-2.0.1,2.3. find the best split of current feature
Release-2.0.1,2.4. update the best split result if possible
Release-2.0.1,"find the best split result of one feature from a server row, used by the PS"
Release-2.0.1,1. set the feature id
Release-2.0.1,2. create the best left stats and right stats
Release-2.0.1,3. the gain of the root node
Release-2.0.1,4. create the temp left and right grad stats
Release-2.0.1,5. loop over all the data in histogram
Release-2.0.1,5.1. get the grad and hess of current hist bin
Release-2.0.1,5.2. check whether we can split with current left hessian
Release-2.0.1,right = root - left
Release-2.0.1,5.3. check whether we can split with current right hessian
Release-2.0.1,5.4. calculate the current loss gain
Release-2.0.1,5.5. check whether we should update the split result with current loss gain
Release-2.0.1,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
Release-2.0.1,the task use index to find fvalue
Release-2.0.1,"5.6. if should update, also update the best left and right grad stats"
Release-2.0.1,6. set the best left and right grad stats
Release-2.0.1,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
Release-2.0.1,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
Release-2.0.1,max and min of each feature
Release-2.0.1,clear all the information
Release-2.0.1,calculate the sum of gradient and hess
Release-2.0.1,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
Release-2.0.1,ridx)
Release-2.0.1,check if necessary information is ready
Release-2.0.1,"same as add, reduce is used in All Reduce"
Release-2.0.1,"features used in this tree, if equals null, means use all the features without sampling"
Release-2.0.1,node in the tree
Release-2.0.1,the gradient info of each instances
Release-2.0.1,initialize nodes
Release-2.0.1,gradient
Release-2.0.1,second order gradient
Release-2.0.1,int sendStartCol = (int) row.getStartCol();
Release-2.0.1,logistic loss for binary classification task.
Release-2.0.1,"logistic loss, but predict un-transformed margin"
Release-2.0.1,check if label in range
Release-2.0.1,return the default evaluation metric for the objective
Release-2.0.1,"task type: classification, regression, or ranking"
Release-2.0.1,"quantile sketch, size = featureNum * splitNum"
Release-2.0.1,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
Release-2.0.1,"active tree nodes, size = pow(2, treeDepth) -1"
Release-2.0.1,sampled features. size = treeNum * sampleRatio * featureNum
Release-2.0.1,categorical feature. size = workerNum * cateFeatNum * splitNum
Release-2.0.1,"split features, size = treeNum * treeNodeNum"
Release-2.0.1,"split values, size = treeNum * treeNodeNum"
Release-2.0.1,"split gains, size = treeNum * treeNodeNum"
Release-2.0.1,"node weights, size = treeNum * treeNodeNum"
Release-2.0.1,"node preds, size = treeNum * treeNodeNum"
Release-2.0.1,if using PS to perform split
Release-2.0.1,step size for a tree
Release-2.0.1,number of class
Release-2.0.1,minimum loss change required for a split
Release-2.0.1,maximum depth of a tree
Release-2.0.1,number of features
Release-2.0.1,number of nonzero
Release-2.0.1,number of candidates split value
Release-2.0.1,----- the rest parameters are less important ----
Release-2.0.1,base instance weight
Release-2.0.1,minimum amount of hessian(weight) allowed in a child
Release-2.0.1,L2 regularization factor
Release-2.0.1,L1 regularization factor
Release-2.0.1,default direction choice
Release-2.0.1,maximum delta update we can add in weight estimation
Release-2.0.1,this parameter can be used to stabilize update
Release-2.0.1,default=0 means no constraint on weight delta
Release-2.0.1,whether we want to do subsample for row
Release-2.0.1,whether to subsample columns for each tree
Release-2.0.1,accuracy of sketch
Release-2.0.1,accuracy of sketch
Release-2.0.1,leaf vector size
Release-2.0.1,option for parallelization
Release-2.0.1,option to open cacheline optimization
Release-2.0.1,whether to not print info during training.
Release-2.0.1,maximum depth of the tree
Release-2.0.1,number of features used for tree construction
Release-2.0.1,"minimum loss change required for a split, otherwise stop split"
Release-2.0.1,----- the rest parameters are less important ----
Release-2.0.1,default direction choice
Release-2.0.1,whether we want to do sample data
Release-2.0.1,whether to sample columns during tree construction
Release-2.0.1,whether to use histogram for split
Release-2.0.1,number of histogram units
Release-2.0.1,whether to print info during training.
Release-2.0.1,----- the rest parameters are obtained after training ----
Release-2.0.1,total number of nodes
Release-2.0.1,number of deleted nodes */
Release-2.0.0,@maxIndex: this variable contains the max index of node/word
Release-2.0.0,some params
Release-2.0.0,max index for node/word
Release-2.0.0,compute number of nodes for one row
Release-2.0.0,check the length of dot values
Release-2.0.0,merge dot values from all partitions
Release-2.0.0,Skip-Gram model
Release-2.0.0,Negative sampling
Release-2.0.0,used to accumulate the updates for input vectors
Release-2.0.0,Negative sampling
Release-2.0.0,accumulate for the hidden layer
Release-2.0.0,update output layer
Release-2.0.0,update the hidden layer
Release-2.0.0,update input
Release-2.0.0,Skip-Gram model
Release-2.0.0,Negative sampling
Release-2.0.0,used to accumulate the updates for input vectors
Release-2.0.0,Negative sampling
Release-2.0.0,accumulate for the hidden layer
Release-2.0.0,update output layer
Release-2.0.0,update the hidden layer
Release-2.0.0,update input
Release-2.0.0,update output
Release-2.0.0,Some params
Release-2.0.0,compute number of nodes for one row
Release-2.0.0,window size
Release-2.0.0,Skip-Gram model
Release-2.0.0,Accumulate the input vectors from context
Release-2.0.0,Negative sampling
Release-2.0.0,used to accumulate the updates for input vectors
Release-2.0.0,window size
Release-2.0.0,skip-gram model
Release-2.0.0,Negative sampling
Release-2.0.0,accumulate for the hidden layer
Release-2.0.0,update output layer
Release-2.0.0,update the hidden layer
Release-2.0.0,update input
Release-2.0.0,update output
Release-2.0.0,some params
Release-2.0.0,batch sentences
Release-2.0.0,max index for node/word
Release-2.0.0,compute number of nodes for one row
Release-2.0.0,check the length of dot values
Release-2.0.0,merge dot values from all partitions
Release-2.0.0,locates the input vectors to local array to prevent randomly access
Release-2.0.0,on the large server row.
Release-2.0.0,fill 0 for context vector
Release-2.0.0,window size
Release-2.0.0,Continuous bag-of-words Models
Release-2.0.0,Accumulate the input vectors from context
Release-2.0.0,Calculate the partial dot values
Release-2.0.0,We should guarantee here that the sample would not equal the ``word``
Release-2.0.0,used to accumulate the context input vectors
Release-2.0.0,locates the input vector into local arrays to prevent randomly access for
Release-2.0.0,the large server row.
Release-2.0.0,window size
Release-2.0.0,while true to prevent sampling out a positive target
Release-2.0.0,how to prevent the randomly access to the output vectors??
Release-2.0.0,accumulate gradients for the input vectors
Release-2.0.0,update output vectors
Release-2.0.0,update input
Release-2.0.0,update output
Release-2.0.0,Some params
Release-2.0.0,compute number of nodes for one row
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy data spliter
Release-2.0.0,get a angel client
Release-2.0.0,add matrix
Release-2.0.0,TODO Auto-generated constructor stub
Release-2.0.0,row 0 is a random uniform
Release-2.0.0,row 1 is a random normal
Release-2.0.0,row 2 is filled with 1.0
Release-2.0.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
Release-2.0.0,"paras[1] = ""abc"";"
Release-2.0.0,"paras[2] = ""123"";"
Release-2.0.0,Add standard Hadoop classes
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,set sgd LR algorithm parameters #feature #epoch
Release-2.0.0,Set input data path
Release-2.0.0,Set save model path
Release-2.0.0,Set actionType train
Release-2.0.0,QSLRRunner runner = new QSLRRunner();
Release-2.0.0,runner.train(conf);
Release-2.0.0,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
Release-2.0.0,Dataset
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,class number
Release-2.0.0,Model type
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,Train batch number per epoch.
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set file system
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Use local deploy mode and data format
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set Softmax algorithm parameters
Release-2.0.0,Set model class
Release-2.0.0,Dataset
Release-2.0.0,Data format
Release-2.0.0,Feature number of train data
Release-2.0.0,Tree number
Release-2.0.0,Tree depth
Release-2.0.0,Split number
Release-2.0.0,Feature sample ratio
Release-2.0.0,Ratio of validation
Release-2.0.0,Learning rate
Release-2.0.0,Set file system
Release-2.0.0,Use local deploy mode and data format
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource, #worker, #task, #PS"
Release-2.0.0,Set GBDT algorithm parameters
Release-2.0.0,Dataset
Release-2.0.0,Set file system
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Use local deploy mode
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set DeepFM algorithm parameters
Release-2.0.0,Set model class
Release-2.0.0,Dataset
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Model type
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set file system
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Use local deploy mode and data format
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set LR algorithm parameters
Release-2.0.0,Set model class
Release-2.0.0,Dataset
Release-2.0.0,Data format
Release-2.0.0,Model type
Release-2.0.0,Cluster center number
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Sample ratio per mini-batch
Release-2.0.0,C
Release-2.0.0,Set file system
Release-2.0.0,Use local deploy mode and data format
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource, #worker, #task, #PS"
Release-2.0.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.0.0,Dataset
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Model type
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set file system
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Use local deploy mode and data format
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set FM algorithm parameters
Release-2.0.0,Set model class
Release-2.0.0,Dataset
Release-2.0.0,Set file system
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Use local deploy mode
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set WideAndDeep algorithm parameters
Release-2.0.0,Set model class
Release-2.0.0,Dataset
Release-2.0.0,Data format
Release-2.0.0,"Set LDA parameters #V, #K"
Release-2.0.0,Set file system
Release-2.0.0,Use local deploy mode and data format
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource, #worker, #task, #PS"
Release-2.0.0,Set LDA algorithm parameters
Release-2.0.0,Dataset
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Model type
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set file system
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Use local deploy mode and data format
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set SVM algorithm parameters
Release-2.0.0,Set model class
Release-2.0.0,Dataset
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Model type
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,Model is classification
Release-2.0.0,Train batch number per epoch.
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set file system
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Use local deploy mode and data format
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set LR algorithm parameters
Release-2.0.0,Set model class
Release-2.0.0,Dataset
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Model type
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,Model is classification
Release-2.0.0,Train batch number per epoch.
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set file system
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Use local deploy mode and data format
Release-2.0.0,Set data path
Release-2.0.0,"Set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set LR algorithm parameters
Release-2.0.0,Set model class
Release-2.0.0,Load model meta
Release-2.0.0,Convert model
Release-2.0.0,"Get input path, output path"
Release-2.0.0,Init serde
Release-2.0.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.0.0,Load model meta
Release-2.0.0,Convert model
Release-2.0.0,load hadoop configuration
Release-2.0.0,"Get input path, output path"
Release-2.0.0,Init serde
Release-2.0.0,"Parse need convert model names, if not set, we will convert all models in input directory"
Release-2.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.0,input.seek(rowOffset.getOffset());
Release-2.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.0,input.seek(rowOffset.getOffset());
Release-2.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.0,input.seek(rowOffset.getOffset());
Release-2.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.0,input.seek(rowOffset.getOffset());
Release-2.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.0,input.seek(rowOffset.getOffset());
Release-2.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.0,input.seek(rowOffset.getOffset());
Release-2.0.0,RowOffset rowOffset = partMeta.getRowMetas().get(rowId);
Release-2.0.0,input.seek(rowOffset.getOffset());
Release-2.0.0,Load model meta
Release-2.0.0,Check row type
Release-2.0.0,Load model
Release-2.0.0,Load model meta
Release-2.0.0,Check row type
Release-2.0.0,Load model
Release-2.0.0,Load model meta
Release-2.0.0,Check row type
Release-2.0.0,Load model
Release-2.0.0,Load model meta
Release-2.0.0,Check row type
Release-2.0.0,Load model
Release-2.0.0,Load model meta
Release-2.0.0,Check row type
Release-2.0.0,Load model
Release-2.0.0,Load model meta
Release-2.0.0,Check row type
Release-2.0.0,Load model
Release-2.0.0,Load model meta
Release-2.0.0,Check row type
Release-2.0.0,Load model
Release-2.0.0,Load model
Release-2.0.0,load hadoop configuration
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add matrix
Release-2.0.0,worker register
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add matrix
Release-2.0.0,add matrix
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add matrix
Release-2.0.0,attempt 0
Release-2.0.0,attempt1
Release-2.0.0,attempt1
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add matrix
Release-2.0.0,TODO Auto-generated constructor stub
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add matrix
Release-2.0.0,set basic configuration keys
Release-2.0.0,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add matrix
Release-2.0.0,Thread.sleep(5000);
Release-2.0.0,"response = master.getJobReport(null, request);"
Release-2.0.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
Release-2.0.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
Release-2.0.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add dense double matrix
Release-2.0.0,add comp dense double matrix
Release-2.0.0,add sparse double matrix
Release-2.0.0,add component sparse double matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense long matrix
Release-2.0.0,add comp dense long matrix
Release-2.0.0,add sparse long matrix
Release-2.0.0,add component sparse long matrix
Release-2.0.0,add comp dense long double matrix
Release-2.0.0,add sparse long-key double matrix
Release-2.0.0,add component long-key sparse double matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add sparse long-key float matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add sparse long-key int matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,add sparse long-key long matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,Start PS
Release-2.0.0,Start to run application
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add dense double matrix
Release-2.0.0,add comp dense double matrix
Release-2.0.0,add sparse double matrix
Release-2.0.0,add component sparse double matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense long matrix
Release-2.0.0,add comp dense long matrix
Release-2.0.0,add sparse long matrix
Release-2.0.0,add component sparse long matrix
Release-2.0.0,add comp dense long double matrix
Release-2.0.0,add sparse long-key double matrix
Release-2.0.0,add component long-key sparse double matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add sparse long-key float matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add sparse long-key int matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,add sparse long-key long matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,Start PS
Release-2.0.0,Start to run application
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add dense double matrix
Release-2.0.0,add comp dense double matrix
Release-2.0.0,add sparse double matrix
Release-2.0.0,add component sparse double matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense long matrix
Release-2.0.0,add comp dense long matrix
Release-2.0.0,add sparse long matrix
Release-2.0.0,add component sparse long matrix
Release-2.0.0,add comp dense long double matrix
Release-2.0.0,add sparse long-key double matrix
Release-2.0.0,add component long-key sparse double matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add sparse long-key float matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add sparse long-key int matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,add sparse long-key long matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,Start PS
Release-2.0.0,Start to run application
Release-2.0.0,testDenseDoubleCompUDF();
Release-2.0.0,testSparseDoubleCompUDF();
Release-2.0.0,testDenseFloatCompUDF();
Release-2.0.0,testSparseFloatCompUDF();
Release-2.0.0,testDenseIntCompUDF();
Release-2.0.0,testSparseIntCompUDF();
Release-2.0.0,testDenseLongCompUDF();
Release-2.0.0,testSparseLongCompUDF();
Release-2.0.0,testSparseDoubleLongKeyCompUDF();
Release-2.0.0,testSparseFloatLongKeyCompUDF();
Release-2.0.0,testSparseIntLongKeyCompUDF();
Release-2.0.0,testSparseLongLongKeyCompUDF();
Release-2.0.0,client1.clock().get();
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,client1.clock().get();
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,TODO Auto-generated constructor stub
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add dense double matrix
Release-2.0.0,add sparse double matrix
Release-2.0.0,add comp dense double matrix
Release-2.0.0,add component sparse double matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense long matrix
Release-2.0.0,add comp dense long matrix
Release-2.0.0,add sparse long matrix
Release-2.0.0,add component sparse long matrix
Release-2.0.0,add comp dense long double matrix
Release-2.0.0,add sparse long-key double matrix
Release-2.0.0,add component long-key sparse double matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add sparse long-key float matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add sparse long-key int matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,add sparse long-key long matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,Start PS
Release-2.0.0,Start to run application
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,Assert.assertTrue(index.length == row.size());
Release-2.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.0.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.0.0,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"LOG.info(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.0.0,IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,for (int i = 0; i < feaNum; i++) {
Release-2.0.0,"deltaVec.set(i, i);"
Release-2.0.0,}
Release-2.0.0,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
Release-2.0.0,IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,"System.out.println(""id="" + id + "", value="" + row.get(id));"
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add dense double matrix
Release-2.0.0,add comp dense double matrix
Release-2.0.0,add sparse double matrix
Release-2.0.0,add component sparse double matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense long matrix
Release-2.0.0,add comp dense long matrix
Release-2.0.0,add sparse long matrix
Release-2.0.0,add component sparse long matrix
Release-2.0.0,add comp dense long double matrix
Release-2.0.0,add sparse long-key double matrix
Release-2.0.0,add component long-key sparse double matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add sparse long-key float matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add sparse long-key int matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,add sparse long-key long matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,Start PS
Release-2.0.0,Start to run application
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add dense double matrix
Release-2.0.0,add comp dense double matrix
Release-2.0.0,add sparse double matrix
Release-2.0.0,add component sparse double matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense float matrix
Release-2.0.0,add comp dense float matrix
Release-2.0.0,add sparse float matrix
Release-2.0.0,add component sparse float matrix
Release-2.0.0,add dense long matrix
Release-2.0.0,add comp dense long matrix
Release-2.0.0,add sparse long matrix
Release-2.0.0,add component sparse long matrix
Release-2.0.0,add comp dense long double matrix
Release-2.0.0,add sparse long-key double matrix
Release-2.0.0,add component long-key sparse double matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add sparse long-key float matrix
Release-2.0.0,add component long-key sparse float matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add sparse long-key int matrix
Release-2.0.0,add component long-key sparse int matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,add sparse long-key long matrix
Release-2.0.0,add component long-key sparse long matrix
Release-2.0.0,Start PS
Release-2.0.0,Start to run application
Release-2.0.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
Release-2.0.0,@RunWith(MockitoJUnitRunner.class)
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
Release-2.0.0,get a angel client
Release-2.0.0,add matrix
Release-2.0.0,psAgent.initAndStart();
Release-2.0.0,test conf
Release-2.0.0,test master location
Release-2.0.0,test app id
Release-2.0.0,test user
Release-2.0.0,test ps agent attempt id
Release-2.0.0,test connection
Release-2.0.0,test master client
Release-2.0.0,test ip
Release-2.0.0,test loc
Release-2.0.0,test master location
Release-2.0.0,test ps location
Release-2.0.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
Release-2.0.0,test all ps ids
Release-2.0.0,test all matrix ids
Release-2.0.0,test all matrix names
Release-2.0.0,test matrix attribute
Release-2.0.0,test matrix meta
Release-2.0.0,test ps location
Release-2.0.0,test partitions
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,System.out.println(content);
Release-2.0.0,https://blog.csdn.net/cocoonyang/article/details/63068108
Release-2.0.0,v1[i] = v1[i] + da * v2[i];
Release-2.0.0,"dgemm(String transa, String transb,"
Release-2.0.0,"int m, int n, int k,"
Release-2.0.0,"double alpha,"
Release-2.0.0,"double[] a, int lda,"
Release-2.0.0,"double[] b, int ldb,"
Release-2.0.0,"double beta,"
Release-2.0.0,"double[] c, int ldc);"
Release-2.0.0,C := alpha*op( A )*op( B ) + beta*C
Release-2.0.0,v1[i] = v1[i] + da * v2[i];
Release-2.0.0,y := alpha*A*x + beta*y
Release-2.0.0,y := alpha*A*x + beta*y
Release-2.0.0,y := alpha*A*x + beta*y
Release-2.0.0,"dgemm(String transa, String transb,"
Release-2.0.0,"int m, int n, int k,"
Release-2.0.0,"double alpha,"
Release-2.0.0,"double[] a, int lda,"
Release-2.0.0,"double[] b, int ldb,"
Release-2.0.0,"double beta,"
Release-2.0.0,"double[] c, int ldc);"
Release-2.0.0,C := alpha*op( A )*op( B ) + beta*C
Release-2.0.0,set basic configuration keys
Release-2.0.0,use local deploy mode and dummy dataspliter
Release-2.0.0,get a angel client
Release-2.0.0,add matrix
Release-2.0.0,test worker getActiveTaskNum
Release-2.0.0,test worker getTaskNum
Release-2.0.0,test worker getTaskManager
Release-2.0.0,test workerId
Release-2.0.0,test workerAttemptId
Release-2.0.0,tet worker initFinished
Release-2.0.0,test worker getInitMinclock
Release-2.0.0,test worker loacation
Release-2.0.0,test AppId
Release-2.0.0,test Conf
Release-2.0.0,test UserName
Release-2.0.0,master location
Release-2.0.0,masterClient
Release-2.0.0,test psAgent
Release-2.0.0,test worker get dataBlockManager
Release-2.0.0,workerGroup.getSplits();
Release-2.0.0,application
Release-2.0.0,lcation
Release-2.0.0,workerGroup info
Release-2.0.0,worker info
Release-2.0.0,task
Release-2.0.0,using mock object
Release-2.0.0,verification
Release-2.0.0,Stubbing
Release-2.0.0,Default does nothing.
Release-2.0.0,The app injection is optional
Release-2.0.0,"renderText(""hello world"");"
Release-2.0.0,"user choose a workerGroupID from the workergroups page,"
Release-2.0.0,now we should change the AngelApp params and render the workergroup page;
Release-2.0.0,"static final String WORKER_ID = ""worker.id"";"
Release-2.0.0,"div(""#logo"")."
Release-2.0.0,"img(""/static/hadoop-st.png"")._()."
Release-2.0.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
Release-2.0.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
Release-2.0.0,JQueryUI.jsnotice(html);
Release-2.0.0,import org.apache.hadoop.conf.Configuration;
Release-2.0.0,import java.lang.reflect.Field;
Release-2.0.0,get block locations from file system
Release-2.0.0,create a list of all block and their locations
Release-2.0.0,"if the file is not splitable, just create the one block with"
Release-2.0.0,full file length
Release-2.0.0,each split can be a maximum of maxSize
Release-2.0.0,if remainder is between max and 2*max - then
Release-2.0.0,"instead of creating splits of size max, left-max we"
Release-2.0.0,create splits of size left/2 and left/2. This is
Release-2.0.0,a heuristic to avoid creating really really small
Release-2.0.0,splits.
Release-2.0.0,add this block to the block --> node locations map
Release-2.0.0,"For blocks that do not have host/rack information,"
Release-2.0.0,assign to default  rack.
Release-2.0.0,add this block to the rack --> block map
Release-2.0.0,Add this host to rackToNodes map
Release-2.0.0,add this block to the node --> block map
Release-2.0.0,"if the file system does not have any rack information, then"
Release-2.0.0,use dummy rack location.
Release-2.0.0,The topology paths have the host name included as the last
Release-2.0.0,component. Strip it.
Release-2.0.0,get tokens for all the required FileSystems..
Release-2.0.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
Release-2.0.0,job.getConfiguration());
Release-2.0.0,Whether we need to recursive look into the directory structure
Release-2.0.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.0.0,user provided one (if any).
Release-2.0.0,all the files in input set
Release-2.0.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
Release-2.0.0,process all nodes and create splits that are local to a node. Generate
Release-2.0.0,"one split per node iteration, and walk over nodes multiple times to"
Release-2.0.0,distribute the splits across nodes.
Release-2.0.0,Skip the node if it has previously been marked as completed.
Release-2.0.0,"for each block, copy it into validBlocks. Delete it from"
Release-2.0.0,blockToNodes so that the same block does not appear in
Release-2.0.0,two different splits.
Release-2.0.0,Remove all blocks which may already have been assigned to other
Release-2.0.0,splits.
Release-2.0.0,"if the accumulated split size exceeds the maximum, then"
Release-2.0.0,create this split.
Release-2.0.0,create an input split and add it to the splits array
Release-2.0.0,Remove entries from blocksInNode so that we don't walk these
Release-2.0.0,again.
Release-2.0.0,Done creating a single split for this node. Move on to the next
Release-2.0.0,node so that splits are distributed across nodes.
Release-2.0.0,This implies that the last few blocks (or all in case maxSize=0)
Release-2.0.0,were not part of a split. The node is complete.
Release-2.0.0,if there were any blocks left over and their combined size is
Release-2.0.0,"larger than minSplitNode, then combine them into one split."
Release-2.0.0,Otherwise add them back to the unprocessed pool. It is likely
Release-2.0.0,that they will be combined with other blocks from the
Release-2.0.0,same rack later on.
Release-2.0.0,This condition also kicks in when max split size is not set. All
Release-2.0.0,blocks on a node will be grouped together into a single split.
Release-2.0.0,haven't created any split on this machine. so its ok to add a
Release-2.0.0,smaller one for parallelism. Otherwise group it in the rack for
Release-2.0.0,balanced size create an input split and add it to the splits
Release-2.0.0,array
Release-2.0.0,Remove entries from blocksInNode so that we don't walk this again.
Release-2.0.0,The node is done. This was the last set of blocks for this node.
Release-2.0.0,Put the unplaced blocks back into the pool for later rack-allocation.
Release-2.0.0,Node is done. All blocks were fit into node-local splits.
Release-2.0.0,Check if node-local assignments are complete.
Release-2.0.0,All nodes have been walked over and marked as completed or all blocks
Release-2.0.0,have been assigned. The rest should be handled via rackLock assignment.
Release-2.0.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
Release-2.0.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
Release-2.0.0,"if blocks in a rack are below the specified minimum size, then keep them"
Release-2.0.0,"in 'overflow'. After the processing of all racks is complete, these"
Release-2.0.0,overflow blocks will be combined into splits.
Release-2.0.0,Process all racks over and over again until there is no more work to do.
Release-2.0.0,Create one split for this rack before moving over to the next rack.
Release-2.0.0,Come back to this rack after creating a single split for each of the
Release-2.0.0,remaining racks.
Release-2.0.0,"Process one rack location at a time, Combine all possible blocks that"
Release-2.0.0,reside on this rack as one split. (constrained by minimum and maximum
Release-2.0.0,split size).
Release-2.0.0,iterate over all racks
Release-2.0.0,"for each block, copy it into validBlocks. Delete it from"
Release-2.0.0,blockToNodes so that the same block does not appear in
Release-2.0.0,two different splits.
Release-2.0.0,"if the accumulated split size exceeds the maximum, then"
Release-2.0.0,create this split.
Release-2.0.0,create an input split and add it to the splits array
Release-2.0.0,"if we created a split, then just go to the next rack"
Release-2.0.0,"if there is a minimum size specified, then create a single split"
Release-2.0.0,"otherwise, store these blocks into overflow data structure"
Release-2.0.0,There were a few blocks in this rack that
Release-2.0.0,remained to be processed. Keep them in 'overflow' block list.
Release-2.0.0,These will be combined later.
Release-2.0.0,Process all overflow blocks
Release-2.0.0,"This might cause an exiting rack location to be re-added,"
Release-2.0.0,but it should be ok.
Release-2.0.0,"if the accumulated split size exceeds the maximum, then"
Release-2.0.0,create this split.
Release-2.0.0,create an input split and add it to the splits array
Release-2.0.0,"Process any remaining blocks, if any."
Release-2.0.0,create an input split
Release-2.0.0,add this split to the list that is returned
Release-2.0.0,long num = totLength / maxSize;
Release-2.0.0,all blocks for all the files in input set
Release-2.0.0,mapping from a rack name to the list of blocks it has
Release-2.0.0,mapping from a block to the nodes on which it has replicas
Release-2.0.0,mapping from a node to the list of blocks that it contains
Release-2.0.0,populate all the blocks for all files
Release-2.0.0,stop all services
Release-2.0.0,1.write application state to file so that the client can get the state of the application
Release-2.0.0,if master exit
Release-2.0.0,2.clear tmp and staging directory
Release-2.0.0,waiting for client to get application state
Release-2.0.0,stop the RPC server
Release-2.0.0,"Security framework already loaded the tokens into current UGI, just use"
Release-2.0.0,them
Release-2.0.0,Now remove the AM->RM token so tasks don't have it
Release-2.0.0,add a shutdown hook
Release-2.0.0,init app state storage
Release-2.0.0,init event dispacher
Release-2.0.0,init location manager
Release-2.0.0,init container allocator
Release-2.0.0,init a rpc service
Release-2.0.0,recover matrix meta if needed
Release-2.0.0,recover ps attempt information if need
Release-2.0.0,Init Client manager
Release-2.0.0,Init PS Client manager
Release-2.0.0,init parameter server manager
Release-2.0.0,recover task information if needed
Release-2.0.0,a dummy data spliter is just for test now
Release-2.0.0,recover data splits information if needed
Release-2.0.0,init worker manager and register worker manager event
Release-2.0.0,register slow worker/ps checker
Release-2.0.0,register app manager event and finish event
Release-2.0.0,Init model saver & loader
Release-2.0.0,start a web service if use yarn deploy mode
Release-2.0.0,load from app state storage first if attempt index great than 1(the master is not the first
Release-2.0.0,retry)
Release-2.0.0,"if load failed, just build a new MatrixMetaManager"
Release-2.0.0,load ps attempt index from app state storage first if attempt index great than 1(the master
Release-2.0.0,is not the first retry)
Release-2.0.0,load task information from app state storage first if attempt index great than 1(the master
Release-2.0.0,is not the first retry)
Release-2.0.0,"if load failed, just build a new AMTaskManager"
Release-2.0.0,load data splits information from app state storage first if attempt index great than 1(the
Release-2.0.0,master is not the first retry)
Release-2.0.0,"if load failed, we need to recalculate the data splits"
Release-2.0.0,Check Workers
Release-2.0.0,Check PSS
Release-2.0.0,Check Clients
Release-2.0.0,Check PS Clients
Release-2.0.0,parse parameter server counters
Release-2.0.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.0.0,refresh last heartbeat timestamp
Release-2.0.0,send a state update event to the specific PSAttempt
Release-2.0.0,Check is there save request
Release-2.0.0,Check is there load request
Release-2.0.0,check matrix metadata inconsistencies between master and parameter server.
Release-2.0.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
Release-2.0.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
Release-2.0.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
Release-2.0.0,choose a unused port
Release-2.0.0,start RPC server
Release-2.0.0,remove this parameter server attempt from monitor set
Release-2.0.0,remove this parameter server attempt from monitor set
Release-2.0.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.0.0,find workergroup in worker manager
Release-2.0.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
Release-2.0.0,"if this worker group run over, just return WORKERGROUP_EXITED"
Release-2.0.0,"if this worker group is running now, return tasks, workers, data splits for it"
Release-2.0.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.0.0,"if worker attempt id is not in monitor set, we should shutdown it"
Release-2.0.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.0.0,"in ANGEL_PS mode, task id may can not know advance"
Release-2.0.0,update the clock for this matrix
Release-2.0.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
Release-2.0.0,"in ANGEL_PS mode, task id may can not know advance"
Release-2.0.0,update task iteration
Release-2.0.0,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
Release-2.0.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
Release-2.0.0,the number of splits equal to the number of tasks
Release-2.0.0,split data
Release-2.0.0,dispatch the splits to workergroups
Release-2.0.0,split data
Release-2.0.0,dispatch the splits to workergroups
Release-2.0.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.0.0,"first, then divided by expected split number"
Release-2.0.0,get input format class from configuration and then instantiation a input format object
Release-2.0.0,split data
Release-2.0.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
Release-2.0.0,"first, then divided by expected split number"
Release-2.0.0,get input format class from configuration and then instantiation a input format object
Release-2.0.0,split data
Release-2.0.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.0.0,need to fine tune the number of workergroup and task based on the actual split number
Release-2.0.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.0.0,Record the location information for the splits in order to data localized schedule
Release-2.0.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
Release-2.0.0,need to fine tune the number of workergroup and task based on the actual split number
Release-2.0.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
Release-2.0.0,Record the location information for the splits in order to data localized schedule
Release-2.0.0,write meta data to a temporary file
Release-2.0.0,rename the temporary file to final file
Release-2.0.0,"if the file exists, read from file and deserialize it"
Release-2.0.0,write task meta
Release-2.0.0,write ps meta
Release-2.0.0,generate a temporary file
Release-2.0.0,write task meta to the temporary file first
Release-2.0.0,rename the temporary file to the final file
Release-2.0.0,"if last final task file exist, remove it"
Release-2.0.0,find task meta file which has max timestamp
Release-2.0.0,"if the file does not exist, just return null"
Release-2.0.0,read task meta from file and deserialize it
Release-2.0.0,generate a temporary file
Release-2.0.0,write ps meta to the temporary file first.
Release-2.0.0,rename the temporary file to the final file
Release-2.0.0,"if the old final file exist, just remove it"
Release-2.0.0,find ps meta file
Release-2.0.0,"if ps meta file does not exist, just return null"
Release-2.0.0,read ps meta from file and deserialize it
Release-2.0.0,"Path tmpPath = new Path(new Path(context.getConf().get(AngelConf.ANGEL_JOB_TMP_OUTPUT_PATH)),"
Release-2.0.0,String.valueOf(requestId));
Release-2.0.0,Path tmpPath = HdfsUtil.toTmpPath(new Path(saveContext.getSavePath()));
Release-2.0.0,saveContext.setTmpSavePath(tmpPath.toString());
Release-2.0.0,Filter old epoch trigger first
Release-2.0.0,Split the user request to sub-requests to pss
Release-2.0.0,Init matrix files meta
Release-2.0.0,Move output files
Release-2.0.0,Write the meta file
Release-2.0.0,Split the user request to sub-requests to pss
Release-2.0.0,check whether psagent heartbeat timeout
Release-2.0.0,Set up the launch command
Release-2.0.0,Duplicate the ByteBuffers for access by multiple containers.
Release-2.0.0,Construct the actual Container
Release-2.0.0,Application resources
Release-2.0.0,Application environment
Release-2.0.0,Service data
Release-2.0.0,Tokens
Release-2.0.0,Set up JobConf to be localized properly on the remote NM.
Release-2.0.0,Setup DistributedCache
Release-2.0.0,Setup up task credentials buffer
Release-2.0.0,LocalStorageToken is needed irrespective of whether security is enabled
Release-2.0.0,or not.
Release-2.0.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
Release-2.0.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
Release-2.0.0,Construct the actual Container
Release-2.0.0,The null fields are per-container and will be constructed for each
Release-2.0.0,container separately.
Release-2.0.0,Set up the launch command
Release-2.0.0,Duplicate the ByteBuffers for access by multiple containers.
Release-2.0.0,Construct the actual Container
Release-2.0.0,"a * in the classpath will only find a .jar, so we need to filter out"
Release-2.0.0,all .jars and add everything else
Release-2.0.0,Propagate the system classpath when using the mini cluster
Release-2.0.0,Add standard Hadoop classes
Release-2.0.0,Add mr
Release-2.0.0,Cache archives
Release-2.0.0,Cache files
Release-2.0.0,Sanity check
Release-2.0.0,Add URI fragment or just the filename
Release-2.0.0,Add the env variables passed by the user
Release-2.0.0,Set logging level in the environment.
Release-2.0.0,Setup the log4j prop
Release-2.0.0,Add main class and its arguments
Release-2.0.0,Finally add the jvmID
Release-2.0.0,vargs.add(String.valueOf(jvmID.getId()));
Release-2.0.0,Final commmand
Release-2.0.0,Add the env variables passed by the user
Release-2.0.0,Set logging level in the environment.
Release-2.0.0,Setup the log4j prop
Release-2.0.0,Add main class and its arguments
Release-2.0.0,Final commmand
Release-2.0.0,"if amTask is not null, we should clone task state from it"
Release-2.0.0,"if all parameter server complete commit, master can commit now"
Release-2.0.0,check whether parameter server heartbeat timeout
Release-2.0.0,Transitions from the NEW state.
Release-2.0.0,Transitions from the UNASSIGNED state.
Release-2.0.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
Release-2.0.0,Transitions from the ASSIGNED state.
Release-2.0.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
Release-2.0.0,PA_CONTAINER_LAUNCHED event
Release-2.0.0,Transitions from the PSAttemptStateInternal.RUNNING state.
Release-2.0.0,Transitions from the PSAttemptStateInternal.COMMITTING state
Release-2.0.0,Transitions from the PSAttemptStateInternal.KILLED state
Release-2.0.0,Transitions from the PSAttemptStateInternal.FAILED state
Release-2.0.0,create the topology tables
Release-2.0.0,reqeuest resource:send a resource request to the resource allocator
Release-2.0.0,"Once the resource is applied, build and send the launch request to the container launcher"
Release-2.0.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
Release-2.0.0,resource allocator
Release-2.0.0,set the launch time
Release-2.0.0,add the ps attempt to the heartbeat timeout monitoring list
Release-2.0.0,parse ps attempt location and put it to location manager
Release-2.0.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
Release-2.0.0,or failed
Release-2.0.0,remove ps attempt id from heartbeat timeout monitor list
Release-2.0.0,release container:send a release request to container launcher
Release-2.0.0,set the finish time only if launch time is set
Release-2.0.0,private long scheduledTime;
Release-2.0.0,Transitions from the NEW state.
Release-2.0.0,Transitions from the SCHEDULED state.
Release-2.0.0,Transitions from the RUNNING state.
Release-2.0.0,"another attempt launched,"
Release-2.0.0,Transitions from the SUCCEEDED state
Release-2.0.0,Transitions from the KILLED state
Release-2.0.0,Transitions from the FAILED state
Release-2.0.0,add diagnostic
Release-2.0.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.0,Refresh ps location & matrix meta
Release-2.0.0,start a new attempt for this ps
Release-2.0.0,notify ps manager
Release-2.0.0,"getContext().getLocationManager().setPsLocation(id, null);"
Release-2.0.0,add diagnostic
Release-2.0.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.0,start a new attempt for this ps
Release-2.0.0,notify ps manager
Release-2.0.0,notify the event handler of state change
Release-2.0.0,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
Release-2.0.0,"if forcedState is set, just return"
Release-2.0.0,else get state from state machine
Release-2.0.0,add this worker group to the success set
Release-2.0.0,check if all worker group run over
Release-2.0.0,add this worker group to the failed set
Release-2.0.0,check if too many worker groups are failed or killed
Release-2.0.0,notify a run failed event
Release-2.0.0,add this worker group to the failed set
Release-2.0.0,check if too many worker groups are failed or killed
Release-2.0.0,notify a run failed event
Release-2.0.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
Release-2.0.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
Release-2.0.0,just return the total task number now
Release-2.0.0,TODO
Release-2.0.0,check whether worker heartbeat timeout
Release-2.0.0,"if workerAttempt is not null, we should clone task state from it"
Release-2.0.0,from NEW state
Release-2.0.0,from SCHEDULED state
Release-2.0.0,get data splits location for data locality
Release-2.0.0,reqeuest resource:send a resource request to the resource allocator
Release-2.0.0,"once the resource is applied, build and send the launch request to the container launcher"
Release-2.0.0,notify failed message to the worker
Release-2.0.0,notify killed message to the worker
Release-2.0.0,release the allocated container
Release-2.0.0,notify failed message to the worker
Release-2.0.0,remove the worker attempt from heartbeat timeout listen list
Release-2.0.0,release the allocated container
Release-2.0.0,notify killed message to the worker
Release-2.0.0,remove the worker attempt from heartbeat timeout listen list
Release-2.0.0,clean the container
Release-2.0.0,notify failed message to the worker
Release-2.0.0,remove the worker attempt from heartbeat timeout listen list
Release-2.0.0,record the finish time
Release-2.0.0,clean the container
Release-2.0.0,notify killed message to the worker
Release-2.0.0,remove the worker attempt from heartbeat timeout listening list
Release-2.0.0,record the finish time
Release-2.0.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
Release-2.0.0,set worker attempt location
Release-2.0.0,notify the register message to the worker
Release-2.0.0,record the launch time
Release-2.0.0,update worker attempt metrics
Release-2.0.0,update tasks metrics
Release-2.0.0,clean the container
Release-2.0.0,notify the worker attempt run successfully message to the worker
Release-2.0.0,record the finish time
Release-2.0.0,init a worker attempt for the worker
Release-2.0.0,schedule the worker attempt
Release-2.0.0,add diagnostic
Release-2.0.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.0,init and start a new attempt for this ps
Release-2.0.0,notify worker manager
Release-2.0.0,add diagnostic
Release-2.0.0,check whether the number of failed attempts is less than the maximum number of allowed
Release-2.0.0,init and start a new attempt for this ps
Release-2.0.0,notify worker manager
Release-2.0.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
Release-2.0.0,register to Yarn RM
Release-2.0.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
Release-2.0.0,"catch YarnRuntimeException, we should exit and need not retry"
Release-2.0.0,build heartbeat request
Release-2.0.0,send heartbeat request to rm
Release-2.0.0,"This can happen if the RM has been restarted. If it is in that state,"
Release-2.0.0,this application must clean itself up.
Release-2.0.0,Setting NMTokens
Release-2.0.0,assgin containers
Release-2.0.0,"if some container is not assigned, release them"
Release-2.0.0,handle finish containers
Release-2.0.0,dispatch container exit message to corresponding components
Release-2.0.0,killed by framework
Release-2.0.0,killed by framework
Release-2.0.0,get application finish state
Release-2.0.0,build application diagnostics
Release-2.0.0,TODO:add a job history for angel
Release-2.0.0,build unregister request
Release-2.0.0,send unregister request to rm
Release-2.0.0,Note this down for next interaction with ResourceManager
Release-2.0.0,based on blacklisting comments above we can end up decrementing more
Release-2.0.0,than requested. so guard for that.
Release-2.0.0,send the updated resource request to RM
Release-2.0.0,send 0 container count requests also to cancel previous requests
Release-2.0.0,Update resource requests
Release-2.0.0,try to assign to all nodes first to match node local
Release-2.0.0,try to match all rack local
Release-2.0.0,assign remaining
Release-2.0.0,Update resource requests
Release-2.0.0,send the container-assigned event to task attempt
Release-2.0.0,build the start container request use launch context
Release-2.0.0,send the start request to Yarn nm
Release-2.0.0,send the message that the container starts successfully to the corresponding component
Release-2.0.0,"after launching, send launched event to task attempt to move"
Release-2.0.0,it from ASSIGNED to RUNNING state
Release-2.0.0,send the message that the container starts failed to the corresponding component
Release-2.0.0,kill the remote container if already launched
Release-2.0.0,start a thread pool to startup the container
Release-2.0.0,See if we need up the pool size only if haven't reached the
Release-2.0.0,maximum limit yet.
Release-2.0.0,nodes where containers will run at *this* point of time. This is
Release-2.0.0,*not* the cluster size and doesn't need to be.
Release-2.0.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
Release-2.0.0,later is just a buffer so we are not always increasing the
Release-2.0.0,pool-size
Release-2.0.0,the events from the queue are handled in parallel
Release-2.0.0,using a thread pool
Release-2.0.0,return if already stopped
Release-2.0.0,shutdown any containers that might be left running
Release-2.0.0,Add one sync matrix
Release-2.0.0,addSyncMatrix();
Release-2.0.0,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
Release-2.0.0,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
Release-2.0.0,"LOG.info(""ps id = "" + psEntry.getKey());"
Release-2.0.0,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
Release-2.0.0,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
Release-2.0.0,"LOG.info(""matrix id = "" + metaEntry.getKey());"
Release-2.0.0,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
Release-2.0.0,}
Release-2.0.0,}
Release-2.0.0,get matrix ids in the parameter server report
Release-2.0.0,get the matrices parameter server need to create and delete
Release-2.0.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
Release-2.0.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
Release-2.0.0,Init control connection manager
Release-2.0.0,Get ps locations from master and put them to the location cache.
Release-2.0.0,Build and initialize rpc client to master
Release-2.0.0,Get psagent id
Release-2.0.0,Build PS control rpc client manager
Release-2.0.0,Build local location
Release-2.0.0,Initialize matrix meta information
Release-2.0.0,Start all services
Release-2.0.0,Stop all modules
Release-2.0.0,Stop all modules
Release-2.0.0,clock first
Release-2.0.0,wait
Release-2.0.0,Update generic resource counters
Release-2.0.0,Updating resources specified in ResourceCalculatorProcessTree
Release-2.0.0,Remove the CPU time consumed previously by JVM reuse
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,/ Plus a vector/matrix to the matrix stored in pss
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,/ Update a vector/matrix to the matrix stored in pss
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,/ Get values from pss use row/column indices
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"/ PSF get/update, use can implement their own psf"
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,/ Get a row or a batch of rows
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,TODO:
Release-2.0.0,Generate a flush request and put it to request queue
Release-2.0.0,Generate a clock request and put it to request queue
Release-2.0.0,Generate a merge request and put it to request queue
Release-2.0.0,Generate a merge request and put it to request queue
Release-2.0.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
Release-2.0.0,matrix
Release-2.0.0,and add it to cache maps
Release-2.0.0,Add the message to the tree map
Release-2.0.0,"If there are flush / clock requests blocked, we need to put this merge request into"
Release-2.0.0,the waiting queue
Release-2.0.0,Launch a merge worker to merge the update to matrix op log cache
Release-2.0.0,Remove the message from the tree map
Release-2.0.0,Wake up blocked flush/clock request
Release-2.0.0,Add flush/clock request to listener list to waiting for all the existing
Release-2.0.0,updates are merged
Release-2.0.0,Wake up blocked flush/clock request
Release-2.0.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
Release-2.0.0,blocked.
Release-2.0.0,Get next merge message sequence id
Release-2.0.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
Release-2.0.0,position
Release-2.0.0,Wake up blocked merge requests
Release-2.0.0,Get minimal sequence id from listeners
Release-2.0.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
Release-2.0.0,should flush updates to local matrix storage
Release-2.0.0,unused now
Release-2.0.0,TODO:
Release-2.0.0,Doing average or not
Release-2.0.0,Filter un-important update
Release-2.0.0,Split this row according the matrix partitions
Release-2.0.0,Set split context
Release-2.0.0,Remove the row from matrix
Release-2.0.0,buf.writeDouble(0.0);
Release-2.0.0,TODO
Release-2.0.0,TODO: write map default value
Release-2.0.0,buf.writeDouble(0);
Release-2.0.0,TODO:
Release-2.0.0,TODO:
Release-2.0.0,TODO:
Release-2.0.0,"For each partition, we generate a update split."
Release-2.0.0,"Although the split is empty for partitions those without any update data,"
Release-2.0.0,we still need to generate a update split to update the clock info on ps.
Release-2.0.0,"For each partition, we generate a update split."
Release-2.0.0,"Although the split is empty for partitions those without any update data,"
Release-2.0.0,we still need to generate a update split to update the clock info on ps.
Release-2.0.0,"For each partition, we generate a update split."
Release-2.0.0,"Although the split is empty for partitions those without any update data,"
Release-2.0.0,we still need to generate a update split to update the clock info on ps.
Release-2.0.0,"For each partition, we generate a update split."
Release-2.0.0,"Although the split is empty for partitions those without any update data,"
Release-2.0.0,we still need to generate a update split to update the clock info on ps.
Release-2.0.0,"For each partition, we generate a update split."
Release-2.0.0,"Although the split is empty for partitions those without any update data,"
Release-2.0.0,we still need to generate a update split to update the clock info on ps.
Release-2.0.0,"For each partition, we generate a update split."
Release-2.0.0,"Although the split is empty for partitions those without any update data,"
Release-2.0.0,we still need to generate a update split to update the clock info on ps.
Release-2.0.0,"LOG.error(""channel "" + ctx.channel() + "" inactive"");"
Release-2.0.0,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
Release-2.0.0,"LOG.error(""put response message queue failed "", e);"
Release-2.0.0,Use Epoll for linux
Release-2.0.0,Update location table
Release-2.0.0,Remove the server from failed list
Release-2.0.0,Notify refresh success message to request dispatcher
Release-2.0.0,Check PS exist or not
Release-2.0.0,Check heartbeat timeout
Release-2.0.0,Check PS restart or not
Release-2.0.0,private final HashSet<ParameterServerId> refreshingServerSet;
Release-2.0.0,Add it to failed rpc list
Release-2.0.0,Add the server to gray server list
Release-2.0.0,Add it to failed rpc list
Release-2.0.0,Add the server to gray server list
Release-2.0.0,Move from gray server list to failed server list
Release-2.0.0,Handle the RPCS to this server
Release-2.0.0,Submit the schedulable failed get RPCS
Release-2.0.0,Submit new get RPCS
Release-2.0.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.0.0,"If the queue is empty, just return 0"
Release-2.0.0,"If request is not over limit, just submit it"
Release-2.0.0,Submit the schedulable failed get RPCS
Release-2.0.0,Submit new put RPCS
Release-2.0.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
Release-2.0.0,"LOG.info(""choose put server "" + psIds[index]);"
Release-2.0.0,Check all pending RPCS
Release-2.0.0,Check get channel context
Release-2.0.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.0.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.0.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
Release-2.0.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
Release-2.0.0,channelManager.printPools();
Release-2.0.0,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
Release-2.0.0,if(ts - entry.getValue() > requestTimeOut * 2)  {
Release-2.0.0,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
Release-2.0.0,"+ "" milliseconds, close all channels to it"");"
Release-2.0.0,closeChannels(entry.getKey());
Release-2.0.0,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
Release-2.0.0,}
Release-2.0.0,}
Release-2.0.0,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
Release-2.0.0,Remove all pending RPCS
Release-2.0.0,Close all channel to this PS
Release-2.0.0,private Channel getChannel(Location loc) throws Exception {
Release-2.0.0,"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));"
Release-2.0.0,}
Release-2.0.0,private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {
Release-2.0.0,"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext"
Release-2.0.0,.get()
Release-2.0.0,.getConf()
Release-2.0.0,".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,"
Release-2.0.0,AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));
Release-2.0.0,}
Release-2.0.0,"LOG.error(""send request "" + request + "" is interrupted"");"
Release-2.0.0,"LOG.error(""send request "" + request + "" failed, "", e);"
Release-2.0.0,Get server id and location for this request
Release-2.0.0,"If location is null, means that the server is not ready"
Release-2.0.0,Get the channel for the location
Release-2.0.0,Check if need get token first
Release-2.0.0,Serialize the request
Release-2.0.0,Send the request
Release-2.0.0,get a channel to server from pool
Release-2.0.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.0.0,request.getContext().setChannelPool(pool);
Release-2.0.0,Allocate the bytebuf and serialize the request
Release-2.0.0,find the partition request context from cache
Release-2.0.0,"updateMatrixCache(request.getPartKey(), response.getPartition());"
Release-2.0.0,"updateMatrixCache(request.getPartKey(), response.getRowsSplit());"
Release-2.0.0,TODO
Release-2.0.0,ServerRow rowSplit = PSAgentContext.get().getMatricesCache()
Release-2.0.0,".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),"
Release-2.0.0,request.getRowIndex());
Release-2.0.0,response.setRowSplit(rowSplit);
Release-2.0.0,"updateMatrixCache(request.getPartKey(), response.getRowSplit());"
Release-2.0.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.0.0,"LOG.info(""user request id "" + request.getUserRequestId());"
Release-2.0.0,TODO
Release-2.0.0,"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {"
Release-2.0.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);"
Release-2.0.0,}
Release-2.0.0,"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {"
Release-2.0.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);"
Release-2.0.0,}
Release-2.0.0,"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {"
Release-2.0.0,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);"
Release-2.0.0,}
Release-2.0.0,Get partitions for this row
Release-2.0.0,Distinct get row requests
Release-2.0.0,Need get from ps or storage/cache
Release-2.0.0,"Switch to new request id, send a new request"
Release-2.0.0,First get this row from matrix storage
Release-2.0.0,MatrixStorage matrixStorage =
Release-2.0.0,PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);
Release-2.0.0,TVector row = matrixStorage.getRow(rowIndex);
Release-2.0.0,if (row != null && row.getClock() >= clock) {
Release-2.0.0,result.set(row);
Release-2.0.0,return row;
Release-2.0.0,}
Release-2.0.0,Get row splits of this row from the matrix cache first
Release-2.0.0,responseCache.addSubResponse(rowSplit);
Release-2.0.0,"If the row split does not exist in cache, get it from parameter server"
Release-2.0.0,Wait the final result
Release-2.0.0,Put it to the matrix cache
Release-2.0.0,"matrixStorage.addRow(rowIndex, row);"
Release-2.0.0,Just wait result
Release-2.0.0,Split the param use matrix partitions
Release-2.0.0,Send request to PSS
Release-2.0.0,Split the matrix oplog according to the matrix partitions
Release-2.0.0,"If need update clock, we should send requests to all partitions"
Release-2.0.0,Send request to PSS
Release-2.0.0,Filter the rowIds which are fetching now
Release-2.0.0,Send the rowIndex to rpc dispatcher and return immediately
Release-2.0.0,"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));"
Release-2.0.0,"LOG.info(""start to request "" + requestId);"
Release-2.0.0,"LOG.info(""start to request "" + requestId);"
Release-2.0.0,Split param use matrix partitons
Release-2.0.0,"If all sub-results are received, just remove request and result cache"
Release-2.0.0,"LOG.info(""request = "" + request + "", cache = "" + cache);"
Release-2.0.0,"LOG.info(""start to merge "" + cache + "" for request "" + request);"
Release-2.0.0,"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.0.0,Split this row according the matrix partitions
Release-2.0.0,Set split context
Release-2.0.0,Split this row according the matrix partitions
Release-2.0.0,Set split context
Release-2.0.0,long startTs = System.currentTimeMillis();
Release-2.0.0,"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));"
Release-2.0.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
Release-2.0.0,Generate dispatch items and add them to the corresponding queues
Release-2.0.0,Filter the rowIds which are fetching now
Release-2.0.0,Sort the parts by partitionId
Release-2.0.0,Sort partition keys use start column index
Release-2.0.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.0.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.0.0,});
Release-2.0.0,"For each partition, we generate a update split."
Release-2.0.0,"Although the split is empty for partitions those without any update data,"
Release-2.0.0,we still need to generate a update split to update the clock info on ps.
Release-2.0.0,Sort the parts by partitionId
Release-2.0.0,Sort partition keys use start column index
Release-2.0.0,"Collections.sort(partKeys, (PartitionKey key1, PartitionKey key2) -> {"
Release-2.0.0,return key1.getStartCol() < key2.getStartCol() ? -1 : 1;
Release-2.0.0,});
Release-2.0.0,"For each partition, we generate a update split."
Release-2.0.0,"Although the split is empty for partitions those without any update data,"
Release-2.0.0,we still need to generate a update split to update the clock info on ps.
Release-2.0.0,Put the row split to the cache(row index to row splits map)
Release-2.0.0,"If all splits of the row are received, means this row can be merged"
Release-2.0.0,TODO
Release-2.0.0,TODO
Release-2.0.0,/////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,TODO
Release-2.0.0,buf.writeDouble(0);
Release-2.0.0,TODO
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,Now we just support pipelined row splits merging for dense type row
Release-2.0.0,Pre-fetching is disable default
Release-2.0.0,matrix id to clock map
Release-2.0.0,"task index, it must be unique for whole application"
Release-2.0.0,Deserialize data splits meta
Release-2.0.0,Get workers
Release-2.0.0,Send request to every ps
Release-2.0.0,Wait the responses
Release-2.0.0,Update clock cache
Release-2.0.0,if(syncNum % 1024 == 0) {
Release-2.0.0,}
Release-2.0.0,"Use simple flow, do not use any cache"
Release-2.0.0,Get row from cache.
Release-2.0.0,"if row clock is satisfy ssp staleness limit, just return."
Release-2.0.0,Get row from ps.
Release-2.0.0,Wait until the clock value of this row is greater than or equal to the value
Release-2.0.0,"For ASYNC mode, just get from pss."
Release-2.0.0,"For BSP/SSP, get rows from storage/cache first"
Release-2.0.0,Get from ps.
Release-2.0.0,Wait until the clock value of this row is greater than or equal to the value
Release-2.0.0,"For ASYNC, just get rows from pss."
Release-2.0.0,no more retries.
Release-2.0.0,calculate sleep time and return.
Release-2.0.0,parse the i-th sleep-time
Release-2.0.0,parse the i-th number-of-retries
Release-2.0.0,calculateSleepTime may overflow.
Release-2.0.0,"A few common retry policies, with no delays."
Release-2.0.0,Read matrix meta from meta file
Release-2.0.0,Save partitions to files use fork-join
Release-2.0.0,Write the ps matrix meta to the meta file
Release-2.0.0,matrix.startServering();
Release-2.0.0,return;
Release-2.0.0,Read matrix meta from meta file
Release-2.0.0,Load partitions from file use fork-join
Release-2.0.0,Read matrix meta from meta file
Release-2.0.0,Sort partitions
Release-2.0.0,int size = rows.length;
Release-2.0.0,int size = rows.length;
Release-2.0.0,int size = rows.size();
Release-2.0.0,int size = rows.size();
Release-2.0.0,int size = rows.size();
Release-2.0.0,int size = rows.size();
Release-2.0.0,int size = rows.size();
Release-2.0.0,int size = rows.size();
Release-2.0.0,close is a local operation and should finish within milliseconds; timeout just to be safe
Release-2.0.0,response will be null for one way messages.
Release-2.0.0,maxFrameLength = 2G
Release-2.0.0,lengthFieldOffset = 0
Release-2.0.0,lengthFieldLength = 8
Release-2.0.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
Release-2.0.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
Release-2.0.0,"return Objects.toStringHelper(this).add(""serial"", serial).add(""listSize"", datas.size())"
Release-2.0.0,.toString();
Release-2.0.0,indicates whether this connection's life cycle is managed
Release-2.0.0,See if we already have a connection (common case)
Release-2.0.0,create a unique lock for this RS + protocol (if necessary)
Release-2.0.0,get the RS lock
Release-2.0.0,do one more lookup in case we were stalled above
Release-2.0.0,Only create isa when we need to.
Release-2.0.0,definitely a cache miss. establish an RPC for
Release-2.0.0,this RS
Release-2.0.0,Throw what the RemoteException was carrying.
Release-2.0.0,check
Release-2.0.0,every
Release-2.0.0,minutes
Release-2.0.0,TODO
Release-2.0.0,创建failoverHandler
Release-2.0.0,"The number of times this invocation handler has ever been failed over,"
Release-2.0.0,before this method invocation attempt. Used to prevent concurrent
Release-2.0.0,failed method invocations from triggering multiple failover attempts.
Release-2.0.0,Make sure that concurrent failed method invocations
Release-2.0.0,only cause a
Release-2.0.0,single actual fail over.
Release-2.0.0,RpcController + Message in the method args
Release-2.0.0,(generated code from RPC bits in .proto files have
Release-2.0.0,RpcController)
Release-2.0.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
Release-2.0.0,+ (System.currentTimeMillis() - beforeConstructTs));
Release-2.0.0,get an instance of the method arg type
Release-2.0.0,RpcController + Message in the method args
Release-2.0.0,(generated code from RPC bits in .proto files have
Release-2.0.0,RpcController)
Release-2.0.0,Message (hand written code usually has only a single
Release-2.0.0,argument)
Release-2.0.0,log any RPC responses that are slower than the configured
Release-2.0.0,warn
Release-2.0.0,response time or larger than configured warning size
Release-2.0.0,"when tagging, we let TooLarge trump TooSmall to keep"
Release-2.0.0,output simple
Release-2.0.0,note that large responses will often also be slow.
Release-2.0.0,provides a count of log-reported slow responses
Release-2.0.0,RpcController + Message in the method args
Release-2.0.0,(generated code from RPC bits in .proto files have
Release-2.0.0,RpcController)
Release-2.0.0,unexpected
Release-2.0.0,"in the protobuf methods, args[1] is the only significant argument"
Release-2.0.0,for JSON encoding
Release-2.0.0,base information that is reported regardless of type of call
Release-2.0.0,Disable Nagle's Algorithm since we don't want packets to wait
Release-2.0.0,Configure the event pipeline factory.
Release-2.0.0,Make a new connection.
Release-2.0.0,Remove all pending requests (will be canceled after relinquishing
Release-2.0.0,write lock).
Release-2.0.0,Cancel any pending requests by sending errors to the callbacks:
Release-2.0.0,Close the channel:
Release-2.0.0,Close the connection:
Release-2.0.0,Shut down all thread pools to exit.
Release-2.0.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
Release-2.0.0,See NettyServer.prepareResponse for where we write out the response.
Release-2.0.0,"It writes the call.id (int), a boolean signifying any error (and if"
Release-2.0.0,"so the exception name/trace), and the response bytes"
Release-2.0.0,Read the call id.
Release-2.0.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
Release-2.0.0,"instead, it returns a null message object."
Release-2.0.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
Release-2.0.0,System.currentTimeMillis());
Release-2.0.0,"It would be good widen this to just Throwable, but IOException is what we"
Release-2.0.0,allow now
Release-2.0.0,not implemented
Release-2.0.0,not implemented
Release-2.0.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
Release-2.0.0,cache of RpcEngines by protocol
Release-2.0.0,return the RpcEngine configured to handle a protocol
Release-2.0.0,We only handle the ConnectException.
Release-2.0.0,This is the exception we can't handle.
Release-2.0.0,check if timed out
Release-2.0.0,wait for retry
Release-2.0.0,IGNORE
Release-2.0.0,return the RpcEngine that handles a proxy object
Release-2.0.0,The default implementation works synchronously
Release-2.0.0,punt: allocate a new buffer & copy into it
Release-2.0.0,Parse cmd parameters
Release-2.0.0,load hadoop configuration
Release-2.0.0,load angel system configuration
Release-2.0.0,load user configuration:
Release-2.0.0,load user config file
Release-2.0.0,load command line parameters
Release-2.0.0,load user job resource files
Release-2.0.0,load ml conf file for graph based algorithm
Release-2.0.0,load user job jar if it exist
Release-2.0.0,Expand the environment variable
Release-2.0.0,Add default fs(local fs) for lib jars.
Release-2.0.0,"LOG.info(System.getProperty(""user.dir""));"
Release-2.0.0,get tokens for all the required FileSystems..
Release-2.0.0,Whether we need to recursive look into the directory structure
Release-2.0.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.0.0,user provided one (if any).
Release-2.0.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.0.0,get tokens for all the required FileSystems..
Release-2.0.0,Whether we need to recursive look into the directory structure
Release-2.0.0,creates a MultiPathFilter with the hiddenFileFilter and the
Release-2.0.0,user provided one (if any).
Release-2.0.0,"LOG.info(""Total input paths to process : "" + result.size());"
Release-2.0.0,a simple hdfs copy function assume src path and dest path are in same hdfs
Release-2.0.0,and FileSystem object has same schema
Release-2.0.0,"LOG.warn(""interrupted while sleeping"", ie);"
Release-2.0.0,public static String getHostname() {
Release-2.0.0,try {
Release-2.0.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
Release-2.0.0,} catch (UnknownHostException uhe) {
Release-2.0.0,}
Release-2.0.0,"return new StringBuilder().append("""").append(uhe).toString();"
Release-2.0.0,}
Release-2.0.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
Release-2.0.0,String hostname = getHostname();
Release-2.0.0,String classname = clazz.getSimpleName();
Release-2.0.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
Release-2.0.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
Release-2.0.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
Release-2.0.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
Release-2.0.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
Release-2.0.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
Release-2.0.0,
Release-2.0.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
Release-2.0.0,public void run() {
Release-2.0.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
Release-2.0.0,"this.val$classname + "" at "" + this.val$hostname}));"
Release-2.0.0,}
Release-2.0.0,});
Release-2.0.0,}
Release-2.0.0,"We we interrupted because we're meant to stop? If not, just"
Release-2.0.0,continue ignoring the interruption
Release-2.0.0,Recalculate waitTime.
Release-2.0.0,// Begin delegation to Thread
Release-2.0.0,// End delegation to Thread
Release-2.0.0,instance submitter class
Release-2.0.0,Obtain filename from path
Release-2.0.0,Split filename to prexif and suffix (extension)
Release-2.0.0,Check if the filename is okay
Release-2.0.0,Prepare temporary file
Release-2.0.0,Prepare buffer for data copying
Release-2.0.0,Open and check input stream
Release-2.0.0,Open output stream and copy data between source file in JAR and the temporary file
Release-2.0.0,"If read/write fails, close streams safely before throwing an exception"
Release-2.0.0,"Finally, load the library"
Release-2.0.0,little endian load order
Release-2.0.0,tail
Release-2.0.0,fallthrough
Release-2.0.0,fallthrough
Release-2.0.0,finalization
Release-2.0.0,fmix(h1);
Release-2.0.0,----------
Release-2.0.0,body
Release-2.0.0,----------
Release-2.0.0,tail
Release-2.0.0,----------
Release-2.0.0,finalization
Release-2.0.0,----------
Release-2.0.0,body
Release-2.0.0,----------
Release-2.0.0,tail
Release-2.0.0,----------
Release-2.0.0,finalization
Release-2.0.0,throw new AngelException(e);
Release-2.0.0,JobStateProto jobState = report.getJobState();
Release-2.0.0,Check need load matrices
Release-2.0.0,Used for java code to get a AngelClient instance
Release-2.0.0,Used for python code to get a AngelClient instance
Release-2.0.0,load user job resource files
Release-2.0.0,the leaf level file should be readable by others
Release-2.0.0,the subdirs in the path should have execute permissions for
Release-2.0.0,others
Release-2.0.0,2.get job id
Release-2.0.0,Credentials credentials = new Credentials();
Release-2.0.0,4.copy resource files to hdfs
Release-2.0.0,5.write configuration to a xml file
Release-2.0.0,6.create am container context
Release-2.0.0,7.Submit to ResourceManager
Release-2.0.0,8.get app master client
Release-2.0.0,Create a number of filenames in the JobTracker's fs namespace
Release-2.0.0,add all the command line files/ jars and archive
Release-2.0.0,first copy them to jobtrackers filesystem
Release-2.0.0,should not throw a uri exception
Release-2.0.0,should not throw an uri excpetion
Release-2.0.0,set the timestamps of the archives and files
Release-2.0.0,set the public/private visibility of the archives and files
Release-2.0.0,get DelegationToken for each cached file
Release-2.0.0,check if we do not need to copy the files
Release-2.0.0,is jt using the same file system.
Release-2.0.0,just checking for uri strings... doing no dns lookups
Release-2.0.0,to see if the filesystems are the same. This is not optimal.
Release-2.0.0,but avoids name resolution.
Release-2.0.0,this might have name collisions. copy will throw an exception
Release-2.0.0,parse the original path to create new path
Release-2.0.0,check for ports
Release-2.0.0,Write job file to JobTracker's fs
Release-2.0.0,Setup resource requirements
Release-2.0.0,Setup LocalResources
Release-2.0.0,Setup security tokens
Release-2.0.0,Setup the command to run the AM
Release-2.0.0,Add AM user command opts
Release-2.0.0,Final command
Release-2.0.0,Setup the CLASSPATH in environment
Release-2.0.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
Release-2.0.0,Setup the environment variables for Admin first
Release-2.0.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
Release-2.0.0,Parse distributed cache
Release-2.0.0,Setup ContainerLaunchContext for AM container
Release-2.0.0,Set up the ApplicationSubmissionContext
Release-2.0.0,private volatile PS2PSPusherImpl ps2PSPusher;
Release-2.0.0,TODO
Release-2.0.0,Add tokens to new user so that it may execute its task correctly.
Release-2.0.0,TODO
Release-2.0.0,to exit
Release-2.0.0,TODO
Release-2.0.0,TODO
Release-2.0.0,ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));
Release-2.0.0,context.getSnapshotManager().processRecovery();
Release-2.0.0,Recover PS from snapshot or load path
Release-2.0.0,First check snapshot
Release-2.0.0,Check load path setting
Release-2.0.0,TODO
Release-2.0.0,if(ps2PSPusher != null) {
Release-2.0.0,ps2PSPusher.start();
Release-2.0.0,}
Release-2.0.0,public PS2PSPusherImpl getPs2PSPusher() {
Release-2.0.0,return ps2PSPusher;
Release-2.0.0,}
Release-2.0.0,"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");"
Release-2.0.0,"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);"
Release-2.0.0,Release the input buffer
Release-2.0.0,Release the input buffer
Release-2.0.0,"1. handle the rpc, get the response"
Release-2.0.0,Release the input buffer
Release-2.0.0,2. Serialize the response
Release-2.0.0,Send the serialized response
Release-2.0.0,Exception happened
Release-2.0.0,write seq id
Release-2.0.0,Just serialize the head
Release-2.0.0,Exception happened
Release-2.0.0,Allocate result buffer
Release-2.0.0,Exception happened
Release-2.0.0,Just serialize the head
Release-2.0.0,Exception happened
Release-2.0.0,Reset the response and allocate buffer again
Release-2.0.0,Get partition and check the partition state
Release-2.0.0,Get the stored pss for this partition
Release-2.0.0,"Check this ps is the master ps for this location, only master ps can accept the update"
Release-2.0.0,Check the partition state again
Release-2.0.0,Start to put the update to the slave pss
Release-2.0.0,TODO
Release-2.0.0,"context.getPS2PSPusher().put(request, in, partLoc);"
Release-2.0.0,Get partition and check the partition state
Release-2.0.0,Get the stored pss for this partition
Release-2.0.0,"Check this ps is the master ps for this partition, if not, just return failed"
Release-2.0.0,Start to put the update to the slave pss
Release-2.0.0,TODO
Release-2.0.0,return ServerState.GENERAL;
Release-2.0.0,Use Epoll for linux
Release-2.0.0,public String uuid;
Release-2.0.0,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
Release-2.0.0,this.channelPool = channelPool;
Release-2.0.0,}
Release-2.0.0,private final ParameterServer psServer;
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO:default value
Release-2.0.0,buf.readDouble();
Release-2.0.0,TODO
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"////// network io method, for model transform"
Release-2.0.0,///////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,public ObjectIterator<Long2FloatMap.Entry> getIter() {
Release-2.0.0,return ((LongFloatVector) row).getStorage().entryIterator();
Release-2.0.0,}
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,public ObjectIterator<Long2LongMap.Entry> getIter() {
Release-2.0.0,return ((LongLongVector) row).getStorage().entryIterator();
Release-2.0.0,}
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
Release-2.0.0,and call endWrite/endRead after
Release-2.0.0,////////////////////////////////////////////////////////////////////////////////////////////////
Release-2.0.0,TODO: dynamic add/delete row
Release-2.0.0,private final List<PartitionKey> partitionKeys;
Release-2.0.0,Use Epoll for linux
Release-2.0.0,find the partition request context from cache
Release-2.0.0,get a channel to server from pool
Release-2.0.0,"if channel is not valid, it means maybe the connections to the server are closed"
Release-2.0.0,channelManager.removeChannelPool(loc);
Release-2.0.0,Generate seq id
Release-2.0.0,Create a RecoverPartRequest
Release-2.0.0,Serialize the request
Release-2.0.0,Change the seqId for the request
Release-2.0.0,Serialize the request
Release-2.0.0,"First check the state of the channels in the pool, if a channel is unused, just return"
Release-2.0.0,"If all channels are in use, create a new channel or wait"
Release-2.0.0,Create a new channel
Release-2.0.0,"add the PSAgentContext,need fix"
Release-2.0.0,TODO:add more vector type
Release-2.0.0,TODO : subDim set
Release-2.0.0,Sort the parts by partitionId
Release-2.0.0,Sort partition keys use start column index
Release-2.0.0,"For each partition, we generate a update split."
Release-2.0.0,"Although the split is empty for partitions those without any update data,"
Release-2.0.0,we still need to generate a update split to update the clock info on ps.
Release-2.0.0,Sort the parts by partitionId
Release-2.0.0,Sort partition keys use start column index
Release-2.0.0,"For each partition, we generate a update split."
Release-2.0.0,"Although the split is empty for partitions those without any update data,"
Release-2.0.0,we still need to generate a update split to update the clock info on ps.
Release-2.0.0,write the max abs
Release-2.0.0,---------------------------------------------------
Release-2.0.0,---------------------------------------------------
Release-2.0.0,---------------------------------------------------------------
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,axis = 0: on rows
Release-2.0.0,axis = 1: on cols
Release-2.0.0,1. find the insert point
Release-2.0.0,2. check the capacity and insert
Release-2.0.0,3. increase size
Release-2.0.0,1. find the insert point
Release-2.0.0,2. check the capacity and insert
Release-2.0.0,3. increase size
Release-2.0.0,1. find the insert point
Release-2.0.0,2. check the capacity and insert
Release-2.0.0,3. increase size
Release-2.0.0,1. find the insert point
Release-2.0.0,2. check the capacity and insert
Release-2.0.0,3. increase size
Release-2.0.0,1. find the insert point
Release-2.0.0,2. check the capacity and insert
Release-2.0.0,3. increase size
Release-2.0.0,1. find the insert point
Release-2.0.0,2. check the capacity and insert
Release-2.0.0,3. increase size
Release-2.0.0,1. find the insert point
Release-2.0.0,2. check the capacity and insert
Release-2.0.0,3. increase size
Release-2.0.0,1. find the insert point
Release-2.0.0,2. check the capacity and insert
Release-2.0.0,3. increase size
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,multi-rehash
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,"slower but memory efficient, for small vector only"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"no rehashor one onle rehash is required, nothing to optimization"
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,to avoid multi-rehash
Release-2.0.0,"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.0.0,"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
Release-2.0.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.0.0,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
Release-2.0.0,add dense double matrix
Release-2.0.0,TODO Auto-generated method stub
Release-2.0.0,TODO Auto-generated method stub
Release-2.0.0,TODO Auto-generated method stub
Release-2.0.0,get configuration from config file
Release-2.0.0,set localDir with enviroment set by nm.
Release-2.0.0,get master location
Release-2.0.0,init task manager and start tasks
Release-2.0.0,start heartbeat thread
Release-2.0.0,taskManager.assignTaskIds(response.getTaskidsList());
Release-2.0.0,todo
Release-2.0.0,"if worker timeout, it may be knocked off."
Release-2.0.0,"SUCCESS, do nothing"
Release-2.0.0,heartbeatFailedTime = 0;
Release-2.0.0,private KEY currentKey;
Release-2.0.0,will be created
Release-2.0.0,TODO Auto-generated method stub
Release-2.0.0,Bitmap bitmap = new Bitmap();
Release-2.0.0,int max = indexArray[size - 1];
Release-2.0.0,byte [] bitIndexArray = new byte[max / 8 + 1];
Release-2.0.0,for(int i = 0; i < size; i++){
Release-2.0.0,int bitIndex = indexArray[i] >> 3;
Release-2.0.0,int bitOffset = indexArray[i] - (bitIndex << 3);
Release-2.0.0,switch(bitOffset){
Release-2.0.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
Release-2.0.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
Release-2.0.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
Release-2.0.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
Release-2.0.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
Release-2.0.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
Release-2.0.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
Release-2.0.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
Release-2.0.0,}
Release-2.0.0,}
Release-2.0.0,"true, false"
Release-2.0.0,//////////////////////////////
Release-2.0.0,Application Configs
Release-2.0.0,//////////////////////////////
Release-2.0.0,//////////////////////////////
Release-2.0.0,Master Configs
Release-2.0.0,//////////////////////////////
Release-2.0.0,//////////////////////////////
Release-2.0.0,Worker Configs
Release-2.0.0,//////////////////////////////
Release-2.0.0,//////////////////////////////
Release-2.0.0,Task Configs
Release-2.0.0,//////////////////////////////
Release-2.0.0,//////////////////////////////
Release-2.0.0,ParameterServer Configs
Release-2.0.0,//////////////////////////////
Release-2.0.0,////////////////// IPC //////////////////////////
Release-2.0.0,//////////////////////////////
Release-2.0.0,Matrix transfer Configs.
Release-2.0.0,//////////////////////////////
Release-2.0.0,//////////////////////////////
Release-2.0.0,Matrix transfer Configs.
Release-2.0.0,//////////////////////////////
Release-2.0.0,Configs used to ANGEL_PS_PSAGENT running mode future.
Release-2.0.0,model parse
Release-2.0.0,Mark whether use pyangel or not.
Release-2.0.0,private Configuration conf;
Release-2.0.0,"Configuration that should be used in python environment, there should only be one"
Release-2.0.0,configuration instance in each Angel context.
Release-2.0.0,Use private access means jconf should not be changed or modified in this way.
Release-2.0.0,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
Release-2.0.0,Do nothing
Release-2.0.0,To-DO: add other ways to justify different value types
Release-2.0.0,"This is so ugly, must re-implement by more elegance way"
Release-2.0.0,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
Release-2.0.0,and other files submitted by user.
Release-2.0.0,Launch python process
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Data format
Release-2.0.0,Feature number of train data
Release-2.0.0,Tree number
Release-2.0.0,Tree depth
Release-2.0.0,Split number
Release-2.0.0,Feature sample ratio
Release-2.0.0,Ratio of validation
Release-2.0.0,Learning rate
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Set data format
Release-2.0.0,"Set angel resource, #worker, #task, #PS"
Release-2.0.0,Set GBDT algorithm parameters
Release-2.0.0,Set training data path
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set predict data path
Release-2.0.0,Set load model path
Release-2.0.0,Set predict result path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Train batch number per epoch.
Release-2.0.0,Batch number
Release-2.0.0,Model type
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Set data format
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,set sgd LR algorithm parameters #feature #epoch
Release-2.0.0,Set trainning data path
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Train batch number per epoch.
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Set data format
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,set sgd FM algorithm parameters #feature #epoch
Release-2.0.0,Set trainning data path
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Model type
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Set data format
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,set sgd LR algorithm parameters #feature #epoch
Release-2.0.0,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
Release-2.0.0,Set trainning data path
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,class number
Release-2.0.0,Model type
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Set data format
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,set sgd LR algorithm parameters #feature #epoch
Release-2.0.0,Set log path
Release-2.0.0,Set trainning data path
Release-2.0.0,Set save model path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set save model path
Release-2.0.0,Set actionType incremental train
Release-2.0.0,Set log path
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set predict result path
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set training data path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Cluster center number
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Sample ratio per mini-batch
Release-2.0.0,C
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,set Kmeans algorithm parameters #cluster #feature #epoch
Release-2.0.0,Set data format
Release-2.0.0,Set trainning data path
Release-2.0.0,Set save model path
Release-2.0.0,Set log save path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set save model path
Release-2.0.0,Set actionType incremental train
Release-2.0.0,Set log path
Release-2.0.0,Set testing data path
Release-2.0.0,Set load model path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Model type
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Set data format
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,set sgd LR algorithm parameters #feature #epoch
Release-2.0.0,Set trainning data path
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set predict result path
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Data is classification
Release-2.0.0,Model is classification
Release-2.0.0,Train batch number per epoch.
Release-2.0.0,loss delta
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Set data format
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,set sgd LR algorithm parameters #feature #epoch
Release-2.0.0,Set trainning data path
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set save model path
Release-2.0.0,Set actionType incremental train
Release-2.0.0,Set log path
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,Feature number of train data
Release-2.0.0,Total iteration number
Release-2.0.0,Validation sample Ratio
Release-2.0.0,"Data format, libsvm or dummy"
Release-2.0.0,Data is classification
Release-2.0.0,Model is classification
Release-2.0.0,Train batch number per epoch.
Release-2.0.0,Learning rate
Release-2.0.0,Decay of learning rate
Release-2.0.0,Regularization coefficient
Release-2.0.0,Set local deploy mode
Release-2.0.0,Set basic configuration keys
Release-2.0.0,Set data format
Release-2.0.0,"set angel resource parameters #worker, #task, #PS"
Release-2.0.0,set sgd LR algorithm parameters #feature #epoch
Release-2.0.0,Set trainning data path
Release-2.0.0,Set save model path
Release-2.0.0,Set log path
Release-2.0.0,Set actionType train
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set save model path
Release-2.0.0,Set actionType incremental train
Release-2.0.0,Set log path
Release-2.0.0,Set trainning data path
Release-2.0.0,Set load model path
Release-2.0.0,Set predict result path
Release-2.0.0,Set actionType prediction
Release-2.0.0,"gradient descent first, then truncated"
Release-2.0.0,TODO: optimize int key indices
Release-2.0.0,"System.out.println(""deserialize cols.length="" + nCols);"
Release-2.0.0,"System.out.print(""deserialize "");"
Release-2.0.0,"System.out.print(cols[c] + "" "");"
Release-2.0.0,System.out.println();
Release-2.0.0,TODO Auto-generated method stub
Release-2.0.0,"ground truth: positive, precision: positive"
Release-2.0.0,start row index for words
Release-2.0.0,start row index for docs
Release-2.0.0,doc ids
Release-2.0.0,topic assignments
Release-2.0.0,word to docs reverse index
Release-2.0.0,count word
Release-2.0.0,build word start index
Release-2.0.0,build word to doc reverse idx
Release-2.0.0,build dks
Release-2.0.0,dks = new TraverseHashMap[n_docs];
Release-2.0.0,for (int d = 0; d < n_docs; d++) {
Release-2.0.0,if (K < Short.MAX_VALUE) {
Release-2.0.0,if (docs.get(d).len < Byte.MAX_VALUE)
Release-2.0.0,dks[d] = new S2BTraverseMap(docs.get(d).len);
Release-2.0.0,if (docs.get(d).len < Short.MAX_VALUE)
Release-2.0.0,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
Release-2.0.0,else
Release-2.0.0,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
Release-2.0.0,} else {
Release-2.0.0,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
Release-2.0.0,}
Release-2.0.0,}
Release-2.0.0,build dks
Release-2.0.0,allocate update maps
Release-2.0.0,Skip if no token for this word
Release-2.0.0,Check whether error when fetching word-topic
Release-2.0.0,Build FTree for current word
Release-2.0.0,current doc
Release-2.0.0,old topic assignment
Release-2.0.0,"Check if error happens. if this happen, it's probably that failures happen to servers."
Release-2.0.0,We need to adjust the memory settings or network fetching parameters.
Release-2.0.0,Update statistics if needed
Release-2.0.0,Calculate psum and sample new topic
Release-2.0.0,Update statistics if needed
Release-2.0.0,Assign new topic
Release-2.0.0,Skip if no token for this word
Release-2.0.0,The starting point
Release-2.0.0,There's always an unused entry.
Release-2.0.0,The starting point
Release-2.0.0,There's always an unused entry.
Release-2.0.0,The starting point
Release-2.0.0,There's always an unused entry.
Release-2.0.0,The starting point
Release-2.0.0,There's always an unused entry.
Release-2.0.0,The starting point
Release-2.0.0,There's always an unused entry.
Release-2.0.0,The starting point
Release-2.0.0,There's always an unused entry.
Release-2.0.0,print();
Release-2.0.0,The starting point
Release-2.0.0,There's always an unused entry.
Release-2.0.0,The starting point
Release-2.0.0,There's always an unused entry.
Release-2.0.0,The starting point
Release-2.0.0,There's always an unused entry.
Release-2.0.0,The starting point
Release-2.0.0,There's always an unused entry.
Release-2.0.0,print();
Release-2.0.0,Write #rows
Release-2.0.0,Write each row
Release-2.0.0,dense
Release-2.0.0,sparse
Release-2.0.0,LOG.info(buf.refCnt());
Release-2.0.0,dense
Release-2.0.0,sparse
Release-2.0.0,calculate columns
Release-2.0.0,loss function
Release-2.0.0,gradient and hessian
Release-2.0.0,"categorical feature set, null: none, empty: all, else: partial"
Release-2.0.0,"node's end index in instancePos, instances in [start, end] belong to a tree node"
Release-2.0.0,initialize the phase
Release-2.0.0,current tree and depth
Release-2.0.0,create loss function
Release-2.0.0,calculate grad info of each instance
Release-2.0.0,"create data sketch, push candidate split value to PS"
Release-2.0.0,1. calculate candidate split value
Release-2.0.0,categorical features
Release-2.0.0,2. push local sketch to PS
Release-2.0.0,the leader worker
Release-2.0.0,merge categorical features
Release-2.0.0,create updates
Release-2.0.0,"pull the global sketch from PS, only called once by each worker"
Release-2.0.0,number of categorical feature
Release-2.0.0,sample feature
Release-2.0.0,push sampled feature set to the current tree
Release-2.0.0,create new tree
Release-2.0.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
Release-2.0.0,calculate gradient
Release-2.0.0,"1. create new tree, initialize tree nodes and node stats"
Release-2.0.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
Release-2.0.0,2.1. pull the sampled features of the current tree
Release-2.0.0,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
Release-2.0.0,"2.2. if use all the features, only called one"
Release-2.0.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
Release-2.0.0,4. set root node to active
Release-2.0.0,"5. reset instance position, set the root node's span"
Release-2.0.0,6. calculate gradient
Release-2.0.0,1. decide nodes that should be calculated
Release-2.0.0,2. decide calculated and subtracted tree nodes
Release-2.0.0,3. calculate threads
Release-2.0.0,wait until all threads finish
Release-2.0.0,4. subtract threads
Release-2.0.0,wait until all threads finish
Release-2.0.0,5. send histograms to PS
Release-2.0.0,6. update histogram cache
Release-2.0.0,clock
Release-2.0.0,find split
Release-2.0.0,"1. find responsible tree node, using RR scheme"
Release-2.0.0,2. pull gradient histogram
Release-2.0.0,2.1. get the name of this node's gradient histogram on PS
Release-2.0.0,2.2. pull the histogram
Release-2.0.0,2.3. find best split result of this tree node
Release-2.0.0,2.3.1 using server split
Release-2.0.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.0.0,update the grad stats of children node
Release-2.0.0,update the left child
Release-2.0.0,update the right child
Release-2.0.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.0.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
Release-2.0.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
Release-2.0.0,2.3.5 reset this tree node's gradient histogram to 0
Release-2.0.0,3. push split feature to PS
Release-2.0.0,4. push split value to PS
Release-2.0.0,5. push split gain to PS
Release-2.0.0,6. set phase to AFTER_SPLIT
Release-2.0.0,this.phase = GBDTPhase.AFTER_SPLIT;
Release-2.0.0,clock
Release-2.0.0,1. get split feature
Release-2.0.0,2. get split value
Release-2.0.0,3. get split gain
Release-2.0.0,4. get node weight
Release-2.0.0,5. split node
Release-2.0.0,update local replica
Release-2.0.0,create AfterSplit task
Release-2.0.0,"2. check thread stats, if all threads finish, return"
Release-2.0.0,6. clock
Release-2.0.0,"split the span of one node, reset the instance position"
Release-2.0.0,in case this worker has no instance on this node
Release-2.0.0,set the span of left child
Release-2.0.0,set the span of right child
Release-2.0.0,"1. left to right, find the first instance that should be in the right child"
Release-2.0.0,"2. right to left, find the first instance that should be in the left child"
Release-2.0.0,3. swap two instances
Release-2.0.0,4. find the cut pos
Release-2.0.0,5. set the span of left child
Release-2.0.0,6. set the span of right child
Release-2.0.0,set tree node to active
Release-2.0.0,set node to leaf
Release-2.0.0,set node to inactive
Release-2.0.0,finish current depth
Release-2.0.0,finish current tree
Release-2.0.0,set the tree phase
Release-2.0.0,check if there is active node
Release-2.0.0,check if finish all the tree
Release-2.0.0,update node's grad stats on PS
Release-2.0.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
Release-2.0.0,the root node's stats is updated by leader worker
Release-2.0.0,1. create the update
Release-2.0.0,2. push the update to PS
Release-2.0.0,1. update predictions of training data
Release-2.0.0,2. update predictions of validation data
Release-2.0.0,the leader task adds node prediction to flush list
Release-2.0.0,1. name of this node's grad histogram on PS
Release-2.0.0,2. build the grad histogram of this node
Release-2.0.0,3. push the histograms to PS
Release-2.0.0,4. reset thread stats to finished
Release-2.0.0,5.1. set the children nodes of this node
Release-2.0.0,5.2. set split info and grad stats to this node
Release-2.0.0,5.2. create children nodes
Release-2.0.0,"5.3. create node stats for children nodes, and add them to the tree"
Release-2.0.0,5.4. reset instance position
Release-2.0.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
Release-2.0.0,5.6. set children nodes to leaf nodes
Release-2.0.0,5.7. set nid to leaf node
Release-2.0.0,5.8. deactivate active node
Release-2.0.0,"get feature type, 0:empty 1:all equal 2:real"
Release-2.0.0,"if not -1, sufficient space will be allocated at once"
Release-2.0.0,copy the highest levels
Release-2.0.0,copy baseBuffer
Release-2.0.0,merge two non-empty quantile sketches
Release-2.0.0,left child <= split value; right child > split value
Release-2.0.0,"the first: minimal, the last: maximal"
Release-2.0.0,categorical features
Release-2.0.0,continuous features
Release-2.0.0,left child <= split value; right child > split value
Release-2.0.0,feature index used to split
Release-2.0.0,feature value used to split
Release-2.0.0,loss change after split this node
Release-2.0.0,grad stats of the left child
Release-2.0.0,grad stats of the right child
Release-2.0.0,"LOG.info(""Constructor with fid = -1"");"
Release-2.0.0,fid = -1: no split currently
Release-2.0.0,the minimal split value is the minimal value of feature
Release-2.0.0,the splits do not include the maximal value of feature
Release-2.0.0,"1. the average distance, (maxValue - minValue) / splitNum"
Release-2.0.0,2. calculate the candidate split value
Release-2.0.0,1. new feature's histogram (grad + hess)
Release-2.0.0,size: sampled_featureNum * (2 * splitNum)
Release-2.0.0,"in other words, concatenate each feature's histogram"
Release-2.0.0,2. get the span of this node
Release-2.0.0,------ 3. using sparse-aware method to build histogram ---
Release-2.0.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
Release-2.0.0,3.1. get the instance index
Release-2.0.0,3.2. get the grad and hess of the instance
Release-2.0.0,3.3. add to the sum
Release-2.0.0,3.4. loop the non-zero entries
Release-2.0.0,3.4.1. get feature value
Release-2.0.0,3.4.2. current feature's position in the sampled feature set
Release-2.0.0,"int fPos = findFidPlace(this.controller.fSet, fid);"
Release-2.0.0,3.4.3. find the position of feature value in a histogram
Release-2.0.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
Release-2.0.0,3.4.4. add the grad and hess to the corresponding bin
Release-2.0.0,3.4.5. add the reverse to the bin that contains 0.0f
Release-2.0.0,4. add the grad and hess sum to the zero bin of all features
Release-2.0.0,find the best split result of the histogram of a tree node
Release-2.0.0,1. calculate the gradStats of the root node
Release-2.0.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
Release-2.0.0,2. loop over features
Release-2.0.0,2.1. get the ture feature id in the sampled feature set
Release-2.0.0,2.2. get the indexes of histogram of this feature
Release-2.0.0,2.3. find the best split of current feature
Release-2.0.0,2.4. update the best split result if possible
Release-2.0.0,"update the grad stats of the root node on PS, only called once by leader worker"
Release-2.0.0,3. update the grad stats of children node
Release-2.0.0,3.1. update the left child
Release-2.0.0,3.2. update the right child
Release-2.0.0,find the best split result of one feature
Release-2.0.0,1. set the feature id
Release-2.0.0,2. create the best left stats and right stats
Release-2.0.0,3. the gain of the root node
Release-2.0.0,4. create the temp left and right grad stats
Release-2.0.0,5. loop over all the data in histogram
Release-2.0.0,5.1. get the grad and hess of current hist bin
Release-2.0.0,5.2. check whether we can split with current left hessian
Release-2.0.0,right = root - left
Release-2.0.0,5.3. check whether we can split with current right hessian
Release-2.0.0,5.4. calculate the current loss gain
Release-2.0.0,5.5. check whether we should update the split result with current loss gain
Release-2.0.0,split value = sketches[splitIdx]
Release-2.0.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.0.0,6. set the best left and right grad stats
Release-2.0.0,partition number
Release-2.0.0,cols of each partition
Release-2.0.0,1. calculate the total grad sum and hess sum
Release-2.0.0,2. create the grad stats of the node
Release-2.0.0,1. calculate the total grad sum and hess sum
Release-2.0.0,2. create the grad stats of the node
Release-2.0.0,1. calculate the total grad sum and hess sum
Release-2.0.0,2. create the grad stats of the node
Release-2.0.0,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
Release-2.0.0,assert fvalue >= sketch[start] && fvalue <= sketch[end];
Release-2.0.0,if (left > end) return end - start;
Release-2.0.0,find the best split result of the histogram of a tree node
Release-2.0.0,2.2. get the indexes of histogram of this feature
Release-2.0.0,2.3. find the best split of current feature
Release-2.0.0,2.4. update the best split result if possible
Release-2.0.0,find the best split result of one feature
Release-2.0.0,1. set the feature id
Release-2.0.0,splitEntry.setFid(fid);
Release-2.0.0,2. create the best left stats and right stats
Release-2.0.0,3. the gain of the root node
Release-2.0.0,4. create the temp left and right grad stats
Release-2.0.0,5. loop over all the data in histogram
Release-2.0.0,5.1. get the grad and hess of current hist bin
Release-2.0.0,5.2. check whether we can split with current left hessian
Release-2.0.0,right = root - left
Release-2.0.0,5.3. check whether we can split with current right hessian
Release-2.0.0,5.4. calculate the current loss gain
Release-2.0.0,5.5. check whether we should update the split result with current loss gain
Release-2.0.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.0.0,6. set the best left and right grad stats
Release-2.0.0,find the best split result of a serve row on the PS
Release-2.0.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
Release-2.0.0,2.2. get the start index in histogram of this feature
Release-2.0.0,2.3. find the best split of current feature
Release-2.0.0,2.4. update the best split result if possible
Release-2.0.0,"find the best split result of one feature from a server row, used by the PS"
Release-2.0.0,1. set the feature id
Release-2.0.0,2. create the best left stats and right stats
Release-2.0.0,3. the gain of the root node
Release-2.0.0,4. create the temp left and right grad stats
Release-2.0.0,5. loop over all the data in histogram
Release-2.0.0,5.1. get the grad and hess of current hist bin
Release-2.0.0,5.2. check whether we can split with current left hessian
Release-2.0.0,right = root - left
Release-2.0.0,5.3. check whether we can split with current right hessian
Release-2.0.0,5.4. calculate the current loss gain
Release-2.0.0,5.5. check whether we should update the split result with current loss gain
Release-2.0.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
Release-2.0.0,the task use index to find fvalue
Release-2.0.0,"5.6. if should update, also update the best left and right grad stats"
Release-2.0.0,6. set the best left and right grad stats
Release-2.0.0,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
Release-2.0.0,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
Release-2.0.0,max and min of each feature
Release-2.0.0,clear all the information
Release-2.0.0,calculate the sum of gradient and hess
Release-2.0.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
Release-2.0.0,ridx)
Release-2.0.0,check if necessary information is ready
Release-2.0.0,"same as add, reduce is used in All Reduce"
Release-2.0.0,"features used in this tree, if equals null, means use all the features without sampling"
Release-2.0.0,node in the tree
Release-2.0.0,the gradient info of each instances
Release-2.0.0,initialize nodes
Release-2.0.0,gradient
Release-2.0.0,second order gradient
Release-2.0.0,TODO: only support dense double now
Release-2.0.0,int sendStartCol = (int) row.getStartCol();
Release-2.0.0,find the max abs
Release-2.0.0,compress data
Release-2.0.0,logistic loss for binary classification task.
Release-2.0.0,"logistic loss, but predict un-transformed margin"
Release-2.0.0,check if label in range
Release-2.0.0,return the default evaluation metric for the objective
Release-2.0.0,"task type: classification, regression, or ranking"
Release-2.0.0,"quantile sketch, size = featureNum * splitNum"
Release-2.0.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
Release-2.0.0,"active tree nodes, size = pow(2, treeDepth) -1"
Release-2.0.0,sampled features. size = treeNum * sampleRatio * featureNum
Release-2.0.0,categorical feature. size = workerNum * cateFeatNum * splitNum
Release-2.0.0,"split features, size = treeNum * treeNodeNum"
Release-2.0.0,"split values, size = treeNum * treeNodeNum"
Release-2.0.0,"split gains, size = treeNum * treeNodeNum"
Release-2.0.0,"node weights, size = treeNum * treeNodeNum"
Release-2.0.0,"node preds, size = treeNum * treeNodeNum"
Release-2.0.0,if using PS to perform split
Release-2.0.0,step size for a tree
Release-2.0.0,number of class
Release-2.0.0,minimum loss change required for a split
Release-2.0.0,maximum depth of a tree
Release-2.0.0,number of features
Release-2.0.0,number of nonzero
Release-2.0.0,number of candidates split value
Release-2.0.0,----- the rest parameters are less important ----
Release-2.0.0,base instance weight
Release-2.0.0,minimum amount of hessian(weight) allowed in a child
Release-2.0.0,L2 regularization factor
Release-2.0.0,L1 regularization factor
Release-2.0.0,default direction choice
Release-2.0.0,maximum delta update we can add in weight estimation
Release-2.0.0,this parameter can be used to stabilize update
Release-2.0.0,default=0 means no constraint on weight delta
Release-2.0.0,whether we want to do subsample for row
Release-2.0.0,whether to subsample columns for each tree
Release-2.0.0,accuracy of sketch
Release-2.0.0,accuracy of sketch
Release-2.0.0,leaf vector size
Release-2.0.0,option for parallelization
Release-2.0.0,option to open cacheline optimization
Release-2.0.0,whether to not print info during training.
Release-2.0.0,maximum depth of the tree
Release-2.0.0,number of features used for tree construction
Release-2.0.0,"minimum loss change required for a split, otherwise stop split"
Release-2.0.0,----- the rest parameters are less important ----
Release-2.0.0,default direction choice
Release-2.0.0,whether we want to do sample data
Release-2.0.0,whether to sample columns during tree construction
Release-2.0.0,whether to use histogram for split
Release-2.0.0,number of histogram units
Release-2.0.0,whether to print info during training.
Release-2.0.0,----- the rest parameters are obtained after training ----
Release-2.0.0,total number of nodes
Release-2.0.0,number of deleted nodes */
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy data spliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add matrix
2.0.0-alpha,TODO Auto-generated constructor stub
2.0.0-alpha,row 0 is a random uniform
2.0.0-alpha,row 1 is a random normal
2.0.0-alpha,row 2 is filled with 1.0
2.0.0-alpha,todo:implements
2.0.0-alpha,todo: implements
2.0.0-alpha,import jdk.nashorn.internal.runtime.regexp.joni.Config;
2.0.0-alpha,"paras[1] = ""abc"";"
2.0.0-alpha,"paras[2] = ""123"";"
2.0.0-alpha,Add standard Hadoop classes
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,set sgd LR algorithm parameters #feature #epoch
2.0.0-alpha,Set input data path
2.0.0-alpha,Set save model path
2.0.0-alpha,Set actionType train
2.0.0-alpha,QSLRRunner runner = new QSLRRunner();
2.0.0-alpha,runner.train(conf);
2.0.0-alpha,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
2.0.0-alpha,Dataset
2.0.0-alpha,Data format
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Tree number
2.0.0-alpha,Tree depth
2.0.0-alpha,Split number
2.0.0-alpha,Feature sample ratio
2.0.0-alpha,Ratio of validation
2.0.0-alpha,Learning rate
2.0.0-alpha,Set file system
2.0.0-alpha,Use local deploy mode and data format
2.0.0-alpha,Set data path
2.0.0-alpha,"Set angel resource, #worker, #task, #PS"
2.0.0-alpha,Set GBDT algorithm parameters
2.0.0-alpha,Dataset
2.0.0-alpha,Set file system
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Use local deploy mode
2.0.0-alpha,Set data path
2.0.0-alpha,"Set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set DeepFM algorithm parameters
2.0.0-alpha,Set model class
2.0.0-alpha,Dataset
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,Model type
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set file system
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Use local deploy mode and data format
2.0.0-alpha,Set data path
2.0.0-alpha,"Set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set LR algorithm parameters
2.0.0-alpha,Set model class
2.0.0-alpha,Dataset
2.0.0-alpha,Data format
2.0.0-alpha,Model type
2.0.0-alpha,Cluster center number
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Sample ratio per mini-batch
2.0.0-alpha,C
2.0.0-alpha,Set file system
2.0.0-alpha,Use local deploy mode and data format
2.0.0-alpha,Set data path
2.0.0-alpha,"Set angel resource, #worker, #task, #PS"
2.0.0-alpha,set Kmeans algorithm parameters #cluster #feature #epoch
2.0.0-alpha,Dataset
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,Model type
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set file system
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Use local deploy mode and data format
2.0.0-alpha,Set data path
2.0.0-alpha,"Set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set FM algorithm parameters
2.0.0-alpha,Set model class
2.0.0-alpha,Dataset
2.0.0-alpha,Set file system
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Use local deploy mode
2.0.0-alpha,Set data path
2.0.0-alpha,"Set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set WideAndDeep algorithm parameters
2.0.0-alpha,Set model class
2.0.0-alpha,Dataset
2.0.0-alpha,Data format
2.0.0-alpha,"Set LDA parameters #V, #K"
2.0.0-alpha,Set file system
2.0.0-alpha,Use local deploy mode and data format
2.0.0-alpha,Set data path
2.0.0-alpha,"Set angel resource, #worker, #task, #PS"
2.0.0-alpha,Set LDA algorithm parameters
2.0.0-alpha,Dataset
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,Model type
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set file system
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Use local deploy mode and data format
2.0.0-alpha,Set data path
2.0.0-alpha,"Set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set SVM algorithm parameters
2.0.0-alpha,Set model class
2.0.0-alpha,Dataset
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,Model type
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,Model is classification
2.0.0-alpha,Train batch number per epoch.
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set file system
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Use local deploy mode and data format
2.0.0-alpha,Set data path
2.0.0-alpha,"Set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set LR algorithm parameters
2.0.0-alpha,Set model class
2.0.0-alpha,Load model meta
2.0.0-alpha,Convert model
2.0.0-alpha,"Get input path, output path"
2.0.0-alpha,Init serde
2.0.0-alpha,"Parse need convert model names, if not set, we will convert all models in input directory"
2.0.0-alpha,Load model meta
2.0.0-alpha,Convert model
2.0.0-alpha,load hadoop configuration
2.0.0-alpha,"Get input path, output path"
2.0.0-alpha,Init serde
2.0.0-alpha,"Parse need convert model names, if not set, we will convert all models in input directory"
2.0.0-alpha,Load model meta
2.0.0-alpha,Check row type
2.0.0-alpha,Load model
2.0.0-alpha,Load model meta
2.0.0-alpha,Check row type
2.0.0-alpha,Load model
2.0.0-alpha,Load model meta
2.0.0-alpha,Check row type
2.0.0-alpha,Load model
2.0.0-alpha,Load model meta
2.0.0-alpha,Check row type
2.0.0-alpha,Load model
2.0.0-alpha,Load model meta
2.0.0-alpha,Check row type
2.0.0-alpha,Load model
2.0.0-alpha,Load model meta
2.0.0-alpha,Check row type
2.0.0-alpha,Load model
2.0.0-alpha,Load model meta
2.0.0-alpha,Check row type
2.0.0-alpha,Load model
2.0.0-alpha,Load model
2.0.0-alpha,load hadoop configuration
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add dense double matrix
2.0.0-alpha,add sparse double matrix
2.0.0-alpha,Start PS
2.0.0-alpha,Start to run application
2.0.0-alpha,mMatrix.setNnz(100000000);
2.0.0-alpha,mMatrix.setNnz(100000000);
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add matrix
2.0.0-alpha,worker register
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add matrix
2.0.0-alpha,add matrix
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add matrix
2.0.0-alpha,attempt 0
2.0.0-alpha,attempt1
2.0.0-alpha,attempt1
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add matrix
2.0.0-alpha,TODO Auto-generated constructor stub
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add matrix
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add matrix
2.0.0-alpha,Thread.sleep(5000);
2.0.0-alpha,"response = master.getJobReport(null, request);"
2.0.0-alpha,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
2.0.0-alpha,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
2.0.0-alpha,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add dense double matrix
2.0.0-alpha,add comp dense double matrix
2.0.0-alpha,add sparse double matrix
2.0.0-alpha,add component sparse double matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense long matrix
2.0.0-alpha,add comp dense long matrix
2.0.0-alpha,add sparse long matrix
2.0.0-alpha,add component sparse long matrix
2.0.0-alpha,add comp dense long double matrix
2.0.0-alpha,add sparse long-key double matrix
2.0.0-alpha,add component long-key sparse double matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add sparse long-key float matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add sparse long-key int matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,add sparse long-key long matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,Start PS
2.0.0-alpha,Start to run application
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add dense double matrix
2.0.0-alpha,add comp dense double matrix
2.0.0-alpha,add sparse double matrix
2.0.0-alpha,add component sparse double matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense long matrix
2.0.0-alpha,add comp dense long matrix
2.0.0-alpha,add sparse long matrix
2.0.0-alpha,add component sparse long matrix
2.0.0-alpha,add comp dense long double matrix
2.0.0-alpha,add sparse long-key double matrix
2.0.0-alpha,add component long-key sparse double matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add sparse long-key float matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add sparse long-key int matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,add sparse long-key long matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,Start PS
2.0.0-alpha,Start to run application
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add matrix
2.0.0-alpha,add matrix
2.0.0-alpha,"DenseIntVector deltaVec = new DenseIntVector(100000, delta);"
2.0.0-alpha,deltaVec.setMatrixId(matrixW1Id);
2.0.0-alpha,deltaVec.setRowId(0);
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add dense double matrix
2.0.0-alpha,add comp dense double matrix
2.0.0-alpha,add sparse double matrix
2.0.0-alpha,add component sparse double matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense long matrix
2.0.0-alpha,add comp dense long matrix
2.0.0-alpha,add sparse long matrix
2.0.0-alpha,add component sparse long matrix
2.0.0-alpha,add comp dense long double matrix
2.0.0-alpha,add sparse long-key double matrix
2.0.0-alpha,add component long-key sparse double matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add sparse long-key float matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add sparse long-key int matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,add sparse long-key long matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,Start PS
2.0.0-alpha,Start to run application
2.0.0-alpha,testDenseDoubleCompUDF();
2.0.0-alpha,testSparseDoubleCompUDF();
2.0.0-alpha,testDenseFloatCompUDF();
2.0.0-alpha,testSparseFloatCompUDF();
2.0.0-alpha,testDenseIntCompUDF();
2.0.0-alpha,testSparseIntCompUDF();
2.0.0-alpha,testDenseLongCompUDF();
2.0.0-alpha,testSparseLongCompUDF();
2.0.0-alpha,testSparseDoubleLongKeyCompUDF();
2.0.0-alpha,testSparseFloatLongKeyCompUDF();
2.0.0-alpha,testSparseIntLongKeyCompUDF();
2.0.0-alpha,testSparseLongLongKeyCompUDF();
2.0.0-alpha,client1.clock().get();
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,client1.clock().get();
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,TODO Auto-generated constructor stub
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add dense double matrix
2.0.0-alpha,add comp dense double matrix
2.0.0-alpha,add sparse double matrix
2.0.0-alpha,add component sparse double matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense long matrix
2.0.0-alpha,add comp dense long matrix
2.0.0-alpha,add sparse long matrix
2.0.0-alpha,add component sparse long matrix
2.0.0-alpha,add comp dense long double matrix
2.0.0-alpha,add sparse long-key double matrix
2.0.0-alpha,add component long-key sparse double matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add sparse long-key float matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add sparse long-key int matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,add sparse long-key long matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,Start PS
2.0.0-alpha,Start to run application
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
2.0.0-alpha,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
2.0.0-alpha,IntDoubleVector row = (IntDoubleVector) ((GetRowResult) client1.get(func)).getRow();
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
2.0.0-alpha,IntFloatVector row = (IntFloatVector) ((GetRowResult) client1.get(func)).getRow();
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"IndexGet func = new IndexGet(new IndexGetParam(matrixW1Id, 0, index));"
2.0.0-alpha,IntLongVector row = (IntLongVector) ((GetRowResult) client1.get(func)).getRow();
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add dense double matrix
2.0.0-alpha,add comp dense double matrix
2.0.0-alpha,add sparse double matrix
2.0.0-alpha,add component sparse double matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense long matrix
2.0.0-alpha,add comp dense long matrix
2.0.0-alpha,add sparse long matrix
2.0.0-alpha,add component sparse long matrix
2.0.0-alpha,add comp dense long double matrix
2.0.0-alpha,add sparse long-key double matrix
2.0.0-alpha,add component long-key sparse double matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add sparse long-key float matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add sparse long-key int matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,add sparse long-key long matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,Start PS
2.0.0-alpha,Start to run application
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,Start PS
2.0.0-alpha,Start to run application
2.0.0-alpha,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
2.0.0-alpha,@RunWith(MockitoJUnitRunner.class)
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
2.0.0-alpha,get a angel client
2.0.0-alpha,add matrix
2.0.0-alpha,psAgent.initAndStart();
2.0.0-alpha,test conf
2.0.0-alpha,test master location
2.0.0-alpha,test app id
2.0.0-alpha,test user
2.0.0-alpha,test ps agent attempt id
2.0.0-alpha,test connection
2.0.0-alpha,test master client
2.0.0-alpha,test ip
2.0.0-alpha,test loc
2.0.0-alpha,test master location
2.0.0-alpha,test ps location
2.0.0-alpha,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
2.0.0-alpha,test all ps ids
2.0.0-alpha,test all matrix ids
2.0.0-alpha,test all matrix names
2.0.0-alpha,test matrix attribute
2.0.0-alpha,test matrix meta
2.0.0-alpha,test ps location
2.0.0-alpha,test partitions
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add dense double matrix
2.0.0-alpha,add comp dense double matrix
2.0.0-alpha,add sparse double matrix
2.0.0-alpha,add component sparse double matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense float matrix
2.0.0-alpha,add comp dense float matrix
2.0.0-alpha,add sparse float matrix
2.0.0-alpha,add component sparse float matrix
2.0.0-alpha,add dense long matrix
2.0.0-alpha,add comp dense long matrix
2.0.0-alpha,add sparse long matrix
2.0.0-alpha,add component sparse long matrix
2.0.0-alpha,add comp dense long double matrix
2.0.0-alpha,add sparse long-key double matrix
2.0.0-alpha,add component long-key sparse double matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add sparse long-key float matrix
2.0.0-alpha,add component long-key sparse float matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add sparse long-key int matrix
2.0.0-alpha,add component long-key sparse int matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,add sparse long-key long matrix
2.0.0-alpha,add component long-key sparse long matrix
2.0.0-alpha,Start PS
2.0.0-alpha,Start to run application
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + mergedRow.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,Assert.assertTrue(index.length == row.size());
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"CompSparseDoubleVector deltaVec = new CompSparseDoubleVector(matrixW1Id, 0, feaNum, feaNum);"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,for (int i = 0; i < feaNum; i++) {
2.0.0-alpha,"deltaVec.set(i, i);"
2.0.0-alpha,}
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,"System.out.println(""id="" + id + "", value="" + row.get(id));"
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,System.out.println(content);
2.0.0-alpha,https://blog.csdn.net/cocoonyang/article/details/63068108
2.0.0-alpha,v1[i] = v1[i] + da * v2[i];
2.0.0-alpha,"dgemm(String transa, String transb,"
2.0.0-alpha,"int m, int n, int k,"
2.0.0-alpha,"double alpha,"
2.0.0-alpha,"double[] a, int lda,"
2.0.0-alpha,"double[] b, int ldb,"
2.0.0-alpha,"double beta,"
2.0.0-alpha,"double[] c, int ldc);"
2.0.0-alpha,C := alpha*op( A )*op( B ) + beta*C
2.0.0-alpha,v1[i] = v1[i] + da * v2[i];
2.0.0-alpha,y := alpha*A*x + beta*y
2.0.0-alpha,y := alpha*A*x + beta*y
2.0.0-alpha,y := alpha*A*x + beta*y
2.0.0-alpha,"dgemm(String transa, String transb,"
2.0.0-alpha,"int m, int n, int k,"
2.0.0-alpha,"double alpha,"
2.0.0-alpha,"double[] a, int lda,"
2.0.0-alpha,"double[] b, int ldb,"
2.0.0-alpha,"double beta,"
2.0.0-alpha,"double[] c, int ldc);"
2.0.0-alpha,C := alpha*op( A )*op( B ) + beta*C
2.0.0-alpha,set basic configuration keys
2.0.0-alpha,use local deploy mode and dummy dataspliter
2.0.0-alpha,get a angel client
2.0.0-alpha,add matrix
2.0.0-alpha,test worker getActiveTaskNum
2.0.0-alpha,test worker getTaskNum
2.0.0-alpha,test worker getTaskManager
2.0.0-alpha,test workerId
2.0.0-alpha,test workerAttemptId
2.0.0-alpha,tet worker initFinished
2.0.0-alpha,test worker getInitMinclock
2.0.0-alpha,test worker loacation
2.0.0-alpha,test AppId
2.0.0-alpha,test Conf
2.0.0-alpha,test UserName
2.0.0-alpha,master location
2.0.0-alpha,masterClient
2.0.0-alpha,test psAgent
2.0.0-alpha,test worker get dataBlockManager
2.0.0-alpha,workerGroup.getSplits();
2.0.0-alpha,application
2.0.0-alpha,lcation
2.0.0-alpha,workerGroup info
2.0.0-alpha,worker info
2.0.0-alpha,task
2.0.0-alpha,Matrix parameters
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Use local deploy mode and dummy data spliter
2.0.0-alpha,Create an Angel client
2.0.0-alpha,Add different types of matrix
2.0.0-alpha,"DenseIntMatrix expect = new DenseIntMatrix(diRow, diCol);"
2.0.0-alpha,using mock object
2.0.0-alpha,verification
2.0.0-alpha,Stubbing
2.0.0-alpha,Default does nothing.
2.0.0-alpha,The app injection is optional
2.0.0-alpha,"renderText(""hello world"");"
2.0.0-alpha,"user choose a workerGroupID from the workergroups page,"
2.0.0-alpha,now we should change the AngelApp params and render the workergroup page;
2.0.0-alpha,"static final String WORKER_ID = ""worker.id"";"
2.0.0-alpha,"div(""#logo"")."
2.0.0-alpha,"img(""/static/hadoop-st.png"")._()."
2.0.0-alpha,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
2.0.0-alpha,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
2.0.0-alpha,JQueryUI.jsnotice(html);
2.0.0-alpha,import org.apache.hadoop.conf.Configuration;
2.0.0-alpha,import java.lang.reflect.Field;
2.0.0-alpha,get block locations from file system
2.0.0-alpha,create a list of all block and their locations
2.0.0-alpha,"if the file is not splitable, just create the one block with"
2.0.0-alpha,full file length
2.0.0-alpha,each split can be a maximum of maxSize
2.0.0-alpha,if remainder is between max and 2*max - then
2.0.0-alpha,"instead of creating splits of size max, left-max we"
2.0.0-alpha,create splits of size left/2 and left/2. This is
2.0.0-alpha,a heuristic to avoid creating really really small
2.0.0-alpha,splits.
2.0.0-alpha,add this block to the block --> node locations map
2.0.0-alpha,"For blocks that do not have host/rack information,"
2.0.0-alpha,assign to default  rack.
2.0.0-alpha,add this block to the rack --> block map
2.0.0-alpha,Add this host to rackToNodes map
2.0.0-alpha,add this block to the node --> block map
2.0.0-alpha,"if the file system does not have any rack information, then"
2.0.0-alpha,use dummy rack location.
2.0.0-alpha,The topology paths have the host name included as the last
2.0.0-alpha,component. Strip it.
2.0.0-alpha,get tokens for all the required FileSystems..
2.0.0-alpha,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
2.0.0-alpha,job.getConfiguration());
2.0.0-alpha,Whether we need to recursive look into the directory structure
2.0.0-alpha,creates a MultiPathFilter with the hiddenFileFilter and the
2.0.0-alpha,user provided one (if any).
2.0.0-alpha,all the files in input set
2.0.0-alpha,it is allowed for maxSize to be 0. Disable smoothing load for such cases
2.0.0-alpha,process all nodes and create splits that are local to a node. Generate
2.0.0-alpha,"one split per node iteration, and walk over nodes multiple times to"
2.0.0-alpha,distribute the splits across nodes.
2.0.0-alpha,Skip the node if it has previously been marked as completed.
2.0.0-alpha,"for each block, copy it into validBlocks. Delete it from"
2.0.0-alpha,blockToNodes so that the same block does not appear in
2.0.0-alpha,two different splits.
2.0.0-alpha,Remove all blocks which may already have been assigned to other
2.0.0-alpha,splits.
2.0.0-alpha,"if the accumulated split size exceeds the maximum, then"
2.0.0-alpha,create this split.
2.0.0-alpha,create an input split and add it to the splits array
2.0.0-alpha,Remove entries from blocksInNode so that we don't walk these
2.0.0-alpha,again.
2.0.0-alpha,Done creating a single split for this node. Move on to the next
2.0.0-alpha,node so that splits are distributed across nodes.
2.0.0-alpha,This implies that the last few blocks (or all in case maxSize=0)
2.0.0-alpha,were not part of a split. The node is complete.
2.0.0-alpha,if there were any blocks left over and their combined size is
2.0.0-alpha,"larger than minSplitNode, then combine them into one split."
2.0.0-alpha,Otherwise add them back to the unprocessed pool. It is likely
2.0.0-alpha,that they will be combined with other blocks from the
2.0.0-alpha,same rack later on.
2.0.0-alpha,This condition also kicks in when max split size is not set. All
2.0.0-alpha,blocks on a node will be grouped together into a single split.
2.0.0-alpha,haven't created any split on this machine. so its ok to add a
2.0.0-alpha,smaller one for parallelism. Otherwise group it in the rack for
2.0.0-alpha,balanced size create an input split and add it to the splits
2.0.0-alpha,array
2.0.0-alpha,Remove entries from blocksInNode so that we don't walk this again.
2.0.0-alpha,The node is done. This was the last set of blocks for this node.
2.0.0-alpha,Put the unplaced blocks back into the pool for later rack-allocation.
2.0.0-alpha,Node is done. All blocks were fit into node-local splits.
2.0.0-alpha,Check if node-local assignments are complete.
2.0.0-alpha,All nodes have been walked over and marked as completed or all blocks
2.0.0-alpha,have been assigned. The rest should be handled via rackLock assignment.
2.0.0-alpha,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
2.0.0-alpha,"+ completedNodes.size() + "", size left: "" + totalLength);"
2.0.0-alpha,"if blocks in a rack are below the specified minimum size, then keep them"
2.0.0-alpha,"in 'overflow'. After the processing of all racks is complete, these"
2.0.0-alpha,overflow blocks will be combined into splits.
2.0.0-alpha,Process all racks over and over again until there is no more work to do.
2.0.0-alpha,Create one split for this rack before moving over to the next rack.
2.0.0-alpha,Come back to this rack after creating a single split for each of the
2.0.0-alpha,remaining racks.
2.0.0-alpha,"Process one rack location at a time, Combine all possible blocks that"
2.0.0-alpha,reside on this rack as one split. (constrained by minimum and maximum
2.0.0-alpha,split size).
2.0.0-alpha,iterate over all racks
2.0.0-alpha,"for each block, copy it into validBlocks. Delete it from"
2.0.0-alpha,blockToNodes so that the same block does not appear in
2.0.0-alpha,two different splits.
2.0.0-alpha,"if the accumulated split size exceeds the maximum, then"
2.0.0-alpha,create this split.
2.0.0-alpha,create an input split and add it to the splits array
2.0.0-alpha,"if we created a split, then just go to the next rack"
2.0.0-alpha,"if there is a minimum size specified, then create a single split"
2.0.0-alpha,"otherwise, store these blocks into overflow data structure"
2.0.0-alpha,There were a few blocks in this rack that
2.0.0-alpha,remained to be processed. Keep them in 'overflow' block list.
2.0.0-alpha,These will be combined later.
2.0.0-alpha,Process all overflow blocks
2.0.0-alpha,"This might cause an exiting rack location to be re-added,"
2.0.0-alpha,but it should be ok.
2.0.0-alpha,"if the accumulated split size exceeds the maximum, then"
2.0.0-alpha,create this split.
2.0.0-alpha,create an input split and add it to the splits array
2.0.0-alpha,"Process any remaining blocks, if any."
2.0.0-alpha,create an input split
2.0.0-alpha,add this split to the list that is returned
2.0.0-alpha,long num = totLength / maxSize;
2.0.0-alpha,all blocks for all the files in input set
2.0.0-alpha,mapping from a rack name to the list of blocks it has
2.0.0-alpha,mapping from a block to the nodes on which it has replicas
2.0.0-alpha,mapping from a node to the list of blocks that it contains
2.0.0-alpha,populate all the blocks for all files
2.0.0-alpha,stop all services
2.0.0-alpha,1.write application state to file so that the client can get the state of the application
2.0.0-alpha,if master exit
2.0.0-alpha,2.clear tmp and staging directory
2.0.0-alpha,waiting for client to get application state
2.0.0-alpha,stop the RPC server
2.0.0-alpha,"Security framework already loaded the tokens into current UGI, just use"
2.0.0-alpha,them
2.0.0-alpha,Now remove the AM->RM token so tasks don't have it
2.0.0-alpha,add a shutdown hook
2.0.0-alpha,init app state storage
2.0.0-alpha,init event dispacher
2.0.0-alpha,init location manager
2.0.0-alpha,init container allocator
2.0.0-alpha,init a rpc service
2.0.0-alpha,recover matrix meta if needed
2.0.0-alpha,recover ps attempt information if need
2.0.0-alpha,Init Client manager
2.0.0-alpha,Init PS Client manager
2.0.0-alpha,init parameter server manager
2.0.0-alpha,recover task information if needed
2.0.0-alpha,a dummy data spliter is just for test now
2.0.0-alpha,recover data splits information if needed
2.0.0-alpha,init worker manager and register worker manager event
2.0.0-alpha,register slow worker/ps checker
2.0.0-alpha,register app manager event and finish event
2.0.0-alpha,Init model saver & loader
2.0.0-alpha,start a web service if use yarn deploy mode
2.0.0-alpha,load from app state storage first if attempt index great than 1(the master is not the first
2.0.0-alpha,retry)
2.0.0-alpha,"if load failed, just build a new MatrixMetaManager"
2.0.0-alpha,load ps attempt index from app state storage first if attempt index great than 1(the master
2.0.0-alpha,is not the first retry)
2.0.0-alpha,load task information from app state storage first if attempt index great than 1(the master
2.0.0-alpha,is not the first retry)
2.0.0-alpha,"if load failed, just build a new AMTaskManager"
2.0.0-alpha,load data splits information from app state storage first if attempt index great than 1(the
2.0.0-alpha,master is not the first retry)
2.0.0-alpha,"if load failed, we need to recalculate the data splits"
2.0.0-alpha,Check Workers
2.0.0-alpha,Check PSS
2.0.0-alpha,Check Clients
2.0.0-alpha,Check PS Clients
2.0.0-alpha,parse parameter server counters
2.0.0-alpha,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
2.0.0-alpha,refresh last heartbeat timestamp
2.0.0-alpha,send a state update event to the specific PSAttempt
2.0.0-alpha,Check is there save request
2.0.0-alpha,Check is there load request
2.0.0-alpha,check matrix metadata inconsistencies between master and parameter server.
2.0.0-alpha,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
2.0.0-alpha,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
2.0.0-alpha,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
2.0.0-alpha,choose a unused port
2.0.0-alpha,start RPC server
2.0.0-alpha,remove this parameter server attempt from monitor set
2.0.0-alpha,remove this parameter server attempt from monitor set
2.0.0-alpha,"if worker attempt id is not in monitor set, we should shutdown it"
2.0.0-alpha,find workergroup in worker manager
2.0.0-alpha,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
2.0.0-alpha,"if this worker group run over, just return WORKERGROUP_EXITED"
2.0.0-alpha,"if this worker group is running now, return tasks, workers, data splits for it"
2.0.0-alpha,"if worker attempt id is not in monitor set, we should shutdown it"
2.0.0-alpha,"if worker attempt id is not in monitor set, we should shutdown it"
2.0.0-alpha,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
2.0.0-alpha,"in ANGEL_PS mode, task id may can not know advance"
2.0.0-alpha,update the clock for this matrix
2.0.0-alpha,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
2.0.0-alpha,"in ANGEL_PS mode, task id may can not know advance"
2.0.0-alpha,update task iteration
2.0.0-alpha,"LOG.info(""Epoch="" + epoch + "" Metrics="" + metrics);"
2.0.0-alpha,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
2.0.0-alpha,the number of splits equal to the number of tasks
2.0.0-alpha,split data
2.0.0-alpha,dispatch the splits to workergroups
2.0.0-alpha,split data
2.0.0-alpha,dispatch the splits to workergroups
2.0.0-alpha,Set split minsize and maxsize to expected split size. We need to get the total size of data
2.0.0-alpha,"first, then divided by expected split number"
2.0.0-alpha,get input format class from configuration and then instantiation a input format object
2.0.0-alpha,split data
2.0.0-alpha,Set split minsize and maxsize to expected split size. We need to get the total size of data
2.0.0-alpha,"first, then divided by expected split number"
2.0.0-alpha,get input format class from configuration and then instantiation a input format object
2.0.0-alpha,split data
2.0.0-alpha,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
2.0.0-alpha,need to fine tune the number of workergroup and task based on the actual split number
2.0.0-alpha,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
2.0.0-alpha,Record the location information for the splits in order to data localized schedule
2.0.0-alpha,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
2.0.0-alpha,need to fine tune the number of workergroup and task based on the actual split number
2.0.0-alpha,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
2.0.0-alpha,Record the location information for the splits in order to data localized schedule
2.0.0-alpha,write meta data to a temporary file
2.0.0-alpha,rename the temporary file to final file
2.0.0-alpha,"if the file exists, read from file and deserialize it"
2.0.0-alpha,write task meta
2.0.0-alpha,write ps meta
2.0.0-alpha,generate a temporary file
2.0.0-alpha,write task meta to the temporary file first
2.0.0-alpha,rename the temporary file to the final file
2.0.0-alpha,"if last final task file exist, remove it"
2.0.0-alpha,find task meta file which has max timestamp
2.0.0-alpha,"if the file does not exist, just return null"
2.0.0-alpha,read task meta from file and deserialize it
2.0.0-alpha,generate a temporary file
2.0.0-alpha,write ps meta to the temporary file first.
2.0.0-alpha,rename the temporary file to the final file
2.0.0-alpha,"if the old final file exist, just remove it"
2.0.0-alpha,find ps meta file
2.0.0-alpha,"if ps meta file does not exist, just return null"
2.0.0-alpha,read ps meta from file and deserialize it
2.0.0-alpha,"saveContext.setTmpSavePath(HdfsUtil.generateTmpDirectory(context.getConf(),"
2.0.0-alpha,"context.getApplicationId().toString(), new Path(saveContext.getSavePath())).toString());"
2.0.0-alpha,Filter old epoch trigger first
2.0.0-alpha,Split the user request to sub-requests to pss
2.0.0-alpha,Init matrix files meta
2.0.0-alpha,Move output files
2.0.0-alpha,Write the meta file
2.0.0-alpha,Split the user request to sub-requests to pss
2.0.0-alpha,check whether psagent heartbeat timeout
2.0.0-alpha,Set up the launch command
2.0.0-alpha,Duplicate the ByteBuffers for access by multiple containers.
2.0.0-alpha,Construct the actual Container
2.0.0-alpha,Application resources
2.0.0-alpha,Application environment
2.0.0-alpha,Service data
2.0.0-alpha,Tokens
2.0.0-alpha,Set up JobConf to be localized properly on the remote NM.
2.0.0-alpha,Setup DistributedCache
2.0.0-alpha,Setup up task credentials buffer
2.0.0-alpha,LocalStorageToken is needed irrespective of whether security is enabled
2.0.0-alpha,or not.
2.0.0-alpha,"TokenCache.setJobToken(jobToken, taskCredentials);"
2.0.0-alpha,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
2.0.0-alpha,Construct the actual Container
2.0.0-alpha,The null fields are per-container and will be constructed for each
2.0.0-alpha,container separately.
2.0.0-alpha,Set up the launch command
2.0.0-alpha,Duplicate the ByteBuffers for access by multiple containers.
2.0.0-alpha,Construct the actual Container
2.0.0-alpha,"a * in the classpath will only find a .jar, so we need to filter out"
2.0.0-alpha,all .jars and add everything else
2.0.0-alpha,Propagate the system classpath when using the mini cluster
2.0.0-alpha,Add standard Hadoop classes
2.0.0-alpha,Add mr
2.0.0-alpha,Cache archives
2.0.0-alpha,Cache files
2.0.0-alpha,Sanity check
2.0.0-alpha,Add URI fragment or just the filename
2.0.0-alpha,Add the env variables passed by the user
2.0.0-alpha,Set logging level in the environment.
2.0.0-alpha,Setup the log4j prop
2.0.0-alpha,Add main class and its arguments
2.0.0-alpha,Finally add the jvmID
2.0.0-alpha,vargs.add(String.valueOf(jvmID.getId()));
2.0.0-alpha,Final commmand
2.0.0-alpha,Add the env variables passed by the user
2.0.0-alpha,Set logging level in the environment.
2.0.0-alpha,Setup the log4j prop
2.0.0-alpha,Add main class and its arguments
2.0.0-alpha,Final commmand
2.0.0-alpha,"if amTask is not null, we should clone task state from it"
2.0.0-alpha,"if all parameter server complete commit, master can commit now"
2.0.0-alpha,check whether parameter server heartbeat timeout
2.0.0-alpha,Transitions from the NEW state.
2.0.0-alpha,Transitions from the UNASSIGNED state.
2.0.0-alpha,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
2.0.0-alpha,Transitions from the ASSIGNED state.
2.0.0-alpha,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
2.0.0-alpha,PA_CONTAINER_LAUNCHED event
2.0.0-alpha,Transitions from the PSAttemptStateInternal.RUNNING state.
2.0.0-alpha,Transitions from the PSAttemptStateInternal.COMMITTING state
2.0.0-alpha,Transitions from the PSAttemptStateInternal.KILLED state
2.0.0-alpha,Transitions from the PSAttemptStateInternal.FAILED state
2.0.0-alpha,create the topology tables
2.0.0-alpha,reqeuest resource:send a resource request to the resource allocator
2.0.0-alpha,"Once the resource is applied, build and send the launch request to the container launcher"
2.0.0-alpha,deallocator the resource of the ps attempt:send a resource deallocator request to the
2.0.0-alpha,resource allocator
2.0.0-alpha,set the launch time
2.0.0-alpha,add the ps attempt to the heartbeat timeout monitoring list
2.0.0-alpha,parse ps attempt location and put it to location manager
2.0.0-alpha,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
2.0.0-alpha,or failed
2.0.0-alpha,remove ps attempt id from heartbeat timeout monitor list
2.0.0-alpha,release container:send a release request to container launcher
2.0.0-alpha,set the finish time only if launch time is set
2.0.0-alpha,private long scheduledTime;
2.0.0-alpha,Transitions from the NEW state.
2.0.0-alpha,Transitions from the SCHEDULED state.
2.0.0-alpha,Transitions from the RUNNING state.
2.0.0-alpha,"another attempt launched,"
2.0.0-alpha,Transitions from the SUCCEEDED state
2.0.0-alpha,Transitions from the KILLED state
2.0.0-alpha,Transitions from the FAILED state
2.0.0-alpha,add diagnostic
2.0.0-alpha,check whether the number of failed attempts is less than the maximum number of allowed
2.0.0-alpha,Refresh ps location & matrix meta
2.0.0-alpha,start a new attempt for this ps
2.0.0-alpha,notify ps manager
2.0.0-alpha,"getContext().getLocationManager().setPsLocation(id, null);"
2.0.0-alpha,add diagnostic
2.0.0-alpha,check whether the number of failed attempts is less than the maximum number of allowed
2.0.0-alpha,start a new attempt for this ps
2.0.0-alpha,notify ps manager
2.0.0-alpha,notify the event handler of state change
2.0.0-alpha,"If new state is not RUNNING and EXECUTE_SUCCESSED, add it to state timeout monitor"
2.0.0-alpha,"if forcedState is set, just return"
2.0.0-alpha,else get state from state machine
2.0.0-alpha,add this worker group to the success set
2.0.0-alpha,check if all worker group run over
2.0.0-alpha,add this worker group to the failed set
2.0.0-alpha,check if too many worker groups are failed or killed
2.0.0-alpha,notify a run failed event
2.0.0-alpha,add this worker group to the failed set
2.0.0-alpha,check if too many worker groups are failed or killed
2.0.0-alpha,notify a run failed event
2.0.0-alpha,calculate the actual number of worker groups and the total number of tasks based on the number of data split
2.0.0-alpha,"init all tasks , workers and worker groups and put them to the corresponding maps"
2.0.0-alpha,just return the total task number now
2.0.0-alpha,TODO
2.0.0-alpha,check whether worker heartbeat timeout
2.0.0-alpha,"if workerAttempt is not null, we should clone task state from it"
2.0.0-alpha,from NEW state
2.0.0-alpha,from SCHEDULED state
2.0.0-alpha,get data splits location for data locality
2.0.0-alpha,reqeuest resource:send a resource request to the resource allocator
2.0.0-alpha,"once the resource is applied, build and send the launch request to the container launcher"
2.0.0-alpha,notify failed message to the worker
2.0.0-alpha,notify killed message to the worker
2.0.0-alpha,release the allocated container
2.0.0-alpha,notify failed message to the worker
2.0.0-alpha,remove the worker attempt from heartbeat timeout listen list
2.0.0-alpha,release the allocated container
2.0.0-alpha,notify killed message to the worker
2.0.0-alpha,remove the worker attempt from heartbeat timeout listen list
2.0.0-alpha,clean the container
2.0.0-alpha,notify failed message to the worker
2.0.0-alpha,remove the worker attempt from heartbeat timeout listen list
2.0.0-alpha,record the finish time
2.0.0-alpha,clean the container
2.0.0-alpha,notify killed message to the worker
2.0.0-alpha,remove the worker attempt from heartbeat timeout listening list
2.0.0-alpha,record the finish time
2.0.0-alpha,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
2.0.0-alpha,set worker attempt location
2.0.0-alpha,notify the register message to the worker
2.0.0-alpha,record the launch time
2.0.0-alpha,update worker attempt metrics
2.0.0-alpha,update tasks metrics
2.0.0-alpha,clean the container
2.0.0-alpha,notify the worker attempt run successfully message to the worker
2.0.0-alpha,record the finish time
2.0.0-alpha,init a worker attempt for the worker
2.0.0-alpha,schedule the worker attempt
2.0.0-alpha,add diagnostic
2.0.0-alpha,check whether the number of failed attempts is less than the maximum number of allowed
2.0.0-alpha,init and start a new attempt for this ps
2.0.0-alpha,notify worker manager
2.0.0-alpha,add diagnostic
2.0.0-alpha,check whether the number of failed attempts is less than the maximum number of allowed
2.0.0-alpha,init and start a new attempt for this ps
2.0.0-alpha,notify worker manager
2.0.0-alpha,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
2.0.0-alpha,register to Yarn RM
2.0.0-alpha,send heartbeat to Yarn RM every rmPollInterval milliseconds
2.0.0-alpha,"catch YarnRuntimeException, we should exit and need not retry"
2.0.0-alpha,build heartbeat request
2.0.0-alpha,send heartbeat request to rm
2.0.0-alpha,"This can happen if the RM has been restarted. If it is in that state,"
2.0.0-alpha,this application must clean itself up.
2.0.0-alpha,Setting NMTokens
2.0.0-alpha,assgin containers
2.0.0-alpha,"if some container is not assigned, release them"
2.0.0-alpha,handle finish containers
2.0.0-alpha,dispatch container exit message to corresponding components
2.0.0-alpha,killed by framework
2.0.0-alpha,killed by framework
2.0.0-alpha,get application finish state
2.0.0-alpha,build application diagnostics
2.0.0-alpha,TODO:add a job history for angel
2.0.0-alpha,build unregister request
2.0.0-alpha,send unregister request to rm
2.0.0-alpha,Note this down for next interaction with ResourceManager
2.0.0-alpha,based on blacklisting comments above we can end up decrementing more
2.0.0-alpha,than requested. so guard for that.
2.0.0-alpha,send the updated resource request to RM
2.0.0-alpha,send 0 container count requests also to cancel previous requests
2.0.0-alpha,Update resource requests
2.0.0-alpha,try to assign to all nodes first to match node local
2.0.0-alpha,try to match all rack local
2.0.0-alpha,assign remaining
2.0.0-alpha,Update resource requests
2.0.0-alpha,send the container-assigned event to task attempt
2.0.0-alpha,build the start container request use launch context
2.0.0-alpha,send the start request to Yarn nm
2.0.0-alpha,send the message that the container starts successfully to the corresponding component
2.0.0-alpha,"after launching, send launched event to task attempt to move"
2.0.0-alpha,it from ASSIGNED to RUNNING state
2.0.0-alpha,send the message that the container starts failed to the corresponding component
2.0.0-alpha,kill the remote container if already launched
2.0.0-alpha,start a thread pool to startup the container
2.0.0-alpha,See if we need up the pool size only if haven't reached the
2.0.0-alpha,maximum limit yet.
2.0.0-alpha,nodes where containers will run at *this* point of time. This is
2.0.0-alpha,*not* the cluster size and doesn't need to be.
2.0.0-alpha,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
2.0.0-alpha,later is just a buffer so we are not always increasing the
2.0.0-alpha,pool-size
2.0.0-alpha,the events from the queue are handled in parallel
2.0.0-alpha,using a thread pool
2.0.0-alpha,return if already stopped
2.0.0-alpha,shutdown any containers that might be left running
2.0.0-alpha,Add one sync matrix
2.0.0-alpha,addSyncMatrix();
2.0.0-alpha,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
2.0.0-alpha,"for(Entry<ParameterServerId, Map<Integer, MatrixMeta>> psEntry : matrixPartitionsOnPS.entrySet()) {"
2.0.0-alpha,"LOG.info(""ps id = "" + psEntry.getKey());"
2.0.0-alpha,"Map<Integer, MatrixMeta> matrixIdToMetaMap = psEntry.getValue();"
2.0.0-alpha,"for(Entry<Integer, MatrixMeta> metaEntry : matrixIdToMetaMap.entrySet()) {"
2.0.0-alpha,"LOG.info(""matrix id = "" + metaEntry.getKey());"
2.0.0-alpha,"LOG.info(""matrix partitons number = "" + metaEntry.getValue().getPartitionMetas().size());"
2.0.0-alpha,}
2.0.0-alpha,}
2.0.0-alpha,get matrix ids in the parameter server report
2.0.0-alpha,get the matrices parameter server need to create and delete
2.0.0-alpha,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
2.0.0-alpha,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
2.0.0-alpha,Init control connection manager
2.0.0-alpha,Get ps locations from master and put them to the location cache.
2.0.0-alpha,Build and initialize rpc client to master
2.0.0-alpha,Get psagent id
2.0.0-alpha,Build PS control rpc client manager
2.0.0-alpha,Build local location
2.0.0-alpha,Initialize matrix meta information
2.0.0-alpha,Start all services
2.0.0-alpha,Stop all modules
2.0.0-alpha,Stop all modules
2.0.0-alpha,clock first
2.0.0-alpha,wait
2.0.0-alpha,Update generic resource counters
2.0.0-alpha,Updating resources specified in ResourceCalculatorProcessTree
2.0.0-alpha,Remove the CPU time consumed previously by JVM reuse
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,/ Plus a vector/matrix to the matrix stored in pss
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,/ Update a vector/matrix to the matrix stored in pss
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,/ Get values from pss use row/column indices
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"/ PSF get/update, use can implement their own psf"
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,/ Get a row or a batch of rows
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,TODO:
2.0.0-alpha,Generate a flush request and put it to request queue
2.0.0-alpha,Generate a clock request and put it to request queue
2.0.0-alpha,Generate a merge request and put it to request queue
2.0.0-alpha,Generate a merge request and put it to request queue
2.0.0-alpha,"If the matrix op log cache does not exist for the matrix, create a new one for the"
2.0.0-alpha,matrix
2.0.0-alpha,and add it to cache maps
2.0.0-alpha,Add the message to the tree map
2.0.0-alpha,"If there are flush / clock requests blocked, we need to put this merge request into"
2.0.0-alpha,the waiting queue
2.0.0-alpha,Launch a merge worker to merge the update to matrix op log cache
2.0.0-alpha,Remove the message from the tree map
2.0.0-alpha,Wake up blocked flush/clock request
2.0.0-alpha,Add flush/clock request to listener list to waiting for all the existing
2.0.0-alpha,updates are merged
2.0.0-alpha,Wake up blocked flush/clock request
2.0.0-alpha,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
2.0.0-alpha,blocked.
2.0.0-alpha,Get next merge message sequence id
2.0.0-alpha,Wake up listeners(flush/clock requests) that have little sequence id than current merge
2.0.0-alpha,position
2.0.0-alpha,Wake up blocked merge requests
2.0.0-alpha,Get minimal sequence id from listeners
2.0.0-alpha,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
2.0.0-alpha,should flush updates to local matrix storage
2.0.0-alpha,unused now
2.0.0-alpha,TODO:
2.0.0-alpha,Doing average or not
2.0.0-alpha,Filter un-important update
2.0.0-alpha,Split this row according the matrix partitions
2.0.0-alpha,Set split context
2.0.0-alpha,Remove the row from matrix
2.0.0-alpha,TODO
2.0.0-alpha,TODO: write map default value
2.0.0-alpha,TODO:
2.0.0-alpha,TODO:
2.0.0-alpha,TODO:
2.0.0-alpha,"For each partition, we generate a update split."
2.0.0-alpha,"Although the split is empty for partitions those without any update data,"
2.0.0-alpha,we still need to generate a update split to update the clock info on ps.
2.0.0-alpha,"For each partition, we generate a update split."
2.0.0-alpha,"Although the split is empty for partitions those without any update data,"
2.0.0-alpha,we still need to generate a update split to update the clock info on ps.
2.0.0-alpha,"For each partition, we generate a update split."
2.0.0-alpha,"Although the split is empty for partitions those without any update data,"
2.0.0-alpha,we still need to generate a update split to update the clock info on ps.
2.0.0-alpha,"For each partition, we generate a update split."
2.0.0-alpha,"Although the split is empty for partitions those without any update data,"
2.0.0-alpha,we still need to generate a update split to update the clock info on ps.
2.0.0-alpha,"For each partition, we generate a update split."
2.0.0-alpha,"Although the split is empty for partitions those without any update data,"
2.0.0-alpha,we still need to generate a update split to update the clock info on ps.
2.0.0-alpha,"For each partition, we generate a update split."
2.0.0-alpha,"Although the split is empty for partitions those without any update data,"
2.0.0-alpha,we still need to generate a update split to update the clock info on ps.
2.0.0-alpha,"LOG.error(""channel "" + ctx.channel() + "" inactive"");"
2.0.0-alpha,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
2.0.0-alpha,"LOG.error(""put response message queue failed "", e);"
2.0.0-alpha,Use Epoll for linux
2.0.0-alpha,Update location table
2.0.0-alpha,Remove the server from failed list
2.0.0-alpha,Notify refresh success message to request dispatcher
2.0.0-alpha,Check PS exist or not
2.0.0-alpha,Check heartbeat timeout
2.0.0-alpha,Check PS restart or not
2.0.0-alpha,private final HashSet<ParameterServerId> refreshingServerSet;
2.0.0-alpha,Add it to failed rpc list
2.0.0-alpha,Add the server to gray server list
2.0.0-alpha,Add it to failed rpc list
2.0.0-alpha,Add the server to gray server list
2.0.0-alpha,Move from gray server list to failed server list
2.0.0-alpha,Handle the RPCS to this server
2.0.0-alpha,Submit the schedulable failed get RPCS
2.0.0-alpha,Submit new get RPCS
2.0.0-alpha,"if submit task in getQueue failed, we should make up the last chosen get queue index"
2.0.0-alpha,"If the queue is empty, just return 0"
2.0.0-alpha,"If request is not over limit, just submit it"
2.0.0-alpha,Submit the schedulable failed get RPCS
2.0.0-alpha,Submit new put RPCS
2.0.0-alpha,"if submit task in getQueue failed, we should make up the last chosen get queue index"
2.0.0-alpha,"LOG.info(""choose put server "" + psIds[index]);"
2.0.0-alpha,Check all pending RPCS
2.0.0-alpha,Check get channel context
2.0.0-alpha,Check all failed PUT RPCS and put it to schedulable list for re-schedule
2.0.0-alpha,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
2.0.0-alpha,Check all failed PUT RPCS and put it to schedulable list for re-schedule
2.0.0-alpha,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
2.0.0-alpha,channelManager.printPools();
2.0.0-alpha,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
2.0.0-alpha,if(ts - entry.getValue() > requestTimeOut * 2)  {
2.0.0-alpha,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
2.0.0-alpha,"+ "" milliseconds, close all channels to it"");"
2.0.0-alpha,closeChannels(entry.getKey());
2.0.0-alpha,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
2.0.0-alpha,}
2.0.0-alpha,}
2.0.0-alpha,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
2.0.0-alpha,Remove all pending RPCS
2.0.0-alpha,Close all channel to this PS
2.0.0-alpha,private Channel getChannel(Location loc) throws Exception {
2.0.0-alpha,"return channelManager.getOrCreateChannel(new Location(loc.getIp(), loc.getPort() + 1));"
2.0.0-alpha,}
2.0.0-alpha,private GenericObjectPool<Channel> getChannelPool(Location loc) throws InterruptedException {
2.0.0-alpha,"return channelManager.getOrCreateChannelPool(new Location(loc.getIp(), loc.getPort() + 1), PSAgentContext"
2.0.0-alpha,.get()
2.0.0-alpha,.getConf()
2.0.0-alpha,".getInt(AngelConf.ANGEL_WORKER_TASK_NUMBER,"
2.0.0-alpha,AngelConf.DEFAULT_ANGEL_WORKER_TASK_NUMBER));
2.0.0-alpha,}
2.0.0-alpha,"LOG.error(""send request "" + request + "" is interrupted"");"
2.0.0-alpha,"LOG.error(""send request "" + request + "" failed, "", e);"
2.0.0-alpha,Get server id and location for this request
2.0.0-alpha,"If location is null, means that the server is not ready"
2.0.0-alpha,Get the channel for the location
2.0.0-alpha,Check if need get token first
2.0.0-alpha,Serialize the request
2.0.0-alpha,Send the request
2.0.0-alpha,get a channel to server from pool
2.0.0-alpha,"if channel is not valid, it means maybe the connections to the server are closed"
2.0.0-alpha,request.getContext().setChannelPool(pool);
2.0.0-alpha,Allocate the bytebuf and serialize the request
2.0.0-alpha,find the partition request context from cache
2.0.0-alpha,"updateMatrixCache(request.getPartKey(), response.getPartition());"
2.0.0-alpha,"updateMatrixCache(request.getPartKey(), response.getRowsSplit());"
2.0.0-alpha,TODO
2.0.0-alpha,ServerRow rowSplit = PSAgentContext.get().getMatricesCache()
2.0.0-alpha,".getRowSplit(request.getPartKey().getMatrixId(), request.getPartKey(),"
2.0.0-alpha,request.getRowIndex());
2.0.0-alpha,response.setRowSplit(rowSplit);
2.0.0-alpha,"updateMatrixCache(request.getPartKey(), response.getRowSplit());"
2.0.0-alpha,"LOG.info(""user request id "" + request.getUserRequestId());"
2.0.0-alpha,"LOG.info(""user request id "" + request.getUserRequestId());"
2.0.0-alpha,TODO
2.0.0-alpha,"private void updateMatrixCache(PartitionKey partKey, ServerPartition partition) {"
2.0.0-alpha,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, partition);"
2.0.0-alpha,}
2.0.0-alpha,"private void updateMatrixCache(PartitionKey partKey, ServerRow rowSplit) {"
2.0.0-alpha,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowSplit);"
2.0.0-alpha,}
2.0.0-alpha,"private void updateMatrixCache(PartitionKey partKey, List<ServerRow> rowsSplit) {"
2.0.0-alpha,"PSAgentContext.get().getMatricesCache().update(partKey.getMatrixId(), partKey, rowsSplit);"
2.0.0-alpha,}
2.0.0-alpha,Get partitions for this row
2.0.0-alpha,Distinct get row requests
2.0.0-alpha,Need get from ps or storage/cache
2.0.0-alpha,"Switch to new request id, send a new request"
2.0.0-alpha,First get this row from matrix storage
2.0.0-alpha,MatrixStorage matrixStorage =
2.0.0-alpha,PSAgentContext.get().getMatrixStorageManager().getMatrixStoage(matrixId);
2.0.0-alpha,TVector row = matrixStorage.getRow(rowIndex);
2.0.0-alpha,if (row != null && row.getClock() >= clock) {
2.0.0-alpha,result.set(row);
2.0.0-alpha,return row;
2.0.0-alpha,}
2.0.0-alpha,Get row splits of this row from the matrix cache first
2.0.0-alpha,responseCache.addSubResponse(rowSplit);
2.0.0-alpha,"If the row split does not exist in cache, get it from parameter server"
2.0.0-alpha,Wait the final result
2.0.0-alpha,Put it to the matrix cache
2.0.0-alpha,"matrixStorage.addRow(rowIndex, row);"
2.0.0-alpha,Just wait result
2.0.0-alpha,Split the param use matrix partitions
2.0.0-alpha,Send request to PSS
2.0.0-alpha,Split the matrix oplog according to the matrix partitions
2.0.0-alpha,"If need update clock, we should send requests to all partitions"
2.0.0-alpha,Send request to PSS
2.0.0-alpha,Filter the rowIds which are fetching now
2.0.0-alpha,Send the rowIndex to rpc dispatcher and return immediately
2.0.0-alpha,"LOG.info(""get row split use time="" + (System.currentTimeMillis() - startTs));"
2.0.0-alpha,"LOG.info(""start to request "" + requestId);"
2.0.0-alpha,"LOG.info(""start to request "" + requestId);"
2.0.0-alpha,Split param use matrix partitons
2.0.0-alpha,"If all sub-results are received, just remove request and result cache"
2.0.0-alpha,"LOG.info(""request = "" + request + "", cache = "" + cache);"
2.0.0-alpha,"LOG.info(""start to merge "" + cache + "" for request "" + request);"
2.0.0-alpha,"LOG.info(""psf get merge use time = "" + (System.currentTimeMillis() - startTs));"
2.0.0-alpha,Split this row according the matrix partitions
2.0.0-alpha,Set split context
2.0.0-alpha,Split this row according the matrix partitions
2.0.0-alpha,Set split context
2.0.0-alpha,long startTs = System.currentTimeMillis();
2.0.0-alpha,"LOG.error(""combine use time = "" + (System.currentTimeMillis() - startTs));"
2.0.0-alpha,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
2.0.0-alpha,Generate dispatch items and add them to the corresponding queues
2.0.0-alpha,Filter the rowIds which are fetching now
2.0.0-alpha,Sort the parts by partitionId
2.0.0-alpha,Sort partition keys use start column index
2.0.0-alpha,"For each partition, we generate a update split."
2.0.0-alpha,"Although the split is empty for partitions those without any update data,"
2.0.0-alpha,we still need to generate a update split to update the clock info on ps.
2.0.0-alpha,Sort the parts by partitionId
2.0.0-alpha,Sort partition keys use start column index
2.0.0-alpha,"For each partition, we generate a update split."
2.0.0-alpha,"Although the split is empty for partitions those without any update data,"
2.0.0-alpha,we still need to generate a update split to update the clock info on ps.
2.0.0-alpha,Put the row split to the cache(row index to row splits map)
2.0.0-alpha,"If all splits of the row are received, means this row can be merged"
2.0.0-alpha,TODO
2.0.0-alpha,TODO
2.0.0-alpha,/////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,TODO
2.0.0-alpha,TODO
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,Now we just support pipelined row splits merging for dense type row
2.0.0-alpha,Pre-fetching is disable default
2.0.0-alpha,matrix id to clock map
2.0.0-alpha,"task index, it must be unique for whole application"
2.0.0-alpha,Deserialize data splits meta
2.0.0-alpha,Get workers
2.0.0-alpha,Send request to every ps
2.0.0-alpha,Wait the responses
2.0.0-alpha,Update clock cache
2.0.0-alpha,if(syncNum % 1024 == 0) {
2.0.0-alpha,}
2.0.0-alpha,"Use simple flow, do not use any cache"
2.0.0-alpha,Get row from cache.
2.0.0-alpha,"if row clock is satisfy ssp staleness limit, just return."
2.0.0-alpha,Get row from ps.
2.0.0-alpha,Wait until the clock value of this row is greater than or equal to the value
2.0.0-alpha,"For ASYNC mode, just get from pss."
2.0.0-alpha,"For BSP/SSP, get rows from storage/cache first"
2.0.0-alpha,Get from ps.
2.0.0-alpha,Wait until the clock value of this row is greater than or equal to the value
2.0.0-alpha,"For ASYNC, just get rows from pss."
2.0.0-alpha,no more retries.
2.0.0-alpha,calculate sleep time and return.
2.0.0-alpha,parse the i-th sleep-time
2.0.0-alpha,parse the i-th number-of-retries
2.0.0-alpha,calculateSleepTime may overflow.
2.0.0-alpha,"A few common retry policies, with no delays."
2.0.0-alpha,close is a local operation and should finish within milliseconds; timeout just to be safe
2.0.0-alpha,response will be null for one way messages.
2.0.0-alpha,maxFrameLength = 2G
2.0.0-alpha,lengthFieldOffset = 0
2.0.0-alpha,lengthFieldLength = 8
2.0.0-alpha,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
2.0.0-alpha,"initialBytesToStrip = 8, i.e. strip out the length field itself"
2.0.0-alpha,indicates whether this connection's life cycle is managed
2.0.0-alpha,See if we already have a connection (common case)
2.0.0-alpha,create a unique lock for this RS + protocol (if necessary)
2.0.0-alpha,get the RS lock
2.0.0-alpha,do one more lookup in case we were stalled above
2.0.0-alpha,Only create isa when we need to.
2.0.0-alpha,definitely a cache miss. establish an RPC for
2.0.0-alpha,this RS
2.0.0-alpha,Throw what the RemoteException was carrying.
2.0.0-alpha,check
2.0.0-alpha,every
2.0.0-alpha,minutes
2.0.0-alpha,TODO
2.0.0-alpha,创建failoverHandler
2.0.0-alpha,"The number of times this invocation handler has ever been failed over,"
2.0.0-alpha,before this method invocation attempt. Used to prevent concurrent
2.0.0-alpha,failed method invocations from triggering multiple failover attempts.
2.0.0-alpha,Make sure that concurrent failed method invocations
2.0.0-alpha,only cause a
2.0.0-alpha,single actual fail over.
2.0.0-alpha,RpcController + Message in the method args
2.0.0-alpha,(generated code from RPC bits in .proto files have
2.0.0-alpha,RpcController)
2.0.0-alpha,"LOG.info(""method "" + method.getName() + ""construct request time = """
2.0.0-alpha,+ (System.currentTimeMillis() - beforeConstructTs));
2.0.0-alpha,get an instance of the method arg type
2.0.0-alpha,RpcController + Message in the method args
2.0.0-alpha,(generated code from RPC bits in .proto files have
2.0.0-alpha,RpcController)
2.0.0-alpha,Message (hand written code usually has only a single
2.0.0-alpha,argument)
2.0.0-alpha,log any RPC responses that are slower than the configured
2.0.0-alpha,warn
2.0.0-alpha,response time or larger than configured warning size
2.0.0-alpha,"when tagging, we let TooLarge trump TooSmall to keep"
2.0.0-alpha,output simple
2.0.0-alpha,note that large responses will often also be slow.
2.0.0-alpha,provides a count of log-reported slow responses
2.0.0-alpha,RpcController + Message in the method args
2.0.0-alpha,(generated code from RPC bits in .proto files have
2.0.0-alpha,RpcController)
2.0.0-alpha,unexpected
2.0.0-alpha,"in the protobuf methods, args[1] is the only significant argument"
2.0.0-alpha,for JSON encoding
2.0.0-alpha,base information that is reported regardless of type of call
2.0.0-alpha,Disable Nagle's Algorithm since we don't want packets to wait
2.0.0-alpha,Configure the event pipeline factory.
2.0.0-alpha,Make a new connection.
2.0.0-alpha,Remove all pending requests (will be canceled after relinquishing
2.0.0-alpha,write lock).
2.0.0-alpha,Cancel any pending requests by sending errors to the callbacks:
2.0.0-alpha,Close the channel:
2.0.0-alpha,Close the connection:
2.0.0-alpha,Shut down all thread pools to exit.
2.0.0-alpha,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
2.0.0-alpha,See NettyServer.prepareResponse for where we write out the response.
2.0.0-alpha,"It writes the call.id (int), a boolean signifying any error (and if"
2.0.0-alpha,"so the exception name/trace), and the response bytes"
2.0.0-alpha,Read the call id.
2.0.0-alpha,"When the stream is closed, protobuf doesn't raise an EOFException,"
2.0.0-alpha,"instead, it returns a null message object."
2.0.0-alpha,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
2.0.0-alpha,System.currentTimeMillis());
2.0.0-alpha,"It would be good widen this to just Throwable, but IOException is what we"
2.0.0-alpha,allow now
2.0.0-alpha,not implemented
2.0.0-alpha,not implemented
2.0.0-alpha,"track what RpcEngine is used by a proxy class, for stopProxy()"
2.0.0-alpha,cache of RpcEngines by protocol
2.0.0-alpha,return the RpcEngine configured to handle a protocol
2.0.0-alpha,We only handle the ConnectException.
2.0.0-alpha,This is the exception we can't handle.
2.0.0-alpha,check if timed out
2.0.0-alpha,wait for retry
2.0.0-alpha,IGNORE
2.0.0-alpha,return the RpcEngine that handles a proxy object
2.0.0-alpha,The default implementation works synchronously
2.0.0-alpha,punt: allocate a new buffer & copy into it
2.0.0-alpha,Parse cmd parameters
2.0.0-alpha,load hadoop configuration
2.0.0-alpha,load angel system configuration
2.0.0-alpha,load user configuration:
2.0.0-alpha,load user config file
2.0.0-alpha,load command line parameters
2.0.0-alpha,load user job resource files
2.0.0-alpha,load ml conf file for graph based algorithm
2.0.0-alpha,load user job jar if it exist
2.0.0-alpha,Expand the environment variable
2.0.0-alpha,Add default fs(local fs) for lib jars.
2.0.0-alpha,"LOG.info(System.getProperty(""user.dir""));"
2.0.0-alpha,get tokens for all the required FileSystems..
2.0.0-alpha,Whether we need to recursive look into the directory structure
2.0.0-alpha,creates a MultiPathFilter with the hiddenFileFilter and the
2.0.0-alpha,user provided one (if any).
2.0.0-alpha,"LOG.info(""Total input paths to process : "" + result.size());"
2.0.0-alpha,get tokens for all the required FileSystems..
2.0.0-alpha,Whether we need to recursive look into the directory structure
2.0.0-alpha,creates a MultiPathFilter with the hiddenFileFilter and the
2.0.0-alpha,user provided one (if any).
2.0.0-alpha,"LOG.info(""Total input paths to process : "" + result.size());"
2.0.0-alpha,a simple hdfs copy function assume src path and dest path are in same hdfs
2.0.0-alpha,and FileSystem object has same schema
2.0.0-alpha,"LOG.warn(""interrupted while sleeping"", ie);"
2.0.0-alpha,public static String getHostname() {
2.0.0-alpha,try {
2.0.0-alpha,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
2.0.0-alpha,} catch (UnknownHostException uhe) {
2.0.0-alpha,}
2.0.0-alpha,"return new StringBuilder().append("""").append(uhe).toString();"
2.0.0-alpha,}
2.0.0-alpha,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
2.0.0-alpha,String hostname = getHostname();
2.0.0-alpha,String classname = clazz.getSimpleName();
2.0.0-alpha,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
2.0.0-alpha,"StringBuilder().append(""Starting "").append(classname).toString(), new"
2.0.0-alpha,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
2.0.0-alpha,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
2.0.0-alpha,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
2.0.0-alpha,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
2.0.0-alpha,
2.0.0-alpha,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
2.0.0-alpha,public void run() {
2.0.0-alpha,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
2.0.0-alpha,"this.val$classname + "" at "" + this.val$hostname}));"
2.0.0-alpha,}
2.0.0-alpha,});
2.0.0-alpha,}
2.0.0-alpha,"We we interrupted because we're meant to stop? If not, just"
2.0.0-alpha,continue ignoring the interruption
2.0.0-alpha,Recalculate waitTime.
2.0.0-alpha,// Begin delegation to Thread
2.0.0-alpha,// End delegation to Thread
2.0.0-alpha,instance submitter class
2.0.0-alpha,Obtain filename from path
2.0.0-alpha,Split filename to prexif and suffix (extension)
2.0.0-alpha,Check if the filename is okay
2.0.0-alpha,Prepare temporary file
2.0.0-alpha,Prepare buffer for data copying
2.0.0-alpha,Open and check input stream
2.0.0-alpha,Open output stream and copy data between source file in JAR and the temporary file
2.0.0-alpha,"If read/write fails, close streams safely before throwing an exception"
2.0.0-alpha,"Finally, load the library"
2.0.0-alpha,little endian load order
2.0.0-alpha,tail
2.0.0-alpha,fallthrough
2.0.0-alpha,fallthrough
2.0.0-alpha,finalization
2.0.0-alpha,fmix(h1);
2.0.0-alpha,----------
2.0.0-alpha,body
2.0.0-alpha,----------
2.0.0-alpha,tail
2.0.0-alpha,----------
2.0.0-alpha,finalization
2.0.0-alpha,----------
2.0.0-alpha,body
2.0.0-alpha,----------
2.0.0-alpha,tail
2.0.0-alpha,----------
2.0.0-alpha,finalization
2.0.0-alpha,throw new AngelException(e);
2.0.0-alpha,JobStateProto jobState = report.getJobState();
2.0.0-alpha,Used for java code to get a AngelClient instance
2.0.0-alpha,Used for python code to get a AngelClient instance
2.0.0-alpha,the leaf level file should be readable by others
2.0.0-alpha,the subdirs in the path should have execute permissions for
2.0.0-alpha,others
2.0.0-alpha,2.get job id
2.0.0-alpha,Credentials credentials = new Credentials();
2.0.0-alpha,4.copy resource files to hdfs
2.0.0-alpha,5.write configuration to a xml file
2.0.0-alpha,6.create am container context
2.0.0-alpha,7.Submit to ResourceManager
2.0.0-alpha,8.get app master client
2.0.0-alpha,Create a number of filenames in the JobTracker's fs namespace
2.0.0-alpha,add all the command line files/ jars and archive
2.0.0-alpha,first copy them to jobtrackers filesystem
2.0.0-alpha,should not throw a uri exception
2.0.0-alpha,should not throw an uri excpetion
2.0.0-alpha,set the timestamps of the archives and files
2.0.0-alpha,set the public/private visibility of the archives and files
2.0.0-alpha,get DelegationToken for each cached file
2.0.0-alpha,check if we do not need to copy the files
2.0.0-alpha,is jt using the same file system.
2.0.0-alpha,just checking for uri strings... doing no dns lookups
2.0.0-alpha,to see if the filesystems are the same. This is not optimal.
2.0.0-alpha,but avoids name resolution.
2.0.0-alpha,this might have name collisions. copy will throw an exception
2.0.0-alpha,parse the original path to create new path
2.0.0-alpha,check for ports
2.0.0-alpha,Write job file to JobTracker's fs
2.0.0-alpha,Setup resource requirements
2.0.0-alpha,Setup LocalResources
2.0.0-alpha,Setup security tokens
2.0.0-alpha,Setup the command to run the AM
2.0.0-alpha,Add AM user command opts
2.0.0-alpha,Final command
2.0.0-alpha,Setup the CLASSPATH in environment
2.0.0-alpha,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
2.0.0-alpha,Setup the environment variables for Admin first
2.0.0-alpha,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
2.0.0-alpha,Parse distributed cache
2.0.0-alpha,Setup ContainerLaunchContext for AM container
2.0.0-alpha,Set up the ApplicationSubmissionContext
2.0.0-alpha,private volatile PS2PSPusherImpl ps2PSPusher;
2.0.0-alpha,TODO
2.0.0-alpha,Add tokens to new user so that it may execute its task correctly.
2.0.0-alpha,TODO
2.0.0-alpha,to exit
2.0.0-alpha,TODO
2.0.0-alpha,TODO
2.0.0-alpha,ps2PSPusher.recover(ProtobufUtil.convert(needRecoverParts.get(i)));
2.0.0-alpha,context.getSnapshotManager().processRecovery();
2.0.0-alpha,First check snapshot
2.0.0-alpha,Check load path setting
2.0.0-alpha,TODO
2.0.0-alpha,if(ps2PSPusher != null) {
2.0.0-alpha,ps2PSPusher.start();
2.0.0-alpha,}
2.0.0-alpha,public PS2PSPusherImpl getPs2PSPusher() {
2.0.0-alpha,return ps2PSPusher;
2.0.0-alpha,}
2.0.0-alpha,"LOG.error(""send response of request "" + requestToString(clientId, seqId) + "" failed "");"
2.0.0-alpha,"LOG.error(""send response of request failed, request seqId="" + seqId + "", channel="" + ch, ex);"
2.0.0-alpha,Release the input buffer
2.0.0-alpha,Release the input buffer
2.0.0-alpha,"1. handle the rpc, get the response"
2.0.0-alpha,Release the input buffer
2.0.0-alpha,2. Serialize the response
2.0.0-alpha,Send the serialized response
2.0.0-alpha,Just serialize the head
2.0.0-alpha,Exception happened
2.0.0-alpha,Just serialize the head
2.0.0-alpha,Exception happened
2.0.0-alpha,Reset the response and allocate buffer again
2.0.0-alpha,Get partition and check the partition state
2.0.0-alpha,Get the stored pss for this partition
2.0.0-alpha,"Check this ps is the master ps for this location, only master ps can accept the update"
2.0.0-alpha,Check the partition state again
2.0.0-alpha,Start to put the update to the slave pss
2.0.0-alpha,TODO
2.0.0-alpha,"context.getPS2PSPusher().put(request, in, partLoc);"
2.0.0-alpha,Get partition and check the partition state
2.0.0-alpha,Get the stored pss for this partition
2.0.0-alpha,"Check this ps is the master ps for this partition, if not, just return failed"
2.0.0-alpha,Start to put the update to the slave pss
2.0.0-alpha,TODO
2.0.0-alpha,return ServerState.GENERAL;
2.0.0-alpha,Use Epoll for linux
2.0.0-alpha,public String uuid;
2.0.0-alpha,public void setChannelPool(GenericObjectPool<Channel> channelPool) {
2.0.0-alpha,this.channelPool = channelPool;
2.0.0-alpha,}
2.0.0-alpha,private final ParameterServer psServer;
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,TODO:default value
2.0.0-alpha,TODO
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"////// disk io method, for model read/load"
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"////// network io method, for model transform"
2.0.0-alpha,///////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,"Methods with out lock operation, you must call startWrite/startRead before using these methods"
2.0.0-alpha,and call endWrite/endRead after
2.0.0-alpha,////////////////////////////////////////////////////////////////////////////////////////////////
2.0.0-alpha,TODO: dynamic add/delete row
2.0.0-alpha,private final List<PartitionKey> partitionKeys;
2.0.0-alpha,Read matrix meta from meta file
2.0.0-alpha,Load partitions from file use fork-join
2.0.0-alpha,Save partitions to files use fork-join
2.0.0-alpha,Write the ps matrix meta to the meta file
2.0.0-alpha,Use Epoll for linux
2.0.0-alpha,find the partition request context from cache
2.0.0-alpha,get a channel to server from pool
2.0.0-alpha,"if channel is not valid, it means maybe the connections to the server are closed"
2.0.0-alpha,channelManager.removeChannelPool(loc);
2.0.0-alpha,Generate seq id
2.0.0-alpha,Create a RecoverPartRequest
2.0.0-alpha,Serialize the request
2.0.0-alpha,Change the seqId for the request
2.0.0-alpha,Serialize the request
2.0.0-alpha,"First check the state of the channels in the pool, if a channel is unused, just return"
2.0.0-alpha,"If all channels are in use, create a new channel or wait"
2.0.0-alpha,Create a new channel
2.0.0-alpha,"add the PSAgentContext,need fix"
2.0.0-alpha,TODO:add more vector type
2.0.0-alpha,TODO : subDim set
2.0.0-alpha,Sort the parts by partitionId
2.0.0-alpha,Sort partition keys use start column index
2.0.0-alpha,"For each partition, we generate a update split."
2.0.0-alpha,"Although the split is empty for partitions those without any update data,"
2.0.0-alpha,we still need to generate a update split to update the clock info on ps.
2.0.0-alpha,Sort the parts by partitionId
2.0.0-alpha,Sort partition keys use start column index
2.0.0-alpha,"For each partition, we generate a update split."
2.0.0-alpha,"Although the split is empty for partitions those without any update data,"
2.0.0-alpha,we still need to generate a update split to update the clock info on ps.
2.0.0-alpha,write the max abs
2.0.0-alpha,---------------------------------------------------
2.0.0-alpha,---------------------------------------------------
2.0.0-alpha,---------------------------------------------------------------
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,axis = 0: on rows
2.0.0-alpha,axis = 1: on cols
2.0.0-alpha,1. find the insert point
2.0.0-alpha,2. check the capacity and insert
2.0.0-alpha,3. increase size
2.0.0-alpha,1. find the insert point
2.0.0-alpha,2. check the capacity and insert
2.0.0-alpha,3. increase size
2.0.0-alpha,1. find the insert point
2.0.0-alpha,2. check the capacity and insert
2.0.0-alpha,3. increase size
2.0.0-alpha,1. find the insert point
2.0.0-alpha,2. check the capacity and insert
2.0.0-alpha,3. increase size
2.0.0-alpha,1. find the insert point
2.0.0-alpha,2. check the capacity and insert
2.0.0-alpha,3. increase size
2.0.0-alpha,1. find the insert point
2.0.0-alpha,2. check the capacity and insert
2.0.0-alpha,3. increase size
2.0.0-alpha,1. find the insert point
2.0.0-alpha,2. check the capacity and insert
2.0.0-alpha,3. increase size
2.0.0-alpha,1. find the insert point
2.0.0-alpha,2. check the capacity and insert
2.0.0-alpha,3. increase size
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,multi-rehash
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,"slower but memory efficient, for small vector only"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"no rehashor one onle rehash is required, nothing to optimization"
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,to avoid multi-rehash
2.0.0-alpha,"dger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
2.0.0-alpha,"sger(int m, int n, double alpha, double [ ] x, int incx, double [ ] y, int incy, double [ ] a, int lda)"
2.0.0-alpha,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
2.0.0-alpha,IntDoubleVector vector = (IntDoubleVector) ((GetRowResult) matrixClient.get(func)).getRow();
2.0.0-alpha,add dense double matrix
2.0.0-alpha,TODO Auto-generated method stub
2.0.0-alpha,TODO Auto-generated method stub
2.0.0-alpha,TODO Auto-generated method stub
2.0.0-alpha,get configuration from config file
2.0.0-alpha,set localDir with enviroment set by nm.
2.0.0-alpha,get master location
2.0.0-alpha,init task manager and start tasks
2.0.0-alpha,start heartbeat thread
2.0.0-alpha,taskManager.assignTaskIds(response.getTaskidsList());
2.0.0-alpha,todo
2.0.0-alpha,"if worker timeout, it may be knocked off."
2.0.0-alpha,"SUCCESS, do nothing"
2.0.0-alpha,heartbeatFailedTime = 0;
2.0.0-alpha,private KEY currentKey;
2.0.0-alpha,will be created
2.0.0-alpha,TODO Auto-generated method stub
2.0.0-alpha,Bitmap bitmap = new Bitmap();
2.0.0-alpha,int max = indexArray[size - 1];
2.0.0-alpha,byte [] bitIndexArray = new byte[max / 8 + 1];
2.0.0-alpha,for(int i = 0; i < size; i++){
2.0.0-alpha,int bitIndex = indexArray[i] >> 3;
2.0.0-alpha,int bitOffset = indexArray[i] - (bitIndex << 3);
2.0.0-alpha,switch(bitOffset){
2.0.0-alpha,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
2.0.0-alpha,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
2.0.0-alpha,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
2.0.0-alpha,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
2.0.0-alpha,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
2.0.0-alpha,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
2.0.0-alpha,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
2.0.0-alpha,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
2.0.0-alpha,}
2.0.0-alpha,}
2.0.0-alpha,"true, false"
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,Application Configs
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,Master Configs
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,Worker Configs
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,Task Configs
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,ParameterServer Configs
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,////////////////// IPC //////////////////////////
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,Matrix transfer Configs.
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,Matrix transfer Configs.
2.0.0-alpha,//////////////////////////////
2.0.0-alpha,Configs used to ANGEL_PS_PSAGENT running mode future.
2.0.0-alpha,model parse
2.0.0-alpha,Mark whether use pyangel or not.
2.0.0-alpha,private Configuration conf;
2.0.0-alpha,"Configuration that should be used in python environment, there should only be one"
2.0.0-alpha,configuration instance in each Angel context.
2.0.0-alpha,Use private access means jconf should not be changed or modified in this way.
2.0.0-alpha,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
2.0.0-alpha,Do nothing
2.0.0-alpha,To-DO: add other ways to justify different value types
2.0.0-alpha,"This is so ugly, must re-implement by more elegance way"
2.0.0-alpha,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
2.0.0-alpha,and other files submitted by user.
2.0.0-alpha,Launch python process
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,Train batch number per epoch.
2.0.0-alpha,Batch number
2.0.0-alpha,Model type
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Set data format
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,set sgd LR algorithm parameters #feature #epoch
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set load model path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,Train batch number per epoch.
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Set data format
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,set sgd FM algorithm parameters #feature #epoch
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set load model path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,Model type
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Set data format
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,set sgd LR algorithm parameters #feature #epoch
2.0.0-alpha,"conf.setDouble(MLConf.ML_DATA_POSNEG_RATIO(), posnegRatio);"
2.0.0-alpha,predictTest();
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set load model path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,class number
2.0.0-alpha,Model type
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Set data format
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,set sgd LR algorithm parameters #feature #epoch
2.0.0-alpha,Set log path
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set save model path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set load model path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,predictTest();
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set training data path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,Cluster center number
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Sample ratio per mini-batch
2.0.0-alpha,C
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,set Kmeans algorithm parameters #cluster #feature #epoch
2.0.0-alpha,Set data format
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log save path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set testing data path
2.0.0-alpha,Set load model path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,predictTest();
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,Model type
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Set data format
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,set sgd LR algorithm parameters #feature #epoch
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set load model path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,Data is classification
2.0.0-alpha,Model is classification
2.0.0-alpha,Train batch number per epoch.
2.0.0-alpha,loss delta
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Set data format
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,set sgd LR algorithm parameters #feature #epoch
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set load model path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,predictTest();
2.0.0-alpha,Feature number of train data
2.0.0-alpha,Total iteration number
2.0.0-alpha,Validation sample Ratio
2.0.0-alpha,"Data format, libsvm or dummy"
2.0.0-alpha,Data is classification
2.0.0-alpha,Model is classification
2.0.0-alpha,Train batch number per epoch.
2.0.0-alpha,Learning rate
2.0.0-alpha,Decay of learning rate
2.0.0-alpha,Regularization coefficient
2.0.0-alpha,Set local deploy mode
2.0.0-alpha,Set basic configuration keys
2.0.0-alpha,Set data format
2.0.0-alpha,"set angel resource parameters #worker, #task, #PS"
2.0.0-alpha,set sgd LR algorithm parameters #feature #epoch
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set save model path
2.0.0-alpha,Set log path
2.0.0-alpha,Set actionType train
2.0.0-alpha,Set trainning data path
2.0.0-alpha,Set load model path
2.0.0-alpha,Set predict result path
2.0.0-alpha,Set actionType prediction
2.0.0-alpha,"gradient descent first, then truncated"
2.0.0-alpha,StringBuffer sb = new StringBuffer();
2.0.0-alpha,"sb.append(""GetColsParam values "");"
2.0.0-alpha,"sb.append(values[i] +"" "");"
2.0.0-alpha,LOG.error(sb.toString());
2.0.0-alpha,StringBuffer sb = new StringBuffer();
2.0.0-alpha,"sb.append(""GetColsParams "");"
2.0.0-alpha,for (int i = 0; i < this.cols.length; i ++) {
2.0.0-alpha,"sb.append(this.cols[i] + "" "");"
2.0.0-alpha,}
2.0.0-alpha,LOG.error(sb.toString());
2.0.0-alpha,Arrays.sort(rows);
2.0.0-alpha,int sum = 0;
2.0.0-alpha,"System.out.println(""pkeys.size="" + pkeys.size());"
2.0.0-alpha,"params.add(new PartitionGetColsParam(matrixId, pkeys.get(0), rows, cols));"
2.0.0-alpha,sum += part.length;
2.0.0-alpha,"LOG.info(""split length = "" + sum + "", cols = "" + cols.length);"
2.0.0-alpha,"System.out.println(""deserialize cols.length="" + nCols);"
2.0.0-alpha,"System.out.print(""deserialize "");"
2.0.0-alpha,"System.out.print(cols[c] + "" "");"
2.0.0-alpha,System.out.println();
2.0.0-alpha,"System.out.print(""PartitionGet "");"
2.0.0-alpha,for (int i = 0; i < cols.length; i ++) {
2.0.0-alpha,"System.out.print(cols[i] + "" "");"
2.0.0-alpha,}
2.0.0-alpha,System.out.println();
2.0.0-alpha,"System.out.println(""doGet Double cols.length="" + cols.length);"
2.0.0-alpha,"LOG.info(""Here merge"");"
2.0.0-alpha,int sum = 0;
2.0.0-alpha,sum += rrr.cols.length;
2.0.0-alpha,"LOG.error(""double rrr.cols = "" + sum + "", map size = "" + maps.size());"
2.0.0-alpha,IntDoubleVector[] allVectors = new IntDoubleVector[allColumns.length];
2.0.0-alpha,for (int i = 0; i < allColumns.length; i ++)
2.0.0-alpha,allVectors[i] = maps.get(allColumns[i]);
2.0.0-alpha,"return new GetColsResult(VFactory.denseLongVector(allColumns),"
2.0.0-alpha,"VFactory.compIntDoubleVector(dim, allVectors, allVectors.length));"
2.0.0-alpha,int sum = 0;
2.0.0-alpha,sum += cols.length;
2.0.0-alpha,"System.out.print("" "" + cols[i]);"
2.0.0-alpha,System.out.println();
2.0.0-alpha,"LOG.error(""float rrr.cols = "" + sum + "", map size = "" + maps.size());"
2.0.0-alpha,IntFloatVector[] allVectors = new IntFloatVector[allColumns.length];
2.0.0-alpha,for (int i = 0; i < allColumns.length; i ++)
2.0.0-alpha,allVectors[i] = maps.get(allColumns[i]);
2.0.0-alpha,"return new GetColsResult(VFactory.denseLongVector(allColumns),"
2.0.0-alpha,"VFactory.compIntFloatVector(dim, allVectors, allVectors.length));"
2.0.0-alpha,TODO Auto-generated method stub
2.0.0-alpha,"ground truth: positive, precision: positive"
2.0.0-alpha,start row index for words
2.0.0-alpha,start row index for docs
2.0.0-alpha,doc ids
2.0.0-alpha,topic assignments
2.0.0-alpha,word to docs reverse index
2.0.0-alpha,count word
2.0.0-alpha,build word start index
2.0.0-alpha,build word to doc reverse idx
2.0.0-alpha,build dks
2.0.0-alpha,dks = new TraverseHashMap[n_docs];
2.0.0-alpha,for (int d = 0; d < n_docs; d++) {
2.0.0-alpha,if (K < Short.MAX_VALUE) {
2.0.0-alpha,if (docs.get(d).len < Byte.MAX_VALUE)
2.0.0-alpha,dks[d] = new S2BTraverseMap(docs.get(d).len);
2.0.0-alpha,if (docs.get(d).len < Short.MAX_VALUE)
2.0.0-alpha,"dks[d] = new S2STraverseMap(Math.min(K, docs.get(d).len));"
2.0.0-alpha,else
2.0.0-alpha,"dks[d] = new S2ITraverseMap(Math.min(K, docs.get(d).len));"
2.0.0-alpha,} else {
2.0.0-alpha,"dks[d] = new I2ITranverseMap(Math.min(K, docs.get(d).len));"
2.0.0-alpha,}
2.0.0-alpha,}
2.0.0-alpha,build dks
2.0.0-alpha,allocate update maps
2.0.0-alpha,Skip if no token for this word
2.0.0-alpha,Check whether error when fetching word-topic
2.0.0-alpha,Build FTree for current word
2.0.0-alpha,current doc
2.0.0-alpha,old topic assignment
2.0.0-alpha,"Check if error happens. if this happen, it's probably that failures happen to servers."
2.0.0-alpha,We need to adjust the memory settings or network fetching parameters.
2.0.0-alpha,Update statistics if needed
2.0.0-alpha,Calculate psum and sample new topic
2.0.0-alpha,Update statistics if needed
2.0.0-alpha,Assign new topic
2.0.0-alpha,Skip if no token for this word
2.0.0-alpha,The starting point
2.0.0-alpha,There's always an unused entry.
2.0.0-alpha,The starting point
2.0.0-alpha,There's always an unused entry.
2.0.0-alpha,The starting point
2.0.0-alpha,There's always an unused entry.
2.0.0-alpha,The starting point
2.0.0-alpha,There's always an unused entry.
2.0.0-alpha,The starting point
2.0.0-alpha,There's always an unused entry.
2.0.0-alpha,The starting point
2.0.0-alpha,There's always an unused entry.
2.0.0-alpha,print();
2.0.0-alpha,The starting point
2.0.0-alpha,There's always an unused entry.
2.0.0-alpha,The starting point
2.0.0-alpha,There's always an unused entry.
2.0.0-alpha,The starting point
2.0.0-alpha,There's always an unused entry.
2.0.0-alpha,The starting point
2.0.0-alpha,There's always an unused entry.
2.0.0-alpha,print();
2.0.0-alpha,Write #rows
2.0.0-alpha,Write each row
2.0.0-alpha,dense
2.0.0-alpha,sparse
2.0.0-alpha,LOG.info(buf.refCnt());
2.0.0-alpha,dense
2.0.0-alpha,sparse
2.0.0-alpha,calculate columns
2.0.0-alpha,loss function
2.0.0-alpha,gradient and hessian
2.0.0-alpha,"categorical feature set, null: none, empty: all, else: partial"
2.0.0-alpha,"node's end index in instancePos, instances in [start, end] belong to a tree node"
2.0.0-alpha,initialize the phase
2.0.0-alpha,current tree and depth
2.0.0-alpha,create loss function
2.0.0-alpha,calculate grad info of each instance
2.0.0-alpha,"create data sketch, push candidate split value to PS"
2.0.0-alpha,1. calculate candidate split value
2.0.0-alpha,categorical features
2.0.0-alpha,2. push local sketch to PS
2.0.0-alpha,the leader worker
2.0.0-alpha,merge categorical features
2.0.0-alpha,create updates
2.0.0-alpha,"pull the global sketch from PS, only called once by each worker"
2.0.0-alpha,number of categorical feature
2.0.0-alpha,sample feature
2.0.0-alpha,push sampled feature set to the current tree
2.0.0-alpha,create new tree
2.0.0-alpha,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
2.0.0-alpha,calculate gradient
2.0.0-alpha,"1. create new tree, initialize tree nodes and node stats"
2.0.0-alpha,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
2.0.0-alpha,2.1. pull the sampled features of the current tree
2.0.0-alpha,this.forest[this.currentTree].fset = sampleFeatureVector.getStorage().getValues();
2.0.0-alpha,"2.2. if use all the features, only called one"
2.0.0-alpha,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
2.0.0-alpha,4. set root node to active
2.0.0-alpha,"5. reset instance position, set the root node's span"
2.0.0-alpha,6. calculate gradient
2.0.0-alpha,1. decide nodes that should be calculated
2.0.0-alpha,2. decide calculated and subtracted tree nodes
2.0.0-alpha,3. calculate threads
2.0.0-alpha,wait until all threads finish
2.0.0-alpha,4. subtract threads
2.0.0-alpha,wait until all threads finish
2.0.0-alpha,5. send histograms to PS
2.0.0-alpha,6. update histogram cache
2.0.0-alpha,clock
2.0.0-alpha,find split
2.0.0-alpha,"1. find responsible tree node, using RR scheme"
2.0.0-alpha,2. pull gradient histogram
2.0.0-alpha,2.1. get the name of this node's gradient histogram on PS
2.0.0-alpha,2.2. pull the histogram
2.0.0-alpha,2.3. find best split result of this tree node
2.0.0-alpha,2.3.1 using server split
2.0.0-alpha,"update the grad stats of the root node on PS, only called once by leader worker"
2.0.0-alpha,update the grad stats of children node
2.0.0-alpha,update the left child
2.0.0-alpha,update the right child
2.0.0-alpha,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
2.0.0-alpha,"2.3.3 otherwise, the returned histogram contains the gradient info"
2.0.0-alpha,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
2.0.0-alpha,2.3.5 reset this tree node's gradient histogram to 0
2.0.0-alpha,3. push split feature to PS
2.0.0-alpha,4. push split value to PS
2.0.0-alpha,5. push split gain to PS
2.0.0-alpha,6. set phase to AFTER_SPLIT
2.0.0-alpha,this.phase = GBDTPhase.AFTER_SPLIT;
2.0.0-alpha,clock
2.0.0-alpha,1. get split feature
2.0.0-alpha,2. get split value
2.0.0-alpha,3. get split gain
2.0.0-alpha,4. get node weight
2.0.0-alpha,5. split node
2.0.0-alpha,update local replica
2.0.0-alpha,create AfterSplit task
2.0.0-alpha,"2. check thread stats, if all threads finish, return"
2.0.0-alpha,6. clock
2.0.0-alpha,"split the span of one node, reset the instance position"
2.0.0-alpha,in case this worker has no instance on this node
2.0.0-alpha,set the span of left child
2.0.0-alpha,set the span of right child
2.0.0-alpha,"1. left to right, find the first instance that should be in the right child"
2.0.0-alpha,"2. right to left, find the first instance that should be in the left child"
2.0.0-alpha,3. swap two instances
2.0.0-alpha,4. find the cut pos
2.0.0-alpha,5. set the span of left child
2.0.0-alpha,6. set the span of right child
2.0.0-alpha,set tree node to active
2.0.0-alpha,set node to leaf
2.0.0-alpha,set node to inactive
2.0.0-alpha,finish current depth
2.0.0-alpha,finish current tree
2.0.0-alpha,set the tree phase
2.0.0-alpha,check if there is active node
2.0.0-alpha,check if finish all the tree
2.0.0-alpha,update node's grad stats on PS
2.0.0-alpha,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
2.0.0-alpha,the root node's stats is updated by leader worker
2.0.0-alpha,1. create the update
2.0.0-alpha,2. push the update to PS
2.0.0-alpha,1. update predictions of training data
2.0.0-alpha,2. update predictions of validation data
2.0.0-alpha,the leader task adds node prediction to flush list
2.0.0-alpha,1. name of this node's grad histogram on PS
2.0.0-alpha,2. build the grad histogram of this node
2.0.0-alpha,3. push the histograms to PS
2.0.0-alpha,4. reset thread stats to finished
2.0.0-alpha,5.1. set the children nodes of this node
2.0.0-alpha,5.2. set split info and grad stats to this node
2.0.0-alpha,5.2. create children nodes
2.0.0-alpha,"5.3. create node stats for children nodes, and add them to the tree"
2.0.0-alpha,5.4. reset instance position
2.0.0-alpha,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
2.0.0-alpha,5.6. set children nodes to leaf nodes
2.0.0-alpha,5.7. set nid to leaf node
2.0.0-alpha,5.8. deactivate active node
2.0.0-alpha,"get feature type, 0:empty 1:all equal 2:real"
2.0.0-alpha,"if not -1, sufficient space will be allocated at once"
2.0.0-alpha,copy the highest levels
2.0.0-alpha,copy baseBuffer
2.0.0-alpha,merge two non-empty quantile sketches
2.0.0-alpha,left child <= split value; right child > split value
2.0.0-alpha,"the first: minimal, the last: maximal"
2.0.0-alpha,categorical features
2.0.0-alpha,continuous features
2.0.0-alpha,left child <= split value; right child > split value
2.0.0-alpha,feature index used to split
2.0.0-alpha,feature value used to split
2.0.0-alpha,loss change after split this node
2.0.0-alpha,grad stats of the left child
2.0.0-alpha,grad stats of the right child
2.0.0-alpha,"LOG.info(""Constructor with fid = -1"");"
2.0.0-alpha,fid = -1: no split currently
2.0.0-alpha,the minimal split value is the minimal value of feature
2.0.0-alpha,the splits do not include the maximal value of feature
2.0.0-alpha,"1. the average distance, (maxValue - minValue) / splitNum"
2.0.0-alpha,2. calculate the candidate split value
2.0.0-alpha,1. new feature's histogram (grad + hess)
2.0.0-alpha,size: sampled_featureNum * (2 * splitNum)
2.0.0-alpha,"in other words, concatenate each feature's histogram"
2.0.0-alpha,2. get the span of this node
2.0.0-alpha,------ 3. using sparse-aware method to build histogram ---
2.0.0-alpha,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
2.0.0-alpha,3.1. get the instance index
2.0.0-alpha,3.2. get the grad and hess of the instance
2.0.0-alpha,3.3. add to the sum
2.0.0-alpha,3.4. loop the non-zero entries
2.0.0-alpha,3.4.1. get feature value
2.0.0-alpha,3.4.2. current feature's position in the sampled feature set
2.0.0-alpha,"int fPos = findFidPlace(this.controller.fSet, fid);"
2.0.0-alpha,3.4.3. find the position of feature value in a histogram
2.0.0-alpha,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
2.0.0-alpha,3.4.4. add the grad and hess to the corresponding bin
2.0.0-alpha,3.4.5. add the reverse to the bin that contains 0.0f
2.0.0-alpha,4. add the grad and hess sum to the zero bin of all features
2.0.0-alpha,find the best split result of the histogram of a tree node
2.0.0-alpha,1. calculate the gradStats of the root node
2.0.0-alpha,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
2.0.0-alpha,2. loop over features
2.0.0-alpha,2.1. get the ture feature id in the sampled feature set
2.0.0-alpha,2.2. get the indexes of histogram of this feature
2.0.0-alpha,2.3. find the best split of current feature
2.0.0-alpha,2.4. update the best split result if possible
2.0.0-alpha,"update the grad stats of the root node on PS, only called once by leader worker"
2.0.0-alpha,3. update the grad stats of children node
2.0.0-alpha,3.1. update the left child
2.0.0-alpha,3.2. update the right child
2.0.0-alpha,find the best split result of one feature
2.0.0-alpha,1. set the feature id
2.0.0-alpha,2. create the best left stats and right stats
2.0.0-alpha,3. the gain of the root node
2.0.0-alpha,4. create the temp left and right grad stats
2.0.0-alpha,5. loop over all the data in histogram
2.0.0-alpha,5.1. get the grad and hess of current hist bin
2.0.0-alpha,5.2. check whether we can split with current left hessian
2.0.0-alpha,right = root - left
2.0.0-alpha,5.3. check whether we can split with current right hessian
2.0.0-alpha,5.4. calculate the current loss gain
2.0.0-alpha,5.5. check whether we should update the split result with current loss gain
2.0.0-alpha,split value = sketches[splitIdx]
2.0.0-alpha,"5.6. if should update, also update the best left and right grad stats"
2.0.0-alpha,6. set the best left and right grad stats
2.0.0-alpha,partition number
2.0.0-alpha,cols of each partition
2.0.0-alpha,1. calculate the total grad sum and hess sum
2.0.0-alpha,2. create the grad stats of the node
2.0.0-alpha,1. calculate the total grad sum and hess sum
2.0.0-alpha,2. create the grad stats of the node
2.0.0-alpha,1. calculate the total grad sum and hess sum
2.0.0-alpha,2. create the grad stats of the node
2.0.0-alpha,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
2.0.0-alpha,if (left > end) return end - start;
2.0.0-alpha,find the best split result of the histogram of a tree node
2.0.0-alpha,2.2. get the indexes of histogram of this feature
2.0.0-alpha,2.3. find the best split of current feature
2.0.0-alpha,2.4. update the best split result if possible
2.0.0-alpha,find the best split result of one feature
2.0.0-alpha,1. set the feature id
2.0.0-alpha,splitEntry.setFid(fid);
2.0.0-alpha,2. create the best left stats and right stats
2.0.0-alpha,3. the gain of the root node
2.0.0-alpha,4. create the temp left and right grad stats
2.0.0-alpha,5. loop over all the data in histogram
2.0.0-alpha,5.1. get the grad and hess of current hist bin
2.0.0-alpha,5.2. check whether we can split with current left hessian
2.0.0-alpha,right = root - left
2.0.0-alpha,5.3. check whether we can split with current right hessian
2.0.0-alpha,5.4. calculate the current loss gain
2.0.0-alpha,5.5. check whether we should update the split result with current loss gain
2.0.0-alpha,"5.6. if should update, also update the best left and right grad stats"
2.0.0-alpha,6. set the best left and right grad stats
2.0.0-alpha,find the best split result of a serve row on the PS
2.0.0-alpha,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
2.0.0-alpha,2.2. get the start index in histogram of this feature
2.0.0-alpha,2.3. find the best split of current feature
2.0.0-alpha,2.4. update the best split result if possible
2.0.0-alpha,"find the best split result of one feature from a server row, used by the PS"
2.0.0-alpha,1. set the feature id
2.0.0-alpha,2. create the best left stats and right stats
2.0.0-alpha,3. the gain of the root node
2.0.0-alpha,4. create the temp left and right grad stats
2.0.0-alpha,5. loop over all the data in histogram
2.0.0-alpha,5.1. get the grad and hess of current hist bin
2.0.0-alpha,5.2. check whether we can split with current left hessian
2.0.0-alpha,right = root - left
2.0.0-alpha,5.3. check whether we can split with current right hessian
2.0.0-alpha,5.4. calculate the current loss gain
2.0.0-alpha,5.5. check whether we should update the split result with current loss gain
2.0.0-alpha,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
2.0.0-alpha,the task use index to find fvalue
2.0.0-alpha,"5.6. if should update, also update the best left and right grad stats"
2.0.0-alpha,6. set the best left and right grad stats
2.0.0-alpha,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
2.0.0-alpha,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
2.0.0-alpha,max and min of each feature
2.0.0-alpha,clear all the information
2.0.0-alpha,calculate the sum of gradient and hess
2.0.0-alpha,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
2.0.0-alpha,ridx)
2.0.0-alpha,check if necessary information is ready
2.0.0-alpha,"same as add, reduce is used in All Reduce"
2.0.0-alpha,"features used in this tree, if equals null, means use all the features without sampling"
2.0.0-alpha,node in the tree
2.0.0-alpha,the gradient info of each instances
2.0.0-alpha,initialize nodes
2.0.0-alpha,gradient
2.0.0-alpha,second order gradient
2.0.0-alpha,TODO: only support dense double now
2.0.0-alpha,int sendStartCol = (int) row.getStartCol();
2.0.0-alpha,find the max abs
2.0.0-alpha,compress data
2.0.0-alpha,logistic loss for binary classification task.
2.0.0-alpha,"logistic loss, but predict un-transformed margin"
2.0.0-alpha,check if label in range
2.0.0-alpha,return the default evaluation metric for the objective
2.0.0-alpha,"task type: classification, regression, or ranking"
2.0.0-alpha,"quantile sketch, size = featureNum * splitNum"
2.0.0-alpha,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
2.0.0-alpha,"active tree nodes, size = pow(2, treeDepth) -1"
2.0.0-alpha,sampled features. size = treeNum * sampleRatio * featureNum
2.0.0-alpha,categorical feature. size = workerNum * cateFeatNum * splitNum
2.0.0-alpha,"split features, size = treeNum * treeNodeNum"
2.0.0-alpha,"split values, size = treeNum * treeNodeNum"
2.0.0-alpha,"split gains, size = treeNum * treeNodeNum"
2.0.0-alpha,"node weights, size = treeNum * treeNodeNum"
2.0.0-alpha,"node preds, size = treeNum * treeNodeNum"
2.0.0-alpha,if using PS to perform split
2.0.0-alpha,step size for a tree
2.0.0-alpha,number of class
2.0.0-alpha,minimum loss change required for a split
2.0.0-alpha,maximum depth of a tree
2.0.0-alpha,number of features
2.0.0-alpha,number of nonzero
2.0.0-alpha,number of candidates split value
2.0.0-alpha,----- the rest parameters are less important ----
2.0.0-alpha,base instance weight
2.0.0-alpha,minimum amount of hessian(weight) allowed in a child
2.0.0-alpha,L2 regularization factor
2.0.0-alpha,L1 regularization factor
2.0.0-alpha,default direction choice
2.0.0-alpha,maximum delta update we can add in weight estimation
2.0.0-alpha,this parameter can be used to stabilize update
2.0.0-alpha,default=0 means no constraint on weight delta
2.0.0-alpha,whether we want to do subsample for row
2.0.0-alpha,whether to subsample columns for each tree
2.0.0-alpha,accuracy of sketch
2.0.0-alpha,accuracy of sketch
2.0.0-alpha,leaf vector size
2.0.0-alpha,option for parallelization
2.0.0-alpha,option to open cacheline optimization
2.0.0-alpha,whether to not print info during training.
2.0.0-alpha,maximum depth of the tree
2.0.0-alpha,number of features used for tree construction
2.0.0-alpha,"minimum loss change required for a split, otherwise stop split"
2.0.0-alpha,----- the rest parameters are less important ----
2.0.0-alpha,default direction choice
2.0.0-alpha,whether we want to do sample data
2.0.0-alpha,whether to sample columns during tree construction
2.0.0-alpha,whether to use histogram for split
2.0.0-alpha,number of histogram units
2.0.0-alpha,whether to print info during training.
2.0.0-alpha,----- the rest parameters are obtained after training ----
2.0.0-alpha,total number of nodes
2.0.0-alpha,number of deleted nodes */
v1.5.1,implement Zip2Map interface
v1.5.1,implement Zip3Map interface
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy data spliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,TODO Auto-generated constructor stub
v1.5.1,row 0 is a random uniform
v1.5.1,row 1 is a random normal
v1.5.1,row 2 is filled with 1.0
v1.5.1,TODO: Have to deal with default values
v1.5.1,asum += Math.abs(data.defaultReturnValue()) * (entireSize - data.size());
v1.5.1,TODO: Have to deal with default values
v1.5.1,sum += (entireSize - keys.size()) * data1.defaultReturnValue() * data2.defaultReturnValue();
v1.5.1,TODO: Have to deal with default values
v1.5.1,"qSum += Math.pow(data.defaultReturnValue(), 2) * (entireSize - data.size());"
v1.5.1,TODO: Have to deal with default values
v1.5.1,asum += data.defaultReturnValue() * (entireSize - data.size());
v1.5.1,find the max abs
v1.5.1,compress data
v1.5.1,TODO: a better way is needed to deal with defaultValue
v1.5.1,TODO: a better way is needed to deal with defaultValue
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,"
v1.5.1,"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:"
v1.5.1,"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"","
v1.5.1,"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output"""
v1.5.1,Feature number of train data
v1.5.1,Number of nonzero features
v1.5.1,Tree number
v1.5.1,Tree depth
v1.5.1,Split number
v1.5.1,Feature sample ratio
v1.5.1,Data format
v1.5.1,Learning rate
v1.5.1,"set input, output path"
v1.5.1,Set GBDT algorithm parameters
v1.5.1,Load Model from HDFS.
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Validation sample ratio
v1.5.1,"Data format,libsvm or dummy"
v1.5.1,Train batch number per epoch
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,"Set job queue, if you use YARN deploy mode, you can set job queue by"
v1.5.1,"self.conf.set('mapreduce.job.queue.name', 'default')"
v1.5.1,Set local deploy mode
v1.5.1,Set basic self.configuration keys
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #tast, #ps"
v1.5.1,set sgd LR algorithim parameters # feature # epoch
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set actionType incremental train
v1.5.1,Set log path
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set actionType prediction
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Set local deploy mode
v1.5.1,Set basic self.configuration keys
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,"Set trainning data, and save model path"
v1.5.1,Set actionType train
v1.5.1,Set MF algorithm parameters
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.5.1,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,"
v1.5.1,"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:"
v1.5.1,"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"","
v1.5.1,"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output"""
v1.5.1,"if you need, you can delete the annotation mark before Line35,Line36,Line61,Line62, so"
v1.5.1,there is no need for you to pass the configs every time you submit the pyangel job.
v1.5.1,"input_path = ""file:///${YOUR_ANGEL_HOME}/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"""
v1.5.1,"output_path = ""file:///${YOUR_ANGEL_HOME}/data/output"""
v1.5.1,Feature number of train data
v1.5.1,Number of nonzero features
v1.5.1,Tree number
v1.5.1,Tree depth
v1.5.1,Split number
v1.5.1,Feature sample ratio
v1.5.1,Data format
v1.5.1,Learning rate
v1.5.1,Use local deploy mode and dummy data spliter
v1.5.1,set input] = output path
v1.5.1,self.conf[AngelConf.ANGEL_TRAIN_DATA_PATH] = input_path
v1.5.1,self.conf[AngelConf.ANGEL_SAVE_MODEL_PATH] = output_path
v1.5.1,Set GBDT algorithm parameters
v1.5.1,Load Model from HDFS.
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Cluster center number
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Sample ratio per mini-batch
v1.5.1,C
v1.5.1,Set local deploy mode
v1.5.1,Set basic self.configuration key
v1.5.1,"Set angel resource parameters #worker, #task, #PS"
v1.5.1,Set Kmeans algorithm parameters #cluster #feature #epoch
v1.5.1,Set data format
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log sava path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set actionType prediction
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Validation sample Ratio
v1.5.1,"Data format, libsvm or dummy"
v1.5.1,Train batch number per epoch.
v1.5.1,Batch number
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set sgd LR algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType incremental train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set log path
v1.5.1,Set actionType prediction
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Rank
v1.5.1,Regularization parameters
v1.5.1,Learn rage
v1.5.1,Set local deploy mode
v1.5.1,Set basic self.configuration keys
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set FM algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set learnType
v1.5.1,Set feature number
v1.5.1,import jdk.nashorn.internal.runtime.regexp.joni.Config;
v1.5.1,"paras[1] = ""abc"";"
v1.5.1,"paras[2] = ""123"";"
v1.5.1,Add standard Hadoop classes
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Learning rate
v1.5.1,Regularization coefficient
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set sgd LR algorithm parameters #feature #epoch
v1.5.1,Set input data path
v1.5.1,Set save model path
v1.5.1,Set actionType train
v1.5.1,Set PS Model values
v1.5.1,Wait for all tasks finish this clock
v1.5.1,Get values of index array
v1.5.1,Set PS Model values
v1.5.1,Wait for all tasks finish this clock
v1.5.1,Get values of index array
v1.5.1,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
v1.5.1,Feature number of train data
v1.5.1,Number of nonzero features
v1.5.1,Tree number
v1.5.1,Tree depth
v1.5.1,Split number
v1.5.1,Feature sample ratio
v1.5.1,Data format
v1.5.1,Learning rate
v1.5.1,Set basic configuration keys
v1.5.1,Use local deploy mode and data format
v1.5.1,"set input, output path"
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,Set GBDT algorithm parameters
v1.5.1,Load Model from HDFS.
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,"Set trainning data, and save model path"
v1.5.1,Set actionType train
v1.5.1,Set MF algorithm parameters
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Validation sample Ratio
v1.5.1,"Data format, libsvm or dummy"
v1.5.1,Train batch number per epoch.
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set sgd LR algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set actionType incremental train
v1.5.1,Set log path
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set actionType prediction
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Validation sample Ratio
v1.5.1,"Data format, libsvm or dummy"
v1.5.1,Train batch number per epoch.
v1.5.1,Batch number
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set sgd LR algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType incremental train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set log path
v1.5.1,Set actionType prediction
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Default location for user home directories #
v1.5.1,Default value for FS_HOME_DIR_KEY #
v1.5.1,Default umask for files created in HDFS #
v1.5.1,Default value for FS_PERMISSIONS_UMASK_KEY #
v1.5.1,How often does RPC client send pings to RPC server #
v1.5.1,Default value for IPC_PING_INTERVAL_KEY #
v1.5.1,Enables pings from RPC client to the server #
v1.5.1,Default value of IPC_CLIENT_PING_KEY #
v1.5.1,Responses larger than this will be logged #
v1.5.1,Default value for IPC_SERVER_RPC_MAX_RESPONSE_SIZE_KEY #
v1.5.1,Number of threads in RPC server reading from the socket #
v1.5.1,Default value for IPC_SERVER_RPC_READ_THREADS_KEY #
v1.5.1,How many calls per handler are allowed in the queue. #
v1.5.1,Default value for IPC_SERVER_HANDLER_QUEUE_SIZE_KEY #
v1.5.1,Internal buffer size for Lzo compressordecompressors #/
v1.5.1,Default value for IO_COMPRESSION_CODEC_LZO_BUFFERSIZE_KEY #
v1.5.1,This is for specifying the implementation for the mappings from
v1.5.1,hostnames to the racks they belong to
v1.5.1,Internal buffer size for Snappy compressordecompressors #/
v1.5.1,Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #
v1.5.1,Internal buffer size for Snappy compressordecompressors #/
v1.5.1,Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #
v1.5.1,Service Authorization
v1.5.1,HA health monitor and failover controller.
v1.5.1,How often to retry connecting to the service.
v1.5.1,How often to check the service.
v1.5.1,How long to sleep after an unexpected RPC error.
v1.5.1,Timeout for the actual monitorHealth() calls. *
v1.5.1,Timeout that the FC waits for the new active to become active
v1.5.1,Timeout that the FC waits for the old active to go to standby
v1.5.1,FC connection retries for graceful fencing
v1.5.1,"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState"
v1.5.1,Static user web-filter properties.
v1.5.1,See StaticUserWebFilter.
v1.5.1,EnableDisable aliases serving from jetty
v1.5.1,Path to the Kerberos ticket cache.  Setting this will force
v1.5.1,UserGroupInformation to use only this ticket cache file when creating a
v1.5.1,FileSystem instance.
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,"
v1.5.1,"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:"
v1.5.1,"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"","
v1.5.1,"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output"""
v1.5.1,Feature number of train data
v1.5.1,Number of nonzero features
v1.5.1,Tree number
v1.5.1,Tree depth
v1.5.1,Split number
v1.5.1,Feature sample ratio
v1.5.1,Data format
v1.5.1,Learning rate
v1.5.1,Set GBDT category feature
v1.5.1,"set input, output path"
v1.5.1,Set GBDT algorithm parameters
v1.5.1,Load Model from HDFS.
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Validation sample ratio
v1.5.1,"Data format,libsvm or dummy"
v1.5.1,Train batch number per epoch
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,"Set job queue, if you use YARN deploy mode, you can set job queue by"
v1.5.1,"self.conf.set('mapreduce.job.queue.name', 'default')"
v1.5.1,Set local deploy mode
v1.5.1,Set basic self.configuration keys
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #tast, #ps"
v1.5.1,set sgd LR algorithim parameters # feature # epoch
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set actionType incremental train
v1.5.1,Set log path
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set actionType prediction
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Set local deploy mode
v1.5.1,Set basic self.configuration keys
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,"Set trainning data, and save model path"
v1.5.1,Set actionType train
v1.5.1,Set MF algorithm parameters
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.5.1,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Load Model from HDFS.
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Cluster center number
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Sample ratio per mini-batch
v1.5.1,C
v1.5.1,Set local deploy mode
v1.5.1,Set basic self.configuration key
v1.5.1,"Set angel resource parameters #worker, #task, #PS"
v1.5.1,Set Kmeans algorithm parameters #cluster #feature #epoch
v1.5.1,Set data format
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log sava path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set actionType prediction
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Validation sample Ratio
v1.5.1,"Data format, libsvm or dummy"
v1.5.1,Train batch number per epoch.
v1.5.1,Batch number
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set sgd LR algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType incremental train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set log path
v1.5.1,Set actionType prediction
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Rank
v1.5.1,Regularization parameters
v1.5.1,Learn rage
v1.5.1,Set local deploy mode
v1.5.1,Set basic self.configuration keys
v1.5.1,"Set angel resource parameters #worker, #task, #PS"
v1.5.1,Set FM algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set learnType
v1.5.1,Set feature number
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,If the enviroment has ANGEL_HOME set trust it.
v1.5.1,Add the path of the PyAngel module if it exists
v1.5.1,If we are installed in edit mode also look two dirs up
v1.5.1,Not pip installed no worries
v1.5.1,If we are installed in edit mode also look two dirs up
v1.5.1,Not pip installed no worries
v1.5.1,Normalize the paths
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Get Java HashMap instance which converted from a python dict
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Run ParameterServer  & ParameterServerAgent
v1.5.1,Only Run ParameterServer
v1.5.1,Run ParameterServer & Worker(embedded ParameterServerAgent)
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,To Do
v1.5.1,Modify the way to get current Angel version
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Launch the Py4j gateway
v1.5.1,Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc
v1.5.1,Don't send ctrl-c / SIGINT to the Java gateway:
v1.5.1,We use select() here in order to avoid blocking indefinitely if the subprocess dies
v1.5.1,before connecting
v1.5.1,Determine which ephemeral port the server started on:
v1.5.1,Connect to the gateway
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.5.1,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.5.1,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,To do: need python edition of TVector
v1.5.1,To do: need python edition of GetFunc
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Create an angel job client
v1.5.1,Submit this application
v1.5.1,Create a KMeans model
v1.5.1,Load model meta to client
v1.5.1,Start
v1.5.1,"Run user task and wait for completion,"
v1.5.1,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.5.1,Save the trained model to HDFS
v1.5.1,Stop
v1.5.1,Create an angel job client
v1.5.1,Submit this application
v1.5.1,Create KMeans model
v1.5.1,Add the model meta to client
v1.5.1,Start
v1.5.1,"Run user task and wait for completion,"
v1.5.1,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.5.1,Stop
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Training job to obtain a model
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Create an angel job client
v1.5.1,Submit this application
v1.5.1,Create a model
v1.5.1,Load model meta to client
v1.5.1,Run user task
v1.5.1,"Wait for completion,"
v1.5.1,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.5.1,Save the incremental trained model to HDFS
v1.5.1,Stop
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,If the enviroment has ANGEL_HOME set trust it.
v1.5.1,Add the path of the PyAngel module if it exists
v1.5.1,If we are installed in edit mode also look two dirs up
v1.5.1,Not pip installed no worries
v1.5.1,If we are installed in edit mode also look two dirs up
v1.5.1,Not pip installed no worries
v1.5.1,Normalize the paths
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Get Java HashMap instance which converted from a python dict
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Run ParameterServer  & ParameterServerAgent
v1.5.1,Only Run ParameterServer
v1.5.1,Run ParameterServer & Worker(embedded ParameterServerAgent)
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,To Do
v1.5.1,Modify the way to get current Angel version
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Launch the Py4j gateway
v1.5.1,Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc
v1.5.1,Don't send ctrl-c / SIGINT to the Java gateway:
v1.5.1,We use select() here in order to avoid blocking indefinitely if the subprocess dies
v1.5.1,before connecting
v1.5.1,Determine which ephemeral port the server started on:
v1.5.1,Connect to the gateway
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.5.1,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,To do: need python edition of TVector
v1.5.1,To do: need python edition of GetFunc
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Training job to obtain a model
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https:#opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Create an angel job client
v1.5.1,Submit this application
v1.5.1,Create a model
v1.5.1,Load model meta to client
v1.5.1,Run user task
v1.5.1,"Wait for completion,"
v1.5.1,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.5.1,Save the incremental trained model to HDFS
v1.5.1,Stop
v1.5.1,
v1.5.1,Tencent is pleased to support the open source community by making Angel available.
v1.5.1,
v1.5.1,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.1,
v1.5.1,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.1,compliance with the License. You may obtain a copy of the License at
v1.5.1,
v1.5.1,https://opensource.org/licenses/BSD-3-Clause
v1.5.1,
v1.5.1,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.1,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.1,either express or implied. See the License for the specific language governing permissions and
v1.5.1,
v1.5.1,Load model meta
v1.5.1,Convert model
v1.5.1,"Get input path, output path"
v1.5.1,Init serde
v1.5.1,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.5.1,Load model meta
v1.5.1,Convert model
v1.5.1,load hadoop configuration
v1.5.1,"Get input path, output path"
v1.5.1,Init serde
v1.5.1,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.5.1,Load model meta
v1.5.1,Check row type
v1.5.1,Load model
v1.5.1,Load model meta
v1.5.1,Check row type
v1.5.1,Load model
v1.5.1,Load model meta
v1.5.1,Check row type
v1.5.1,Load model
v1.5.1,Load model meta
v1.5.1,Check row type
v1.5.1,Load model
v1.5.1,Load model meta
v1.5.1,Check row type
v1.5.1,Load model
v1.5.1,Load model meta
v1.5.1,Check row type
v1.5.1,Load model
v1.5.1,Load model meta
v1.5.1,Check row type
v1.5.1,Load model
v1.5.1,Load model
v1.5.1,load hadoop configuration
v1.5.1,mMatrix.setNnz(100000000);
v1.5.1,mMatrix.setNnz(100000000);
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,worker register
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,add matrix
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,attempt 0
v1.5.1,attempt1
v1.5.1,attempt1
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,TODO Auto-generated constructor stub
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,set basic configuration keys
v1.5.1,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,Thread.sleep(5000);
v1.5.1,"response = master.getJobReport(null, request);"
v1.5.1,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
v1.5.1,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
v1.5.1,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,"DenseIntVector deltaVec = new DenseIntVector(100000, delta);"
v1.5.1,deltaVec.setMatrixId(matrixW1Id);
v1.5.1,deltaVec.setRowId(0);
v1.5.1,TODO Auto-generated constructor stub
v1.5.1,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
v1.5.1,@RunWith(MockitoJUnitRunner.class)
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,psAgent.initAndStart();
v1.5.1,test conf
v1.5.1,test master location
v1.5.1,test app id
v1.5.1,test user
v1.5.1,test ps agent attempt id
v1.5.1,test connection
v1.5.1,test master client
v1.5.1,test ip
v1.5.1,test loc
v1.5.1,test master location
v1.5.1,test ps location
v1.5.1,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
v1.5.1,test all ps ids
v1.5.1,test all matrix ids
v1.5.1,test all matrix names
v1.5.1,test matrix attribute
v1.5.1,test matrix meta
v1.5.1,test ps location
v1.5.1,test partitions
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,group0Id = new WorkerGroupId(0);
v1.5.1,"worker0Id = new WorkerId(group0Id, 0);"
v1.5.1,"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);"
v1.5.1,task0Id = new TaskId(0);
v1.5.1,task1Id = new TaskId(1);
v1.5.1,test this func in testWriteTo
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,"LOG.info(index[0] + "" "" + value[0]);"
v1.5.1,"LOG.info(index[1] + "" "" + value[1]);"
v1.5.1,"LOG.info(index[2] + "" "" + value[2]);"
v1.5.1,dot
v1.5.1,plus
v1.5.1,plusBy
v1.5.1,dot
v1.5.1,plus
v1.5.1,plusBy
v1.5.1,dot
v1.5.1,plus
v1.5.1,plusBy
v1.5.1,dot
v1.5.1,plusBy
v1.5.1,@Test
v1.5.1,public void dotDenseFloatVector() throws Exception {
v1.5.1,int dim = 1000;
v1.5.1,Random random = new Random(System.currentTimeMillis());
v1.5.1,
v1.5.1,double[] values = new double[dim];
v1.5.1,float[] values_1 = new float[dim];
v1.5.1,for (int i = 0; i < dim; i++) {
v1.5.1,values[i] = random.nextDouble();
v1.5.1,values_1[i] = random.nextFloat();
v1.5.1,}
v1.5.1,
v1.5.1,"DenseDoubleVector vec = new DenseDoubleVector(dim, values);"
v1.5.1,"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);"
v1.5.1,
v1.5.1,double sum = 0.0;
v1.5.1,for (int i = 0; i < dim; i++) {
v1.5.1,sum += values[i] * values_1[i];
v1.5.1,}
v1.5.1,
v1.5.1,"assertEquals(sum, vec.dot(vec_1));"
v1.5.1,
v1.5.1,}
v1.5.1,@Test
v1.5.1,public void plusDenseFlaotVector() throws Exception {
v1.5.1,"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};"
v1.5.1,"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};"
v1.5.1,"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);"
v1.5.1,"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);"
v1.5.1,
v1.5.1,TDoubleVector vec_2 = vec.plus(vec_1);
v1.5.1,for (int i = 0; i < vec.size(); i++)
v1.5.1,"assertEquals(value_1[i] + value_2[i], vec_2.get(i));"
v1.5.1,
v1.5.1,
v1.5.1,"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);"
v1.5.1,
v1.5.1,for (int i = 0; i < vec.size(); i++)
v1.5.1,"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);"
v1.5.1,
v1.5.1,double[] oldValues = vec.getValues().clone();
v1.5.1,
v1.5.1,vec.plusBy(vec_1);
v1.5.1,
v1.5.1,for (int i = 0; i < vec.size(); i++)
v1.5.1,"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));"
v1.5.1,
v1.5.1,oldValues = vec.getValues().clone();
v1.5.1,
v1.5.1,"vec.plusBy(vec_1, 3);"
v1.5.1,
v1.5.1,for (int i = 0; i < vec.size(); i++)
v1.5.1,"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));"
v1.5.1,}
v1.5.1,dot
v1.5.1,plus
v1.5.1,plusBy
v1.5.1,dot
v1.5.1,plus
v1.5.1,plusBy
v1.5.1,@Test
v1.5.1,public void plusBy3() throws Exception {
v1.5.1,"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};"
v1.5.1,"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);"
v1.5.1,"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});"
v1.5.1,vec.setRowId(0);
v1.5.1,"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});"
v1.5.1,vec_1.setRowId(1);
v1.5.1,TDoubleVector vec_2 = new SparseDoubleVector(2);
v1.5.1,"vec_2.set(1, 1.0);"
v1.5.1,vec_2.setRowId(0);
v1.5.1,
v1.5.1,mat.plusBy(vec);
v1.5.1,mat.plusBy(vec_1);
v1.5.1,mat.plusBy(vec_2);
v1.5.1,
v1.5.1,"assertEquals(2.0f, mat.get(0, 0));"
v1.5.1,"assertEquals(4.0f, mat.get(0, 1));"
v1.5.1,"assertEquals(4.0f, mat.get(1, 0));"
v1.5.1,"assertEquals(5.0f, mat.get(1, 1));"
v1.5.1,}
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add dense double matrix
v1.5.1,add sparse double matrix
v1.5.1,add component sparse double matrix
v1.5.1,add sparse long-key double matrix
v1.5.1,add component long-key sparse double matrix
v1.5.1,add dense float matrix
v1.5.1,add sparse float matrix
v1.5.1,add component sparse float matrix
v1.5.1,add dense float matrix
v1.5.1,add sparse float matrix
v1.5.1,add component sparse float matrix
v1.5.1,Start PS
v1.5.1,Start to run application
v1.5.1,Assert.assertTrue(index.length == row.size());
v1.5.1,Assert.assertTrue(index.length == row.size());
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,test worker getActiveTaskNum
v1.5.1,test worker getTaskNum
v1.5.1,test worker getTaskManager
v1.5.1,test workerId
v1.5.1,test workerAttemptId
v1.5.1,tet worker initFinished
v1.5.1,test worker getInitMinclock
v1.5.1,test worker loacation
v1.5.1,test AppId
v1.5.1,test Conf
v1.5.1,test UserName
v1.5.1,master location
v1.5.1,masterClient
v1.5.1,test psAgent
v1.5.1,test worker get dataBlockManager
v1.5.1,workerGroup.getSplits();
v1.5.1,application
v1.5.1,lcation
v1.5.1,workerGroup info
v1.5.1,worker info
v1.5.1,task
v1.5.1,Matrix parameters
v1.5.1,Set basic configuration keys
v1.5.1,Use local deploy mode and dummy data spliter
v1.5.1,Create an Angel client
v1.5.1,Add different types of matrix
v1.5.1,using mock object
v1.5.1,verification
v1.5.1,Stubbing
v1.5.1,Default does nothing.
v1.5.1,The app injection is optional
v1.5.1,"renderText(""hello world"");"
v1.5.1,"user choose a workerGroupID from the workergroups page,"
v1.5.1,now we should change the AngelApp params and render the workergroup page;
v1.5.1,"static final String WORKER_ID = ""worker.id"";"
v1.5.1,"div(""#logo"")."
v1.5.1,"img(""/static/hadoop-st.png"")._()."
v1.5.1,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
v1.5.1,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
v1.5.1,JQueryUI.jsnotice(html);
v1.5.1,import org.apache.hadoop.conf.Configuration;
v1.5.1,import java.lang.reflect.Field;
v1.5.1,get block locations from file system
v1.5.1,create a list of all block and their locations
v1.5.1,"if the file is not splitable, just create the one block with"
v1.5.1,full file length
v1.5.1,each split can be a maximum of maxSize
v1.5.1,if remainder is between max and 2*max - then
v1.5.1,"instead of creating splits of size max, left-max we"
v1.5.1,create splits of size left/2 and left/2. This is
v1.5.1,a heuristic to avoid creating really really small
v1.5.1,splits.
v1.5.1,add this block to the block --> node locations map
v1.5.1,"For blocks that do not have host/rack information,"
v1.5.1,assign to default  rack.
v1.5.1,add this block to the rack --> block map
v1.5.1,Add this host to rackToNodes map
v1.5.1,add this block to the node --> block map
v1.5.1,"if the file system does not have any rack information, then"
v1.5.1,use dummy rack location.
v1.5.1,The topology paths have the host name included as the last
v1.5.1,component. Strip it.
v1.5.1,get tokens for all the required FileSystems..
v1.5.1,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
v1.5.1,job.getConfiguration());
v1.5.1,Whether we need to recursive look into the directory structure
v1.5.1,creates a MultiPathFilter with the hiddenFileFilter and the
v1.5.1,user provided one (if any).
v1.5.1,all the files in input set
v1.5.1,it is allowed for maxSize to be 0. Disable smoothing load for such cases
v1.5.1,process all nodes and create splits that are local to a node. Generate
v1.5.1,"one split per node iteration, and walk over nodes multiple times to"
v1.5.1,distribute the splits across nodes.
v1.5.1,Skip the node if it has previously been marked as completed.
v1.5.1,"for each block, copy it into validBlocks. Delete it from"
v1.5.1,blockToNodes so that the same block does not appear in
v1.5.1,two different splits.
v1.5.1,Remove all blocks which may already have been assigned to other
v1.5.1,splits.
v1.5.1,"if the accumulated split size exceeds the maximum, then"
v1.5.1,create this split.
v1.5.1,create an input split and add it to the splits array
v1.5.1,Remove entries from blocksInNode so that we don't walk these
v1.5.1,again.
v1.5.1,Done creating a single split for this node. Move on to the next
v1.5.1,node so that splits are distributed across nodes.
v1.5.1,This implies that the last few blocks (or all in case maxSize=0)
v1.5.1,were not part of a split. The node is complete.
v1.5.1,if there were any blocks left over and their combined size is
v1.5.1,"larger than minSplitNode, then combine them into one split."
v1.5.1,Otherwise add them back to the unprocessed pool. It is likely
v1.5.1,that they will be combined with other blocks from the
v1.5.1,same rack later on.
v1.5.1,This condition also kicks in when max split size is not set. All
v1.5.1,blocks on a node will be grouped together into a single split.
v1.5.1,haven't created any split on this machine. so its ok to add a
v1.5.1,smaller one for parallelism. Otherwise group it in the rack for
v1.5.1,balanced size create an input split and add it to the splits
v1.5.1,array
v1.5.1,Remove entries from blocksInNode so that we don't walk this again.
v1.5.1,The node is done. This was the last set of blocks for this node.
v1.5.1,Put the unplaced blocks back into the pool for later rack-allocation.
v1.5.1,Node is done. All blocks were fit into node-local splits.
v1.5.1,Check if node-local assignments are complete.
v1.5.1,All nodes have been walked over and marked as completed or all blocks
v1.5.1,have been assigned. The rest should be handled via rackLock assignment.
v1.5.1,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
v1.5.1,"+ completedNodes.size() + "", size left: "" + totalLength);"
v1.5.1,"if blocks in a rack are below the specified minimum size, then keep them"
v1.5.1,"in 'overflow'. After the processing of all racks is complete, these"
v1.5.1,overflow blocks will be combined into splits.
v1.5.1,Process all racks over and over again until there is no more work to do.
v1.5.1,Create one split for this rack before moving over to the next rack.
v1.5.1,Come back to this rack after creating a single split for each of the
v1.5.1,remaining racks.
v1.5.1,"Process one rack location at a time, Combine all possible blocks that"
v1.5.1,reside on this rack as one split. (constrained by minimum and maximum
v1.5.1,split size).
v1.5.1,iterate over all racks
v1.5.1,"for each block, copy it into validBlocks. Delete it from"
v1.5.1,blockToNodes so that the same block does not appear in
v1.5.1,two different splits.
v1.5.1,"if the accumulated split size exceeds the maximum, then"
v1.5.1,create this split.
v1.5.1,create an input split and add it to the splits array
v1.5.1,"if we created a split, then just go to the next rack"
v1.5.1,"if there is a minimum size specified, then create a single split"
v1.5.1,"otherwise, store these blocks into overflow data structure"
v1.5.1,There were a few blocks in this rack that
v1.5.1,remained to be processed. Keep them in 'overflow' block list.
v1.5.1,These will be combined later.
v1.5.1,Process all overflow blocks
v1.5.1,"This might cause an exiting rack location to be re-added,"
v1.5.1,but it should be ok.
v1.5.1,"if the accumulated split size exceeds the maximum, then"
v1.5.1,create this split.
v1.5.1,create an input split and add it to the splits array
v1.5.1,"Process any remaining blocks, if any."
v1.5.1,create an input split
v1.5.1,add this split to the list that is returned
v1.5.1,long num = totLength / maxSize;
v1.5.1,all blocks for all the files in input set
v1.5.1,mapping from a rack name to the list of blocks it has
v1.5.1,mapping from a block to the nodes on which it has replicas
v1.5.1,mapping from a node to the list of blocks that it contains
v1.5.1,populate all the blocks for all files
v1.5.1,stop all services
v1.5.1,1.write application state to file so that the client can get the state of the application
v1.5.1,if master exit
v1.5.1,2.clear tmp and staging directory
v1.5.1,waiting for client to get application state
v1.5.1,stop the RPC server
v1.5.1,"Security framework already loaded the tokens into current UGI, just use"
v1.5.1,them
v1.5.1,Now remove the AM->RM token so tasks don't have it
v1.5.1,add a shutdown hook
v1.5.1,init app state storage
v1.5.1,init event dispacher
v1.5.1,init location manager
v1.5.1,init container allocator
v1.5.1,init a rpc service
v1.5.1,recover matrix meta if needed
v1.5.1,recover ps attempt information if need
v1.5.1,Init Client manager
v1.5.1,Init PS Client manager
v1.5.1,init parameter server manager
v1.5.1,recover task information if needed
v1.5.1,a dummy data spliter is just for test now
v1.5.1,recover data splits information if needed
v1.5.1,init worker manager and register worker manager event
v1.5.1,register slow worker/ps checker
v1.5.1,register app manager event and finish event
v1.5.1,start a web service if use yarn deploy mode
v1.5.1,load from app state storage first if attempt index great than 1(the master is not the first
v1.5.1,retry)
v1.5.1,"if load failed, just build a new MatrixMetaManager"
v1.5.1,load ps attempt index from app state storage first if attempt index great than 1(the master
v1.5.1,is not the first retry)
v1.5.1,load task information from app state storage first if attempt index great than 1(the master
v1.5.1,is not the first retry)
v1.5.1,"if load failed, just build a new AMTaskManager"
v1.5.1,load data splits information from app state storage first if attempt index great than 1(the
v1.5.1,master is not the first retry)
v1.5.1,"if load failed, we need to recalculate the data splits"
v1.5.1,Check Workers
v1.5.1,Check PSS
v1.5.1,Check Clients
v1.5.1,Check PS Clients
v1.5.1,parse parameter server counters
v1.5.1,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.5.1,refresh last heartbeat timestamp
v1.5.1,send a state update event to the specific PSAttempt
v1.5.1,check if parameter server can commit now.
v1.5.1,check matrix metadata inconsistencies between master and parameter server.
v1.5.1,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
v1.5.1,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
v1.5.1,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.5.1,choose a unused port
v1.5.1,start RPC server
v1.5.1,remove this parameter server attempt from monitor set
v1.5.1,remove this parameter server attempt from monitor set
v1.5.1,"if worker attempt id is not in monitor set, we should shutdown it"
v1.5.1,find workergroup in worker manager
v1.5.1,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
v1.5.1,"if this worker group run over, just return WORKERGROUP_EXITED"
v1.5.1,"if this worker group is running now, return tasks, workers, data splits for it"
v1.5.1,"if worker attempt id is not in monitor set, we should shutdown it"
v1.5.1,"if worker attempt id is not in monitor set, we should shutdown it"
v1.5.1,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.5.1,"in ANGEL_PS mode, task id may can not know advance"
v1.5.1,update the clock for this matrix
v1.5.1,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.5.1,"in ANGEL_PS mode, task id may can not know advance"
v1.5.1,update task iteration
v1.5.1,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
v1.5.1,the number of splits equal to the number of tasks
v1.5.1,split data
v1.5.1,dispatch the splits to workergroups
v1.5.1,split data
v1.5.1,dispatch the splits to workergroups
v1.5.1,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.5.1,"first, then divided by expected split number"
v1.5.1,get input format class from configuration and then instantiation a input format object
v1.5.1,split data
v1.5.1,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.5.1,"first, then divided by expected split number"
v1.5.1,get input format class from configuration and then instantiation a input format object
v1.5.1,split data
v1.5.1,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.5.1,need to fine tune the number of workergroup and task based on the actual split number
v1.5.1,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.5.1,Record the location information for the splits in order to data localized schedule
v1.5.1,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.5.1,need to fine tune the number of workergroup and task based on the actual split number
v1.5.1,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.5.1,Record the location information for the splits in order to data localized schedule
v1.5.1,write meta data to a temporary file
v1.5.1,rename the temporary file to final file
v1.5.1,"if the file exists, read from file and deserialize it"
v1.5.1,write task meta
v1.5.1,write ps meta
v1.5.1,generate a temporary file
v1.5.1,write task meta to the temporary file first
v1.5.1,rename the temporary file to the final file
v1.5.1,"if last final task file exist, remove it"
v1.5.1,find task meta file which has max timestamp
v1.5.1,"if the file does not exist, just return null"
v1.5.1,read task meta from file and deserialize it
v1.5.1,generate a temporary file
v1.5.1,write ps meta to the temporary file first.
v1.5.1,rename the temporary file to the final file
v1.5.1,"if the old final file exist, just remove it"
v1.5.1,find ps meta file
v1.5.1,"if ps meta file does not exist, just return null"
v1.5.1,read ps meta from file and deserialize it
v1.5.1,Init matrix files meta
v1.5.1,Move output files
v1.5.1,Write the meta file
v1.5.1,check whether psagent heartbeat timeout
v1.5.1,Set up the launch command
v1.5.1,Duplicate the ByteBuffers for access by multiple containers.
v1.5.1,Construct the actual Container
v1.5.1,Application resources
v1.5.1,Application environment
v1.5.1,Service data
v1.5.1,Tokens
v1.5.1,Set up JobConf to be localized properly on the remote NM.
v1.5.1,Setup DistributedCache
v1.5.1,Setup up task credentials buffer
v1.5.1,LocalStorageToken is needed irrespective of whether security is enabled
v1.5.1,or not.
v1.5.1,"TokenCache.setJobToken(jobToken, taskCredentials);"
v1.5.1,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.5.1,Construct the actual Container
v1.5.1,The null fields are per-container and will be constructed for each
v1.5.1,container separately.
v1.5.1,Set up the launch command
v1.5.1,Duplicate the ByteBuffers for access by multiple containers.
v1.5.1,Construct the actual Container
v1.5.1,"a * in the classpath will only find a .jar, so we need to filter out"
v1.5.1,all .jars and add everything else
v1.5.1,Propagate the system classpath when using the mini cluster
v1.5.1,Add standard Hadoop classes
v1.5.1,Add mr
v1.5.1,Cache archives
v1.5.1,Cache files
v1.5.1,Sanity check
v1.5.1,Add URI fragment or just the filename
v1.5.1,Add the env variables passed by the user
v1.5.1,Set logging level in the environment.
v1.5.1,Setup the log4j prop
v1.5.1,Add main class and its arguments
v1.5.1,Finally add the jvmID
v1.5.1,vargs.add(String.valueOf(jvmID.getId()));
v1.5.1,Final commmand
v1.5.1,Add the env variables passed by the user
v1.5.1,Set logging level in the environment.
v1.5.1,Setup the log4j prop
v1.5.1,Add main class and its arguments
v1.5.1,Final commmand
v1.5.1,"if amTask is not null, we should clone task state from it"
v1.5.1,"if all parameter server complete commit, master can commit now"
v1.5.1,init and start master committer
v1.5.1,check whether parameter server heartbeat timeout
v1.5.1,Transitions from the NEW state.
v1.5.1,Transitions from the UNASSIGNED state.
v1.5.1,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
v1.5.1,Transitions from the ASSIGNED state.
v1.5.1,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
v1.5.1,PA_CONTAINER_LAUNCHED event
v1.5.1,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.5.1,Transitions from the PSAttemptStateInternal.COMMITTING state
v1.5.1,Transitions from the PSAttemptStateInternal.KILLED state
v1.5.1,Transitions from the PSAttemptStateInternal.FAILED state
v1.5.1,create the topology tables
v1.5.1,reqeuest resource:send a resource request to the resource allocator
v1.5.1,"Once the resource is applied, build and send the launch request to the container launcher"
v1.5.1,deallocator the resource of the ps attempt:send a resource deallocator request to the
v1.5.1,resource allocator
v1.5.1,set the launch time
v1.5.1,add the ps attempt to the heartbeat timeout monitoring list
v1.5.1,parse ps attempt location and put it to location manager
v1.5.1,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
v1.5.1,or failed
v1.5.1,remove ps attempt id from heartbeat timeout monitor list
v1.5.1,release container:send a release request to container launcher
v1.5.1,set the finish time only if launch time is set
v1.5.1,private long scheduledTime;
v1.5.1,Transitions from the NEW state.
v1.5.1,Transitions from the SCHEDULED state.
v1.5.1,Transitions from the RUNNING state.
v1.5.1,"another attempt launched,"
v1.5.1,Transitions from the SUCCEEDED state
v1.5.1,Transitions from the KILLED state
v1.5.1,Transitions from the FAILED state
v1.5.1,add diagnostic
v1.5.1,check whether the number of failed attempts is less than the maximum number of allowed
v1.5.1,Refresh ps location & matrix meta
v1.5.1,start a new attempt for this ps
v1.5.1,notify ps manager
v1.5.1,"getContext().getLocationManager().setPsLocation(id, null);"
v1.5.1,add diagnostic
v1.5.1,check whether the number of failed attempts is less than the maximum number of allowed
v1.5.1,start a new attempt for this ps
v1.5.1,notify ps manager
v1.5.1,notify the event handler of state change
v1.5.1,"If new state is not RUNNING and COMMITTING, add it to state timeout monitor"
v1.5.1,"if forcedState is set, just return"
v1.5.1,else get state from state machine
v1.5.1,add this worker group to the success set
v1.5.1,check if all worker group run over
v1.5.1,add this worker group to the failed set
v1.5.1,check if too many worker groups are failed or killed
v1.5.1,notify a run failed event
v1.5.1,add this worker group to the failed set
v1.5.1,check if too many worker groups are failed or killed
v1.5.1,notify a run failed event
v1.5.1,calculate the actual number of worker groups and the total number of tasks based on the number of data split
v1.5.1,"init all tasks , workers and worker groups and put them to the corresponding maps"
v1.5.1,just return the total task number now
v1.5.1,TODO
v1.5.1,check whether worker heartbeat timeout
v1.5.1,"if workerAttempt is not null, we should clone task state from it"
v1.5.1,from NEW state
v1.5.1,from SCHEDULED state
v1.5.1,get data splits location for data locality
v1.5.1,reqeuest resource:send a resource request to the resource allocator
v1.5.1,"once the resource is applied, build and send the launch request to the container launcher"
v1.5.1,notify failed message to the worker
v1.5.1,notify killed message to the worker
v1.5.1,release the allocated container
v1.5.1,notify failed message to the worker
v1.5.1,remove the worker attempt from heartbeat timeout listen list
v1.5.1,release the allocated container
v1.5.1,notify killed message to the worker
v1.5.1,remove the worker attempt from heartbeat timeout listen list
v1.5.1,clean the container
v1.5.1,notify failed message to the worker
v1.5.1,remove the worker attempt from heartbeat timeout listen list
v1.5.1,record the finish time
v1.5.1,clean the container
v1.5.1,notify killed message to the worker
v1.5.1,remove the worker attempt from heartbeat timeout listening list
v1.5.1,record the finish time
v1.5.1,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
v1.5.1,set worker attempt location
v1.5.1,notify the register message to the worker
v1.5.1,record the launch time
v1.5.1,update worker attempt metrics
v1.5.1,update tasks metrics
v1.5.1,clean the container
v1.5.1,notify the worker attempt run successfully message to the worker
v1.5.1,record the finish time
v1.5.1,init a worker attempt for the worker
v1.5.1,schedule the worker attempt
v1.5.1,add diagnostic
v1.5.1,check whether the number of failed attempts is less than the maximum number of allowed
v1.5.1,init and start a new attempt for this ps
v1.5.1,notify worker manager
v1.5.1,add diagnostic
v1.5.1,check whether the number of failed attempts is less than the maximum number of allowed
v1.5.1,init and start a new attempt for this ps
v1.5.1,notify worker manager
v1.5.1,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
v1.5.1,register to Yarn RM
v1.5.1,send heartbeat to Yarn RM every rmPollInterval milliseconds
v1.5.1,"catch YarnRuntimeException, we should exit and need not retry"
v1.5.1,build heartbeat request
v1.5.1,send heartbeat request to rm
v1.5.1,"This can happen if the RM has been restarted. If it is in that state,"
v1.5.1,this application must clean itself up.
v1.5.1,Setting NMTokens
v1.5.1,assgin containers
v1.5.1,"if some container is not assigned, release them"
v1.5.1,handle finish containers
v1.5.1,dispatch container exit message to corresponding components
v1.5.1,killed by framework
v1.5.1,killed by framework
v1.5.1,get application finish state
v1.5.1,build application diagnostics
v1.5.1,TODO:add a job history for angel
v1.5.1,build unregister request
v1.5.1,send unregister request to rm
v1.5.1,Note this down for next interaction with ResourceManager
v1.5.1,based on blacklisting comments above we can end up decrementing more
v1.5.1,than requested. so guard for that.
v1.5.1,send the updated resource request to RM
v1.5.1,send 0 container count requests also to cancel previous requests
v1.5.1,Update resource requests
v1.5.1,try to assign to all nodes first to match node local
v1.5.1,try to match all rack local
v1.5.1,assign remaining
v1.5.1,Update resource requests
v1.5.1,send the container-assigned event to task attempt
v1.5.1,build the start container request use launch context
v1.5.1,send the start request to Yarn nm
v1.5.1,send the message that the container starts successfully to the corresponding component
v1.5.1,"after launching, send launched event to task attempt to move"
v1.5.1,it from ASSIGNED to RUNNING state
v1.5.1,send the message that the container starts failed to the corresponding component
v1.5.1,kill the remote container if already launched
v1.5.1,start a thread pool to startup the container
v1.5.1,See if we need up the pool size only if haven't reached the
v1.5.1,maximum limit yet.
v1.5.1,nodes where containers will run at *this* point of time. This is
v1.5.1,*not* the cluster size and doesn't need to be.
v1.5.1,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
v1.5.1,later is just a buffer so we are not always increasing the
v1.5.1,pool-size
v1.5.1,the events from the queue are handled in parallel
v1.5.1,using a thread pool
v1.5.1,return if already stopped
v1.5.1,shutdown any containers that might be left running
v1.5.1,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.5.1,get matrix ids in the parameter server report
v1.5.1,get the matrices parameter server need to create and delete
v1.5.1,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
v1.5.1,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
v1.5.1,Init control connection manager
v1.5.1,Get ps locations from master and put them to the location cache.
v1.5.1,Build and initialize rpc client to master
v1.5.1,Get psagent id
v1.5.1,Build PS control rpc client manager
v1.5.1,Build local location
v1.5.1,Initialize matrix meta information
v1.5.1,Start all services
v1.5.1,Stop all modules
v1.5.1,Stop all modules
v1.5.1,Update generic resource counters
v1.5.1,Updating resources specified in ResourceCalculatorProcessTree
v1.5.1,Remove the CPU time consumed previously by JVM reuse
v1.5.1,Generate a flush request and put it to request queue
v1.5.1,Generate a clock request and put it to request queue
v1.5.1,Generate a merge request and put it to request queue
v1.5.1,"If the matrix op log cache does not exist for the matrix, create a new one for the"
v1.5.1,matrix
v1.5.1,and add it to cache maps
v1.5.1,Add the message to the tree map
v1.5.1,"If there are flush / clock requests blocked, we need to put this merge request into"
v1.5.1,the waiting queue
v1.5.1,Launch a merge worker to merge the update to matrix op log cache
v1.5.1,Remove the message from the tree map
v1.5.1,Wake up blocked flush/clock request
v1.5.1,Add flush/clock request to listener list to waiting for all the existing
v1.5.1,updates are merged
v1.5.1,Wake up blocked flush/clock request
v1.5.1,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
v1.5.1,blocked.
v1.5.1,Get next merge message sequence id
v1.5.1,Wake up listeners(flush/clock requests) that have little sequence id than current merge
v1.5.1,position
v1.5.1,Wake up blocked merge requests
v1.5.1,Get minimal sequence id from listeners
v1.5.1,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
v1.5.1,should flush updates to local matrix storage
v1.5.1,unused now
v1.5.1,"Filter it, removing zero values"
v1.5.1,Doing average or not
v1.5.1,Split this row according the matrix partitions
v1.5.1,Add the splits to the result container
v1.5.1,"For each partition, we generate a update split."
v1.5.1,"Although the split is empty for partitions those without any update data,"
v1.5.1,we still need to generate a update split to update the clock info on ps.
v1.5.1,"For each partition, we generate a update split."
v1.5.1,"Although the split is empty for partitions those without any update data,"
v1.5.1,we still need to generate a update split to update the clock info on ps.
v1.5.1,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
v1.5.1,TODO: use Epoll for linux future
v1.5.1,Update location table
v1.5.1,Remove the server from failed list
v1.5.1,Notify refresh success message to request dispatcher
v1.5.1,Check PS exist or not
v1.5.1,Check heartbeat timeout
v1.5.1,Check PS restart or not
v1.5.1,private final HashSet<ParameterServerId> refreshingServerSet;
v1.5.1,Add it to failed rpc list
v1.5.1,Add the server to gray server list
v1.5.1,Add it to failed rpc list
v1.5.1,Add the server to gray server list
v1.5.1,Move from gray server list to failed server list
v1.5.1,Handle the RPCS to this server
v1.5.1,Submit the schedulable failed get RPCS
v1.5.1,Submit new get RPCS
v1.5.1,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.5.1,"If the queue is empty, just return 0"
v1.5.1,"If request is not over limit, just submit it"
v1.5.1,Submit the schedulable failed get RPCS
v1.5.1,Submit new put RPCS
v1.5.1,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.5.1,"LOG.info(""choose put server "" + psIds[index]);"
v1.5.1,Check all pending RPCS
v1.5.1,Check get channel context
v1.5.1,Check all failed PUT RPCS and put it to schedulable list for re-schedule
v1.5.1,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
v1.5.1,Check all failed PUT RPCS and put it to schedulable list for re-schedule
v1.5.1,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
v1.5.1,channelManager.printPools();
v1.5.1,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
v1.5.1,if(ts - entry.getValue() > requestTimeOut * 2)  {
v1.5.1,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
v1.5.1,"+ "" milliseconds, close all channels to it"");"
v1.5.1,closeChannels(entry.getKey());
v1.5.1,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
v1.5.1,}
v1.5.1,}
v1.5.1,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
v1.5.1,Remove all pending RPCS
v1.5.1,Close all channel to this PS
v1.5.1,Get server id and location for this request
v1.5.1,"If location is null, means that the server is not ready"
v1.5.1,Get the channel for the location
v1.5.1,Check if need get token first
v1.5.1,Serialize the request
v1.5.1,Send the request
v1.5.1,get a channel to server from pool
v1.5.1,"if channel is not valid, it means maybe the connections to the server are closed"
v1.5.1,Allocate the bytebuf and serialize the request
v1.5.1,find the partition request context from cache
v1.5.1,Check if the result of the sub-request is received
v1.5.1,Update received result number
v1.5.1,Get row splits received
v1.5.1,Put the row split to the cache(row index to row splits map)
v1.5.1,"If all splits of the row are received, means this row can be merged"
v1.5.1,TODO Auto-generated method stub
v1.5.1,"Check futures, if the result of a sub-request is received, put it to the result queue"
v1.5.1,Now we just support pipelined row splits merging for dense type row
v1.5.1,Get partitions for this row
v1.5.1,First get this row from matrix storage
v1.5.1,"If the row exists in the matrix storage and the clock value meets the requirements, just"
v1.5.1,return
v1.5.1,Get row splits of this row from the matrix cache first
v1.5.1,"If the row split does not exist in cache, get it from parameter server"
v1.5.1,Wait the final result
v1.5.1,Put it to the matrix cache
v1.5.1,Split the matrix oplog according to the matrix partitions
v1.5.1,"If need update clock, we should send requests to all partitions"
v1.5.1,Filter the rowIds which are fetching now
v1.5.1,Send the rowIndex to rpc dispatcher and return immediately
v1.5.1,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
v1.5.1,Generate dispatch items and add them to the corresponding queues
v1.5.1,Filter the rowIds which are fetching now
v1.5.1,Pre-fetching is disable default
v1.5.1,matrix id to clock map
v1.5.1,"task index, it must be unique for whole application"
v1.5.1,Deserialize data splits meta
v1.5.1,Get workers
v1.5.1,Send request to every ps
v1.5.1,Wait the responses
v1.5.1,Update clock cache
v1.5.1,if(syncNum % 1024 == 0) {
v1.5.1,}
v1.5.1,"Use simple flow, do not use any cache"
v1.5.1,Get row from cache.
v1.5.1,"if row clock is satisfy ssp staleness limit, just return."
v1.5.1,Get row from ps.
v1.5.1,Wait until the clock value of this row is greater than or equal to the value
v1.5.1,"For ASYNC mode, just get from pss."
v1.5.1,"For BSP/SSP, get rows from storage/cache first"
v1.5.1,Get from ps.
v1.5.1,Wait until the clock value of this row is greater than or equal to the value
v1.5.1,"For ASYNC, just get rows from pss."
v1.5.1,no more retries.
v1.5.1,calculate sleep time and return.
v1.5.1,parse the i-th sleep-time
v1.5.1,parse the i-th number-of-retries
v1.5.1,calculateSleepTime may overflow.
v1.5.1,"A few common retry policies, with no delays."
v1.5.1,close is a local operation and should finish within milliseconds; timeout just to be safe
v1.5.1,response will be null for one way messages.
v1.5.1,maxFrameLength = 2G
v1.5.1,lengthFieldOffset = 0
v1.5.1,lengthFieldLength = 8
v1.5.1,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
v1.5.1,"initialBytesToStrip = 8, i.e. strip out the length field itself"
v1.5.1,indicates whether this connection's life cycle is managed
v1.5.1,See if we already have a connection (common case)
v1.5.1,create a unique lock for this RS + protocol (if necessary)
v1.5.1,get the RS lock
v1.5.1,do one more lookup in case we were stalled above
v1.5.1,Only create isa when we need to.
v1.5.1,definitely a cache miss. establish an RPC for
v1.5.1,this RS
v1.5.1,Throw what the RemoteException was carrying.
v1.5.1,check
v1.5.1,every
v1.5.1,minutes
v1.5.1,TODO
v1.5.1,创建failoverHandler
v1.5.1,"The number of times this invocation handler has ever been failed over,"
v1.5.1,before this method invocation attempt. Used to prevent concurrent
v1.5.1,failed method invocations from triggering multiple failover attempts.
v1.5.1,Make sure that concurrent failed method invocations
v1.5.1,only cause a
v1.5.1,single actual fail over.
v1.5.1,RpcController + Message in the method args
v1.5.1,(generated code from RPC bits in .proto files have
v1.5.1,RpcController)
v1.5.1,"LOG.info(""method "" + method.getName() + ""construct request time = """
v1.5.1,+ (System.currentTimeMillis() - beforeConstructTs));
v1.5.1,get an instance of the method arg type
v1.5.1,RpcController + Message in the method args
v1.5.1,(generated code from RPC bits in .proto files have
v1.5.1,RpcController)
v1.5.1,Message (hand written code usually has only a single
v1.5.1,argument)
v1.5.1,log any RPC responses that are slower than the configured
v1.5.1,warn
v1.5.1,response time or larger than configured warning size
v1.5.1,"when tagging, we let TooLarge trump TooSmall to keep"
v1.5.1,output simple
v1.5.1,note that large responses will often also be slow.
v1.5.1,provides a count of log-reported slow responses
v1.5.1,RpcController + Message in the method args
v1.5.1,(generated code from RPC bits in .proto files have
v1.5.1,RpcController)
v1.5.1,unexpected
v1.5.1,"in the protobuf methods, args[1] is the only significant argument"
v1.5.1,for JSON encoding
v1.5.1,base information that is reported regardless of type of call
v1.5.1,Disable Nagle's Algorithm since we don't want packets to wait
v1.5.1,Configure the event pipeline factory.
v1.5.1,Make a new connection.
v1.5.1,Remove all pending requests (will be canceled after relinquishing
v1.5.1,write lock).
v1.5.1,Cancel any pending requests by sending errors to the callbacks:
v1.5.1,Close the channel:
v1.5.1,Close the connection:
v1.5.1,Shut down all thread pools to exit.
v1.5.1,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
v1.5.1,See NettyServer.prepareResponse for where we write out the response.
v1.5.1,"It writes the call.id (int), a boolean signifying any error (and if"
v1.5.1,"so the exception name/trace), and the response bytes"
v1.5.1,Read the call id.
v1.5.1,"When the stream is closed, protobuf doesn't raise an EOFException,"
v1.5.1,"instead, it returns a null message object."
v1.5.1,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
v1.5.1,System.currentTimeMillis());
v1.5.1,"It would be good widen this to just Throwable, but IOException is what we"
v1.5.1,allow now
v1.5.1,not implemented
v1.5.1,not implemented
v1.5.1,"track what RpcEngine is used by a proxy class, for stopProxy()"
v1.5.1,cache of RpcEngines by protocol
v1.5.1,return the RpcEngine configured to handle a protocol
v1.5.1,We only handle the ConnectException.
v1.5.1,This is the exception we can't handle.
v1.5.1,check if timed out
v1.5.1,wait for retry
v1.5.1,IGNORE
v1.5.1,return the RpcEngine that handles a proxy object
v1.5.1,The default implementation works synchronously
v1.5.1,punt: allocate a new buffer & copy into it
v1.5.1,Parse cmd parameters
v1.5.1,load hadoop configuration
v1.5.1,load angel system configuration
v1.5.1,load user configuration:
v1.5.1,load user config file
v1.5.1,load command line parameters
v1.5.1,load user job resource files
v1.5.1,load user job jar if it exist
v1.5.1,Expand the environment variable
v1.5.1,Add default fs(local fs) for lib jars.
v1.5.1,"LOG.info(System.getProperty(""user.dir""));"
v1.5.1,get tokens for all the required FileSystems..
v1.5.1,Whether we need to recursive look into the directory structure
v1.5.1,creates a MultiPathFilter with the hiddenFileFilter and the
v1.5.1,user provided one (if any).
v1.5.1,"LOG.info(""Total input paths to process : "" + result.size());"
v1.5.1,get tokens for all the required FileSystems..
v1.5.1,Whether we need to recursive look into the directory structure
v1.5.1,creates a MultiPathFilter with the hiddenFileFilter and the
v1.5.1,user provided one (if any).
v1.5.1,"LOG.info(""Total input paths to process : "" + result.size());"
v1.5.1,a simple hdfs copy function assume src path and dest path are in same hdfs
v1.5.1,and FileSystem object has same schema
v1.5.1,"LOG.warn(""interrupted while sleeping"", ie);"
v1.5.1,public static String getHostname() {
v1.5.1,try {
v1.5.1,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
v1.5.1,} catch (UnknownHostException uhe) {
v1.5.1,}
v1.5.1,"return new StringBuilder().append("""").append(uhe).toString();"
v1.5.1,}
v1.5.1,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
v1.5.1,String hostname = getHostname();
v1.5.1,String classname = clazz.getSimpleName();
v1.5.1,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
v1.5.1,"StringBuilder().append(""Starting "").append(classname).toString(), new"
v1.5.1,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
v1.5.1,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
v1.5.1,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
v1.5.1,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
v1.5.1,
v1.5.1,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
v1.5.1,public void run() {
v1.5.1,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
v1.5.1,"this.val$classname + "" at "" + this.val$hostname}));"
v1.5.1,}
v1.5.1,});
v1.5.1,}
v1.5.1,"We we interrupted because we're meant to stop? If not, just"
v1.5.1,continue ignoring the interruption
v1.5.1,Recalculate waitTime.
v1.5.1,// Begin delegation to Thread
v1.5.1,// End delegation to Thread
v1.5.1,instance submitter class
v1.5.1,Obtain filename from path
v1.5.1,Split filename to prexif and suffix (extension)
v1.5.1,Check if the filename is okay
v1.5.1,Prepare temporary file
v1.5.1,Prepare buffer for data copying
v1.5.1,Open and check input stream
v1.5.1,Open output stream and copy data between source file in JAR and the temporary file
v1.5.1,"If read/write fails, close streams safely before throwing an exception"
v1.5.1,"Finally, load the library"
v1.5.1,little endian load order
v1.5.1,tail
v1.5.1,fallthrough
v1.5.1,fallthrough
v1.5.1,finalization
v1.5.1,fmix(h1);
v1.5.1,----------
v1.5.1,body
v1.5.1,----------
v1.5.1,tail
v1.5.1,----------
v1.5.1,finalization
v1.5.1,----------
v1.5.1,body
v1.5.1,----------
v1.5.1,tail
v1.5.1,----------
v1.5.1,finalization
v1.5.1,JobStateProto jobState = report.getJobState();
v1.5.1,Used for java code to get a AngelClient instance
v1.5.1,Used for python code to get a AngelClient instance
v1.5.1,the leaf level file should be readable by others
v1.5.1,the subdirs in the path should have execute permissions for
v1.5.1,others
v1.5.1,2.get job id
v1.5.1,Credentials credentials = new Credentials();
v1.5.1,4.copy resource files to hdfs
v1.5.1,5.write configuration to a xml file
v1.5.1,6.create am container context
v1.5.1,7.Submit to ResourceManager
v1.5.1,8.get app master client
v1.5.1,Create a number of filenames in the JobTracker's fs namespace
v1.5.1,add all the command line files/ jars and archive
v1.5.1,first copy them to jobtrackers filesystem
v1.5.1,should not throw a uri exception
v1.5.1,should not throw an uri excpetion
v1.5.1,set the timestamps of the archives and files
v1.5.1,set the public/private visibility of the archives and files
v1.5.1,get DelegationToken for each cached file
v1.5.1,check if we do not need to copy the files
v1.5.1,is jt using the same file system.
v1.5.1,just checking for uri strings... doing no dns lookups
v1.5.1,to see if the filesystems are the same. This is not optimal.
v1.5.1,but avoids name resolution.
v1.5.1,this might have name collisions. copy will throw an exception
v1.5.1,parse the original path to create new path
v1.5.1,check for ports
v1.5.1,Write job file to JobTracker's fs
v1.5.1,Setup resource requirements
v1.5.1,Setup LocalResources
v1.5.1,Setup security tokens
v1.5.1,Setup the command to run the AM
v1.5.1,Add AM user command opts
v1.5.1,Final command
v1.5.1,Setup the CLASSPATH in environment
v1.5.1,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
v1.5.1,Setup the environment variables for Admin first
v1.5.1,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
v1.5.1,Parse distributed cache
v1.5.1,Setup ContainerLaunchContext for AM container
v1.5.1,Set up the ApplicationSubmissionContext
v1.5.1,Get partition and check the partition state
v1.5.1,Get the stored pss for this partition
v1.5.1,"Check this ps is the master ps for this location, only master ps can accept the update"
v1.5.1,Check the partition state again
v1.5.1,Start to put the update to the slave pss
v1.5.1,Get partition and check the partition state
v1.5.1,Get the stored pss for this partition
v1.5.1,"Check this ps is the master ps for this partition, if not, just return failed"
v1.5.1,Start to put the update to the slave pss
v1.5.1,Add tokens to new user so that it may execute its task correctly.
v1.5.1,to exit
v1.5.1,context.getSnapshotManager().processRecovery();
v1.5.1,private final ParameterServer psServer;
v1.5.1,return ServerState.GENERAL;
v1.5.1,lock.readLock().lock();
v1.5.1,lock.readLock().unlock();
v1.5.1,data.rewind();
v1.5.1,lock.readLock().lock();
v1.5.1,lock.readLock().unlock();
v1.5.1,data.rewind();
v1.5.1,lock.readLock().lock();
v1.5.1,lock.readLock().unlock();
v1.5.1,data.rewind();
v1.5.1,output.writeInt(clock);
v1.5.1,clock = input.readInt();
v1.5.1,output.writeDouble(getDefaultValue());
v1.5.1,private final List<PartitionKey> partitionKeys;
v1.5.1,Read matrix meta from meta file
v1.5.1,Load partitions from file use fork-join
v1.5.1,Save partitions to files use fork-join
v1.5.1,Write the ps matrix meta to the meta file
v1.5.1,Mapping from taskId to clock value.
v1.5.1,TODO: use Epoll for linux future
v1.5.1,find the partition request context from cache
v1.5.1,get a channel to server from pool
v1.5.1,"if channel is not valid, it means maybe the connections to the server are closed"
v1.5.1,channelManager.removeChannelPool(loc);
v1.5.1,Generate seq id
v1.5.1,Create a RecoverPartRequest
v1.5.1,Serialize the request
v1.5.1,Change the seqId for the request
v1.5.1,Serialize the request
v1.5.1,"add the PSAgentContext,need fix"
v1.5.1,return this;
v1.5.1,return this;
v1.5.1,return this;
v1.5.1,return this;
v1.5.1,return this;
v1.5.1,TODO Should be implemented
v1.5.1,TODO Should be implemented
v1.5.1,Sort the parts by partitionId
v1.5.1,Sort partition keys use start column index
v1.5.1,"For each partition, we generate a update split."
v1.5.1,"Although the split is empty for partitions those without any update data,"
v1.5.1,we still need to generate a update split to update the clock info on ps.
v1.5.1,Sort the parts by partitionId
v1.5.1,Sort partition keys use start column index
v1.5.1,"For each partition, we generate a update split."
v1.5.1,"Although the split is empty for partitions those without any update data,"
v1.5.1,we still need to generate a update split to update the clock info on ps.
v1.5.1,TODO:
v1.5.1,public String uuid;
v1.5.1,this.uuid = UUID.randomUUID().toString();
v1.5.1,byte [] data = uuid.getBytes();
v1.5.1,buf.writeInt(data.length);
v1.5.1,buf.writeBytes(data);
v1.5.1,int size = buf.readInt();
v1.5.1,byte [] data = new byte[size];
v1.5.1,buf.readBytes(data);
v1.5.1,uuid = new String(data);
v1.5.1,"return ""PartitionRequest{"" + ""clock="" + clock + "", partKey="" + partKey + "", uuid="" + uuid + "", comeFromPs="""
v1.5.1,"+ comeFromPs + ""} "" + super.toString();"
v1.5.1,public String uuid;
v1.5.1,write the max abs
v1.5.1,TODO Auto-generated method stub
v1.5.1,TODO Auto-generated method stub
v1.5.1,TODO Auto-generated method stub
v1.5.1,get configuration from config file
v1.5.1,set localDir with enviroment set by nm.
v1.5.1,get master location
v1.5.1,init task manager and start tasks
v1.5.1,start heartbeat thread
v1.5.1,taskManager.assignTaskIds(response.getTaskidsList());
v1.5.1,todo
v1.5.1,"if worker timeout, it may be knocked off."
v1.5.1,"SUCCESS, do nothing"
v1.5.1,heartbeatFailedTime = 0;
v1.5.1,private KEY currentKey;
v1.5.1,will be created
v1.5.1,TODO Auto-generated method stub
v1.5.1,Bitmap bitmap = new Bitmap();
v1.5.1,int max = indexArray[size - 1];
v1.5.1,byte [] bitIndexArray = new byte[max / 8 + 1];
v1.5.1,for(int i = 0; i < size; i++){
v1.5.1,int bitIndex = indexArray[i] >> 3;
v1.5.1,int bitOffset = indexArray[i] - (bitIndex << 3);
v1.5.1,switch(bitOffset){
v1.5.1,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
v1.5.1,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
v1.5.1,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
v1.5.1,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
v1.5.1,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
v1.5.1,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
v1.5.1,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
v1.5.1,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
v1.5.1,}
v1.5.1,}
v1.5.1,//////////////////////////////
v1.5.1,Application Configs
v1.5.1,//////////////////////////////
v1.5.1,//////////////////////////////
v1.5.1,Master Configs
v1.5.1,//////////////////////////////
v1.5.1,//////////////////////////////
v1.5.1,Worker Configs
v1.5.1,//////////////////////////////
v1.5.1,//////////////////////////////
v1.5.1,Task Configs
v1.5.1,//////////////////////////////
v1.5.1,//////////////////////////////
v1.5.1,ParameterServer Configs
v1.5.1,//////////////////////////////
v1.5.1,////////////////// IPC //////////////////////////
v1.5.1,//////////////////////////////
v1.5.1,Matrix transfer Configs.
v1.5.1,//////////////////////////////
v1.5.1,//////////////////////////////
v1.5.1,Matrix transfer Configs.
v1.5.1,//////////////////////////////
v1.5.1,Configs used to ANGEL_PS_PSAGENT running mode future.
v1.5.1,model parse
v1.5.1,Mark whether use pyangel or not.
v1.5.1,private Configuration conf;
v1.5.1,"Configuration that should be used in python environment, there should only be one"
v1.5.1,configuration instance in each Angel context.
v1.5.1,Use private access means jconf should not be changed or modified in this way.
v1.5.1,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
v1.5.1,Do nothing
v1.5.1,To-DO: add other ways to justify different value types
v1.5.1,"This is so ugly, must re-implement by more elegance way"
v1.5.1,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
v1.5.1,and other files submitted by user.
v1.5.1,Launch python process
v1.5.1,TODO Auto-generated constructor stub
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Rank
v1.5.1,Regularization parameters
v1.5.1,Learn rage
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set FM algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set feature number
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set FM predict output path
v1.5.1,Set actionType train
v1.5.1,Set feature number
v1.5.1,Feature number of train data
v1.5.1,Number of nonzero features
v1.5.1,Tree number
v1.5.1,Tree depth
v1.5.1,Split number
v1.5.1,Feature sample ratio
v1.5.1,Data format
v1.5.1,Learning rate
v1.5.1,Set basic configuration keys
v1.5.1,Use local deploy mode and dummy data spliter
v1.5.1,"set input, output path"
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,Set GBDT algorithm parameters
v1.5.1,Submit GBDT Train Task
v1.5.1,Load Model from HDFS.
v1.5.1,set basic configuration keys
v1.5.1,use local deploy mode and dummy dataspliter
v1.5.1,get a angel client
v1.5.1,add matrix
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,"Set trainning data, save model, log path"
v1.5.1,Set actionType train
v1.5.1,Set MF algorithm parameters
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,number of mini batch within a update periorid
v1.5.1,"Data format, libsvm or dummy"
v1.5.1,Batch size
v1.5.1,Model type
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,"conf.setBoolean(MLConf.ML_INDEX_GET_ENABLE(), true);"
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set sgd LR algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set actionType prediction
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,"Data format, libsvm or dummy"
v1.5.1,Train batch number per epoch.
v1.5.1,Sample ratio
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set FTRL LR algorithm parameters #feature #epoch
v1.5.1,FtrlLRPredictTest();
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType incremental train
v1.5.1,Set predict data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set actionType train
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Validation sample Ratio
v1.5.1,"Data format, libsvm or dummy"
v1.5.1,Train batch number per epoch.
v1.5.1,Batch number
v1.5.1,Model type
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set sgd LR algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType incremental train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set actionType prediction
v1.5.1,LOG.info(sigmoid(data[i]));
v1.5.1,LOG.info(Math.exp(-data[i]));
v1.5.1,when b is a negative number
v1.5.1,LOG.info(sigmoid(data[i]));
v1.5.1,LOG.info(Math.exp(-data[i]));
v1.5.1,when b is a negative number
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Validation sample Ratio
v1.5.1,"Data format, libsvm or dummy"
v1.5.1,Train batch number per epoch.
v1.5.1,Batch number
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set MLR algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType incremental train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set actionType prediction
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Rank
v1.5.1,Regularization parameters
v1.5.1,Learn rage
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set FM algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set feature number
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set learnType
v1.5.1,Set feature number
v1.5.1,Set
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set FM predict output path
v1.5.1,Set actionType train
v1.5.1,Set learnType
v1.5.1,Set feature number
v1.5.1,Cluster center number
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Sample ratio per mini-batch
v1.5.1,C
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set Kmeans algorithm parameters #cluster #feature #epoch
v1.5.1,Set data format
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log sava path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set actionType prediction
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Validation Ratio
v1.5.1,Data format
v1.5.1,Train batch number per epoch.
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,Set basic configuration keys
v1.5.1,Set data format
v1.5.1,Use local deploy mode
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set sgd SVM algorithm parameters
v1.5.1,"set input, output path"
v1.5.1,Set save model path
v1.5.1,Set actionType train
v1.5.1,Set log path
v1.5.1,Submit LR Train Task
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set actionType incremental train
v1.5.1,Set log path
v1.5.1,Feature number of train data
v1.5.1,Total iteration number
v1.5.1,Validation sample Ratio
v1.5.1,"Data format, libsvm or dummy"
v1.5.1,Train batch number per epoch.
v1.5.1,Learning rate
v1.5.1,Decay of learning rate
v1.5.1,Regularization coefficient
v1.5.1,Set local deploy mode
v1.5.1,Set basic configuration keys
v1.5.1,Set data format
v1.5.1,"set angel resource parameters #worker, #task, #PS"
v1.5.1,set sgd LR algorithm parameters #feature #epoch
v1.5.1,Set trainning data path
v1.5.1,Set save model path
v1.5.1,Set log path
v1.5.1,Set actionType train
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set save model path
v1.5.1,Set actionType incremental train
v1.5.1,Set log path
v1.5.1,Set trainning data path
v1.5.1,Set load model path
v1.5.1,Set predict result path
v1.5.1,Set log sava path
v1.5.1,Set actionType prediction
v1.5.1,double z=pre*y;
v1.5.1,if(z<=0) return 0.5-z;
v1.5.1,"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);"
v1.5.1,return 0.0;
v1.5.1,logistic loss for binary classification task.
v1.5.1,"logistic loss, but predict un-transformed margin"
v1.5.1,check if label in range
v1.5.1,return the default evaluation metric for the objective
v1.5.1,TODO Auto-generated method stub
v1.5.1,start row index for words
v1.5.1,doc ids
v1.5.1,topic assignments
v1.5.1,count word
v1.5.1,build word start index
v1.5.1,build dks
v1.5.1,"model.wtMat().increment(w, update);"
v1.5.1,"update.plusBy(t, 1);"
v1.5.1,"model.wtMat().increment(w, update);"
v1.5.1,The starting point
v1.5.1,There's always an unused entry.
v1.5.1,The starting point
v1.5.1,There's always an unused entry.
v1.5.1,The starting point
v1.5.1,There's always an unused entry.
v1.5.1,The starting point
v1.5.1,There's always an unused entry.
v1.5.1,The starting point
v1.5.1,There's always an unused entry.
v1.5.1,The starting point
v1.5.1,There's always an unused entry.
v1.5.1,print();
v1.5.1,The starting point
v1.5.1,There's always an unused entry.
v1.5.1,The starting point
v1.5.1,There's always an unused entry.
v1.5.1,The starting point
v1.5.1,There's always an unused entry.
v1.5.1,The starting point
v1.5.1,There's always an unused entry.
v1.5.1,print();
v1.5.1,Write #rows
v1.5.1,Write each row
v1.5.1,dense
v1.5.1,sparse
v1.5.1,LOG.info(buf.refCnt());
v1.5.1,dense
v1.5.1,sparse
v1.5.1,LOG.info(buf.refCnt());
v1.5.1,calculate columns
v1.5.1,loss function
v1.5.1,gradient and hessian
v1.5.1,"categorical feature set, null: none, empty: all, else: partial"
v1.5.1,tree node
v1.5.1,initialize the phase
v1.5.1,current tree and depth
v1.5.1,create loss function
v1.5.1,calculate grad info of each instance
v1.5.1,"create data sketch, push candidate split value to PS"
v1.5.1,1. calculate candidate split value
v1.5.1,categorical features
v1.5.1,2. push local sketch to PS
v1.5.1,3. set phase to GET_SKETCH
v1.5.1,the leader worker
v1.5.1,merge categorical features
v1.5.1,create updates
v1.5.1,"pull the global sketch from PS, only called once by each worker"
v1.5.1,number of categorical feature
v1.5.1,sample feature
v1.5.1,push sampled feature set to the current tree
v1.5.1,create new tree
v1.5.1,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
v1.5.1,calculate gradient
v1.5.1,"1. create new tree, initialize tree nodes and node stats"
v1.5.1,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
v1.5.1,2.1. pull the sampled features of the current tree
v1.5.1,"2.2. if use all the features, only called one"
v1.5.1,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
v1.5.1,4. set root node to active
v1.5.1,"5. reset instance position, set the root node's span"
v1.5.1,6. calculate gradient
v1.5.1,7. set phase to run active
v1.5.1,1. start threads of active tree nodes
v1.5.1,1.1. start threads for active nodes to generate histogram
v1.5.1,1.2. set thread status to batch num
v1.5.1,1.3. set the oplog to active
v1.5.1,"2. check thread stats, if all threads finish, return"
v1.5.1,clock
v1.5.1,find split
v1.5.1,"1. find responsible tree node, using RR scheme"
v1.5.1,2. pull gradient histogram
v1.5.1,2.1. get the name of this node's gradient histogram on PS
v1.5.1,2.2. pull the histogram
v1.5.1,histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();
v1.5.1,2.3. find best split result of this tree node
v1.5.1,2.3.1 using server split
v1.5.1,"update the grad stats of the root node on PS, only called once by leader worker"
v1.5.1,update the grad stats of children node
v1.5.1,update the left child
v1.5.1,update the right child
v1.5.1,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
v1.5.1,"2.3.3 otherwise, the returned histogram contains the gradient info"
v1.5.1,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
v1.5.1,2.3.5 reset this tree node's gradient histogram to 0
v1.5.1,3. push split feature to PS
v1.5.1,4. push split value to PS
v1.5.1,5. push split gain to PS
v1.5.1,6. set phase to AFTER_SPLIT
v1.5.1,clock
v1.5.1,1. get split feature
v1.5.1,2. get split value
v1.5.1,3. get split gain
v1.5.1,4. get node weight
v1.5.1,5. split node
v1.5.1,"2. check thread stats, if all threads finish, return"
v1.5.1,6. clock
v1.5.1,"split the span of one node, reset the instance position"
v1.5.1,in case this worker has no instance on this node
v1.5.1,set the span of left child
v1.5.1,set the span of right child
v1.5.1,"1. left to right, find the first instance that should be in the right child"
v1.5.1,"2. right to left, find the first instance that should be in the left child"
v1.5.1,3. swap two instances
v1.5.1,4. find the cut pos
v1.5.1,than the split value
v1.5.1,5. set the span of left child
v1.5.1,6. set the span of right child
v1.5.1,set tree node to active
v1.5.1,set node to leaf
v1.5.1,set node to inactive
v1.5.1,finish current tree
v1.5.1,finish current depth
v1.5.1,set the tree phase
v1.5.1,check if there is active node
v1.5.1,check if finish all the tree
v1.5.1,update node's grad stats on PS
v1.5.1,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
v1.5.1,the root node's stats is updated by leader worker
v1.5.1,1. create the update
v1.5.1,2. push the update to PS
v1.5.1,the leader task adds node prediction to flush list
v1.5.1,1. name of this node's grad histogram on PS
v1.5.1,2. build the grad histogram of this node
v1.5.1,3. push the histograms to PS
v1.5.1,4. reset thread stats to finished
v1.5.1,5.1. set the children nodes of this node
v1.5.1,5.2. set split info and grad stats to this node
v1.5.1,5.2. create children nodes
v1.5.1,"5.3. create node stats for children nodes, and add them to the tree"
v1.5.1,5.4. reset instance position
v1.5.1,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
v1.5.1,5.6. set children nodes to leaf nodes
v1.5.1,5.7. set nid to leaf node
v1.5.1,5.8. deactivate active node
v1.5.1,"get feature type, 0:empty 1:all equal 2:real"
v1.5.1,left child <= split value; right child > split value
v1.5.1,"the first: minimal, the last: maximal"
v1.5.1,categorical features
v1.5.1,continuous features
v1.5.1,left child <= split value; right child > split value
v1.5.1,feature index used to split
v1.5.1,feature value used to split
v1.5.1,loss change after split this node
v1.5.1,grad stats of the left child
v1.5.1,grad stats of the right child
v1.5.1,"LOG.info(""Constructor with fid = -1"");"
v1.5.1,fid = -1: no split currently
v1.5.1,the minimal split value is the minimal value of feature
v1.5.1,the splits do not include the maximal value of feature
v1.5.1,"1. the average distance, (maxValue - minValue) / splitNum"
v1.5.1,2. calculate the candidate split value
v1.5.1,1. new feature's histogram (grad + hess)
v1.5.1,size: sampled_featureNum * (2 * splitNum)
v1.5.1,"in other words, concatenate each feature's histogram"
v1.5.1,2. get the span of this node
v1.5.1,------ 3. using sparse-aware method to build histogram ---
v1.5.1,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
v1.5.1,3.1. get the instance index
v1.5.1,3.2. get the grad and hess of the instance
v1.5.1,3.3. add to the sum
v1.5.1,3.4. loop the non-zero entries
v1.5.1,3.4.1. get feature value
v1.5.1,3.4.2. current feature's position in the sampled feature set
v1.5.1,3.4.3. find the position of feature value in a histogram
v1.5.1,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
v1.5.1,3.4.4. add the grad and hess to the corresponding bin
v1.5.1,3.4.5. add the reverse to the bin that contains 0.0f
v1.5.1,4. add the grad and hess sum to the zero bin of all features
v1.5.1,find the best split result of the histogram of a tree node
v1.5.1,1. calculate the gradStats of the root node
v1.5.1,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
v1.5.1,2. loop over features
v1.5.1,2.1. get the ture feature id in the sampled feature set
v1.5.1,2.2. get the indexes of histogram of this feature
v1.5.1,2.3. find the best split of current feature
v1.5.1,2.4. update the best split result if possible
v1.5.1,"update the grad stats of the root node on PS, only called once by leader worker"
v1.5.1,3. update the grad stats of children node
v1.5.1,3.1. update the left child
v1.5.1,3.2. update the right child
v1.5.1,find the best split result of one feature
v1.5.1,1. set the feature id
v1.5.1,2. create the best left stats and right stats
v1.5.1,3. the gain of the root node
v1.5.1,4. create the temp left and right grad stats
v1.5.1,5. loop over all the data in histogram
v1.5.1,5.1. get the grad and hess of current hist bin
v1.5.1,5.2. check whether we can split with current left hessian
v1.5.1,right = root - left
v1.5.1,5.3. check whether we can split with current right hessian
v1.5.1,5.4. calculate the current loss gain
v1.5.1,5.5. check whether we should update the split result with current loss gain
v1.5.1,split value = sketches[splitIdx]
v1.5.1,"5.6. if should update, also update the best left and right grad stats"
v1.5.1,6. set the best left and right grad stats
v1.5.1,partition number
v1.5.1,cols of each partition
v1.5.1,1. calculate the total grad sum and hess sum
v1.5.1,2. create the grad stats of the node
v1.5.1,1. calculate the total grad sum and hess sum
v1.5.1,2. create the grad stats of the node
v1.5.1,1. calculate the total grad sum and hess sum
v1.5.1,2. create the grad stats of the node
v1.5.1,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
v1.5.1,find the best split result of the histogram of a tree node
v1.5.1,2.2. get the indexes of histogram of this feature
v1.5.1,2.3. find the best split of current feature
v1.5.1,2.4. update the best split result if possible
v1.5.1,find the best split result of one feature
v1.5.1,1. set the feature id
v1.5.1,splitEntry.setFid(fid);
v1.5.1,2. create the best left stats and right stats
v1.5.1,3. the gain of the root node
v1.5.1,4. create the temp left and right grad stats
v1.5.1,5. loop over all the data in histogram
v1.5.1,5.1. get the grad and hess of current hist bin
v1.5.1,5.2. check whether we can split with current left hessian
v1.5.1,right = root - left
v1.5.1,5.3. check whether we can split with current right hessian
v1.5.1,5.4. calculate the current loss gain
v1.5.1,5.5. check whether we should update the split result with current loss gain
v1.5.1,"5.6. if should update, also update the best left and right grad stats"
v1.5.1,6. set the best left and right grad stats
v1.5.1,find the best split result of a serve row on the PS
v1.5.1,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.5.1,2.2. get the start index in histogram of this feature
v1.5.1,2.3. find the best split of current feature
v1.5.1,2.4. update the best split result if possible
v1.5.1,"find the best split result of one feature from a server row, used by the PS"
v1.5.1,1. set the feature id
v1.5.1,2. create the best left stats and right stats
v1.5.1,3. the gain of the root node
v1.5.1,4. create the temp left and right grad stats
v1.5.1,5. loop over all the data in histogram
v1.5.1,5.1. get the grad and hess of current hist bin
v1.5.1,5.2. check whether we can split with current left hessian
v1.5.1,right = root - left
v1.5.1,5.3. check whether we can split with current right hessian
v1.5.1,5.4. calculate the current loss gain
v1.5.1,5.5. check whether we should update the split result with current loss gain
v1.5.1,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
v1.5.1,the task use index to find fvalue
v1.5.1,"5.6. if should update, also update the best left and right grad stats"
v1.5.1,6. set the best left and right grad stats
v1.5.1,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
v1.5.1,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
v1.5.1,clear all the information
v1.5.1,calculate the sum of gradient and hess
v1.5.1,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v1.5.1,ridx)
v1.5.1,check if necessary information is ready
v1.5.1,"same as add, reduce is used in All Reduce"
v1.5.1,"features used in this tree, if equals null, means use all the features without sampling"
v1.5.1,node in the tree
v1.5.1,the gradient info of each instances
v1.5.1,initialize nodes
v1.5.1,gradient
v1.5.1,second order gradient
v1.5.1,int sendStartCol = startFid * 7; // each split contains 7 doubles
v1.5.1,"System.out.println(""Indices: "" + Arrays.toString(indices));"
v1.5.1,t[i][code]++;
v1.5.1,else if (Math.random() > 0.5) {
v1.5.1,t[i][code] = freq;
v1.5.1,}
v1.5.1,"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);"
v1.5.1,"ret = Math.min(ret, t[i][h[i].encode(key)]);"
v1.5.1,"Get input path, output path"
v1.5.1,Init serde
v1.5.1,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.5.1,"Get input path, output path"
v1.5.1,Init serde
v1.5.1,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.5.1,"task type: classification, regression, or ranking"
v1.5.1,"quantile sketch, size = featureNum * splitNum"
v1.5.1,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
v1.5.1,"active tree nodes, size = pow(2, treeDepth) -1"
v1.5.1,sampled features. size = treeNum * sampleRatio * featureNum
v1.5.1,categorical feature. size = workerNum * cateFeatNum * splitNum
v1.5.1,"split features, size = treeNum * treeNodeNum"
v1.5.1,"split values, size = treeNum * treeNodeNum"
v1.5.1,"split gains, size = treeNum * treeNodeNum"
v1.5.1,"node weights, size = treeNum * treeNodeNum"
v1.5.1,"node preds, size = treeNum * treeNodeNum"
v1.5.1,if using PS to perform split
v1.5.1,step size for a tree
v1.5.1,number of class
v1.5.1,minimum loss change required for a split
v1.5.1,maximum depth of a tree
v1.5.1,number of features
v1.5.1,number of nonzero
v1.5.1,number of candidates split value
v1.5.1,----- the rest parameters are less important ----
v1.5.1,base instance weight
v1.5.1,minimum amount of hessian(weight) allowed in a child
v1.5.1,L2 regularization factor
v1.5.1,L1 regularization factor
v1.5.1,default direction choice
v1.5.1,maximum delta update we can add in weight estimation
v1.5.1,this parameter can be used to stabilize update
v1.5.1,default=0 means no constraint on weight delta
v1.5.1,whether we want to do subsample for row
v1.5.1,whether to subsample columns for each tree
v1.5.1,accuracy of sketch
v1.5.1,accuracy of sketch
v1.5.1,leaf vector size
v1.5.1,option for parallelization
v1.5.1,option to open cacheline optimization
v1.5.1,whether to not print info during training.
v1.5.1,maximum depth of the tree
v1.5.1,number of features used for tree construction
v1.5.1,"minimum loss change required for a split, otherwise stop split"
v1.5.1,----- the rest parameters are less important ----
v1.5.1,default direction choice
v1.5.1,whether we want to do sample data
v1.5.1,whether to sample columns during tree construction
v1.5.1,whether to use histogram for split
v1.5.1,number of histogram units
v1.5.1,whether to print info during training.
v1.5.1,----- the rest parameters are obtained after training ----
v1.5.1,total number of nodes
v1.5.1,number of deleted nodes */
v1.5.0,implement Zip2Map interface
v1.5.0,implement Zip3Map interface
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy data spliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,TODO Auto-generated constructor stub
v1.5.0,row 0 is a random uniform
v1.5.0,row 1 is a random normal
v1.5.0,row 2 is filled with 1.0
v1.5.0,TODO: Have to deal with default values
v1.5.0,asum += Math.abs(data.defaultReturnValue()) * (entireSize - data.size());
v1.5.0,TODO: Have to deal with default values
v1.5.0,sum += (entireSize - keys.size()) * data1.defaultReturnValue() * data2.defaultReturnValue();
v1.5.0,TODO: Have to deal with default values
v1.5.0,"qSum += Math.pow(data.defaultReturnValue(), 2) * (entireSize - data.size());"
v1.5.0,TODO: Have to deal with default values
v1.5.0,asum += data.defaultReturnValue() * (entireSize - data.size());
v1.5.0,find the max abs
v1.5.0,compress data
v1.5.0,TODO: a better way is needed to deal with defaultValue
v1.5.0,TODO: a better way is needed to deal with defaultValue
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,"
v1.5.0,"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:"
v1.5.0,"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"","
v1.5.0,"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output"""
v1.5.0,Feature number of train data
v1.5.0,Number of nonzero features
v1.5.0,Tree number
v1.5.0,Tree depth
v1.5.0,Split number
v1.5.0,Feature sample ratio
v1.5.0,Data format
v1.5.0,Learning rate
v1.5.0,"set input, output path"
v1.5.0,Set GBDT algorithm parameters
v1.5.0,Load Model from HDFS.
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Validation sample ratio
v1.5.0,"Data format,libsvm or dummy"
v1.5.0,Train batch number per epoch
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,"Set job queue, if you use YARN deploy mode, you can set job queue by"
v1.5.0,"self.conf.set('mapreduce.job.queue.name', 'default')"
v1.5.0,Set local deploy mode
v1.5.0,Set basic self.configuration keys
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #tast, #ps"
v1.5.0,set sgd LR algorithim parameters # feature # epoch
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set actionType incremental train
v1.5.0,Set log path
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set actionType prediction
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Set local deploy mode
v1.5.0,Set basic self.configuration keys
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,"Set trainning data, and save model path"
v1.5.0,Set actionType train
v1.5.0,Set MF algorithm parameters
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.5.0,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,"
v1.5.0,"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:"
v1.5.0,"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"","
v1.5.0,"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output"""
v1.5.0,"if you need, you can delete the annotation mark before Line35,Line36,Line61,Line62, so"
v1.5.0,there is no need for you to pass the configs every time you submit the pyangel job.
v1.5.0,"input_path = ""file:///${YOUR_ANGEL_HOME}/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"""
v1.5.0,"output_path = ""file:///${YOUR_ANGEL_HOME}/data/output"""
v1.5.0,Feature number of train data
v1.5.0,Number of nonzero features
v1.5.0,Tree number
v1.5.0,Tree depth
v1.5.0,Split number
v1.5.0,Feature sample ratio
v1.5.0,Data format
v1.5.0,Learning rate
v1.5.0,Use local deploy mode and dummy data spliter
v1.5.0,set input] = output path
v1.5.0,self.conf[AngelConf.ANGEL_TRAIN_DATA_PATH] = input_path
v1.5.0,self.conf[AngelConf.ANGEL_SAVE_MODEL_PATH] = output_path
v1.5.0,Set GBDT algorithm parameters
v1.5.0,Load Model from HDFS.
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Cluster center number
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Sample ratio per mini-batch
v1.5.0,C
v1.5.0,Set local deploy mode
v1.5.0,Set basic self.configuration key
v1.5.0,"Set angel resource parameters #worker, #task, #PS"
v1.5.0,Set Kmeans algorithm parameters #cluster #feature #epoch
v1.5.0,Set data format
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log sava path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set actionType prediction
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Validation sample Ratio
v1.5.0,"Data format, libsvm or dummy"
v1.5.0,Train batch number per epoch.
v1.5.0,Batch number
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set sgd LR algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType incremental train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set log path
v1.5.0,Set actionType prediction
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Rank
v1.5.0,Regularization parameters
v1.5.0,Learn rage
v1.5.0,Set local deploy mode
v1.5.0,Set basic self.configuration keys
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set FM algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set learnType
v1.5.0,Set feature number
v1.5.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
v1.5.0,"paras[1] = ""abc"";"
v1.5.0,"paras[2] = ""123"";"
v1.5.0,Add standard Hadoop classes
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Learning rate
v1.5.0,Regularization coefficient
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set sgd LR algorithm parameters #feature #epoch
v1.5.0,Set input data path
v1.5.0,Set save model path
v1.5.0,Set actionType train
v1.5.0,Set PS Model values
v1.5.0,Wait for all tasks finish this clock
v1.5.0,Get values of index array
v1.5.0,Set PS Model values
v1.5.0,Wait for all tasks finish this clock
v1.5.0,Get values of index array
v1.5.0,"Pull func1 = new Pull(client.getMatrixId(), 1);"
v1.5.0,taskContext.globalSync(client.getMatrixId());
v1.5.0,TVector row1 = ((GetRowResult) client.get(func1)).getRow();
v1.5.0,double [] delta1 = new double[col];
v1.5.0,for(int i = 0; i < col; i++) {
v1.5.0,delta1[i] = 2.0;
v1.5.0,}
v1.5.0,"DenseDoubleVector deltaV1 = new DenseDoubleVector(col, delta1);"
v1.5.0,deltaV1.setMatrixId(client.getMatrixId());
v1.5.0,deltaV1.setRowId(1);
v1.5.0,client.increment(deltaV1);
v1.5.0,"conf.setInt(AngelConf.ANGEL_STALENESS, -1);"
v1.5.0,Feature number of train data
v1.5.0,Number of nonzero features
v1.5.0,Tree number
v1.5.0,Tree depth
v1.5.0,Split number
v1.5.0,Feature sample ratio
v1.5.0,Data format
v1.5.0,Learning rate
v1.5.0,Set basic configuration keys
v1.5.0,Use local deploy mode and data format
v1.5.0,"set input, output path"
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,Set GBDT algorithm parameters
v1.5.0,Load Model from HDFS.
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,"Set trainning data, and save model path"
v1.5.0,Set actionType train
v1.5.0,Set MF algorithm parameters
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Validation sample Ratio
v1.5.0,"Data format, libsvm or dummy"
v1.5.0,Train batch number per epoch.
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set sgd LR algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set actionType incremental train
v1.5.0,Set log path
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set actionType prediction
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Validation sample Ratio
v1.5.0,"Data format, libsvm or dummy"
v1.5.0,Train batch number per epoch.
v1.5.0,Batch number
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set sgd LR algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType incremental train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set log path
v1.5.0,Set actionType prediction
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Default location for user home directories #
v1.5.0,Default value for FS_HOME_DIR_KEY #
v1.5.0,Default umask for files created in HDFS #
v1.5.0,Default value for FS_PERMISSIONS_UMASK_KEY #
v1.5.0,How often does RPC client send pings to RPC server #
v1.5.0,Default value for IPC_PING_INTERVAL_KEY #
v1.5.0,Enables pings from RPC client to the server #
v1.5.0,Default value of IPC_CLIENT_PING_KEY #
v1.5.0,Responses larger than this will be logged #
v1.5.0,Default value for IPC_SERVER_RPC_MAX_RESPONSE_SIZE_KEY #
v1.5.0,Number of threads in RPC server reading from the socket #
v1.5.0,Default value for IPC_SERVER_RPC_READ_THREADS_KEY #
v1.5.0,How many calls per handler are allowed in the queue. #
v1.5.0,Default value for IPC_SERVER_HANDLER_QUEUE_SIZE_KEY #
v1.5.0,Internal buffer size for Lzo compressordecompressors #/
v1.5.0,Default value for IO_COMPRESSION_CODEC_LZO_BUFFERSIZE_KEY #
v1.5.0,This is for specifying the implementation for the mappings from
v1.5.0,hostnames to the racks they belong to
v1.5.0,Internal buffer size for Snappy compressordecompressors #/
v1.5.0,Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #
v1.5.0,Internal buffer size for Snappy compressordecompressors #/
v1.5.0,Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #
v1.5.0,Service Authorization
v1.5.0,HA health monitor and failover controller.
v1.5.0,How often to retry connecting to the service.
v1.5.0,How often to check the service.
v1.5.0,How long to sleep after an unexpected RPC error.
v1.5.0,Timeout for the actual monitorHealth() calls. *
v1.5.0,Timeout that the FC waits for the new active to become active
v1.5.0,Timeout that the FC waits for the old active to go to standby
v1.5.0,FC connection retries for graceful fencing
v1.5.0,"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState"
v1.5.0,Static user web-filter properties.
v1.5.0,See StaticUserWebFilter.
v1.5.0,EnableDisable aliases serving from jetty
v1.5.0,Path to the Kerberos ticket cache.  Setting this will force
v1.5.0,UserGroupInformation to use only this ticket cache file when creating a
v1.5.0,FileSystem instance.
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,"
v1.5.0,"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:"
v1.5.0,"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"","
v1.5.0,"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output"""
v1.5.0,Feature number of train data
v1.5.0,Number of nonzero features
v1.5.0,Tree number
v1.5.0,Tree depth
v1.5.0,Split number
v1.5.0,Feature sample ratio
v1.5.0,Data format
v1.5.0,Learning rate
v1.5.0,Set GBDT category feature
v1.5.0,"set input, output path"
v1.5.0,Set GBDT algorithm parameters
v1.5.0,Load Model from HDFS.
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Validation sample ratio
v1.5.0,"Data format,libsvm or dummy"
v1.5.0,Train batch number per epoch
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,"Set job queue, if you use YARN deploy mode, you can set job queue by"
v1.5.0,"self.conf.set('mapreduce.job.queue.name', 'default')"
v1.5.0,Set local deploy mode
v1.5.0,Set basic self.configuration keys
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #tast, #ps"
v1.5.0,set sgd LR algorithim parameters # feature # epoch
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set actionType incremental train
v1.5.0,Set log path
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set actionType prediction
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Set local deploy mode
v1.5.0,Set basic self.configuration keys
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,"Set trainning data, and save model path"
v1.5.0,Set actionType train
v1.5.0,Set MF algorithm parameters
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.5.0,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Load Model from HDFS.
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Cluster center number
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Sample ratio per mini-batch
v1.5.0,C
v1.5.0,Set local deploy mode
v1.5.0,Set basic self.configuration key
v1.5.0,"Set angel resource parameters #worker, #task, #PS"
v1.5.0,Set Kmeans algorithm parameters #cluster #feature #epoch
v1.5.0,Set data format
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log sava path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set actionType prediction
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Validation sample Ratio
v1.5.0,"Data format, libsvm or dummy"
v1.5.0,Train batch number per epoch.
v1.5.0,Batch number
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set sgd LR algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType incremental train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set log path
v1.5.0,Set actionType prediction
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Rank
v1.5.0,Regularization parameters
v1.5.0,Learn rage
v1.5.0,Set local deploy mode
v1.5.0,Set basic self.configuration keys
v1.5.0,"Set angel resource parameters #worker, #task, #PS"
v1.5.0,Set FM algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set learnType
v1.5.0,Set feature number
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,If the enviroment has ANGEL_HOME set trust it.
v1.5.0,Add the path of the PyAngel module if it exists
v1.5.0,If we are installed in edit mode also look two dirs up
v1.5.0,Not pip installed no worries
v1.5.0,If we are installed in edit mode also look two dirs up
v1.5.0,Not pip installed no worries
v1.5.0,Normalize the paths
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Get Java HashMap instance which converted from a python dict
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Run ParameterServer  & ParameterServerAgent
v1.5.0,Only Run ParameterServer
v1.5.0,Run ParameterServer & Worker(embedded ParameterServerAgent)
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,To Do
v1.5.0,Modify the way to get current Angel version
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Launch the Py4j gateway
v1.5.0,Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc
v1.5.0,Don't send ctrl-c / SIGINT to the Java gateway:
v1.5.0,We use select() here in order to avoid blocking indefinitely if the subprocess dies
v1.5.0,before connecting
v1.5.0,Determine which ephemeral port the server started on:
v1.5.0,Connect to the gateway
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.5.0,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.5.0,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,To do: need python edition of TVector
v1.5.0,To do: need python edition of GetFunc
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Create an angel job client
v1.5.0,Submit this application
v1.5.0,Create a KMeans model
v1.5.0,Load model meta to client
v1.5.0,Start
v1.5.0,"Run user task and wait for completion,"
v1.5.0,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.5.0,Save the trained model to HDFS
v1.5.0,Stop
v1.5.0,Create an angel job client
v1.5.0,Submit this application
v1.5.0,Create KMeans model
v1.5.0,Add the model meta to client
v1.5.0,Start
v1.5.0,"Run user task and wait for completion,"
v1.5.0,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.5.0,Stop
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Training job to obtain a model
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Create an angel job client
v1.5.0,Submit this application
v1.5.0,Create a model
v1.5.0,Load model meta to client
v1.5.0,Run user task
v1.5.0,"Wait for completion,"
v1.5.0,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.5.0,Save the incremental trained model to HDFS
v1.5.0,Stop
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,If the enviroment has ANGEL_HOME set trust it.
v1.5.0,Add the path of the PyAngel module if it exists
v1.5.0,If we are installed in edit mode also look two dirs up
v1.5.0,Not pip installed no worries
v1.5.0,If we are installed in edit mode also look two dirs up
v1.5.0,Not pip installed no worries
v1.5.0,Normalize the paths
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Get Java HashMap instance which converted from a python dict
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Run ParameterServer  & ParameterServerAgent
v1.5.0,Only Run ParameterServer
v1.5.0,Run ParameterServer & Worker(embedded ParameterServerAgent)
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,To Do
v1.5.0,Modify the way to get current Angel version
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Launch the Py4j gateway
v1.5.0,Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc
v1.5.0,Don't send ctrl-c / SIGINT to the Java gateway:
v1.5.0,We use select() here in order to avoid blocking indefinitely if the subprocess dies
v1.5.0,before connecting
v1.5.0,Determine which ephemeral port the server started on:
v1.5.0,Connect to the gateway
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.5.0,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,To do: need python edition of TVector
v1.5.0,To do: need python edition of GetFunc
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Training job to obtain a model
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https:#opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Create an angel job client
v1.5.0,Submit this application
v1.5.0,Create a model
v1.5.0,Load model meta to client
v1.5.0,Run user task
v1.5.0,"Wait for completion,"
v1.5.0,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.5.0,Save the incremental trained model to HDFS
v1.5.0,Stop
v1.5.0,
v1.5.0,Tencent is pleased to support the open source community by making Angel available.
v1.5.0,
v1.5.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.5.0,
v1.5.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.5.0,compliance with the License. You may obtain a copy of the License at
v1.5.0,
v1.5.0,https://opensource.org/licenses/BSD-3-Clause
v1.5.0,
v1.5.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.5.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.5.0,either express or implied. See the License for the specific language governing permissions and
v1.5.0,
v1.5.0,Load model meta
v1.5.0,Convert model
v1.5.0,"Get input path, output path"
v1.5.0,Init serde
v1.5.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.5.0,Load model meta
v1.5.0,Convert model
v1.5.0,load hadoop configuration
v1.5.0,"Get input path, output path"
v1.5.0,Init serde
v1.5.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.5.0,Load model meta
v1.5.0,Check row type
v1.5.0,Load model
v1.5.0,Load model meta
v1.5.0,Check row type
v1.5.0,Load model
v1.5.0,Load model meta
v1.5.0,Check row type
v1.5.0,Load model
v1.5.0,Load model meta
v1.5.0,Check row type
v1.5.0,Load model
v1.5.0,Load model meta
v1.5.0,Check row type
v1.5.0,Load model
v1.5.0,Load model meta
v1.5.0,Check row type
v1.5.0,Load model
v1.5.0,Load model meta
v1.5.0,Check row type
v1.5.0,Load model
v1.5.0,Load model
v1.5.0,load hadoop configuration
v1.5.0,mMatrix.setNnz(100000000);
v1.5.0,mMatrix.setNnz(100000000);
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,worker register
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,add matrix
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,attempt 0
v1.5.0,attempt1
v1.5.0,attempt1
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,TODO Auto-generated constructor stub
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,set basic configuration keys
v1.5.0,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,Thread.sleep(5000);
v1.5.0,"response = master.getJobReport(null, request);"
v1.5.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
v1.5.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
v1.5.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,"DenseIntVector deltaVec = new DenseIntVector(100000, delta);"
v1.5.0,deltaVec.setMatrixId(matrixW1Id);
v1.5.0,deltaVec.setRowId(0);
v1.5.0,TODO Auto-generated constructor stub
v1.5.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
v1.5.0,@RunWith(MockitoJUnitRunner.class)
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,psAgent.initAndStart();
v1.5.0,test conf
v1.5.0,test master location
v1.5.0,test app id
v1.5.0,test user
v1.5.0,test ps agent attempt id
v1.5.0,test connection
v1.5.0,test master client
v1.5.0,test ip
v1.5.0,test loc
v1.5.0,test master location
v1.5.0,test ps location
v1.5.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
v1.5.0,test all ps ids
v1.5.0,test all matrix ids
v1.5.0,test all matrix names
v1.5.0,test matrix attribute
v1.5.0,test matrix meta
v1.5.0,test ps location
v1.5.0,test partitions
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,group0Id = new WorkerGroupId(0);
v1.5.0,"worker0Id = new WorkerId(group0Id, 0);"
v1.5.0,"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);"
v1.5.0,task0Id = new TaskId(0);
v1.5.0,task1Id = new TaskId(1);
v1.5.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.5.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.5.0,test this func in testWriteTo
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,"LOG.info(index[0] + "" "" + value[0]);"
v1.5.0,"LOG.info(index[1] + "" "" + value[1]);"
v1.5.0,"LOG.info(index[2] + "" "" + value[2]);"
v1.5.0,dot
v1.5.0,plus
v1.5.0,plusBy
v1.5.0,dot
v1.5.0,plus
v1.5.0,plusBy
v1.5.0,dot
v1.5.0,plus
v1.5.0,plusBy
v1.5.0,dot
v1.5.0,plusBy
v1.5.0,@Test
v1.5.0,public void dotDenseFloatVector() throws Exception {
v1.5.0,int dim = 1000;
v1.5.0,Random random = new Random(System.currentTimeMillis());
v1.5.0,
v1.5.0,double[] values = new double[dim];
v1.5.0,float[] values_1 = new float[dim];
v1.5.0,for (int i = 0; i < dim; i++) {
v1.5.0,values[i] = random.nextDouble();
v1.5.0,values_1[i] = random.nextFloat();
v1.5.0,}
v1.5.0,
v1.5.0,"DenseDoubleVector vec = new DenseDoubleVector(dim, values);"
v1.5.0,"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);"
v1.5.0,
v1.5.0,double sum = 0.0;
v1.5.0,for (int i = 0; i < dim; i++) {
v1.5.0,sum += values[i] * values_1[i];
v1.5.0,}
v1.5.0,
v1.5.0,"assertEquals(sum, vec.dot(vec_1));"
v1.5.0,
v1.5.0,}
v1.5.0,@Test
v1.5.0,public void plusDenseFlaotVector() throws Exception {
v1.5.0,"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};"
v1.5.0,"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};"
v1.5.0,"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);"
v1.5.0,"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);"
v1.5.0,
v1.5.0,TDoubleVector vec_2 = vec.plus(vec_1);
v1.5.0,for (int i = 0; i < vec.size(); i++)
v1.5.0,"assertEquals(value_1[i] + value_2[i], vec_2.get(i));"
v1.5.0,
v1.5.0,
v1.5.0,"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);"
v1.5.0,
v1.5.0,for (int i = 0; i < vec.size(); i++)
v1.5.0,"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);"
v1.5.0,
v1.5.0,double[] oldValues = vec.getValues().clone();
v1.5.0,
v1.5.0,vec.plusBy(vec_1);
v1.5.0,
v1.5.0,for (int i = 0; i < vec.size(); i++)
v1.5.0,"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));"
v1.5.0,
v1.5.0,oldValues = vec.getValues().clone();
v1.5.0,
v1.5.0,"vec.plusBy(vec_1, 3);"
v1.5.0,
v1.5.0,for (int i = 0; i < vec.size(); i++)
v1.5.0,"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));"
v1.5.0,}
v1.5.0,dot
v1.5.0,plus
v1.5.0,plusBy
v1.5.0,dot
v1.5.0,plus
v1.5.0,plusBy
v1.5.0,@Test
v1.5.0,public void plusBy3() throws Exception {
v1.5.0,"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};"
v1.5.0,"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);"
v1.5.0,"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});"
v1.5.0,vec.setRowId(0);
v1.5.0,"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});"
v1.5.0,vec_1.setRowId(1);
v1.5.0,TDoubleVector vec_2 = new SparseDoubleVector(2);
v1.5.0,"vec_2.set(1, 1.0);"
v1.5.0,vec_2.setRowId(0);
v1.5.0,
v1.5.0,mat.plusBy(vec);
v1.5.0,mat.plusBy(vec_1);
v1.5.0,mat.plusBy(vec_2);
v1.5.0,
v1.5.0,"assertEquals(2.0f, mat.get(0, 0));"
v1.5.0,"assertEquals(4.0f, mat.get(0, 1));"
v1.5.0,"assertEquals(4.0f, mat.get(1, 0));"
v1.5.0,"assertEquals(5.0f, mat.get(1, 1));"
v1.5.0,}
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add dense double matrix
v1.5.0,add sparse double matrix
v1.5.0,add component sparse double matrix
v1.5.0,add sparse long-key double matrix
v1.5.0,add component long-key sparse double matrix
v1.5.0,add dense float matrix
v1.5.0,add sparse float matrix
v1.5.0,add component sparse float matrix
v1.5.0,add dense float matrix
v1.5.0,add sparse float matrix
v1.5.0,add component sparse float matrix
v1.5.0,Start PS
v1.5.0,Start to run application
v1.5.0,Assert.assertTrue(index.length == row.size());
v1.5.0,Assert.assertTrue(index.length == row.size());
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,test worker getActiveTaskNum
v1.5.0,test worker getTaskNum
v1.5.0,test worker getTaskManager
v1.5.0,test workerId
v1.5.0,test workerAttemptId
v1.5.0,tet worker initFinished
v1.5.0,test worker getInitMinclock
v1.5.0,test worker loacation
v1.5.0,test AppId
v1.5.0,test Conf
v1.5.0,test UserName
v1.5.0,master location
v1.5.0,masterClient
v1.5.0,test psAgent
v1.5.0,test worker get dataBlockManager
v1.5.0,workerGroup.getSplits();
v1.5.0,application
v1.5.0,lcation
v1.5.0,workerGroup info
v1.5.0,worker info
v1.5.0,task
v1.5.0,Matrix parameters
v1.5.0,Set basic configuration keys
v1.5.0,Use local deploy mode and dummy data spliter
v1.5.0,Create an Angel client
v1.5.0,Add different types of matrix
v1.5.0,using mock object
v1.5.0,verification
v1.5.0,Stubbing
v1.5.0,Default does nothing.
v1.5.0,The app injection is optional
v1.5.0,"renderText(""hello world"");"
v1.5.0,"user choose a workerGroupID from the workergroups page,"
v1.5.0,now we should change the AngelApp params and render the workergroup page;
v1.5.0,"static final String WORKER_ID = ""worker.id"";"
v1.5.0,"div(""#logo"")."
v1.5.0,"img(""/static/hadoop-st.png"")._()."
v1.5.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
v1.5.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
v1.5.0,JQueryUI.jsnotice(html);
v1.5.0,import org.apache.hadoop.conf.Configuration;
v1.5.0,import java.lang.reflect.Field;
v1.5.0,get block locations from file system
v1.5.0,create a list of all block and their locations
v1.5.0,"if the file is not splitable, just create the one block with"
v1.5.0,full file length
v1.5.0,each split can be a maximum of maxSize
v1.5.0,if remainder is between max and 2*max - then
v1.5.0,"instead of creating splits of size max, left-max we"
v1.5.0,create splits of size left/2 and left/2. This is
v1.5.0,a heuristic to avoid creating really really small
v1.5.0,splits.
v1.5.0,add this block to the block --> node locations map
v1.5.0,"For blocks that do not have host/rack information,"
v1.5.0,assign to default  rack.
v1.5.0,add this block to the rack --> block map
v1.5.0,Add this host to rackToNodes map
v1.5.0,add this block to the node --> block map
v1.5.0,"if the file system does not have any rack information, then"
v1.5.0,use dummy rack location.
v1.5.0,The topology paths have the host name included as the last
v1.5.0,component. Strip it.
v1.5.0,get tokens for all the required FileSystems..
v1.5.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
v1.5.0,job.getConfiguration());
v1.5.0,Whether we need to recursive look into the directory structure
v1.5.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.5.0,user provided one (if any).
v1.5.0,all the files in input set
v1.5.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
v1.5.0,process all nodes and create splits that are local to a node. Generate
v1.5.0,"one split per node iteration, and walk over nodes multiple times to"
v1.5.0,distribute the splits across nodes.
v1.5.0,Skip the node if it has previously been marked as completed.
v1.5.0,"for each block, copy it into validBlocks. Delete it from"
v1.5.0,blockToNodes so that the same block does not appear in
v1.5.0,two different splits.
v1.5.0,Remove all blocks which may already have been assigned to other
v1.5.0,splits.
v1.5.0,"if the accumulated split size exceeds the maximum, then"
v1.5.0,create this split.
v1.5.0,create an input split and add it to the splits array
v1.5.0,Remove entries from blocksInNode so that we don't walk these
v1.5.0,again.
v1.5.0,Done creating a single split for this node. Move on to the next
v1.5.0,node so that splits are distributed across nodes.
v1.5.0,This implies that the last few blocks (or all in case maxSize=0)
v1.5.0,were not part of a split. The node is complete.
v1.5.0,if there were any blocks left over and their combined size is
v1.5.0,"larger than minSplitNode, then combine them into one split."
v1.5.0,Otherwise add them back to the unprocessed pool. It is likely
v1.5.0,that they will be combined with other blocks from the
v1.5.0,same rack later on.
v1.5.0,This condition also kicks in when max split size is not set. All
v1.5.0,blocks on a node will be grouped together into a single split.
v1.5.0,haven't created any split on this machine. so its ok to add a
v1.5.0,smaller one for parallelism. Otherwise group it in the rack for
v1.5.0,balanced size create an input split and add it to the splits
v1.5.0,array
v1.5.0,Remove entries from blocksInNode so that we don't walk this again.
v1.5.0,The node is done. This was the last set of blocks for this node.
v1.5.0,Put the unplaced blocks back into the pool for later rack-allocation.
v1.5.0,Node is done. All blocks were fit into node-local splits.
v1.5.0,Check if node-local assignments are complete.
v1.5.0,All nodes have been walked over and marked as completed or all blocks
v1.5.0,have been assigned. The rest should be handled via rackLock assignment.
v1.5.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
v1.5.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
v1.5.0,"if blocks in a rack are below the specified minimum size, then keep them"
v1.5.0,"in 'overflow'. After the processing of all racks is complete, these"
v1.5.0,overflow blocks will be combined into splits.
v1.5.0,Process all racks over and over again until there is no more work to do.
v1.5.0,Create one split for this rack before moving over to the next rack.
v1.5.0,Come back to this rack after creating a single split for each of the
v1.5.0,remaining racks.
v1.5.0,"Process one rack location at a time, Combine all possible blocks that"
v1.5.0,reside on this rack as one split. (constrained by minimum and maximum
v1.5.0,split size).
v1.5.0,iterate over all racks
v1.5.0,"for each block, copy it into validBlocks. Delete it from"
v1.5.0,blockToNodes so that the same block does not appear in
v1.5.0,two different splits.
v1.5.0,"if the accumulated split size exceeds the maximum, then"
v1.5.0,create this split.
v1.5.0,create an input split and add it to the splits array
v1.5.0,"if we created a split, then just go to the next rack"
v1.5.0,"if there is a minimum size specified, then create a single split"
v1.5.0,"otherwise, store these blocks into overflow data structure"
v1.5.0,There were a few blocks in this rack that
v1.5.0,remained to be processed. Keep them in 'overflow' block list.
v1.5.0,These will be combined later.
v1.5.0,Process all overflow blocks
v1.5.0,"This might cause an exiting rack location to be re-added,"
v1.5.0,but it should be ok.
v1.5.0,"if the accumulated split size exceeds the maximum, then"
v1.5.0,create this split.
v1.5.0,create an input split and add it to the splits array
v1.5.0,"Process any remaining blocks, if any."
v1.5.0,create an input split
v1.5.0,add this split to the list that is returned
v1.5.0,long num = totLength / maxSize;
v1.5.0,all blocks for all the files in input set
v1.5.0,mapping from a rack name to the list of blocks it has
v1.5.0,mapping from a block to the nodes on which it has replicas
v1.5.0,mapping from a node to the list of blocks that it contains
v1.5.0,populate all the blocks for all files
v1.5.0,stop all services
v1.5.0,1.write application state to file so that the client can get the state of the application
v1.5.0,if master exit
v1.5.0,2.clear tmp and staging directory
v1.5.0,waiting for client to get application state
v1.5.0,stop the RPC server
v1.5.0,"Security framework already loaded the tokens into current UGI, just use"
v1.5.0,them
v1.5.0,Now remove the AM->RM token so tasks don't have it
v1.5.0,add a shutdown hook
v1.5.0,init app state storage
v1.5.0,init event dispacher
v1.5.0,init location manager
v1.5.0,init container allocator
v1.5.0,init a rpc service
v1.5.0,recover matrix meta if needed
v1.5.0,recover ps attempt information if need
v1.5.0,Init Client manager
v1.5.0,Init PS Client manager
v1.5.0,init parameter server manager
v1.5.0,recover task information if needed
v1.5.0,a dummy data spliter is just for test now
v1.5.0,recover data splits information if needed
v1.5.0,init worker manager and register worker manager event
v1.5.0,register slow worker/ps checker
v1.5.0,register app manager event and finish event
v1.5.0,start a web service if use yarn deploy mode
v1.5.0,load from app state storage first if attempt index great than 1(the master is not the first
v1.5.0,retry)
v1.5.0,"if load failed, just build a new MatrixMetaManager"
v1.5.0,load ps attempt index from app state storage first if attempt index great than 1(the master
v1.5.0,is not the first retry)
v1.5.0,load task information from app state storage first if attempt index great than 1(the master
v1.5.0,is not the first retry)
v1.5.0,"if load failed, just build a new AMTaskManager"
v1.5.0,load data splits information from app state storage first if attempt index great than 1(the
v1.5.0,master is not the first retry)
v1.5.0,"if load failed, we need to recalculate the data splits"
v1.5.0,Check Workers
v1.5.0,Check PSS
v1.5.0,Check Clients
v1.5.0,Check PS Clients
v1.5.0,parse parameter server counters
v1.5.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.5.0,refresh last heartbeat timestamp
v1.5.0,send a state update event to the specific PSAttempt
v1.5.0,check if parameter server can commit now.
v1.5.0,check matrix metadata inconsistencies between master and parameter server.
v1.5.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
v1.5.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
v1.5.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.5.0,choose a unused port
v1.5.0,start RPC server
v1.5.0,remove this parameter server attempt from monitor set
v1.5.0,remove this parameter server attempt from monitor set
v1.5.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.5.0,find workergroup in worker manager
v1.5.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
v1.5.0,"if this worker group run over, just return WORKERGROUP_EXITED"
v1.5.0,"if this worker group is running now, return tasks, workers, data splits for it"
v1.5.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.5.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.5.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.5.0,"in ANGEL_PS mode, task id may can not know advance"
v1.5.0,update the clock for this matrix
v1.5.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.5.0,"in ANGEL_PS mode, task id may can not know advance"
v1.5.0,update task iteration
v1.5.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
v1.5.0,the number of splits equal to the number of tasks
v1.5.0,split data
v1.5.0,dispatch the splits to workergroups
v1.5.0,split data
v1.5.0,dispatch the splits to workergroups
v1.5.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.5.0,"first, then divided by expected split number"
v1.5.0,get input format class from configuration and then instantiation a input format object
v1.5.0,split data
v1.5.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.5.0,"first, then divided by expected split number"
v1.5.0,get input format class from configuration and then instantiation a input format object
v1.5.0,split data
v1.5.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.5.0,need to fine tune the number of workergroup and task based on the actual split number
v1.5.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.5.0,Record the location information for the splits in order to data localized schedule
v1.5.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.5.0,need to fine tune the number of workergroup and task based on the actual split number
v1.5.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.5.0,Record the location information for the splits in order to data localized schedule
v1.5.0,write meta data to a temporary file
v1.5.0,rename the temporary file to final file
v1.5.0,"if the file exists, read from file and deserialize it"
v1.5.0,write task meta
v1.5.0,write ps meta
v1.5.0,generate a temporary file
v1.5.0,write task meta to the temporary file first
v1.5.0,rename the temporary file to the final file
v1.5.0,"if last final task file exist, remove it"
v1.5.0,find task meta file which has max timestamp
v1.5.0,"if the file does not exist, just return null"
v1.5.0,read task meta from file and deserialize it
v1.5.0,generate a temporary file
v1.5.0,write ps meta to the temporary file first.
v1.5.0,rename the temporary file to the final file
v1.5.0,"if the old final file exist, just remove it"
v1.5.0,find ps meta file
v1.5.0,"if ps meta file does not exist, just return null"
v1.5.0,read ps meta from file and deserialize it
v1.5.0,Init matrix files meta
v1.5.0,Move output files
v1.5.0,Write the meta file
v1.5.0,check whether psagent heartbeat timeout
v1.5.0,Set up the launch command
v1.5.0,Duplicate the ByteBuffers for access by multiple containers.
v1.5.0,Construct the actual Container
v1.5.0,Application resources
v1.5.0,Application environment
v1.5.0,Service data
v1.5.0,Tokens
v1.5.0,Set up JobConf to be localized properly on the remote NM.
v1.5.0,Setup DistributedCache
v1.5.0,Setup up task credentials buffer
v1.5.0,LocalStorageToken is needed irrespective of whether security is enabled
v1.5.0,or not.
v1.5.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
v1.5.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.5.0,Construct the actual Container
v1.5.0,The null fields are per-container and will be constructed for each
v1.5.0,container separately.
v1.5.0,Set up the launch command
v1.5.0,Duplicate the ByteBuffers for access by multiple containers.
v1.5.0,Construct the actual Container
v1.5.0,"a * in the classpath will only find a .jar, so we need to filter out"
v1.5.0,all .jars and add everything else
v1.5.0,Propagate the system classpath when using the mini cluster
v1.5.0,Add standard Hadoop classes
v1.5.0,Cache archives
v1.5.0,Cache files
v1.5.0,Sanity check
v1.5.0,Add URI fragment or just the filename
v1.5.0,Add the env variables passed by the user
v1.5.0,Set logging level in the environment.
v1.5.0,Setup the log4j prop
v1.5.0,Add main class and its arguments
v1.5.0,Finally add the jvmID
v1.5.0,vargs.add(String.valueOf(jvmID.getId()));
v1.5.0,Final commmand
v1.5.0,Add the env variables passed by the user
v1.5.0,Set logging level in the environment.
v1.5.0,Setup the log4j prop
v1.5.0,Add main class and its arguments
v1.5.0,Final commmand
v1.5.0,"if amTask is not null, we should clone task state from it"
v1.5.0,"if all parameter server complete commit, master can commit now"
v1.5.0,init and start master committer
v1.5.0,check whether parameter server heartbeat timeout
v1.5.0,Transitions from the NEW state.
v1.5.0,Transitions from the UNASSIGNED state.
v1.5.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
v1.5.0,Transitions from the ASSIGNED state.
v1.5.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
v1.5.0,PA_CONTAINER_LAUNCHED event
v1.5.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.5.0,Transitions from the PSAttemptStateInternal.COMMITTING state
v1.5.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.5.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.5.0,create the topology tables
v1.5.0,reqeuest resource:send a resource request to the resource allocator
v1.5.0,"Once the resource is applied, build and send the launch request to the container launcher"
v1.5.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
v1.5.0,resource allocator
v1.5.0,set the launch time
v1.5.0,add the ps attempt to the heartbeat timeout monitoring list
v1.5.0,parse ps attempt location and put it to location manager
v1.5.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
v1.5.0,or failed
v1.5.0,remove ps attempt id from heartbeat timeout monitor list
v1.5.0,release container:send a release request to container launcher
v1.5.0,set the finish time only if launch time is set
v1.5.0,private long scheduledTime;
v1.5.0,Transitions from the NEW state.
v1.5.0,Transitions from the SCHEDULED state.
v1.5.0,Transitions from the RUNNING state.
v1.5.0,"another attempt launched,"
v1.5.0,Transitions from the SUCCEEDED state
v1.5.0,Transitions from the KILLED state
v1.5.0,Transitions from the FAILED state
v1.5.0,add diagnostic
v1.5.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.5.0,Refresh ps location & matrix meta
v1.5.0,start a new attempt for this ps
v1.5.0,notify ps manager
v1.5.0,"getContext().getLocationManager().setPsLocation(id, null);"
v1.5.0,add diagnostic
v1.5.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.5.0,start a new attempt for this ps
v1.5.0,notify ps manager
v1.5.0,notify the event handler of state change
v1.5.0,"If new state is not RUNNING and COMMITTING, add it to state timeout monitor"
v1.5.0,"if forcedState is set, just return"
v1.5.0,else get state from state machine
v1.5.0,add this worker group to the success set
v1.5.0,check if all worker group run over
v1.5.0,add this worker group to the failed set
v1.5.0,check if too many worker groups are failed or killed
v1.5.0,notify a run failed event
v1.5.0,add this worker group to the failed set
v1.5.0,check if too many worker groups are failed or killed
v1.5.0,notify a run failed event
v1.5.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
v1.5.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
v1.5.0,just return the total task number now
v1.5.0,TODO
v1.5.0,check whether worker heartbeat timeout
v1.5.0,"if workerAttempt is not null, we should clone task state from it"
v1.5.0,from NEW state
v1.5.0,from SCHEDULED state
v1.5.0,get data splits location for data locality
v1.5.0,reqeuest resource:send a resource request to the resource allocator
v1.5.0,"once the resource is applied, build and send the launch request to the container launcher"
v1.5.0,notify failed message to the worker
v1.5.0,notify killed message to the worker
v1.5.0,release the allocated container
v1.5.0,notify failed message to the worker
v1.5.0,remove the worker attempt from heartbeat timeout listen list
v1.5.0,release the allocated container
v1.5.0,notify killed message to the worker
v1.5.0,remove the worker attempt from heartbeat timeout listen list
v1.5.0,clean the container
v1.5.0,notify failed message to the worker
v1.5.0,remove the worker attempt from heartbeat timeout listen list
v1.5.0,record the finish time
v1.5.0,clean the container
v1.5.0,notify killed message to the worker
v1.5.0,remove the worker attempt from heartbeat timeout listening list
v1.5.0,record the finish time
v1.5.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
v1.5.0,set worker attempt location
v1.5.0,notify the register message to the worker
v1.5.0,record the launch time
v1.5.0,update worker attempt metrics
v1.5.0,update tasks metrics
v1.5.0,clean the container
v1.5.0,notify the worker attempt run successfully message to the worker
v1.5.0,record the finish time
v1.5.0,init a worker attempt for the worker
v1.5.0,schedule the worker attempt
v1.5.0,add diagnostic
v1.5.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.5.0,init and start a new attempt for this ps
v1.5.0,notify worker manager
v1.5.0,add diagnostic
v1.5.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.5.0,init and start a new attempt for this ps
v1.5.0,notify worker manager
v1.5.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
v1.5.0,register to Yarn RM
v1.5.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
v1.5.0,"catch YarnRuntimeException, we should exit and need not retry"
v1.5.0,build heartbeat request
v1.5.0,send heartbeat request to rm
v1.5.0,"This can happen if the RM has been restarted. If it is in that state,"
v1.5.0,this application must clean itself up.
v1.5.0,Setting NMTokens
v1.5.0,assgin containers
v1.5.0,"if some container is not assigned, release them"
v1.5.0,handle finish containers
v1.5.0,dispatch container exit message to corresponding components
v1.5.0,killed by framework
v1.5.0,killed by framework
v1.5.0,get application finish state
v1.5.0,build application diagnostics
v1.5.0,TODO:add a job history for angel
v1.5.0,build unregister request
v1.5.0,send unregister request to rm
v1.5.0,Note this down for next interaction with ResourceManager
v1.5.0,based on blacklisting comments above we can end up decrementing more
v1.5.0,than requested. so guard for that.
v1.5.0,send the updated resource request to RM
v1.5.0,send 0 container count requests also to cancel previous requests
v1.5.0,Update resource requests
v1.5.0,try to assign to all nodes first to match node local
v1.5.0,try to match all rack local
v1.5.0,assign remaining
v1.5.0,Update resource requests
v1.5.0,send the container-assigned event to task attempt
v1.5.0,build the start container request use launch context
v1.5.0,send the start request to Yarn nm
v1.5.0,send the message that the container starts successfully to the corresponding component
v1.5.0,"after launching, send launched event to task attempt to move"
v1.5.0,it from ASSIGNED to RUNNING state
v1.5.0,send the message that the container starts failed to the corresponding component
v1.5.0,kill the remote container if already launched
v1.5.0,start a thread pool to startup the container
v1.5.0,See if we need up the pool size only if haven't reached the
v1.5.0,maximum limit yet.
v1.5.0,nodes where containers will run at *this* point of time. This is
v1.5.0,*not* the cluster size and doesn't need to be.
v1.5.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
v1.5.0,later is just a buffer so we are not always increasing the
v1.5.0,pool-size
v1.5.0,the events from the queue are handled in parallel
v1.5.0,using a thread pool
v1.5.0,return if already stopped
v1.5.0,shutdown any containers that might be left running
v1.5.0,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.5.0,get matrix ids in the parameter server report
v1.5.0,get the matrices parameter server need to create and delete
v1.5.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
v1.5.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
v1.5.0,Init control connection manager
v1.5.0,Get ps locations from master and put them to the location cache.
v1.5.0,Build and initialize rpc client to master
v1.5.0,Get psagent id
v1.5.0,Build PS control rpc client manager
v1.5.0,Build local location
v1.5.0,Initialize matrix meta information
v1.5.0,Start all services
v1.5.0,Stop all modules
v1.5.0,Stop all modules
v1.5.0,Update generic resource counters
v1.5.0,Updating resources specified in ResourceCalculatorProcessTree
v1.5.0,Remove the CPU time consumed previously by JVM reuse
v1.5.0,Generate a flush request and put it to request queue
v1.5.0,Generate a clock request and put it to request queue
v1.5.0,Generate a merge request and put it to request queue
v1.5.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
v1.5.0,matrix
v1.5.0,and add it to cache maps
v1.5.0,Add the message to the tree map
v1.5.0,"If there are flush / clock requests blocked, we need to put this merge request into"
v1.5.0,the waiting queue
v1.5.0,Launch a merge worker to merge the update to matrix op log cache
v1.5.0,Remove the message from the tree map
v1.5.0,Wake up blocked flush/clock request
v1.5.0,Add flush/clock request to listener list to waiting for all the existing
v1.5.0,updates are merged
v1.5.0,Wake up blocked flush/clock request
v1.5.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
v1.5.0,blocked.
v1.5.0,Get next merge message sequence id
v1.5.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
v1.5.0,position
v1.5.0,Wake up blocked merge requests
v1.5.0,Get minimal sequence id from listeners
v1.5.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
v1.5.0,should flush updates to local matrix storage
v1.5.0,unused now
v1.5.0,"Filter it, removing zero values"
v1.5.0,Doing average or not
v1.5.0,Split this row according the matrix partitions
v1.5.0,Add the splits to the result container
v1.5.0,"For each partition, we generate a update split."
v1.5.0,"Although the split is empty for partitions those without any update data,"
v1.5.0,we still need to generate a update split to update the clock info on ps.
v1.5.0,"For each partition, we generate a update split."
v1.5.0,"Although the split is empty for partitions those without any update data,"
v1.5.0,we still need to generate a update split to update the clock info on ps.
v1.5.0,"LOG.debug(""receive a message "" + ((ByteBuf) msg).readableBytes());"
v1.5.0,TODO: use Epoll for linux future
v1.5.0,Update location table
v1.5.0,Remove the server from failed list
v1.5.0,Notify refresh success message to request dispatcher
v1.5.0,Check PS exist or not
v1.5.0,Check heartbeat timeout
v1.5.0,Check PS restart or not
v1.5.0,private final HashSet<ParameterServerId> refreshingServerSet;
v1.5.0,Add it to failed rpc list
v1.5.0,Add the server to gray server list
v1.5.0,Add it to failed rpc list
v1.5.0,Add the server to gray server list
v1.5.0,Move from gray server list to failed server list
v1.5.0,Handle the RPCS to this server
v1.5.0,Submit the schedulable failed get RPCS
v1.5.0,Submit new get RPCS
v1.5.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.5.0,"If the queue is empty, just return 0"
v1.5.0,"If request is not over limit, just submit it"
v1.5.0,Submit the schedulable failed get RPCS
v1.5.0,Submit new put RPCS
v1.5.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.5.0,"LOG.info(""choose put server "" + psIds[index]);"
v1.5.0,Check all pending RPCS
v1.5.0,Check get channel context
v1.5.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
v1.5.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
v1.5.0,Check all failed PUT RPCS and put it to schedulable list for re-schedule
v1.5.0,&& (ts - item.getContext().getFailedTs() >= retryIntervalMs)) {
v1.5.0,"for(Entry<PSLocation, Long> entry : psLocToLastChannelTsMap.entrySet()) {"
v1.5.0,if(ts - entry.getValue() > requestTimeOut * 2)  {
v1.5.0,"LOG.error(""Can not get channel for PS "" + entry.getKey() + "" over "" + (ts - entry.getValue())"
v1.5.0,"+ "" milliseconds, close all channels to it"");"
v1.5.0,closeChannels(entry.getKey());
v1.5.0,"psLocToLastChannelTsMap.put(entry.getKey(), ts);"
v1.5.0,}
v1.5.0,}
v1.5.0,"LOG.debug(""request failed "" + request + "", failedType="" + failedType + "", errorLog="" + errorLog);"
v1.5.0,Remove all pending RPCS
v1.5.0,Close all channel to this PS
v1.5.0,Get server id and location for this request
v1.5.0,"If location is null, means that the server is not ready"
v1.5.0,Get the channel for the location
v1.5.0,Check if need get token first
v1.5.0,Serialize the request
v1.5.0,Send the request
v1.5.0,get a channel to server from pool
v1.5.0,"if channel is not valid, it means maybe the connections to the server are closed"
v1.5.0,Allocate the bytebuf and serialize the request
v1.5.0,find the partition request context from cache
v1.5.0,Check if the result of the sub-request is received
v1.5.0,Update received result number
v1.5.0,Get row splits received
v1.5.0,Put the row split to the cache(row index to row splits map)
v1.5.0,"If all splits of the row are received, means this row can be merged"
v1.5.0,TODO Auto-generated method stub
v1.5.0,"Check futures, if the result of a sub-request is received, put it to the result queue"
v1.5.0,Now we just support pipelined row splits merging for dense type row
v1.5.0,Wait until the clock value of this row is greater than or equal to the value
v1.5.0,Get partitions for this row
v1.5.0,First get this row from matrix storage
v1.5.0,"If the row exists in the matrix storage and the clock value meets the requirements, just"
v1.5.0,return
v1.5.0,Get row splits of this row from the matrix cache first
v1.5.0,"If the row split does not exist in cache, get it from parameter server"
v1.5.0,Wait the final result
v1.5.0,Put it to the matrix cache
v1.5.0,Split the matrix oplog according to the matrix partitions
v1.5.0,"If need update clock, we should send requests to all partitions"
v1.5.0,Filter the rowIds which are fetching now
v1.5.0,Send the rowIndex to rpc dispatcher and return immediately
v1.5.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
v1.5.0,Generate dispatch items and add them to the corresponding queues
v1.5.0,Filter the rowIds which are fetching now
v1.5.0,Pre-fetching is disable default
v1.5.0,matrix id to clock map
v1.5.0,"task index, it must be unique for whole application"
v1.5.0,Deserialize data splits meta
v1.5.0,Get workers
v1.5.0,Send request to every ps
v1.5.0,Wait the responses
v1.5.0,Update clock cache
v1.5.0,if(syncNum % 1024 == 0) {
v1.5.0,}
v1.5.0,"Use simple flow, do not use any cache"
v1.5.0,Get row from cache.
v1.5.0,"if row clock is satisfy ssp staleness limit, just return."
v1.5.0,Get row from ps.
v1.5.0,"For ASYNC mode, just get from pss."
v1.5.0,"For BSP/SSP, get rows from storage/cache first"
v1.5.0,Get from ps.
v1.5.0,"For ASYNC, just get rows from pss."
v1.5.0,no more retries.
v1.5.0,calculate sleep time and return.
v1.5.0,parse the i-th sleep-time
v1.5.0,parse the i-th number-of-retries
v1.5.0,calculateSleepTime may overflow.
v1.5.0,"A few common retry policies, with no delays."
v1.5.0,close is a local operation and should finish within milliseconds; timeout just to be safe
v1.5.0,response will be null for one way messages.
v1.5.0,maxFrameLength = 2G
v1.5.0,lengthFieldOffset = 0
v1.5.0,lengthFieldLength = 8
v1.5.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
v1.5.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
v1.5.0,indicates whether this connection's life cycle is managed
v1.5.0,See if we already have a connection (common case)
v1.5.0,create a unique lock for this RS + protocol (if necessary)
v1.5.0,get the RS lock
v1.5.0,do one more lookup in case we were stalled above
v1.5.0,Only create isa when we need to.
v1.5.0,definitely a cache miss. establish an RPC for
v1.5.0,this RS
v1.5.0,Throw what the RemoteException was carrying.
v1.5.0,check
v1.5.0,every
v1.5.0,minutes
v1.5.0,TODO
v1.5.0,创建failoverHandler
v1.5.0,"The number of times this invocation handler has ever been failed over,"
v1.5.0,before this method invocation attempt. Used to prevent concurrent
v1.5.0,failed method invocations from triggering multiple failover attempts.
v1.5.0,Make sure that concurrent failed method invocations
v1.5.0,only cause a
v1.5.0,single actual fail over.
v1.5.0,RpcController + Message in the method args
v1.5.0,(generated code from RPC bits in .proto files have
v1.5.0,RpcController)
v1.5.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
v1.5.0,+ (System.currentTimeMillis() - beforeConstructTs));
v1.5.0,get an instance of the method arg type
v1.5.0,RpcController + Message in the method args
v1.5.0,(generated code from RPC bits in .proto files have
v1.5.0,RpcController)
v1.5.0,Message (hand written code usually has only a single
v1.5.0,argument)
v1.5.0,log any RPC responses that are slower than the configured
v1.5.0,warn
v1.5.0,response time or larger than configured warning size
v1.5.0,"when tagging, we let TooLarge trump TooSmall to keep"
v1.5.0,output simple
v1.5.0,note that large responses will often also be slow.
v1.5.0,provides a count of log-reported slow responses
v1.5.0,RpcController + Message in the method args
v1.5.0,(generated code from RPC bits in .proto files have
v1.5.0,RpcController)
v1.5.0,unexpected
v1.5.0,"in the protobuf methods, args[1] is the only significant argument"
v1.5.0,for JSON encoding
v1.5.0,base information that is reported regardless of type of call
v1.5.0,Disable Nagle's Algorithm since we don't want packets to wait
v1.5.0,Configure the event pipeline factory.
v1.5.0,Make a new connection.
v1.5.0,Remove all pending requests (will be canceled after relinquishing
v1.5.0,write lock).
v1.5.0,Cancel any pending requests by sending errors to the callbacks:
v1.5.0,Close the channel:
v1.5.0,Close the connection:
v1.5.0,Shut down all thread pools to exit.
v1.5.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
v1.5.0,See NettyServer.prepareResponse for where we write out the response.
v1.5.0,"It writes the call.id (int), a boolean signifying any error (and if"
v1.5.0,"so the exception name/trace), and the response bytes"
v1.5.0,Read the call id.
v1.5.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
v1.5.0,"instead, it returns a null message object."
v1.5.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
v1.5.0,System.currentTimeMillis());
v1.5.0,"It would be good widen this to just Throwable, but IOException is what we"
v1.5.0,allow now
v1.5.0,not implemented
v1.5.0,not implemented
v1.5.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
v1.5.0,cache of RpcEngines by protocol
v1.5.0,return the RpcEngine configured to handle a protocol
v1.5.0,We only handle the ConnectException.
v1.5.0,This is the exception we can't handle.
v1.5.0,check if timed out
v1.5.0,wait for retry
v1.5.0,IGNORE
v1.5.0,return the RpcEngine that handles a proxy object
v1.5.0,The default implementation works synchronously
v1.5.0,punt: allocate a new buffer & copy into it
v1.5.0,Parse cmd parameters
v1.5.0,load hadoop configuration
v1.5.0,load angel system configuration
v1.5.0,load user configuration:
v1.5.0,load user config file
v1.5.0,load command line parameters
v1.5.0,load user job resource files
v1.5.0,load user job jar if it exist
v1.5.0,Expand the environment variable
v1.5.0,Add default fs(local fs) for lib jars.
v1.5.0,"LOG.info(System.getProperty(""user.dir""));"
v1.5.0,get tokens for all the required FileSystems..
v1.5.0,Whether we need to recursive look into the directory structure
v1.5.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.5.0,user provided one (if any).
v1.5.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.5.0,get tokens for all the required FileSystems..
v1.5.0,Whether we need to recursive look into the directory structure
v1.5.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.5.0,user provided one (if any).
v1.5.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.5.0,a simple hdfs copy function assume src path and dest path are in same hdfs
v1.5.0,and FileSystem object has same schema
v1.5.0,"LOG.warn(""interrupted while sleeping"", ie);"
v1.5.0,public static String getHostname() {
v1.5.0,try {
v1.5.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
v1.5.0,} catch (UnknownHostException uhe) {
v1.5.0,}
v1.5.0,"return new StringBuilder().append("""").append(uhe).toString();"
v1.5.0,}
v1.5.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
v1.5.0,String hostname = getHostname();
v1.5.0,String classname = clazz.getSimpleName();
v1.5.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
v1.5.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
v1.5.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
v1.5.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
v1.5.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
v1.5.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
v1.5.0,
v1.5.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
v1.5.0,public void run() {
v1.5.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
v1.5.0,"this.val$classname + "" at "" + this.val$hostname}));"
v1.5.0,}
v1.5.0,});
v1.5.0,}
v1.5.0,"We we interrupted because we're meant to stop? If not, just"
v1.5.0,continue ignoring the interruption
v1.5.0,Recalculate waitTime.
v1.5.0,// Begin delegation to Thread
v1.5.0,// End delegation to Thread
v1.5.0,instance submitter class
v1.5.0,Obtain filename from path
v1.5.0,Split filename to prexif and suffix (extension)
v1.5.0,Check if the filename is okay
v1.5.0,Prepare temporary file
v1.5.0,Prepare buffer for data copying
v1.5.0,Open and check input stream
v1.5.0,Open output stream and copy data between source file in JAR and the temporary file
v1.5.0,"If read/write fails, close streams safely before throwing an exception"
v1.5.0,"Finally, load the library"
v1.5.0,little endian load order
v1.5.0,tail
v1.5.0,fallthrough
v1.5.0,fallthrough
v1.5.0,finalization
v1.5.0,fmix(h1);
v1.5.0,----------
v1.5.0,body
v1.5.0,----------
v1.5.0,tail
v1.5.0,----------
v1.5.0,finalization
v1.5.0,----------
v1.5.0,body
v1.5.0,----------
v1.5.0,tail
v1.5.0,----------
v1.5.0,finalization
v1.5.0,JobStateProto jobState = report.getJobState();
v1.5.0,Used for java code to get a AngelClient instance
v1.5.0,Used for python code to get a AngelClient instance
v1.5.0,the leaf level file should be readable by others
v1.5.0,the subdirs in the path should have execute permissions for
v1.5.0,others
v1.5.0,2.get job id
v1.5.0,Credentials credentials = new Credentials();
v1.5.0,4.copy resource files to hdfs
v1.5.0,5.write configuration to a xml file
v1.5.0,6.create am container context
v1.5.0,7.Submit to ResourceManager
v1.5.0,8.get app master client
v1.5.0,Create a number of filenames in the JobTracker's fs namespace
v1.5.0,add all the command line files/ jars and archive
v1.5.0,first copy them to jobtrackers filesystem
v1.5.0,should not throw a uri exception
v1.5.0,should not throw an uri excpetion
v1.5.0,set the timestamps of the archives and files
v1.5.0,set the public/private visibility of the archives and files
v1.5.0,get DelegationToken for each cached file
v1.5.0,check if we do not need to copy the files
v1.5.0,is jt using the same file system.
v1.5.0,just checking for uri strings... doing no dns lookups
v1.5.0,to see if the filesystems are the same. This is not optimal.
v1.5.0,but avoids name resolution.
v1.5.0,this might have name collisions. copy will throw an exception
v1.5.0,parse the original path to create new path
v1.5.0,check for ports
v1.5.0,Write job file to JobTracker's fs
v1.5.0,Setup resource requirements
v1.5.0,Setup LocalResources
v1.5.0,Setup security tokens
v1.5.0,Setup the command to run the AM
v1.5.0,Add AM user command opts
v1.5.0,Final command
v1.5.0,Setup the CLASSPATH in environment
v1.5.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
v1.5.0,Setup the environment variables for Admin first
v1.5.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
v1.5.0,Parse distributed cache
v1.5.0,Setup ContainerLaunchContext for AM container
v1.5.0,Set up the ApplicationSubmissionContext
v1.5.0,Get partition and check the partition state
v1.5.0,Get the stored pss for this partition
v1.5.0,"Check this ps is the master ps for this location, only master ps can accept the update"
v1.5.0,Check the partition state again
v1.5.0,Start to put the update to the slave pss
v1.5.0,Get partition and check the partition state
v1.5.0,Get the stored pss for this partition
v1.5.0,"Check this ps is the master ps for this partition, if not, just return failed"
v1.5.0,Start to put the update to the slave pss
v1.5.0,Add tokens to new user so that it may execute its task correctly.
v1.5.0,to exit
v1.5.0,context.getSnapshotManager().processRecovery();
v1.5.0,private final ParameterServer psServer;
v1.5.0,return ServerState.GENERAL;
v1.5.0,lock.readLock().lock();
v1.5.0,lock.readLock().unlock();
v1.5.0,data.rewind();
v1.5.0,lock.readLock().lock();
v1.5.0,lock.readLock().unlock();
v1.5.0,data.rewind();
v1.5.0,lock.readLock().lock();
v1.5.0,lock.readLock().unlock();
v1.5.0,data.rewind();
v1.5.0,output.writeInt(clock);
v1.5.0,clock = input.readInt();
v1.5.0,private final List<PartitionKey> partitionKeys;
v1.5.0,Read matrix meta from meta file
v1.5.0,Load partitions from file use fork-join
v1.5.0,Save partitions to files use fork-join
v1.5.0,Write the ps matrix meta to the meta file
v1.5.0,Mapping from taskId to clock value.
v1.5.0,int[] keys = sparseRep.getKeys();
v1.5.0,int[] values = sparseRep.getValues();
v1.5.0,boolean[] used = sparseRep.getUsed();
v1.5.0,nnz = 0;
v1.5.0,for (int i = 0; i < keys.length; i++)
v1.5.0,if (used[i]) {
v1.5.0,"denseRep.put(keys[i], values[i]);"
v1.5.0,nnz++;
v1.5.0,}
v1.5.0,sparseRep = null;
v1.5.0,int[] keys = sparseRep.getKeys();
v1.5.0,int[] values = sparseRep.getValues();
v1.5.0,boolean[] used = sparseRep.getUsed();
v1.5.0,for (int i = 0; i < keys.length; i++)
v1.5.0,if (used[i]) {
v1.5.0,"denseRep.put(keys[i], values[i]);"
v1.5.0,}
v1.5.0,sparseRep = null;
v1.5.0,output.writeInt(data.length);
v1.5.0,@Override
v1.5.0,public void serialize(ByteBuf buf) {
v1.5.0,if (sparseRep != null)
v1.5.0,return serializeSparse();
v1.5.0,else if (denseRep != null)
v1.5.0,return serializeDense();
v1.5.0,return serializeEmpty();
v1.5.0,}
v1.5.0,int[] keys = sparseRep.getKeys();
v1.5.0,int[] values = sparseRep.getValues();
v1.5.0,boolean[] used = sparseRep.getUsed();
v1.5.0,int idx = 0;
v1.5.0,for (int i = 0; i < keys.length; i++)
v1.5.0,if (used[i]) {
v1.5.0,"keysBuf.put(idx, keys[i]);"
v1.5.0,"valuesBuf.put(idx, values[i]);"
v1.5.0,idx++;
v1.5.0,}
v1.5.0,int[] keys = sparseRep.getKeys();
v1.5.0,int[] values = sparseRep.getValues();
v1.5.0,boolean[] used = sparseRep.getUsed();
v1.5.0,"int ov, k, v;"
v1.5.0,for (int i = 0; i < keys.length; i++) {
v1.5.0,if (used[i]) {
v1.5.0,k = keys[i];
v1.5.0,ov = denseRep.get(k);
v1.5.0,v = ov + values[i];
v1.5.0,"denseRep.put(k, v);"
v1.5.0,if (ov != 0 && v == 0)
v1.5.0,nnz--;
v1.5.0,}
v1.5.0,}
v1.5.0,TODO: use Epoll for linux future
v1.5.0,find the partition request context from cache
v1.5.0,get a channel to server from pool
v1.5.0,"if channel is not valid, it means maybe the connections to the server are closed"
v1.5.0,channelManager.removeChannelPool(loc);
v1.5.0,Generate seq id
v1.5.0,Create a RecoverPartRequest
v1.5.0,Serialize the request
v1.5.0,Change the seqId for the request
v1.5.0,Serialize the request
v1.5.0,"add the PSAgentContext,need fix"
v1.5.0,return this;
v1.5.0,return this;
v1.5.0,return this;
v1.5.0,return this;
v1.5.0,return this;
v1.5.0,TODO Should be implemented
v1.5.0,TODO Should be implemented
v1.5.0,Sort the parts by partitionId
v1.5.0,Sort partition keys use start column index
v1.5.0,"For each partition, we generate a update split."
v1.5.0,"Although the split is empty for partitions those without any update data,"
v1.5.0,we still need to generate a update split to update the clock info on ps.
v1.5.0,Sort the parts by partitionId
v1.5.0,Sort partition keys use start column index
v1.5.0,"For each partition, we generate a update split."
v1.5.0,"Although the split is empty for partitions those without any update data,"
v1.5.0,we still need to generate a update split to update the clock info on ps.
v1.5.0,TODO:
v1.5.0,public String uuid;
v1.5.0,this.uuid = UUID.randomUUID().toString();
v1.5.0,byte [] data = uuid.getBytes();
v1.5.0,buf.writeInt(data.length);
v1.5.0,buf.writeBytes(data);
v1.5.0,int size = buf.readInt();
v1.5.0,byte [] data = new byte[size];
v1.5.0,buf.readBytes(data);
v1.5.0,uuid = new String(data);
v1.5.0,"return ""PartitionRequest{"" + ""clock="" + clock + "", partKey="" + partKey + "", uuid="" + uuid + "", comeFromPs="""
v1.5.0,"+ comeFromPs + ""} "" + super.toString();"
v1.5.0,public String uuid;
v1.5.0,write the max abs
v1.5.0,TODO Auto-generated method stub
v1.5.0,TODO Auto-generated method stub
v1.5.0,TODO Auto-generated method stub
v1.5.0,get configuration from config file
v1.5.0,set localDir with enviroment set by nm.
v1.5.0,get master location
v1.5.0,init task manager and start tasks
v1.5.0,start heartbeat thread
v1.5.0,taskManager.assignTaskIds(response.getTaskidsList());
v1.5.0,todo
v1.5.0,"if worker timeout, it may be knocked off."
v1.5.0,"SUCCESS, do nothing"
v1.5.0,heartbeatFailedTime = 0;
v1.5.0,private KEY currentKey;
v1.5.0,will be created
v1.5.0,TODO Auto-generated method stub
v1.5.0,Bitmap bitmap = new Bitmap();
v1.5.0,int max = indexArray[size - 1];
v1.5.0,byte [] bitIndexArray = new byte[max / 8 + 1];
v1.5.0,for(int i = 0; i < size; i++){
v1.5.0,int bitIndex = indexArray[i] >> 3;
v1.5.0,int bitOffset = indexArray[i] - (bitIndex << 3);
v1.5.0,switch(bitOffset){
v1.5.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
v1.5.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
v1.5.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
v1.5.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
v1.5.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
v1.5.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
v1.5.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
v1.5.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
v1.5.0,}
v1.5.0,}
v1.5.0,//////////////////////////////
v1.5.0,Application Configs
v1.5.0,//////////////////////////////
v1.5.0,//////////////////////////////
v1.5.0,Master Configs
v1.5.0,//////////////////////////////
v1.5.0,//////////////////////////////
v1.5.0,Worker Configs
v1.5.0,//////////////////////////////
v1.5.0,//////////////////////////////
v1.5.0,Task Configs
v1.5.0,//////////////////////////////
v1.5.0,//////////////////////////////
v1.5.0,ParameterServer Configs
v1.5.0,//////////////////////////////
v1.5.0,////////////////// IPC //////////////////////////
v1.5.0,//////////////////////////////
v1.5.0,Matrix transfer Configs.
v1.5.0,//////////////////////////////
v1.5.0,//////////////////////////////
v1.5.0,Matrix transfer Configs.
v1.5.0,//////////////////////////////
v1.5.0,Configs used to ANGEL_PS_PSAGENT running mode future.
v1.5.0,model parse
v1.5.0,Mark whether use pyangel or not.
v1.5.0,private Configuration conf;
v1.5.0,"Configuration that should be used in python environment, there should only be one"
v1.5.0,configuration instance in each Angel context.
v1.5.0,Use private access means jconf should not be changed or modified in this way.
v1.5.0,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
v1.5.0,Do nothing
v1.5.0,To-DO: add other ways to justify different value types
v1.5.0,"This is so ugly, must re-implement by more elegance way"
v1.5.0,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
v1.5.0,and other files submitted by user.
v1.5.0,Launch python process
v1.5.0,TODO Auto-generated constructor stub
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Rank
v1.5.0,Regularization parameters
v1.5.0,Learn rage
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set FM algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set feature number
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set FM predict output path
v1.5.0,Set actionType train
v1.5.0,Set feature number
v1.5.0,Feature number of train data
v1.5.0,Number of nonzero features
v1.5.0,Tree number
v1.5.0,Tree depth
v1.5.0,Split number
v1.5.0,Feature sample ratio
v1.5.0,Data format
v1.5.0,Learning rate
v1.5.0,Set basic configuration keys
v1.5.0,Use local deploy mode and dummy data spliter
v1.5.0,"set input, output path"
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,Set GBDT algorithm parameters
v1.5.0,Submit GBDT Train Task
v1.5.0,Load Model from HDFS.
v1.5.0,set basic configuration keys
v1.5.0,use local deploy mode and dummy dataspliter
v1.5.0,get a angel client
v1.5.0,add matrix
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,"Set trainning data, save model, log path"
v1.5.0,Set actionType train
v1.5.0,Set MF algorithm parameters
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,number of mini batch within a update periorid
v1.5.0,"Data format, libsvm or dummy"
v1.5.0,Batch size
v1.5.0,Model type
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,"conf.setBoolean(MLConf.ML_INDEX_GET_ENABLE(), true);"
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set sgd LR algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set actionType prediction
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,"Data format, libsvm or dummy"
v1.5.0,Train batch number per epoch.
v1.5.0,Sample ratio
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set FTRL LR algorithm parameters #feature #epoch
v1.5.0,FtrlLRPredictTest();
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType incremental train
v1.5.0,Set predict data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set actionType train
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Validation sample Ratio
v1.5.0,"Data format, libsvm or dummy"
v1.5.0,Train batch number per epoch.
v1.5.0,Batch number
v1.5.0,Model type
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set sgd LR algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType incremental train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set actionType prediction
v1.5.0,LOG.info(sigmoid(data[i]));
v1.5.0,LOG.info(Math.exp(-data[i]));
v1.5.0,when b is a negative number
v1.5.0,LOG.info(sigmoid(data[i]));
v1.5.0,LOG.info(Math.exp(-data[i]));
v1.5.0,when b is a negative number
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Validation sample Ratio
v1.5.0,"Data format, libsvm or dummy"
v1.5.0,Train batch number per epoch.
v1.5.0,Batch number
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set MLR algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType incremental train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set actionType prediction
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Rank
v1.5.0,Regularization parameters
v1.5.0,Learn rage
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set FM algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set feature number
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set learnType
v1.5.0,Set feature number
v1.5.0,Set
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set FM predict output path
v1.5.0,Set actionType train
v1.5.0,Set learnType
v1.5.0,Set feature number
v1.5.0,Cluster center number
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Sample ratio per mini-batch
v1.5.0,C
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set Kmeans algorithm parameters #cluster #feature #epoch
v1.5.0,Set data format
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log sava path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set actionType prediction
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Validation Ratio
v1.5.0,Data format
v1.5.0,Train batch number per epoch.
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,Set basic configuration keys
v1.5.0,Set data format
v1.5.0,Use local deploy mode
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set sgd SVM algorithm parameters
v1.5.0,"set input, output path"
v1.5.0,Set save model path
v1.5.0,Set actionType train
v1.5.0,Set log path
v1.5.0,Submit LR Train Task
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set actionType incremental train
v1.5.0,Set log path
v1.5.0,Feature number of train data
v1.5.0,Total iteration number
v1.5.0,Validation sample Ratio
v1.5.0,"Data format, libsvm or dummy"
v1.5.0,Train batch number per epoch.
v1.5.0,Learning rate
v1.5.0,Decay of learning rate
v1.5.0,Regularization coefficient
v1.5.0,Set local deploy mode
v1.5.0,Set basic configuration keys
v1.5.0,Set data format
v1.5.0,"set angel resource parameters #worker, #task, #PS"
v1.5.0,set sgd LR algorithm parameters #feature #epoch
v1.5.0,Set trainning data path
v1.5.0,Set save model path
v1.5.0,Set log path
v1.5.0,Set actionType train
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set save model path
v1.5.0,Set actionType incremental train
v1.5.0,Set log path
v1.5.0,Set trainning data path
v1.5.0,Set load model path
v1.5.0,Set predict result path
v1.5.0,Set log sava path
v1.5.0,Set actionType prediction
v1.5.0,double z=pre*y;
v1.5.0,if(z<=0) return 0.5-z;
v1.5.0,"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);"
v1.5.0,return 0.0;
v1.5.0,logistic loss for binary classification task.
v1.5.0,"logistic loss, but predict un-transformed margin"
v1.5.0,check if label in range
v1.5.0,return the default evaluation metric for the objective
v1.5.0,TODO Auto-generated method stub
v1.5.0,start row index for words
v1.5.0,doc ids
v1.5.0,topic assignments
v1.5.0,count word
v1.5.0,build word start index
v1.5.0,build dks
v1.5.0,"model.wtMat().increment(w, update);"
v1.5.0,"update.plusBy(t, 1);"
v1.5.0,"model.wtMat().increment(w, update);"
v1.5.0,The starting point
v1.5.0,There's always an unused entry.
v1.5.0,The starting point
v1.5.0,There's always an unused entry.
v1.5.0,The starting point
v1.5.0,There's always an unused entry.
v1.5.0,The starting point
v1.5.0,There's always an unused entry.
v1.5.0,The starting point
v1.5.0,There's always an unused entry.
v1.5.0,The starting point
v1.5.0,There's always an unused entry.
v1.5.0,print();
v1.5.0,The starting point
v1.5.0,There's always an unused entry.
v1.5.0,The starting point
v1.5.0,There's always an unused entry.
v1.5.0,The starting point
v1.5.0,There's always an unused entry.
v1.5.0,The starting point
v1.5.0,There's always an unused entry.
v1.5.0,print();
v1.5.0,Write #rows
v1.5.0,Write each row
v1.5.0,dense
v1.5.0,sparse
v1.5.0,LOG.info(buf.refCnt());
v1.5.0,dense
v1.5.0,sparse
v1.5.0,LOG.info(buf.refCnt());
v1.5.0,calculate columns
v1.5.0,loss function
v1.5.0,gradient and hessian
v1.5.0,"categorical feature set, null: none, empty: all, else: partial"
v1.5.0,tree node
v1.5.0,initialize the phase
v1.5.0,current tree and depth
v1.5.0,create loss function
v1.5.0,calculate grad info of each instance
v1.5.0,"create data sketch, push candidate split value to PS"
v1.5.0,1. calculate candidate split value
v1.5.0,categorical features
v1.5.0,2. push local sketch to PS
v1.5.0,3. set phase to GET_SKETCH
v1.5.0,the leader worker
v1.5.0,merge categorical features
v1.5.0,create updates
v1.5.0,"pull the global sketch from PS, only called once by each worker"
v1.5.0,number of categorical feature
v1.5.0,sample feature
v1.5.0,push sampled feature set to the current tree
v1.5.0,create new tree
v1.5.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
v1.5.0,calculate gradient
v1.5.0,"1. create new tree, initialize tree nodes and node stats"
v1.5.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
v1.5.0,2.1. pull the sampled features of the current tree
v1.5.0,"2.2. if use all the features, only called one"
v1.5.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
v1.5.0,4. set root node to active
v1.5.0,"5. reset instance position, set the root node's span"
v1.5.0,6. calculate gradient
v1.5.0,7. set phase to run active
v1.5.0,1. start threads of active tree nodes
v1.5.0,1.1. start threads for active nodes to generate histogram
v1.5.0,1.2. set thread status to batch num
v1.5.0,1.3. set the oplog to active
v1.5.0,"2. check thread stats, if all threads finish, return"
v1.5.0,clock
v1.5.0,find split
v1.5.0,"1. find responsible tree node, using RR scheme"
v1.5.0,2. pull gradient histogram
v1.5.0,2.1. get the name of this node's gradient histogram on PS
v1.5.0,2.2. pull the histogram
v1.5.0,histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();
v1.5.0,2.3. find best split result of this tree node
v1.5.0,2.3.1 using server split
v1.5.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.5.0,update the grad stats of children node
v1.5.0,update the left child
v1.5.0,update the right child
v1.5.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
v1.5.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
v1.5.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
v1.5.0,2.3.5 reset this tree node's gradient histogram to 0
v1.5.0,3. push split feature to PS
v1.5.0,4. push split value to PS
v1.5.0,5. push split gain to PS
v1.5.0,6. set phase to AFTER_SPLIT
v1.5.0,clock
v1.5.0,1. get split feature
v1.5.0,2. get split value
v1.5.0,3. get split gain
v1.5.0,4. get node weight
v1.5.0,5. split node
v1.5.0,"2. check thread stats, if all threads finish, return"
v1.5.0,6. clock
v1.5.0,"split the span of one node, reset the instance position"
v1.5.0,in case this worker has no instance on this node
v1.5.0,set the span of left child
v1.5.0,set the span of right child
v1.5.0,"1. left to right, find the first instance that should be in the right child"
v1.5.0,"2. right to left, find the first instance that should be in the left child"
v1.5.0,3. swap two instances
v1.5.0,4. find the cut pos
v1.5.0,than the split value
v1.5.0,5. set the span of left child
v1.5.0,6. set the span of right child
v1.5.0,set tree node to active
v1.5.0,set node to leaf
v1.5.0,set node to inactive
v1.5.0,finish current tree
v1.5.0,finish current depth
v1.5.0,set the tree phase
v1.5.0,check if there is active node
v1.5.0,check if finish all the tree
v1.5.0,update node's grad stats on PS
v1.5.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
v1.5.0,the root node's stats is updated by leader worker
v1.5.0,1. create the update
v1.5.0,2. push the update to PS
v1.5.0,the leader task adds node prediction to flush list
v1.5.0,1. name of this node's grad histogram on PS
v1.5.0,2. build the grad histogram of this node
v1.5.0,3. push the histograms to PS
v1.5.0,4. reset thread stats to finished
v1.5.0,5.1. set the children nodes of this node
v1.5.0,5.2. set split info and grad stats to this node
v1.5.0,5.2. create children nodes
v1.5.0,"5.3. create node stats for children nodes, and add them to the tree"
v1.5.0,5.4. reset instance position
v1.5.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
v1.5.0,5.6. set children nodes to leaf nodes
v1.5.0,5.7. set nid to leaf node
v1.5.0,5.8. deactivate active node
v1.5.0,"get feature type, 0:empty 1:all equal 2:real"
v1.5.0,left child <= split value; right child > split value
v1.5.0,"the first: minimal, the last: maximal"
v1.5.0,categorical features
v1.5.0,continuous features
v1.5.0,left child <= split value; right child > split value
v1.5.0,feature index used to split
v1.5.0,feature value used to split
v1.5.0,loss change after split this node
v1.5.0,grad stats of the left child
v1.5.0,grad stats of the right child
v1.5.0,"LOG.info(""Constructor with fid = -1"");"
v1.5.0,fid = -1: no split currently
v1.5.0,the minimal split value is the minimal value of feature
v1.5.0,the splits do not include the maximal value of feature
v1.5.0,"1. the average distance, (maxValue - minValue) / splitNum"
v1.5.0,2. calculate the candidate split value
v1.5.0,1. new feature's histogram (grad + hess)
v1.5.0,size: sampled_featureNum * (2 * splitNum)
v1.5.0,"in other words, concatenate each feature's histogram"
v1.5.0,2. get the span of this node
v1.5.0,------ 3. using sparse-aware method to build histogram ---
v1.5.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
v1.5.0,3.1. get the instance index
v1.5.0,3.2. get the grad and hess of the instance
v1.5.0,3.3. add to the sum
v1.5.0,3.4. loop the non-zero entries
v1.5.0,3.4.1. get feature value
v1.5.0,3.4.2. current feature's position in the sampled feature set
v1.5.0,3.4.3. find the position of feature value in a histogram
v1.5.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
v1.5.0,3.4.4. add the grad and hess to the corresponding bin
v1.5.0,3.4.5. add the reverse to the bin that contains 0.0f
v1.5.0,4. add the grad and hess sum to the zero bin of all features
v1.5.0,find the best split result of the histogram of a tree node
v1.5.0,1. calculate the gradStats of the root node
v1.5.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
v1.5.0,2. loop over features
v1.5.0,2.1. get the ture feature id in the sampled feature set
v1.5.0,2.2. get the indexes of histogram of this feature
v1.5.0,2.3. find the best split of current feature
v1.5.0,2.4. update the best split result if possible
v1.5.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.5.0,3. update the grad stats of children node
v1.5.0,3.1. update the left child
v1.5.0,3.2. update the right child
v1.5.0,find the best split result of one feature
v1.5.0,1. set the feature id
v1.5.0,2. create the best left stats and right stats
v1.5.0,3. the gain of the root node
v1.5.0,4. create the temp left and right grad stats
v1.5.0,5. loop over all the data in histogram
v1.5.0,5.1. get the grad and hess of current hist bin
v1.5.0,5.2. check whether we can split with current left hessian
v1.5.0,right = root - left
v1.5.0,5.3. check whether we can split with current right hessian
v1.5.0,5.4. calculate the current loss gain
v1.5.0,5.5. check whether we should update the split result with current loss gain
v1.5.0,split value = sketches[splitIdx]
v1.5.0,"5.6. if should update, also update the best left and right grad stats"
v1.5.0,6. set the best left and right grad stats
v1.5.0,partition number
v1.5.0,cols of each partition
v1.5.0,1. calculate the total grad sum and hess sum
v1.5.0,2. create the grad stats of the node
v1.5.0,1. calculate the total grad sum and hess sum
v1.5.0,2. create the grad stats of the node
v1.5.0,1. calculate the total grad sum and hess sum
v1.5.0,2. create the grad stats of the node
v1.5.0,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
v1.5.0,find the best split result of the histogram of a tree node
v1.5.0,2.2. get the indexes of histogram of this feature
v1.5.0,2.3. find the best split of current feature
v1.5.0,2.4. update the best split result if possible
v1.5.0,find the best split result of one feature
v1.5.0,1. set the feature id
v1.5.0,splitEntry.setFid(fid);
v1.5.0,2. create the best left stats and right stats
v1.5.0,3. the gain of the root node
v1.5.0,4. create the temp left and right grad stats
v1.5.0,5. loop over all the data in histogram
v1.5.0,5.1. get the grad and hess of current hist bin
v1.5.0,5.2. check whether we can split with current left hessian
v1.5.0,right = root - left
v1.5.0,5.3. check whether we can split with current right hessian
v1.5.0,5.4. calculate the current loss gain
v1.5.0,5.5. check whether we should update the split result with current loss gain
v1.5.0,"5.6. if should update, also update the best left and right grad stats"
v1.5.0,6. set the best left and right grad stats
v1.5.0,find the best split result of a serve row on the PS
v1.5.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.5.0,2.2. get the start index in histogram of this feature
v1.5.0,2.3. find the best split of current feature
v1.5.0,2.4. update the best split result if possible
v1.5.0,"find the best split result of one feature from a server row, used by the PS"
v1.5.0,1. set the feature id
v1.5.0,2. create the best left stats and right stats
v1.5.0,3. the gain of the root node
v1.5.0,4. create the temp left and right grad stats
v1.5.0,5. loop over all the data in histogram
v1.5.0,5.1. get the grad and hess of current hist bin
v1.5.0,5.2. check whether we can split with current left hessian
v1.5.0,right = root - left
v1.5.0,5.3. check whether we can split with current right hessian
v1.5.0,5.4. calculate the current loss gain
v1.5.0,5.5. check whether we should update the split result with current loss gain
v1.5.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
v1.5.0,the task use index to find fvalue
v1.5.0,"5.6. if should update, also update the best left and right grad stats"
v1.5.0,6. set the best left and right grad stats
v1.5.0,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
v1.5.0,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
v1.5.0,clear all the information
v1.5.0,calculate the sum of gradient and hess
v1.5.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v1.5.0,ridx)
v1.5.0,check if necessary information is ready
v1.5.0,"same as add, reduce is used in All Reduce"
v1.5.0,"features used in this tree, if equals null, means use all the features without sampling"
v1.5.0,node in the tree
v1.5.0,the gradient info of each instances
v1.5.0,initialize nodes
v1.5.0,gradient
v1.5.0,second order gradient
v1.5.0,int sendStartCol = startFid * 7; // each split contains 7 doubles
v1.5.0,"System.out.println(""Indices: "" + Arrays.toString(indices));"
v1.5.0,t[i][code]++;
v1.5.0,else if (Math.random() > 0.5) {
v1.5.0,t[i][code] = freq;
v1.5.0,}
v1.5.0,"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);"
v1.5.0,"ret = Math.min(ret, t[i][h[i].encode(key)]);"
v1.5.0,"Get input path, output path"
v1.5.0,Init serde
v1.5.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.5.0,"Get input path, output path"
v1.5.0,Init serde
v1.5.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.5.0,"task type: classification, regression, or ranking"
v1.5.0,"quantile sketch, size = featureNum * splitNum"
v1.5.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
v1.5.0,"active tree nodes, size = pow(2, treeDepth) -1"
v1.5.0,sampled features. size = treeNum * sampleRatio * featureNum
v1.5.0,categorical feature. size = workerNum * cateFeatNum * splitNum
v1.5.0,"split features, size = treeNum * treeNodeNum"
v1.5.0,"split values, size = treeNum * treeNodeNum"
v1.5.0,"split gains, size = treeNum * treeNodeNum"
v1.5.0,"node weights, size = treeNum * treeNodeNum"
v1.5.0,"node preds, size = treeNum * treeNodeNum"
v1.5.0,if using PS to perform split
v1.5.0,step size for a tree
v1.5.0,number of class
v1.5.0,minimum loss change required for a split
v1.5.0,maximum depth of a tree
v1.5.0,number of features
v1.5.0,number of nonzero
v1.5.0,number of candidates split value
v1.5.0,----- the rest parameters are less important ----
v1.5.0,base instance weight
v1.5.0,minimum amount of hessian(weight) allowed in a child
v1.5.0,L2 regularization factor
v1.5.0,L1 regularization factor
v1.5.0,default direction choice
v1.5.0,maximum delta update we can add in weight estimation
v1.5.0,this parameter can be used to stabilize update
v1.5.0,default=0 means no constraint on weight delta
v1.5.0,whether we want to do subsample for row
v1.5.0,whether to subsample columns for each tree
v1.5.0,accuracy of sketch
v1.5.0,accuracy of sketch
v1.5.0,leaf vector size
v1.5.0,option for parallelization
v1.5.0,option to open cacheline optimization
v1.5.0,whether to not print info during training.
v1.5.0,maximum depth of the tree
v1.5.0,number of features used for tree construction
v1.5.0,"minimum loss change required for a split, otherwise stop split"
v1.5.0,----- the rest parameters are less important ----
v1.5.0,default direction choice
v1.5.0,whether we want to do sample data
v1.5.0,whether to sample columns during tree construction
v1.5.0,whether to use histogram for split
v1.5.0,number of histogram units
v1.5.0,whether to print info during training.
v1.5.0,----- the rest parameters are obtained after training ----
v1.5.0,total number of nodes
v1.5.0,number of deleted nodes */
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy data spliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,row 0 is a random uniform
v1.4.0,row 1 is a random normal
v1.4.0,row 2 is filled with 1.0
v1.4.0,in different part
v1.4.0,TODO Auto-generated constructor stub
v1.4.0,row 0 is a random uniform
v1.4.0,row 1 is a random normal
v1.4.0,row 2 is filled with 1.0
v1.4.0,find the max abs
v1.4.0,compress data
v1.4.0,TODO: a better way is needed to deal with defaultValue
v1.4.0,TODO: a better way is needed to deal with defaultValue
v1.4.0,TODO: a better way is needed to deal with defaultValue
v1.4.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
v1.4.0,"paras[1] = ""abc"";"
v1.4.0,"paras[2] = ""123"";"
v1.4.0,Add standard Hadoop classes
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Learning rate
v1.4.0,Regularization coefficient
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,set sgd LR algorithm parameters #feature #epoch
v1.4.0,Set input data path
v1.4.0,Set save model path
v1.4.0,Set actionType train
v1.4.0,Set PS Model values
v1.4.0,Wait for all tasks finish this clock
v1.4.0,Get values of index array
v1.4.0,Set PS Model values
v1.4.0,Wait for all tasks finish this clock
v1.4.0,Get values of index array
v1.4.0,Feature number of train data
v1.4.0,Number of nonzero features
v1.4.0,Tree number
v1.4.0,Tree depth
v1.4.0,Split number
v1.4.0,Feature sample ratio
v1.4.0,Data format
v1.4.0,Learning rate
v1.4.0,Set basic configuration keys
v1.4.0,Use local deploy mode and data format
v1.4.0,"set input, output path"
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,Set GBDT algorithm parameters
v1.4.0,Load Model from HDFS.
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,"Set trainning data, and save model path"
v1.4.0,Set actionType train
v1.4.0,Set MF algorithm parameters
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Validation sample Ratio
v1.4.0,"Data format, libsvm or dummy"
v1.4.0,Train batch number per epoch.
v1.4.0,Learning rate
v1.4.0,Decay of learning rate
v1.4.0,Regularization coefficient
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,Set data format
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,set sgd LR algorithm parameters #feature #epoch
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set save model path
v1.4.0,Set actionType incremental train
v1.4.0,Set log path
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set predict result path
v1.4.0,Set actionType prediction
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Validation sample Ratio
v1.4.0,"Data format, libsvm or dummy"
v1.4.0,Train batch number per epoch.
v1.4.0,Batch number
v1.4.0,Learning rate
v1.4.0,Decay of learning rate
v1.4.0,Regularization coefficient
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,Set data format
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,set sgd LR algorithm parameters #feature #epoch
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType incremental train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set predict result path
v1.4.0,Set log path
v1.4.0,Set actionType prediction
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Default location for user home directories #
v1.4.0,Default value for FS_HOME_DIR_KEY #
v1.4.0,Default umask for files created in HDFS #
v1.4.0,Default value for FS_PERMISSIONS_UMASK_KEY #
v1.4.0,How often does RPC client send pings to RPC server #
v1.4.0,Default value for IPC_PING_INTERVAL_KEY #
v1.4.0,Enables pings from RPC client to the server #
v1.4.0,Default value of IPC_CLIENT_PING_KEY #
v1.4.0,Responses larger than this will be logged #
v1.4.0,Default value for IPC_SERVER_RPC_MAX_RESPONSE_SIZE_KEY #
v1.4.0,Number of threads in RPC server reading from the socket #
v1.4.0,Default value for IPC_SERVER_RPC_READ_THREADS_KEY #
v1.4.0,How many calls per handler are allowed in the queue. #
v1.4.0,Default value for IPC_SERVER_HANDLER_QUEUE_SIZE_KEY #
v1.4.0,Internal buffer size for Lzo compressordecompressors #/
v1.4.0,Default value for IO_COMPRESSION_CODEC_LZO_BUFFERSIZE_KEY #
v1.4.0,This is for specifying the implementation for the mappings from
v1.4.0,hostnames to the racks they belong to
v1.4.0,Internal buffer size for Snappy compressordecompressors #/
v1.4.0,Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #
v1.4.0,Internal buffer size for Snappy compressordecompressors #/
v1.4.0,Default value for IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY #
v1.4.0,Service Authorization
v1.4.0,HA health monitor and failover controller.
v1.4.0,How often to retry connecting to the service.
v1.4.0,How often to check the service.
v1.4.0,How long to sleep after an unexpected RPC error.
v1.4.0,Timeout for the actual monitorHealth() calls. *
v1.4.0,Timeout that the FC waits for the new active to become active
v1.4.0,Timeout that the FC waits for the old active to go to standby
v1.4.0,FC connection retries for graceful fencing
v1.4.0,"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState"
v1.4.0,Static user web-filter properties.
v1.4.0,See StaticUserWebFilter.
v1.4.0,EnableDisable aliases serving from jetty
v1.4.0,Path to the Kerberos ticket cache.  Setting this will force
v1.4.0,UserGroupInformation to use only this ticket cache file when creating a
v1.4.0,FileSystem instance.
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,"Input Path, please modify ${YOUR_ANGEL_HOME} as your local angel installation path,"
v1.4.0,"e.g. if your path is /home/angel/angel_1.3.0, your input_path should be:"
v1.4.0,"""file:///home/angel/angel_1.3.0/data/exampledata/GBDTLocalExampleData/agaricus.txt.train"","
v1.4.0,"and your out_path could be: ""file:///home/angel/angel_1.3.0/data/output"""
v1.4.0,Feature number of train data
v1.4.0,Number of nonzero features
v1.4.0,Tree number
v1.4.0,Tree depth
v1.4.0,Split number
v1.4.0,Feature sample ratio
v1.4.0,Data format
v1.4.0,Learning rate
v1.4.0,Set GBDT category feature
v1.4.0,"set input, output path"
v1.4.0,Set GBDT algorithm parameters
v1.4.0,Load Model from HDFS.
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Validation sample ratio
v1.4.0,"Data format,libsvm or dummy"
v1.4.0,Train batch number per epoch
v1.4.0,Learning rate
v1.4.0,Decay of learning rate
v1.4.0,Regularization coefficient
v1.4.0,"Set job queue, if you use YARN deploy mode, you can set job queue by"
v1.4.0,"self.conf.set('mapreduce.job.queue.name', 'default')"
v1.4.0,Set local deploy mode
v1.4.0,Set basic self.configuration keys
v1.4.0,Set data format
v1.4.0,"set angel resource parameters #worker, #tast, #ps"
v1.4.0,set sgd LR algorithim parameters # feature # epoch
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set save model path
v1.4.0,Set actionType incremental train
v1.4.0,Set log path
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set predict result path
v1.4.0,Set actionType prediction
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Set local deploy mode
v1.4.0,Set basic self.configuration keys
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,"Set trainning data, and save model path"
v1.4.0,Set actionType train
v1.4.0,Set MF algorithm parameters
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.4.0,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Load Model from HDFS.
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Cluster center number
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Sample ratio per mini-batch
v1.4.0,C
v1.4.0,Set local deploy mode
v1.4.0,Set basic self.configuration key
v1.4.0,"Set angel resource parameters #worker, #task, #PS"
v1.4.0,Set Kmeans algorithm parameters #cluster #feature #epoch
v1.4.0,Set data format
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log sava path
v1.4.0,Set actionType train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set predict result path
v1.4.0,Set actionType prediction
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Validation sample Ratio
v1.4.0,"Data format, libsvm or dummy"
v1.4.0,Train batch number per epoch.
v1.4.0,Batch number
v1.4.0,Learning rate
v1.4.0,Decay of learning rate
v1.4.0,Regularization coefficient
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,Set data format
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,set sgd LR algorithm parameters #feature #epoch
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType incremental train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set predict result path
v1.4.0,Set log path
v1.4.0,Set actionType prediction
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Rank
v1.4.0,Regularization parameters
v1.4.0,Learn rage
v1.4.0,Set local deploy mode
v1.4.0,Set basic self.configuration keys
v1.4.0,"Set angel resource parameters #worker, #task, #PS"
v1.4.0,Set FM algorithm parameters #feature #epoch
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Set learnType
v1.4.0,Set feature number
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,If the enviroment has ANGEL_HOME set trust it.
v1.4.0,Add the path of the PyAngel module if it exists
v1.4.0,If we are installed in edit mode also look two dirs up
v1.4.0,Not pip installed no worries
v1.4.0,If we are installed in edit mode also look two dirs up
v1.4.0,Not pip installed no worries
v1.4.0,Normalize the paths
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Get Java HashMap instance which converted from a python dict
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Run ParameterServer  & ParameterServerAgent
v1.4.0,Only Run ParameterServer
v1.4.0,Run ParameterServer & Worker(embedded ParameterServerAgent)
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,To Do
v1.4.0,Modify the way to get current Angel version
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Launch the Py4j gateway
v1.4.0,Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc
v1.4.0,Don't send ctrl-c / SIGINT to the Java gateway:
v1.4.0,We use select() here in order to avoid blocking indefinitely if the subprocess dies
v1.4.0,before connecting
v1.4.0,Determine which ephemeral port the server started on:
v1.4.0,Connect to the gateway
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.4.0,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.4.0,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,To do: need python edition of TVector
v1.4.0,To do: need python edition of GetFunc
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Create an angel job client
v1.4.0,Submit this application
v1.4.0,Create a KMeans model
v1.4.0,Load model meta to client
v1.4.0,Start
v1.4.0,"Run user task and wait for completion,"
v1.4.0,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.4.0,Save the trained model to HDFS
v1.4.0,Stop
v1.4.0,Create an angel job client
v1.4.0,Submit this application
v1.4.0,Create KMeans model
v1.4.0,Add the model meta to client
v1.4.0,Start
v1.4.0,"Run user task and wait for completion,"
v1.4.0,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.4.0,Stop
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Training job to obtain a model
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Create an angel job client
v1.4.0,Submit this application
v1.4.0,Create a model
v1.4.0,Load model meta to client
v1.4.0,Run user task
v1.4.0,"Wait for completion,"
v1.4.0,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.4.0,Save the incremental trained model to HDFS
v1.4.0,Stop
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,If the enviroment has ANGEL_HOME set trust it.
v1.4.0,Add the path of the PyAngel module if it exists
v1.4.0,If we are installed in edit mode also look two dirs up
v1.4.0,Not pip installed no worries
v1.4.0,If we are installed in edit mode also look two dirs up
v1.4.0,Not pip installed no worries
v1.4.0,Normalize the paths
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Get Java HashMap instance which converted from a python dict
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Run ParameterServer  & ParameterServerAgent
v1.4.0,Only Run ParameterServer
v1.4.0,Run ParameterServer & Worker(embedded ParameterServerAgent)
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,To Do
v1.4.0,Modify the way to get current Angel version
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Launch the Py4j gateway
v1.4.0,Start a socket that will be used by PythonGatewayServer to communicate its port to python sub-proc
v1.4.0,Don't send ctrl-c / SIGINT to the Java gateway:
v1.4.0,We use select() here in order to avoid blocking indefinitely if the subprocess dies
v1.4.0,before connecting
v1.4.0,Determine which ephemeral port the server started on:
v1.4.0,Connect to the gateway
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,Copyright (C) 2017 THL A29 Limited] = a Tencent company. All rights reserved.
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,Unless required by applicable law or agreed to in writing] = software distributed under the License is
v1.4.0,"distributed on an ""AS IS"" BASIS] = WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,To do: need python edition of TVector
v1.4.0,To do: need python edition of GetFunc
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Training job to obtain a model
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License"") you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https:#opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Create an angel job client
v1.4.0,Submit this application
v1.4.0,Create a model
v1.4.0,Load model meta to client
v1.4.0,Run user task
v1.4.0,"Wait for completion,"
v1.4.0,User task is set in AngelConf.ANGEL_TASK_USER_TASKCLASS
v1.4.0,Save the incremental trained model to HDFS
v1.4.0,Stop
v1.4.0,
v1.4.0,Tencent is pleased to support the open source community by making Angel available.
v1.4.0,
v1.4.0,"Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved."
v1.4.0,
v1.4.0,"Licensed under the BSD 3-Clause License (the ""License""); you may not use this file except in"
v1.4.0,compliance with the License. You may obtain a copy of the License at
v1.4.0,
v1.4.0,https://opensource.org/licenses/BSD-3-Clause
v1.4.0,
v1.4.0,"Unless required by applicable law or agreed to in writing, software distributed under the License is"
v1.4.0,"distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"
v1.4.0,either express or implied. See the License for the specific language governing permissions and
v1.4.0,
v1.4.0,Load model meta
v1.4.0,Convert model
v1.4.0,"Get input path, output path"
v1.4.0,Init serde
v1.4.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.4.0,Load model meta
v1.4.0,Convert model
v1.4.0,load hadoop configuration
v1.4.0,"Get input path, output path"
v1.4.0,Init serde
v1.4.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.4.0,Load model meta
v1.4.0,Check row type
v1.4.0,Load model
v1.4.0,Load model meta
v1.4.0,Check row type
v1.4.0,Load model
v1.4.0,Load model meta
v1.4.0,Check row type
v1.4.0,Load model
v1.4.0,Load model meta
v1.4.0,Check row type
v1.4.0,Load model
v1.4.0,Load model meta
v1.4.0,Check row type
v1.4.0,Load model
v1.4.0,Load model meta
v1.4.0,Check row type
v1.4.0,Load model
v1.4.0,Load model meta
v1.4.0,Check row type
v1.4.0,Load model
v1.4.0,Load model
v1.4.0,load hadoop configuration
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,"conf.setBoolean(AngelConf.ANGEL_PS_HA_USE_EVENT_PUSH, true);"
v1.4.0,"conf.setBoolean(AngelConf.ANGEL_PS_HA_PUSH_SYNC, true);"
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,worker register
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,"conf.setBoolean(AngelConf.ANGEL_PS_HA_USE_EVENT_PUSH, true);"
v1.4.0,"conf.setBoolean(AngelConf.ANGEL_PS_HA_PUSH_SYNC, true);"
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,add matrix
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,attempt 0
v1.4.0,attempt1
v1.4.0,attempt1
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,TODO Auto-generated constructor stub
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,set basic configuration keys
v1.4.0,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,Thread.sleep(5000);
v1.4.0,"response = master.getJobReport(null, request);"
v1.4.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
v1.4.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
v1.4.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,"DenseIntVector deltaVec = new DenseIntVector(100000, delta);"
v1.4.0,deltaVec.setMatrixId(matrixW1Id);
v1.4.0,deltaVec.setRowId(0);
v1.4.0,TODO Auto-generated constructor stub
v1.4.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
v1.4.0,@RunWith(MockitoJUnitRunner.class)
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,psAgent.initAndStart();
v1.4.0,test conf
v1.4.0,test master location
v1.4.0,test app id
v1.4.0,test user
v1.4.0,test ps agent attempt id
v1.4.0,test ps agent id
v1.4.0,test connection
v1.4.0,test master client
v1.4.0,test ip
v1.4.0,test loc
v1.4.0,test master location
v1.4.0,test ps location
v1.4.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
v1.4.0,test all ps ids
v1.4.0,test all matrix ids
v1.4.0,test all matrix names
v1.4.0,test matrix attribute
v1.4.0,test matrix meta
v1.4.0,test ps location
v1.4.0,test partitions
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,group0Id = new WorkerGroupId(0);
v1.4.0,"worker0Id = new WorkerId(group0Id, 0);"
v1.4.0,"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);"
v1.4.0,task0Id = new TaskId(0);
v1.4.0,task1Id = new TaskId(1);
v1.4.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.4.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.4.0,test this func in testWriteTo
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,"LOG.info(index[0] + "" "" + value[0]);"
v1.4.0,"LOG.info(index[1] + "" "" + value[1]);"
v1.4.0,"LOG.info(index[2] + "" "" + value[2]);"
v1.4.0,dot
v1.4.0,plus
v1.4.0,plusBy
v1.4.0,dot
v1.4.0,plus
v1.4.0,plusBy
v1.4.0,dot
v1.4.0,plus
v1.4.0,plusBy
v1.4.0,dot
v1.4.0,plusBy
v1.4.0,@Test
v1.4.0,public void dotDenseFloatVector() throws Exception {
v1.4.0,int dim = 1000;
v1.4.0,Random random = new Random(System.currentTimeMillis());
v1.4.0,
v1.4.0,double[] values = new double[dim];
v1.4.0,float[] values_1 = new float[dim];
v1.4.0,for (int i = 0; i < dim; i++) {
v1.4.0,values[i] = random.nextDouble();
v1.4.0,values_1[i] = random.nextFloat();
v1.4.0,}
v1.4.0,
v1.4.0,"DenseDoubleVector vec = new DenseDoubleVector(dim, values);"
v1.4.0,"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);"
v1.4.0,
v1.4.0,double sum = 0.0;
v1.4.0,for (int i = 0; i < dim; i++) {
v1.4.0,sum += values[i] * values_1[i];
v1.4.0,}
v1.4.0,
v1.4.0,"assertEquals(sum, vec.dot(vec_1));"
v1.4.0,
v1.4.0,}
v1.4.0,@Test
v1.4.0,public void plusDenseFlaotVector() throws Exception {
v1.4.0,"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};"
v1.4.0,"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};"
v1.4.0,"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);"
v1.4.0,"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);"
v1.4.0,
v1.4.0,TDoubleVector vec_2 = vec.plus(vec_1);
v1.4.0,for (int i = 0; i < vec.size(); i++)
v1.4.0,"assertEquals(value_1[i] + value_2[i], vec_2.get(i));"
v1.4.0,
v1.4.0,
v1.4.0,"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);"
v1.4.0,
v1.4.0,for (int i = 0; i < vec.size(); i++)
v1.4.0,"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);"
v1.4.0,
v1.4.0,double[] oldValues = vec.getValues().clone();
v1.4.0,
v1.4.0,vec.plusBy(vec_1);
v1.4.0,
v1.4.0,for (int i = 0; i < vec.size(); i++)
v1.4.0,"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));"
v1.4.0,
v1.4.0,oldValues = vec.getValues().clone();
v1.4.0,
v1.4.0,"vec.plusBy(vec_1, 3);"
v1.4.0,
v1.4.0,for (int i = 0; i < vec.size(); i++)
v1.4.0,"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));"
v1.4.0,}
v1.4.0,dot
v1.4.0,plus
v1.4.0,plusBy
v1.4.0,dot
v1.4.0,plus
v1.4.0,plusBy
v1.4.0,@Test
v1.4.0,public void plusBy3() throws Exception {
v1.4.0,"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};"
v1.4.0,"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);"
v1.4.0,"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});"
v1.4.0,vec.setRowId(0);
v1.4.0,"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});"
v1.4.0,vec_1.setRowId(1);
v1.4.0,TDoubleVector vec_2 = new SparseDoubleVector(2);
v1.4.0,"vec_2.set(1, 1.0);"
v1.4.0,vec_2.setRowId(0);
v1.4.0,
v1.4.0,mat.plusBy(vec);
v1.4.0,mat.plusBy(vec_1);
v1.4.0,mat.plusBy(vec_2);
v1.4.0,
v1.4.0,"assertEquals(2.0f, mat.get(0, 0));"
v1.4.0,"assertEquals(4.0f, mat.get(0, 1));"
v1.4.0,"assertEquals(4.0f, mat.get(1, 0));"
v1.4.0,"assertEquals(5.0f, mat.get(1, 1));"
v1.4.0,}
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add dense double matrix
v1.4.0,add sparse double matrix
v1.4.0,add component sparse double matrix
v1.4.0,add sparse long-key double matrix
v1.4.0,add component long-key sparse double matrix
v1.4.0,add dense float matrix
v1.4.0,add sparse float matrix
v1.4.0,add component sparse float matrix
v1.4.0,add dense float matrix
v1.4.0,add sparse float matrix
v1.4.0,add component sparse float matrix
v1.4.0,Start PS
v1.4.0,Start to run application
v1.4.0,Assert.assertTrue(index.length == row.size());
v1.4.0,Assert.assertTrue(index.length == row.size());
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,test worker getActiveTaskNum
v1.4.0,test worker getTaskNum
v1.4.0,test worker getTaskManager
v1.4.0,test workerId
v1.4.0,test workerAttemptId
v1.4.0,tet worker initFinished
v1.4.0,test worker getInitMinclock
v1.4.0,test worker loacation
v1.4.0,test AppId
v1.4.0,test Conf
v1.4.0,test UserName
v1.4.0,master location
v1.4.0,masterClient
v1.4.0,test psAgent
v1.4.0,test worker get dataBlockManager
v1.4.0,workerGroup.getSplits();
v1.4.0,application
v1.4.0,lcation
v1.4.0,workerGroup info
v1.4.0,worker info
v1.4.0,task
v1.4.0,Matrix parameters
v1.4.0,Set basic configuration keys
v1.4.0,Use local deploy mode and dummy data spliter
v1.4.0,Create an Angel client
v1.4.0,Add different types of matrix
v1.4.0,using mock object
v1.4.0,verification
v1.4.0,Stubbing
v1.4.0,Default does nothing.
v1.4.0,The app injection is optional
v1.4.0,"renderText(""hello world"");"
v1.4.0,"user choose a workerGroupID from the workergroups page,"
v1.4.0,now we should change the AngelApp params and render the workergroup page;
v1.4.0,"static final String WORKER_ID = ""worker.id"";"
v1.4.0,"div(""#logo"")."
v1.4.0,"img(""/static/hadoop-st.png"")._()."
v1.4.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
v1.4.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
v1.4.0,JQueryUI.jsnotice(html);
v1.4.0,import org.apache.hadoop.conf.Configuration;
v1.4.0,import java.lang.reflect.Field;
v1.4.0,get block locations from file system
v1.4.0,create a list of all block and their locations
v1.4.0,"if the file is not splitable, just create the one block with"
v1.4.0,full file length
v1.4.0,each split can be a maximum of maxSize
v1.4.0,if remainder is between max and 2*max - then
v1.4.0,"instead of creating splits of size max, left-max we"
v1.4.0,create splits of size left/2 and left/2. This is
v1.4.0,a heuristic to avoid creating really really small
v1.4.0,splits.
v1.4.0,add this block to the block --> node locations map
v1.4.0,"For blocks that do not have host/rack information,"
v1.4.0,assign to default  rack.
v1.4.0,add this block to the rack --> block map
v1.4.0,Add this host to rackToNodes map
v1.4.0,add this block to the node --> block map
v1.4.0,"if the file system does not have any rack information, then"
v1.4.0,use dummy rack location.
v1.4.0,The topology paths have the host name included as the last
v1.4.0,component. Strip it.
v1.4.0,get tokens for all the required FileSystems..
v1.4.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
v1.4.0,job.getConfiguration());
v1.4.0,Whether we need to recursive look into the directory structure
v1.4.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.4.0,user provided one (if any).
v1.4.0,all the files in input set
v1.4.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
v1.4.0,process all nodes and create splits that are local to a node. Generate
v1.4.0,"one split per node iteration, and walk over nodes multiple times to"
v1.4.0,distribute the splits across nodes.
v1.4.0,Skip the node if it has previously been marked as completed.
v1.4.0,"for each block, copy it into validBlocks. Delete it from"
v1.4.0,blockToNodes so that the same block does not appear in
v1.4.0,two different splits.
v1.4.0,Remove all blocks which may already have been assigned to other
v1.4.0,splits.
v1.4.0,"if the accumulated split size exceeds the maximum, then"
v1.4.0,create this split.
v1.4.0,create an input split and add it to the splits array
v1.4.0,Remove entries from blocksInNode so that we don't walk these
v1.4.0,again.
v1.4.0,Done creating a single split for this node. Move on to the next
v1.4.0,node so that splits are distributed across nodes.
v1.4.0,This implies that the last few blocks (or all in case maxSize=0)
v1.4.0,were not part of a split. The node is complete.
v1.4.0,if there were any blocks left over and their combined size is
v1.4.0,"larger than minSplitNode, then combine them into one split."
v1.4.0,Otherwise add them back to the unprocessed pool. It is likely
v1.4.0,that they will be combined with other blocks from the
v1.4.0,same rack later on.
v1.4.0,This condition also kicks in when max split size is not set. All
v1.4.0,blocks on a node will be grouped together into a single split.
v1.4.0,haven't created any split on this machine. so its ok to add a
v1.4.0,smaller one for parallelism. Otherwise group it in the rack for
v1.4.0,balanced size create an input split and add it to the splits
v1.4.0,array
v1.4.0,Remove entries from blocksInNode so that we don't walk this again.
v1.4.0,The node is done. This was the last set of blocks for this node.
v1.4.0,Put the unplaced blocks back into the pool for later rack-allocation.
v1.4.0,Node is done. All blocks were fit into node-local splits.
v1.4.0,Check if node-local assignments are complete.
v1.4.0,All nodes have been walked over and marked as completed or all blocks
v1.4.0,have been assigned. The rest should be handled via rackLock assignment.
v1.4.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
v1.4.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
v1.4.0,"if blocks in a rack are below the specified minimum size, then keep them"
v1.4.0,"in 'overflow'. After the processing of all racks is complete, these"
v1.4.0,overflow blocks will be combined into splits.
v1.4.0,Process all racks over and over again until there is no more work to do.
v1.4.0,Create one split for this rack before moving over to the next rack.
v1.4.0,Come back to this rack after creating a single split for each of the
v1.4.0,remaining racks.
v1.4.0,"Process one rack location at a time, Combine all possible blocks that"
v1.4.0,reside on this rack as one split. (constrained by minimum and maximum
v1.4.0,split size).
v1.4.0,iterate over all racks
v1.4.0,"for each block, copy it into validBlocks. Delete it from"
v1.4.0,blockToNodes so that the same block does not appear in
v1.4.0,two different splits.
v1.4.0,"if the accumulated split size exceeds the maximum, then"
v1.4.0,create this split.
v1.4.0,create an input split and add it to the splits array
v1.4.0,"if we created a split, then just go to the next rack"
v1.4.0,"if there is a minimum size specified, then create a single split"
v1.4.0,"otherwise, store these blocks into overflow data structure"
v1.4.0,There were a few blocks in this rack that
v1.4.0,remained to be processed. Keep them in 'overflow' block list.
v1.4.0,These will be combined later.
v1.4.0,Process all overflow blocks
v1.4.0,"This might cause an exiting rack location to be re-added,"
v1.4.0,but it should be ok.
v1.4.0,"if the accumulated split size exceeds the maximum, then"
v1.4.0,create this split.
v1.4.0,create an input split and add it to the splits array
v1.4.0,"Process any remaining blocks, if any."
v1.4.0,create an input split
v1.4.0,add this split to the list that is returned
v1.4.0,long num = totLength / maxSize;
v1.4.0,all blocks for all the files in input set
v1.4.0,mapping from a rack name to the list of blocks it has
v1.4.0,mapping from a block to the nodes on which it has replicas
v1.4.0,mapping from a node to the list of blocks that it contains
v1.4.0,populate all the blocks for all files
v1.4.0,stop all services
v1.4.0,1.write application state to file so that the client can get the state of the application
v1.4.0,if master exit
v1.4.0,2.clear tmp and staging directory
v1.4.0,waiting for client to get application state
v1.4.0,stop the RPC server
v1.4.0,"Security framework already loaded the tokens into current UGI, just use"
v1.4.0,them
v1.4.0,Now remove the AM->RM token so tasks don't have it
v1.4.0,add a shutdown hook
v1.4.0,init app state storage
v1.4.0,init event dispacher
v1.4.0,init location manager
v1.4.0,init container allocator
v1.4.0,init a rpc service
v1.4.0,recover matrix meta if needed
v1.4.0,recover ps attempt information if need
v1.4.0,init parameter server manager
v1.4.0,recover task information if needed
v1.4.0,init psagent manager and register psagent manager event
v1.4.0,a dummy data spliter is just for test now
v1.4.0,recover data splits information if needed
v1.4.0,init worker manager and register worker manager event
v1.4.0,register slow worker/ps checker
v1.4.0,register app manager event and finish event
v1.4.0,start a web service if use yarn deploy mode
v1.4.0,load from app state storage first if attempt index great than 1(the master is not the first
v1.4.0,retry)
v1.4.0,"if load failed, just build a new MatrixMetaManager"
v1.4.0,load ps attempt index from app state storage first if attempt index great than 1(the master
v1.4.0,is not the first retry)
v1.4.0,load task information from app state storage first if attempt index great than 1(the master
v1.4.0,is not the first retry)
v1.4.0,"if load failed, just build a new AMTaskManager"
v1.4.0,load data splits information from app state storage first if attempt index great than 1(the
v1.4.0,master is not the first retry)
v1.4.0,"if load failed, we need to recalculate the data splits"
v1.4.0,parse parameter server counters
v1.4.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.4.0,refresh last heartbeat timestamp
v1.4.0,send a state update event to the specific PSAttempt
v1.4.0,check if parameter server can commit now.
v1.4.0,Update PS failed counters
v1.4.0,check matrix metadata inconsistencies between master and parameter server.
v1.4.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
v1.4.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
v1.4.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.4.0,check whether psagent heartbeat timeout
v1.4.0,check whether parameter server heartbeat timeout
v1.4.0,check whether worker heartbeat timeout
v1.4.0,choose a unused port
v1.4.0,start RPC server
v1.4.0,remove this parameter server attempt from monitor set
v1.4.0,remove this parameter server attempt from monitor set
v1.4.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.4.0,find workergroup in worker manager
v1.4.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
v1.4.0,"if this worker group run over, just return WORKERGROUP_EXITED"
v1.4.0,"if this worker group is running now, return tasks, workers, data splits for it"
v1.4.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.4.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.4.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.4.0,"in ANGEL_PS mode, task id may can not know advance"
v1.4.0,update the clock for this matrix
v1.4.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.4.0,"in ANGEL_PS mode, task id may can not know advance"
v1.4.0,update task iteration
v1.4.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
v1.4.0,the number of splits equal to the number of tasks
v1.4.0,split data
v1.4.0,dispatch the splits to workergroups
v1.4.0,split data
v1.4.0,dispatch the splits to workergroups
v1.4.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.4.0,"first, then divided by expected split number"
v1.4.0,get input format class from configuration and then instantiation a input format object
v1.4.0,split data
v1.4.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.4.0,"first, then divided by expected split number"
v1.4.0,get input format class from configuration and then instantiation a input format object
v1.4.0,split data
v1.4.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.4.0,need to fine tune the number of workergroup and task based on the actual split number
v1.4.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.4.0,Record the location information for the splits in order to data localized schedule
v1.4.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.4.0,need to fine tune the number of workergroup and task based on the actual split number
v1.4.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.4.0,Record the location information for the splits in order to data localized schedule
v1.4.0,write meta data to a temporary file
v1.4.0,rename the temporary file to final file
v1.4.0,"if the file exists, read from file and deserialize it"
v1.4.0,write task meta
v1.4.0,write ps meta
v1.4.0,generate a temporary file
v1.4.0,write task meta to the temporary file first
v1.4.0,rename the temporary file to the final file
v1.4.0,"if last final task file exist, remove it"
v1.4.0,find task meta file which has max timestamp
v1.4.0,"if the file does not exist, just return null"
v1.4.0,read task meta from file and deserialize it
v1.4.0,generate a temporary file
v1.4.0,write ps meta to the temporary file first.
v1.4.0,rename the temporary file to the final file
v1.4.0,"if the old final file exist, just remove it"
v1.4.0,find ps meta file
v1.4.0,"if ps meta file does not exist, just return null"
v1.4.0,read ps meta from file and deserialize it
v1.4.0,Init matrix files meta
v1.4.0,Move output files
v1.4.0,Write the meta file
v1.4.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.4.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.4.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.4.0,create the topology tables
v1.4.0,Transitions from the NEW state.
v1.4.0,PA_FAILMSG
v1.4.0,Transitions from the UNASSIGNED state.
v1.4.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG"
v1.4.0,event
v1.4.0,Transitions from the ASSIGNED state.
v1.4.0,"this happened when launch thread run slowly, and PA_REGISTER event"
v1.4.0,dispatched before PA_CONTAINER_LAUNCHED event
v1.4.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.4.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.4.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.4.0,create the topology tables
v1.4.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will"
v1.4.0,retry another attempt or failed
v1.4.0,release container
v1.4.0,TODO
v1.4.0,set the launch time
v1.4.0,"set tarckerName,httpPort, which used by webserver"
v1.4.0,added to psManager so psManager can monitor it;
v1.4.0,psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);
v1.4.0,set the finish time only if launch time is set
v1.4.0,"ParameterServerJVM.setVMEnv(myEnv, conf);"
v1.4.0,Set up the launch command
v1.4.0,Duplicate the ByteBuffers for access by multiple containers.
v1.4.0,Construct the actual Container
v1.4.0,Application resources
v1.4.0,Application environment
v1.4.0,Service data
v1.4.0,Tokens
v1.4.0,Set up JobConf to be localized properly on the remote NM.
v1.4.0,Setup DistributedCache
v1.4.0,Setup up task credentials buffer
v1.4.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.4.0,Add the env variables passed by the user
v1.4.0,Set logging level in the environment.
v1.4.0,"This is so that, if the child forks another ""bin/hadoop"" (common in"
v1.4.0,streaming) it will have the correct loglevel.
v1.4.0,Setup the log4j prop
v1.4.0,Add main class and its arguments
v1.4.0,Finally add the jvmID
v1.4.0,vargs.add(String.valueOf(jvmID.getId()));
v1.4.0,Final commmand
v1.4.0,Transitions from the NEW state.
v1.4.0,Transitions from the RUNNING state.
v1.4.0,Transitions from the SUCCEEDED state
v1.4.0,Transitions from the KILLED state
v1.4.0,Transitions from the FAILED state
v1.4.0,Transitions from the NEW state.
v1.4.0,Transitions from the SCHEDULED state.
v1.4.0,Transitions from the RUNNING state.
v1.4.0,"another attempt launched,"
v1.4.0,Transitions from the SUCCEEDED state
v1.4.0,Transitions from the KILLED state
v1.4.0,Transitions from the FAILED state
v1.4.0,add diagnostic
v1.4.0,Set up the launch command
v1.4.0,Duplicate the ByteBuffers for access by multiple containers.
v1.4.0,Construct the actual Container
v1.4.0,Application resources
v1.4.0,Application environment
v1.4.0,Service data
v1.4.0,Tokens
v1.4.0,Set up JobConf to be localized properly on the remote NM.
v1.4.0,Setup DistributedCache
v1.4.0,Setup up task credentials buffer
v1.4.0,LocalStorageToken is needed irrespective of whether security is enabled
v1.4.0,or not.
v1.4.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
v1.4.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.4.0,Construct the actual Container
v1.4.0,The null fields are per-container and will be constructed for each
v1.4.0,container separately.
v1.4.0,Set up the launch command
v1.4.0,Duplicate the ByteBuffers for access by multiple containers.
v1.4.0,Construct the actual Container
v1.4.0,"a * in the classpath will only find a .jar, so we need to filter out"
v1.4.0,all .jars and add everything else
v1.4.0,Propagate the system classpath when using the mini cluster
v1.4.0,Add standard Hadoop classes
v1.4.0,Cache archives
v1.4.0,Cache files
v1.4.0,Sanity check
v1.4.0,Add URI fragment or just the filename
v1.4.0,Add the env variables passed by the user
v1.4.0,Set logging level in the environment.
v1.4.0,Setup the log4j prop
v1.4.0,Add main class and its arguments
v1.4.0,Finally add the jvmID
v1.4.0,vargs.add(String.valueOf(jvmID.getId()));
v1.4.0,Final commmand
v1.4.0,Add the env variables passed by the user
v1.4.0,Set logging level in the environment.
v1.4.0,Setup the log4j prop
v1.4.0,Add main class and its arguments
v1.4.0,Final commmand
v1.4.0,"if amTask is not null, we should clone task state from it"
v1.4.0,"if all parameter server complete commit, master can commit now"
v1.4.0,init and start master committer
v1.4.0,Transitions from the NEW state.
v1.4.0,Transitions from the UNASSIGNED state.
v1.4.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
v1.4.0,Transitions from the ASSIGNED state.
v1.4.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
v1.4.0,PA_CONTAINER_LAUNCHED event
v1.4.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.4.0,Transitions from the PSAttemptStateInternal.COMMITTING state
v1.4.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.4.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.4.0,create the topology tables
v1.4.0,reqeuest resource:send a resource request to the resource allocator
v1.4.0,"Once the resource is applied, build and send the launch request to the container launcher"
v1.4.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
v1.4.0,resource allocator
v1.4.0,set the launch time
v1.4.0,add the ps attempt to the heartbeat timeout monitoring list
v1.4.0,parse ps attempt location and put it to location manager
v1.4.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
v1.4.0,or failed
v1.4.0,remove ps attempt id from heartbeat timeout monitor list
v1.4.0,release container:send a release request to container launcher
v1.4.0,set the finish time only if launch time is set
v1.4.0,private long scheduledTime;
v1.4.0,Transitions from the NEW state.
v1.4.0,Transitions from the SCHEDULED state.
v1.4.0,Transitions from the RUNNING state.
v1.4.0,"another attempt launched,"
v1.4.0,Transitions from the SUCCEEDED state
v1.4.0,Transitions from the KILLED state
v1.4.0,Transitions from the FAILED state
v1.4.0,add diagnostic
v1.4.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.4.0,Refresh ps location & matrix meta
v1.4.0,start a new attempt for this ps
v1.4.0,notify ps manager
v1.4.0,"getContext().getLocationManager().setPsLocation(id, null);"
v1.4.0,add diagnostic
v1.4.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.4.0,start a new attempt for this ps
v1.4.0,notify ps manager
v1.4.0,notify the event handler of state change
v1.4.0,"If new state is not RUNNING and COMMITTING, add it to state timeout monitor"
v1.4.0,"if forcedState is set, just return"
v1.4.0,else get state from state machine
v1.4.0,add this worker group to the success set
v1.4.0,check if all worker group run over
v1.4.0,add this worker group to the failed set
v1.4.0,check if too many worker groups are failed or killed
v1.4.0,notify a run failed event
v1.4.0,add this worker group to the failed set
v1.4.0,check if too many worker groups are failed or killed
v1.4.0,notify a run failed event
v1.4.0,"sb.append(""killed and failed workergroup is over tolerate "").append(tolerateFailedGroup);"
v1.4.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
v1.4.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
v1.4.0,just return the total task number now
v1.4.0,TODO
v1.4.0,"if workerAttempt is not null, we should clone task state from it"
v1.4.0,from NEW state
v1.4.0,from SCHEDULED state
v1.4.0,get data splits location for data locality
v1.4.0,reqeuest resource:send a resource request to the resource allocator
v1.4.0,"once the resource is applied, build and send the launch request to the container launcher"
v1.4.0,notify failed message to the worker
v1.4.0,notify killed message to the worker
v1.4.0,release the allocated container
v1.4.0,notify failed message to the worker
v1.4.0,remove the worker attempt from heartbeat timeout listen list
v1.4.0,release the allocated container
v1.4.0,notify killed message to the worker
v1.4.0,remove the worker attempt from heartbeat timeout listen list
v1.4.0,clean the container
v1.4.0,notify failed message to the worker
v1.4.0,remove the worker attempt from heartbeat timeout listen list
v1.4.0,record the finish time
v1.4.0,clean the container
v1.4.0,notify killed message to the worker
v1.4.0,remove the worker attempt from heartbeat timeout listening list
v1.4.0,record the finish time
v1.4.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
v1.4.0,set worker attempt location
v1.4.0,notify the register message to the worker
v1.4.0,record the launch time
v1.4.0,update worker attempt metrics
v1.4.0,update tasks metrics
v1.4.0,clean the container
v1.4.0,notify the worker attempt run successfully message to the worker
v1.4.0,record the finish time
v1.4.0,init a worker attempt for the worker
v1.4.0,schedule the worker attempt
v1.4.0,add diagnostic
v1.4.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.4.0,init and start a new attempt for this ps
v1.4.0,notify worker manager
v1.4.0,add diagnostic
v1.4.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.4.0,init and start a new attempt for this ps
v1.4.0,notify worker manager
v1.4.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
v1.4.0,register to Yarn RM
v1.4.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
v1.4.0,"catch YarnRuntimeException, we should exit and need not retry"
v1.4.0,build heartbeat request
v1.4.0,send heartbeat request to rm
v1.4.0,"This can happen if the RM has been restarted. If it is in that state,"
v1.4.0,this application must clean itself up.
v1.4.0,Setting NMTokens
v1.4.0,assgin containers
v1.4.0,"if some container is not assigned, release them"
v1.4.0,handle finish containers
v1.4.0,dispatch container exit message to corresponding components
v1.4.0,killed by framework
v1.4.0,killed by framework
v1.4.0,killed by framework
v1.4.0,get application finish state
v1.4.0,build application diagnostics
v1.4.0,TODO:add a job history for angel
v1.4.0,build unregister request
v1.4.0,send unregister request to rm
v1.4.0,Note this down for next interaction with ResourceManager
v1.4.0,based on blacklisting comments above we can end up decrementing more
v1.4.0,than requested. so guard for that.
v1.4.0,send the updated resource request to RM
v1.4.0,send 0 container count requests also to cancel previous requests
v1.4.0,Update resource requests
v1.4.0,try to assign to all nodes first to match node local
v1.4.0,try to match all rack local
v1.4.0,assign remaining
v1.4.0,Update resource requests
v1.4.0,send the container-assigned event to task attempt
v1.4.0,build the start container request use launch context
v1.4.0,send the start request to Yarn nm
v1.4.0,send the message that the container starts successfully to the corresponding component
v1.4.0,"after launching, send launched event to task attempt to move"
v1.4.0,it from ASSIGNED to RUNNING state
v1.4.0,send the message that the container starts failed to the corresponding component
v1.4.0,kill the remote container if already launched
v1.4.0,start a thread pool to startup the container
v1.4.0,See if we need up the pool size only if haven't reached the
v1.4.0,maximum limit yet.
v1.4.0,nodes where containers will run at *this* point of time. This is
v1.4.0,*not* the cluster size and doesn't need to be.
v1.4.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
v1.4.0,later is just a buffer so we are not always increasing the
v1.4.0,pool-size
v1.4.0,the events from the queue are handled in parallel
v1.4.0,using a thread pool
v1.4.0,return if already stopped
v1.4.0,shutdown any containers that might be left running
v1.4.0,"Check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.4.0,get matrix ids in the parameter server report
v1.4.0,get the matrices parameter server need to create and delete
v1.4.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
v1.4.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
v1.4.0,Get ps locations from master and put them to the location cache.
v1.4.0,Build and initialize rpc client to master
v1.4.0,Build local location
v1.4.0,Initialize matrix meta information
v1.4.0,Start heartbeat thread if need
v1.4.0,Start all services
v1.4.0,Register to master first
v1.4.0,Report state to master every specified time
v1.4.0,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.4.0,Stop all modules
v1.4.0,Exit the process if on ANGEL_PS_PSAGENT mode
v1.4.0,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.4.0,Stop all modules
v1.4.0,Exit the process if on ANGEL_PS_PSAGENT mode
v1.4.0,get configuration from config file
v1.4.0,set localDir with enviroment set by nm.
v1.4.0,Update generic resource counters
v1.4.0,Updating resources specified in ResourceCalculatorProcessTree
v1.4.0,Remove the CPU time consumed previously by JVM reuse
v1.4.0,Generate a flush request and put it to request queue
v1.4.0,Generate a clock request and put it to request queue
v1.4.0,Generate a merge request and put it to request queue
v1.4.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
v1.4.0,matrix
v1.4.0,and add it to cache maps
v1.4.0,Add the message to the tree map
v1.4.0,"If there are flush / clock requests blocked, we need to put this merge request into"
v1.4.0,the waiting queue
v1.4.0,Launch a merge worker to merge the update to matrix op log cache
v1.4.0,Remove the message from the tree map
v1.4.0,Wake up blocked flush/clock request
v1.4.0,Add flush/clock request to listener list to waiting for all the existing
v1.4.0,updates are merged
v1.4.0,Wake up blocked flush/clock request
v1.4.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
v1.4.0,blocked.
v1.4.0,Get next merge message sequence id
v1.4.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
v1.4.0,position
v1.4.0,Wake up blocked merge requests
v1.4.0,Get minimal sequence id from listeners
v1.4.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
v1.4.0,should flush updates to local matrix storage
v1.4.0,unused now
v1.4.0,"Filter it, removing zero values"
v1.4.0,Doing average or not
v1.4.0,Split this row according the matrix partitions
v1.4.0,Add the splits to the result container
v1.4.0,"For each partition, we generate a update split."
v1.4.0,"Although the split is empty for partitions those without any update data,"
v1.4.0,we still need to generate a update split to update the clock info on ps.
v1.4.0,"For each partition, we generate a update split."
v1.4.0,"Although the split is empty for partitions those without any update data,"
v1.4.0,we still need to generate a update split to update the clock info on ps.
v1.4.0,int seqId = ((ByteBuf) msg).readInt();
v1.4.0,"LOG.info(""receive result of seqId="" + seqId);"
v1.4.0,((ByteBuf) msg).resetReaderIndex();
v1.4.0,TODO: use Epoll for linux future
v1.4.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.4.0,Then submit normal task until reach upper limit of flow control or all tasks are submit
v1.4.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.4.0,"LOG.info(""choose put server "" + psIds[index]);"
v1.4.0,"check the location of server is ready, if not, we should wait"
v1.4.0,allocate the bytebuf
v1.4.0,get a channel to server from pool
v1.4.0,"if channel is not valid, it means maybe the connections to the server are closed"
v1.4.0,channelManager.removeChannelPool(loc);
v1.4.0,find the partition request context from cache
v1.4.0,Check if the result of the sub-request is received
v1.4.0,Update received result number
v1.4.0,Get row splits received
v1.4.0,Put the row split to the cache(row index to row splits map)
v1.4.0,"If all splits of the row are received, means this row can be merged"
v1.4.0,TODO Auto-generated method stub
v1.4.0,"Check futures, if the result of a sub-request is received, put it to the result queue"
v1.4.0,Now we just support pipelined row splits merging for dense type row
v1.4.0,Wait until the clock value of this row is greater than or equal to the value
v1.4.0,Get partitions for this row
v1.4.0,First get this row from matrix storage
v1.4.0,"If the row exists in the matrix storage and the clock value meets the requirements, just"
v1.4.0,return
v1.4.0,Get row splits of this row from the matrix cache first
v1.4.0,"If the row split does not exist in cache, get it from parameter server"
v1.4.0,Wait the final result
v1.4.0,Put it to the matrix cache
v1.4.0,Split the matrix oplog according to the matrix partitions
v1.4.0,"If need update clock, we should send requests to all partitions"
v1.4.0,Filter the rowIds which are fetching now
v1.4.0,Send the rowIndex to rpc dispatcher and return immediately
v1.4.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
v1.4.0,Generate dispatch items and add them to the corresponding queues
v1.4.0,Filter the rowIds which are fetching now
v1.4.0,Pre-fetching is disable default
v1.4.0,matrix id to clock map
v1.4.0,"task index, it must be unique for whole application"
v1.4.0,Deserialize data splits meta
v1.4.0,Get workers
v1.4.0,Send request to every ps
v1.4.0,Wait the responses
v1.4.0,Update clock cache
v1.4.0,if(syncNum % 1024 == 0) {
v1.4.0,}
v1.4.0,Get row from cache.
v1.4.0,"if row clock is satisfy ssp staleness limit, just return."
v1.4.0,Get row from ps.
v1.4.0,"For ASYNC mode, just get from pss."
v1.4.0,"For BSP/SSP, get rows from storage/cache first"
v1.4.0,Get from ps.
v1.4.0,"For ASYNC, just get rows from pss."
v1.4.0,no more retries.
v1.4.0,calculate sleep time and return.
v1.4.0,parse the i-th sleep-time
v1.4.0,parse the i-th number-of-retries
v1.4.0,calculateSleepTime may overflow.
v1.4.0,"A few common retry policies, with no delays."
v1.4.0,close is a local operation and should finish within milliseconds; timeout just to be safe
v1.4.0,response will be null for one way messages.
v1.4.0,maxFrameLength = 2G
v1.4.0,lengthFieldOffset = 0
v1.4.0,lengthFieldLength = 8
v1.4.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
v1.4.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
v1.4.0,indicates whether this connection's life cycle is managed
v1.4.0,See if we already have a connection (common case)
v1.4.0,create a unique lock for this RS + protocol (if necessary)
v1.4.0,get the RS lock
v1.4.0,do one more lookup in case we were stalled above
v1.4.0,Only create isa when we need to.
v1.4.0,definitely a cache miss. establish an RPC for
v1.4.0,this RS
v1.4.0,Throw what the RemoteException was carrying.
v1.4.0,check
v1.4.0,every
v1.4.0,minutes
v1.4.0,TODO
v1.4.0,创建failoverHandler
v1.4.0,"The number of times this invocation handler has ever been failed over,"
v1.4.0,before this method invocation attempt. Used to prevent concurrent
v1.4.0,failed method invocations from triggering multiple failover attempts.
v1.4.0,Make sure that concurrent failed method invocations
v1.4.0,only cause a
v1.4.0,single actual fail over.
v1.4.0,RpcController + Message in the method args
v1.4.0,(generated code from RPC bits in .proto files have
v1.4.0,RpcController)
v1.4.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
v1.4.0,+ (System.currentTimeMillis() - beforeConstructTs));
v1.4.0,get an instance of the method arg type
v1.4.0,RpcController + Message in the method args
v1.4.0,(generated code from RPC bits in .proto files have
v1.4.0,RpcController)
v1.4.0,Message (hand written code usually has only a single
v1.4.0,argument)
v1.4.0,log any RPC responses that are slower than the configured
v1.4.0,warn
v1.4.0,response time or larger than configured warning size
v1.4.0,"when tagging, we let TooLarge trump TooSmall to keep"
v1.4.0,output simple
v1.4.0,note that large responses will often also be slow.
v1.4.0,provides a count of log-reported slow responses
v1.4.0,RpcController + Message in the method args
v1.4.0,(generated code from RPC bits in .proto files have
v1.4.0,RpcController)
v1.4.0,unexpected
v1.4.0,"in the protobuf methods, args[1] is the only significant argument"
v1.4.0,for JSON encoding
v1.4.0,base information that is reported regardless of type of call
v1.4.0,Disable Nagle's Algorithm since we don't want packets to wait
v1.4.0,Configure the event pipeline factory.
v1.4.0,Make a new connection.
v1.4.0,Remove all pending requests (will be canceled after relinquishing
v1.4.0,write lock).
v1.4.0,Cancel any pending requests by sending errors to the callbacks:
v1.4.0,Close the channel:
v1.4.0,Close the connection:
v1.4.0,Shut down all thread pools to exit.
v1.4.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
v1.4.0,See NettyServer.prepareResponse for where we write out the response.
v1.4.0,"It writes the call.id (int), a boolean signifying any error (and if"
v1.4.0,"so the exception name/trace), and the response bytes"
v1.4.0,Read the call id.
v1.4.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
v1.4.0,"instead, it returns a null message object."
v1.4.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
v1.4.0,System.currentTimeMillis());
v1.4.0,"It would be good widen this to just Throwable, but IOException is what we"
v1.4.0,allow now
v1.4.0,not implemented
v1.4.0,not implemented
v1.4.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
v1.4.0,cache of RpcEngines by protocol
v1.4.0,return the RpcEngine configured to handle a protocol
v1.4.0,We only handle the ConnectException.
v1.4.0,This is the exception we can't handle.
v1.4.0,check if timed out
v1.4.0,wait for retry
v1.4.0,IGNORE
v1.4.0,return the RpcEngine that handles a proxy object
v1.4.0,The default implementation works synchronously
v1.4.0,punt: allocate a new buffer & copy into it
v1.4.0,Parse cmd parameters
v1.4.0,load hadoop configuration
v1.4.0,load angel system configuration
v1.4.0,load user configuration:
v1.4.0,load user config file
v1.4.0,load command line parameters
v1.4.0,load user job resource files
v1.4.0,load user job jar if it exist
v1.4.0,Expand the environment variable
v1.4.0,Add default fs(local fs) for lib jars.
v1.4.0,"LOG.info(System.getProperty(""user.dir""));"
v1.4.0,get tokens for all the required FileSystems..
v1.4.0,Whether we need to recursive look into the directory structure
v1.4.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.4.0,user provided one (if any).
v1.4.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.4.0,get tokens for all the required FileSystems..
v1.4.0,Whether we need to recursive look into the directory structure
v1.4.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.4.0,user provided one (if any).
v1.4.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.4.0,a simple hdfs copy function assume src path and dest path are in same hdfs
v1.4.0,and FileSystem object has same schema
v1.4.0,"LOG.warn(""interrupted while sleeping"", ie);"
v1.4.0,private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);
v1.4.0,public static String getHostname() {
v1.4.0,try {
v1.4.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
v1.4.0,} catch (UnknownHostException uhe) {
v1.4.0,}
v1.4.0,"return new StringBuilder().append("""").append(uhe).toString();"
v1.4.0,}
v1.4.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
v1.4.0,String hostname = getHostname();
v1.4.0,String classname = clazz.getSimpleName();
v1.4.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
v1.4.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
v1.4.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
v1.4.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
v1.4.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
v1.4.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
v1.4.0,
v1.4.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
v1.4.0,public void run() {
v1.4.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
v1.4.0,"this.val$classname + "" at "" + this.val$hostname}));"
v1.4.0,}
v1.4.0,});
v1.4.0,}
v1.4.0,"We we interrupted because we're meant to stop? If not, just"
v1.4.0,continue ignoring the interruption
v1.4.0,Recalculate waitTime.
v1.4.0,// Begin delegation to Thread
v1.4.0,// End delegation to Thread
v1.4.0,instance submitter class
v1.4.0,Obtain filename from path
v1.4.0,Split filename to prexif and suffix (extension)
v1.4.0,Check if the filename is okay
v1.4.0,Prepare temporary file
v1.4.0,Prepare buffer for data copying
v1.4.0,Open and check input stream
v1.4.0,Open output stream and copy data between source file in JAR and the temporary file
v1.4.0,"If read/write fails, close streams safely before throwing an exception"
v1.4.0,"Finally, load the library"
v1.4.0,little endian load order
v1.4.0,tail
v1.4.0,fallthrough
v1.4.0,fallthrough
v1.4.0,finalization
v1.4.0,fmix(h1);
v1.4.0,----------
v1.4.0,body
v1.4.0,----------
v1.4.0,tail
v1.4.0,----------
v1.4.0,finalization
v1.4.0,----------
v1.4.0,body
v1.4.0,----------
v1.4.0,tail
v1.4.0,----------
v1.4.0,finalization
v1.4.0,JobStateProto jobState = report.getJobState();
v1.4.0,Used for java code to get a AngelClient instance
v1.4.0,Used for python code to get a AngelClient instance
v1.4.0,the leaf level file should be readable by others
v1.4.0,the subdirs in the path should have execute permissions for
v1.4.0,others
v1.4.0,2.get job id
v1.4.0,Credentials credentials = new Credentials();
v1.4.0,4.copy resource files to hdfs
v1.4.0,5.write configuration to a xml file
v1.4.0,6.create am container context
v1.4.0,7.Submit to ResourceManager
v1.4.0,8.get app master client
v1.4.0,Create a number of filenames in the JobTracker's fs namespace
v1.4.0,add all the command line files/ jars and archive
v1.4.0,first copy them to jobtrackers filesystem
v1.4.0,should not throw a uri exception
v1.4.0,should not throw an uri excpetion
v1.4.0,set the timestamps of the archives and files
v1.4.0,set the public/private visibility of the archives and files
v1.4.0,get DelegationToken for each cached file
v1.4.0,check if we do not need to copy the files
v1.4.0,is jt using the same file system.
v1.4.0,just checking for uri strings... doing no dns lookups
v1.4.0,to see if the filesystems are the same. This is not optimal.
v1.4.0,but avoids name resolution.
v1.4.0,this might have name collisions. copy will throw an exception
v1.4.0,parse the original path to create new path
v1.4.0,check for ports
v1.4.0,Write job file to JobTracker's fs
v1.4.0,Setup resource requirements
v1.4.0,Setup LocalResources
v1.4.0,Setup security tokens
v1.4.0,Setup the command to run the AM
v1.4.0,Add AM user command opts
v1.4.0,Final command
v1.4.0,Setup the CLASSPATH in environment
v1.4.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
v1.4.0,Setup the environment variables for Admin first
v1.4.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
v1.4.0,Parse distributed cache
v1.4.0,Setup ContainerLaunchContext for AM container
v1.4.0,Set up the ApplicationSubmissionContext
v1.4.0,Get partition and check the partition state
v1.4.0,Get the stored pss for this partition
v1.4.0,"Check this ps is the master ps for this location, only master ps can accept the update"
v1.4.0,Check the partition state again
v1.4.0,Start to put the update to the slave pss
v1.4.0,Get partition and check the partition state
v1.4.0,Get the stored pss for this partition
v1.4.0,"Check this ps is the master ps for this partition, if not, just return failed"
v1.4.0,Start to put the update to the slave pss
v1.4.0,resposne.encode(buf);
v1.4.0,TODO:
v1.4.0,resposne.encode(buf);
v1.4.0,TODO:
v1.4.0,"context.getMatrixStorageManager().update(partKey, in);"
v1.4.0,resposne.encode(buf);
v1.4.0,TODO:
v1.4.0,resposne.encode(buf);
v1.4.0,TODO:
v1.4.0,Add tokens to new user so that it may execute its task correctly.
v1.4.0,to exit
v1.4.0,context.getSnapshotManager().processRecovery();
v1.4.0,private final ParameterServer psServer;
v1.4.0,data.rewind();
v1.4.0,data.rewind();
v1.4.0,data.rewind();
v1.4.0,output.writeInt(clock);
v1.4.0,clock = input.readInt();
v1.4.0,private final List<PartitionKey> partitionKeys;
v1.4.0,Read matrix meta from meta file
v1.4.0,Load partitions from file use fork-join
v1.4.0,Save partitions to files use fork-join
v1.4.0,Write the ps matrix meta to the meta file
v1.4.0,Mapping from taskId to clock value.
v1.4.0,int[] keys = sparseRep.getKeys();
v1.4.0,int[] values = sparseRep.getValues();
v1.4.0,boolean[] used = sparseRep.getUsed();
v1.4.0,nnz = 0;
v1.4.0,for (int i = 0; i < keys.length; i++)
v1.4.0,if (used[i]) {
v1.4.0,"denseRep.put(keys[i], values[i]);"
v1.4.0,nnz++;
v1.4.0,}
v1.4.0,sparseRep = null;
v1.4.0,int[] keys = sparseRep.getKeys();
v1.4.0,int[] values = sparseRep.getValues();
v1.4.0,boolean[] used = sparseRep.getUsed();
v1.4.0,for (int i = 0; i < keys.length; i++)
v1.4.0,if (used[i]) {
v1.4.0,"denseRep.put(keys[i], values[i]);"
v1.4.0,}
v1.4.0,sparseRep = null;
v1.4.0,output.writeInt(data.length);
v1.4.0,@Override
v1.4.0,public void serialize(ByteBuf buf) {
v1.4.0,if (sparseRep != null)
v1.4.0,return serializeSparse();
v1.4.0,else if (denseRep != null)
v1.4.0,return serializeDense();
v1.4.0,return serializeEmpty();
v1.4.0,}
v1.4.0,int[] keys = sparseRep.getKeys();
v1.4.0,int[] values = sparseRep.getValues();
v1.4.0,boolean[] used = sparseRep.getUsed();
v1.4.0,int idx = 0;
v1.4.0,for (int i = 0; i < keys.length; i++)
v1.4.0,if (used[i]) {
v1.4.0,"keysBuf.put(idx, keys[i]);"
v1.4.0,"valuesBuf.put(idx, values[i]);"
v1.4.0,idx++;
v1.4.0,}
v1.4.0,int[] keys = sparseRep.getKeys();
v1.4.0,int[] values = sparseRep.getValues();
v1.4.0,boolean[] used = sparseRep.getUsed();
v1.4.0,"int ov, k, v;"
v1.4.0,for (int i = 0; i < keys.length; i++) {
v1.4.0,if (used[i]) {
v1.4.0,k = keys[i];
v1.4.0,ov = denseRep.get(k);
v1.4.0,v = ov + values[i];
v1.4.0,"denseRep.put(k, v);"
v1.4.0,if (ov != 0 && v == 0)
v1.4.0,nnz--;
v1.4.0,}
v1.4.0,}
v1.4.0,TODO: use Epoll for linux future
v1.4.0,find the partition request context from cache
v1.4.0,get a channel to server from pool
v1.4.0,"if channel is not valid, it means maybe the connections to the server are closed"
v1.4.0,channelManager.removeChannelPool(loc);
v1.4.0,Generate seq id
v1.4.0,Create a RecoverPartRequest
v1.4.0,Serialize the request
v1.4.0,Change the seqId for the request
v1.4.0,Serialize the request
v1.4.0,"add the PSAgentContext,need fix"
v1.4.0,return this;
v1.4.0,return this;
v1.4.0,return this;
v1.4.0,TODO Should be implemented
v1.4.0,TODO Should be implemented
v1.4.0,Sort the parts by partitionId
v1.4.0,Sort partition keys use start column index
v1.4.0,"For each partition, we generate a update split."
v1.4.0,"Although the split is empty for partitions those without any update data,"
v1.4.0,we still need to generate a update split to update the clock info on ps.
v1.4.0,Sort the parts by partitionId
v1.4.0,Sort partition keys use start column index
v1.4.0,"For each partition, we generate a update split."
v1.4.0,"Although the split is empty for partitions those without any update data,"
v1.4.0,we still need to generate a update split to update the clock info on ps.
v1.4.0,TODO:
v1.4.0,protected ParameterServerId psId;
v1.4.0,protected Location location;
v1.4.0,if(comeFromPs) {
v1.4.0,buf.writeInt(psId.getIndex());
v1.4.0,byte[] data = location.getIp().getBytes();
v1.4.0,buf.writeInt(data.length);
v1.4.0,buf.writeBytes(data);
v1.4.0,buf.writeInt(location.getPort());
v1.4.0,}
v1.4.0,if(comeFromPs) {
v1.4.0,psId = new ParameterServerId(buf.readInt());
v1.4.0,int size = buf.readInt();
v1.4.0,byte[] data = new byte[size];
v1.4.0,buf.readBytes(data);
v1.4.0,"location = new Location(new String(data), buf.readInt());"
v1.4.0,}
v1.4.0,write the max abs
v1.4.0,TODO Auto-generated method stub
v1.4.0,TODO Auto-generated method stub
v1.4.0,TODO Auto-generated method stub
v1.4.0,get configuration from config file
v1.4.0,set localDir with enviroment set by nm.
v1.4.0,get master location
v1.4.0,init task manager and start tasks
v1.4.0,start heartbeat thread
v1.4.0,taskManager.assignTaskIds(response.getTaskidsList());
v1.4.0,todo
v1.4.0,"if worker timeout, it may be knocked off."
v1.4.0,"SUCCESS, do nothing"
v1.4.0,heartbeatFailedTime = 0;
v1.4.0,private KEY currentKey;
v1.4.0,will be created
v1.4.0,TODO Auto-generated method stub
v1.4.0,Bitmap bitmap = new Bitmap();
v1.4.0,int max = indexArray[size - 1];
v1.4.0,byte [] bitIndexArray = new byte[max / 8 + 1];
v1.4.0,for(int i = 0; i < size; i++){
v1.4.0,int bitIndex = indexArray[i] >> 3;
v1.4.0,int bitOffset = indexArray[i] - (bitIndex << 3);
v1.4.0,switch(bitOffset){
v1.4.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
v1.4.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
v1.4.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
v1.4.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
v1.4.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
v1.4.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
v1.4.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
v1.4.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
v1.4.0,}
v1.4.0,}
v1.4.0,//////////////////////////////
v1.4.0,Application Configs
v1.4.0,//////////////////////////////
v1.4.0,//////////////////////////////
v1.4.0,Master Configs
v1.4.0,//////////////////////////////
v1.4.0,//////////////////////////////
v1.4.0,Worker Configs
v1.4.0,//////////////////////////////
v1.4.0,//////////////////////////////
v1.4.0,Task Configs
v1.4.0,//////////////////////////////
v1.4.0,//////////////////////////////
v1.4.0,ParameterServer Configs
v1.4.0,//////////////////////////////
v1.4.0,////////////////// IPC //////////////////////////
v1.4.0,//////////////////////////////
v1.4.0,Matrix transfer Configs.
v1.4.0,//////////////////////////////
v1.4.0,//////////////////////////////
v1.4.0,Matrix transfer Configs.
v1.4.0,//////////////////////////////
v1.4.0,Configs used to ANGEL_PS_PSAGENT running mode future.
v1.4.0,model parse
v1.4.0,Mark whether use pyangel or not.
v1.4.0,private Configuration conf;
v1.4.0,"Configuration that should be used in python environment, there should only be one"
v1.4.0,configuration instance in each Angel context.
v1.4.0,Use private access means jconf should not be changed or modified in this way.
v1.4.0,Exit on EOF or broken pipe to ensure that this process dies when the Python driver dies:
v1.4.0,Do nothing
v1.4.0,To-DO: add other ways to justify different value types
v1.4.0,"This is so ugly, must re-implement by more elegance way"
v1.4.0,"Create python path which include angel's jars, the python directory in ANGEL_HOME,"
v1.4.0,and other files submitted by user.
v1.4.0,Launch python process
v1.4.0,TODO Auto-generated constructor stub
v1.4.0,Feature number of train data
v1.4.0,Number of nonzero features
v1.4.0,Tree number
v1.4.0,Tree depth
v1.4.0,Split number
v1.4.0,Feature sample ratio
v1.4.0,Data format
v1.4.0,Learning rate
v1.4.0,Set basic configuration keys
v1.4.0,Use local deploy mode and dummy data spliter
v1.4.0,"set input, output path"
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,Set GBDT algorithm parameters
v1.4.0,Submit GBDT Train Task
v1.4.0,Load Model from HDFS.
v1.4.0,set basic configuration keys
v1.4.0,use local deploy mode and dummy dataspliter
v1.4.0,get a angel client
v1.4.0,add matrix
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,"Set trainning data, save model, log path"
v1.4.0,Set actionType train
v1.4.0,Set MF algorithm parameters
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Validation sample Ratio
v1.4.0,"Data format, libsvm or dummy"
v1.4.0,Train batch number per epoch.
v1.4.0,Batch number
v1.4.0,Model type
v1.4.0,Learning rate
v1.4.0,Decay of learning rate
v1.4.0,Regularization coefficient
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,Set data format
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,set sgd LR algorithm parameters #feature #epoch
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType incremental train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set predict result path
v1.4.0,Set actionType prediction
v1.4.0,LOG.info(sigmoid(data[i]));
v1.4.0,LOG.info(Math.exp(-data[i]));
v1.4.0,when b is a negative number
v1.4.0,LOG.info(sigmoid(data[i]));
v1.4.0,LOG.info(Math.exp(-data[i]));
v1.4.0,when b is a negative number
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Validation sample Ratio
v1.4.0,"Data format, libsvm or dummy"
v1.4.0,Train batch number per epoch.
v1.4.0,Batch number
v1.4.0,Learning rate
v1.4.0,Decay of learning rate
v1.4.0,Regularization coefficient
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,Set data format
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,set MLR algorithm parameters #feature #epoch
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType incremental train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set predict result path
v1.4.0,Set actionType prediction
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Rank
v1.4.0,Regularization parameters
v1.4.0,Learn rage
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,set FM algorithm parameters #feature #epoch
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Set learnType
v1.4.0,Set feature number
v1.4.0,Cluster center number
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Sample ratio per mini-batch
v1.4.0,C
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,set Kmeans algorithm parameters #cluster #feature #epoch
v1.4.0,Set data format
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log sava path
v1.4.0,Set actionType train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set predict result path
v1.4.0,Set actionType prediction
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Validation Ratio
v1.4.0,Data format
v1.4.0,Train batch number per epoch.
v1.4.0,Learning rate
v1.4.0,Decay of learning rate
v1.4.0,Regularization coefficient
v1.4.0,Set basic configuration keys
v1.4.0,Set data format
v1.4.0,Use local deploy mode
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,set sgd SVM algorithm parameters
v1.4.0,"set input, output path"
v1.4.0,Set save model path
v1.4.0,Set actionType train
v1.4.0,Set log path
v1.4.0,Submit LR Train Task
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set save model path
v1.4.0,Set actionType incremental train
v1.4.0,Set log path
v1.4.0,Feature number of train data
v1.4.0,Total iteration number
v1.4.0,Validation sample Ratio
v1.4.0,"Data format, libsvm or dummy"
v1.4.0,Train batch number per epoch.
v1.4.0,Learning rate
v1.4.0,Decay of learning rate
v1.4.0,Regularization coefficient
v1.4.0,Set local deploy mode
v1.4.0,Set basic configuration keys
v1.4.0,Set data format
v1.4.0,"set angel resource parameters #worker, #task, #PS"
v1.4.0,set sgd LR algorithm parameters #feature #epoch
v1.4.0,Set trainning data path
v1.4.0,Set save model path
v1.4.0,Set log path
v1.4.0,Set actionType train
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set save model path
v1.4.0,Set actionType incremental train
v1.4.0,Set log path
v1.4.0,Set trainning data path
v1.4.0,Set load model path
v1.4.0,Set predict result path
v1.4.0,Set log sava path
v1.4.0,Set actionType prediction
v1.4.0,double z=pre*y;
v1.4.0,if(z<=0) return 0.5-z;
v1.4.0,"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);"
v1.4.0,return 0.0;
v1.4.0,logistic loss for binary classification task.
v1.4.0,"logistic loss, but predict un-transformed margin"
v1.4.0,check if label in range
v1.4.0,return the default evaluation metric for the objective
v1.4.0,TODO Auto-generated method stub
v1.4.0,start row index for words
v1.4.0,doc ids
v1.4.0,topic assignments
v1.4.0,count word
v1.4.0,build word start index
v1.4.0,build dks
v1.4.0,"model.wtMat().increment(w, update);"
v1.4.0,"update.plusBy(t, 1);"
v1.4.0,"model.wtMat().increment(w, update);"
v1.4.0,The starting point
v1.4.0,There's always an unused entry.
v1.4.0,The starting point
v1.4.0,There's always an unused entry.
v1.4.0,The starting point
v1.4.0,There's always an unused entry.
v1.4.0,The starting point
v1.4.0,There's always an unused entry.
v1.4.0,The starting point
v1.4.0,There's always an unused entry.
v1.4.0,The starting point
v1.4.0,There's always an unused entry.
v1.4.0,print();
v1.4.0,The starting point
v1.4.0,There's always an unused entry.
v1.4.0,The starting point
v1.4.0,There's always an unused entry.
v1.4.0,The starting point
v1.4.0,There's always an unused entry.
v1.4.0,The starting point
v1.4.0,There's always an unused entry.
v1.4.0,print();
v1.4.0,Write #rows
v1.4.0,Write each row
v1.4.0,dense
v1.4.0,sparse
v1.4.0,LOG.info(buf.refCnt());
v1.4.0,dense
v1.4.0,sparse
v1.4.0,LOG.info(buf.refCnt());
v1.4.0,loss function
v1.4.0,gradient and hessian
v1.4.0,tree node
v1.4.0,initialize the phase
v1.4.0,current tree and depth
v1.4.0,create loss function
v1.4.0,calculate grad info of each instance
v1.4.0,"create data sketch, push candidate split value to PS"
v1.4.0,1. calculate candidate split value
v1.4.0,categorical features
v1.4.0,2. push local sketch to PS
v1.4.0,3. set phase to GET_SKETCH
v1.4.0,the leader worker
v1.4.0,merge categorical features
v1.4.0,create updates
v1.4.0,"pull the global sketch from PS, only called once by each worker"
v1.4.0,number of categorical feature
v1.4.0,sample feature
v1.4.0,push sampled feature set to the current tree
v1.4.0,create new tree
v1.4.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
v1.4.0,calculate gradient
v1.4.0,"1. create new tree, initialize tree nodes and node stats"
v1.4.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
v1.4.0,2.1. pull the sampled features of the current tree
v1.4.0,"2.2. if use all the features, only called one"
v1.4.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
v1.4.0,4. set root node to active
v1.4.0,"5. reset instance position, set the root node's span"
v1.4.0,6. calculate gradient
v1.4.0,7. set phase to run active
v1.4.0,1. start threads of active tree nodes
v1.4.0,1.1. start threads for active nodes to generate histogram
v1.4.0,1.2. set thread status to batch num
v1.4.0,1.3. set the oplog to active
v1.4.0,"2. check thread stats, if all threads finish, return"
v1.4.0,clock
v1.4.0,find split
v1.4.0,"1. find responsible tree node, using RR scheme"
v1.4.0,2. pull gradient histogram
v1.4.0,2.1. get the name of this node's gradient histogram on PS
v1.4.0,2.2. pull the histogram
v1.4.0,histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();
v1.4.0,2.3. find best split result of this tree node
v1.4.0,2.3.1 using server split
v1.4.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.4.0,update the grad stats of children node
v1.4.0,update the left child
v1.4.0,update the right child
v1.4.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
v1.4.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
v1.4.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
v1.4.0,2.3.5 reset this tree node's gradient histogram to 0
v1.4.0,3. push split feature to PS
v1.4.0,4. push split value to PS
v1.4.0,5. push split gain to PS
v1.4.0,6. set phase to AFTER_SPLIT
v1.4.0,clock
v1.4.0,1. get split feature
v1.4.0,2. get split value
v1.4.0,3. get split gain
v1.4.0,4. get node weight
v1.4.0,5. split node
v1.4.0,"2. check thread stats, if all threads finish, return"
v1.4.0,6. clock
v1.4.0,"split the span of one node, reset the instance position"
v1.4.0,in case this worker has no instance on this node
v1.4.0,set the span of left child
v1.4.0,set the span of right child
v1.4.0,"1. left to right, find the first instance that should be in the right child"
v1.4.0,"2. right to left, find the first instance that should be in the left child"
v1.4.0,3. swap two instances
v1.4.0,4. find the cut pos
v1.4.0,than the split value
v1.4.0,5. set the span of left child
v1.4.0,6. set the span of right child
v1.4.0,set tree node to active
v1.4.0,set node to leaf
v1.4.0,set node to inactive
v1.4.0,finish current tree
v1.4.0,finish current depth
v1.4.0,set the tree phase
v1.4.0,check if there is active node
v1.4.0,check if finish all the tree
v1.4.0,update node's grad stats on PS
v1.4.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
v1.4.0,the root node's stats is updated by leader worker
v1.4.0,1. create the update
v1.4.0,2. push the update to PS
v1.4.0,the leader task adds node prediction to flush list
v1.4.0,1. name of this node's grad histogram on PS
v1.4.0,2. build the grad histogram of this node
v1.4.0,3. push the histograms to PS
v1.4.0,4. reset thread stats to finished
v1.4.0,5.1. set the children nodes of this node
v1.4.0,5.2. set split info and grad stats to this node
v1.4.0,5.2. create children nodes
v1.4.0,"5.3. create node stats for children nodes, and add them to the tree"
v1.4.0,5.4. reset instance position
v1.4.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
v1.4.0,5.6. set children nodes to leaf nodes
v1.4.0,5.7. set nid to leaf node
v1.4.0,5.8. deactivate active node
v1.4.0,"get feature type, 0:empty 1:all equal 2:real"
v1.4.0,left child <= split value; right child > split value
v1.4.0,"the first: minimal, the last: maximal"
v1.4.0,categorical features
v1.4.0,continuous features
v1.4.0,left child <= split value; right child > split value
v1.4.0,feature index used to split
v1.4.0,feature value used to split
v1.4.0,loss change after split this node
v1.4.0,grad stats of the left child
v1.4.0,grad stats of the right child
v1.4.0,"LOG.info(""Constructor with fid = -1"");"
v1.4.0,fid = -1: no split currently
v1.4.0,the minimal split value is the minimal value of feature
v1.4.0,the splits do not include the maximal value of feature
v1.4.0,"1. the average distance, (maxValue - minValue) / splitNum"
v1.4.0,2. calculate the candidate split value
v1.4.0,1. new feature's histogram (grad + hess)
v1.4.0,size: sampled_featureNum * (2 * splitNum)
v1.4.0,"in other words, concatenate each feature's histogram"
v1.4.0,2. get the span of this node
v1.4.0,------ 3. using sparse-aware method to build histogram ---
v1.4.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
v1.4.0,3.1. get the instance index
v1.4.0,3.2. get the grad and hess of the instance
v1.4.0,3.3. add to the sum
v1.4.0,3.4. loop the non-zero entries
v1.4.0,3.4.1. get feature value
v1.4.0,3.4.2. current feature's position in the sampled feature set
v1.4.0,3.4.3. find the position of feature value in a histogram
v1.4.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
v1.4.0,3.4.4. add the grad and hess to the corresponding bin
v1.4.0,3.4.5. add the reverse to the bin that contains 0.0f
v1.4.0,4. add the grad and hess sum to the zero bin of all features
v1.4.0,find the best split result of the histogram of a tree node
v1.4.0,1. calculate the gradStats of the root node
v1.4.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
v1.4.0,2. loop over features
v1.4.0,2.1. get the ture feature id in the sampled feature set
v1.4.0,2.2. get the indexes of histogram of this feature
v1.4.0,2.3. find the best split of current feature
v1.4.0,2.4. update the best split result if possible
v1.4.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.4.0,3. update the grad stats of children node
v1.4.0,3.1. update the left child
v1.4.0,3.2. update the right child
v1.4.0,find the best split result of one feature
v1.4.0,1. set the feature id
v1.4.0,2. create the best left stats and right stats
v1.4.0,3. the gain of the root node
v1.4.0,4. create the temp left and right grad stats
v1.4.0,5. loop over all the data in histogram
v1.4.0,5.1. get the grad and hess of current hist bin
v1.4.0,5.2. check whether we can split with current left hessian
v1.4.0,right = root - left
v1.4.0,5.3. check whether we can split with current right hessian
v1.4.0,5.4. calculate the current loss gain
v1.4.0,5.5. check whether we should update the split result with current loss gain
v1.4.0,split value = sketches[splitIdx]
v1.4.0,"5.6. if should update, also update the best left and right grad stats"
v1.4.0,6. set the best left and right grad stats
v1.4.0,partition number
v1.4.0,cols of each partition
v1.4.0,1. calculate the total grad sum and hess sum
v1.4.0,2. create the grad stats of the node
v1.4.0,1. calculate the total grad sum and hess sum
v1.4.0,2. create the grad stats of the node
v1.4.0,1. calculate the total grad sum and hess sum
v1.4.0,2. create the grad stats of the node
v1.4.0,"loop all the possible split value, start from split[0], the first item is the minimal feature value"
v1.4.0,find the best split result of the histogram of a tree node
v1.4.0,2.2. get the indexes of histogram of this feature
v1.4.0,2.3. find the best split of current feature
v1.4.0,2.4. update the best split result if possible
v1.4.0,find the best split result of one feature
v1.4.0,1. set the feature id
v1.4.0,splitEntry.setFid(fid);
v1.4.0,2. create the best left stats and right stats
v1.4.0,3. the gain of the root node
v1.4.0,4. create the temp left and right grad stats
v1.4.0,5. loop over all the data in histogram
v1.4.0,5.1. get the grad and hess of current hist bin
v1.4.0,5.2. check whether we can split with current left hessian
v1.4.0,right = root - left
v1.4.0,5.3. check whether we can split with current right hessian
v1.4.0,5.4. calculate the current loss gain
v1.4.0,5.5. check whether we should update the split result with current loss gain
v1.4.0,"5.6. if should update, also update the best left and right grad stats"
v1.4.0,6. set the best left and right grad stats
v1.4.0,find the best split result of a serve row on the PS
v1.4.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.4.0,2.2. get the start index in histogram of this feature
v1.4.0,2.3. find the best split of current feature
v1.4.0,2.4. update the best split result if possible
v1.4.0,"find the best split result of one feature from a server row, used by the PS"
v1.4.0,1. set the feature id
v1.4.0,2. create the best left stats and right stats
v1.4.0,3. the gain of the root node
v1.4.0,4. create the temp left and right grad stats
v1.4.0,5. loop over all the data in histogram
v1.4.0,5.1. get the grad and hess of current hist bin
v1.4.0,5.2. check whether we can split with current left hessian
v1.4.0,right = root - left
v1.4.0,5.3. check whether we can split with current right hessian
v1.4.0,5.4. calculate the current loss gain
v1.4.0,5.5. check whether we should update the split result with current loss gain
v1.4.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
v1.4.0,the task use index to find fvalue
v1.4.0,"5.6. if should update, also update the best left and right grad stats"
v1.4.0,6. set the best left and right grad stats
v1.4.0,"public int[] rootIndex; // specified root index of each instances, can be used for multi task setting"
v1.4.0,"public int[] groupPtr; // the index of begin and end of a group, needed when the learning task is ranking."
v1.4.0,clear all the information
v1.4.0,calculate the sum of gradient and hess
v1.4.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v1.4.0,ridx)
v1.4.0,check if necessary information is ready
v1.4.0,"same as add, reduce is used in All Reduce"
v1.4.0,"features used in this tree, if equals null, means use all the features without sampling"
v1.4.0,node in the tree
v1.4.0,the gradient info of each instances
v1.4.0,initialize nodes
v1.4.0,gradient
v1.4.0,second order gradient
v1.4.0,int sendStartCol = startFid * 7; // each split contains 7 doubles
v1.4.0,"System.out.println(""Indices: "" + Arrays.toString(indices));"
v1.4.0,t[i][code]++;
v1.4.0,else if (Math.random() > 0.5) {
v1.4.0,t[i][code] = freq;
v1.4.0,}
v1.4.0,"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);"
v1.4.0,"ret = Math.min(ret, t[i][h[i].encode(key)]);"
v1.4.0,"Get input path, output path"
v1.4.0,Init serde
v1.4.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.4.0,"Get input path, output path"
v1.4.0,Init serde
v1.4.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.4.0,"task type: classification, regression, or ranking"
v1.4.0,"quantile sketch, size = featureNum * splitNum"
v1.4.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
v1.4.0,"active tree nodes, size = pow(2, treeDepth) -1"
v1.4.0,sampled features. size = treeNum * sampleRatio * featureNum
v1.4.0,categorical feature. size = workerNum * cateFeatNum * splitNum
v1.4.0,"split features, size = treeNum * treeNodeNum"
v1.4.0,"split values, size = treeNum * treeNodeNum"
v1.4.0,"split gains, size = treeNum * treeNodeNum"
v1.4.0,"node weights, size = treeNum * treeNodeNum"
v1.4.0,"node preds, size = treeNum * treeNodeNum"
v1.4.0,if using PS to perform split
v1.4.0,step size for a tree
v1.4.0,number of class
v1.4.0,minimum loss change required for a split
v1.4.0,maximum depth of a tree
v1.4.0,number of features
v1.4.0,number of nonzero
v1.4.0,number of candidates split value
v1.4.0,----- the rest parameters are less important ----
v1.4.0,base instance weight
v1.4.0,minimum amount of hessian(weight) allowed in a child
v1.4.0,L2 regularization factor
v1.4.0,L1 regularization factor
v1.4.0,default direction choice
v1.4.0,maximum delta update we can add in weight estimation
v1.4.0,this parameter can be used to stabilize update
v1.4.0,default=0 means no constraint on weight delta
v1.4.0,whether we want to do subsample for row
v1.4.0,whether to subsample columns for each tree
v1.4.0,accuracy of sketch
v1.4.0,accuracy of sketch
v1.4.0,leaf vector size
v1.4.0,option for parallelization
v1.4.0,option to open cacheline optimization
v1.4.0,whether to not print info during training.
v1.4.0,maximum depth of the tree
v1.4.0,number of features used for tree construction
v1.4.0,"minimum loss change required for a split, otherwise stop split"
v1.4.0,----- the rest parameters are less important ----
v1.4.0,default direction choice
v1.4.0,whether we want to do sample data
v1.4.0,whether to sample columns during tree construction
v1.4.0,whether to use histogram for split
v1.4.0,number of histogram units
v1.4.0,whether to print info during training.
v1.4.0,----- the rest parameters are obtained after training ----
v1.4.0,total number of nodes
v1.4.0,number of deleted nodes */
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy data spliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,row 0 is a random uniform
v1.3.0,row 1 is a random normal
v1.3.0,row 2 is filled with 1.0
v1.3.0,in different part
v1.3.0,TODO Auto-generated constructor stub
v1.3.0,row 0 is a random uniform
v1.3.0,row 1 is a random normal
v1.3.0,row 2 is filled with 1.0
v1.3.0,find the max abs
v1.3.0,compress data
v1.3.0,import jdk.nashorn.internal.runtime.regexp.joni.Config;
v1.3.0,"paras[1] = ""abc"";"
v1.3.0,"paras[2] = ""123"";"
v1.3.0,Add standard Hadoop classes
v1.3.0,Feature number of train data
v1.3.0,Total iteration number
v1.3.0,Learning rate
v1.3.0,Regularization coefficient
v1.3.0,Set local deploy mode
v1.3.0,Set basic configuration keys
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,set sgd LR algorithm parameters #feature #epoch
v1.3.0,Set input data path
v1.3.0,Set save model path
v1.3.0,Set actionType train
v1.3.0,Feature number of train data
v1.3.0,Number of nonzero features
v1.3.0,Tree number
v1.3.0,Tree depth
v1.3.0,Split number
v1.3.0,Feature sample ratio
v1.3.0,Data format
v1.3.0,Learning rate
v1.3.0,Set basic configuration keys
v1.3.0,Use local deploy mode and dummy data spliter
v1.3.0,"set input, output path"
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,Set GBDT algorithm parameters
v1.3.0,Load Model from HDFS.
v1.3.0,Set local deploy mode
v1.3.0,Set basic configuration keys
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,"Set trainning data, and save model path"
v1.3.0,Set actionType train
v1.3.0,Set MF algorithm parameters
v1.3.0,Feature number of train data
v1.3.0,Total iteration number
v1.3.0,Validation sample Ratio
v1.3.0,"Data format, libsvm or dummy"
v1.3.0,Train batch number per epoch.
v1.3.0,Learning rate
v1.3.0,Decay of learning rate
v1.3.0,Regularization coefficient
v1.3.0,Set local deploy mode
v1.3.0,Set basic configuration keys
v1.3.0,Set data format
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,set sgd LR algorithm parameters #feature #epoch
v1.3.0,Set trainning data path
v1.3.0,Set save model path
v1.3.0,Set log path
v1.3.0,Set actionType train
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set save model path
v1.3.0,Set actionType incremental train
v1.3.0,Set log path
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set predict result path
v1.3.0,Set actionType prediction
v1.3.0,Feature number of train data
v1.3.0,Total iteration number
v1.3.0,Validation sample Ratio
v1.3.0,"Data format, libsvm or dummy"
v1.3.0,Train batch number per epoch.
v1.3.0,Batch number
v1.3.0,Learning rate
v1.3.0,Decay of learning rate
v1.3.0,Regularization coefficient
v1.3.0,Set local deploy mode
v1.3.0,Set basic configuration keys
v1.3.0,Set data format
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,set sgd LR algorithm parameters #feature #epoch
v1.3.0,Set trainning data path
v1.3.0,Set save model path
v1.3.0,Set log path
v1.3.0,Set actionType train
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set save model path
v1.3.0,Set log path
v1.3.0,Set actionType incremental train
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set predict result path
v1.3.0,Set log path
v1.3.0,Set actionType prediction
v1.3.0,Load model meta
v1.3.0,Convert model
v1.3.0,"Get input path, output path"
v1.3.0,Init serde
v1.3.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.3.0,Load model meta
v1.3.0,Convert model
v1.3.0,load hadoop configuration
v1.3.0,"Get input path, output path"
v1.3.0,Init serde
v1.3.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.3.0,Load model meta
v1.3.0,Check row type
v1.3.0,Load model
v1.3.0,Load model meta
v1.3.0,Check row type
v1.3.0,Load model
v1.3.0,Load model meta
v1.3.0,Check row type
v1.3.0,Load model
v1.3.0,Load model meta
v1.3.0,Check row type
v1.3.0,Load model
v1.3.0,Load model meta
v1.3.0,Check row type
v1.3.0,Load model
v1.3.0,Load model meta
v1.3.0,Check row type
v1.3.0,Load model
v1.3.0,Load model meta
v1.3.0,Check row type
v1.3.0,Load model
v1.3.0,Load model
v1.3.0,load hadoop configuration
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,worker register
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,add matrix
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,attempt 0
v1.3.0,attempt1
v1.3.0,attempt1
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,TODO Auto-generated constructor stub
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,set basic configuration keys
v1.3.0,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,Thread.sleep(5000);
v1.3.0,"response = master.getJobReport(null, request);"
v1.3.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
v1.3.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
v1.3.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,"DenseIntVector deltaVec = new DenseIntVector(100000, delta);"
v1.3.0,deltaVec.setMatrixId(matrixW1Id);
v1.3.0,deltaVec.setRowId(0);
v1.3.0,TODO Auto-generated constructor stub
v1.3.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
v1.3.0,@RunWith(MockitoJUnitRunner.class)
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,psAgent.initAndStart();
v1.3.0,test conf
v1.3.0,test master location
v1.3.0,test app id
v1.3.0,test user
v1.3.0,test ps agent attempt id
v1.3.0,test ps agent id
v1.3.0,test connection
v1.3.0,test master client
v1.3.0,test ip
v1.3.0,test loc
v1.3.0,test master location
v1.3.0,test ps location
v1.3.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
v1.3.0,test all ps ids
v1.3.0,test all matrix ids
v1.3.0,test all matrix names
v1.3.0,test matrix attribute
v1.3.0,test matrix meta
v1.3.0,test ps location
v1.3.0,test partitions
v1.3.0,"Note:[startRow,endRow)"
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,group0Id = new WorkerGroupId(0);
v1.3.0,"worker0Id = new WorkerId(group0Id, 0);"
v1.3.0,"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);"
v1.3.0,task0Id = new TaskId(0);
v1.3.0,task1Id = new TaskId(1);
v1.3.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.3.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.3.0,test this func in testWriteTo
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,"LOG.info(index[0] + "" "" + value[0]);"
v1.3.0,"LOG.info(index[1] + "" "" + value[1]);"
v1.3.0,"LOG.info(index[2] + "" "" + value[2]);"
v1.3.0,dot
v1.3.0,plus
v1.3.0,plusBy
v1.3.0,dot
v1.3.0,plus
v1.3.0,plusBy
v1.3.0,dot
v1.3.0,plus
v1.3.0,plusBy
v1.3.0,dot
v1.3.0,plusBy
v1.3.0,@Test
v1.3.0,public void dotDenseFloatVector() throws Exception {
v1.3.0,int dim = 1000;
v1.3.0,Random random = new Random(System.currentTimeMillis());
v1.3.0,
v1.3.0,double[] values = new double[dim];
v1.3.0,float[] values_1 = new float[dim];
v1.3.0,for (int i = 0; i < dim; i++) {
v1.3.0,values[i] = random.nextDouble();
v1.3.0,values_1[i] = random.nextFloat();
v1.3.0,}
v1.3.0,
v1.3.0,"DenseDoubleVector vec = new DenseDoubleVector(dim, values);"
v1.3.0,"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);"
v1.3.0,
v1.3.0,double sum = 0.0;
v1.3.0,for (int i = 0; i < dim; i++) {
v1.3.0,sum += values[i] * values_1[i];
v1.3.0,}
v1.3.0,
v1.3.0,"assertEquals(sum, vec.dot(vec_1));"
v1.3.0,
v1.3.0,}
v1.3.0,@Test
v1.3.0,public void plusDenseFlaotVector() throws Exception {
v1.3.0,"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};"
v1.3.0,"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};"
v1.3.0,"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);"
v1.3.0,"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);"
v1.3.0,
v1.3.0,TDoubleVector vec_2 = vec.plus(vec_1);
v1.3.0,for (int i = 0; i < vec.size(); i++)
v1.3.0,"assertEquals(value_1[i] + value_2[i], vec_2.get(i));"
v1.3.0,
v1.3.0,
v1.3.0,"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);"
v1.3.0,
v1.3.0,for (int i = 0; i < vec.size(); i++)
v1.3.0,"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);"
v1.3.0,
v1.3.0,double[] oldValues = vec.getValues().clone();
v1.3.0,
v1.3.0,vec.plusBy(vec_1);
v1.3.0,
v1.3.0,for (int i = 0; i < vec.size(); i++)
v1.3.0,"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));"
v1.3.0,
v1.3.0,oldValues = vec.getValues().clone();
v1.3.0,
v1.3.0,"vec.plusBy(vec_1, 3);"
v1.3.0,
v1.3.0,for (int i = 0; i < vec.size(); i++)
v1.3.0,"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));"
v1.3.0,}
v1.3.0,dot
v1.3.0,plus
v1.3.0,plusBy
v1.3.0,dot
v1.3.0,plus
v1.3.0,plusBy
v1.3.0,@Test
v1.3.0,public void plusBy3() throws Exception {
v1.3.0,"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};"
v1.3.0,"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);"
v1.3.0,"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});"
v1.3.0,vec.setRowId(0);
v1.3.0,"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});"
v1.3.0,vec_1.setRowId(1);
v1.3.0,TDoubleVector vec_2 = new SparseDoubleVector(2);
v1.3.0,"vec_2.set(1, 1.0);"
v1.3.0,vec_2.setRowId(0);
v1.3.0,
v1.3.0,mat.plusBy(vec);
v1.3.0,mat.plusBy(vec_1);
v1.3.0,mat.plusBy(vec_2);
v1.3.0,
v1.3.0,"assertEquals(2.0f, mat.get(0, 0));"
v1.3.0,"assertEquals(4.0f, mat.get(0, 1));"
v1.3.0,"assertEquals(4.0f, mat.get(1, 0));"
v1.3.0,"assertEquals(5.0f, mat.get(1, 1));"
v1.3.0,}
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,test worker getActiveTaskNum
v1.3.0,test worker getTaskNum
v1.3.0,test worker getTaskManager
v1.3.0,test workerId
v1.3.0,test workerAttemptId
v1.3.0,tet worker initFinished
v1.3.0,test worker getInitMinclock
v1.3.0,test worker loacation
v1.3.0,test AppId
v1.3.0,test Conf
v1.3.0,test UserName
v1.3.0,master location
v1.3.0,masterClient
v1.3.0,test psAgent
v1.3.0,test worker get dataBlockManager
v1.3.0,workerGroup.getSplits();
v1.3.0,application
v1.3.0,lcation
v1.3.0,workerGroup info
v1.3.0,worker info
v1.3.0,task
v1.3.0,Matrix parameters
v1.3.0,Set basic configuration keys
v1.3.0,Use local deploy mode and dummy data spliter
v1.3.0,Create an Angel client
v1.3.0,Add different types of matrix
v1.3.0,using mock object
v1.3.0,verification
v1.3.0,Stubbing
v1.3.0,Default does nothing.
v1.3.0,The app injection is optional
v1.3.0,"renderText(""hello world"");"
v1.3.0,"user choose a workerGroupID from the workergroups page,"
v1.3.0,now we should change the AngelApp params and render the workergroup page;
v1.3.0,"static final String WORKER_ID = ""worker.id"";"
v1.3.0,"div(""#logo"")."
v1.3.0,"img(""/static/hadoop-st.png"")._()."
v1.3.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
v1.3.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
v1.3.0,JQueryUI.jsnotice(html);
v1.3.0,import org.apache.hadoop.conf.Configuration;
v1.3.0,import java.lang.reflect.Field;
v1.3.0,get block locations from file system
v1.3.0,create a list of all block and their locations
v1.3.0,"if the file is not splitable, just create the one block with"
v1.3.0,full file length
v1.3.0,each split can be a maximum of maxSize
v1.3.0,if remainder is between max and 2*max - then
v1.3.0,"instead of creating splits of size max, left-max we"
v1.3.0,create splits of size left/2 and left/2. This is
v1.3.0,a heuristic to avoid creating really really small
v1.3.0,splits.
v1.3.0,add this block to the block --> node locations map
v1.3.0,"For blocks that do not have host/rack information,"
v1.3.0,assign to default  rack.
v1.3.0,add this block to the rack --> block map
v1.3.0,Add this host to rackToNodes map
v1.3.0,add this block to the node --> block map
v1.3.0,"if the file system does not have any rack information, then"
v1.3.0,use dummy rack location.
v1.3.0,The topology paths have the host name included as the last
v1.3.0,component. Strip it.
v1.3.0,get tokens for all the required FileSystems..
v1.3.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
v1.3.0,job.getConfiguration());
v1.3.0,Whether we need to recursive look into the directory structure
v1.3.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.3.0,user provided one (if any).
v1.3.0,all the files in input set
v1.3.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
v1.3.0,process all nodes and create splits that are local to a node. Generate
v1.3.0,"one split per node iteration, and walk over nodes multiple times to"
v1.3.0,distribute the splits across nodes.
v1.3.0,Skip the node if it has previously been marked as completed.
v1.3.0,"for each block, copy it into validBlocks. Delete it from"
v1.3.0,blockToNodes so that the same block does not appear in
v1.3.0,two different splits.
v1.3.0,Remove all blocks which may already have been assigned to other
v1.3.0,splits.
v1.3.0,"if the accumulated split size exceeds the maximum, then"
v1.3.0,create this split.
v1.3.0,create an input split and add it to the splits array
v1.3.0,Remove entries from blocksInNode so that we don't walk these
v1.3.0,again.
v1.3.0,Done creating a single split for this node. Move on to the next
v1.3.0,node so that splits are distributed across nodes.
v1.3.0,This implies that the last few blocks (or all in case maxSize=0)
v1.3.0,were not part of a split. The node is complete.
v1.3.0,if there were any blocks left over and their combined size is
v1.3.0,"larger than minSplitNode, then combine them into one split."
v1.3.0,Otherwise add them back to the unprocessed pool. It is likely
v1.3.0,that they will be combined with other blocks from the
v1.3.0,same rack later on.
v1.3.0,This condition also kicks in when max split size is not set. All
v1.3.0,blocks on a node will be grouped together into a single split.
v1.3.0,haven't created any split on this machine. so its ok to add a
v1.3.0,smaller one for parallelism. Otherwise group it in the rack for
v1.3.0,balanced size create an input split and add it to the splits
v1.3.0,array
v1.3.0,Remove entries from blocksInNode so that we don't walk this again.
v1.3.0,The node is done. This was the last set of blocks for this node.
v1.3.0,Put the unplaced blocks back into the pool for later rack-allocation.
v1.3.0,Node is done. All blocks were fit into node-local splits.
v1.3.0,Check if node-local assignments are complete.
v1.3.0,All nodes have been walked over and marked as completed or all blocks
v1.3.0,have been assigned. The rest should be handled via rackLock assignment.
v1.3.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
v1.3.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
v1.3.0,"if blocks in a rack are below the specified minimum size, then keep them"
v1.3.0,"in 'overflow'. After the processing of all racks is complete, these"
v1.3.0,overflow blocks will be combined into splits.
v1.3.0,Process all racks over and over again until there is no more work to do.
v1.3.0,Create one split for this rack before moving over to the next rack.
v1.3.0,Come back to this rack after creating a single split for each of the
v1.3.0,remaining racks.
v1.3.0,"Process one rack location at a time, Combine all possible blocks that"
v1.3.0,reside on this rack as one split. (constrained by minimum and maximum
v1.3.0,split size).
v1.3.0,iterate over all racks
v1.3.0,"for each block, copy it into validBlocks. Delete it from"
v1.3.0,blockToNodes so that the same block does not appear in
v1.3.0,two different splits.
v1.3.0,"if the accumulated split size exceeds the maximum, then"
v1.3.0,create this split.
v1.3.0,create an input split and add it to the splits array
v1.3.0,"if we created a split, then just go to the next rack"
v1.3.0,"if there is a minimum size specified, then create a single split"
v1.3.0,"otherwise, store these blocks into overflow data structure"
v1.3.0,There were a few blocks in this rack that
v1.3.0,remained to be processed. Keep them in 'overflow' block list.
v1.3.0,These will be combined later.
v1.3.0,Process all overflow blocks
v1.3.0,"This might cause an exiting rack location to be re-added,"
v1.3.0,but it should be ok.
v1.3.0,"if the accumulated split size exceeds the maximum, then"
v1.3.0,create this split.
v1.3.0,create an input split and add it to the splits array
v1.3.0,"Process any remaining blocks, if any."
v1.3.0,create an input split
v1.3.0,add this split to the list that is returned
v1.3.0,long num = totLength / maxSize;
v1.3.0,all blocks for all the files in input set
v1.3.0,mapping from a rack name to the list of blocks it has
v1.3.0,mapping from a block to the nodes on which it has replicas
v1.3.0,mapping from a node to the list of blocks that it contains
v1.3.0,populate all the blocks for all files
v1.3.0,stop all services
v1.3.0,1.write application state to file so that the client can get the state of the application
v1.3.0,if master exit
v1.3.0,2.clear tmp and staging directory
v1.3.0,waiting for client to get application state
v1.3.0,stop the RPC server
v1.3.0,"Security framework already loaded the tokens into current UGI, just use"
v1.3.0,them
v1.3.0,Now remove the AM->RM token so tasks don't have it
v1.3.0,add a shutdown hook
v1.3.0,init app state storage
v1.3.0,init event dispacher
v1.3.0,init location manager
v1.3.0,init container allocator
v1.3.0,init a rpc service
v1.3.0,recover matrix meta if needed
v1.3.0,recover ps attempt information if need
v1.3.0,init parameter server manager
v1.3.0,recover task information if needed
v1.3.0,init psagent manager and register psagent manager event
v1.3.0,a dummy data spliter is just for test now
v1.3.0,recover data splits information if needed
v1.3.0,init worker manager and register worker manager event
v1.3.0,register slow worker/ps checker
v1.3.0,register app manager event and finish event
v1.3.0,start a web service if use yarn deploy mode
v1.3.0,load from app state storage first if attempt index great than 1(the master is not the first
v1.3.0,retry)
v1.3.0,"if load failed, just build a new MatrixMetaManager"
v1.3.0,load ps attempt index from app state storage first if attempt index great than 1(the master
v1.3.0,is not the first retry)
v1.3.0,load task information from app state storage first if attempt index great than 1(the master
v1.3.0,is not the first retry)
v1.3.0,"if load failed, just build a new AMTaskManager"
v1.3.0,load data splits information from app state storage first if attempt index great than 1(the
v1.3.0,master is not the first retry)
v1.3.0,"if load failed, we need to recalculate the data splits"
v1.3.0,parse parameter server counters
v1.3.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.3.0,refresh last heartbeat timestamp
v1.3.0,send a state update event to the specific PSAttempt
v1.3.0,check if parameter server can commit now.
v1.3.0,check matrix metadata inconsistencies between master and parameter server.
v1.3.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
v1.3.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
v1.3.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.3.0,check whether psagent heartbeat timeout
v1.3.0,check whether parameter server heartbeat timeout
v1.3.0,check whether worker heartbeat timeout
v1.3.0,choose a unused port
v1.3.0,start RPC server
v1.3.0,find matrix partitions from master matrix meta manager for this parameter server
v1.3.0,remove this parameter server attempt from monitor set
v1.3.0,remove this parameter server attempt from monitor set
v1.3.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.3.0,find workergroup in worker manager
v1.3.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
v1.3.0,"if this worker group run over, just return WORKERGROUP_EXITED"
v1.3.0,"if this worker group is running now, return tasks, workers, data splits for it"
v1.3.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.3.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.3.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.3.0,"in ANGEL_PS mode, task id may can not know advance"
v1.3.0,update the clock for this matrix
v1.3.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.3.0,"in ANGEL_PS mode, task id may can not know advance"
v1.3.0,update task iteration
v1.3.0,private boolean matrixInited;
v1.3.0,"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.3.0,dispatch matrix partitions to parameter servers
v1.3.0,update matrix id generator
v1.3.0,"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.3.0,dispatch matrix partitions to parameter servers
v1.3.0,get matrix ids in the parameter server report
v1.3.0,get the matrices parameter server need to create and delete
v1.3.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
v1.3.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
v1.3.0,waitForMatrixReleaseOnPS(matrixId);
v1.3.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
v1.3.0,the number of splits equal to the number of tasks
v1.3.0,split data
v1.3.0,dispatch the splits to workergroups
v1.3.0,split data
v1.3.0,dispatch the splits to workergroups
v1.3.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.3.0,"first, then divided by expected split number"
v1.3.0,get input format class from configuration and then instantiation a input format object
v1.3.0,split data
v1.3.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.3.0,"first, then divided by expected split number"
v1.3.0,get input format class from configuration and then instantiation a input format object
v1.3.0,split data
v1.3.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.3.0,need to fine tune the number of workergroup and task based on the actual split number
v1.3.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.3.0,Record the location information for the splits in order to data localized schedule
v1.3.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.3.0,need to fine tune the number of workergroup and task based on the actual split number
v1.3.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.3.0,Record the location information for the splits in order to data localized schedule
v1.3.0,write meta data to a temporary file
v1.3.0,rename the temporary file to final file
v1.3.0,"if the file exists, read from file and deserialize it"
v1.3.0,write task meta
v1.3.0,write ps meta
v1.3.0,generate a temporary file
v1.3.0,write task meta to the temporary file first
v1.3.0,rename the temporary file to the final file
v1.3.0,"if last final task file exist, remove it"
v1.3.0,find task meta file which has max timestamp
v1.3.0,"if the file does not exist, just return null"
v1.3.0,read task meta from file and deserialize it
v1.3.0,generate a temporary file
v1.3.0,write ps meta to the temporary file first.
v1.3.0,rename the temporary file to the final file
v1.3.0,"if the old final file exist, just remove it"
v1.3.0,find ps meta file
v1.3.0,"if ps meta file does not exist, just return null"
v1.3.0,read ps meta from file and deserialize it
v1.3.0,Init matrix files meta
v1.3.0,Move output files
v1.3.0,Write the meta file
v1.3.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.3.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.3.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.3.0,create the topology tables
v1.3.0,Transitions from the NEW state.
v1.3.0,PA_FAILMSG
v1.3.0,Transitions from the UNASSIGNED state.
v1.3.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG"
v1.3.0,event
v1.3.0,Transitions from the ASSIGNED state.
v1.3.0,"this happened when launch thread run slowly, and PA_REGISTER event"
v1.3.0,dispatched before PA_CONTAINER_LAUNCHED event
v1.3.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.3.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.3.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.3.0,create the topology tables
v1.3.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will"
v1.3.0,retry another attempt or failed
v1.3.0,release container
v1.3.0,TODO
v1.3.0,set the launch time
v1.3.0,"set tarckerName,httpPort, which used by webserver"
v1.3.0,added to psManager so psManager can monitor it;
v1.3.0,psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);
v1.3.0,set the finish time only if launch time is set
v1.3.0,"ParameterServerJVM.setVMEnv(myEnv, conf);"
v1.3.0,Set up the launch command
v1.3.0,Duplicate the ByteBuffers for access by multiple containers.
v1.3.0,Construct the actual Container
v1.3.0,Application resources
v1.3.0,Application environment
v1.3.0,Service data
v1.3.0,Tokens
v1.3.0,Set up JobConf to be localized properly on the remote NM.
v1.3.0,Setup DistributedCache
v1.3.0,Setup up task credentials buffer
v1.3.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.3.0,Add the env variables passed by the user
v1.3.0,Set logging level in the environment.
v1.3.0,"This is so that, if the child forks another ""bin/hadoop"" (common in"
v1.3.0,streaming) it will have the correct loglevel.
v1.3.0,Setup the log4j prop
v1.3.0,Add main class and its arguments
v1.3.0,Finally add the jvmID
v1.3.0,vargs.add(String.valueOf(jvmID.getId()));
v1.3.0,Final commmand
v1.3.0,Transitions from the NEW state.
v1.3.0,Transitions from the RUNNING state.
v1.3.0,Transitions from the SUCCEEDED state
v1.3.0,Transitions from the KILLED state
v1.3.0,Transitions from the FAILED state
v1.3.0,Transitions from the NEW state.
v1.3.0,Transitions from the SCHEDULED state.
v1.3.0,Transitions from the RUNNING state.
v1.3.0,"another attempt launched,"
v1.3.0,Transitions from the SUCCEEDED state
v1.3.0,Transitions from the KILLED state
v1.3.0,Transitions from the FAILED state
v1.3.0,add diagnostic
v1.3.0,Set up the launch command
v1.3.0,Duplicate the ByteBuffers for access by multiple containers.
v1.3.0,Construct the actual Container
v1.3.0,Application resources
v1.3.0,Application environment
v1.3.0,Service data
v1.3.0,Tokens
v1.3.0,Set up JobConf to be localized properly on the remote NM.
v1.3.0,Setup DistributedCache
v1.3.0,Setup up task credentials buffer
v1.3.0,LocalStorageToken is needed irrespective of whether security is enabled
v1.3.0,or not.
v1.3.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
v1.3.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.3.0,Construct the actual Container
v1.3.0,The null fields are per-container and will be constructed for each
v1.3.0,container separately.
v1.3.0,Set up the launch command
v1.3.0,Duplicate the ByteBuffers for access by multiple containers.
v1.3.0,Construct the actual Container
v1.3.0,"a * in the classpath will only find a .jar, so we need to filter out"
v1.3.0,all .jars and add everything else
v1.3.0,Propagate the system classpath when using the mini cluster
v1.3.0,Add standard Hadoop classes
v1.3.0,Cache archives
v1.3.0,Cache files
v1.3.0,Sanity check
v1.3.0,Add URI fragment or just the filename
v1.3.0,Add the env variables passed by the user
v1.3.0,Set logging level in the environment.
v1.3.0,Setup the log4j prop
v1.3.0,Add main class and its arguments
v1.3.0,Finally add the jvmID
v1.3.0,vargs.add(String.valueOf(jvmID.getId()));
v1.3.0,Final commmand
v1.3.0,Add the env variables passed by the user
v1.3.0,Set logging level in the environment.
v1.3.0,Setup the log4j prop
v1.3.0,Add main class and its arguments
v1.3.0,Final commmand
v1.3.0,"if amTask is not null, we should clone task state from it"
v1.3.0,"if all parameter server complete commit, master can commit now"
v1.3.0,init and start master committer
v1.3.0,Transitions from the NEW state.
v1.3.0,Transitions from the UNASSIGNED state.
v1.3.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
v1.3.0,Transitions from the ASSIGNED state.
v1.3.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
v1.3.0,PA_CONTAINER_LAUNCHED event
v1.3.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.3.0,Transitions from the PSAttemptStateInternal.COMMITTING state
v1.3.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.3.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.3.0,create the topology tables
v1.3.0,reqeuest resource:send a resource request to the resource allocator
v1.3.0,"Once the resource is applied, build and send the launch request to the container launcher"
v1.3.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
v1.3.0,resource allocator
v1.3.0,set the launch time
v1.3.0,add the ps attempt to the heartbeat timeout monitoring list
v1.3.0,parse ps attempt location and put it to location manager
v1.3.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
v1.3.0,or failed
v1.3.0,remove ps attempt id from heartbeat timeout monitor list
v1.3.0,release container:send a release request to container launcher
v1.3.0,set the finish time only if launch time is set
v1.3.0,private long scheduledTime;
v1.3.0,Transitions from the NEW state.
v1.3.0,Transitions from the SCHEDULED state.
v1.3.0,Transitions from the RUNNING state.
v1.3.0,"another attempt launched,"
v1.3.0,Transitions from the SUCCEEDED state
v1.3.0,Transitions from the KILLED state
v1.3.0,Transitions from the FAILED state
v1.3.0,add diagnostic
v1.3.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.3.0,start a new attempt for this ps
v1.3.0,notify ps manager
v1.3.0,add diagnostic
v1.3.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.3.0,start a new attempt for this ps
v1.3.0,notify ps manager
v1.3.0,notify the event handler of state change
v1.3.0,"If new state is not RUNNING and COMMITTING, add it to state timeout monitor"
v1.3.0,"if forcedState is set, just return"
v1.3.0,else get state from state machine
v1.3.0,add this worker group to the success set
v1.3.0,check if all worker group run over
v1.3.0,add this worker group to the failed set
v1.3.0,check if too many worker groups are failed or killed
v1.3.0,notify a run failed event
v1.3.0,add this worker group to the failed set
v1.3.0,check if too many worker groups are failed or killed
v1.3.0,notify a run failed event
v1.3.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
v1.3.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
v1.3.0,just return the total task number now
v1.3.0,TODO
v1.3.0,"if workerAttempt is not null, we should clone task state from it"
v1.3.0,from NEW state
v1.3.0,from SCHEDULED state
v1.3.0,get data splits location for data locality
v1.3.0,reqeuest resource:send a resource request to the resource allocator
v1.3.0,"once the resource is applied, build and send the launch request to the container launcher"
v1.3.0,notify failed message to the worker
v1.3.0,notify killed message to the worker
v1.3.0,release the allocated container
v1.3.0,notify failed message to the worker
v1.3.0,remove the worker attempt from heartbeat timeout listen list
v1.3.0,release the allocated container
v1.3.0,notify killed message to the worker
v1.3.0,remove the worker attempt from heartbeat timeout listen list
v1.3.0,clean the container
v1.3.0,notify failed message to the worker
v1.3.0,remove the worker attempt from heartbeat timeout listen list
v1.3.0,record the finish time
v1.3.0,clean the container
v1.3.0,notify killed message to the worker
v1.3.0,remove the worker attempt from heartbeat timeout listening list
v1.3.0,record the finish time
v1.3.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
v1.3.0,set worker attempt location
v1.3.0,notify the register message to the worker
v1.3.0,record the launch time
v1.3.0,update worker attempt metrics
v1.3.0,update tasks metrics
v1.3.0,clean the container
v1.3.0,notify the worker attempt run successfully message to the worker
v1.3.0,record the finish time
v1.3.0,init a worker attempt for the worker
v1.3.0,schedule the worker attempt
v1.3.0,add diagnostic
v1.3.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.3.0,init and start a new attempt for this ps
v1.3.0,notify worker manager
v1.3.0,add diagnostic
v1.3.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.3.0,init and start a new attempt for this ps
v1.3.0,notify worker manager
v1.3.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
v1.3.0,register to Yarn RM
v1.3.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
v1.3.0,"catch YarnRuntimeException, we should exit and need not retry"
v1.3.0,build heartbeat request
v1.3.0,send heartbeat request to rm
v1.3.0,"This can happen if the RM has been restarted. If it is in that state,"
v1.3.0,this application must clean itself up.
v1.3.0,Setting NMTokens
v1.3.0,assgin containers
v1.3.0,"if some container is not assigned, release them"
v1.3.0,handle finish containers
v1.3.0,dispatch container exit message to corresponding components
v1.3.0,killed by framework
v1.3.0,killed by framework
v1.3.0,killed by framework
v1.3.0,get application finish state
v1.3.0,build application diagnostics
v1.3.0,TODO:add a job history for angel
v1.3.0,build unregister request
v1.3.0,send unregister request to rm
v1.3.0,Note this down for next interaction with ResourceManager
v1.3.0,based on blacklisting comments above we can end up decrementing more
v1.3.0,than requested. so guard for that.
v1.3.0,send the updated resource request to RM
v1.3.0,send 0 container count requests also to cancel previous requests
v1.3.0,Update resource requests
v1.3.0,try to assign to all nodes first to match node local
v1.3.0,try to match all rack local
v1.3.0,assign remaining
v1.3.0,Update resource requests
v1.3.0,send the container-assigned event to task attempt
v1.3.0,build the start container request use launch context
v1.3.0,send the start request to Yarn nm
v1.3.0,send the message that the container starts successfully to the corresponding component
v1.3.0,"after launching, send launched event to task attempt to move"
v1.3.0,it from ASSIGNED to RUNNING state
v1.3.0,send the message that the container starts failed to the corresponding component
v1.3.0,kill the remote container if already launched
v1.3.0,start a thread pool to startup the container
v1.3.0,See if we need up the pool size only if haven't reached the
v1.3.0,maximum limit yet.
v1.3.0,nodes where containers will run at *this* point of time. This is
v1.3.0,*not* the cluster size and doesn't need to be.
v1.3.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
v1.3.0,later is just a buffer so we are not always increasing the
v1.3.0,pool-size
v1.3.0,the events from the queue are handled in parallel
v1.3.0,using a thread pool
v1.3.0,return if already stopped
v1.3.0,shutdown any containers that might be left running
v1.3.0,Build and initialize rpc client to master
v1.3.0,Build local location
v1.3.0,"Initialize matrix info, this method will wait until master accepts the information from"
v1.3.0,client
v1.3.0,Get ps locations from master and put them to the location cache.
v1.3.0,Initialize matrix meta information
v1.3.0,Start heartbeat thread if need
v1.3.0,Start all services
v1.3.0,Register to master first
v1.3.0,Report state to master every specified time
v1.3.0,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.3.0,Stop all modules
v1.3.0,Exit the process if on ANGEL_PS_PSAGENT mode
v1.3.0,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.3.0,Stop all modules
v1.3.0,Exit the process if on ANGEL_PS_PSAGENT mode
v1.3.0,get configuration from config file
v1.3.0,set localDir with enviroment set by nm.
v1.3.0,Update generic resource counters
v1.3.0,Updating resources specified in ResourceCalculatorProcessTree
v1.3.0,Remove the CPU time consumed previously by JVM reuse
v1.3.0,array stores clock for each row and clock
v1.3.0,local task num
v1.3.0,mapping from task index to taskId
v1.3.0,mapping from taskId to task index
v1.3.0,TODO Auto-generated method stub
v1.3.0,Generate a flush request and put it to request queue
v1.3.0,Generate a clock request and put it to request queue
v1.3.0,Generate a merge request and put it to request queue
v1.3.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
v1.3.0,matrix
v1.3.0,and add it to cache maps
v1.3.0,Add the message to the tree map
v1.3.0,"If there are flush / clock requests blocked, we need to put this merge request into"
v1.3.0,the waiting queue
v1.3.0,Launch a merge worker to merge the update to matrix op log cache
v1.3.0,Remove the message from the tree map
v1.3.0,Wake up blocked flush/clock request
v1.3.0,Add flush/clock request to listener list to waiting for all the existing
v1.3.0,updates are merged
v1.3.0,Wake up blocked flush/clock request
v1.3.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
v1.3.0,blocked.
v1.3.0,Get next merge message sequence id
v1.3.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
v1.3.0,position
v1.3.0,Wake up blocked merge requests
v1.3.0,Get minimal sequence id from listeners
v1.3.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
v1.3.0,should flush updates to local matrix storage
v1.3.0,unused now
v1.3.0,Get partitions for the matrix
v1.3.0,"Filter it, removing zero values"
v1.3.0,Doing average or not
v1.3.0,Split this row according the matrix partitions
v1.3.0,Add the splits to the result container
v1.3.0,"For each partition, we generate a update split."
v1.3.0,"Although the split is empty for partitions those without any update data,"
v1.3.0,we still need to generate a update split to update the clock info on ps.
v1.3.0,"For each partition, we generate a update split."
v1.3.0,"Although the split is empty for partitions those without any update data,"
v1.3.0,we still need to generate a update split to update the clock info on ps.
v1.3.0,int seqId = ((ByteBuf) msg).readInt();
v1.3.0,"LOG.info(""receive result of seqId="" + seqId);"
v1.3.0,((ByteBuf) msg).resetReaderIndex();
v1.3.0,TODO: use Epoll for linux future
v1.3.0,closeChannelForServer(request.getServerId());
v1.3.0,closeChannelForServer(request.getServerId());
v1.3.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.3.0,Then submit normal task until reach upper limit of flow control or all tasks are submit
v1.3.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.3.0,"LOG.info(""choose put server "" + psIds[index]);"
v1.3.0,allocate the bytebuf
v1.3.0,"check the location of server is ready, if not, we should wait"
v1.3.0,get a channel to server from pool
v1.3.0,"if channel is not valid, it means maybe the connections to the server are closed"
v1.3.0,channelManager.removeChannelPool(loc);
v1.3.0,find the partition request context from cache
v1.3.0,Check if the result of the sub-request is received
v1.3.0,Update received result number
v1.3.0,Get row splits received
v1.3.0,Put the row split to the cache(row index to row splits map)
v1.3.0,"If all splits of the row are received, means this row can be merged"
v1.3.0,TODO Auto-generated method stub
v1.3.0,"Check futures, if the result of a sub-request is received, put it to the result queue"
v1.3.0,Now we just support pipelined row splits merging for dense type row
v1.3.0,Wait until the clock value of this row is greater than or equal to the value
v1.3.0,Get partitions for this row
v1.3.0,First get this row from matrix storage
v1.3.0,"If the row exists in the matrix storage and the clock value meets the requirements, just"
v1.3.0,return
v1.3.0,Get row splits of this row from the matrix cache first
v1.3.0,"If the row split does not exist in cache, get it from parameter server"
v1.3.0,Wait the final result
v1.3.0,Put it to the matrix cache
v1.3.0,Split the matrix oplog according to the matrix partitions
v1.3.0,"If need update clock, we should send requests to all partitions"
v1.3.0,use update index if exist
v1.3.0,Filter the rowIds which are fetching now
v1.3.0,Send the rowIndex to rpc dispatcher and return immediately
v1.3.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
v1.3.0,Generate dispatch items and add them to the corresponding queues
v1.3.0,Pre-fetching is disable default
v1.3.0,matrix id to clock map
v1.3.0,"task index, it must be unique for whole application"
v1.3.0,if (matrixManager.getMatrixMeta(createResponse.getMatrixId()) != null) {
v1.3.0,return matrixManager.getMatrixMeta(createResponse.getMatrixId());
v1.3.0,}
v1.3.0,Deserialize data splits meta
v1.3.0,Get workers
v1.3.0,Send request to every ps
v1.3.0,Wait the responses
v1.3.0,Update clock cache
v1.3.0,Get row from cache.
v1.3.0,"if row clock is satisfy ssp staleness limit, just return."
v1.3.0,Get row from ps.
v1.3.0,"For ASYNC mode, just get from pss."
v1.3.0,"For BSP/SSP, get rows from storage/cache first"
v1.3.0,Get from ps.
v1.3.0,"For ASYNC, just get rows from pss."
v1.3.0,no more retries.
v1.3.0,calculate sleep time and return.
v1.3.0,parse the i-th sleep-time
v1.3.0,parse the i-th number-of-retries
v1.3.0,calculateSleepTime may overflow.
v1.3.0,"A few common retry policies, with no delays."
v1.3.0,close is a local operation and should finish within milliseconds; timeout just to be safe
v1.3.0,response will be null for one way messages.
v1.3.0,maxFrameLength = 2G
v1.3.0,lengthFieldOffset = 0
v1.3.0,lengthFieldLength = 8
v1.3.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
v1.3.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
v1.3.0,indicates whether this connection's life cycle is managed
v1.3.0,See if we already have a connection (common case)
v1.3.0,create a unique lock for this RS + protocol (if necessary)
v1.3.0,get the RS lock
v1.3.0,do one more lookup in case we were stalled above
v1.3.0,Only create isa when we need to.
v1.3.0,definitely a cache miss. establish an RPC for
v1.3.0,this RS
v1.3.0,Throw what the RemoteException was carrying.
v1.3.0,check
v1.3.0,every
v1.3.0,minutes
v1.3.0,TODO
v1.3.0,创建failoverHandler
v1.3.0,"The number of times this invocation handler has ever been failed over,"
v1.3.0,before this method invocation attempt. Used to prevent concurrent
v1.3.0,failed method invocations from triggering multiple failover attempts.
v1.3.0,Make sure that concurrent failed method invocations
v1.3.0,only cause a
v1.3.0,single actual fail over.
v1.3.0,RpcController + Message in the method args
v1.3.0,(generated code from RPC bits in .proto files have
v1.3.0,RpcController)
v1.3.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
v1.3.0,+ (System.currentTimeMillis() - beforeConstructTs));
v1.3.0,get an instance of the method arg type
v1.3.0,RpcController + Message in the method args
v1.3.0,(generated code from RPC bits in .proto files have
v1.3.0,RpcController)
v1.3.0,Message (hand written code usually has only a single
v1.3.0,argument)
v1.3.0,log any RPC responses that are slower than the configured
v1.3.0,warn
v1.3.0,response time or larger than configured warning size
v1.3.0,"when tagging, we let TooLarge trump TooSmall to keep"
v1.3.0,output simple
v1.3.0,note that large responses will often also be slow.
v1.3.0,provides a count of log-reported slow responses
v1.3.0,RpcController + Message in the method args
v1.3.0,(generated code from RPC bits in .proto files have
v1.3.0,RpcController)
v1.3.0,unexpected
v1.3.0,"in the protobuf methods, args[1] is the only significant argument"
v1.3.0,for JSON encoding
v1.3.0,base information that is reported regardless of type of call
v1.3.0,Disable Nagle's Algorithm since we don't want packets to wait
v1.3.0,Configure the event pipeline factory.
v1.3.0,Make a new connection.
v1.3.0,Remove all pending requests (will be canceled after relinquishing
v1.3.0,write lock).
v1.3.0,Cancel any pending requests by sending errors to the callbacks:
v1.3.0,Close the channel:
v1.3.0,Close the connection:
v1.3.0,Shut down all thread pools to exit.
v1.3.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
v1.3.0,See NettyServer.prepareResponse for where we write out the response.
v1.3.0,"It writes the call.id (int), a boolean signifying any error (and if"
v1.3.0,"so the exception name/trace), and the response bytes"
v1.3.0,Read the call id.
v1.3.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
v1.3.0,"instead, it returns a null message object."
v1.3.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
v1.3.0,System.currentTimeMillis());
v1.3.0,"It would be good widen this to just Throwable, but IOException is what we"
v1.3.0,allow now
v1.3.0,not implemented
v1.3.0,not implemented
v1.3.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
v1.3.0,cache of RpcEngines by protocol
v1.3.0,return the RpcEngine configured to handle a protocol
v1.3.0,We only handle the ConnectException.
v1.3.0,This is the exception we can't handle.
v1.3.0,check if timed out
v1.3.0,wait for retry
v1.3.0,IGNORE
v1.3.0,return the RpcEngine that handles a proxy object
v1.3.0,The default implementation works synchronously
v1.3.0,punt: allocate a new buffer & copy into it
v1.3.0,Parse cmd parameters
v1.3.0,load hadoop configuration
v1.3.0,load angel system configuration
v1.3.0,load user configuration:
v1.3.0,load user config file
v1.3.0,load command line parameters
v1.3.0,load user job resource files
v1.3.0,load user job jar if it exist
v1.3.0,Expand the environment variable
v1.3.0,Add default fs(local fs) for lib jars.
v1.3.0,"LOG.info(System.getProperty(""user.dir""));"
v1.3.0,get tokens for all the required FileSystems..
v1.3.0,Whether we need to recursive look into the directory structure
v1.3.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.3.0,user provided one (if any).
v1.3.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.3.0,get tokens for all the required FileSystems..
v1.3.0,Whether we need to recursive look into the directory structure
v1.3.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.3.0,user provided one (if any).
v1.3.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.3.0,a simple hdfs copy function assume src path and dest path are in same hdfs
v1.3.0,and FileSystem object has same schema
v1.3.0,"LOG.warn(""interrupted while sleeping"", ie);"
v1.3.0,private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);
v1.3.0,public static String getHostname() {
v1.3.0,try {
v1.3.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
v1.3.0,} catch (UnknownHostException uhe) {
v1.3.0,}
v1.3.0,"return new StringBuilder().append("""").append(uhe).toString();"
v1.3.0,}
v1.3.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
v1.3.0,String hostname = getHostname();
v1.3.0,String classname = clazz.getSimpleName();
v1.3.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
v1.3.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
v1.3.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
v1.3.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
v1.3.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
v1.3.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
v1.3.0,
v1.3.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
v1.3.0,public void run() {
v1.3.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
v1.3.0,"this.val$classname + "" at "" + this.val$hostname}));"
v1.3.0,}
v1.3.0,});
v1.3.0,}
v1.3.0,"We we interrupted because we're meant to stop? If not, just"
v1.3.0,continue ignoring the interruption
v1.3.0,Recalculate waitTime.
v1.3.0,// Begin delegation to Thread
v1.3.0,// End delegation to Thread
v1.3.0,instance submitter class
v1.3.0,Obtain filename from path
v1.3.0,Split filename to prexif and suffix (extension)
v1.3.0,Check if the filename is okay
v1.3.0,Prepare temporary file
v1.3.0,Prepare buffer for data copying
v1.3.0,Open and check input stream
v1.3.0,Open output stream and copy data between source file in JAR and the temporary file
v1.3.0,"If read/write fails, close streams safely before throwing an exception"
v1.3.0,"Finally, load the library"
v1.3.0,little endian load order
v1.3.0,tail
v1.3.0,fallthrough
v1.3.0,fallthrough
v1.3.0,finalization
v1.3.0,fmix(h1);
v1.3.0,----------
v1.3.0,body
v1.3.0,----------
v1.3.0,tail
v1.3.0,----------
v1.3.0,finalization
v1.3.0,----------
v1.3.0,body
v1.3.0,----------
v1.3.0,tail
v1.3.0,----------
v1.3.0,finalization
v1.3.0,JobStateProto jobState = report.getJobState();
v1.3.0,the leaf level file should be readable by others
v1.3.0,the subdirs in the path should have execute permissions for
v1.3.0,others
v1.3.0,2.get job id
v1.3.0,Credentials credentials = new Credentials();
v1.3.0,4.copy resource files to hdfs
v1.3.0,5.write configuration to a xml file
v1.3.0,6.create am container context
v1.3.0,7.Submit to ResourceManager
v1.3.0,8.get app master client
v1.3.0,Create a number of filenames in the JobTracker's fs namespace
v1.3.0,add all the command line files/ jars and archive
v1.3.0,first copy them to jobtrackers filesystem
v1.3.0,should not throw a uri exception
v1.3.0,should not throw an uri excpetion
v1.3.0,set the timestamps of the archives and files
v1.3.0,set the public/private visibility of the archives and files
v1.3.0,get DelegationToken for each cached file
v1.3.0,check if we do not need to copy the files
v1.3.0,is jt using the same file system.
v1.3.0,just checking for uri strings... doing no dns lookups
v1.3.0,to see if the filesystems are the same. This is not optimal.
v1.3.0,but avoids name resolution.
v1.3.0,this might have name collisions. copy will throw an exception
v1.3.0,parse the original path to create new path
v1.3.0,check for ports
v1.3.0,Write job file to JobTracker's fs
v1.3.0,Setup resource requirements
v1.3.0,Setup LocalResources
v1.3.0,Setup security tokens
v1.3.0,Setup the command to run the AM
v1.3.0,Add AM user command opts
v1.3.0,Final command
v1.3.0,Setup the CLASSPATH in environment
v1.3.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
v1.3.0,Setup the environment variables for Admin first
v1.3.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
v1.3.0,Parse distributed cache
v1.3.0,Setup ContainerLaunchContext for AM container
v1.3.0,Set up the ApplicationSubmissionContext
v1.3.0,resposne.encode(buf);
v1.3.0,TODO:
v1.3.0,resposne.encode(buf);
v1.3.0,TODO:
v1.3.0,resposne.encode(buf);
v1.3.0,TODO:
v1.3.0,resposne.encode(buf);
v1.3.0,TODO:
v1.3.0,Add tokens to new user so that it may execute its task correctly.
v1.3.0,to exit
v1.3.0,private final ParameterServer psServer;
v1.3.0,TODO
v1.3.0,"when we should write snapshot to hdfs? clearly, we have two methods:"
v1.3.0,"1. write snapshot at regular time, if there are updates, just write them."
v1.3.0,"2. write snapshot every N iterations, this method depends on notification of master"
v1.3.0,"FSDataOutputStream output = fileContext.create(snapshotsTempFilePath,"
v1.3.0,EnumSet.of(CreateFlag.CREATE));
v1.3.0,@brief get filename of the old snapshot written before
v1.3.0,"no snapshotFile write before, maybe write snapshots the first time"
v1.3.0,data.rewind();
v1.3.0,data.rewind();
v1.3.0,data.rewind();
v1.3.0,output.writeInt(clock);
v1.3.0,clock = input.readInt();
v1.3.0,Read matrix meta from meta file
v1.3.0,Load partitions from file use fork-join
v1.3.0,Save partitions to files use fork-join
v1.3.0,Write the ps matrix meta to the meta file
v1.3.0,Mapping from taskId to clock value.
v1.3.0,int[] keys = sparseRep.getKeys();
v1.3.0,int[] values = sparseRep.getValues();
v1.3.0,boolean[] used = sparseRep.getUsed();
v1.3.0,nnz = 0;
v1.3.0,for (int i = 0; i < keys.length; i++)
v1.3.0,if (used[i]) {
v1.3.0,"denseRep.put(keys[i], values[i]);"
v1.3.0,nnz++;
v1.3.0,}
v1.3.0,sparseRep = null;
v1.3.0,int[] keys = sparseRep.getKeys();
v1.3.0,int[] values = sparseRep.getValues();
v1.3.0,boolean[] used = sparseRep.getUsed();
v1.3.0,for (int i = 0; i < keys.length; i++)
v1.3.0,if (used[i]) {
v1.3.0,"denseRep.put(keys[i], values[i]);"
v1.3.0,}
v1.3.0,sparseRep = null;
v1.3.0,output.writeInt(data.length);
v1.3.0,@Override
v1.3.0,public void serialize(ByteBuf buf) {
v1.3.0,if (sparseRep != null)
v1.3.0,return serializeSparse();
v1.3.0,else if (denseRep != null)
v1.3.0,return serializeDense();
v1.3.0,return serializeEmpty();
v1.3.0,}
v1.3.0,int[] keys = sparseRep.getKeys();
v1.3.0,int[] values = sparseRep.getValues();
v1.3.0,boolean[] used = sparseRep.getUsed();
v1.3.0,int idx = 0;
v1.3.0,for (int i = 0; i < keys.length; i++)
v1.3.0,if (used[i]) {
v1.3.0,"keysBuf.put(idx, keys[i]);"
v1.3.0,"valuesBuf.put(idx, values[i]);"
v1.3.0,idx++;
v1.3.0,}
v1.3.0,int[] keys = sparseRep.getKeys();
v1.3.0,int[] values = sparseRep.getValues();
v1.3.0,boolean[] used = sparseRep.getUsed();
v1.3.0,"int ov, k, v;"
v1.3.0,for (int i = 0; i < keys.length; i++) {
v1.3.0,if (used[i]) {
v1.3.0,k = keys[i];
v1.3.0,ov = denseRep.get(k);
v1.3.0,v = ov + values[i];
v1.3.0,"denseRep.put(k, v);"
v1.3.0,if (ov != 0 && v == 0)
v1.3.0,nnz--;
v1.3.0,}
v1.3.0,}
v1.3.0,"add the PSAgentContext,need fix"
v1.3.0,set MatrixPartitionLocation
v1.3.0,set attribute
v1.3.0,return this;
v1.3.0,return this;
v1.3.0,return this;
v1.3.0,TODO:
v1.3.0,write the max abs
v1.3.0,TODO Auto-generated method stub
v1.3.0,TODO Auto-generated method stub
v1.3.0,TODO Auto-generated method stub
v1.3.0,get configuration from config file
v1.3.0,set localDir with enviroment set by nm.
v1.3.0,get master location
v1.3.0,init task manager and start tasks
v1.3.0,start heartbeat thread
v1.3.0,taskManager.assignTaskIds(response.getTaskidsList());
v1.3.0,todo
v1.3.0,"if worker timeout, it may be knocked off."
v1.3.0,"SUCCESS, do nothing"
v1.3.0,heartbeatFailedTime = 0;
v1.3.0,private KEY currentKey;
v1.3.0,will be created
v1.3.0,TODO Auto-generated method stub
v1.3.0,Bitmap bitmap = new Bitmap();
v1.3.0,int max = indexArray[size - 1];
v1.3.0,byte [] bitIndexArray = new byte[max / 8 + 1];
v1.3.0,for(int i = 0; i < size; i++){
v1.3.0,int bitIndex = indexArray[i] >> 3;
v1.3.0,int bitOffset = indexArray[i] - (bitIndex << 3);
v1.3.0,switch(bitOffset){
v1.3.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
v1.3.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
v1.3.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
v1.3.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
v1.3.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
v1.3.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
v1.3.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
v1.3.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
v1.3.0,}
v1.3.0,}
v1.3.0,//////////////////////////////
v1.3.0,Application Configs
v1.3.0,//////////////////////////////
v1.3.0,//////////////////////////////
v1.3.0,Master Configs
v1.3.0,//////////////////////////////
v1.3.0,//////////////////////////////
v1.3.0,Worker Configs
v1.3.0,//////////////////////////////
v1.3.0,//////////////////////////////
v1.3.0,Task Configs
v1.3.0,//////////////////////////////
v1.3.0,//////////////////////////////
v1.3.0,ParameterServer Configs
v1.3.0,//////////////////////////////
v1.3.0,////////////////// IPC //////////////////////////
v1.3.0,//////////////////////////////
v1.3.0,Matrix transfer Configs.
v1.3.0,//////////////////////////////
v1.3.0,//////////////////////////////
v1.3.0,Matrix transfer Configs.
v1.3.0,//////////////////////////////
v1.3.0,Configs used to ANGEL_PS_PSAGENT running mode future.
v1.3.0,model parse
v1.3.0,private Configuration conf;
v1.3.0,TODO Auto-generated constructor stub
v1.3.0,Feature number of train data
v1.3.0,Number of nonzero features
v1.3.0,Tree number
v1.3.0,Tree depth
v1.3.0,Split number
v1.3.0,Feature sample ratio
v1.3.0,Data format
v1.3.0,Learning rate
v1.3.0,Set basic configuration keys
v1.3.0,Use local deploy mode and dummy data spliter
v1.3.0,"set input, output path"
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,Set GBDT algorithm parameters
v1.3.0,Submit GBDT Train Task
v1.3.0,Load Model from HDFS.
v1.3.0,set basic configuration keys
v1.3.0,use local deploy mode and dummy dataspliter
v1.3.0,get a angel client
v1.3.0,add matrix
v1.3.0,Set local deploy mode
v1.3.0,Set basic configuration keys
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,"Set trainning data, save model, log path"
v1.3.0,Set actionType train
v1.3.0,Set MF algorithm parameters
v1.3.0,Feature number of train data
v1.3.0,Total iteration number
v1.3.0,Validation sample Ratio
v1.3.0,"Data format, libsvm or dummy"
v1.3.0,Train batch number per epoch.
v1.3.0,Batch number
v1.3.0,Model type
v1.3.0,Learning rate
v1.3.0,Decay of learning rate
v1.3.0,Regularization coefficient
v1.3.0,Set local deploy mode
v1.3.0,Set basic configuration keys
v1.3.0,Set data format
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,set sgd LR algorithm parameters #feature #epoch
v1.3.0,Set trainning data path
v1.3.0,Set save model path
v1.3.0,Set log path
v1.3.0,Set actionType train
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set save model path
v1.3.0,Set log path
v1.3.0,Set actionType incremental train
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set predict result path
v1.3.0,Set actionType prediction
v1.3.0,LOG.info(sigmoid(data[i]));
v1.3.0,LOG.info(Math.exp(-data[i]));
v1.3.0,when b is a negative number
v1.3.0,LOG.info(sigmoid(data[i]));
v1.3.0,LOG.info(Math.exp(-data[i]));
v1.3.0,when b is a negative number
v1.3.0,Feature number of train data
v1.3.0,Total iteration number
v1.3.0,Validation sample Ratio
v1.3.0,"Data format, libsvm or dummy"
v1.3.0,Train batch number per epoch.
v1.3.0,Batch number
v1.3.0,Learning rate
v1.3.0,Decay of learning rate
v1.3.0,Regularization coefficient
v1.3.0,Set local deploy mode
v1.3.0,Set basic configuration keys
v1.3.0,Set data format
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,set MLR algorithm parameters #feature #epoch
v1.3.0,Set trainning data path
v1.3.0,Set save model path
v1.3.0,Set log path
v1.3.0,Set actionType train
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set save model path
v1.3.0,Set log path
v1.3.0,Set actionType incremental train
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set predict result path
v1.3.0,Set actionType prediction
v1.3.0,Feature number of train data
v1.3.0,Total iteration number
v1.3.0,Rank
v1.3.0,Regularization parameters
v1.3.0,Learn rage
v1.3.0,Set local deploy mode
v1.3.0,Set basic configuration keys
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,set FM algorithm parameters #feature #epoch
v1.3.0,Set trainning data path
v1.3.0,Set save model path
v1.3.0,Set log path
v1.3.0,Set actionType train
v1.3.0,Set trainning data path
v1.3.0,Set save model path
v1.3.0,Set log path
v1.3.0,Set actionType train
v1.3.0,Set learnType
v1.3.0,Set feature number
v1.3.0,Cluster center number
v1.3.0,Feature number of train data
v1.3.0,Total iteration number
v1.3.0,Sample ratio per mini-batch
v1.3.0,C
v1.3.0,Set local deploy mode
v1.3.0,Set basic configuration keys
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,set Kmeans algorithm parameters #cluster #feature #epoch
v1.3.0,Set data format
v1.3.0,Set trainning data path
v1.3.0,Set save model path
v1.3.0,Set log sava path
v1.3.0,Set actionType train
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set predict result path
v1.3.0,Set actionType prediction
v1.3.0,Feature number of train data
v1.3.0,Total iteration number
v1.3.0,Validation Ratio
v1.3.0,Data format
v1.3.0,Train batch number per epoch.
v1.3.0,Learning rate
v1.3.0,Decay of learning rate
v1.3.0,Regularization coefficient
v1.3.0,Set basic configuration keys
v1.3.0,Set data format
v1.3.0,Use local deploy mode
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,set sgd SVM algorithm parameters
v1.3.0,"set input, output path"
v1.3.0,Set save model path
v1.3.0,Set actionType train
v1.3.0,Set log path
v1.3.0,Submit LR Train Task
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set save model path
v1.3.0,Set actionType incremental train
v1.3.0,Set log path
v1.3.0,Feature number of train data
v1.3.0,Total iteration number
v1.3.0,Validation sample Ratio
v1.3.0,"Data format, libsvm or dummy"
v1.3.0,Train batch number per epoch.
v1.3.0,Learning rate
v1.3.0,Decay of learning rate
v1.3.0,Regularization coefficient
v1.3.0,Set local deploy mode
v1.3.0,Set basic configuration keys
v1.3.0,Set data format
v1.3.0,"set angel resource parameters #worker, #task, #PS"
v1.3.0,set sgd LR algorithm parameters #feature #epoch
v1.3.0,Set trainning data path
v1.3.0,Set save model path
v1.3.0,Set log path
v1.3.0,Set actionType train
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set save model path
v1.3.0,Set actionType incremental train
v1.3.0,Set log path
v1.3.0,Set trainning data path
v1.3.0,Set load model path
v1.3.0,Set predict result path
v1.3.0,Set log sava path
v1.3.0,Set actionType prediction
v1.3.0,double z=pre*y;
v1.3.0,if(z<=0) return 0.5-z;
v1.3.0,"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);"
v1.3.0,return 0.0;
v1.3.0,logistic loss for binary classification task.
v1.3.0,"logistic loss, but predict un-transformed margin"
v1.3.0,check if label in range
v1.3.0,return the default evaluation metric for the objective
v1.3.0,TODO Auto-generated method stub
v1.3.0,start row index for words
v1.3.0,doc ids
v1.3.0,topic assignments
v1.3.0,count word
v1.3.0,build word start index
v1.3.0,build dks
v1.3.0,"model.wtMat().increment(w, update);"
v1.3.0,"update.plusBy(t, 1);"
v1.3.0,"model.wtMat().increment(w, update);"
v1.3.0,The starting point
v1.3.0,There's always an unused entry.
v1.3.0,The starting point
v1.3.0,There's always an unused entry.
v1.3.0,The starting point
v1.3.0,There's always an unused entry.
v1.3.0,The starting point
v1.3.0,There's always an unused entry.
v1.3.0,The starting point
v1.3.0,There's always an unused entry.
v1.3.0,The starting point
v1.3.0,There's always an unused entry.
v1.3.0,print();
v1.3.0,The starting point
v1.3.0,There's always an unused entry.
v1.3.0,The starting point
v1.3.0,There's always an unused entry.
v1.3.0,The starting point
v1.3.0,There's always an unused entry.
v1.3.0,The starting point
v1.3.0,There's always an unused entry.
v1.3.0,print();
v1.3.0,Write #rows
v1.3.0,Write each row
v1.3.0,dense
v1.3.0,sparse
v1.3.0,LOG.info(buf.refCnt());
v1.3.0,dense
v1.3.0,sparse
v1.3.0,LOG.info(buf.refCnt());
v1.3.0,loss function
v1.3.0,gradient and hessian
v1.3.0,tree node
v1.3.0,initialize the phase
v1.3.0,current tree and depth
v1.3.0,create loss function
v1.3.0,calculate grad info of each instance
v1.3.0,"create data sketch, push candidate split value to PS"
v1.3.0,1. calculate candidate split value
v1.3.0,2. push local sketch to PS
v1.3.0,3. set phase to GET_SKETCH
v1.3.0,"pull the global sketch from PS, only called once by each worker"
v1.3.0,sample feature
v1.3.0,push sampled feature set to the current tree
v1.3.0,create new tree
v1.3.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
v1.3.0,calculate gradient
v1.3.0,"1. create new tree, initialize tree nodes and node stats"
v1.3.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
v1.3.0,2.1. pull the sampled features of the current tree
v1.3.0,"2.2. if use all the features, only called one"
v1.3.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
v1.3.0,4. set root node to active
v1.3.0,"5. reset instance position, set the root node's span"
v1.3.0,6. calculate gradient
v1.3.0,7. set phase to run active
v1.3.0,1. start threads of active tree nodes
v1.3.0,1.1. start threads for active nodes to generate histogram
v1.3.0,1.2. set thread status to batch num
v1.3.0,1.3. set the oplog to active
v1.3.0,"2. check thread stats, if all threads finish, return"
v1.3.0,clock
v1.3.0,find split
v1.3.0,"1. find responsible tree node, using RR scheme"
v1.3.0,2. pull gradient histogram
v1.3.0,2.1. get the name of this node's gradient histogram on PS
v1.3.0,2.2. pull the histogram
v1.3.0,histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();
v1.3.0,2.3. find best split result of this tree node
v1.3.0,2.3.1 using server split
v1.3.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.3.0,update the grad stats of children node
v1.3.0,update the left child
v1.3.0,update the right child
v1.3.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
v1.3.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
v1.3.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
v1.3.0,2.3.5 reset this tree node's gradient histogram to 0
v1.3.0,3. push split feature to PS
v1.3.0,4. push split value to PS
v1.3.0,5. push split gain to PS
v1.3.0,6. set phase to AFTER_SPLIT
v1.3.0,clock
v1.3.0,1. get split feature
v1.3.0,2. get split value
v1.3.0,3. get split gain
v1.3.0,4. get node weight
v1.3.0,5. split node
v1.3.0,"2. check thread stats, if all threads finish, return"
v1.3.0,6. clock
v1.3.0,"split the span of one node, reset the instance position"
v1.3.0,in case this worker has no instance on this node
v1.3.0,set the span of left child
v1.3.0,set the span of right child
v1.3.0,"1. left to right, find the first instance that should be in the right child"
v1.3.0,"2. right to left, find the first instance that should be in the left child"
v1.3.0,3. swap two instances
v1.3.0,4. find the cut pos
v1.3.0,than the split value
v1.3.0,5. set the span of left child
v1.3.0,6. set the span of right child
v1.3.0,set tree node to active
v1.3.0,set node to leaf
v1.3.0,set node to inactive
v1.3.0,finish current tree
v1.3.0,finish current depth
v1.3.0,set the tree phase
v1.3.0,check if there is active node
v1.3.0,check if finish all the tree
v1.3.0,update node's grad stats on PS
v1.3.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
v1.3.0,the root node's stats is updated by leader worker
v1.3.0,1. create the update
v1.3.0,2. push the update to PS
v1.3.0,the leader task adds node prediction to flush list
v1.3.0,1. name of this node's grad histogram on PS
v1.3.0,2. build the grad histogram of this node
v1.3.0,3. push the histograms to PS
v1.3.0,4. reset thread stats to finished
v1.3.0,5.1. set the children nodes of this node
v1.3.0,5.2. set split info and grad stats to this node
v1.3.0,5.2. create children nodes
v1.3.0,"5.3. create node stats for children nodes, and add them to the tree"
v1.3.0,5.4. reset instance position
v1.3.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
v1.3.0,5.6. set children nodes to leaf nodes
v1.3.0,5.7. set nid to leaf node
v1.3.0,5.8. deactivate active node
v1.3.0,"get feature type, 0:empty 1:all equal 2:real"
v1.3.0,feature index used to split
v1.3.0,feature value used to split
v1.3.0,loss change after split this node
v1.3.0,grad stats of the left child
v1.3.0,grad stats of the right child
v1.3.0,"LOG.info(""Constructor with fid = -1"");"
v1.3.0,fid = -1: no split currently
v1.3.0,the minimal split value is the minimal value of feature
v1.3.0,the splits do not include the maximal value of feature
v1.3.0,"1. the average distance, (maxValue - minValue) / splitNum"
v1.3.0,2. calculate the candidate split value
v1.3.0,1. new feature's histogram (grad + hess)
v1.3.0,size: sampled_featureNum * (2 * splitNum)
v1.3.0,"in other words, concatenate each feature's histogram"
v1.3.0,2. get the span of this node
v1.3.0,int nodeStart = this.controller.nodePosStart[nid];
v1.3.0,int nodeEnd = this.controller.nodePosEnd[nid];
v1.3.0,------ 3. using sparse-aware method to build histogram ---
v1.3.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
v1.3.0,3.1. get the instance index
v1.3.0,3.2. get the grad and hess of the instance
v1.3.0,3.3. add to the sum
v1.3.0,3.4. loop the non-zero entries
v1.3.0,3.4.1. get feature value
v1.3.0,3.4.2. current feature's position in the sampled feature set
v1.3.0,3.4.3. find the position of feature value in a histogram
v1.3.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
v1.3.0,3.4.4. add the grad and hess to the corresponding bin
v1.3.0,3.4.5. add the reverse to the bin that contains 0.0f
v1.3.0,4. add the grad and hess sum to the zero bin of all features
v1.3.0,int startIdx = fid * 2 * splitNum;
v1.3.0,find the best split result of the histogram of a tree node
v1.3.0,1. calculate the gradStats of the root node
v1.3.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
v1.3.0,2. loop over features
v1.3.0,2.1. get the ture feature id in the sampled feature set
v1.3.0,2.2. get the indexes of histogram of this feature
v1.3.0,2.3. find the best split of current feature
v1.3.0,2.4. update the best split result if possible
v1.3.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.3.0,3. update the grad stats of children node
v1.3.0,3.1. update the left child
v1.3.0,3.2. update the right child
v1.3.0,find the best split result of one feature
v1.3.0,1. set the feature id
v1.3.0,2. create the best left stats and right stats
v1.3.0,3. the gain of the root node
v1.3.0,4. create the temp left and right grad stats
v1.3.0,5. loop over all the data in histogram
v1.3.0,5.1. get the grad and hess of current hist bin
v1.3.0,5.2. check whether we can split with current left hessian
v1.3.0,right = root - left
v1.3.0,5.3. check whether we can split with current right hessian
v1.3.0,5.4. calculate the current loss gain
v1.3.0,5.5. check whether we should update the split result with current loss gain
v1.3.0,split value = sketches[splitIdx+1]
v1.3.0,"5.6. if should update, also update the best left and right grad stats"
v1.3.0,6. set the best left and right grad stats
v1.3.0,partition number
v1.3.0,cols of each partition
v1.3.0,"// update the grad stats of the root node on PS, only called once by leader worker"
v1.3.0,if (this.nid == 0) {
v1.3.0,GradStats rootStats = new GradStats(splitEntry.leftGradStat);
v1.3.0,rootStats.add(splitEntry.rightGradStat);
v1.3.0,"this.controller.updateNodeGradStats(this.nid, rootStats);"
v1.3.0,}
v1.3.0,
v1.3.0,// 3. update the grad stats of children node
v1.3.0,if (splitEntry.fid != -1) {
v1.3.0,// 3.1. update the left child
v1.3.0,"this.controller.updateNodeGradStats(2 * this.nid + 1, splitEntry.leftGradStat);"
v1.3.0,// 3.2. update the right child
v1.3.0,"this.controller.updateNodeGradStats(2 * this.nid + 2, splitEntry.rightGradStat);"
v1.3.0,}
v1.3.0,1. calculate the total grad sum and hess sum
v1.3.0,2. create the grad stats of the node
v1.3.0,1. calculate the total grad sum and hess sum
v1.3.0,2. create the grad stats of the node
v1.3.0,1. calculate the total grad sum and hess sum
v1.3.0,2. create the grad stats of the node
v1.3.0,"loop all the possible split value, start from split[1], since the first item is the minimal"
v1.3.0,feature value
v1.3.0,find the best split result of the histogram of a tree node
v1.3.0,2.2. get the indexes of histogram of this feature
v1.3.0,2.3. find the best split of current feature
v1.3.0,2.4. update the best split result if possible
v1.3.0,find the best split result of one feature
v1.3.0,1. set the feature id
v1.3.0,splitEntry.setFid(fid);
v1.3.0,2. create the best left stats and right stats
v1.3.0,3. the gain of the root node
v1.3.0,4. create the temp left and right grad stats
v1.3.0,5. loop over all the data in histogram
v1.3.0,5.1. get the grad and hess of current hist bin
v1.3.0,5.2. check whether we can split with current left hessian
v1.3.0,right = root - left
v1.3.0,5.3. check whether we can split with current right hessian
v1.3.0,5.4. calculate the current loss gain
v1.3.0,5.5. check whether we should update the split result with current loss gain
v1.3.0,"5.6. if should update, also update the best left and right grad stats"
v1.3.0,6. set the best left and right grad stats
v1.3.0,find the best split result of a serve row on the PS
v1.3.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.3.0,2.2. get the start index in histogram of this feature
v1.3.0,2.3. find the best split of current feature
v1.3.0,2.4. update the best split result if possible
v1.3.0,"find the best split result of one feature from a server row, used by the PS"
v1.3.0,1. set the feature id
v1.3.0,2. create the best left stats and right stats
v1.3.0,3. the gain of the root node
v1.3.0,4. create the temp left and right grad stats
v1.3.0,5. loop over all the data in histogram
v1.3.0,5.1. get the grad and hess of current hist bin
v1.3.0,5.2. check whether we can split with current left hessian
v1.3.0,right = root - left
v1.3.0,5.3. check whether we can split with current right hessian
v1.3.0,5.4. calculate the current loss gain
v1.3.0,5.5. check whether we should update the split result with current loss gain
v1.3.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
v1.3.0,the task use index to find fvalue
v1.3.0,"5.6. if should update, also update the best left and right grad stats"
v1.3.0,6. set the best left and right grad stats
v1.3.0,clear all the information
v1.3.0,calculate the sum of gradient and hess
v1.3.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v1.3.0,ridx)
v1.3.0,check if necessary information is ready
v1.3.0,"same as add, reduce is used in All Reduce"
v1.3.0,"features used in this tree, if equals null, means use all the features without sampling"
v1.3.0,node in the tree
v1.3.0,the gradient info of each instances
v1.3.0,initialize nodes
v1.3.0,gradient
v1.3.0,second order gradient
v1.3.0,int sendStartCol = startFid * 7; // each split contains 7 doubles
v1.3.0,1. calculate the total grad sum and hess sum
v1.3.0,2. create the grad stats of the node
v1.3.0,find the best split result of a serve row on the PS
v1.3.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.3.0,2.2. get the start index in histogram of this feature
v1.3.0,2.3. find the best split of current feature
v1.3.0,2.4. update the best split result if possible
v1.3.0,"find the best split result of one feature from a server row, used by the PS"
v1.3.0,1. set the feature id
v1.3.0,2. create the best left stats and right stats
v1.3.0,3. the gain of the root node
v1.3.0,4. create the temp left and right grad stats
v1.3.0,5. loop over all the data in histogram
v1.3.0,5.1. get the grad and hess of current hist bin
v1.3.0,5.2. check whether we can split with current left hessian
v1.3.0,right = root - left
v1.3.0,5.3. check whether we can split with current right hessian
v1.3.0,5.4. calculate the current loss gain
v1.3.0,5.5. check whether we should update the split result with current loss gain
v1.3.0,"tips: here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
v1.3.0,the task use index to find fvalue
v1.3.0,"5.6. if should update, also update the best left and right grad stats"
v1.3.0,6. set the best left and right grad stats
v1.3.0,"System.out.println(""Indices: "" + Arrays.toString(indices));"
v1.3.0,t[i][code]++;
v1.3.0,else if (Math.random() > 0.5) {
v1.3.0,t[i][code] = freq;
v1.3.0,}
v1.3.0,"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);"
v1.3.0,"ret = Math.min(ret, t[i][h[i].encode(key)]);"
v1.3.0,"Get input path, output path"
v1.3.0,Init serde
v1.3.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.3.0,"Get input path, output path"
v1.3.0,Init serde
v1.3.0,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.3.0,"quantile sketch, size = featureNum * splitNum"
v1.3.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
v1.3.0,"active tree nodes, size = pow(2, treeDepth) -1"
v1.3.0,sampled features. size = treeNum * sampleRatio * featureNum
v1.3.0,"split features, size = treeNum * treeNodeNum"
v1.3.0,"split values, size = treeNum * treeNodeNum"
v1.3.0,"split gains, size = treeNum * treeNodeNum"
v1.3.0,"node weights, size = treeNum * treeNodeNum"
v1.3.0,"node preds, size = treeNum * treeNodeNum"
v1.3.0,if using PS to perform split
v1.3.0,step size for a tree
v1.3.0,number of class
v1.3.0,minimum loss change required for a split
v1.3.0,maximum depth of a tree
v1.3.0,number of features
v1.3.0,number of nonzero
v1.3.0,number of candidates split value
v1.3.0,----- the rest parameters are less important ----
v1.3.0,base instance weight
v1.3.0,minimum amount of hessian(weight) allowed in a child
v1.3.0,L2 regularization factor
v1.3.0,L1 regularization factor
v1.3.0,default direction choice
v1.3.0,maximum delta update we can add in weight estimation
v1.3.0,this parameter can be used to stabilize update
v1.3.0,default=0 means no constraint on weight delta
v1.3.0,whether we want to do subsample for row
v1.3.0,whether to subsample columns for each tree
v1.3.0,accuracy of sketch
v1.3.0,accuracy of sketch
v1.3.0,leaf vector size
v1.3.0,option for parallelization
v1.3.0,option to open cacheline optimization
v1.3.0,whether to not print info during training.
v1.3.0,"get feature type, 0:empty 1:all equal 2:real"
v1.3.0,maximum depth of the tree
v1.3.0,number of features used for tree construction
v1.3.0,"minimum loss change required for a split, otherwise stop split"
v1.3.0,----- the rest parameters are less important ----
v1.3.0,default direction choice
v1.3.0,whether we want to do sample data
v1.3.0,whether to sample columns during tree construction
v1.3.0,whether to use histogram for split
v1.3.0,number of histogram units
v1.3.0,whether to print info during training.
v1.3.0,----- the rest parameters are obtained after training ----
v1.3.0,total number of nodes
v1.3.0,number of deleted nodes */
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy data spliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,row 0 is a random uniform
v1.2.1,row 1 is a random normal
v1.2.1,row 2 is filled with 1.0
v1.2.1,in different part
v1.2.1,TODO Auto-generated constructor stub
v1.2.1,row 0 is a random uniform
v1.2.1,row 1 is a random normal
v1.2.1,row 2 is filled with 1.0
v1.2.1,find the max abs
v1.2.1,compress data
v1.2.1,import jdk.nashorn.internal.runtime.regexp.joni.Config;
v1.2.1,"paras[1] = ""abc"";"
v1.2.1,"paras[2] = ""123"";"
v1.2.1,Add standard Hadoop classes
v1.2.1,Feature number of train data
v1.2.1,Total iteration number
v1.2.1,Learning rate
v1.2.1,Regularization coefficient
v1.2.1,Set local deploy mode
v1.2.1,Set basic configuration keys
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,set sgd LR algorithm parameters #feature #epoch
v1.2.1,Set input data path
v1.2.1,Set save model path
v1.2.1,Set actionType train
v1.2.1,Feature number of train data
v1.2.1,Number of nonzero features
v1.2.1,Tree number
v1.2.1,Tree depth
v1.2.1,Split number
v1.2.1,Feature sample ratio
v1.2.1,Data format
v1.2.1,Learning rate
v1.2.1,Set basic configuration keys
v1.2.1,Use local deploy mode and dummy data spliter
v1.2.1,"set input, output path"
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,Set GBDT algorithm parameters
v1.2.1,Load Model from HDFS.
v1.2.1,Set local deploy mode
v1.2.1,Set basic configuration keys
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,"Set trainning data, and save model path"
v1.2.1,Set actionType train
v1.2.1,Set MF algorithm parameters
v1.2.1,Feature number of train data
v1.2.1,Total iteration number
v1.2.1,Validation sample Ratio
v1.2.1,"Data format, libsvm or dummy"
v1.2.1,Train batch number per epoch.
v1.2.1,Learning rate
v1.2.1,Decay of learning rate
v1.2.1,Regularization coefficient
v1.2.1,Set local deploy mode
v1.2.1,Set basic configuration keys
v1.2.1,Set data format
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,set sgd LR algorithm parameters #feature #epoch
v1.2.1,Set trainning data path
v1.2.1,Set save model path
v1.2.1,Set log path
v1.2.1,Set actionType train
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set save model path
v1.2.1,Set actionType incremental train
v1.2.1,Set log path
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set predict result path
v1.2.1,Set actionType prediction
v1.2.1,Feature number of train data
v1.2.1,Total iteration number
v1.2.1,Validation sample Ratio
v1.2.1,"Data format, libsvm or dummy"
v1.2.1,Train batch number per epoch.
v1.2.1,Batch number
v1.2.1,Learning rate
v1.2.1,Decay of learning rate
v1.2.1,Regularization coefficient
v1.2.1,Set local deploy mode
v1.2.1,Set basic configuration keys
v1.2.1,Set data format
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,set sgd LR algorithm parameters #feature #epoch
v1.2.1,Set trainning data path
v1.2.1,Set save model path
v1.2.1,Set log path
v1.2.1,Set actionType train
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set save model path
v1.2.1,Set log path
v1.2.1,Set actionType incremental train
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set predict result path
v1.2.1,Set log path
v1.2.1,Set actionType prediction
v1.2.1,Load model meta
v1.2.1,Convert model
v1.2.1,"Get input path, output path"
v1.2.1,Init serde
v1.2.1,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.2.1,Load model meta
v1.2.1,Convert model
v1.2.1,load hadoop configuration
v1.2.1,"Get input path, output path"
v1.2.1,Init serde
v1.2.1,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.2.1,Load model meta
v1.2.1,Check row type
v1.2.1,Load model
v1.2.1,Load model meta
v1.2.1,Check row type
v1.2.1,Load model
v1.2.1,Load model meta
v1.2.1,Check row type
v1.2.1,Load model
v1.2.1,Load model meta
v1.2.1,Check row type
v1.2.1,Load model
v1.2.1,Load model meta
v1.2.1,Check row type
v1.2.1,Load model
v1.2.1,Load model meta
v1.2.1,Check row type
v1.2.1,Load model
v1.2.1,Load model meta
v1.2.1,Check row type
v1.2.1,Load model
v1.2.1,Load model
v1.2.1,load hadoop configuration
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,worker register
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,add matrix
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,attempt 0
v1.2.1,attempt1
v1.2.1,attempt1
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,TODO Auto-generated constructor stub
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,set basic configuration keys
v1.2.1,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,Thread.sleep(5000);
v1.2.1,"response = master.getJobReport(null, request);"
v1.2.1,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
v1.2.1,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
v1.2.1,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,"DenseIntVector deltaVec = new DenseIntVector(100000, delta);"
v1.2.1,deltaVec.setMatrixId(matrixW1Id);
v1.2.1,deltaVec.setRowId(0);
v1.2.1,TODO Auto-generated constructor stub
v1.2.1,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
v1.2.1,@RunWith(MockitoJUnitRunner.class)
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,psAgent.initAndStart();
v1.2.1,test conf
v1.2.1,test master location
v1.2.1,test app id
v1.2.1,test user
v1.2.1,test ps agent attempt id
v1.2.1,test ps agent id
v1.2.1,test connection
v1.2.1,test master client
v1.2.1,test ip
v1.2.1,test loc
v1.2.1,test master location
v1.2.1,test ps location
v1.2.1,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
v1.2.1,test all ps ids
v1.2.1,test all matrix ids
v1.2.1,test all matrix names
v1.2.1,test matrix attribute
v1.2.1,test matrix meta
v1.2.1,test ps location
v1.2.1,test partitions
v1.2.1,"Note:[startRow,endRow)"
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,group0Id = new WorkerGroupId(0);
v1.2.1,"worker0Id = new WorkerId(group0Id, 0);"
v1.2.1,"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);"
v1.2.1,task0Id = new TaskId(0);
v1.2.1,task1Id = new TaskId(1);
v1.2.1,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.2.1,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.2.1,test this func in testWriteTo
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,"LOG.info(index[0] + "" "" + value[0]);"
v1.2.1,"LOG.info(index[1] + "" "" + value[1]);"
v1.2.1,"LOG.info(index[2] + "" "" + value[2]);"
v1.2.1,dot
v1.2.1,plus
v1.2.1,plusBy
v1.2.1,dot
v1.2.1,plus
v1.2.1,plusBy
v1.2.1,dot
v1.2.1,plus
v1.2.1,plusBy
v1.2.1,dot
v1.2.1,plusBy
v1.2.1,@Test
v1.2.1,public void dotDenseFloatVector() throws Exception {
v1.2.1,int dim = 1000;
v1.2.1,Random random = new Random(System.currentTimeMillis());
v1.2.1,
v1.2.1,double[] values = new double[dim];
v1.2.1,float[] values_1 = new float[dim];
v1.2.1,for (int i = 0; i < dim; i++) {
v1.2.1,values[i] = random.nextDouble();
v1.2.1,values_1[i] = random.nextFloat();
v1.2.1,}
v1.2.1,
v1.2.1,"DenseDoubleVector vec = new DenseDoubleVector(dim, values);"
v1.2.1,"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);"
v1.2.1,
v1.2.1,double sum = 0.0;
v1.2.1,for (int i = 0; i < dim; i++) {
v1.2.1,sum += values[i] * values_1[i];
v1.2.1,}
v1.2.1,
v1.2.1,"assertEquals(sum, vec.dot(vec_1));"
v1.2.1,
v1.2.1,}
v1.2.1,@Test
v1.2.1,public void plusDenseFlaotVector() throws Exception {
v1.2.1,"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};"
v1.2.1,"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};"
v1.2.1,"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);"
v1.2.1,"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);"
v1.2.1,
v1.2.1,TDoubleVector vec_2 = vec.plus(vec_1);
v1.2.1,for (int i = 0; i < vec.size(); i++)
v1.2.1,"assertEquals(value_1[i] + value_2[i], vec_2.get(i));"
v1.2.1,
v1.2.1,
v1.2.1,"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);"
v1.2.1,
v1.2.1,for (int i = 0; i < vec.size(); i++)
v1.2.1,"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);"
v1.2.1,
v1.2.1,double[] oldValues = vec.getValues().clone();
v1.2.1,
v1.2.1,vec.plusBy(vec_1);
v1.2.1,
v1.2.1,for (int i = 0; i < vec.size(); i++)
v1.2.1,"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));"
v1.2.1,
v1.2.1,oldValues = vec.getValues().clone();
v1.2.1,
v1.2.1,"vec.plusBy(vec_1, 3);"
v1.2.1,
v1.2.1,for (int i = 0; i < vec.size(); i++)
v1.2.1,"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));"
v1.2.1,}
v1.2.1,dot
v1.2.1,plus
v1.2.1,plusBy
v1.2.1,dot
v1.2.1,plus
v1.2.1,plusBy
v1.2.1,@Test
v1.2.1,public void plusBy3() throws Exception {
v1.2.1,"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};"
v1.2.1,"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);"
v1.2.1,"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});"
v1.2.1,vec.setRowId(0);
v1.2.1,"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});"
v1.2.1,vec_1.setRowId(1);
v1.2.1,TDoubleVector vec_2 = new SparseDoubleVector(2);
v1.2.1,"vec_2.set(1, 1.0);"
v1.2.1,vec_2.setRowId(0);
v1.2.1,
v1.2.1,mat.plusBy(vec);
v1.2.1,mat.plusBy(vec_1);
v1.2.1,mat.plusBy(vec_2);
v1.2.1,
v1.2.1,"assertEquals(2.0f, mat.get(0, 0));"
v1.2.1,"assertEquals(4.0f, mat.get(0, 1));"
v1.2.1,"assertEquals(4.0f, mat.get(1, 0));"
v1.2.1,"assertEquals(5.0f, mat.get(1, 1));"
v1.2.1,}
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,test worker getActiveTaskNum
v1.2.1,test worker getTaskNum
v1.2.1,test worker getTaskManager
v1.2.1,test workerId
v1.2.1,test workerAttemptId
v1.2.1,tet worker initFinished
v1.2.1,test worker getInitMinclock
v1.2.1,test worker loacation
v1.2.1,test AppId
v1.2.1,test Conf
v1.2.1,test UserName
v1.2.1,master location
v1.2.1,masterClient
v1.2.1,test psAgent
v1.2.1,test worker get dataBlockManager
v1.2.1,workerGroup.getSplits();
v1.2.1,application
v1.2.1,lcation
v1.2.1,workerGroup info
v1.2.1,worker info
v1.2.1,task
v1.2.1,Matrix parameters
v1.2.1,Set basic configuration keys
v1.2.1,Use local deploy mode and dummy data spliter
v1.2.1,Create an Angel client
v1.2.1,Add different types of matrix
v1.2.1,using mock object
v1.2.1,verification
v1.2.1,Stubbing
v1.2.1,Default does nothing.
v1.2.1,The app injection is optional
v1.2.1,"renderText(""hello world"");"
v1.2.1,"user choose a workerGroupID from the workergroups page,"
v1.2.1,now we should change the AngelApp params and render the workergroup page;
v1.2.1,"static final String WORKER_ID = ""worker.id"";"
v1.2.1,"div(""#logo"")."
v1.2.1,"img(""/static/hadoop-st.png"")._()."
v1.2.1,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
v1.2.1,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
v1.2.1,JQueryUI.jsnotice(html);
v1.2.1,import org.apache.hadoop.conf.Configuration;
v1.2.1,import java.lang.reflect.Field;
v1.2.1,get block locations from file system
v1.2.1,create a list of all block and their locations
v1.2.1,"if the file is not splitable, just create the one block with"
v1.2.1,full file length
v1.2.1,each split can be a maximum of maxSize
v1.2.1,if remainder is between max and 2*max - then
v1.2.1,"instead of creating splits of size max, left-max we"
v1.2.1,create splits of size left/2 and left/2. This is
v1.2.1,a heuristic to avoid creating really really small
v1.2.1,splits.
v1.2.1,add this block to the block --> node locations map
v1.2.1,"For blocks that do not have host/rack information,"
v1.2.1,assign to default  rack.
v1.2.1,add this block to the rack --> block map
v1.2.1,Add this host to rackToNodes map
v1.2.1,add this block to the node --> block map
v1.2.1,"if the file system does not have any rack information, then"
v1.2.1,use dummy rack location.
v1.2.1,The topology paths have the host name included as the last
v1.2.1,component. Strip it.
v1.2.1,get tokens for all the required FileSystems..
v1.2.1,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
v1.2.1,job.getConfiguration());
v1.2.1,Whether we need to recursive look into the directory structure
v1.2.1,creates a MultiPathFilter with the hiddenFileFilter and the
v1.2.1,user provided one (if any).
v1.2.1,all the files in input set
v1.2.1,it is allowed for maxSize to be 0. Disable smoothing load for such cases
v1.2.1,process all nodes and create splits that are local to a node. Generate
v1.2.1,"one split per node iteration, and walk over nodes multiple times to"
v1.2.1,distribute the splits across nodes.
v1.2.1,Skip the node if it has previously been marked as completed.
v1.2.1,"for each block, copy it into validBlocks. Delete it from"
v1.2.1,blockToNodes so that the same block does not appear in
v1.2.1,two different splits.
v1.2.1,Remove all blocks which may already have been assigned to other
v1.2.1,splits.
v1.2.1,"if the accumulated split size exceeds the maximum, then"
v1.2.1,create this split.
v1.2.1,create an input split and add it to the splits array
v1.2.1,Remove entries from blocksInNode so that we don't walk these
v1.2.1,again.
v1.2.1,Done creating a single split for this node. Move on to the next
v1.2.1,node so that splits are distributed across nodes.
v1.2.1,This implies that the last few blocks (or all in case maxSize=0)
v1.2.1,were not part of a split. The node is complete.
v1.2.1,if there were any blocks left over and their combined size is
v1.2.1,"larger than minSplitNode, then combine them into one split."
v1.2.1,Otherwise add them back to the unprocessed pool. It is likely
v1.2.1,that they will be combined with other blocks from the
v1.2.1,same rack later on.
v1.2.1,This condition also kicks in when max split size is not set. All
v1.2.1,blocks on a node will be grouped together into a single split.
v1.2.1,haven't created any split on this machine. so its ok to add a
v1.2.1,smaller one for parallelism. Otherwise group it in the rack for
v1.2.1,balanced size create an input split and add it to the splits
v1.2.1,array
v1.2.1,Remove entries from blocksInNode so that we don't walk this again.
v1.2.1,The node is done. This was the last set of blocks for this node.
v1.2.1,Put the unplaced blocks back into the pool for later rack-allocation.
v1.2.1,Node is done. All blocks were fit into node-local splits.
v1.2.1,Check if node-local assignments are complete.
v1.2.1,All nodes have been walked over and marked as completed or all blocks
v1.2.1,have been assigned. The rest should be handled via rackLock assignment.
v1.2.1,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
v1.2.1,"+ completedNodes.size() + "", size left: "" + totalLength);"
v1.2.1,"if blocks in a rack are below the specified minimum size, then keep them"
v1.2.1,"in 'overflow'. After the processing of all racks is complete, these"
v1.2.1,overflow blocks will be combined into splits.
v1.2.1,Process all racks over and over again until there is no more work to do.
v1.2.1,Create one split for this rack before moving over to the next rack.
v1.2.1,Come back to this rack after creating a single split for each of the
v1.2.1,remaining racks.
v1.2.1,"Process one rack location at a time, Combine all possible blocks that"
v1.2.1,reside on this rack as one split. (constrained by minimum and maximum
v1.2.1,split size).
v1.2.1,iterate over all racks
v1.2.1,"for each block, copy it into validBlocks. Delete it from"
v1.2.1,blockToNodes so that the same block does not appear in
v1.2.1,two different splits.
v1.2.1,"if the accumulated split size exceeds the maximum, then"
v1.2.1,create this split.
v1.2.1,create an input split and add it to the splits array
v1.2.1,"if we created a split, then just go to the next rack"
v1.2.1,"if there is a minimum size specified, then create a single split"
v1.2.1,"otherwise, store these blocks into overflow data structure"
v1.2.1,There were a few blocks in this rack that
v1.2.1,remained to be processed. Keep them in 'overflow' block list.
v1.2.1,These will be combined later.
v1.2.1,Process all overflow blocks
v1.2.1,"This might cause an exiting rack location to be re-added,"
v1.2.1,but it should be ok.
v1.2.1,"if the accumulated split size exceeds the maximum, then"
v1.2.1,create this split.
v1.2.1,create an input split and add it to the splits array
v1.2.1,"Process any remaining blocks, if any."
v1.2.1,create an input split
v1.2.1,add this split to the list that is returned
v1.2.1,long num = totLength / maxSize;
v1.2.1,all blocks for all the files in input set
v1.2.1,mapping from a rack name to the list of blocks it has
v1.2.1,mapping from a block to the nodes on which it has replicas
v1.2.1,mapping from a node to the list of blocks that it contains
v1.2.1,populate all the blocks for all files
v1.2.1,stop all services
v1.2.1,1.write application state to file so that the client can get the state of the application
v1.2.1,if master exit
v1.2.1,2.clear tmp and staging directory
v1.2.1,waiting for client to get application state
v1.2.1,stop the RPC server
v1.2.1,"Security framework already loaded the tokens into current UGI, just use"
v1.2.1,them
v1.2.1,Now remove the AM->RM token so tasks don't have it
v1.2.1,add a shutdown hook
v1.2.1,init app state storage
v1.2.1,init event dispacher
v1.2.1,init location manager
v1.2.1,init container allocator
v1.2.1,init a rpc service
v1.2.1,recover matrix meta if needed
v1.2.1,recover ps attempt information if need
v1.2.1,init parameter server manager
v1.2.1,recover task information if needed
v1.2.1,init psagent manager and register psagent manager event
v1.2.1,a dummy data spliter is just for test now
v1.2.1,recover data splits information if needed
v1.2.1,init worker manager and register worker manager event
v1.2.1,register slow worker/ps checker
v1.2.1,register app manager event and finish event
v1.2.1,start a web service if use yarn deploy mode
v1.2.1,load from app state storage first if attempt index great than 1(the master is not the first
v1.2.1,retry)
v1.2.1,"if load failed, just build a new MatrixMetaManager"
v1.2.1,load ps attempt index from app state storage first if attempt index great than 1(the master
v1.2.1,is not the first retry)
v1.2.1,load task information from app state storage first if attempt index great than 1(the master
v1.2.1,is not the first retry)
v1.2.1,"if load failed, just build a new AMTaskManager"
v1.2.1,load data splits information from app state storage first if attempt index great than 1(the
v1.2.1,master is not the first retry)
v1.2.1,"if load failed, we need to recalculate the data splits"
v1.2.1,parse parameter server counters
v1.2.1,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.2.1,refresh last heartbeat timestamp
v1.2.1,send a state update event to the specific PSAttempt
v1.2.1,check if parameter server can commit now.
v1.2.1,check matrix metadata inconsistencies between master and parameter server.
v1.2.1,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
v1.2.1,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
v1.2.1,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.2.1,check whether psagent heartbeat timeout
v1.2.1,check whether parameter server heartbeat timeout
v1.2.1,check whether worker heartbeat timeout
v1.2.1,choose a unused port
v1.2.1,start RPC server
v1.2.1,find matrix partitions from master matrix meta manager for this parameter server
v1.2.1,remove this parameter server attempt from monitor set
v1.2.1,remove this parameter server attempt from monitor set
v1.2.1,"if worker attempt id is not in monitor set, we should shutdown it"
v1.2.1,find workergroup in worker manager
v1.2.1,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
v1.2.1,"if this worker group run over, just return WORKERGROUP_EXITED"
v1.2.1,"if this worker group is running now, return tasks, workers, data splits for it"
v1.2.1,"if worker attempt id is not in monitor set, we should shutdown it"
v1.2.1,"if worker attempt id is not in monitor set, we should shutdown it"
v1.2.1,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.2.1,"in ANGEL_PS mode, task id may can not know advance"
v1.2.1,update the clock for this matrix
v1.2.1,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.2.1,"in ANGEL_PS mode, task id may can not know advance"
v1.2.1,update task iteration
v1.2.1,private boolean matrixInited;
v1.2.1,"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.2.1,dispatch matrix partitions to parameter servers
v1.2.1,update matrix id generator
v1.2.1,"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.2.1,dispatch matrix partitions to parameter servers
v1.2.1,get matrix ids in the parameter server report
v1.2.1,get the matrices parameter server need to create and delete
v1.2.1,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
v1.2.1,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
v1.2.1,waitForMatrixReleaseOnPS(matrixId);
v1.2.1,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
v1.2.1,the number of splits equal to the number of tasks
v1.2.1,split data
v1.2.1,dispatch the splits to workergroups
v1.2.1,split data
v1.2.1,dispatch the splits to workergroups
v1.2.1,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.2.1,"first, then divided by expected split number"
v1.2.1,get input format class from configuration and then instantiation a input format object
v1.2.1,split data
v1.2.1,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.2.1,"first, then divided by expected split number"
v1.2.1,get input format class from configuration and then instantiation a input format object
v1.2.1,split data
v1.2.1,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.2.1,need to fine tune the number of workergroup and task based on the actual split number
v1.2.1,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.2.1,Record the location information for the splits in order to data localized schedule
v1.2.1,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.2.1,need to fine tune the number of workergroup and task based on the actual split number
v1.2.1,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.2.1,Record the location information for the splits in order to data localized schedule
v1.2.1,write meta data to a temporary file
v1.2.1,rename the temporary file to final file
v1.2.1,"if the file exists, read from file and deserialize it"
v1.2.1,write task meta
v1.2.1,write ps meta
v1.2.1,generate a temporary file
v1.2.1,write task meta to the temporary file first
v1.2.1,rename the temporary file to the final file
v1.2.1,"if last final task file exist, remove it"
v1.2.1,find task meta file which has max timestamp
v1.2.1,"if the file does not exist, just return null"
v1.2.1,read task meta from file and deserialize it
v1.2.1,generate a temporary file
v1.2.1,write ps meta to the temporary file first.
v1.2.1,rename the temporary file to the final file
v1.2.1,"if the old final file exist, just remove it"
v1.2.1,find ps meta file
v1.2.1,"if ps meta file does not exist, just return null"
v1.2.1,read ps meta from file and deserialize it
v1.2.1,Init matrix files meta
v1.2.1,Move output files
v1.2.1,Write the meta file
v1.2.1,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.2.1,Transitions from the PSAttemptStateInternal.KILLED state
v1.2.1,Transitions from the PSAttemptStateInternal.FAILED state
v1.2.1,create the topology tables
v1.2.1,Transitions from the NEW state.
v1.2.1,PA_FAILMSG
v1.2.1,Transitions from the UNASSIGNED state.
v1.2.1,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG"
v1.2.1,event
v1.2.1,Transitions from the ASSIGNED state.
v1.2.1,"this happened when launch thread run slowly, and PA_REGISTER event"
v1.2.1,dispatched before PA_CONTAINER_LAUNCHED event
v1.2.1,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.2.1,Transitions from the PSAttemptStateInternal.KILLED state
v1.2.1,Transitions from the PSAttemptStateInternal.FAILED state
v1.2.1,create the topology tables
v1.2.1,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will"
v1.2.1,retry another attempt or failed
v1.2.1,release container
v1.2.1,TODO
v1.2.1,set the launch time
v1.2.1,"set tarckerName,httpPort, which used by webserver"
v1.2.1,added to psManager so psManager can monitor it;
v1.2.1,psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);
v1.2.1,set the finish time only if launch time is set
v1.2.1,"ParameterServerJVM.setVMEnv(myEnv, conf);"
v1.2.1,Set up the launch command
v1.2.1,Duplicate the ByteBuffers for access by multiple containers.
v1.2.1,Construct the actual Container
v1.2.1,Application resources
v1.2.1,Application environment
v1.2.1,Service data
v1.2.1,Tokens
v1.2.1,Set up JobConf to be localized properly on the remote NM.
v1.2.1,Setup DistributedCache
v1.2.1,Setup up task credentials buffer
v1.2.1,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.2.1,Add the env variables passed by the user
v1.2.1,Set logging level in the environment.
v1.2.1,"This is so that, if the child forks another ""bin/hadoop"" (common in"
v1.2.1,streaming) it will have the correct loglevel.
v1.2.1,Setup the log4j prop
v1.2.1,Add main class and its arguments
v1.2.1,Finally add the jvmID
v1.2.1,vargs.add(String.valueOf(jvmID.getId()));
v1.2.1,Final commmand
v1.2.1,Transitions from the NEW state.
v1.2.1,Transitions from the RUNNING state.
v1.2.1,Transitions from the SUCCEEDED state
v1.2.1,Transitions from the KILLED state
v1.2.1,Transitions from the FAILED state
v1.2.1,Transitions from the NEW state.
v1.2.1,Transitions from the SCHEDULED state.
v1.2.1,Transitions from the RUNNING state.
v1.2.1,"another attempt launched,"
v1.2.1,Transitions from the SUCCEEDED state
v1.2.1,Transitions from the KILLED state
v1.2.1,Transitions from the FAILED state
v1.2.1,add diagnostic
v1.2.1,Set up the launch command
v1.2.1,Duplicate the ByteBuffers for access by multiple containers.
v1.2.1,Construct the actual Container
v1.2.1,Application resources
v1.2.1,Application environment
v1.2.1,Service data
v1.2.1,Tokens
v1.2.1,Set up JobConf to be localized properly on the remote NM.
v1.2.1,Setup DistributedCache
v1.2.1,Setup up task credentials buffer
v1.2.1,LocalStorageToken is needed irrespective of whether security is enabled
v1.2.1,or not.
v1.2.1,"TokenCache.setJobToken(jobToken, taskCredentials);"
v1.2.1,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.2.1,Construct the actual Container
v1.2.1,The null fields are per-container and will be constructed for each
v1.2.1,container separately.
v1.2.1,Set up the launch command
v1.2.1,Duplicate the ByteBuffers for access by multiple containers.
v1.2.1,Construct the actual Container
v1.2.1,"a * in the classpath will only find a .jar, so we need to filter out"
v1.2.1,all .jars and add everything else
v1.2.1,Propagate the system classpath when using the mini cluster
v1.2.1,Add standard Hadoop classes
v1.2.1,Cache archives
v1.2.1,Cache files
v1.2.1,Sanity check
v1.2.1,Add URI fragment or just the filename
v1.2.1,Add the env variables passed by the user
v1.2.1,Set logging level in the environment.
v1.2.1,Setup the log4j prop
v1.2.1,Add main class and its arguments
v1.2.1,Finally add the jvmID
v1.2.1,vargs.add(String.valueOf(jvmID.getId()));
v1.2.1,Final commmand
v1.2.1,Add the env variables passed by the user
v1.2.1,Set logging level in the environment.
v1.2.1,Setup the log4j prop
v1.2.1,Add main class and its arguments
v1.2.1,Final commmand
v1.2.1,"if amTask is not null, we should clone task state from it"
v1.2.1,"if all parameter server complete commit, master can commit now"
v1.2.1,init and start master committer
v1.2.1,Transitions from the NEW state.
v1.2.1,Transitions from the UNASSIGNED state.
v1.2.1,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
v1.2.1,Transitions from the ASSIGNED state.
v1.2.1,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
v1.2.1,PA_CONTAINER_LAUNCHED event
v1.2.1,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.2.1,Transitions from the PSAttemptStateInternal.COMMITTING state
v1.2.1,Transitions from the PSAttemptStateInternal.KILLED state
v1.2.1,Transitions from the PSAttemptStateInternal.FAILED state
v1.2.1,create the topology tables
v1.2.1,reqeuest resource:send a resource request to the resource allocator
v1.2.1,"Once the resource is applied, build and send the launch request to the container launcher"
v1.2.1,deallocator the resource of the ps attempt:send a resource deallocator request to the
v1.2.1,resource allocator
v1.2.1,set the launch time
v1.2.1,add the ps attempt to the heartbeat timeout monitoring list
v1.2.1,parse ps attempt location and put it to location manager
v1.2.1,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
v1.2.1,or failed
v1.2.1,remove ps attempt id from heartbeat timeout monitor list
v1.2.1,release container:send a release request to container launcher
v1.2.1,set the finish time only if launch time is set
v1.2.1,private long scheduledTime;
v1.2.1,Transitions from the NEW state.
v1.2.1,Transitions from the SCHEDULED state.
v1.2.1,Transitions from the RUNNING state.
v1.2.1,"another attempt launched,"
v1.2.1,Transitions from the SUCCEEDED state
v1.2.1,Transitions from the KILLED state
v1.2.1,Transitions from the FAILED state
v1.2.1,add diagnostic
v1.2.1,check whether the number of failed attempts is less than the maximum number of allowed
v1.2.1,start a new attempt for this ps
v1.2.1,notify ps manager
v1.2.1,add diagnostic
v1.2.1,check whether the number of failed attempts is less than the maximum number of allowed
v1.2.1,start a new attempt for this ps
v1.2.1,notify ps manager
v1.2.1,notify the event handler of state change
v1.2.1,"If new state is not RUNNING and COMMITTING, add it to state timeout monitor"
v1.2.1,"if forcedState is set, just return"
v1.2.1,else get state from state machine
v1.2.1,add this worker group to the success set
v1.2.1,check if all worker group run over
v1.2.1,add this worker group to the failed set
v1.2.1,check if too many worker groups are failed or killed
v1.2.1,notify a run failed event
v1.2.1,add this worker group to the failed set
v1.2.1,check if too many worker groups are failed or killed
v1.2.1,notify a run failed event
v1.2.1,calculate the actual number of worker groups and the total number of tasks based on the number of data split
v1.2.1,"init all tasks , workers and worker groups and put them to the corresponding maps"
v1.2.1,just return the total task number now
v1.2.1,TODO
v1.2.1,"if workerAttempt is not null, we should clone task state from it"
v1.2.1,from NEW state
v1.2.1,from SCHEDULED state
v1.2.1,get data splits location for data locality
v1.2.1,reqeuest resource:send a resource request to the resource allocator
v1.2.1,"once the resource is applied, build and send the launch request to the container launcher"
v1.2.1,notify failed message to the worker
v1.2.1,notify killed message to the worker
v1.2.1,release the allocated container
v1.2.1,notify failed message to the worker
v1.2.1,remove the worker attempt from heartbeat timeout listen list
v1.2.1,release the allocated container
v1.2.1,notify killed message to the worker
v1.2.1,remove the worker attempt from heartbeat timeout listen list
v1.2.1,clean the container
v1.2.1,notify failed message to the worker
v1.2.1,remove the worker attempt from heartbeat timeout listen list
v1.2.1,record the finish time
v1.2.1,clean the container
v1.2.1,notify killed message to the worker
v1.2.1,remove the worker attempt from heartbeat timeout listening list
v1.2.1,record the finish time
v1.2.1,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
v1.2.1,set worker attempt location
v1.2.1,notify the register message to the worker
v1.2.1,record the launch time
v1.2.1,update worker attempt metrics
v1.2.1,update tasks metrics
v1.2.1,clean the container
v1.2.1,notify the worker attempt run successfully message to the worker
v1.2.1,record the finish time
v1.2.1,init a worker attempt for the worker
v1.2.1,schedule the worker attempt
v1.2.1,add diagnostic
v1.2.1,check whether the number of failed attempts is less than the maximum number of allowed
v1.2.1,init and start a new attempt for this ps
v1.2.1,notify worker manager
v1.2.1,add diagnostic
v1.2.1,check whether the number of failed attempts is less than the maximum number of allowed
v1.2.1,init and start a new attempt for this ps
v1.2.1,notify worker manager
v1.2.1,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
v1.2.1,register to Yarn RM
v1.2.1,send heartbeat to Yarn RM every rmPollInterval milliseconds
v1.2.1,"catch YarnRuntimeException, we should exit and need not retry"
v1.2.1,build heartbeat request
v1.2.1,send heartbeat request to rm
v1.2.1,"This can happen if the RM has been restarted. If it is in that state,"
v1.2.1,this application must clean itself up.
v1.2.1,Setting NMTokens
v1.2.1,assgin containers
v1.2.1,"if some container is not assigned, release them"
v1.2.1,handle finish containers
v1.2.1,dispatch container exit message to corresponding components
v1.2.1,killed by framework
v1.2.1,killed by framework
v1.2.1,killed by framework
v1.2.1,get application finish state
v1.2.1,build application diagnostics
v1.2.1,TODO:add a job history for angel
v1.2.1,build unregister request
v1.2.1,send unregister request to rm
v1.2.1,Note this down for next interaction with ResourceManager
v1.2.1,based on blacklisting comments above we can end up decrementing more
v1.2.1,than requested. so guard for that.
v1.2.1,send the updated resource request to RM
v1.2.1,send 0 container count requests also to cancel previous requests
v1.2.1,Update resource requests
v1.2.1,try to assign to all nodes first to match node local
v1.2.1,try to match all rack local
v1.2.1,assign remaining
v1.2.1,Update resource requests
v1.2.1,send the container-assigned event to task attempt
v1.2.1,build the start container request use launch context
v1.2.1,send the start request to Yarn nm
v1.2.1,send the message that the container starts successfully to the corresponding component
v1.2.1,"after launching, send launched event to task attempt to move"
v1.2.1,it from ASSIGNED to RUNNING state
v1.2.1,send the message that the container starts failed to the corresponding component
v1.2.1,kill the remote container if already launched
v1.2.1,start a thread pool to startup the container
v1.2.1,See if we need up the pool size only if haven't reached the
v1.2.1,maximum limit yet.
v1.2.1,nodes where containers will run at *this* point of time. This is
v1.2.1,*not* the cluster size and doesn't need to be.
v1.2.1,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
v1.2.1,later is just a buffer so we are not always increasing the
v1.2.1,pool-size
v1.2.1,the events from the queue are handled in parallel
v1.2.1,using a thread pool
v1.2.1,return if already stopped
v1.2.1,shutdown any containers that might be left running
v1.2.1,Build and initialize rpc client to master
v1.2.1,Build local location
v1.2.1,"Initialize matrix info, this method will wait until master accepts the information from"
v1.2.1,client
v1.2.1,Get ps locations from master and put them to the location cache.
v1.2.1,Initialize matrix meta information
v1.2.1,Start heartbeat thread if need
v1.2.1,Start all services
v1.2.1,Register to master first
v1.2.1,Report state to master every specified time
v1.2.1,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.2.1,Stop all modules
v1.2.1,Exit the process if on ANGEL_PS_PSAGENT mode
v1.2.1,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.2.1,Stop all modules
v1.2.1,Exit the process if on ANGEL_PS_PSAGENT mode
v1.2.1,get configuration from config file
v1.2.1,set localDir with enviroment set by nm.
v1.2.1,Update generic resource counters
v1.2.1,Updating resources specified in ResourceCalculatorProcessTree
v1.2.1,Remove the CPU time consumed previously by JVM reuse
v1.2.1,array stores clock for each row and clock
v1.2.1,local task num
v1.2.1,mapping from task index to taskId
v1.2.1,mapping from taskId to task index
v1.2.1,TODO Auto-generated method stub
v1.2.1,Generate a flush request and put it to request queue
v1.2.1,Generate a clock request and put it to request queue
v1.2.1,Generate a merge request and put it to request queue
v1.2.1,"If the matrix op log cache does not exist for the matrix, create a new one for the"
v1.2.1,matrix
v1.2.1,and add it to cache maps
v1.2.1,Add the message to the tree map
v1.2.1,"If there are flush / clock requests blocked, we need to put this merge request into"
v1.2.1,the waiting queue
v1.2.1,Launch a merge worker to merge the update to matrix op log cache
v1.2.1,Remove the message from the tree map
v1.2.1,Wake up blocked flush/clock request
v1.2.1,Add flush/clock request to listener list to waiting for all the existing
v1.2.1,updates are merged
v1.2.1,Wake up blocked flush/clock request
v1.2.1,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
v1.2.1,blocked.
v1.2.1,Get next merge message sequence id
v1.2.1,Wake up listeners(flush/clock requests) that have little sequence id than current merge
v1.2.1,position
v1.2.1,Wake up blocked merge requests
v1.2.1,Get minimal sequence id from listeners
v1.2.1,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
v1.2.1,should flush updates to local matrix storage
v1.2.1,unused now
v1.2.1,Get partitions for the matrix
v1.2.1,"Filter it, removing zero values"
v1.2.1,Doing average or not
v1.2.1,Split this row according the matrix partitions
v1.2.1,Add the splits to the result container
v1.2.1,"For each partition, we generate a update split."
v1.2.1,"Although the split is empty for partitions those without any update data,"
v1.2.1,we still need to generate a update split to update the clock info on ps.
v1.2.1,"For each partition, we generate a update split."
v1.2.1,"Although the split is empty for partitions those without any update data,"
v1.2.1,we still need to generate a update split to update the clock info on ps.
v1.2.1,int seqId = ((ByteBuf) msg).readInt();
v1.2.1,"LOG.info(""receive result of seqId="" + seqId);"
v1.2.1,((ByteBuf) msg).resetReaderIndex();
v1.2.1,TODO: use Epoll for linux future
v1.2.1,closeChannelForServer(request.getServerId());
v1.2.1,closeChannelForServer(request.getServerId());
v1.2.1,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.2.1,Then submit normal task until reach upper limit of flow control or all tasks are submit
v1.2.1,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.2.1,"LOG.info(""choose put server "" + psIds[index]);"
v1.2.1,allocate the bytebuf
v1.2.1,"check the location of server is ready, if not, we should wait"
v1.2.1,get a channel to server from pool
v1.2.1,"if channel is not valid, it means maybe the connections to the server are closed"
v1.2.1,channelManager.removeChannelPool(loc);
v1.2.1,find the partition request context from cache
v1.2.1,Check if the result of the sub-request is received
v1.2.1,Update received result number
v1.2.1,Get row splits received
v1.2.1,Put the row split to the cache(row index to row splits map)
v1.2.1,"If all splits of the row are received, means this row can be merged"
v1.2.1,TODO Auto-generated method stub
v1.2.1,"Check futures, if the result of a sub-request is received, put it to the result queue"
v1.2.1,Now we just support pipelined row splits merging for dense type row
v1.2.1,Wait until the clock value of this row is greater than or equal to the value
v1.2.1,Get partitions for this row
v1.2.1,First get this row from matrix storage
v1.2.1,"If the row exists in the matrix storage and the clock value meets the requirements, just"
v1.2.1,return
v1.2.1,Get row splits of this row from the matrix cache first
v1.2.1,"If the row split does not exist in cache, get it from parameter server"
v1.2.1,Wait the final result
v1.2.1,Put it to the matrix cache
v1.2.1,Split the matrix oplog according to the matrix partitions
v1.2.1,"If need update clock, we should send requests to all partitions"
v1.2.1,use update index if exist
v1.2.1,Filter the rowIds which are fetching now
v1.2.1,Send the rowIndex to rpc dispatcher and return immediately
v1.2.1,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
v1.2.1,Generate dispatch items and add them to the corresponding queues
v1.2.1,Pre-fetching is disable default
v1.2.1,matrix id to clock map
v1.2.1,"task index, it must be unique for whole application"
v1.2.1,if (matrixManager.getMatrixMeta(createResponse.getMatrixId()) != null) {
v1.2.1,return matrixManager.getMatrixMeta(createResponse.getMatrixId());
v1.2.1,}
v1.2.1,Deserialize data splits meta
v1.2.1,Get workers
v1.2.1,Send request to every ps
v1.2.1,Wait the responses
v1.2.1,Update clock cache
v1.2.1,Get row from cache.
v1.2.1,"if row clock is satisfy ssp staleness limit, just return."
v1.2.1,Get row from ps.
v1.2.1,"For ASYNC mode, just get from pss."
v1.2.1,"For BSP/SSP, get rows from storage/cache first"
v1.2.1,Get from ps.
v1.2.1,"For ASYNC, just get rows from pss."
v1.2.1,no more retries.
v1.2.1,calculate sleep time and return.
v1.2.1,parse the i-th sleep-time
v1.2.1,parse the i-th number-of-retries
v1.2.1,calculateSleepTime may overflow.
v1.2.1,"A few common retry policies, with no delays."
v1.2.1,close is a local operation and should finish within milliseconds; timeout just to be safe
v1.2.1,response will be null for one way messages.
v1.2.1,maxFrameLength = 2G
v1.2.1,lengthFieldOffset = 0
v1.2.1,lengthFieldLength = 8
v1.2.1,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
v1.2.1,"initialBytesToStrip = 8, i.e. strip out the length field itself"
v1.2.1,indicates whether this connection's life cycle is managed
v1.2.1,See if we already have a connection (common case)
v1.2.1,create a unique lock for this RS + protocol (if necessary)
v1.2.1,get the RS lock
v1.2.1,do one more lookup in case we were stalled above
v1.2.1,Only create isa when we need to.
v1.2.1,definitely a cache miss. establish an RPC for
v1.2.1,this RS
v1.2.1,Throw what the RemoteException was carrying.
v1.2.1,check
v1.2.1,every
v1.2.1,minutes
v1.2.1,TODO
v1.2.1,创建failoverHandler
v1.2.1,"The number of times this invocation handler has ever been failed over,"
v1.2.1,before this method invocation attempt. Used to prevent concurrent
v1.2.1,failed method invocations from triggering multiple failover attempts.
v1.2.1,Make sure that concurrent failed method invocations
v1.2.1,only cause a
v1.2.1,single actual fail over.
v1.2.1,RpcController + Message in the method args
v1.2.1,(generated code from RPC bits in .proto files have
v1.2.1,RpcController)
v1.2.1,"LOG.info(""method "" + method.getName() + ""construct request time = """
v1.2.1,+ (System.currentTimeMillis() - beforeConstructTs));
v1.2.1,get an instance of the method arg type
v1.2.1,RpcController + Message in the method args
v1.2.1,(generated code from RPC bits in .proto files have
v1.2.1,RpcController)
v1.2.1,Message (hand written code usually has only a single
v1.2.1,argument)
v1.2.1,log any RPC responses that are slower than the configured
v1.2.1,warn
v1.2.1,response time or larger than configured warning size
v1.2.1,"when tagging, we let TooLarge trump TooSmall to keep"
v1.2.1,output simple
v1.2.1,note that large responses will often also be slow.
v1.2.1,provides a count of log-reported slow responses
v1.2.1,RpcController + Message in the method args
v1.2.1,(generated code from RPC bits in .proto files have
v1.2.1,RpcController)
v1.2.1,unexpected
v1.2.1,"in the protobuf methods, args[1] is the only significant argument"
v1.2.1,for JSON encoding
v1.2.1,base information that is reported regardless of type of call
v1.2.1,Disable Nagle's Algorithm since we don't want packets to wait
v1.2.1,Configure the event pipeline factory.
v1.2.1,Make a new connection.
v1.2.1,Remove all pending requests (will be canceled after relinquishing
v1.2.1,write lock).
v1.2.1,Cancel any pending requests by sending errors to the callbacks:
v1.2.1,Close the channel:
v1.2.1,Close the connection:
v1.2.1,Shut down all thread pools to exit.
v1.2.1,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
v1.2.1,See NettyServer.prepareResponse for where we write out the response.
v1.2.1,"It writes the call.id (int), a boolean signifying any error (and if"
v1.2.1,"so the exception name/trace), and the response bytes"
v1.2.1,Read the call id.
v1.2.1,"When the stream is closed, protobuf doesn't raise an EOFException,"
v1.2.1,"instead, it returns a null message object."
v1.2.1,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
v1.2.1,System.currentTimeMillis());
v1.2.1,"It would be good widen this to just Throwable, but IOException is what we"
v1.2.1,allow now
v1.2.1,not implemented
v1.2.1,not implemented
v1.2.1,"track what RpcEngine is used by a proxy class, for stopProxy()"
v1.2.1,cache of RpcEngines by protocol
v1.2.1,return the RpcEngine configured to handle a protocol
v1.2.1,We only handle the ConnectException.
v1.2.1,This is the exception we can't handle.
v1.2.1,check if timed out
v1.2.1,wait for retry
v1.2.1,IGNORE
v1.2.1,return the RpcEngine that handles a proxy object
v1.2.1,The default implementation works synchronously
v1.2.1,punt: allocate a new buffer & copy into it
v1.2.1,Parse cmd parameters
v1.2.1,load hadoop configuration
v1.2.1,load angel system configuration
v1.2.1,load user configuration:
v1.2.1,load user config file
v1.2.1,load command line parameters
v1.2.1,load user job resource files
v1.2.1,load user job jar if it exist
v1.2.1,Expand the environment variable
v1.2.1,Add default fs(local fs) for lib jars.
v1.2.1,"LOG.info(System.getProperty(""user.dir""));"
v1.2.1,get tokens for all the required FileSystems..
v1.2.1,Whether we need to recursive look into the directory structure
v1.2.1,creates a MultiPathFilter with the hiddenFileFilter and the
v1.2.1,user provided one (if any).
v1.2.1,"LOG.info(""Total input paths to process : "" + result.size());"
v1.2.1,get tokens for all the required FileSystems..
v1.2.1,Whether we need to recursive look into the directory structure
v1.2.1,creates a MultiPathFilter with the hiddenFileFilter and the
v1.2.1,user provided one (if any).
v1.2.1,"LOG.info(""Total input paths to process : "" + result.size());"
v1.2.1,a simple hdfs copy function assume src path and dest path are in same hdfs
v1.2.1,and FileSystem object has same schema
v1.2.1,"LOG.warn(""interrupted while sleeping"", ie);"
v1.2.1,private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);
v1.2.1,public static String getHostname() {
v1.2.1,try {
v1.2.1,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
v1.2.1,} catch (UnknownHostException uhe) {
v1.2.1,}
v1.2.1,"return new StringBuilder().append("""").append(uhe).toString();"
v1.2.1,}
v1.2.1,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
v1.2.1,String hostname = getHostname();
v1.2.1,String classname = clazz.getSimpleName();
v1.2.1,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
v1.2.1,"StringBuilder().append(""Starting "").append(classname).toString(), new"
v1.2.1,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
v1.2.1,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
v1.2.1,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
v1.2.1,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
v1.2.1,
v1.2.1,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
v1.2.1,public void run() {
v1.2.1,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
v1.2.1,"this.val$classname + "" at "" + this.val$hostname}));"
v1.2.1,}
v1.2.1,});
v1.2.1,}
v1.2.1,"We we interrupted because we're meant to stop? If not, just"
v1.2.1,continue ignoring the interruption
v1.2.1,Recalculate waitTime.
v1.2.1,// Begin delegation to Thread
v1.2.1,// End delegation to Thread
v1.2.1,instance submitter class
v1.2.1,Obtain filename from path
v1.2.1,Split filename to prexif and suffix (extension)
v1.2.1,Check if the filename is okay
v1.2.1,Prepare temporary file
v1.2.1,Prepare buffer for data copying
v1.2.1,Open and check input stream
v1.2.1,Open output stream and copy data between source file in JAR and the temporary file
v1.2.1,"If read/write fails, close streams safely before throwing an exception"
v1.2.1,"Finally, load the library"
v1.2.1,little endian load order
v1.2.1,tail
v1.2.1,fallthrough
v1.2.1,fallthrough
v1.2.1,finalization
v1.2.1,fmix(h1);
v1.2.1,----------
v1.2.1,body
v1.2.1,----------
v1.2.1,tail
v1.2.1,----------
v1.2.1,finalization
v1.2.1,----------
v1.2.1,body
v1.2.1,----------
v1.2.1,tail
v1.2.1,----------
v1.2.1,finalization
v1.2.1,JobStateProto jobState = report.getJobState();
v1.2.1,the leaf level file should be readable by others
v1.2.1,the subdirs in the path should have execute permissions for
v1.2.1,others
v1.2.1,2.get job id
v1.2.1,Credentials credentials = new Credentials();
v1.2.1,4.copy resource files to hdfs
v1.2.1,5.write configuration to a xml file
v1.2.1,6.create am container context
v1.2.1,7.Submit to ResourceManager
v1.2.1,8.get app master client
v1.2.1,Create a number of filenames in the JobTracker's fs namespace
v1.2.1,add all the command line files/ jars and archive
v1.2.1,first copy them to jobtrackers filesystem
v1.2.1,should not throw a uri exception
v1.2.1,should not throw an uri excpetion
v1.2.1,set the timestamps of the archives and files
v1.2.1,set the public/private visibility of the archives and files
v1.2.1,get DelegationToken for each cached file
v1.2.1,check if we do not need to copy the files
v1.2.1,is jt using the same file system.
v1.2.1,just checking for uri strings... doing no dns lookups
v1.2.1,to see if the filesystems are the same. This is not optimal.
v1.2.1,but avoids name resolution.
v1.2.1,this might have name collisions. copy will throw an exception
v1.2.1,parse the original path to create new path
v1.2.1,check for ports
v1.2.1,Write job file to JobTracker's fs
v1.2.1,Setup resource requirements
v1.2.1,Setup LocalResources
v1.2.1,Setup security tokens
v1.2.1,Setup the command to run the AM
v1.2.1,Add AM user command opts
v1.2.1,Final command
v1.2.1,Setup the CLASSPATH in environment
v1.2.1,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
v1.2.1,Setup the environment variables for Admin first
v1.2.1,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
v1.2.1,Parse distributed cache
v1.2.1,Setup ContainerLaunchContext for AM container
v1.2.1,Set up the ApplicationSubmissionContext
v1.2.1,resposne.encode(buf);
v1.2.1,TODO:
v1.2.1,resposne.encode(buf);
v1.2.1,TODO:
v1.2.1,resposne.encode(buf);
v1.2.1,TODO:
v1.2.1,resposne.encode(buf);
v1.2.1,TODO:
v1.2.1,Add tokens to new user so that it may execute its task correctly.
v1.2.1,to exit
v1.2.1,private final ParameterServer psServer;
v1.2.1,TODO
v1.2.1,"when we should write snapshot to hdfs? clearly, we have two methods:"
v1.2.1,"1. write snapshot at regular time, if there are updates, just write them."
v1.2.1,"2. write snapshot every N iterations, this method depends on notification of master"
v1.2.1,"FSDataOutputStream output = fileContext.create(snapshotsTempFilePath,"
v1.2.1,EnumSet.of(CreateFlag.CREATE));
v1.2.1,@brief get filename of the old snapshot written before
v1.2.1,"no snapshotFile write before, maybe write snapshots the first time"
v1.2.1,data.rewind();
v1.2.1,data.rewind();
v1.2.1,data.rewind();
v1.2.1,output.writeInt(clock);
v1.2.1,clock = input.readInt();
v1.2.1,Read matrix meta from meta file
v1.2.1,Load partitions from file use fork-join
v1.2.1,Save partitions to files use fork-join
v1.2.1,Write the ps matrix meta to the meta file
v1.2.1,Mapping from taskId to clock value.
v1.2.1,int[] keys = sparseRep.getKeys();
v1.2.1,int[] values = sparseRep.getValues();
v1.2.1,boolean[] used = sparseRep.getUsed();
v1.2.1,nnz = 0;
v1.2.1,for (int i = 0; i < keys.length; i++)
v1.2.1,if (used[i]) {
v1.2.1,"denseRep.put(keys[i], values[i]);"
v1.2.1,nnz++;
v1.2.1,}
v1.2.1,sparseRep = null;
v1.2.1,int[] keys = sparseRep.getKeys();
v1.2.1,int[] values = sparseRep.getValues();
v1.2.1,boolean[] used = sparseRep.getUsed();
v1.2.1,for (int i = 0; i < keys.length; i++)
v1.2.1,if (used[i]) {
v1.2.1,"denseRep.put(keys[i], values[i]);"
v1.2.1,}
v1.2.1,sparseRep = null;
v1.2.1,output.writeInt(data.length);
v1.2.1,@Override
v1.2.1,public void serialize(ByteBuf buf) {
v1.2.1,if (sparseRep != null)
v1.2.1,return serializeSparse();
v1.2.1,else if (denseRep != null)
v1.2.1,return serializeDense();
v1.2.1,return serializeEmpty();
v1.2.1,}
v1.2.1,int[] keys = sparseRep.getKeys();
v1.2.1,int[] values = sparseRep.getValues();
v1.2.1,boolean[] used = sparseRep.getUsed();
v1.2.1,int idx = 0;
v1.2.1,for (int i = 0; i < keys.length; i++)
v1.2.1,if (used[i]) {
v1.2.1,"keysBuf.put(idx, keys[i]);"
v1.2.1,"valuesBuf.put(idx, values[i]);"
v1.2.1,idx++;
v1.2.1,}
v1.2.1,int[] keys = sparseRep.getKeys();
v1.2.1,int[] values = sparseRep.getValues();
v1.2.1,boolean[] used = sparseRep.getUsed();
v1.2.1,"int ov, k, v;"
v1.2.1,for (int i = 0; i < keys.length; i++) {
v1.2.1,if (used[i]) {
v1.2.1,k = keys[i];
v1.2.1,ov = denseRep.get(k);
v1.2.1,v = ov + values[i];
v1.2.1,"denseRep.put(k, v);"
v1.2.1,if (ov != 0 && v == 0)
v1.2.1,nnz--;
v1.2.1,}
v1.2.1,}
v1.2.1,"add the PSAgentContext,need fix"
v1.2.1,set MatrixPartitionLocation
v1.2.1,set attribute
v1.2.1,return this;
v1.2.1,return this;
v1.2.1,return this;
v1.2.1,TODO:
v1.2.1,write the max abs
v1.2.1,TODO Auto-generated method stub
v1.2.1,TODO Auto-generated method stub
v1.2.1,TODO Auto-generated method stub
v1.2.1,get configuration from config file
v1.2.1,set localDir with enviroment set by nm.
v1.2.1,get master location
v1.2.1,init task manager and start tasks
v1.2.1,start heartbeat thread
v1.2.1,taskManager.assignTaskIds(response.getTaskidsList());
v1.2.1,todo
v1.2.1,"if worker timeout, it may be knocked off."
v1.2.1,"SUCCESS, do nothing"
v1.2.1,heartbeatFailedTime = 0;
v1.2.1,private KEY currentKey;
v1.2.1,will be created
v1.2.1,TODO Auto-generated method stub
v1.2.1,Bitmap bitmap = new Bitmap();
v1.2.1,int max = indexArray[size - 1];
v1.2.1,byte [] bitIndexArray = new byte[max / 8 + 1];
v1.2.1,for(int i = 0; i < size; i++){
v1.2.1,int bitIndex = indexArray[i] >> 3;
v1.2.1,int bitOffset = indexArray[i] - (bitIndex << 3);
v1.2.1,switch(bitOffset){
v1.2.1,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
v1.2.1,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
v1.2.1,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
v1.2.1,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
v1.2.1,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
v1.2.1,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
v1.2.1,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
v1.2.1,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
v1.2.1,}
v1.2.1,}
v1.2.1,//////////////////////////////
v1.2.1,Application Configs
v1.2.1,//////////////////////////////
v1.2.1,//////////////////////////////
v1.2.1,Master Configs
v1.2.1,//////////////////////////////
v1.2.1,//////////////////////////////
v1.2.1,Worker Configs
v1.2.1,//////////////////////////////
v1.2.1,//////////////////////////////
v1.2.1,Task Configs
v1.2.1,//////////////////////////////
v1.2.1,//////////////////////////////
v1.2.1,ParameterServer Configs
v1.2.1,//////////////////////////////
v1.2.1,////////////////// IPC //////////////////////////
v1.2.1,//////////////////////////////
v1.2.1,Matrix transfer Configs.
v1.2.1,//////////////////////////////
v1.2.1,//////////////////////////////
v1.2.1,Matrix transfer Configs.
v1.2.1,//////////////////////////////
v1.2.1,Configs used to ANGEL_PS_PSAGENT running mode future.
v1.2.1,model parse
v1.2.1,private Configuration conf;
v1.2.1,TODO Auto-generated constructor stub
v1.2.1,Feature number of train data
v1.2.1,Number of nonzero features
v1.2.1,Tree number
v1.2.1,Tree depth
v1.2.1,Split number
v1.2.1,Feature sample ratio
v1.2.1,Data format
v1.2.1,Learning rate
v1.2.1,Set basic configuration keys
v1.2.1,Use local deploy mode and dummy data spliter
v1.2.1,"set input, output path"
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,Set GBDT algorithm parameters
v1.2.1,Submit GBDT Train Task
v1.2.1,Load Model from HDFS.
v1.2.1,set basic configuration keys
v1.2.1,use local deploy mode and dummy dataspliter
v1.2.1,get a angel client
v1.2.1,add matrix
v1.2.1,Set local deploy mode
v1.2.1,Set basic configuration keys
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,"Set trainning data, save model, log path"
v1.2.1,Set actionType train
v1.2.1,Set MF algorithm parameters
v1.2.1,Feature number of train data
v1.2.1,Total iteration number
v1.2.1,Validation sample Ratio
v1.2.1,"Data format, libsvm or dummy"
v1.2.1,Train batch number per epoch.
v1.2.1,Batch number
v1.2.1,Model type
v1.2.1,Learning rate
v1.2.1,Decay of learning rate
v1.2.1,Regularization coefficient
v1.2.1,Set local deploy mode
v1.2.1,Set basic configuration keys
v1.2.1,Set data format
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,set sgd LR algorithm parameters #feature #epoch
v1.2.1,Set trainning data path
v1.2.1,Set save model path
v1.2.1,Set log path
v1.2.1,Set actionType train
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set save model path
v1.2.1,Set log path
v1.2.1,Set actionType incremental train
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set predict result path
v1.2.1,Set actionType prediction
v1.2.1,LOG.info(sigmoid(data[i]));
v1.2.1,LOG.info(Math.exp(-data[i]));
v1.2.1,when b is a negative number
v1.2.1,LOG.info(sigmoid(data[i]));
v1.2.1,LOG.info(Math.exp(-data[i]));
v1.2.1,when b is a negative number
v1.2.1,Feature number of train data
v1.2.1,Total iteration number
v1.2.1,Validation sample Ratio
v1.2.1,"Data format, libsvm or dummy"
v1.2.1,Train batch number per epoch.
v1.2.1,Batch number
v1.2.1,Learning rate
v1.2.1,Decay of learning rate
v1.2.1,Regularization coefficient
v1.2.1,Set local deploy mode
v1.2.1,Set basic configuration keys
v1.2.1,Set data format
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,set MLR algorithm parameters #feature #epoch
v1.2.1,Set trainning data path
v1.2.1,Set save model path
v1.2.1,Set log path
v1.2.1,Set actionType train
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set save model path
v1.2.1,Set log path
v1.2.1,Set actionType incremental train
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set predict result path
v1.2.1,Set actionType prediction
v1.2.1,Feature number of train data
v1.2.1,Total iteration number
v1.2.1,Rank
v1.2.1,Regularization parameters
v1.2.1,Learn rage
v1.2.1,Set local deploy mode
v1.2.1,Set basic configuration keys
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,set FM algorithm parameters #feature #epoch
v1.2.1,Set trainning data path
v1.2.1,Set save model path
v1.2.1,Set log path
v1.2.1,Set actionType train
v1.2.1,Set trainning data path
v1.2.1,Set save model path
v1.2.1,Set log path
v1.2.1,Set actionType train
v1.2.1,Set learnType
v1.2.1,Set feature number
v1.2.1,Cluster center number
v1.2.1,Feature number of train data
v1.2.1,Total iteration number
v1.2.1,Sample ratio per mini-batch
v1.2.1,C
v1.2.1,Set local deploy mode
v1.2.1,Set basic configuration keys
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,set Kmeans algorithm parameters #cluster #feature #epoch
v1.2.1,Set data format
v1.2.1,Set trainning data path
v1.2.1,Set save model path
v1.2.1,Set log sava path
v1.2.1,Set actionType train
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set predict result path
v1.2.1,Set actionType prediction
v1.2.1,Feature number of train data
v1.2.1,Total iteration number
v1.2.1,Validation Ratio
v1.2.1,Data format
v1.2.1,Train batch number per epoch.
v1.2.1,Learning rate
v1.2.1,Decay of learning rate
v1.2.1,Regularization coefficient
v1.2.1,Set basic configuration keys
v1.2.1,Set data format
v1.2.1,Use local deploy mode
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,set sgd SVM algorithm parameters
v1.2.1,"set input, output path"
v1.2.1,Set save model path
v1.2.1,Set actionType train
v1.2.1,Set log path
v1.2.1,Submit LR Train Task
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set save model path
v1.2.1,Set actionType incremental train
v1.2.1,Set log path
v1.2.1,Feature number of train data
v1.2.1,Total iteration number
v1.2.1,Validation sample Ratio
v1.2.1,"Data format, libsvm or dummy"
v1.2.1,Train batch number per epoch.
v1.2.1,Learning rate
v1.2.1,Decay of learning rate
v1.2.1,Regularization coefficient
v1.2.1,Set local deploy mode
v1.2.1,Set basic configuration keys
v1.2.1,Set data format
v1.2.1,"set angel resource parameters #worker, #task, #PS"
v1.2.1,set sgd LR algorithm parameters #feature #epoch
v1.2.1,Set trainning data path
v1.2.1,Set save model path
v1.2.1,Set log path
v1.2.1,Set actionType train
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set save model path
v1.2.1,Set actionType incremental train
v1.2.1,Set log path
v1.2.1,Set trainning data path
v1.2.1,Set load model path
v1.2.1,Set predict result path
v1.2.1,Set log sava path
v1.2.1,Set actionType prediction
v1.2.1,double z=pre*y;
v1.2.1,if(z<=0) return 0.5-z;
v1.2.1,"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);"
v1.2.1,return 0.0;
v1.2.1,logistic loss for binary classification task.
v1.2.1,"logistic loss, but predict un-transformed margin"
v1.2.1,check if label in range
v1.2.1,return the default evaluation metric for the objective
v1.2.1,TODO Auto-generated method stub
v1.2.1,start row index for words
v1.2.1,doc ids
v1.2.1,topic assignments
v1.2.1,count word
v1.2.1,build word start index
v1.2.1,build dks
v1.2.1,"model.wtMat().increment(w, update);"
v1.2.1,"update.plusBy(t, 1);"
v1.2.1,"model.wtMat().increment(w, update);"
v1.2.1,The starting point
v1.2.1,There's always an unused entry.
v1.2.1,The starting point
v1.2.1,There's always an unused entry.
v1.2.1,The starting point
v1.2.1,There's always an unused entry.
v1.2.1,The starting point
v1.2.1,There's always an unused entry.
v1.2.1,The starting point
v1.2.1,There's always an unused entry.
v1.2.1,The starting point
v1.2.1,There's always an unused entry.
v1.2.1,print();
v1.2.1,The starting point
v1.2.1,There's always an unused entry.
v1.2.1,The starting point
v1.2.1,There's always an unused entry.
v1.2.1,The starting point
v1.2.1,There's always an unused entry.
v1.2.1,The starting point
v1.2.1,There's always an unused entry.
v1.2.1,print();
v1.2.1,Write #rows
v1.2.1,Write each row
v1.2.1,dense
v1.2.1,sparse
v1.2.1,LOG.info(buf.refCnt());
v1.2.1,dense
v1.2.1,sparse
v1.2.1,LOG.info(buf.refCnt());
v1.2.1,loss function
v1.2.1,gradient and hessian
v1.2.1,tree node
v1.2.1,initialize the phase
v1.2.1,current tree and depth
v1.2.1,create loss function
v1.2.1,calculate grad info of each instance
v1.2.1,"create data sketch, push candidate split value to PS"
v1.2.1,1. calculate candidate split value
v1.2.1,2. push local sketch to PS
v1.2.1,3. set phase to GET_SKETCH
v1.2.1,"pull the global sketch from PS, only called once by each worker"
v1.2.1,sample feature
v1.2.1,push sampled feature set to the current tree
v1.2.1,create new tree
v1.2.1,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
v1.2.1,calculate gradient
v1.2.1,"1. create new tree, initialize tree nodes and node stats"
v1.2.1,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
v1.2.1,2.1. pull the sampled features of the current tree
v1.2.1,"2.2. if use all the features, only called one"
v1.2.1,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
v1.2.1,4. set root node to active
v1.2.1,"5. reset instance position, set the root node's span"
v1.2.1,6. calculate gradient
v1.2.1,7. set phase to run active
v1.2.1,1. start threads of active tree nodes
v1.2.1,1.1. start threads for active nodes to generate histogram
v1.2.1,1.2. set thread status to batch num
v1.2.1,1.3. set the oplog to active
v1.2.1,"2. check thread stats, if all threads finish, return"
v1.2.1,clock
v1.2.1,find split
v1.2.1,"1. find responsible tree node, using RR scheme"
v1.2.1,2. pull gradient histogram
v1.2.1,2.1. get the name of this node's gradient histogram on PS
v1.2.1,2.2. pull the histogram
v1.2.1,histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();
v1.2.1,2.3. find best split result of this tree node
v1.2.1,2.3.1 using server split
v1.2.1,"update the grad stats of the root node on PS, only called once by leader worker"
v1.2.1,update the grad stats of children node
v1.2.1,update the left child
v1.2.1,update the right child
v1.2.1,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
v1.2.1,"2.3.3 otherwise, the returned histogram contains the gradient info"
v1.2.1,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
v1.2.1,2.3.5 reset this tree node's gradient histogram to 0
v1.2.1,3. push split feature to PS
v1.2.1,4. push split value to PS
v1.2.1,5. push split gain to PS
v1.2.1,6. set phase to AFTER_SPLIT
v1.2.1,clock
v1.2.1,1. get split feature
v1.2.1,2. get split value
v1.2.1,3. get split gain
v1.2.1,4. get node weight
v1.2.1,5. split node
v1.2.1,"2. check thread stats, if all threads finish, return"
v1.2.1,6. clock
v1.2.1,"split the span of one node, reset the instance position"
v1.2.1,in case this worker has no instance on this node
v1.2.1,set the span of left child
v1.2.1,set the span of right child
v1.2.1,"1. left to right, find the first instance that should be in the right child"
v1.2.1,"2. right to left, find the first instance that should be in the left child"
v1.2.1,3. swap two instances
v1.2.1,4. find the cut pos
v1.2.1,than the split value
v1.2.1,5. set the span of left child
v1.2.1,6. set the span of right child
v1.2.1,set tree node to active
v1.2.1,set node to leaf
v1.2.1,set node to inactive
v1.2.1,finish current tree
v1.2.1,finish current depth
v1.2.1,set the tree phase
v1.2.1,check if there is active node
v1.2.1,check if finish all the tree
v1.2.1,update node's grad stats on PS
v1.2.1,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
v1.2.1,the root node's stats is updated by leader worker
v1.2.1,1. create the update
v1.2.1,2. push the update to PS
v1.2.1,the leader task adds node prediction to flush list
v1.2.1,1. name of this node's grad histogram on PS
v1.2.1,2. build the grad histogram of this node
v1.2.1,3. push the histograms to PS
v1.2.1,4. reset thread stats to finished
v1.2.1,5.1. set the children nodes of this node
v1.2.1,5.2. set split info and grad stats to this node
v1.2.1,5.2. create children nodes
v1.2.1,"5.3. create node stats for children nodes, and add them to the tree"
v1.2.1,5.4. reset instance position
v1.2.1,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
v1.2.1,5.6. set children nodes to leaf nodes
v1.2.1,5.7. set nid to leaf node
v1.2.1,5.8. deactivate active node
v1.2.1,"get feature type, 0:empty 1:all equal 2:real"
v1.2.1,feature index used to split
v1.2.1,feature value used to split
v1.2.1,loss change after split this node
v1.2.1,grad stats of the left child
v1.2.1,grad stats of the right child
v1.2.1,"LOG.info(""Constructor with fid = -1"");"
v1.2.1,fid = -1: no split currently
v1.2.1,the minimal split value is the minimal value of feature
v1.2.1,the splits do not include the maximal value of feature
v1.2.1,"1. the average distance, (maxValue - minValue) / splitNum"
v1.2.1,2. calculate the candidate split value
v1.2.1,1. new feature's histogram (grad + hess)
v1.2.1,size: sampled_featureNum * (2 * splitNum)
v1.2.1,"in other words, concatenate each feature's histogram"
v1.2.1,2. get the span of this node
v1.2.1,int nodeStart = this.controller.nodePosStart[nid];
v1.2.1,int nodeEnd = this.controller.nodePosEnd[nid];
v1.2.1,------ 3. using sparse-aware method to build histogram ---
v1.2.1,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
v1.2.1,3.1. get the instance index
v1.2.1,3.2. get the grad and hess of the instance
v1.2.1,3.3. add to the sum
v1.2.1,3.4. loop the non-zero entries
v1.2.1,3.4.1. get feature value
v1.2.1,3.4.2. current feature's position in the sampled feature set
v1.2.1,3.4.3. find the position of feature value in a histogram
v1.2.1,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
v1.2.1,3.4.4. add the grad and hess to the corresponding bin
v1.2.1,3.4.5. add the reverse to the bin that contains 0.0f
v1.2.1,4. add the grad and hess sum to the zero bin of all features
v1.2.1,int startIdx = fid * 2 * splitNum;
v1.2.1,find the best split result of the histogram of a tree node
v1.2.1,1. calculate the gradStats of the root node
v1.2.1,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
v1.2.1,2. loop over features
v1.2.1,2.1. get the ture feature id in the sampled feature set
v1.2.1,2.2. get the indexes of histogram of this feature
v1.2.1,2.3. find the best split of current feature
v1.2.1,2.4. update the best split result if possible
v1.2.1,"update the grad stats of the root node on PS, only called once by leader worker"
v1.2.1,3. update the grad stats of children node
v1.2.1,3.1. update the left child
v1.2.1,3.2. update the right child
v1.2.1,find the best split result of one feature
v1.2.1,1. set the feature id
v1.2.1,2. create the best left stats and right stats
v1.2.1,3. the gain of the root node
v1.2.1,4. create the temp left and right grad stats
v1.2.1,5. loop over all the data in histogram
v1.2.1,5.1. get the grad and hess of current hist bin
v1.2.1,5.2. check whether we can split with current left hessian
v1.2.1,right = root - left
v1.2.1,5.3. check whether we can split with current right hessian
v1.2.1,5.4. calculate the current loss gain
v1.2.1,5.5. check whether we should update the split result with current loss gain
v1.2.1,split value = sketches[splitIdx+1]
v1.2.1,"5.6. if should update, also update the best left and right grad stats"
v1.2.1,6. set the best left and right grad stats
v1.2.1,partition number
v1.2.1,cols of each partition
v1.2.1,"// update the grad stats of the root node on PS, only called once by leader worker"
v1.2.1,if (this.nid == 0) {
v1.2.1,GradStats rootStats = new GradStats(splitEntry.leftGradStat);
v1.2.1,rootStats.add(splitEntry.rightGradStat);
v1.2.1,"this.controller.updateNodeGradStats(this.nid, rootStats);"
v1.2.1,}
v1.2.1,
v1.2.1,// 3. update the grad stats of children node
v1.2.1,if (splitEntry.fid != -1) {
v1.2.1,// 3.1. update the left child
v1.2.1,"this.controller.updateNodeGradStats(2 * this.nid + 1, splitEntry.leftGradStat);"
v1.2.1,// 3.2. update the right child
v1.2.1,"this.controller.updateNodeGradStats(2 * this.nid + 2, splitEntry.rightGradStat);"
v1.2.1,}
v1.2.1,1. calculate the total grad sum and hess sum
v1.2.1,2. create the grad stats of the node
v1.2.1,1. calculate the total grad sum and hess sum
v1.2.1,2. create the grad stats of the node
v1.2.1,1. calculate the total grad sum and hess sum
v1.2.1,2. create the grad stats of the node
v1.2.1,"loop all the possible split value, start from split[1], since the first item is the minimal"
v1.2.1,feature value
v1.2.1,find the best split result of the histogram of a tree node
v1.2.1,2.2. get the indexes of histogram of this feature
v1.2.1,2.3. find the best split of current feature
v1.2.1,2.4. update the best split result if possible
v1.2.1,find the best split result of one feature
v1.2.1,1. set the feature id
v1.2.1,splitEntry.setFid(fid);
v1.2.1,2. create the best left stats and right stats
v1.2.1,3. the gain of the root node
v1.2.1,4. create the temp left and right grad stats
v1.2.1,5. loop over all the data in histogram
v1.2.1,5.1. get the grad and hess of current hist bin
v1.2.1,5.2. check whether we can split with current left hessian
v1.2.1,right = root - left
v1.2.1,5.3. check whether we can split with current right hessian
v1.2.1,5.4. calculate the current loss gain
v1.2.1,5.5. check whether we should update the split result with current loss gain
v1.2.1,"5.6. if should update, also update the best left and right grad stats"
v1.2.1,6. set the best left and right grad stats
v1.2.1,find the best split result of a serve row on the PS
v1.2.1,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.2.1,2.2. get the start index in histogram of this feature
v1.2.1,2.3. find the best split of current feature
v1.2.1,2.4. update the best split result if possible
v1.2.1,"find the best split result of one feature from a server row, used by the PS"
v1.2.1,1. set the feature id
v1.2.1,2. create the best left stats and right stats
v1.2.1,3. the gain of the root node
v1.2.1,4. create the temp left and right grad stats
v1.2.1,5. loop over all the data in histogram
v1.2.1,5.1. get the grad and hess of current hist bin
v1.2.1,5.2. check whether we can split with current left hessian
v1.2.1,right = root - left
v1.2.1,5.3. check whether we can split with current right hessian
v1.2.1,5.4. calculate the current loss gain
v1.2.1,5.5. check whether we should update the split result with current loss gain
v1.2.1,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
v1.2.1,the task use index to find fvalue
v1.2.1,"5.6. if should update, also update the best left and right grad stats"
v1.2.1,6. set the best left and right grad stats
v1.2.1,clear all the information
v1.2.1,calculate the sum of gradient and hess
v1.2.1,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v1.2.1,ridx)
v1.2.1,check if necessary information is ready
v1.2.1,"same as add, reduce is used in All Reduce"
v1.2.1,"features used in this tree, if equals null, means use all the features without sampling"
v1.2.1,node in the tree
v1.2.1,the gradient info of each instances
v1.2.1,initialize nodes
v1.2.1,gradient
v1.2.1,second order gradient
v1.2.1,int sendStartCol = startFid * 7; // each split contains 7 doubles
v1.2.1,1. calculate the total grad sum and hess sum
v1.2.1,2. create the grad stats of the node
v1.2.1,find the best split result of a serve row on the PS
v1.2.1,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.2.1,2.2. get the start index in histogram of this feature
v1.2.1,2.3. find the best split of current feature
v1.2.1,2.4. update the best split result if possible
v1.2.1,"find the best split result of one feature from a server row, used by the PS"
v1.2.1,1. set the feature id
v1.2.1,2. create the best left stats and right stats
v1.2.1,3. the gain of the root node
v1.2.1,4. create the temp left and right grad stats
v1.2.1,5. loop over all the data in histogram
v1.2.1,5.1. get the grad and hess of current hist bin
v1.2.1,5.2. check whether we can split with current left hessian
v1.2.1,right = root - left
v1.2.1,5.3. check whether we can split with current right hessian
v1.2.1,5.4. calculate the current loss gain
v1.2.1,5.5. check whether we should update the split result with current loss gain
v1.2.1,"tips: here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
v1.2.1,the task use index to find fvalue
v1.2.1,"5.6. if should update, also update the best left and right grad stats"
v1.2.1,6. set the best left and right grad stats
v1.2.1,"System.out.println(""Indices: "" + Arrays.toString(indices));"
v1.2.1,t[i][code]++;
v1.2.1,else if (Math.random() > 0.5) {
v1.2.1,t[i][code] = freq;
v1.2.1,}
v1.2.1,"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);"
v1.2.1,"ret = Math.min(ret, t[i][h[i].encode(key)]);"
v1.2.1,"Get input path, output path"
v1.2.1,Init serde
v1.2.1,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.2.1,"Get input path, output path"
v1.2.1,Init serde
v1.2.1,"Parse need convert model names, if not set, we will convert all models in input directory"
v1.2.1,"quantile sketch, size = featureNum * splitNum"
v1.2.1,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
v1.2.1,"active tree nodes, size = pow(2, treeDepth) -1"
v1.2.1,sampled features. size = treeNum * sampleRatio * featureNum
v1.2.1,"split features, size = treeNum * treeNodeNum"
v1.2.1,"split values, size = treeNum * treeNodeNum"
v1.2.1,"split gains, size = treeNum * treeNodeNum"
v1.2.1,"node weights, size = treeNum * treeNodeNum"
v1.2.1,"node preds, size = treeNum * treeNodeNum"
v1.2.1,if using PS to perform split
v1.2.1,step size for a tree
v1.2.1,number of class
v1.2.1,minimum loss change required for a split
v1.2.1,maximum depth of a tree
v1.2.1,number of features
v1.2.1,number of nonzero
v1.2.1,number of candidates split value
v1.2.1,----- the rest parameters are less important ----
v1.2.1,base instance weight
v1.2.1,minimum amount of hessian(weight) allowed in a child
v1.2.1,L2 regularization factor
v1.2.1,L1 regularization factor
v1.2.1,default direction choice
v1.2.1,maximum delta update we can add in weight estimation
v1.2.1,this parameter can be used to stabilize update
v1.2.1,default=0 means no constraint on weight delta
v1.2.1,whether we want to do subsample for row
v1.2.1,whether to subsample columns for each tree
v1.2.1,accuracy of sketch
v1.2.1,accuracy of sketch
v1.2.1,leaf vector size
v1.2.1,option for parallelization
v1.2.1,option to open cacheline optimization
v1.2.1,whether to not print info during training.
v1.2.1,"get feature type, 0:empty 1:all equal 2:real"
v1.2.1,maximum depth of the tree
v1.2.1,number of features used for tree construction
v1.2.1,"minimum loss change required for a split, otherwise stop split"
v1.2.1,----- the rest parameters are less important ----
v1.2.1,default direction choice
v1.2.1,whether we want to do sample data
v1.2.1,whether to sample columns during tree construction
v1.2.1,whether to use histogram for split
v1.2.1,number of histogram units
v1.2.1,whether to print info during training.
v1.2.1,----- the rest parameters are obtained after training ----
v1.2.1,total number of nodes
v1.2.1,number of deleted nodes */
v1.2.0,TODO: SplitNum
v1.2.0,TODO: implement split method
v1.2.0,SplitEntry splitEntry = GradHistHelper.findSplitOfServerRow(row);
v1.2.0,int sendStartCol = startFid * 7; // each split contains 7 doubles
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy data spliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,row 0 is a random uniform
v1.2.0,row 1 is a random normal
v1.2.0,row 2 is filled with 1.0
v1.2.0,in different part
v1.2.0,TODO Auto-generated constructor stub
v1.2.0,row 0 is a random uniform
v1.2.0,row 1 is a random normal
v1.2.0,row 2 is filled with 1.0
v1.2.0,find the max abs
v1.2.0,compress data
v1.2.0,"paras[1] = ""abc"";"
v1.2.0,"paras[2] = ""123"";"
v1.2.0,Add standard Hadoop classes
v1.2.0,Feature number of train data
v1.2.0,Total iteration number
v1.2.0,Learning rate
v1.2.0,Regularization coefficient
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,set sgd LR algorithm parameters #feature #epoch
v1.2.0,Set input data path
v1.2.0,Set save model path
v1.2.0,Set actionType train
v1.2.0,Feature number of train data
v1.2.0,Number of nonzero features
v1.2.0,Tree number
v1.2.0,Tree depth
v1.2.0,Split number
v1.2.0,Feature sample ratio
v1.2.0,Data format
v1.2.0,Learning rate
v1.2.0,Set basic configuration keys
v1.2.0,Use local deploy mode and dummy data spliter
v1.2.0,"set input, output path"
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,Set GBDT algorithm parameters
v1.2.0,Load Model from HDFS.
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,"Set trainning data, and save model path"
v1.2.0,Set actionType train
v1.2.0,Set MF algorithm parameters
v1.2.0,Feature number of train data
v1.2.0,Total iteration number
v1.2.0,Validation sample Ratio
v1.2.0,"Data format, libsvm or dummy"
v1.2.0,Train batch number per epoch.
v1.2.0,Learning rate
v1.2.0,Decay of learning rate
v1.2.0,Regularization coefficient
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,Set data format
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,set sgd LR algorithm parameters #feature #epoch
v1.2.0,Set trainning data path
v1.2.0,Set save model path
v1.2.0,Set log path
v1.2.0,Set actionType train
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set save model path
v1.2.0,Set actionType incremental train
v1.2.0,Set log path
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set predict result path
v1.2.0,Set actionType prediction
v1.2.0,Feature number of train data
v1.2.0,Total iteration number
v1.2.0,Validation sample Ratio
v1.2.0,"Data format, libsvm or dummy"
v1.2.0,Train batch number per epoch.
v1.2.0,Batch number
v1.2.0,Learning rate
v1.2.0,Decay of learning rate
v1.2.0,Regularization coefficient
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,Set data format
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,set sgd LR algorithm parameters #feature #epoch
v1.2.0,Set trainning data path
v1.2.0,Set save model path
v1.2.0,Set log path
v1.2.0,Set actionType train
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set save model path
v1.2.0,Set log path
v1.2.0,Set actionType incremental train
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set predict result path
v1.2.0,Set log path
v1.2.0,Set actionType prediction
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,worker register
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,add matrix
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,attempt 0
v1.2.0,attempt1
v1.2.0,attempt1
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,TODO Auto-generated constructor stub
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,set basic configuration keys
v1.2.0,"conf.set(AngelConf.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,Thread.sleep(5000);
v1.2.0,"response = master.getJobReport(null, request);"
v1.2.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
v1.2.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
v1.2.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,"DenseIntVector deltaVec = new DenseIntVector(100000, delta);"
v1.2.0,deltaVec.setMatrixId(matrixW1Id);
v1.2.0,deltaVec.setRowId(0);
v1.2.0,TODO Auto-generated constructor stub
v1.2.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
v1.2.0,@RunWith(MockitoJUnitRunner.class)
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,"conf.setInt(AngelConf.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,psAgent.initAndStart();
v1.2.0,test conf
v1.2.0,test master location
v1.2.0,test app id
v1.2.0,test user
v1.2.0,test ps agent attempt id
v1.2.0,test ps agent id
v1.2.0,test connection
v1.2.0,test master client
v1.2.0,test ip
v1.2.0,test loc
v1.2.0,test master location
v1.2.0,test ps location
v1.2.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
v1.2.0,test all ps ids
v1.2.0,test all matrix ids
v1.2.0,test all matrix names
v1.2.0,test matrix attribute
v1.2.0,test matrix meta
v1.2.0,test ps location
v1.2.0,test partitions
v1.2.0,"Note:[startRow,endRow)"
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,group0Id = new WorkerGroupId(0);
v1.2.0,"worker0Id = new WorkerId(group0Id, 0);"
v1.2.0,"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);"
v1.2.0,task0Id = new TaskId(0);
v1.2.0,task1Id = new TaskId(1);
v1.2.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.2.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.2.0,test this func in testWriteTo
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,"LOG.info(index[0] + "" "" + value[0]);"
v1.2.0,"LOG.info(index[1] + "" "" + value[1]);"
v1.2.0,"LOG.info(index[2] + "" "" + value[2]);"
v1.2.0,dot
v1.2.0,plus
v1.2.0,plusBy
v1.2.0,dot
v1.2.0,plus
v1.2.0,plusBy
v1.2.0,dot
v1.2.0,plus
v1.2.0,plusBy
v1.2.0,dot
v1.2.0,plusBy
v1.2.0,@Test
v1.2.0,public void dotDenseFloatVector() throws Exception {
v1.2.0,int dim = 1000;
v1.2.0,Random random = new Random(System.currentTimeMillis());
v1.2.0,
v1.2.0,double[] values = new double[dim];
v1.2.0,float[] values_1 = new float[dim];
v1.2.0,for (int i = 0; i < dim; i++) {
v1.2.0,values[i] = random.nextDouble();
v1.2.0,values_1[i] = random.nextFloat();
v1.2.0,}
v1.2.0,
v1.2.0,"DenseDoubleVector vec = new DenseDoubleVector(dim, values);"
v1.2.0,"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);"
v1.2.0,
v1.2.0,double sum = 0.0;
v1.2.0,for (int i = 0; i < dim; i++) {
v1.2.0,sum += values[i] * values_1[i];
v1.2.0,}
v1.2.0,
v1.2.0,"assertEquals(sum, vec.dot(vec_1));"
v1.2.0,
v1.2.0,}
v1.2.0,@Test
v1.2.0,public void plusDenseFlaotVector() throws Exception {
v1.2.0,"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};"
v1.2.0,"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};"
v1.2.0,"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);"
v1.2.0,"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);"
v1.2.0,
v1.2.0,TDoubleVector vec_2 = vec.plus(vec_1);
v1.2.0,for (int i = 0; i < vec.size(); i++)
v1.2.0,"assertEquals(value_1[i] + value_2[i], vec_2.get(i));"
v1.2.0,
v1.2.0,
v1.2.0,"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);"
v1.2.0,
v1.2.0,for (int i = 0; i < vec.size(); i++)
v1.2.0,"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);"
v1.2.0,
v1.2.0,double[] oldValues = vec.getValues().clone();
v1.2.0,
v1.2.0,vec.plusBy(vec_1);
v1.2.0,
v1.2.0,for (int i = 0; i < vec.size(); i++)
v1.2.0,"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));"
v1.2.0,
v1.2.0,oldValues = vec.getValues().clone();
v1.2.0,
v1.2.0,"vec.plusBy(vec_1, 3);"
v1.2.0,
v1.2.0,for (int i = 0; i < vec.size(); i++)
v1.2.0,"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));"
v1.2.0,}
v1.2.0,dot
v1.2.0,plus
v1.2.0,plusBy
v1.2.0,dot
v1.2.0,plus
v1.2.0,plusBy
v1.2.0,@Test
v1.2.0,public void plusBy3() throws Exception {
v1.2.0,"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};"
v1.2.0,"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);"
v1.2.0,"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});"
v1.2.0,vec.setRowId(0);
v1.2.0,"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});"
v1.2.0,vec_1.setRowId(1);
v1.2.0,TDoubleVector vec_2 = new SparseDoubleVector(2);
v1.2.0,"vec_2.set(1, 1.0);"
v1.2.0,vec_2.setRowId(0);
v1.2.0,
v1.2.0,mat.plusBy(vec);
v1.2.0,mat.plusBy(vec_1);
v1.2.0,mat.plusBy(vec_2);
v1.2.0,
v1.2.0,"assertEquals(2.0f, mat.get(0, 0));"
v1.2.0,"assertEquals(4.0f, mat.get(0, 1));"
v1.2.0,"assertEquals(4.0f, mat.get(1, 0));"
v1.2.0,"assertEquals(5.0f, mat.get(1, 1));"
v1.2.0,}
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,test worker getActiveTaskNum
v1.2.0,test worker getTaskNum
v1.2.0,test worker getTaskManager
v1.2.0,test workerId
v1.2.0,test workerAttemptId
v1.2.0,tet worker initFinished
v1.2.0,test worker getInitMinclock
v1.2.0,test worker loacation
v1.2.0,test AppId
v1.2.0,test Conf
v1.2.0,test UserName
v1.2.0,master location
v1.2.0,masterClient
v1.2.0,test psAgent
v1.2.0,test worker get dataBlockManager
v1.2.0,workerGroup.getSplits();
v1.2.0,application
v1.2.0,lcation
v1.2.0,workerGroup info
v1.2.0,worker info
v1.2.0,task
v1.2.0,Matrix parameters
v1.2.0,Set basic configuration keys
v1.2.0,Use local deploy mode and dummy data spliter
v1.2.0,Create an Angel client
v1.2.0,Add different types of matrix
v1.2.0,using mock object
v1.2.0,verification
v1.2.0,Stubbing
v1.2.0,Default does nothing.
v1.2.0,The app injection is optional
v1.2.0,"renderText(""hello world"");"
v1.2.0,"user choose a workerGroupID from the workergroups page,"
v1.2.0,now we should change the AngelApp params and render the workergroup page;
v1.2.0,"static final String WORKER_ID = ""worker.id"";"
v1.2.0,"div(""#logo"")."
v1.2.0,"img(""/static/hadoop-st.png"")._()."
v1.2.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
v1.2.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
v1.2.0,JQueryUI.jsnotice(html);
v1.2.0,import org.apache.hadoop.conf.Configuration;
v1.2.0,import java.lang.reflect.Field;
v1.2.0,get block locations from file system
v1.2.0,create a list of all block and their locations
v1.2.0,"if the file is not splitable, just create the one block with"
v1.2.0,full file length
v1.2.0,each split can be a maximum of maxSize
v1.2.0,if remainder is between max and 2*max - then
v1.2.0,"instead of creating splits of size max, left-max we"
v1.2.0,create splits of size left/2 and left/2. This is
v1.2.0,a heuristic to avoid creating really really small
v1.2.0,splits.
v1.2.0,add this block to the block --> node locations map
v1.2.0,"For blocks that do not have host/rack information,"
v1.2.0,assign to default  rack.
v1.2.0,add this block to the rack --> block map
v1.2.0,Add this host to rackToNodes map
v1.2.0,add this block to the node --> block map
v1.2.0,"if the file system does not have any rack information, then"
v1.2.0,use dummy rack location.
v1.2.0,The topology paths have the host name included as the last
v1.2.0,component. Strip it.
v1.2.0,get tokens for all the required FileSystems..
v1.2.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
v1.2.0,job.getConfiguration());
v1.2.0,Whether we need to recursive look into the directory structure
v1.2.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.2.0,user provided one (if any).
v1.2.0,all the files in input set
v1.2.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
v1.2.0,process all nodes and create splits that are local to a node. Generate
v1.2.0,"one split per node iteration, and walk over nodes multiple times to"
v1.2.0,distribute the splits across nodes.
v1.2.0,Skip the node if it has previously been marked as completed.
v1.2.0,"for each block, copy it into validBlocks. Delete it from"
v1.2.0,blockToNodes so that the same block does not appear in
v1.2.0,two different splits.
v1.2.0,Remove all blocks which may already have been assigned to other
v1.2.0,splits.
v1.2.0,"if the accumulated split size exceeds the maximum, then"
v1.2.0,create this split.
v1.2.0,create an input split and add it to the splits array
v1.2.0,Remove entries from blocksInNode so that we don't walk these
v1.2.0,again.
v1.2.0,Done creating a single split for this node. Move on to the next
v1.2.0,node so that splits are distributed across nodes.
v1.2.0,This implies that the last few blocks (or all in case maxSize=0)
v1.2.0,were not part of a split. The node is complete.
v1.2.0,if there were any blocks left over and their combined size is
v1.2.0,"larger than minSplitNode, then combine them into one split."
v1.2.0,Otherwise add them back to the unprocessed pool. It is likely
v1.2.0,that they will be combined with other blocks from the
v1.2.0,same rack later on.
v1.2.0,This condition also kicks in when max split size is not set. All
v1.2.0,blocks on a node will be grouped together into a single split.
v1.2.0,haven't created any split on this machine. so its ok to add a
v1.2.0,smaller one for parallelism. Otherwise group it in the rack for
v1.2.0,balanced size create an input split and add it to the splits
v1.2.0,array
v1.2.0,Remove entries from blocksInNode so that we don't walk this again.
v1.2.0,The node is done. This was the last set of blocks for this node.
v1.2.0,Put the unplaced blocks back into the pool for later rack-allocation.
v1.2.0,Node is done. All blocks were fit into node-local splits.
v1.2.0,Check if node-local assignments are complete.
v1.2.0,All nodes have been walked over and marked as completed or all blocks
v1.2.0,have been assigned. The rest should be handled via rackLock assignment.
v1.2.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
v1.2.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
v1.2.0,"if blocks in a rack are below the specified minimum size, then keep them"
v1.2.0,"in 'overflow'. After the processing of all racks is complete, these"
v1.2.0,overflow blocks will be combined into splits.
v1.2.0,Process all racks over and over again until there is no more work to do.
v1.2.0,Create one split for this rack before moving over to the next rack.
v1.2.0,Come back to this rack after creating a single split for each of the
v1.2.0,remaining racks.
v1.2.0,"Process one rack location at a time, Combine all possible blocks that"
v1.2.0,reside on this rack as one split. (constrained by minimum and maximum
v1.2.0,split size).
v1.2.0,iterate over all racks
v1.2.0,"for each block, copy it into validBlocks. Delete it from"
v1.2.0,blockToNodes so that the same block does not appear in
v1.2.0,two different splits.
v1.2.0,"if the accumulated split size exceeds the maximum, then"
v1.2.0,create this split.
v1.2.0,create an input split and add it to the splits array
v1.2.0,"if we created a split, then just go to the next rack"
v1.2.0,"if there is a minimum size specified, then create a single split"
v1.2.0,"otherwise, store these blocks into overflow data structure"
v1.2.0,There were a few blocks in this rack that
v1.2.0,remained to be processed. Keep them in 'overflow' block list.
v1.2.0,These will be combined later.
v1.2.0,Process all overflow blocks
v1.2.0,"This might cause an exiting rack location to be re-added,"
v1.2.0,but it should be ok.
v1.2.0,"if the accumulated split size exceeds the maximum, then"
v1.2.0,create this split.
v1.2.0,create an input split and add it to the splits array
v1.2.0,"Process any remaining blocks, if any."
v1.2.0,create an input split
v1.2.0,add this split to the list that is returned
v1.2.0,long num = totLength / maxSize;
v1.2.0,all blocks for all the files in input set
v1.2.0,mapping from a rack name to the list of blocks it has
v1.2.0,mapping from a block to the nodes on which it has replicas
v1.2.0,mapping from a node to the list of blocks that it contains
v1.2.0,populate all the blocks for all files
v1.2.0,stop all services
v1.2.0,1.write application state to file so that the client can get the state of the application
v1.2.0,if master exit
v1.2.0,2.clear tmp and staging directory
v1.2.0,waiting for client to get application state
v1.2.0,stop the RPC server
v1.2.0,"Security framework already loaded the tokens into current UGI, just use"
v1.2.0,them
v1.2.0,Now remove the AM->RM token so tasks don't have it
v1.2.0,add a shutdown hook
v1.2.0,init app state storage
v1.2.0,init event dispacher
v1.2.0,init location manager
v1.2.0,init container allocator
v1.2.0,init a rpc service
v1.2.0,recover matrix meta if needed
v1.2.0,recover ps attempt information if need
v1.2.0,init parameter server manager
v1.2.0,recover task information if needed
v1.2.0,init psagent manager and register psagent manager event
v1.2.0,a dummy data spliter is just for test now
v1.2.0,recover data splits information if needed
v1.2.0,init worker manager and register worker manager event
v1.2.0,register slow worker/ps checker
v1.2.0,register app manager event and finish event
v1.2.0,start a web service if use yarn deploy mode
v1.2.0,load from app state storage first if attempt index great than 1(the master is not the first
v1.2.0,retry)
v1.2.0,"if load failed, just build a new MatrixMetaManager"
v1.2.0,load ps attempt index from app state storage first if attempt index great than 1(the master
v1.2.0,is not the first retry)
v1.2.0,load task information from app state storage first if attempt index great than 1(the master
v1.2.0,is not the first retry)
v1.2.0,"if load failed, just build a new AMTaskManager"
v1.2.0,load data splits information from app state storage first if attempt index great than 1(the
v1.2.0,master is not the first retry)
v1.2.0,"if load failed, we need to recalculate the data splits"
v1.2.0,parse parameter server counters
v1.2.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.2.0,refresh last heartbeat timestamp
v1.2.0,send a state update event to the specific PSAttempt
v1.2.0,check if parameter server can commit now.
v1.2.0,check matrix metadata inconsistencies between master and parameter server.
v1.2.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
v1.2.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
v1.2.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.2.0,check whether psagent heartbeat timeout
v1.2.0,check whether parameter server heartbeat timeout
v1.2.0,check whether worker heartbeat timeout
v1.2.0,choose a unused port
v1.2.0,start RPC server
v1.2.0,find matrix partitions from master matrix meta manager for this parameter server
v1.2.0,remove this parameter server attempt from monitor set
v1.2.0,remove this parameter server attempt from monitor set
v1.2.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.2.0,find workergroup in worker manager
v1.2.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
v1.2.0,"if this worker group run over, just return WORKERGROUP_EXITED"
v1.2.0,"if this worker group is running now, return tasks, workers, data splits for it"
v1.2.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.2.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.2.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.2.0,"in ANGEL_PS mode, task id may can not know advance"
v1.2.0,update the clock for this matrix
v1.2.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.2.0,"in ANGEL_PS mode, task id may can not know advance"
v1.2.0,update task iteration
v1.2.0,private boolean matrixInited;
v1.2.0,"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.2.0,dispatch matrix partitions to parameter servers
v1.2.0,update matrix id generator
v1.2.0,"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.2.0,dispatch matrix partitions to parameter servers
v1.2.0,get matrix ids in the parameter server report
v1.2.0,get the matrices parameter server need to create and delete
v1.2.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
v1.2.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
v1.2.0,waitForMatrixReleaseOnPS(matrixId);
v1.2.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
v1.2.0,the number of splits equal to the number of tasks
v1.2.0,split data
v1.2.0,dispatch the splits to workergroups
v1.2.0,split data
v1.2.0,dispatch the splits to workergroups
v1.2.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.2.0,"first, then divided by expected split number"
v1.2.0,get input format class from configuration and then instantiation a input format object
v1.2.0,split data
v1.2.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.2.0,"first, then divided by expected split number"
v1.2.0,get input format class from configuration and then instantiation a input format object
v1.2.0,split data
v1.2.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.2.0,need to fine tune the number of workergroup and task based on the actual split number
v1.2.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.2.0,Record the location information for the splits in order to data localized schedule
v1.2.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.2.0,need to fine tune the number of workergroup and task based on the actual split number
v1.2.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.2.0,Record the location information for the splits in order to data localized schedule
v1.2.0,write meta data to a temporary file
v1.2.0,rename the temporary file to final file
v1.2.0,"if the file exists, read from file and deserialize it"
v1.2.0,write task meta
v1.2.0,write ps meta
v1.2.0,generate a temporary file
v1.2.0,write task meta to the temporary file first
v1.2.0,rename the temporary file to the final file
v1.2.0,"if last final task file exist, remove it"
v1.2.0,find task meta file which has max timestamp
v1.2.0,"if the file does not exist, just return null"
v1.2.0,read task meta from file and deserialize it
v1.2.0,generate a temporary file
v1.2.0,write ps meta to the temporary file first.
v1.2.0,rename the temporary file to the final file
v1.2.0,"if the old final file exist, just remove it"
v1.2.0,find ps meta file
v1.2.0,"if ps meta file does not exist, just return null"
v1.2.0,read ps meta from file and deserialize it
v1.2.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.2.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.2.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.2.0,create the topology tables
v1.2.0,Transitions from the NEW state.
v1.2.0,PA_FAILMSG
v1.2.0,Transitions from the UNASSIGNED state.
v1.2.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG"
v1.2.0,event
v1.2.0,Transitions from the ASSIGNED state.
v1.2.0,"this happened when launch thread run slowly, and PA_REGISTER event"
v1.2.0,dispatched before PA_CONTAINER_LAUNCHED event
v1.2.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.2.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.2.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.2.0,create the topology tables
v1.2.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will"
v1.2.0,retry another attempt or failed
v1.2.0,release container
v1.2.0,TODO
v1.2.0,set the launch time
v1.2.0,"set tarckerName,httpPort, which used by webserver"
v1.2.0,added to psManager so psManager can monitor it;
v1.2.0,psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);
v1.2.0,set the finish time only if launch time is set
v1.2.0,"ParameterServerJVM.setVMEnv(myEnv, conf);"
v1.2.0,Set up the launch command
v1.2.0,Duplicate the ByteBuffers for access by multiple containers.
v1.2.0,Construct the actual Container
v1.2.0,Application resources
v1.2.0,Application environment
v1.2.0,Service data
v1.2.0,Tokens
v1.2.0,Set up JobConf to be localized properly on the remote NM.
v1.2.0,Setup DistributedCache
v1.2.0,Setup up task credentials buffer
v1.2.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.2.0,Add the env variables passed by the user
v1.2.0,Set logging level in the environment.
v1.2.0,"This is so that, if the child forks another ""bin/hadoop"" (common in"
v1.2.0,streaming) it will have the correct loglevel.
v1.2.0,Setup the log4j prop
v1.2.0,Add main class and its arguments
v1.2.0,Finally add the jvmID
v1.2.0,vargs.add(String.valueOf(jvmID.getId()));
v1.2.0,Final commmand
v1.2.0,Transitions from the NEW state.
v1.2.0,Transitions from the RUNNING state.
v1.2.0,Transitions from the SUCCEEDED state
v1.2.0,Transitions from the KILLED state
v1.2.0,Transitions from the FAILED state
v1.2.0,Transitions from the NEW state.
v1.2.0,Transitions from the SCHEDULED state.
v1.2.0,Transitions from the RUNNING state.
v1.2.0,"another attempt launched,"
v1.2.0,Transitions from the SUCCEEDED state
v1.2.0,Transitions from the KILLED state
v1.2.0,Transitions from the FAILED state
v1.2.0,add diagnostic
v1.2.0,Set up the launch command
v1.2.0,Duplicate the ByteBuffers for access by multiple containers.
v1.2.0,Construct the actual Container
v1.2.0,Application resources
v1.2.0,Application environment
v1.2.0,Service data
v1.2.0,Tokens
v1.2.0,Set up JobConf to be localized properly on the remote NM.
v1.2.0,Setup DistributedCache
v1.2.0,Setup up task credentials buffer
v1.2.0,LocalStorageToken is needed irrespective of whether security is enabled
v1.2.0,or not.
v1.2.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
v1.2.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.2.0,Construct the actual Container
v1.2.0,The null fields are per-container and will be constructed for each
v1.2.0,container separately.
v1.2.0,Set up the launch command
v1.2.0,Duplicate the ByteBuffers for access by multiple containers.
v1.2.0,Construct the actual Container
v1.2.0,"a * in the classpath will only find a .jar, so we need to filter out"
v1.2.0,all .jars and add everything else
v1.2.0,Propagate the system classpath when using the mini cluster
v1.2.0,Add standard Hadoop classes
v1.2.0,Cache archives
v1.2.0,Cache files
v1.2.0,Sanity check
v1.2.0,Add URI fragment or just the filename
v1.2.0,Add the env variables passed by the user
v1.2.0,Set logging level in the environment.
v1.2.0,Setup the log4j prop
v1.2.0,Add main class and its arguments
v1.2.0,Finally add the jvmID
v1.2.0,vargs.add(String.valueOf(jvmID.getId()));
v1.2.0,Final commmand
v1.2.0,Add the env variables passed by the user
v1.2.0,Set logging level in the environment.
v1.2.0,Setup the log4j prop
v1.2.0,Add main class and its arguments
v1.2.0,Final commmand
v1.2.0,"if amTask is not null, we should clone task state from it"
v1.2.0,"if all parameter server complete commit, master can commit now"
v1.2.0,init and start master committer
v1.2.0,Transitions from the NEW state.
v1.2.0,Transitions from the UNASSIGNED state.
v1.2.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
v1.2.0,Transitions from the ASSIGNED state.
v1.2.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
v1.2.0,PA_CONTAINER_LAUNCHED event
v1.2.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.2.0,Transitions from the PSAttemptStateInternal.COMMITTING state
v1.2.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.2.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.2.0,create the topology tables
v1.2.0,reqeuest resource:send a resource request to the resource allocator
v1.2.0,"Once the resource is applied, build and send the launch request to the container launcher"
v1.2.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
v1.2.0,resource allocator
v1.2.0,set the launch time
v1.2.0,add the ps attempt to the heartbeat timeout monitoring list
v1.2.0,parse ps attempt location and put it to location manager
v1.2.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
v1.2.0,or failed
v1.2.0,remove ps attempt id from heartbeat timeout monitor list
v1.2.0,release container:send a release request to container launcher
v1.2.0,set the finish time only if launch time is set
v1.2.0,private long scheduledTime;
v1.2.0,Transitions from the NEW state.
v1.2.0,Transitions from the SCHEDULED state.
v1.2.0,Transitions from the RUNNING state.
v1.2.0,"another attempt launched,"
v1.2.0,Transitions from the SUCCEEDED state
v1.2.0,Transitions from the KILLED state
v1.2.0,Transitions from the FAILED state
v1.2.0,add diagnostic
v1.2.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.2.0,start a new attempt for this ps
v1.2.0,notify ps manager
v1.2.0,add diagnostic
v1.2.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.2.0,start a new attempt for this ps
v1.2.0,notify ps manager
v1.2.0,notify the event handler of state change
v1.2.0,"If new state is not RUNNING and COMMITTING, add it to state timeout monitor"
v1.2.0,"if forcedState is set, just return"
v1.2.0,else get state from state machine
v1.2.0,add this worker group to the success set
v1.2.0,check if all worker group run over
v1.2.0,add this worker group to the failed set
v1.2.0,check if too many worker groups are failed or killed
v1.2.0,notify a run failed event
v1.2.0,add this worker group to the failed set
v1.2.0,check if too many worker groups are failed or killed
v1.2.0,notify a run failed event
v1.2.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
v1.2.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
v1.2.0,just return the total task number now
v1.2.0,TODO
v1.2.0,"if workerAttempt is not null, we should clone task state from it"
v1.2.0,from NEW state
v1.2.0,from SCHEDULED state
v1.2.0,get data splits location for data locality
v1.2.0,reqeuest resource:send a resource request to the resource allocator
v1.2.0,"once the resource is applied, build and send the launch request to the container launcher"
v1.2.0,notify failed message to the worker
v1.2.0,notify killed message to the worker
v1.2.0,release the allocated container
v1.2.0,notify failed message to the worker
v1.2.0,remove the worker attempt from heartbeat timeout listen list
v1.2.0,release the allocated container
v1.2.0,notify killed message to the worker
v1.2.0,remove the worker attempt from heartbeat timeout listen list
v1.2.0,clean the container
v1.2.0,notify failed message to the worker
v1.2.0,remove the worker attempt from heartbeat timeout listen list
v1.2.0,record the finish time
v1.2.0,clean the container
v1.2.0,notify killed message to the worker
v1.2.0,remove the worker attempt from heartbeat timeout listening list
v1.2.0,record the finish time
v1.2.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
v1.2.0,set worker attempt location
v1.2.0,notify the register message to the worker
v1.2.0,record the launch time
v1.2.0,update worker attempt metrics
v1.2.0,update tasks metrics
v1.2.0,clean the container
v1.2.0,notify the worker attempt run successfully message to the worker
v1.2.0,record the finish time
v1.2.0,init a worker attempt for the worker
v1.2.0,schedule the worker attempt
v1.2.0,add diagnostic
v1.2.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.2.0,init and start a new attempt for this ps
v1.2.0,notify worker manager
v1.2.0,add diagnostic
v1.2.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.2.0,init and start a new attempt for this ps
v1.2.0,notify worker manager
v1.2.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
v1.2.0,register to Yarn RM
v1.2.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
v1.2.0,"catch YarnRuntimeException, we should exit and need not retry"
v1.2.0,build heartbeat request
v1.2.0,send heartbeat request to rm
v1.2.0,"This can happen if the RM has been restarted. If it is in that state,"
v1.2.0,this application must clean itself up.
v1.2.0,Setting NMTokens
v1.2.0,assgin containers
v1.2.0,"if some container is not assigned, release them"
v1.2.0,handle finish containers
v1.2.0,dispatch container exit message to corresponding components
v1.2.0,killed by framework
v1.2.0,killed by framework
v1.2.0,killed by framework
v1.2.0,get application finish state
v1.2.0,build application diagnostics
v1.2.0,TODO:add a job history for angel
v1.2.0,build unregister request
v1.2.0,send unregister request to rm
v1.2.0,Note this down for next interaction with ResourceManager
v1.2.0,based on blacklisting comments above we can end up decrementing more
v1.2.0,than requested. so guard for that.
v1.2.0,send the updated resource request to RM
v1.2.0,send 0 container count requests also to cancel previous requests
v1.2.0,Update resource requests
v1.2.0,try to assign to all nodes first to match node local
v1.2.0,try to match all rack local
v1.2.0,assign remaining
v1.2.0,Update resource requests
v1.2.0,send the container-assigned event to task attempt
v1.2.0,build the start container request use launch context
v1.2.0,send the start request to Yarn nm
v1.2.0,send the message that the container starts successfully to the corresponding component
v1.2.0,"after launching, send launched event to task attempt to move"
v1.2.0,it from ASSIGNED to RUNNING state
v1.2.0,send the message that the container starts failed to the corresponding component
v1.2.0,kill the remote container if already launched
v1.2.0,start a thread pool to startup the container
v1.2.0,See if we need up the pool size only if haven't reached the
v1.2.0,maximum limit yet.
v1.2.0,nodes where containers will run at *this* point of time. This is
v1.2.0,*not* the cluster size and doesn't need to be.
v1.2.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
v1.2.0,later is just a buffer so we are not always increasing the
v1.2.0,pool-size
v1.2.0,the events from the queue are handled in parallel
v1.2.0,using a thread pool
v1.2.0,return if already stopped
v1.2.0,shutdown any containers that might be left running
v1.2.0,Build and initialize rpc client to master
v1.2.0,Build local location
v1.2.0,"Initialize matrix info, this method will wait until master accepts the information from"
v1.2.0,client
v1.2.0,Get ps locations from master and put them to the location cache.
v1.2.0,Initialize matrix meta information
v1.2.0,Start heartbeat thread if need
v1.2.0,Start all services
v1.2.0,Register to master first
v1.2.0,Report state to master every specified time
v1.2.0,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.2.0,Stop all modules
v1.2.0,Exit the process if on ANGEL_PS_PSAGENT mode
v1.2.0,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.2.0,Stop all modules
v1.2.0,Exit the process if on ANGEL_PS_PSAGENT mode
v1.2.0,get configuration from config file
v1.2.0,set localDir with enviroment set by nm.
v1.2.0,Update generic resource counters
v1.2.0,Updating resources specified in ResourceCalculatorProcessTree
v1.2.0,Remove the CPU time consumed previously by JVM reuse
v1.2.0,array stores clock for each row and clock
v1.2.0,local task num
v1.2.0,mapping from task index to taskId
v1.2.0,mapping from taskId to task index
v1.2.0,TODO Auto-generated method stub
v1.2.0,Generate a flush request and put it to request queue
v1.2.0,Generate a clock request and put it to request queue
v1.2.0,Generate a merge request and put it to request queue
v1.2.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
v1.2.0,matrix
v1.2.0,and add it to cache maps
v1.2.0,Add the message to the tree map
v1.2.0,"If there are flush / clock requests blocked, we need to put this merge request into"
v1.2.0,the waiting queue
v1.2.0,Launch a merge worker to merge the update to matrix op log cache
v1.2.0,Remove the message from the tree map
v1.2.0,Wake up blocked flush/clock request
v1.2.0,Add flush/clock request to listener list to waiting for all the existing
v1.2.0,updates are merged
v1.2.0,Wake up blocked flush/clock request
v1.2.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
v1.2.0,blocked.
v1.2.0,Get next merge message sequence id
v1.2.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
v1.2.0,position
v1.2.0,Wake up blocked merge requests
v1.2.0,Get minimal sequence id from listeners
v1.2.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
v1.2.0,should flush updates to local matrix storage
v1.2.0,unused now
v1.2.0,Get partitions for the matrix
v1.2.0,"Filter it, removing zero values"
v1.2.0,Doing average or not
v1.2.0,Split this row according the matrix partitions
v1.2.0,Add the splits to the result container
v1.2.0,"For each partition, we generate a update split."
v1.2.0,"Although the split is empty for partitions those without any update data,"
v1.2.0,we still need to generate a update split to update the clock info on ps.
v1.2.0,"For each partition, we generate a update split."
v1.2.0,"Although the split is empty for partitions those without any update data,"
v1.2.0,we still need to generate a update split to update the clock info on ps.
v1.2.0,int seqId = ((ByteBuf) msg).readInt();
v1.2.0,"LOG.info(""receive result of seqId="" + seqId);"
v1.2.0,((ByteBuf) msg).resetReaderIndex();
v1.2.0,TODO: use Epoll for linux future
v1.2.0,closeChannelForServer(request.getServerId());
v1.2.0,closeChannelForServer(request.getServerId());
v1.2.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.2.0,Then submit normal task until reach upper limit of flow control or all tasks are submit
v1.2.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.2.0,"LOG.info(""choose put server "" + psIds[index]);"
v1.2.0,allocate the bytebuf
v1.2.0,"check the location of server is ready, if not, we should wait"
v1.2.0,get a channel to server from pool
v1.2.0,"if channel is not valid, it means maybe the connections to the server are closed"
v1.2.0,channelManager.removeChannelPool(loc);
v1.2.0,find the partition request context from cache
v1.2.0,Check if the result of the sub-request is received
v1.2.0,Update received result number
v1.2.0,Get row splits received
v1.2.0,Put the row split to the cache(row index to row splits map)
v1.2.0,"If all splits of the row are received, means this row can be merged"
v1.2.0,TODO Auto-generated method stub
v1.2.0,"Check futures, if the result of a sub-request is received, put it to the result queue"
v1.2.0,Now we just support pipelined row splits merging for dense type row
v1.2.0,Wait until the clock value of this row is greater than or equal to the value
v1.2.0,Get partitions for this row
v1.2.0,First get this row from matrix storage
v1.2.0,"If the row exists in the matrix storage and the clock value meets the requirements, just"
v1.2.0,return
v1.2.0,Get row splits of this row from the matrix cache first
v1.2.0,"If the row split does not exist in cache, get it from parameter server"
v1.2.0,Wait the final result
v1.2.0,Put it to the matrix cache
v1.2.0,Split the matrix oplog according to the matrix partitions
v1.2.0,"If need update clock, we should send requests to all partitions"
v1.2.0,use update index if exist
v1.2.0,Filter the rowIds which are fetching now
v1.2.0,Send the rowIndex to rpc dispatcher and return immediately
v1.2.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
v1.2.0,Generate dispatch items and add them to the corresponding queues
v1.2.0,Pre-fetching is disable default
v1.2.0,matrix id to clock map
v1.2.0,"task index, it must be unique for whole application"
v1.2.0,if (matrixManager.getMatrixMeta(createResponse.getMatrixId()) != null) {
v1.2.0,return matrixManager.getMatrixMeta(createResponse.getMatrixId());
v1.2.0,}
v1.2.0,Deserialize data splits meta
v1.2.0,Get workers
v1.2.0,Send request to every ps
v1.2.0,Wait the responses
v1.2.0,Update clock cache
v1.2.0,Get row from cache.
v1.2.0,"if row clock is satisfy ssp staleness limit, just return."
v1.2.0,Get row from ps.
v1.2.0,"For ASYNC mode, just get from pss."
v1.2.0,"For BSP/SSP, get rows from storage/cache first"
v1.2.0,Get from ps.
v1.2.0,"For ASYNC, just get rows from pss."
v1.2.0,no more retries.
v1.2.0,calculate sleep time and return.
v1.2.0,parse the i-th sleep-time
v1.2.0,parse the i-th number-of-retries
v1.2.0,calculateSleepTime may overflow.
v1.2.0,"A few common retry policies, with no delays."
v1.2.0,close is a local operation and should finish within milliseconds; timeout just to be safe
v1.2.0,response will be null for one way messages.
v1.2.0,maxFrameLength = 2G
v1.2.0,lengthFieldOffset = 0
v1.2.0,lengthFieldLength = 8
v1.2.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
v1.2.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
v1.2.0,indicates whether this connection's life cycle is managed
v1.2.0,See if we already have a connection (common case)
v1.2.0,create a unique lock for this RS + protocol (if necessary)
v1.2.0,get the RS lock
v1.2.0,do one more lookup in case we were stalled above
v1.2.0,Only create isa when we need to.
v1.2.0,definitely a cache miss. establish an RPC for
v1.2.0,this RS
v1.2.0,Throw what the RemoteException was carrying.
v1.2.0,check
v1.2.0,every
v1.2.0,minutes
v1.2.0,TODO
v1.2.0,创建failoverHandler
v1.2.0,"The number of times this invocation handler has ever been failed over,"
v1.2.0,before this method invocation attempt. Used to prevent concurrent
v1.2.0,failed method invocations from triggering multiple failover attempts.
v1.2.0,Make sure that concurrent failed method invocations
v1.2.0,only cause a
v1.2.0,single actual fail over.
v1.2.0,RpcController + Message in the method args
v1.2.0,(generated code from RPC bits in .proto files have
v1.2.0,RpcController)
v1.2.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
v1.2.0,+ (System.currentTimeMillis() - beforeConstructTs));
v1.2.0,get an instance of the method arg type
v1.2.0,RpcController + Message in the method args
v1.2.0,(generated code from RPC bits in .proto files have
v1.2.0,RpcController)
v1.2.0,Message (hand written code usually has only a single
v1.2.0,argument)
v1.2.0,log any RPC responses that are slower than the configured
v1.2.0,warn
v1.2.0,response time or larger than configured warning size
v1.2.0,"when tagging, we let TooLarge trump TooSmall to keep"
v1.2.0,output simple
v1.2.0,note that large responses will often also be slow.
v1.2.0,provides a count of log-reported slow responses
v1.2.0,RpcController + Message in the method args
v1.2.0,(generated code from RPC bits in .proto files have
v1.2.0,RpcController)
v1.2.0,unexpected
v1.2.0,"in the protobuf methods, args[1] is the only significant argument"
v1.2.0,for JSON encoding
v1.2.0,base information that is reported regardless of type of call
v1.2.0,Disable Nagle's Algorithm since we don't want packets to wait
v1.2.0,Configure the event pipeline factory.
v1.2.0,Make a new connection.
v1.2.0,Remove all pending requests (will be canceled after relinquishing
v1.2.0,write lock).
v1.2.0,Cancel any pending requests by sending errors to the callbacks:
v1.2.0,Close the channel:
v1.2.0,Close the connection:
v1.2.0,Shut down all thread pools to exit.
v1.2.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
v1.2.0,See NettyServer.prepareResponse for where we write out the response.
v1.2.0,"It writes the call.id (int), a boolean signifying any error (and if"
v1.2.0,"so the exception name/trace), and the response bytes"
v1.2.0,Read the call id.
v1.2.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
v1.2.0,"instead, it returns a null message object."
v1.2.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
v1.2.0,System.currentTimeMillis());
v1.2.0,"It would be good widen this to just Throwable, but IOException is what we"
v1.2.0,allow now
v1.2.0,not implemented
v1.2.0,not implemented
v1.2.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
v1.2.0,cache of RpcEngines by protocol
v1.2.0,return the RpcEngine configured to handle a protocol
v1.2.0,We only handle the ConnectException.
v1.2.0,This is the exception we can't handle.
v1.2.0,check if timed out
v1.2.0,wait for retry
v1.2.0,IGNORE
v1.2.0,return the RpcEngine that handles a proxy object
v1.2.0,The default implementation works synchronously
v1.2.0,punt: allocate a new buffer & copy into it
v1.2.0,"LOG.info(System.getProperty(""user.dir""));"
v1.2.0,get tokens for all the required FileSystems..
v1.2.0,Whether we need to recursive look into the directory structure
v1.2.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.2.0,user provided one (if any).
v1.2.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.2.0,get tokens for all the required FileSystems..
v1.2.0,Whether we need to recursive look into the directory structure
v1.2.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.2.0,user provided one (if any).
v1.2.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.2.0,a simple hdfs copy function assume src path and dest path are in same hdfs
v1.2.0,and FileSystem object has same schema
v1.2.0,"LOG.warn(""interrupted while sleeping"", ie);"
v1.2.0,private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);
v1.2.0,public static String getHostname() {
v1.2.0,try {
v1.2.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
v1.2.0,} catch (UnknownHostException uhe) {
v1.2.0,}
v1.2.0,"return new StringBuilder().append("""").append(uhe).toString();"
v1.2.0,}
v1.2.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
v1.2.0,String hostname = getHostname();
v1.2.0,String classname = clazz.getSimpleName();
v1.2.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
v1.2.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
v1.2.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
v1.2.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
v1.2.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
v1.2.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
v1.2.0,
v1.2.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
v1.2.0,public void run() {
v1.2.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
v1.2.0,"this.val$classname + "" at "" + this.val$hostname}));"
v1.2.0,}
v1.2.0,});
v1.2.0,}
v1.2.0,"We we interrupted because we're meant to stop? If not, just"
v1.2.0,continue ignoring the interruption
v1.2.0,Recalculate waitTime.
v1.2.0,// Begin delegation to Thread
v1.2.0,// End delegation to Thread
v1.2.0,load hadoop configuration
v1.2.0,load angel system configuration
v1.2.0,load user configuration:
v1.2.0,load user config file
v1.2.0,load command line parameters
v1.2.0,load user job resource files
v1.2.0,load user job jar if it exist
v1.2.0,Expand the environment variable
v1.2.0,instance submitter class
v1.2.0,Add default fs(local fs) for lib jars.
v1.2.0,Obtain filename from path
v1.2.0,Split filename to prexif and suffix (extension)
v1.2.0,Check if the filename is okay
v1.2.0,Prepare temporary file
v1.2.0,Prepare buffer for data copying
v1.2.0,Open and check input stream
v1.2.0,Open output stream and copy data between source file in JAR and the temporary file
v1.2.0,"If read/write fails, close streams safely before throwing an exception"
v1.2.0,"Finally, load the library"
v1.2.0,little endian load order
v1.2.0,tail
v1.2.0,fallthrough
v1.2.0,fallthrough
v1.2.0,finalization
v1.2.0,fmix(h1);
v1.2.0,----------
v1.2.0,body
v1.2.0,----------
v1.2.0,tail
v1.2.0,----------
v1.2.0,finalization
v1.2.0,----------
v1.2.0,body
v1.2.0,----------
v1.2.0,tail
v1.2.0,----------
v1.2.0,finalization
v1.2.0,JobStateProto jobState = report.getJobState();
v1.2.0,the leaf level file should be readable by others
v1.2.0,the subdirs in the path should have execute permissions for
v1.2.0,others
v1.2.0,2.get job id
v1.2.0,Credentials credentials = new Credentials();
v1.2.0,4.copy resource files to hdfs
v1.2.0,5.write configuration to a xml file
v1.2.0,6.create am container context
v1.2.0,7.Submit to ResourceManager
v1.2.0,8.get app master client
v1.2.0,Create a number of filenames in the JobTracker's fs namespace
v1.2.0,add all the command line files/ jars and archive
v1.2.0,first copy them to jobtrackers filesystem
v1.2.0,should not throw a uri exception
v1.2.0,should not throw an uri excpetion
v1.2.0,set the timestamps of the archives and files
v1.2.0,set the public/private visibility of the archives and files
v1.2.0,get DelegationToken for each cached file
v1.2.0,check if we do not need to copy the files
v1.2.0,is jt using the same file system.
v1.2.0,just checking for uri strings... doing no dns lookups
v1.2.0,to see if the filesystems are the same. This is not optimal.
v1.2.0,but avoids name resolution.
v1.2.0,this might have name collisions. copy will throw an exception
v1.2.0,parse the original path to create new path
v1.2.0,check for ports
v1.2.0,Write job file to JobTracker's fs
v1.2.0,Setup resource requirements
v1.2.0,Setup LocalResources
v1.2.0,Setup security tokens
v1.2.0,Setup the command to run the AM
v1.2.0,Add AM user command opts
v1.2.0,Final command
v1.2.0,Setup the CLASSPATH in environment
v1.2.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
v1.2.0,Setup the environment variables for Admin first
v1.2.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
v1.2.0,Parse distributed cache
v1.2.0,Setup ContainerLaunchContext for AM container
v1.2.0,Set up the ApplicationSubmissionContext
v1.2.0,resposne.encode(buf);
v1.2.0,TODO:
v1.2.0,resposne.encode(buf);
v1.2.0,TODO:
v1.2.0,resposne.encode(buf);
v1.2.0,TODO:
v1.2.0,resposne.encode(buf);
v1.2.0,TODO:
v1.2.0,Add tokens to new user so that it may execute its task correctly.
v1.2.0,to exit
v1.2.0,mkdir does not throw exception if path exits
v1.2.0,commitTaskPool.shutdown();
v1.2.0,"LOG.info(""rowId = "" + rowId + "" rowType = "" + rowType + "" size = "" + size + "" request "" +"
v1.2.0,"""update"");"
v1.2.0,private final ParameterServer psServer;
v1.2.0,TODO
v1.2.0,"when we should write snapshot to hdfs? clearly, we have two methods:"
v1.2.0,"1. write snapshot at regular time, if there are updates, just write them."
v1.2.0,"2. write snapshot every N iterations, this method depends on notification of master"
v1.2.0,"FSDataOutputStream output = fileContext.create(snapshotsTempFilePath,"
v1.2.0,EnumSet.of(CreateFlag.CREATE));
v1.2.0,@brief get filename of the old snapshot written before
v1.2.0,"no snapshotFile write before, maybe write snapshots the first time"
v1.2.0,start end
v1.2.0,rowtype
v1.2.0,data.rewind();
v1.2.0,data.rewind();
v1.2.0,data.rewind();
v1.2.0,Pass the matrix and partition number field
v1.2.0,Mapping from taskId to clock value.
v1.2.0,int[] keys = sparseRep.getKeys();
v1.2.0,int[] values = sparseRep.getValues();
v1.2.0,boolean[] used = sparseRep.getUsed();
v1.2.0,nnz = 0;
v1.2.0,for (int i = 0; i < keys.length; i++)
v1.2.0,if (used[i]) {
v1.2.0,"denseRep.put(keys[i], values[i]);"
v1.2.0,nnz++;
v1.2.0,}
v1.2.0,sparseRep = null;
v1.2.0,int[] keys = sparseRep.getKeys();
v1.2.0,int[] values = sparseRep.getValues();
v1.2.0,boolean[] used = sparseRep.getUsed();
v1.2.0,for (int i = 0; i < keys.length; i++)
v1.2.0,if (used[i]) {
v1.2.0,"denseRep.put(keys[i], values[i]);"
v1.2.0,}
v1.2.0,sparseRep = null;
v1.2.0,output.writeInt(data.length);
v1.2.0,@Override
v1.2.0,public void serialize(ByteBuf buf) {
v1.2.0,if (sparseRep != null)
v1.2.0,return serializeSparse();
v1.2.0,else if (denseRep != null)
v1.2.0,return serializeDense();
v1.2.0,return serializeEmpty();
v1.2.0,}
v1.2.0,int[] keys = sparseRep.getKeys();
v1.2.0,int[] values = sparseRep.getValues();
v1.2.0,boolean[] used = sparseRep.getUsed();
v1.2.0,int idx = 0;
v1.2.0,for (int i = 0; i < keys.length; i++)
v1.2.0,if (used[i]) {
v1.2.0,"keysBuf.put(idx, keys[i]);"
v1.2.0,"valuesBuf.put(idx, values[i]);"
v1.2.0,idx++;
v1.2.0,}
v1.2.0,int[] keys = sparseRep.getKeys();
v1.2.0,int[] values = sparseRep.getValues();
v1.2.0,boolean[] used = sparseRep.getUsed();
v1.2.0,"int ov, k, v;"
v1.2.0,for (int i = 0; i < keys.length; i++) {
v1.2.0,if (used[i]) {
v1.2.0,k = keys[i];
v1.2.0,ov = denseRep.get(k);
v1.2.0,v = ov + values[i];
v1.2.0,"denseRep.put(k, v);"
v1.2.0,if (ov != 0 && v == 0)
v1.2.0,nnz--;
v1.2.0,}
v1.2.0,}
v1.2.0,"add the PSAgentContext,need fix"
v1.2.0,set MatrixPartitionLocation
v1.2.0,set attribute
v1.2.0,TODO:
v1.2.0,write the max abs
v1.2.0,TODO Auto-generated method stub
v1.2.0,TODO Auto-generated method stub
v1.2.0,TODO Auto-generated method stub
v1.2.0,get configuration from config file
v1.2.0,set localDir with enviroment set by nm.
v1.2.0,get master location
v1.2.0,init task manager and start tasks
v1.2.0,start heartbeat thread
v1.2.0,taskManager.assignTaskIds(response.getTaskidsList());
v1.2.0,todo
v1.2.0,"if worker timeout, it may be knocked off."
v1.2.0,"SUCCESS, do nothing"
v1.2.0,heartbeatFailedTime = 0;
v1.2.0,private KEY currentKey;
v1.2.0,will be created
v1.2.0,TODO Auto-generated method stub
v1.2.0,Bitmap bitmap = new Bitmap();
v1.2.0,int max = indexArray[size - 1];
v1.2.0,byte [] bitIndexArray = new byte[max / 8 + 1];
v1.2.0,for(int i = 0; i < size; i++){
v1.2.0,int bitIndex = indexArray[i] >> 3;
v1.2.0,int bitOffset = indexArray[i] - (bitIndex << 3);
v1.2.0,switch(bitOffset){
v1.2.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
v1.2.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
v1.2.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
v1.2.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
v1.2.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
v1.2.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
v1.2.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
v1.2.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
v1.2.0,}
v1.2.0,}
v1.2.0,//////////////////////////////
v1.2.0,Application Configs
v1.2.0,//////////////////////////////
v1.2.0,//////////////////////////////
v1.2.0,Master Configs
v1.2.0,//////////////////////////////
v1.2.0,//////////////////////////////
v1.2.0,Worker Configs
v1.2.0,//////////////////////////////
v1.2.0,//////////////////////////////
v1.2.0,Task Configs
v1.2.0,//////////////////////////////
v1.2.0,//////////////////////////////
v1.2.0,ParameterServer Configs
v1.2.0,//////////////////////////////
v1.2.0,////////////////// IPC //////////////////////////
v1.2.0,//////////////////////////////
v1.2.0,Matrix transfer Configs.
v1.2.0,//////////////////////////////
v1.2.0,//////////////////////////////
v1.2.0,Matrix transfer Configs.
v1.2.0,//////////////////////////////
v1.2.0,Configs used to ANGEL_PS_PSAGENT running mode future.
v1.2.0,model parse
v1.2.0,private Configuration conf;
v1.2.0,TODO Auto-generated constructor stub
v1.2.0,Feature number of train data
v1.2.0,Number of nonzero features
v1.2.0,Tree number
v1.2.0,Tree depth
v1.2.0,Split number
v1.2.0,Feature sample ratio
v1.2.0,Data format
v1.2.0,Learning rate
v1.2.0,Set basic configuration keys
v1.2.0,Use local deploy mode and dummy data spliter
v1.2.0,"set input, output path"
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,Set GBDT algorithm parameters
v1.2.0,Submit GBDT Train Task
v1.2.0,Load Model from HDFS.
v1.2.0,set basic configuration keys
v1.2.0,use local deploy mode and dummy dataspliter
v1.2.0,get a angel client
v1.2.0,add matrix
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,"Set trainning data, save model, log path"
v1.2.0,Set actionType train
v1.2.0,Set MF algorithm parameters
v1.2.0,Feature number of train data
v1.2.0,Total iteration number
v1.2.0,Validation sample Ratio
v1.2.0,"Data format, libsvm or dummy"
v1.2.0,Train batch number per epoch.
v1.2.0,Batch number
v1.2.0,Learning rate
v1.2.0,Decay of learning rate
v1.2.0,Regularization coefficient
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,Set data format
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,set sgd LR algorithm parameters #feature #epoch
v1.2.0,Set trainning data path
v1.2.0,Set save model path
v1.2.0,Set log path
v1.2.0,Set actionType train
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set save model path
v1.2.0,Set log path
v1.2.0,Set actionType incremental train
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set predict result path
v1.2.0,Set actionType prediction
v1.2.0,LOG.info(sigmoid(data[i]));
v1.2.0,LOG.info(Math.exp(-data[i]));
v1.2.0,when b is a negative number
v1.2.0,Feature number of train data
v1.2.0,Total iteration number
v1.2.0,Validation sample Ratio
v1.2.0,"Data format, libsvm or dummy"
v1.2.0,Train batch number per epoch.
v1.2.0,Batch number
v1.2.0,Learning rate
v1.2.0,Decay of learning rate
v1.2.0,Regularization coefficient
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,Set data format
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,set MLR algorithm parameters #feature #epoch
v1.2.0,Set trainning data path
v1.2.0,Set save model path
v1.2.0,Set log path
v1.2.0,Set actionType train
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set save model path
v1.2.0,Set log path
v1.2.0,Set actionType incremental train
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set predict result path
v1.2.0,Set actionType prediction
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,Set actionType train
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,parseSparseDouble();
v1.2.0,parseDenseFloat();
v1.2.0,parseDenseInt();
v1.2.0,parseSparseInt();
v1.2.0,Set model path
v1.2.0,Set model path
v1.2.0,Set model path
v1.2.0,Set model path
v1.2.0,Set model path
v1.2.0,Feature number of train data
v1.2.0,Total iteration number
v1.2.0,Rank
v1.2.0,Regularization parameters
v1.2.0,Learn rage
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,set FM algorithm parameters #feature #epoch
v1.2.0,Set trainning data path
v1.2.0,Set save model path
v1.2.0,Set log path
v1.2.0,Set actionType train
v1.2.0,Set trainning data path
v1.2.0,Set save model path
v1.2.0,Set log path
v1.2.0,Set actionType train
v1.2.0,Set learnType
v1.2.0,Set feature number
v1.2.0,Cluster center number
v1.2.0,Feature number of train data
v1.2.0,Total iteration number
v1.2.0,Sample ratio per mini-batch
v1.2.0,C
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,set Kmeans algorithm parameters #cluster #feature #epoch
v1.2.0,Set data format
v1.2.0,Set trainning data path
v1.2.0,Set save model path
v1.2.0,Set log sava path
v1.2.0,Set actionType train
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set predict result path
v1.2.0,Set actionType prediction
v1.2.0,Feature number of train data
v1.2.0,Number of nonzero features
v1.2.0,Tree number
v1.2.0,Tree depth
v1.2.0,Split number
v1.2.0,Feature sample ratio
v1.2.0,Data format
v1.2.0,Learning rate
v1.2.0,Set basic configuration keys
v1.2.0,Use local deploy mode and dummy data spliter
v1.2.0,"set input, output path"
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,Set GBDT algorithm parameters
v1.2.0,Submit GBDT Train Task
v1.2.0,Load Model from HDFS.
v1.2.0,Feature number of train data
v1.2.0,Total iteration number
v1.2.0,Validation Ratio
v1.2.0,Data format
v1.2.0,Train batch number per epoch.
v1.2.0,Learning rate
v1.2.0,Decay of learning rate
v1.2.0,Regularization coefficient
v1.2.0,Set basic configuration keys
v1.2.0,Set data format
v1.2.0,Use local deploy mode
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,set sgd SVM algorithm parameters
v1.2.0,"set input, output path"
v1.2.0,Set save model path
v1.2.0,Set actionType train
v1.2.0,Set log path
v1.2.0,Submit LR Train Task
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set save model path
v1.2.0,Set actionType incremental train
v1.2.0,Set log path
v1.2.0,Feature number of train data
v1.2.0,Total iteration number
v1.2.0,Validation sample Ratio
v1.2.0,"Data format, libsvm or dummy"
v1.2.0,Train batch number per epoch.
v1.2.0,Learning rate
v1.2.0,Decay of learning rate
v1.2.0,Regularization coefficient
v1.2.0,Set local deploy mode
v1.2.0,Set basic configuration keys
v1.2.0,Set data format
v1.2.0,"set angel resource parameters #worker, #task, #PS"
v1.2.0,set sgd LR algorithm parameters #feature #epoch
v1.2.0,Set trainning data path
v1.2.0,Set save model path
v1.2.0,Set log path
v1.2.0,Set actionType train
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set save model path
v1.2.0,Set actionType incremental train
v1.2.0,Set log path
v1.2.0,Set trainning data path
v1.2.0,Set load model path
v1.2.0,Set predict result path
v1.2.0,Set log sava path
v1.2.0,Set actionType prediction
v1.2.0,double z=pre*y;
v1.2.0,if(z<=0) return 0.5-z;
v1.2.0,"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);"
v1.2.0,return 0.0;
v1.2.0,if (pre * y <= 0)
v1.2.0,return y;
v1.2.0,else if (pre * y > 0 && pre * y < 1)
v1.2.0,return (1 - pre * y) * y;
v1.2.0,return 0.0;
v1.2.0,logistic loss for binary classification task.
v1.2.0,"logistic loss, but predict un-transformed margin"
v1.2.0,check if label in range
v1.2.0,return the default evaluation metric for the objective
v1.2.0,TODO Auto-generated method stub
v1.2.0,start row index for words
v1.2.0,doc ids
v1.2.0,topic assignments
v1.2.0,count word
v1.2.0,build word start index
v1.2.0,build dks
v1.2.0,The starting point
v1.2.0,There's always an unused entry.
v1.2.0,The starting point
v1.2.0,There's always an unused entry.
v1.2.0,The starting point
v1.2.0,There's always an unused entry.
v1.2.0,The starting point
v1.2.0,There's always an unused entry.
v1.2.0,The starting point
v1.2.0,There's always an unused entry.
v1.2.0,The starting point
v1.2.0,There's always an unused entry.
v1.2.0,print();
v1.2.0,The starting point
v1.2.0,There's always an unused entry.
v1.2.0,The starting point
v1.2.0,There's always an unused entry.
v1.2.0,The starting point
v1.2.0,There's always an unused entry.
v1.2.0,The starting point
v1.2.0,There's always an unused entry.
v1.2.0,print();
v1.2.0,Write #rows
v1.2.0,Write each row
v1.2.0,dense
v1.2.0,sparse
v1.2.0,LOG.info(buf.refCnt());
v1.2.0,dense
v1.2.0,sparse
v1.2.0,LOG.info(buf.refCnt());
v1.2.0,loss function
v1.2.0,gradient and hessian
v1.2.0,tree node
v1.2.0,initialize the phase
v1.2.0,current tree and depth
v1.2.0,create loss function
v1.2.0,calculate grad info of each instance
v1.2.0,"create data sketch, push candidate split value to PS"
v1.2.0,1. calculate candidate split value
v1.2.0,2. push local sketch to PS
v1.2.0,3. set phase to GET_SKETCH
v1.2.0,"pull the global sketch from PS, only called once by each worker"
v1.2.0,sample feature
v1.2.0,push sampled feature set to the current tree
v1.2.0,create new tree
v1.2.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
v1.2.0,calculate gradient
v1.2.0,"1. create new tree, initialize tree nodes and node stats"
v1.2.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
v1.2.0,2.1. pull the sampled features of the current tree
v1.2.0,"2.2. if use all the features, only called one"
v1.2.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
v1.2.0,4. set root node to active
v1.2.0,"5. reset instance position, set the root node's span"
v1.2.0,6. calculate gradient
v1.2.0,7. set phase to run active
v1.2.0,1. start threads of active tree nodes
v1.2.0,1.1. start threads for active nodes to generate histogram
v1.2.0,1.2. set thread status to batch num
v1.2.0,1.3. set the oplog to active
v1.2.0,"2. check thread stats, if all threads finish, return"
v1.2.0,clock
v1.2.0,find split
v1.2.0,"1. find responsible tree node, using RR scheme"
v1.2.0,2. pull gradient histogram
v1.2.0,2.1. get the name of this node's gradient histogram on PS
v1.2.0,2.2. pull the histogram
v1.2.0,histogram = (TDoubleVector) ((GetRowResult) histMat.get(func)).getRow();
v1.2.0,2.3. find best split result of this tree node
v1.2.0,2.3.1 using server split
v1.2.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.2.0,update the grad stats of children node
v1.2.0,update the left child
v1.2.0,update the right child
v1.2.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
v1.2.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
v1.2.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
v1.2.0,2.3.5 reset this tree node's gradient histogram to 0
v1.2.0,3. push split feature to PS
v1.2.0,4. push split value to PS
v1.2.0,5. push split gain to PS
v1.2.0,6. set phase to AFTER_SPLIT
v1.2.0,clock
v1.2.0,1. get split feature
v1.2.0,2. get split value
v1.2.0,3. get split gain
v1.2.0,4. get node weight
v1.2.0,5. split node
v1.2.0,"2. check thread stats, if all threads finish, return"
v1.2.0,6. clock
v1.2.0,"split the span of one node, reset the instance position"
v1.2.0,in case this worker has no instance on this node
v1.2.0,set the span of left child
v1.2.0,set the span of right child
v1.2.0,"1. left to right, find the first instance that should be in the right child"
v1.2.0,"2. right to left, find the first instance that should be in the left child"
v1.2.0,3. swap two instances
v1.2.0,4. find the cut pos
v1.2.0,than the split value
v1.2.0,5. set the span of left child
v1.2.0,6. set the span of right child
v1.2.0,set tree node to active
v1.2.0,set node to leaf
v1.2.0,set node to inactive
v1.2.0,finish current tree
v1.2.0,finish current depth
v1.2.0,set the tree phase
v1.2.0,check if there is active node
v1.2.0,check if finish all the tree
v1.2.0,update node's grad stats on PS
v1.2.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
v1.2.0,the root node's stats is updated by leader worker
v1.2.0,1. create the update
v1.2.0,2. push the update to PS
v1.2.0,the leader task adds node prediction to flush list
v1.2.0,1. name of this node's grad histogram on PS
v1.2.0,2. build the grad histogram of this node
v1.2.0,3. push the histograms to PS
v1.2.0,4. reset thread stats to finished
v1.2.0,5.1. set the children nodes of this node
v1.2.0,5.2. set split info and grad stats to this node
v1.2.0,5.2. create children nodes
v1.2.0,"5.3. create node stats for children nodes, and add them to the tree"
v1.2.0,5.4. reset instance position
v1.2.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
v1.2.0,5.6. set children nodes to leaf nodes
v1.2.0,5.7. set nid to leaf node
v1.2.0,5.8. deactivate active node
v1.2.0,"get feature type, 0:empty 1:all equal 2:real"
v1.2.0,feature index used to split
v1.2.0,feature value used to split
v1.2.0,loss change after split this node
v1.2.0,grad stats of the left child
v1.2.0,grad stats of the right child
v1.2.0,"LOG.info(""Constructor with fid = -1"");"
v1.2.0,fid = -1: no split currently
v1.2.0,the minimal split value is the minimal value of feature
v1.2.0,the splits do not include the maximal value of feature
v1.2.0,"1. the average distance, (maxValue - minValue) / splitNum"
v1.2.0,2. calculate the candidate split value
v1.2.0,1. new feature's histogram (grad + hess)
v1.2.0,size: sampled_featureNum * (2 * splitNum)
v1.2.0,"in other words, concatenate each feature's histogram"
v1.2.0,2. get the span of this node
v1.2.0,int nodeStart = this.controller.nodePosStart[nid];
v1.2.0,int nodeEnd = this.controller.nodePosEnd[nid];
v1.2.0,------ 3. using sparse-aware method to build histogram ---
v1.2.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
v1.2.0,3.1. get the instance index
v1.2.0,3.2. get the grad and hess of the instance
v1.2.0,3.3. add to the sum
v1.2.0,3.4. loop the non-zero entries
v1.2.0,3.4.1. get feature value
v1.2.0,3.4.2. current feature's position in the sampled feature set
v1.2.0,3.4.3. find the position of feature value in a histogram
v1.2.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
v1.2.0,3.4.4. add the grad and hess to the corresponding bin
v1.2.0,3.4.5. add the reverse to the bin that contains 0.0f
v1.2.0,4. add the grad and hess sum to the zero bin of all features
v1.2.0,int startIdx = fid * 2 * splitNum;
v1.2.0,find the best split result of the histogram of a tree node
v1.2.0,1. calculate the gradStats of the root node
v1.2.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
v1.2.0,2. loop over features
v1.2.0,2.1. get the ture feature id in the sampled feature set
v1.2.0,2.2. get the indexes of histogram of this feature
v1.2.0,2.3. find the best split of current feature
v1.2.0,2.4. update the best split result if possible
v1.2.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.2.0,3. update the grad stats of children node
v1.2.0,3.1. update the left child
v1.2.0,3.2. update the right child
v1.2.0,find the best split result of one feature
v1.2.0,1. set the feature id
v1.2.0,2. create the best left stats and right stats
v1.2.0,3. the gain of the root node
v1.2.0,4. create the temp left and right grad stats
v1.2.0,5. loop over all the data in histogram
v1.2.0,5.1. get the grad and hess of current hist bin
v1.2.0,5.2. check whether we can split with current left hessian
v1.2.0,right = root - left
v1.2.0,5.3. check whether we can split with current right hessian
v1.2.0,5.4. calculate the current loss gain
v1.2.0,5.5. check whether we should update the split result with current loss gain
v1.2.0,split value = sketches[splitIdx+1]
v1.2.0,"5.6. if should update, also update the best left and right grad stats"
v1.2.0,6. set the best left and right grad stats
v1.2.0,partition number
v1.2.0,cols of each partition
v1.2.0,"// update the grad stats of the root node on PS, only called once by leader worker"
v1.2.0,if (this.nid == 0) {
v1.2.0,GradStats rootStats = new GradStats(splitEntry.leftGradStat);
v1.2.0,rootStats.add(splitEntry.rightGradStat);
v1.2.0,"this.controller.updateNodeGradStats(this.nid, rootStats);"
v1.2.0,}
v1.2.0,
v1.2.0,// 3. update the grad stats of children node
v1.2.0,if (splitEntry.fid != -1) {
v1.2.0,// 3.1. update the left child
v1.2.0,"this.controller.updateNodeGradStats(2 * this.nid + 1, splitEntry.leftGradStat);"
v1.2.0,// 3.2. update the right child
v1.2.0,"this.controller.updateNodeGradStats(2 * this.nid + 2, splitEntry.rightGradStat);"
v1.2.0,}
v1.2.0,1. calculate the total grad sum and hess sum
v1.2.0,2. create the grad stats of the node
v1.2.0,1. calculate the total grad sum and hess sum
v1.2.0,2. create the grad stats of the node
v1.2.0,1. calculate the total grad sum and hess sum
v1.2.0,2. create the grad stats of the node
v1.2.0,"loop all the possible split value, start from split[1], since the first item is the minimal"
v1.2.0,feature value
v1.2.0,find the best split result of the histogram of a tree node
v1.2.0,2.2. get the indexes of histogram of this feature
v1.2.0,2.3. find the best split of current feature
v1.2.0,2.4. update the best split result if possible
v1.2.0,find the best split result of one feature
v1.2.0,1. set the feature id
v1.2.0,splitEntry.setFid(fid);
v1.2.0,2. create the best left stats and right stats
v1.2.0,3. the gain of the root node
v1.2.0,4. create the temp left and right grad stats
v1.2.0,5. loop over all the data in histogram
v1.2.0,5.1. get the grad and hess of current hist bin
v1.2.0,5.2. check whether we can split with current left hessian
v1.2.0,right = root - left
v1.2.0,5.3. check whether we can split with current right hessian
v1.2.0,5.4. calculate the current loss gain
v1.2.0,5.5. check whether we should update the split result with current loss gain
v1.2.0,"5.6. if should update, also update the best left and right grad stats"
v1.2.0,6. set the best left and right grad stats
v1.2.0,find the best split result of a serve row on the PS
v1.2.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.2.0,2.2. get the start index in histogram of this feature
v1.2.0,2.3. find the best split of current feature
v1.2.0,2.4. update the best split result if possible
v1.2.0,"find the best split result of one feature from a server row, used by the PS"
v1.2.0,1. set the feature id
v1.2.0,2. create the best left stats and right stats
v1.2.0,3. the gain of the root node
v1.2.0,4. create the temp left and right grad stats
v1.2.0,5. loop over all the data in histogram
v1.2.0,5.1. get the grad and hess of current hist bin
v1.2.0,5.2. check whether we can split with current left hessian
v1.2.0,right = root - left
v1.2.0,5.3. check whether we can split with current right hessian
v1.2.0,5.4. calculate the current loss gain
v1.2.0,5.5. check whether we should update the split result with current loss gain
v1.2.0,"here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
v1.2.0,the task use index to find fvalue
v1.2.0,"5.6. if should update, also update the best left and right grad stats"
v1.2.0,6. set the best left and right grad stats
v1.2.0,clear all the information
v1.2.0,calculate the sum of gradient and hess
v1.2.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v1.2.0,ridx)
v1.2.0,check if necessary information is ready
v1.2.0,"same as add, reduce is used in All Reduce"
v1.2.0,"features used in this tree, if equals null, means use all the features without sampling"
v1.2.0,node in the tree
v1.2.0,the gradient info of each instances
v1.2.0,initialize nodes
v1.2.0,gradient
v1.2.0,second order gradient
v1.2.0,int sendStartCol = startFid * 7; // each split contains 7 doubles
v1.2.0,1. calculate the total grad sum and hess sum
v1.2.0,2. create the grad stats of the node
v1.2.0,find the best split result of a serve row on the PS
v1.2.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.2.0,2.2. get the start index in histogram of this feature
v1.2.0,2.3. find the best split of current feature
v1.2.0,2.4. update the best split result if possible
v1.2.0,"find the best split result of one feature from a server row, used by the PS"
v1.2.0,1. set the feature id
v1.2.0,2. create the best left stats and right stats
v1.2.0,3. the gain of the root node
v1.2.0,4. create the temp left and right grad stats
v1.2.0,5. loop over all the data in histogram
v1.2.0,5.1. get the grad and hess of current hist bin
v1.2.0,5.2. check whether we can split with current left hessian
v1.2.0,right = root - left
v1.2.0,5.3. check whether we can split with current right hessian
v1.2.0,5.4. calculate the current loss gain
v1.2.0,5.5. check whether we should update the split result with current loss gain
v1.2.0,"tips: here we set the fvalue=splitIndex, true split value = sketches[splitIdx+1]"
v1.2.0,the task use index to find fvalue
v1.2.0,"5.6. if should update, also update the best left and right grad stats"
v1.2.0,6. set the best left and right grad stats
v1.2.0,grad.timesBy(-1.0 * lr);
v1.2.0,"System.out.println(""Quantile sketch indices: "" + Arrays.toString(qSketch.getValues()));"
v1.2.0,"System.out.println(""Max: "" + qSketch.max() + "", min: "" + qSketch.min());"
v1.2.0,"System.out.println(""Quantile sketch count: "" + Arrays.toString(qSketch.getCounts()));"
v1.2.0,"System.out.println(""Zero index: "" + qSketch.getZeroIndex() + "", """
v1.2.0,"+ qSketch.get(qSketch.getZeroIndex()) + "", "" + qSketch.get(qSketch.getZeroIndex()-1));"
v1.2.0,cmSketch.distribution();
v1.2.0,"write2File(cmSketch.getTable(0), ""E:\\dropbox\\code\\github\\sketchML\\table0"");"
v1.2.0,"write2File(cmSketch.getTable(1), ""E:\\dropbox\\code\\github\\sketchML\\table1"");"
v1.2.0,"write2File(cmSketch.getTable(2), ""E:\\dropbox\\code\\github\\sketchML\\table2"");"
v1.2.0,"System.out.println(""true freq: "" + trueFreq + "", sketch freq: "" + cmFreq);"
v1.2.0,"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + (- qSketch.get(cmFreq)));"
v1.2.0,ratioArr[ratio]++;
v1.2.0,"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(cmFreq));"
v1.2.0,"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(cmFreq));"
v1.2.0,ratioArr[ratio]++;
v1.2.0,System.out.println(Arrays.toString(distArr));
v1.2.0,System.out.println(Arrays.toString(ratioArr));
v1.2.0,"System.out.println(""Nnz grad: "" + nnz +"", zero grad: "" + zeroGrad + "", negative grad: "" + negCount + "", larger grad: "" + largeCount + "", smaller grad: "" + smallCount);"
v1.2.0,"write2File(distArr, ""E:\\dropbox\\code\\github\\sketchML\\error_hist"");"
v1.2.0,for (int i = 0; i < grad.getDimension(); i++) {
v1.2.0,"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(qSketch.indexOf(grad.get(i))));"
v1.2.0,"grad.set(i, qSketch.get(qSketch.indexOf(grad.get(i))));"
v1.2.0,}
v1.2.0,"System.out.println(""Cur index of rIndex: "" + Arrays.toString(curIdx));"
v1.2.0,change to delta store
v1.2.0,"System.out.println(""Before compression: "" + Arrays.toString(rIndex[i]));"
v1.2.0,"System.out.println(""After compression: "" + Arrays.toString(rIndex[i]));"
v1.2.0,"System.out.println(""Compressed "" + nnz + "" item to "" + bytes"
v1.2.0,"+ "" bytes, average bytes per item: "" + (double) bytes / qSketch.totalCount()"
v1.2.0,"+ "", uncompressed bytes: "" + 8 * grad.getDimension());"
v1.2.0,for (int i = 0; i < qSketchSize; i++) {
v1.2.0,int tmp = 0;
v1.2.0,for (int j = 0; j < curIdx[i]; j++) {
v1.2.0,tmp += rIndex[i][j];
v1.2.0,"grad.set(tmp, qSketch.getSplit(i));"
v1.2.0,}
v1.2.0,}
v1.2.0,"System.out.println(""Start calculate loss and auc, sample number: "" + totalNum);"
v1.2.0,"test.trainSGD2(dataset, 47001, 20, 0.01, 0.01, 100);"
v1.2.0,"System.out.println(""Indices: "" + Arrays.toString(indices));"
v1.2.0,t[i][code]++;
v1.2.0,else if (Math.random() > 0.5) {
v1.2.0,t[i][code] = freq;
v1.2.0,}
v1.2.0,"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);"
v1.2.0,"ret = Math.min(ret, t[i][h[i].encode(key)]);"
v1.2.0,"quantile sketch, size = featureNum * splitNum"
v1.2.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
v1.2.0,"active tree nodes, size = pow(2, treeDepth) -1"
v1.2.0,sampled features. size = treeNum * sampleRatio * featureNum
v1.2.0,"split features, size = treeNum * treeNodeNum"
v1.2.0,"split values, size = treeNum * treeNodeNum"
v1.2.0,"split gains, size = treeNum * treeNodeNum"
v1.2.0,"node weights, size = treeNum * treeNodeNum"
v1.2.0,"node preds, size = treeNum * treeNodeNum"
v1.2.0,if using PS to perform split
v1.2.0,step size for a tree
v1.2.0,number of class
v1.2.0,minimum loss change required for a split
v1.2.0,maximum depth of a tree
v1.2.0,number of features
v1.2.0,number of nonzero
v1.2.0,number of candidates split value
v1.2.0,----- the rest parameters are less important ----
v1.2.0,base instance weight
v1.2.0,minimum amount of hessian(weight) allowed in a child
v1.2.0,L2 regularization factor
v1.2.0,L1 regularization factor
v1.2.0,default direction choice
v1.2.0,maximum delta update we can add in weight estimation
v1.2.0,this parameter can be used to stabilize update
v1.2.0,default=0 means no constraint on weight delta
v1.2.0,whether we want to do subsample for row
v1.2.0,whether to subsample columns for each tree
v1.2.0,accuracy of sketch
v1.2.0,accuracy of sketch
v1.2.0,leaf vector size
v1.2.0,option for parallelization
v1.2.0,option to open cacheline optimization
v1.2.0,whether to not print info during training.
v1.2.0,"get feature type, 0:empty 1:all equal 2:real"
v1.2.0,maximum depth of the tree
v1.2.0,number of features used for tree construction
v1.2.0,"minimum loss change required for a split, otherwise stop split"
v1.2.0,----- the rest parameters are less important ----
v1.2.0,default direction choice
v1.2.0,whether we want to do sample data
v1.2.0,whether to sample columns during tree construction
v1.2.0,whether to use histogram for split
v1.2.0,number of histogram units
v1.2.0,whether to print info during training.
v1.2.0,----- the rest parameters are obtained after training ----
v1.2.0,total number of nodes
v1.2.0,number of deleted nodes */
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy data spliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,row 0 is a random uniform
v1.1.0,row 1 is a random normal
v1.1.0,row 2 is filled with 1.0
v1.1.0,TODO Auto-generated constructor stub
v1.1.0,row 0 is a random uniform
v1.1.0,row 1 is a random normal
v1.1.0,row 2 is filled with 1.0
v1.1.0,Feature number of train data
v1.1.0,Number of nonzero features
v1.1.0,Tree number
v1.1.0,Tree depth
v1.1.0,Split number
v1.1.0,Feature sample ratio
v1.1.0,Data format
v1.1.0,Learning rate
v1.1.0,Set basic configuration keys
v1.1.0,Use local deploy mode and dummy data spliter
v1.1.0,"set input, output path"
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,Set GBDT algorithm parameters
v1.1.0,Load Model from HDFS.
v1.1.0,Set local deploy mode
v1.1.0,Set basic configuration keys
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,"Set trainning data, and save model path"
v1.1.0,Set actionType train
v1.1.0,Set MF algorithm parameters
v1.1.0,Feature number of train data
v1.1.0,Total iteration number
v1.1.0,Validation sample Ratio
v1.1.0,"Data format, libsvm or dummy"
v1.1.0,Train batch number per epoch.
v1.1.0,Learning rate
v1.1.0,Decay of learning rate
v1.1.0,Regularization coefficient
v1.1.0,Set local deploy mode
v1.1.0,Set basic configuration keys
v1.1.0,Set data format
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,set sgd LR algorithm parameters #feature #epoch
v1.1.0,Set trainning data path
v1.1.0,Set save model path
v1.1.0,Set log path
v1.1.0,Set actionType train
v1.1.0,Set trainning data path
v1.1.0,Set load model path
v1.1.0,Set save model path
v1.1.0,Set actionType incremental train
v1.1.0,Set log path
v1.1.0,Set trainning data path
v1.1.0,Set load model path
v1.1.0,Set predict result path
v1.1.0,Set actionType prediction
v1.1.0,Feature number of train data
v1.1.0,Total iteration number
v1.1.0,Validation sample Ratio
v1.1.0,"Data format, libsvm or dummy"
v1.1.0,Train batch number per epoch.
v1.1.0,Batch number
v1.1.0,Learning rate
v1.1.0,Decay of learning rate
v1.1.0,Regularization coefficient
v1.1.0,Set local deploy mode
v1.1.0,Set basic configuration keys
v1.1.0,Set data format
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,set sgd LR algorithm parameters #feature #epoch
v1.1.0,Set trainning data path
v1.1.0,Set save model path
v1.1.0,Set log path
v1.1.0,Set actionType train
v1.1.0,Set trainning data path
v1.1.0,Set load model path
v1.1.0,Set save model path
v1.1.0,Set log path
v1.1.0,Set actionType incremental train
v1.1.0,Set trainning data path
v1.1.0,Set load model path
v1.1.0,Set predict result path
v1.1.0,Set log path
v1.1.0,Set actionType prediction
v1.1.0,Feature number of train data
v1.1.0,Total iteration number
v1.1.0,Learning rate
v1.1.0,Regularization coefficient
v1.1.0,Set local deploy mode
v1.1.0,Set basic configuration keys
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,set sgd LR algorithm parameters #feature #epoch
v1.1.0,Set input data path
v1.1.0,Set save model path
v1.1.0,Set actionType train
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,worker register
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,add matrix
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,attempt 0
v1.1.0,attempt1
v1.1.0,attempt1
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,TODO Auto-generated constructor stub
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,set basic configuration keys
v1.1.0,"conf.set(AngelConfiguration.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,Thread.sleep(5000);
v1.1.0,"response = master.getJobReport(null, request);"
v1.1.0,"assertEquals(response.getJobReport().getJobState(), JobStateProto.J_FAILED);"
v1.1.0,"assertEquals(response.getJobReport().getCurIteration(), jobIteration);"
v1.1.0,"assertEquals(response.getJobReport().getDiagnostics(), ""failed"");"
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,"DenseIntVector deltaVec = new DenseIntVector(100000, delta);"
v1.1.0,deltaVec.setMatrixId(matrixW1Id);
v1.1.0,deltaVec.setRowId(0);
v1.1.0,TODO Auto-generated constructor stub
v1.1.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
v1.1.0,@RunWith(MockitoJUnitRunner.class)
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,"conf.setInt(AngelConfiguration.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,psAgent.initAndStart();
v1.1.0,test conf
v1.1.0,test master location
v1.1.0,test app id
v1.1.0,test user
v1.1.0,test ps agent attempt id
v1.1.0,test ps agent id
v1.1.0,test connection
v1.1.0,test master client
v1.1.0,test ip
v1.1.0,test loc
v1.1.0,test master location
v1.1.0,test ps location
v1.1.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
v1.1.0,test all ps ids
v1.1.0,test all matrix ids
v1.1.0,test all matrix names
v1.1.0,test matrix attribute
v1.1.0,test matrix meta
v1.1.0,test ps location
v1.1.0,test partitions
v1.1.0,"Note:[startRow,endRow)"
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,group0Id = new WorkerGroupId(0);
v1.1.0,"worker0Id = new WorkerId(group0Id, 0);"
v1.1.0,"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);"
v1.1.0,task0Id = new TaskId(0);
v1.1.0,task1Id = new TaskId(1);
v1.1.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.1.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.1.0,test this func in testWriteTo
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,"LOG.info(index[0] + "" "" + value[0]);"
v1.1.0,"LOG.info(index[1] + "" "" + value[1]);"
v1.1.0,"LOG.info(index[2] + "" "" + value[2]);"
v1.1.0,dot
v1.1.0,plus
v1.1.0,plusBy
v1.1.0,dot
v1.1.0,plus
v1.1.0,plusBy
v1.1.0,dot
v1.1.0,plus
v1.1.0,plusBy
v1.1.0,dot
v1.1.0,plusBy
v1.1.0,@Test
v1.1.0,public void dotDenseFloatVector() throws Exception {
v1.1.0,int dim = 1000;
v1.1.0,Random random = new Random(System.currentTimeMillis());
v1.1.0,
v1.1.0,double[] values = new double[dim];
v1.1.0,float[] values_1 = new float[dim];
v1.1.0,for (int i = 0; i < dim; i++) {
v1.1.0,values[i] = random.nextDouble();
v1.1.0,values_1[i] = random.nextFloat();
v1.1.0,}
v1.1.0,
v1.1.0,"DenseDoubleVector vec = new DenseDoubleVector(dim, values);"
v1.1.0,"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);"
v1.1.0,
v1.1.0,double sum = 0.0;
v1.1.0,for (int i = 0; i < dim; i++) {
v1.1.0,sum += values[i] * values_1[i];
v1.1.0,}
v1.1.0,
v1.1.0,"assertEquals(sum, vec.dot(vec_1));"
v1.1.0,
v1.1.0,}
v1.1.0,@Test
v1.1.0,public void plusDenseFlaotVector() throws Exception {
v1.1.0,"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};"
v1.1.0,"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};"
v1.1.0,"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);"
v1.1.0,"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);"
v1.1.0,
v1.1.0,TDoubleVector vec_2 = vec.plus(vec_1);
v1.1.0,for (int i = 0; i < vec.size(); i++)
v1.1.0,"assertEquals(value_1[i] + value_2[i], vec_2.get(i));"
v1.1.0,
v1.1.0,
v1.1.0,"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);"
v1.1.0,
v1.1.0,for (int i = 0; i < vec.size(); i++)
v1.1.0,"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);"
v1.1.0,
v1.1.0,double[] oldValues = vec.getValues().clone();
v1.1.0,
v1.1.0,vec.plusBy(vec_1);
v1.1.0,
v1.1.0,for (int i = 0; i < vec.size(); i++)
v1.1.0,"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));"
v1.1.0,
v1.1.0,oldValues = vec.getValues().clone();
v1.1.0,
v1.1.0,"vec.plusBy(vec_1, 3);"
v1.1.0,
v1.1.0,for (int i = 0; i < vec.size(); i++)
v1.1.0,"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));"
v1.1.0,}
v1.1.0,@Test
v1.1.0,public void plusByArrayTest() {
v1.1.0,DenseFloatVector vec = new DenseFloatVector(dim);
v1.1.0,"int[] index = genSortedIndexs(nnz, dim);"
v1.1.0,float[] deltF = genFloatArray(nnz);
v1.1.0,double[] deltD = genDoubleArray(nnz);
v1.1.0,
v1.1.0,float[] oldVal = vec.getValues().clone();
v1.1.0,"vec.plusBy(index, deltF);"
v1.1.0,for (int i = 0; i < nnz; i++) {
v1.1.0,int idx = index[i];
v1.1.0,"assertEquals(oldVal[idx] + deltF[i], vec.get(idx));"
v1.1.0,}
v1.1.0,
v1.1.0,oldVal = vec.getValues().clone();
v1.1.0,"vec.plusBy(index, deltD);"
v1.1.0,for (int i = 0; i < nnz; i++) {
v1.1.0,int idx = index[i];
v1.1.0,"assertEquals(oldVal[idx] + (float) deltD[i], vec.get(idx));"
v1.1.0,}
v1.1.0,
v1.1.0,oldVal = vec.getValues().clone();
v1.1.0,"vec.plusBy(index, deltF);"
v1.1.0,for (int i = 0; i < nnz; i++) {
v1.1.0,int idx = index[i];
v1.1.0,"assertEquals(oldVal[idx] + deltF[i], vec.get(idx));"
v1.1.0,}
v1.1.0,
v1.1.0,oldVal = vec.getValues().clone();
v1.1.0,"vec.plusBy(index, deltD);"
v1.1.0,for (int i = 0; i < nnz; i++) {
v1.1.0,int idx = index[i];
v1.1.0,"assertEquals(oldVal[idx] + (float) deltD[i], vec.get(idx));"
v1.1.0,}
v1.1.0,}
v1.1.0,dot
v1.1.0,plus
v1.1.0,plusBy
v1.1.0,dot
v1.1.0,plus
v1.1.0,plusBy
v1.1.0,@Test
v1.1.0,public void plusBy3() throws Exception {
v1.1.0,"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};"
v1.1.0,"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);"
v1.1.0,"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});"
v1.1.0,vec.setRowId(0);
v1.1.0,"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});"
v1.1.0,vec_1.setRowId(1);
v1.1.0,TDoubleVector vec_2 = new SparseDoubleVector(2);
v1.1.0,"vec_2.set(1, 1.0);"
v1.1.0,vec_2.setRowId(0);
v1.1.0,
v1.1.0,mat.plusBy(vec);
v1.1.0,mat.plusBy(vec_1);
v1.1.0,mat.plusBy(vec_2);
v1.1.0,
v1.1.0,"assertEquals(2.0f, mat.get(0, 0));"
v1.1.0,"assertEquals(4.0f, mat.get(0, 1));"
v1.1.0,"assertEquals(4.0f, mat.get(1, 0));"
v1.1.0,"assertEquals(5.0f, mat.get(1, 1));"
v1.1.0,}
v1.1.0,set basic configuration keys
v1.1.0,use local deploy mode and dummy dataspliter
v1.1.0,get a angel client
v1.1.0,add matrix
v1.1.0,test worker getActiveTaskNum
v1.1.0,test worker getTaskNum
v1.1.0,test worker getTaskManager
v1.1.0,test workerId
v1.1.0,test workerAttemptId
v1.1.0,tet worker initFinished
v1.1.0,test worker getInitMinclock
v1.1.0,test worker loacation
v1.1.0,test AppId
v1.1.0,test Conf
v1.1.0,test UserName
v1.1.0,master location
v1.1.0,masterClient
v1.1.0,test psAgent
v1.1.0,test worker get dataBlockManager
v1.1.0,workerGroup.getSplits();
v1.1.0,application
v1.1.0,lcation
v1.1.0,workerGroup info
v1.1.0,worker info
v1.1.0,task
v1.1.0,Matrix parameters
v1.1.0,Set basic configuration keys
v1.1.0,Use local deploy mode and dummy data spliter
v1.1.0,Create an Angel client
v1.1.0,Add different types of matrix
v1.1.0,using mock object
v1.1.0,verification
v1.1.0,Stubbing
v1.1.0,Default does nothing.
v1.1.0,The app injection is optional
v1.1.0,"renderText(""hello world"");"
v1.1.0,"user choose a workerGroupID from the workergroups page,"
v1.1.0,now we should change the AngelApp params and render the workergroup page;
v1.1.0,"static final String WORKER_ID = ""worker.id"";"
v1.1.0,"div(""#logo"")."
v1.1.0,"img(""/static/hadoop-st.png"")._()."
v1.1.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
v1.1.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
v1.1.0,JQueryUI.jsnotice(html);
v1.1.0,import org.apache.hadoop.conf.Configuration;
v1.1.0,import java.lang.reflect.Field;
v1.1.0,get block locations from file system
v1.1.0,create a list of all block and their locations
v1.1.0,"if the file is not splitable, just create the one block with"
v1.1.0,full file length
v1.1.0,each split can be a maximum of maxSize
v1.1.0,if remainder is between max and 2*max - then
v1.1.0,"instead of creating splits of size max, left-max we"
v1.1.0,create splits of size left/2 and left/2. This is
v1.1.0,a heuristic to avoid creating really really small
v1.1.0,splits.
v1.1.0,add this block to the block --> node locations map
v1.1.0,"For blocks that do not have host/rack information,"
v1.1.0,assign to default  rack.
v1.1.0,add this block to the rack --> block map
v1.1.0,Add this host to rackToNodes map
v1.1.0,add this block to the node --> block map
v1.1.0,"if the file system does not have any rack information, then"
v1.1.0,use dummy rack location.
v1.1.0,The topology paths have the host name included as the last
v1.1.0,component. Strip it.
v1.1.0,get tokens for all the required FileSystems..
v1.1.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
v1.1.0,job.getConfiguration());
v1.1.0,Whether we need to recursive look into the directory structure
v1.1.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.1.0,user provided one (if any).
v1.1.0,all the files in input set
v1.1.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
v1.1.0,process all nodes and create splits that are local to a node. Generate
v1.1.0,"one split per node iteration, and walk over nodes multiple times to"
v1.1.0,distribute the splits across nodes.
v1.1.0,Skip the node if it has previously been marked as completed.
v1.1.0,"for each block, copy it into validBlocks. Delete it from"
v1.1.0,blockToNodes so that the same block does not appear in
v1.1.0,two different splits.
v1.1.0,Remove all blocks which may already have been assigned to other
v1.1.0,splits.
v1.1.0,"if the accumulated split size exceeds the maximum, then"
v1.1.0,create this split.
v1.1.0,create an input split and add it to the splits array
v1.1.0,Remove entries from blocksInNode so that we don't walk these
v1.1.0,again.
v1.1.0,Done creating a single split for this node. Move on to the next
v1.1.0,node so that splits are distributed across nodes.
v1.1.0,This implies that the last few blocks (or all in case maxSize=0)
v1.1.0,were not part of a split. The node is complete.
v1.1.0,if there were any blocks left over and their combined size is
v1.1.0,"larger than minSplitNode, then combine them into one split."
v1.1.0,Otherwise add them back to the unprocessed pool. It is likely
v1.1.0,that they will be combined with other blocks from the
v1.1.0,same rack later on.
v1.1.0,This condition also kicks in when max split size is not set. All
v1.1.0,blocks on a node will be grouped together into a single split.
v1.1.0,haven't created any split on this machine. so its ok to add a
v1.1.0,smaller one for parallelism. Otherwise group it in the rack for
v1.1.0,balanced size create an input split and add it to the splits
v1.1.0,array
v1.1.0,Remove entries from blocksInNode so that we don't walk this again.
v1.1.0,The node is done. This was the last set of blocks for this node.
v1.1.0,Put the unplaced blocks back into the pool for later rack-allocation.
v1.1.0,Node is done. All blocks were fit into node-local splits.
v1.1.0,Check if node-local assignments are complete.
v1.1.0,All nodes have been walked over and marked as completed or all blocks
v1.1.0,have been assigned. The rest should be handled via rackLock assignment.
v1.1.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
v1.1.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
v1.1.0,"if blocks in a rack are below the specified minimum size, then keep them"
v1.1.0,"in 'overflow'. After the processing of all racks is complete, these"
v1.1.0,overflow blocks will be combined into splits.
v1.1.0,Process all racks over and over again until there is no more work to do.
v1.1.0,Create one split for this rack before moving over to the next rack.
v1.1.0,Come back to this rack after creating a single split for each of the
v1.1.0,remaining racks.
v1.1.0,"Process one rack location at a time, Combine all possible blocks that"
v1.1.0,reside on this rack as one split. (constrained by minimum and maximum
v1.1.0,split size).
v1.1.0,iterate over all racks
v1.1.0,"for each block, copy it into validBlocks. Delete it from"
v1.1.0,blockToNodes so that the same block does not appear in
v1.1.0,two different splits.
v1.1.0,"if the accumulated split size exceeds the maximum, then"
v1.1.0,create this split.
v1.1.0,create an input split and add it to the splits array
v1.1.0,"if we created a split, then just go to the next rack"
v1.1.0,"if there is a minimum size specified, then create a single split"
v1.1.0,"otherwise, store these blocks into overflow data structure"
v1.1.0,There were a few blocks in this rack that
v1.1.0,remained to be processed. Keep them in 'overflow' block list.
v1.1.0,These will be combined later.
v1.1.0,Process all overflow blocks
v1.1.0,"This might cause an exiting rack location to be re-added,"
v1.1.0,but it should be ok.
v1.1.0,"if the accumulated split size exceeds the maximum, then"
v1.1.0,create this split.
v1.1.0,create an input split and add it to the splits array
v1.1.0,"Process any remaining blocks, if any."
v1.1.0,create an input split
v1.1.0,add this split to the list that is returned
v1.1.0,long num = totLength / maxSize;
v1.1.0,all blocks for all the files in input set
v1.1.0,mapping from a rack name to the list of blocks it has
v1.1.0,mapping from a block to the nodes on which it has replicas
v1.1.0,mapping from a node to the list of blocks that it contains
v1.1.0,populate all the blocks for all files
v1.1.0,stop all services
v1.1.0,1.write application state to file so that the client can get the state of the application
v1.1.0,if master exit
v1.1.0,2.clear tmp and staging directory
v1.1.0,waiting for client to get application state
v1.1.0,stop the RPC server
v1.1.0,"Security framework already loaded the tokens into current UGI, just use"
v1.1.0,them
v1.1.0,Now remove the AM->RM token so tasks don't have it
v1.1.0,add a shutdown hook
v1.1.0,init app state storage
v1.1.0,init event dispacher
v1.1.0,init location manager
v1.1.0,init container allocator
v1.1.0,init a rpc service
v1.1.0,recover matrix meta if needed
v1.1.0,recover ps attempt information if need
v1.1.0,init parameter server manager
v1.1.0,recover task information if needed
v1.1.0,init psagent manager and register psagent manager event
v1.1.0,a dummy data spliter is just for test now
v1.1.0,recover data splits information if needed
v1.1.0,init worker manager and register worker manager event
v1.1.0,register slow worker/ps checker
v1.1.0,register app manager event and finish event
v1.1.0,start a web service if use yarn deploy mode
v1.1.0,load from app state storage first if attempt index great than 1(the master is not the first
v1.1.0,retry)
v1.1.0,"if load failed, just build a new MatrixMetaManager"
v1.1.0,load ps attempt index from app state storage first if attempt index great than 1(the master
v1.1.0,is not the first retry)
v1.1.0,load task information from app state storage first if attempt index great than 1(the master
v1.1.0,is not the first retry)
v1.1.0,"if load failed, just build a new AMTaskManager"
v1.1.0,load data splits information from app state storage first if attempt index great than 1(the
v1.1.0,master is not the first retry)
v1.1.0,"if load failed, we need to recalculate the data splits"
v1.1.0,parse parameter server counters
v1.1.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.1.0,refresh last heartbeat timestamp
v1.1.0,send a state update event to the specific PSAttempt
v1.1.0,check if parameter server can commit now.
v1.1.0,check matrix metadata inconsistencies between master and parameter server.
v1.1.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
v1.1.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
v1.1.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.1.0,check whether psagent heartbeat timeout
v1.1.0,check whether parameter server heartbeat timeout
v1.1.0,check whether worker heartbeat timeout
v1.1.0,choose a unused port
v1.1.0,start RPC server
v1.1.0,find matrix partitions from master matrix meta manager for this parameter server
v1.1.0,remove this parameter server attempt from monitor set
v1.1.0,remove this parameter server attempt from monitor set
v1.1.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.1.0,find workergroup in worker manager
v1.1.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
v1.1.0,"if this worker group run over, just return WORKERGROUP_EXITED"
v1.1.0,"if this worker group is running now, return tasks, workers, data splits for it"
v1.1.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.1.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.1.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.1.0,"in ANGEL_PS mode, task id may can not know advance"
v1.1.0,update the clock for this matrix
v1.1.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.1.0,"in ANGEL_PS mode, task id may can not know advance"
v1.1.0,update task iteration
v1.1.0,private boolean matrixInited;
v1.1.0,"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.1.0,dispatch matrix partitions to parameter servers
v1.1.0,update matrix id generator
v1.1.0,"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.1.0,dispatch matrix partitions to parameter servers
v1.1.0,get matrix ids in the parameter server report
v1.1.0,get the matrices parameter server need to create and delete
v1.1.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
v1.1.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
v1.1.0,waitForMatrixReleaseOnPS(matrixId);
v1.1.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
v1.1.0,the number of splits equal to the number of tasks
v1.1.0,split data
v1.1.0,dispatch the splits to workergroups
v1.1.0,split data
v1.1.0,dispatch the splits to workergroups
v1.1.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.1.0,"first, then divided by expected split number"
v1.1.0,get input format class from configuration and then instantiation a input format object
v1.1.0,split data
v1.1.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.1.0,"first, then divided by expected split number"
v1.1.0,get input format class from configuration and then instantiation a input format object
v1.1.0,split data
v1.1.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.1.0,need to fine tune the number of workergroup and task based on the actual split number
v1.1.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.1.0,Record the location information for the splits in order to data localized schedule
v1.1.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.1.0,need to fine tune the number of workergroup and task based on the actual split number
v1.1.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.1.0,Record the location information for the splits in order to data localized schedule
v1.1.0,write meta data to a temporary file
v1.1.0,rename the temporary file to final file
v1.1.0,"if the file exists, read from file and deserialize it"
v1.1.0,write task meta
v1.1.0,write ps meta
v1.1.0,generate a temporary file
v1.1.0,write task meta to the temporary file first
v1.1.0,rename the temporary file to the final file
v1.1.0,"if last final task file exist, remove it"
v1.1.0,find task meta file which has max timestamp
v1.1.0,"if the file does not exist, just return null"
v1.1.0,read task meta from file and deserialize it
v1.1.0,generate a temporary file
v1.1.0,write ps meta to the temporary file first.
v1.1.0,rename the temporary file to the final file
v1.1.0,"if the old final file exist, just remove it"
v1.1.0,find ps meta file
v1.1.0,"if ps meta file does not exist, just return null"
v1.1.0,read ps meta from file and deserialize it
v1.1.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.1.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.1.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.1.0,create the topology tables
v1.1.0,Transitions from the NEW state.
v1.1.0,PA_FAILMSG
v1.1.0,Transitions from the UNASSIGNED state.
v1.1.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG"
v1.1.0,event
v1.1.0,Transitions from the ASSIGNED state.
v1.1.0,"this happened when launch thread run slowly, and PA_REGISTER event"
v1.1.0,dispatched before PA_CONTAINER_LAUNCHED event
v1.1.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.1.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.1.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.1.0,create the topology tables
v1.1.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will"
v1.1.0,retry another attempt or failed
v1.1.0,release container
v1.1.0,TODO
v1.1.0,set the launch time
v1.1.0,"set tarckerName,httpPort, which used by webserver"
v1.1.0,added to psManager so psManager can monitor it;
v1.1.0,psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);
v1.1.0,set the finish time only if launch time is set
v1.1.0,"ParameterServerJVM.setVMEnv(myEnv, conf);"
v1.1.0,Set up the launch command
v1.1.0,Duplicate the ByteBuffers for access by multiple containers.
v1.1.0,Construct the actual Container
v1.1.0,Application resources
v1.1.0,Application environment
v1.1.0,Service data
v1.1.0,Tokens
v1.1.0,Set up JobConf to be localized properly on the remote NM.
v1.1.0,Setup DistributedCache
v1.1.0,Setup up task credentials buffer
v1.1.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.1.0,Add the env variables passed by the user
v1.1.0,Set logging level in the environment.
v1.1.0,"This is so that, if the child forks another ""bin/hadoop"" (common in"
v1.1.0,streaming) it will have the correct loglevel.
v1.1.0,Setup the log4j prop
v1.1.0,Add main class and its arguments
v1.1.0,Finally add the jvmID
v1.1.0,vargs.add(String.valueOf(jvmID.getId()));
v1.1.0,Final commmand
v1.1.0,Transitions from the NEW state.
v1.1.0,Transitions from the RUNNING state.
v1.1.0,Transitions from the SUCCEEDED state
v1.1.0,Transitions from the KILLED state
v1.1.0,Transitions from the FAILED state
v1.1.0,Transitions from the NEW state.
v1.1.0,Transitions from the SCHEDULED state.
v1.1.0,Transitions from the RUNNING state.
v1.1.0,"another attempt launched,"
v1.1.0,Transitions from the SUCCEEDED state
v1.1.0,Transitions from the KILLED state
v1.1.0,Transitions from the FAILED state
v1.1.0,add diagnostic
v1.1.0,Set up the launch command
v1.1.0,Duplicate the ByteBuffers for access by multiple containers.
v1.1.0,Construct the actual Container
v1.1.0,Application resources
v1.1.0,Application environment
v1.1.0,Service data
v1.1.0,Tokens
v1.1.0,Set up JobConf to be localized properly on the remote NM.
v1.1.0,Setup DistributedCache
v1.1.0,Setup up task credentials buffer
v1.1.0,LocalStorageToken is needed irrespective of whether security is enabled
v1.1.0,or not.
v1.1.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
v1.1.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.1.0,Construct the actual Container
v1.1.0,The null fields are per-container and will be constructed for each
v1.1.0,container separately.
v1.1.0,Set up the launch command
v1.1.0,Duplicate the ByteBuffers for access by multiple containers.
v1.1.0,Construct the actual Container
v1.1.0,"a * in the classpath will only find a .jar, so we need to filter out"
v1.1.0,all .jars and add everything else
v1.1.0,Propagate the system classpath when using the mini cluster
v1.1.0,Add standard Hadoop classes
v1.1.0,Cache archives
v1.1.0,Cache files
v1.1.0,Sanity check
v1.1.0,Add URI fragment or just the filename
v1.1.0,Add the env variables passed by the user
v1.1.0,Set logging level in the environment.
v1.1.0,Setup the log4j prop
v1.1.0,Add main class and its arguments
v1.1.0,Finally add the jvmID
v1.1.0,vargs.add(String.valueOf(jvmID.getId()));
v1.1.0,Final commmand
v1.1.0,Add the env variables passed by the user
v1.1.0,Set logging level in the environment.
v1.1.0,Setup the log4j prop
v1.1.0,Add main class and its arguments
v1.1.0,Final commmand
v1.1.0,"if amTask is not null, we should clone task state from it"
v1.1.0,"if all parameter server complete commit, master can commit now"
v1.1.0,init and start master committer
v1.1.0,Transitions from the NEW state.
v1.1.0,Transitions from the UNASSIGNED state.
v1.1.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
v1.1.0,Transitions from the ASSIGNED state.
v1.1.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
v1.1.0,PA_CONTAINER_LAUNCHED event
v1.1.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.1.0,Transitions from the PSAttemptStateInternal.COMMITTING state
v1.1.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.1.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.1.0,create the topology tables
v1.1.0,reqeuest resource:send a resource request to the resource allocator
v1.1.0,"Once the resource is applied, build and send the launch request to the container launcher"
v1.1.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
v1.1.0,resource allocator
v1.1.0,set the launch time
v1.1.0,add the ps attempt to the heartbeat timeout monitoring list
v1.1.0,parse ps attempt location and put it to location manager
v1.1.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
v1.1.0,or failed
v1.1.0,remove ps attempt id from heartbeat timeout monitor list
v1.1.0,release container:send a release request to container launcher
v1.1.0,set the finish time only if launch time is set
v1.1.0,private long scheduledTime;
v1.1.0,Transitions from the NEW state.
v1.1.0,Transitions from the SCHEDULED state.
v1.1.0,Transitions from the RUNNING state.
v1.1.0,"another attempt launched,"
v1.1.0,Transitions from the SUCCEEDED state
v1.1.0,Transitions from the KILLED state
v1.1.0,Transitions from the FAILED state
v1.1.0,add diagnostic
v1.1.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.1.0,start a new attempt for this ps
v1.1.0,notify ps manager
v1.1.0,add diagnostic
v1.1.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.1.0,start a new attempt for this ps
v1.1.0,notify ps manager
v1.1.0,notify the event handler of state change
v1.1.0,"If new state is not RUNNING and COMMITTING, add it to state timeout monitor"
v1.1.0,"if forcedState is set, just return"
v1.1.0,else get state from state machine
v1.1.0,add this worker group to the success set
v1.1.0,check if all worker group run over
v1.1.0,add this worker group to the failed set
v1.1.0,check if too many worker groups are failed or killed
v1.1.0,notify a run failed event
v1.1.0,add this worker group to the failed set
v1.1.0,check if too many worker groups are failed or killed
v1.1.0,notify a run failed event
v1.1.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
v1.1.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
v1.1.0,just return the total task number now
v1.1.0,TODO
v1.1.0,"if workerAttempt is not null, we should clone task state from it"
v1.1.0,from NEW state
v1.1.0,from SCHEDULED state
v1.1.0,get data splits location for data locality
v1.1.0,reqeuest resource:send a resource request to the resource allocator
v1.1.0,"once the resource is applied, build and send the launch request to the container launcher"
v1.1.0,notify failed message to the worker
v1.1.0,notify killed message to the worker
v1.1.0,release the allocated container
v1.1.0,notify failed message to the worker
v1.1.0,remove the worker attempt from heartbeat timeout listen list
v1.1.0,release the allocated container
v1.1.0,notify killed message to the worker
v1.1.0,remove the worker attempt from heartbeat timeout listen list
v1.1.0,clean the container
v1.1.0,notify failed message to the worker
v1.1.0,remove the worker attempt from heartbeat timeout listen list
v1.1.0,record the finish time
v1.1.0,clean the container
v1.1.0,notify killed message to the worker
v1.1.0,remove the worker attempt from heartbeat timeout listening list
v1.1.0,record the finish time
v1.1.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
v1.1.0,set worker attempt location
v1.1.0,notify the register message to the worker
v1.1.0,record the launch time
v1.1.0,update worker attempt metrics
v1.1.0,update tasks metrics
v1.1.0,clean the container
v1.1.0,notify the worker attempt run successfully message to the worker
v1.1.0,record the finish time
v1.1.0,init a worker attempt for the worker
v1.1.0,schedule the worker attempt
v1.1.0,add diagnostic
v1.1.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.1.0,init and start a new attempt for this ps
v1.1.0,notify worker manager
v1.1.0,add diagnostic
v1.1.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.1.0,init and start a new attempt for this ps
v1.1.0,notify worker manager
v1.1.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
v1.1.0,register to Yarn RM
v1.1.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
v1.1.0,"catch YarnRuntimeException, we should exit and need not retry"
v1.1.0,build heartbeat request
v1.1.0,send heartbeat request to rm
v1.1.0,"This can happen if the RM has been restarted. If it is in that state,"
v1.1.0,this application must clean itself up.
v1.1.0,Setting NMTokens
v1.1.0,assgin containers
v1.1.0,"if some container is not assigned, release them"
v1.1.0,handle finish containers
v1.1.0,dispatch container exit message to corresponding components
v1.1.0,killed by framework
v1.1.0,killed by framework
v1.1.0,killed by framework
v1.1.0,get application finish state
v1.1.0,build application diagnostics
v1.1.0,TODO:add a job history for angel
v1.1.0,build unregister request
v1.1.0,send unregister request to rm
v1.1.0,Note this down for next interaction with ResourceManager
v1.1.0,based on blacklisting comments above we can end up decrementing more
v1.1.0,than requested. so guard for that.
v1.1.0,send the updated resource request to RM
v1.1.0,send 0 container count requests also to cancel previous requests
v1.1.0,Update resource requests
v1.1.0,try to assign to all nodes first to match node local
v1.1.0,try to match all rack local
v1.1.0,assign remaining
v1.1.0,Update resource requests
v1.1.0,send the container-assigned event to task attempt
v1.1.0,build the start container request use launch context
v1.1.0,send the start request to Yarn nm
v1.1.0,send the message that the container starts successfully to the corresponding component
v1.1.0,"after launching, send launched event to task attempt to move"
v1.1.0,it from ASSIGNED to RUNNING state
v1.1.0,send the message that the container starts failed to the corresponding component
v1.1.0,kill the remote container if already launched
v1.1.0,start a thread pool to startup the container
v1.1.0,See if we need up the pool size only if haven't reached the
v1.1.0,maximum limit yet.
v1.1.0,nodes where containers will run at *this* point of time. This is
v1.1.0,*not* the cluster size and doesn't need to be.
v1.1.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
v1.1.0,later is just a buffer so we are not always increasing the
v1.1.0,pool-size
v1.1.0,the events from the queue are handled in parallel
v1.1.0,using a thread pool
v1.1.0,return if already stopped
v1.1.0,shutdown any containers that might be left running
v1.1.0,Build and initialize rpc client to master
v1.1.0,Build local location
v1.1.0,"Initialize matrix info, this method will wait until master accepts the information from"
v1.1.0,client
v1.1.0,Get ps locations from master and put them to the location cache.
v1.1.0,Initialize matrix meta information
v1.1.0,Start heartbeat thread if need
v1.1.0,Start all services
v1.1.0,Register to master first
v1.1.0,Report state to master every specified time
v1.1.0,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.1.0,Stop all modules
v1.1.0,Exit the process if on ANGEL_PS_PSAGENT mode
v1.1.0,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.1.0,Stop all modules
v1.1.0,Exit the process if on ANGEL_PS_PSAGENT mode
v1.1.0,get configuration from config file
v1.1.0,set localDir with enviroment set by nm.
v1.1.0,Update generic resource counters
v1.1.0,Updating resources specified in ResourceCalculatorProcessTree
v1.1.0,Remove the CPU time consumed previously by JVM reuse
v1.1.0,array stores clock for each row and clock
v1.1.0,local task num
v1.1.0,mapping from task index to taskId
v1.1.0,mapping from taskId to task index
v1.1.0,TODO Auto-generated method stub
v1.1.0,Generate a flush request and put it to request queue
v1.1.0,Generate a clock request and put it to request queue
v1.1.0,Generate a merge request and put it to request queue
v1.1.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
v1.1.0,matrix
v1.1.0,and add it to cache maps
v1.1.0,Add the message to the tree map
v1.1.0,"If there are flush / clock requests blocked, we need to put this merge request into"
v1.1.0,the waiting queue
v1.1.0,Launch a merge worker to merge the update to matrix op log cache
v1.1.0,Remove the message from the tree map
v1.1.0,Wake up blocked flush/clock request
v1.1.0,Add flush/clock request to listener list to waiting for all the existing
v1.1.0,updates are merged
v1.1.0,Wake up blocked flush/clock request
v1.1.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
v1.1.0,blocked.
v1.1.0,Get next merge message sequence id
v1.1.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
v1.1.0,position
v1.1.0,Wake up blocked merge requests
v1.1.0,Get minimal sequence id from listeners
v1.1.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
v1.1.0,should flush updates to local matrix storage
v1.1.0,unused now
v1.1.0,Get partitions for the matrix
v1.1.0,"Filter it, removing zero values"
v1.1.0,Doing average or not
v1.1.0,Split this row according the matrix partitions
v1.1.0,Add the splits to the result container
v1.1.0,"For each partition, we generate a update split."
v1.1.0,"Although the split is empty for partitions those without any update data,"
v1.1.0,we still need to generate a update split to update the clock info on ps.
v1.1.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.1.0,Then submit normal task until reach upper limit of flow control or all tasks are submit
v1.1.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.1.0,"LOG.info(""choose put server "" + psIds[index]);"
v1.1.0,allocate the bytebuf
v1.1.0,"check the location of server is ready, if not, we should wait"
v1.1.0,get a channel to server from pool
v1.1.0,"if channel is not valid, it means maybe the connections to the server are closed"
v1.1.0,channelManager.removeChannelPool(loc);
v1.1.0,find the partition request context from cache
v1.1.0,Check if the result of the sub-request is received
v1.1.0,Update received result number
v1.1.0,Get row splits received
v1.1.0,Put the row split to the cache(row index to row splits map)
v1.1.0,"If all splits of the row are received, means this row can be merged"
v1.1.0,TODO Auto-generated method stub
v1.1.0,"Check futures, if the result of a sub-request is received, put it to the result queue"
v1.1.0,Now we just support pipelined row splits merging for dense type row
v1.1.0,Wait until the clock value of this row is greater than or equal to the value
v1.1.0,Get partitions for this row
v1.1.0,First get this row from matrix storage
v1.1.0,"If the row exists in the matrix storage and the clock value meets the requirements, just"
v1.1.0,return
v1.1.0,Get row splits of this row from the matrix cache first
v1.1.0,"If the row split does not exist in cache, get it from parameter server"
v1.1.0,Wait the final result
v1.1.0,Put it to the matrix cache
v1.1.0,Split the matrix oplog according to the matrix partitions
v1.1.0,"If need update clock, we should send requests to all partitions"
v1.1.0,use update index if exist
v1.1.0,Filter the rowIds which are fetching now
v1.1.0,Send the rowIndex to rpc dispatcher and return immediately
v1.1.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
v1.1.0,Generate dispatch items and add them to the corresponding queues
v1.1.0,Pre-fetching is disable default
v1.1.0,matrix id to clock map
v1.1.0,"task index, it must be unique for whole application"
v1.1.0,matrix id to task update index map. each task may only update some specific part for a matrix
v1.1.0,if (matrixManager.getMatrixMeta(createResponse.getMatrixId()) != null) {
v1.1.0,return matrixManager.getMatrixMeta(createResponse.getMatrixId());
v1.1.0,}
v1.1.0,Deserialize data splits meta
v1.1.0,Get workers
v1.1.0,Send request to every ps
v1.1.0,Wait the responses
v1.1.0,Update clock cache
v1.1.0,Get row from cache.
v1.1.0,"if row clock is satisfy ssp staleness limit, just return."
v1.1.0,Get row from ps.
v1.1.0,"For ASYNC mode, just get from pss."
v1.1.0,"For BSP/SSP, get rows from storage/cache first"
v1.1.0,Get from ps.
v1.1.0,"For ASYNC, just get rows from pss."
v1.1.0,no more retries.
v1.1.0,calculate sleep time and return.
v1.1.0,parse the i-th sleep-time
v1.1.0,parse the i-th number-of-retries
v1.1.0,calculateSleepTime may overflow.
v1.1.0,"A few common retry policies, with no delays."
v1.1.0,close is a local operation and should finish within milliseconds; timeout just to be safe
v1.1.0,response will be null for one way messages.
v1.1.0,maxFrameLength = 2G
v1.1.0,lengthFieldOffset = 0
v1.1.0,lengthFieldLength = 8
v1.1.0,"lengthAdjustment = -8, i.e. exclude the 8 byte length itself"
v1.1.0,"initialBytesToStrip = 8, i.e. strip out the length field itself"
v1.1.0,indicates whether this connection's life cycle is managed
v1.1.0,See if we already have a connection (common case)
v1.1.0,create a unique lock for this RS + protocol (if necessary)
v1.1.0,get the RS lock
v1.1.0,do one more lookup in case we were stalled above
v1.1.0,Only create isa when we need to.
v1.1.0,definitely a cache miss. establish an RPC for
v1.1.0,this RS
v1.1.0,Throw what the RemoteException was carrying.
v1.1.0,check
v1.1.0,every
v1.1.0,minutes
v1.1.0,TODO
v1.1.0,创建failoverHandler
v1.1.0,"The number of times this invocation handler has ever been failed over,"
v1.1.0,before this method invocation attempt. Used to prevent concurrent
v1.1.0,failed method invocations from triggering multiple failover attempts.
v1.1.0,Make sure that concurrent failed method invocations
v1.1.0,only cause a
v1.1.0,single actual fail over.
v1.1.0,RpcController + Message in the method args
v1.1.0,(generated code from RPC bits in .proto files have
v1.1.0,RpcController)
v1.1.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
v1.1.0,+ (System.currentTimeMillis() - beforeConstructTs));
v1.1.0,get an instance of the method arg type
v1.1.0,RpcController + Message in the method args
v1.1.0,(generated code from RPC bits in .proto files have
v1.1.0,RpcController)
v1.1.0,Message (hand written code usually has only a single
v1.1.0,argument)
v1.1.0,log any RPC responses that are slower than the configured
v1.1.0,warn
v1.1.0,response time or larger than configured warning size
v1.1.0,"when tagging, we let TooLarge trump TooSmall to keep"
v1.1.0,output simple
v1.1.0,note that large responses will often also be slow.
v1.1.0,provides a count of log-reported slow responses
v1.1.0,RpcController + Message in the method args
v1.1.0,(generated code from RPC bits in .proto files have
v1.1.0,RpcController)
v1.1.0,unexpected
v1.1.0,"in the protobuf methods, args[1] is the only significant argument"
v1.1.0,for JSON encoding
v1.1.0,base information that is reported regardless of type of call
v1.1.0,Disable Nagle's Algorithm since we don't want packets to wait
v1.1.0,Configure the event pipeline factory.
v1.1.0,Make a new connection.
v1.1.0,Remove all pending requests (will be canceled after relinquishing
v1.1.0,write lock).
v1.1.0,Cancel any pending requests by sending errors to the callbacks:
v1.1.0,Close the channel:
v1.1.0,Close the connection:
v1.1.0,Shut down all thread pools to exit.
v1.1.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
v1.1.0,See NettyServer.prepareResponse for where we write out the response.
v1.1.0,"It writes the call.id (int), a boolean signifying any error (and if"
v1.1.0,"so the exception name/trace), and the response bytes"
v1.1.0,Read the call id.
v1.1.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
v1.1.0,"instead, it returns a null message object."
v1.1.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
v1.1.0,System.currentTimeMillis());
v1.1.0,"It would be good widen this to just Throwable, but IOException is what we"
v1.1.0,allow now
v1.1.0,not implemented
v1.1.0,not implemented
v1.1.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
v1.1.0,cache of RpcEngines by protocol
v1.1.0,return the RpcEngine configured to handle a protocol
v1.1.0,We only handle the ConnectException.
v1.1.0,This is the exception we can't handle.
v1.1.0,check if timed out
v1.1.0,wait for retry
v1.1.0,IGNORE
v1.1.0,return the RpcEngine that handles a proxy object
v1.1.0,The default implementation works synchronously
v1.1.0,punt: allocate a new buffer & copy into it
v1.1.0,"LOG.info(System.getProperty(""user.dir""));"
v1.1.0,get tokens for all the required FileSystems..
v1.1.0,Whether we need to recursive look into the directory structure
v1.1.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.1.0,user provided one (if any).
v1.1.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.1.0,get tokens for all the required FileSystems..
v1.1.0,Whether we need to recursive look into the directory structure
v1.1.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.1.0,user provided one (if any).
v1.1.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.1.0,a simple hdfs copy function assume src path and dest path are in same hdfs
v1.1.0,and FileSystem object has same schema
v1.1.0,"LOG.warn(""interrupted while sleeping"", ie);"
v1.1.0,private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);
v1.1.0,public static String getHostname() {
v1.1.0,try {
v1.1.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
v1.1.0,} catch (UnknownHostException uhe) {
v1.1.0,}
v1.1.0,"return new StringBuilder().append("""").append(uhe).toString();"
v1.1.0,}
v1.1.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
v1.1.0,String hostname = getHostname();
v1.1.0,String classname = clazz.getSimpleName();
v1.1.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
v1.1.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
v1.1.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
v1.1.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
v1.1.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
v1.1.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
v1.1.0,
v1.1.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
v1.1.0,public void run() {
v1.1.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
v1.1.0,"this.val$classname + "" at "" + this.val$hostname}));"
v1.1.0,}
v1.1.0,});
v1.1.0,}
v1.1.0,"We we interrupted because we're meant to stop? If not, just"
v1.1.0,continue ignoring the interruption
v1.1.0,Recalculate waitTime.
v1.1.0,// Begin delegation to Thread
v1.1.0,// End delegation to Thread
v1.1.0,load angel system configuration
v1.1.0,load user configuration:
v1.1.0,1. user config file
v1.1.0,2. command lines
v1.1.0,"add user resource files to ""angel.lib.jars"" to upload to hdfs"
v1.1.0,load user job jar if it exist
v1.1.0,Expand the environment variable
v1.1.0,instance submitter class
v1.1.0,Add default fs(local fs) for lib jars.
v1.1.0,Obtain filename from path
v1.1.0,Split filename to prexif and suffix (extension)
v1.1.0,Check if the filename is okay
v1.1.0,Prepare temporary file
v1.1.0,Prepare buffer for data copying
v1.1.0,Open and check input stream
v1.1.0,Open output stream and copy data between source file in JAR and the temporary file
v1.1.0,"If read/write fails, close streams safely before throwing an exception"
v1.1.0,"Finally, load the library"
v1.1.0,little endian load order
v1.1.0,tail
v1.1.0,fallthrough
v1.1.0,fallthrough
v1.1.0,finalization
v1.1.0,fmix(h1);
v1.1.0,----------
v1.1.0,body
v1.1.0,----------
v1.1.0,tail
v1.1.0,----------
v1.1.0,finalization
v1.1.0,----------
v1.1.0,body
v1.1.0,----------
v1.1.0,tail
v1.1.0,----------
v1.1.0,finalization
v1.1.0,JobStateProto jobState = report.getJobState();
v1.1.0,the leaf level file should be readable by others
v1.1.0,the subdirs in the path should have execute permissions for
v1.1.0,others
v1.1.0,2.get job id
v1.1.0,Credentials credentials = new Credentials();
v1.1.0,4.copy resource files to hdfs
v1.1.0,5.write configuration to a xml file
v1.1.0,6.create am container context
v1.1.0,7.Submit to ResourceManager
v1.1.0,8.get app master client
v1.1.0,Create a number of filenames in the JobTracker's fs namespace
v1.1.0,add all the command line files/ jars and archive
v1.1.0,first copy them to jobtrackers filesystem
v1.1.0,should not throw a uri exception
v1.1.0,should not throw an uri excpetion
v1.1.0,set the timestamps of the archives and files
v1.1.0,set the public/private visibility of the archives and files
v1.1.0,get DelegationToken for each cached file
v1.1.0,check if we do not need to copy the files
v1.1.0,is jt using the same file system.
v1.1.0,just checking for uri strings... doing no dns lookups
v1.1.0,to see if the filesystems are the same. This is not optimal.
v1.1.0,but avoids name resolution.
v1.1.0,this might have name collisions. copy will throw an exception
v1.1.0,parse the original path to create new path
v1.1.0,check for ports
v1.1.0,Write job file to JobTracker's fs
v1.1.0,Setup resource requirements
v1.1.0,Setup LocalResources
v1.1.0,Setup security tokens
v1.1.0,Setup the command to run the AM
v1.1.0,Add AM user command opts
v1.1.0,Final command
v1.1.0,Setup the CLASSPATH in environment
v1.1.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
v1.1.0,Setup the environment variables for Admin first
v1.1.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
v1.1.0,Parse distributed cache
v1.1.0,Setup ContainerLaunchContext for AM container
v1.1.0,Set up the ApplicationSubmissionContext
v1.1.0,resposne.encode(buf);
v1.1.0,TODO:
v1.1.0,resposne.encode(buf);
v1.1.0,TODO:
v1.1.0,resposne.encode(buf);
v1.1.0,TODO:
v1.1.0,resposne.encode(buf);
v1.1.0,TODO:
v1.1.0,private final ParameterServerId serverId;
v1.1.0,private final PSIdProto idProto;
v1.1.0,Add tokens to new user so that it may execute its task correctly.
v1.1.0,to exit
v1.1.0,mkdir does not throw exception if path exits
v1.1.0,commitTaskPool.shutdown();
v1.1.0,private final ParameterServer psServer;
v1.1.0,TODO
v1.1.0,"when we should write snapshot to hdfs? clearly, we have two methods:"
v1.1.0,"1. write snapshot at regular time, if there are updates, just write them."
v1.1.0,"2. write snapshot every N iterations, this method depends on notification of master"
v1.1.0,"FSDataOutputStream output = fileContext.create(snapshotsTempFilePath,"
v1.1.0,EnumSet.of(CreateFlag.CREATE));
v1.1.0,@brief get filename of the old snapshot written before
v1.1.0,"no snapshotFile write before, maybe write snapshots the first time"
v1.1.0,start end
v1.1.0,rowtype
v1.1.0,data.rewind();
v1.1.0,data.rewind();
v1.1.0,data.rewind();
v1.1.0,Pass the matrix and partition number field
v1.1.0,Mapping from taskId to clock value.
v1.1.0,int[] keys = sparseRep.getKeys();
v1.1.0,int[] values = sparseRep.getValues();
v1.1.0,boolean[] used = sparseRep.getUsed();
v1.1.0,nnz = 0;
v1.1.0,for (int i = 0; i < keys.length; i++)
v1.1.0,if (used[i]) {
v1.1.0,"denseRep.put(keys[i], values[i]);"
v1.1.0,nnz++;
v1.1.0,}
v1.1.0,sparseRep = null;
v1.1.0,int[] keys = sparseRep.getKeys();
v1.1.0,int[] values = sparseRep.getValues();
v1.1.0,boolean[] used = sparseRep.getUsed();
v1.1.0,for (int i = 0; i < keys.length; i++)
v1.1.0,if (used[i]) {
v1.1.0,"denseRep.put(keys[i], values[i]);"
v1.1.0,}
v1.1.0,sparseRep = null;
v1.1.0,output.writeInt(data.length);
v1.1.0,@Override
v1.1.0,public void serialize(ByteBuf buf) {
v1.1.0,if (sparseRep != null)
v1.1.0,return serializeSparse();
v1.1.0,else if (denseRep != null)
v1.1.0,return serializeDense();
v1.1.0,return serializeEmpty();
v1.1.0,}
v1.1.0,int[] keys = sparseRep.getKeys();
v1.1.0,int[] values = sparseRep.getValues();
v1.1.0,boolean[] used = sparseRep.getUsed();
v1.1.0,int idx = 0;
v1.1.0,for (int i = 0; i < keys.length; i++)
v1.1.0,if (used[i]) {
v1.1.0,"keysBuf.put(idx, keys[i]);"
v1.1.0,"valuesBuf.put(idx, values[i]);"
v1.1.0,idx++;
v1.1.0,}
v1.1.0,int[] keys = sparseRep.getKeys();
v1.1.0,int[] values = sparseRep.getValues();
v1.1.0,boolean[] used = sparseRep.getUsed();
v1.1.0,"int ov, k, v;"
v1.1.0,for (int i = 0; i < keys.length; i++) {
v1.1.0,if (used[i]) {
v1.1.0,k = keys[i];
v1.1.0,ov = denseRep.get(k);
v1.1.0,v = ov + values[i];
v1.1.0,"denseRep.put(k, v);"
v1.1.0,if (ov != 0 && v == 0)
v1.1.0,nnz--;
v1.1.0,}
v1.1.0,}
v1.1.0,"add the PSAgentContext,need fix"
v1.1.0,set MatrixPartitionLocation
v1.1.0,set attribute
v1.1.0,TODO Auto-generated method stub
v1.1.0,@brief Sorted index for non-zero items
v1.1.0,@brief Number of non-zero items in this vector
v1.1.0,@brief Array to store values.
v1.1.0,@brief sum of the square of all of element
v1.1.0,"LOG.error(""Cannot perform plus operation on SparseDoubleSortedVector"");"
v1.1.0,TODO Auto-generated method stub
v1.1.0,TODO:
v1.1.0,TODO:
v1.1.0,write the max abs
v1.1.0,"Thread.currentThread().getContextClassLoader().getResourceAsStream(""feature_conf.xml"");"
v1.1.0,this.matchList = new ArrayList<Match>();
v1.1.0,"LOG.debug(String.format(""index: %d, value: %s"", index, value));"
v1.1.0,"LOG.debug(String.format(""index: %d, value: %s"", index, value));"
v1.1.0,"LOG.debug(String.format(""index: %d, value: %s"", index, value));"
v1.1.0,"LOG.debug(String.format(""index: %d, value: %s"", index, value));"
v1.1.0,"LOG.debug(""target="" + target);"
v1.1.0,TODO Auto-generated method stub
v1.1.0,TODO Auto-generated method stub
v1.1.0,TODO Auto-generated method stub
v1.1.0,get configuration from config file
v1.1.0,set localDir with enviroment set by nm.
v1.1.0,get master location
v1.1.0,Add tokens to new user so that it may execute its task correctly.
v1.1.0,init task manager and start tasks
v1.1.0,start heartbeat thread
v1.1.0,taskManager.assignTaskIds(response.getTaskidsList());
v1.1.0,todo
v1.1.0,"if worker timeout, it may be knocked off."
v1.1.0,"SUCCESS, do nothing"
v1.1.0,heartbeatFailedTime = 0;
v1.1.0,private KEY currentKey;
v1.1.0,will be created
v1.1.0,TODO Auto-generated method stub
v1.1.0,Bitmap bitmap = new Bitmap();
v1.1.0,int max = indexArray[size - 1];
v1.1.0,byte [] bitIndexArray = new byte[max / 8 + 1];
v1.1.0,for(int i = 0; i < size; i++){
v1.1.0,int bitIndex = indexArray[i] >> 3;
v1.1.0,int bitOffset = indexArray[i] - (bitIndex << 3);
v1.1.0,switch(bitOffset){
v1.1.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
v1.1.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
v1.1.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
v1.1.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
v1.1.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
v1.1.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
v1.1.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
v1.1.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
v1.1.0,}
v1.1.0,}
v1.1.0,//////////////////////////////
v1.1.0,Application Configs
v1.1.0,//////////////////////////////
v1.1.0,//////////////////////////////
v1.1.0,Master Configs
v1.1.0,//////////////////////////////
v1.1.0,//////////////////////////////
v1.1.0,Worker Configs
v1.1.0,//////////////////////////////
v1.1.0,//////////////////////////////
v1.1.0,Task Configs
v1.1.0,//////////////////////////////
v1.1.0,//////////////////////////////
v1.1.0,ParameterServer Configs
v1.1.0,//////////////////////////////
v1.1.0,////////////////// IPC //////////////////////////
v1.1.0,//////////////////////////////
v1.1.0,Matrix transfer Configs.
v1.1.0,//////////////////////////////
v1.1.0,//////////////////////////////
v1.1.0,Matrix transfer Configs.
v1.1.0,//////////////////////////////
v1.1.0,Configs used to ANGEL_PS_PSAGENT running mode future.
v1.1.0,model parse
v1.1.0,private Configuration conf;
v1.1.0,TODO Auto-generated constructor stub
v1.1.0,Feature number of train data
v1.1.0,Number of nonzero features
v1.1.0,Tree number
v1.1.0,Tree depth
v1.1.0,Split number
v1.1.0,Feature sample ratio
v1.1.0,Data format
v1.1.0,Learning rate
v1.1.0,Set basic configuration keys
v1.1.0,Use local deploy mode and dummy data spliter
v1.1.0,"set input, output path"
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,Set GBDT algorithm parameters
v1.1.0,Submit GBDT Train Task
v1.1.0,Load Model from HDFS.
v1.1.0,Set local deploy mode
v1.1.0,Set basic configuration keys
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,"Set trainning data, save model, log path"
v1.1.0,Set actionType train
v1.1.0,Set MF algorithm parameters
v1.1.0,Feature number of train data
v1.1.0,Total iteration number
v1.1.0,Validation sample Ratio
v1.1.0,"Data format, libsvm or dummy"
v1.1.0,Train batch number per epoch.
v1.1.0,Batch number
v1.1.0,Learning rate
v1.1.0,Decay of learning rate
v1.1.0,Regularization coefficient
v1.1.0,Set local deploy mode
v1.1.0,Set basic configuration keys
v1.1.0,Set data format
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,set sgd LR algorithm parameters #feature #epoch
v1.1.0,Set trainning data path
v1.1.0,Set save model path
v1.1.0,Set log path
v1.1.0,Set actionType train
v1.1.0,Set trainning data path
v1.1.0,Set load model path
v1.1.0,Set save model path
v1.1.0,Set log path
v1.1.0,Set actionType incremental train
v1.1.0,Set trainning data path
v1.1.0,Set load model path
v1.1.0,Set predict result path
v1.1.0,Set actionType prediction
v1.1.0,LOG.info(sigmoid(data[i]));
v1.1.0,LOG.info(Math.exp(-data[i]));
v1.1.0,when b is a negative number
v1.1.0,Cluster center number
v1.1.0,Feature number of train data
v1.1.0,Total iteration number
v1.1.0,Sample ratio per mini-batch
v1.1.0,C
v1.1.0,Set local deploy mode
v1.1.0,Set basic configuration keys
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,set Kmeans algorithm parameters #cluster #feature #epoch
v1.1.0,Set trainning data path
v1.1.0,Set data format
v1.1.0,Set save model path
v1.1.0,Set log sava path
v1.1.0,Set actionType train
v1.1.0,Set load model path
v1.1.0,Set predict result path
v1.1.0,Set actionType prediction
v1.1.0,Set log sava path
v1.1.0,Feature number of train data
v1.1.0,Total iteration number
v1.1.0,Validation Ratio
v1.1.0,Data format
v1.1.0,Train batch number per epoch.
v1.1.0,Learning rate
v1.1.0,Decay of learning rate
v1.1.0,Regularization coefficient
v1.1.0,Set basic configuration keys
v1.1.0,Set data format
v1.1.0,Use local deploy mode
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,set sgd SVM algorithm parameters
v1.1.0,"set input, output path"
v1.1.0,Set save model path
v1.1.0,Set actionType train
v1.1.0,Set log path
v1.1.0,Submit LR Train Task
v1.1.0,Set trainning data path
v1.1.0,Set load model path
v1.1.0,Set save model path
v1.1.0,Set actionType incremental train
v1.1.0,Set log path
v1.1.0,Feature number of train data
v1.1.0,Total iteration number
v1.1.0,Validation sample Ratio
v1.1.0,"Data format, libsvm or dummy"
v1.1.0,Train batch number per epoch.
v1.1.0,Learning rate
v1.1.0,Decay of learning rate
v1.1.0,Regularization coefficient
v1.1.0,Set local deploy mode
v1.1.0,Set basic configuration keys
v1.1.0,Set data format
v1.1.0,"set angel resource parameters #worker, #task, #PS"
v1.1.0,set sgd LR algorithm parameters #feature #epoch
v1.1.0,Set trainning data path
v1.1.0,Set save model path
v1.1.0,Set log path
v1.1.0,Set actionType train
v1.1.0,Set trainning data path
v1.1.0,Set load model path
v1.1.0,Set save model path
v1.1.0,Set actionType incremental train
v1.1.0,Set log path
v1.1.0,Set trainning data path
v1.1.0,Set load model path
v1.1.0,Set predict result path
v1.1.0,Set log sava path
v1.1.0,Set actionType prediction
v1.1.0,double z=pre*y;
v1.1.0,if(z<=0) return 0.5-z;
v1.1.0,"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);"
v1.1.0,return 0.0;
v1.1.0,if (pre * y <= 0)
v1.1.0,return y;
v1.1.0,else if (pre * y > 0 && pre * y < 1)
v1.1.0,return (1 - pre * y) * y;
v1.1.0,return 0.0;
v1.1.0,feature index used to split
v1.1.0,feature value used to split
v1.1.0,loss change after split this node
v1.1.0,grad stats of the left child
v1.1.0,grad stats of the right child
v1.1.0,"LOG.info(""Constructor with fid = -1"");"
v1.1.0,fid = -1: no split currently
v1.1.0,the minimal split value is the minimal value of feature
v1.1.0,the splits do not include the maximal value of feature
v1.1.0,"1. the average distance, (maxValue - minValue) / splitNum"
v1.1.0,2. calculate the candidate split value
v1.1.0,1. new feature's histogram (grad + hess)
v1.1.0,size: sampled_featureNum * (2 * splitNum)
v1.1.0,"in other words, concatenate each feature's histogram"
v1.1.0,2. get the span of this node
v1.1.0,------ 3. using sparse-aware method to build histogram ---
v1.1.0,"first add grads of all instances to the zero bin of all features, then loop the non-zero entries of all the instances"
v1.1.0,3.1. get the instance index
v1.1.0,3.2. get the grad and hess of the instance
v1.1.0,3.3. add to the sum
v1.1.0,3.4. loop the non-zero entries
v1.1.0,3.4.1. get feature value
v1.1.0,3.4.2. current feature's position in the sampled feature set
v1.1.0,3.4.3. find the position of feature value in a histogram
v1.1.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
v1.1.0,3.4.4. add the grad and hess to the corresponding bin
v1.1.0,3.4.5. add the reverse to the bin that contains 0.0f
v1.1.0,4. add the grad and hess sum to the zero bin of all features
v1.1.0,int startIdx = fid * 2 * splitNum;
v1.1.0,find the best split result of the histogram of a tree node
v1.1.0,1. calculate the gradStats of the root node
v1.1.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
v1.1.0,2. loop over features
v1.1.0,2.1. get the ture feature id in the sampled feature set
v1.1.0,2.2. get the indexes of histogram of this feature
v1.1.0,2.3. find the best split of current feature
v1.1.0,2.4. update the best split result if possible
v1.1.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.1.0,3. update the grad stats of children node
v1.1.0,3.1. update the left child
v1.1.0,3.2. update the right child
v1.1.0,find the best split result of one feature
v1.1.0,1. set the feature id
v1.1.0,2. create the best left stats and right stats
v1.1.0,3. the gain of the root node
v1.1.0,4. create the temp left and right grad stats
v1.1.0,5. loop over all the data in histogram
v1.1.0,5.1. get the grad and hess of current hist bin
v1.1.0,5.2. check whether we can split with current left hessian
v1.1.0,right = root - left
v1.1.0,5.3. check whether we can split with current right hessian
v1.1.0,5.4. calculate the current loss gain
v1.1.0,5.5. check whether we should update the split result with current loss gain
v1.1.0,split value = sketches[splitIdx+1]
v1.1.0,"5.6. if should update, also update the best left and right grad stats"
v1.1.0,6. set the best left and right grad stats
v1.1.0,partition number
v1.1.0,cols of each partition
v1.1.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.1.0,3. update the grad stats of children node
v1.1.0,3.1. update the left child
v1.1.0,3.2. update the right child
v1.1.0,1. calculate the total grad sum and hess sum
v1.1.0,2. create the grad stats of the node
v1.1.0,1. calculate the total grad sum and hess sum
v1.1.0,2. create the grad stats of the node
v1.1.0,1. calculate the total grad sum and hess sum
v1.1.0,2. create the grad stats of the node
v1.1.0,"loop all the possible split value, start from split[1], since the first item is the minimal"
v1.1.0,feature value
v1.1.0,find the best split result of the histogram of a tree node
v1.1.0,2.2. get the indexes of histogram of this feature
v1.1.0,2.3. find the best split of current feature
v1.1.0,2.4. update the best split result if possible
v1.1.0,find the best split result of one feature
v1.1.0,1. set the feature id
v1.1.0,splitEntry.setFid(fid);
v1.1.0,2. create the best left stats and right stats
v1.1.0,3. the gain of the root node
v1.1.0,4. create the temp left and right grad stats
v1.1.0,5. loop over all the data in histogram
v1.1.0,5.1. get the grad and hess of current hist bin
v1.1.0,5.2. check whether we can split with current left hessian
v1.1.0,right = root - left
v1.1.0,5.3. check whether we can split with current right hessian
v1.1.0,5.4. calculate the current loss gain
v1.1.0,5.5. check whether we should update the split result with current loss gain
v1.1.0,"5.6. if should update, also update the best left and right grad stats"
v1.1.0,6. set the best left and right grad stats
v1.1.0,find the best split result of a serve row on the PS
v1.1.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.1.0,2.2. get the start index in histogram of this feature
v1.1.0,2.3. find the best split of current feature
v1.1.0,2.4. update the best split result if possible
v1.1.0,"find the best split result of one feature from a server row, used by the PS"
v1.1.0,1. set the feature id
v1.1.0,2. create the best left stats and right stats
v1.1.0,3. the gain of the root node
v1.1.0,4. create the temp left and right grad stats
v1.1.0,5. loop over all the data in histogram
v1.1.0,5.1. get the grad and hess of current hist bin
v1.1.0,5.2. check whether we can split with current left hessian
v1.1.0,right = root - left
v1.1.0,5.3. check whether we can split with current right hessian
v1.1.0,5.4. calculate the current loss gain
v1.1.0,5.5. check whether we should update the split result with current loss gain
v1.1.0,"here we set the fvalue=splitIndex, split value = sketches[splitIdx+1], the task use"
v1.1.0,index to find fvalue
v1.1.0,"5.6. if should update, also update the best left and right grad stats"
v1.1.0,6. set the best left and right grad stats
v1.1.0,clear all the information
v1.1.0,calculate the sum of gradient and hess
v1.1.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v1.1.0,ridx)
v1.1.0,check if necessary information is ready
v1.1.0,"same as add, reduce is used in All Reduce"
v1.1.0,"features used in this tree, if equals null, means use all the features without sampling"
v1.1.0,node in the tree
v1.1.0,the gradient info of each instances
v1.1.0,initialize nodes
v1.1.0,gradient
v1.1.0,second order gradient
v1.1.0,logistic loss for binary classification task.
v1.1.0,"logistic loss, but predict un-transformed margin"
v1.1.0,check if label in range
v1.1.0,return the default evaluation metric for the objective
v1.1.0,read partition header
v1.1.0,deal with row according the rowType
v1.1.0,TODO Auto-generated method stub
v1.1.0,tree.tree[topic + K] = (row.get(topic) + beta) / (n_k.get(topic) + vbeta);
v1.1.0,Inc update to local buffers
v1.1.0,The starting point
v1.1.0,There's always an unused entry.
v1.1.0,The starting point
v1.1.0,There's always an unused entry.
v1.1.0,The starting point
v1.1.0,There's always an unused entry.
v1.1.0,The starting point
v1.1.0,There's always an unused entry.
v1.1.0,The starting point
v1.1.0,There's always an unused entry.
v1.1.0,The starting point
v1.1.0,There's always an unused entry.
v1.1.0,print();
v1.1.0,The starting point
v1.1.0,There's always an unused entry.
v1.1.0,The starting point
v1.1.0,There's always an unused entry.
v1.1.0,loss function
v1.1.0,gradient and hessian
v1.1.0,tree node
v1.1.0,initialize the phase
v1.1.0,current tree and depth
v1.1.0,create loss function
v1.1.0,calculate grad info of each instance
v1.1.0,"create data sketch, push candidate split value to PS"
v1.1.0,1. calculate candidate split value
v1.1.0,2. push local sketch to PS
v1.1.0,3. set phase to GET_SKETCH
v1.1.0,"pull the global sketch from PS, only called once by each worker"
v1.1.0,sample feature
v1.1.0,push sampled feature set to the current tree
v1.1.0,create new tree
v1.1.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
v1.1.0,calculate gradient
v1.1.0,"1. create new tree, initialize tree nodes and node stats"
v1.1.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
v1.1.0,2.1. pull the sampled features of the current tree
v1.1.0,"2.2. if use all the features, only called one"
v1.1.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
v1.1.0,4. set root node to active
v1.1.0,"5. reset instance position, set the root node's span"
v1.1.0,6. calculate gradient
v1.1.0,7. set phase to run active
v1.1.0,1. start threads of active tree nodes
v1.1.0,1.1. start threads for active nodes to generate histogram
v1.1.0,1.2. set thread status to running
v1.1.0,1.3. set the oplog to active
v1.1.0,"2. check thread stats, if all threads finish, return"
v1.1.0,clock
v1.1.0,find split
v1.1.0,"1. find responsible tree node, using RR scheme"
v1.1.0,2. pull gradient histogram
v1.1.0,2.1. get the name of this node's gradient histogram on PS
v1.1.0,2.2. pull the histogram
v1.1.0,2.3. find best split result of this tree node
v1.1.0,"2.3.1 using server split, each partition of the histogram contains its best split result"
v1.1.0,find the best split from all partitions
v1.1.0,"2.3.2 the updated split result (tree node/feature/value/gain) on PS,"
v1.1.0,"2.3.3 otherwise, the returned histogram contains the gradient info"
v1.1.0,"2.3.4 the updated split result (tree node/feature/value/gain) on PS,"
v1.1.0,2.3.5 reset this tree node's gradient histogram to 0
v1.1.0,3. push split feature to PS
v1.1.0,4. push split value to PS
v1.1.0,5. push split gain to PS
v1.1.0,6. set phase to AFTER_SPLIT
v1.1.0,clock
v1.1.0,1. get split feature
v1.1.0,2. get split value
v1.1.0,3. get split gain
v1.1.0,4. get node weight
v1.1.0,5. split node
v1.1.0,5.1. set the children nodes of this node
v1.1.0,5.2. set split info and grad stats to this node
v1.1.0,5.2. create children nodes
v1.1.0,"5.3. create node stats for children nodes, and add them to the tree"
v1.1.0,5.4. reset instance position
v1.1.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
v1.1.0,5.6. set children nodes to leaf nodes
v1.1.0,5.7. set nid to leaf node
v1.1.0,5.8. deactivate active node
v1.1.0,6. clock
v1.1.0,"split the span of one node, reset the instance position"
v1.1.0,in case this worker has no instance on this node
v1.1.0,set the span of left child
v1.1.0,set the span of right child
v1.1.0,"1. left to right, find the first instance that should be in the right child"
v1.1.0,"2. right to left, find the first instance that should be in the left child"
v1.1.0,3. swap two instances
v1.1.0,4. find the cut pos
v1.1.0,than the split value
v1.1.0,5. set the span of left child
v1.1.0,6. set the span of right child
v1.1.0,set tree node to active
v1.1.0,set node to leaf
v1.1.0,set node to inactive
v1.1.0,finish current tree
v1.1.0,calculate the error
v1.1.0,predict();
v1.1.0,finish current depth
v1.1.0,set the tree phase
v1.1.0,check if there is active node
v1.1.0,check if finish all the tree
v1.1.0,update node's grad stats on PS
v1.1.0,"called during splitting in GradHistHelper, update the grad stats of children nodes after finding the best split"
v1.1.0,the root node's stats is updated by leader worker
v1.1.0,1. create the update
v1.1.0,2. push the update to PS
v1.1.0,the leader task adds node prediction to flush list
v1.1.0,1. name of this node's grad histogram on PS
v1.1.0,2. build the grad histogram of this node
v1.1.0,3. push the histograms to PS
v1.1.0,"LOG.info(String.format(""Histogram: size[%d] %s"", histogram.getDimension(),"
v1.1.0,Arrays.toString(histogram.getValues())));
v1.1.0,4. reset thread stats to finished
v1.1.0,int sendStartCol = startFid * 7; // each split contains 7 doubles
v1.1.0,step size for a tree
v1.1.0,number of class
v1.1.0,minimum loss change required for a split
v1.1.0,maximum depth of a tree
v1.1.0,number of features
v1.1.0,number of nonzero
v1.1.0,number of candidates split value
v1.1.0,----- the rest parameters are less important ----
v1.1.0,base instance weight
v1.1.0,minimum amount of hessian(weight) allowed in a child
v1.1.0,L2 regularization factor
v1.1.0,L1 regularization factor
v1.1.0,default direction choice
v1.1.0,maximum delta update we can add in weight estimation
v1.1.0,this parameter can be used to stabilize update
v1.1.0,default=0 means no constraint on weight delta
v1.1.0,whether we want to do subsample for row
v1.1.0,whether to subsample columns for each tree
v1.1.0,accuracy of sketch
v1.1.0,accuracy of sketch
v1.1.0,leaf vector size
v1.1.0,option for parallelization
v1.1.0,option to open cacheline optimization
v1.1.0,whether to not print info during training.
v1.1.0,return MathUtils.sqr(sumGrad) / (sumHess + regLambda);
v1.1.0,feature index used to split
v1.1.0,feature value used to split
v1.1.0,loss change after split this node
v1.1.0,grad stats of the left child
v1.1.0,grad stats of the right child
v1.1.0,"LOG.info(""Constructor with fid = -1"");"
v1.1.0,fid = -1: no split currently
v1.1.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v1.1.0,ridx)
v1.1.0,"same as add, reduce is used in All Reduce"
v1.1.0,maximum depth of the tree
v1.1.0,number of features used for tree construction
v1.1.0,"minimum loss change required for a split, otherwise stop split"
v1.1.0,----- the rest parameters are less important ----
v1.1.0,default direction choice
v1.1.0,whether we want to do sample data
v1.1.0,whether to sample columns during tree construction
v1.1.0,whether to use histogram for split
v1.1.0,number of histogram units
v1.1.0,whether to print info during training.
v1.1.0,----- the rest parameters are obtained after training ----
v1.1.0,total number of nodes
v1.1.0,number of deleted nodes */
v1.1.0,gradient
v1.1.0,second order gradient
v1.1.0,1. calculate the total grad sum and hess sum
v1.1.0,2. create the grad stats of the node
v1.1.0,find the best split result of a serve row on the PS
v1.1.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.1.0,2.2. get the start index in histogram of this feature
v1.1.0,"LOG.info(String.format(""Histogram of feature[%d]: %s"", trueFid,"
v1.1.0,Arrays.toString(curHistogram.getValues())));
v1.1.0,2.3. find the best split of current feature
v1.1.0,"LOG.info(String.format(""Best split of feature[%d]: value[%f], gain[%f]"","
v1.1.0,"trueFid, curSplit.getFvalue(), curSplit.getLossChg()));"
v1.1.0,2.4. update the best split result if possible
v1.1.0,"find the best split result of one feature from a server row, used by the PS"
v1.1.0,"LOG.info(String.format(""Find the best split for fid[%d] in server row, size[%d], startIdx[%d]"","
v1.1.0,"fid, row.size(), startIdx));"
v1.1.0,StringBuilder sb = new StringBuilder();
v1.1.0,for (int i = startIdx; i < startIdx + 2 * splitNum; i++) {
v1.1.0,"sb.append(row.getData().get(i) + "", "");"
v1.1.0,}
v1.1.0,"LOG.info(""Server row: "" + sb.toString());"
v1.1.0,1. set the feature id
v1.1.0,splitEntry.setFid(fid);
v1.1.0,2. create the best left stats and right stats
v1.1.0,3. the gain of the root node
v1.1.0,"LOG.info(String.format(""Feature[%d]: sumGrad[%f], sumHess[%f], gain[%f]"","
v1.1.0,"fid, rootStats.sumGrad, rootStats.sumHess, rootGain));"
v1.1.0,4. create the temp left and right grad stats
v1.1.0,5. loop over all the data in histogram
v1.1.0,5.1. get the grad and hess of current hist bin
v1.1.0,5.2. check whether we can split with current left hessian
v1.1.0,right = root - left
v1.1.0,5.3. check whether we can split with current right hessian
v1.1.0,5.4. calculate the current loss gain
v1.1.0,5.5. check whether we should update the split result with current loss gain
v1.1.0,"LOG.info(String.format(""The current split: fid[%d], split index[%f], lossChg[%f]"","
v1.1.0,"fid, (float) splitIdx, lossChg));"
v1.1.0,"here we set the fvalue=splitIndex, split value = sketches[splitIdx+1], the task use"
v1.1.0,index to find fvalue
v1.1.0,"LOG.info(String.format(""Find new best split: fid[%d], fvalue[%f], lossChg[%f]"","
v1.1.0,"splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg));"
v1.1.0,"LOG.info(String.format(""Left child of node[%d]: sumGrad[%f], sumHess[%f]; "" +"
v1.1.0,"""right child of node[%d]: sumGrad[%f], sumHess[%f]; lossChg[%f]"","
v1.1.0,"this.nid, leftStats.sumGrad, leftStats.sumHess,"
v1.1.0,"this.nid, rightStats.sumGrad, rightStats.sumHess, lossChg));"
v1.1.0,"5.6. if should update, also update the best left and right grad stats"
v1.1.0,6. set the best left and right grad stats
v1.1.0,"LOG.info(String.format(""The best split for fid[%d], split feature[%d]: split index[%f], lossChg[%f], """
v1.1.0,+
v1.1.0,"""leftSumGrad[%f], leftSumHess[%f], rightSumGrad[%f], rightSumHess[%f]"","
v1.1.0,"fid, splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg,"
v1.1.0,"splitEntry.leftGradStat.sumGrad, splitEntry.leftGradStat.sumHess,"
v1.1.0,"splitEntry.rightGradStat.sumGrad, splitEntry.rightGradStat.sumHess));"
v1.1.0,grad.timesBy(-1.0 * lr);
v1.1.0,"System.out.println(""Quantile sketch indices: "" + Arrays.toString(qSketch.getValues()));"
v1.1.0,"System.out.println(""Max: "" + qSketch.max() + "", min: "" + qSketch.min());"
v1.1.0,"System.out.println(""Quantile sketch count: "" + Arrays.toString(qSketch.getCounts()));"
v1.1.0,"System.out.println(""Zero index: "" + qSketch.getZeroIndex() + "", """
v1.1.0,"+ qSketch.get(qSketch.getZeroIndex()) + "", "" + qSketch.get(qSketch.getZeroIndex()-1));"
v1.1.0,cmSketch.distribution();
v1.1.0,"write2File(cmSketch.getTable(0), ""E:\\dropbox\\code\\github\\sketchML\\table0"");"
v1.1.0,"write2File(cmSketch.getTable(1), ""E:\\dropbox\\code\\github\\sketchML\\table1"");"
v1.1.0,"write2File(cmSketch.getTable(2), ""E:\\dropbox\\code\\github\\sketchML\\table2"");"
v1.1.0,"System.out.println(""true freq: "" + trueFreq + "", sketch freq: "" + cmFreq);"
v1.1.0,"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + (- qSketch.get(cmFreq)));"
v1.1.0,ratioArr[ratio]++;
v1.1.0,"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(cmFreq));"
v1.1.0,"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(cmFreq));"
v1.1.0,ratioArr[ratio]++;
v1.1.0,System.out.println(Arrays.toString(distArr));
v1.1.0,System.out.println(Arrays.toString(ratioArr));
v1.1.0,"System.out.println(""Nnz grad: "" + nnz +"", zero grad: "" + zeroGrad + "", negative grad: "" + negCount + "", larger grad: "" + largeCount + "", smaller grad: "" + smallCount);"
v1.1.0,"write2File(distArr, ""E:\\dropbox\\code\\github\\sketchML\\error_hist"");"
v1.1.0,for (int i = 0; i < grad.getDimension(); i++) {
v1.1.0,"System.out.println(""true grad: "" + grad.get(i) + "", sketch grad: "" + qSketch.get(qSketch.indexOf(grad.get(i))));"
v1.1.0,"grad.set(i, qSketch.get(qSketch.indexOf(grad.get(i))));"
v1.1.0,}
v1.1.0,"System.out.println(""Cur index of rIndex: "" + Arrays.toString(curIdx));"
v1.1.0,change to delta store
v1.1.0,"System.out.println(""Before compression: "" + Arrays.toString(rIndex[i]));"
v1.1.0,"System.out.println(""After compression: "" + Arrays.toString(rIndex[i]));"
v1.1.0,"System.out.println(""Compressed "" + nnz + "" item to "" + bytes"
v1.1.0,"+ "" bytes, average bytes per item: "" + (double) bytes / qSketch.totalCount()"
v1.1.0,"+ "", uncompressed bytes: "" + 8 * grad.getDimension());"
v1.1.0,for (int i = 0; i < qSketchSize; i++) {
v1.1.0,int tmp = 0;
v1.1.0,for (int j = 0; j < curIdx[i]; j++) {
v1.1.0,tmp += rIndex[i][j];
v1.1.0,"grad.set(tmp, qSketch.getSplit(i));"
v1.1.0,}
v1.1.0,}
v1.1.0,"System.out.println(""Start calculate loss and auc, sample number: "" + totalNum);"
v1.1.0,"test.trainSGD2(dataset, 47001, 20, 0.01, 0.01, 100);"
v1.1.0,"System.out.println(""Indices: "" + Arrays.toString(indices));"
v1.1.0,t[i][code]++;
v1.1.0,else if (Math.random() > 0.5) {
v1.1.0,t[i][code] = freq;
v1.1.0,}
v1.1.0,"System.out.println(""Change from "" + t[i][code] + "" to "" + freq);"
v1.1.0,"ret = Math.min(ret, t[i][h[i].encode(key)]);"
v1.1.0,"quantile sketch, size = featureNum * splitNum"
v1.1.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
v1.1.0,"active tree nodes, size = pow(2, treeDepth) -1"
v1.1.0,sampled features. size = treeNum * sampleRatio * featureNum
v1.1.0,"split features, size = treeNum * treeNodeNum"
v1.1.0,"split values, size = treeNum * treeNodeNum"
v1.1.0,"split gains, size = treeNum * treeNodeNum"
v1.1.0,"node weights, size = treeNum * treeNodeNum"
v1.1.0,"node preds, size = treeNum * treeNodeNum"
v1.1.0,if using PS to perform split
v1.1.0,step size for a tree
v1.1.0,number of class
v1.1.0,minimum loss change required for a split
v1.1.0,maximum depth of a tree
v1.1.0,number of features
v1.1.0,number of nonzero
v1.1.0,number of candidates split value
v1.1.0,----- the rest parameters are less important ----
v1.1.0,base instance weight
v1.1.0,minimum amount of hessian(weight) allowed in a child
v1.1.0,L2 regularization factor
v1.1.0,L1 regularization factor
v1.1.0,default direction choice
v1.1.0,maximum delta update we can add in weight estimation
v1.1.0,this parameter can be used to stabilize update
v1.1.0,default=0 means no constraint on weight delta
v1.1.0,whether we want to do subsample for row
v1.1.0,whether to subsample columns for each tree
v1.1.0,accuracy of sketch
v1.1.0,accuracy of sketch
v1.1.0,leaf vector size
v1.1.0,option for parallelization
v1.1.0,option to open cacheline optimization
v1.1.0,whether to not print info during training.
v1.1.0,"get feature type, 0:empty 1:all equal 2:real"
v1.1.0,maximum depth of the tree
v1.1.0,number of features used for tree construction
v1.1.0,"minimum loss change required for a split, otherwise stop split"
v1.1.0,----- the rest parameters are less important ----
v1.1.0,default direction choice
v1.1.0,whether we want to do sample data
v1.1.0,whether to sample columns during tree construction
v1.1.0,whether to use histogram for split
v1.1.0,number of histogram units
v1.1.0,whether to print info during training.
v1.1.0,----- the rest parameters are obtained after training ----
v1.1.0,total number of nodes
v1.1.0,number of deleted nodes */
v1.0.0,row 0 is a random uniform
v1.0.0,row 1 is a random normal
v1.0.0,row 2 is filled with 1.0
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy data spliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,TODO Auto-generated constructor stub
v1.0.0,row 0 is a random uniform
v1.0.0,row 1 is a random normal
v1.0.0,row 2 is filled with 1.0
v1.0.0,Feature number of train data
v1.0.0,Number of nonzero features
v1.0.0,Tree number
v1.0.0,Tree depth
v1.0.0,Split number
v1.0.0,Feature sample ratio
v1.0.0,Data format
v1.0.0,Learning rate
v1.0.0,Set basic configuration keys
v1.0.0,Use local deploy mode and dummy data spliter
v1.0.0,"set input, output path"
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,Set GBDT algorithm parameters
v1.0.0,Load Model from HDFS.
v1.0.0,Set local deploy mode
v1.0.0,Set basic configuration keys
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,"Set trainning data, and save model path"
v1.0.0,Set actionType train
v1.0.0,Set MF algorithm parameters
v1.0.0,Feature number of train data
v1.0.0,Total iteration number
v1.0.0,Validation sample Ratio
v1.0.0,"Data format, libsvm or dummy"
v1.0.0,Train batch number per epoch.
v1.0.0,Learning rate
v1.0.0,Decay of learning rate
v1.0.0,Regularization coefficient
v1.0.0,Set local deploy mode
v1.0.0,Set basic configuration keys
v1.0.0,Set data format
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,set sgd LR algorithm parameters #feature #epoch
v1.0.0,Set trainning data path
v1.0.0,Set save model path
v1.0.0,Set log path
v1.0.0,Set actionType train
v1.0.0,Set trainning data path
v1.0.0,Set load model path
v1.0.0,Set save model path
v1.0.0,Set actionType incremental train
v1.0.0,Set log path
v1.0.0,Set trainning data path
v1.0.0,Set load model path
v1.0.0,Set predict result path
v1.0.0,Set actionType prediction
v1.0.0,Feature number of train data
v1.0.0,Total iteration number
v1.0.0,Validation sample Ratio
v1.0.0,"Data format, libsvm or dummy"
v1.0.0,Train batch number per epoch.
v1.0.0,Batch number
v1.0.0,Learning rate
v1.0.0,Decay of learning rate
v1.0.0,Regularization coefficient
v1.0.0,Set local deploy mode
v1.0.0,Set basic configuration keys
v1.0.0,Set data format
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,set sgd LR algorithm parameters #feature #epoch
v1.0.0,Set trainning data path
v1.0.0,Set save model path
v1.0.0,Set log path
v1.0.0,Set actionType train
v1.0.0,Set trainning data path
v1.0.0,Set load model path
v1.0.0,Set save model path
v1.0.0,Set log path
v1.0.0,Set actionType incremental train
v1.0.0,Set trainning data path
v1.0.0,Set load model path
v1.0.0,Set predict result path
v1.0.0,Set log path
v1.0.0,Set actionType prediction
v1.0.0,Feature number of train data
v1.0.0,Total iteration number
v1.0.0,Learning rate
v1.0.0,Regularization coefficient
v1.0.0,Set local deploy mode
v1.0.0,Set basic configuration keys
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,set sgd LR algorithm parameters #feature #epoch
v1.0.0,Set input data path
v1.0.0,Set save model path
v1.0.0,Set actionType train
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,worker register
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,attempt 0
v1.0.0,attempt 1
v1.0.0,attempt 2
v1.0.0,attempt 3
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,add matrix
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,attempt 0
v1.0.0,attempt1
v1.0.0,attempt1
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,TODO Auto-generated constructor stub
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,set basic configuration keys
v1.0.0,"conf.set(AngelConfiguration.ANGEL_TASK_USER_TASKCLASS, DummyTask.class.getName());"
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,"DenseIntVector deltaVec = new DenseIntVector(100000, delta);"
v1.0.0,deltaVec.setMatrixId(matrixW1Id);
v1.0.0,deltaVec.setRowId(0);
v1.0.0,TODO Auto-generated constructor stub
v1.0.0,import com.tencent.angel.psagent.consistency.SSPConsistencyController;
v1.0.0,@RunWith(MockitoJUnitRunner.class)
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,"conf.setInt(AngelConfiguration.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,test create matrix
v1.0.0,Int2IntOpenHashMap matrix1Clocks = task1.getMatrixClocks();
v1.0.0,"assertEquals(matrix1Clocks.size(), 2);"
v1.0.0,"assertEquals(matrix1Clocks.get(w1Id), 1);"
v1.0.0,"assertEquals(matrix1Clocks.get(w2Id), 1);"
v1.0.0,psAgent.initAndStart();
v1.0.0,test conf
v1.0.0,test master location
v1.0.0,test app id
v1.0.0,test user
v1.0.0,test ps agent attempt id
v1.0.0,test ps agent id
v1.0.0,test connection
v1.0.0,test master client
v1.0.0,test ip
v1.0.0,test loc
v1.0.0,test master location
v1.0.0,test ps location
v1.0.0,"assertEquals(psLoc, locationCache.updateAndGetPSLocation(psId));"
v1.0.0,test all ps ids
v1.0.0,test all matrix ids
v1.0.0,test all matrix names
v1.0.0,test matrix attribute
v1.0.0,test matrix meta
v1.0.0,test ps location
v1.0.0,test partitions
v1.0.0,"PartitionKey part1 = matrixPartitionRouter.getPartitionKey(1, 0);"
v1.0.0,assertTrue(part1 != null);
v1.0.0,"assertEquals(part1, partition1Keys.get(0));"
v1.0.0,"PartitionKey part2 = matrixPartitionRouter.getPartitionKey(2, 0);"
v1.0.0,assertTrue(part2 != null);
v1.0.0,"assertEquals(part2, partition2Keys.get(1));"
v1.0.0,"assertEquals(((SSPConsistencyController) consistControl).getStaleness(), staleness);"
v1.0.0,"PartitionKey part1 = psAgent.getMatrixPartitionRouter().getPartitionKey(1, 0);"
v1.0.0,"int part1Clock = clockCache.getClock(1, part1);"
v1.0.0,"assertEquals(part1Clock, 0);"
v1.0.0,
v1.0.0,"PartitionKey part2 = psAgent.getMatrixPartitionRouter().getPartitionKey(2, 0);"
v1.0.0,"int part2Clock = clockCache.getClock(2, part2);"
v1.0.0,"assertEquals(part2Clock, 0);"
v1.0.0,"Note:[startRow,endRow)"
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,test this func in testWriteMatrix
v1.0.0,test this func in testClock
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,group0Id = new WorkerGroupId(0);
v1.0.0,"worker0Id = new WorkerId(group0Id, 0);"
v1.0.0,"worker0Attempt0Id = new WorkerAttemptId(worker0Id, 0);"
v1.0.0,task0Id = new TaskId(0);
v1.0.0,task1Id = new TaskId(1);
v1.0.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.0.0,LOG.info(serverArbitraryIntRow1.getSparseRep());
v1.0.0,test this func in testWriteTo
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,"LOG.info(index[0] + "" "" + value[0]);"
v1.0.0,"LOG.info(index[1] + "" "" + value[1]);"
v1.0.0,"LOG.info(index[2] + "" "" + value[2]);"
v1.0.0,dot
v1.0.0,plus
v1.0.0,plusBy
v1.0.0,dot
v1.0.0,plus
v1.0.0,plusBy
v1.0.0,dot
v1.0.0,plus
v1.0.0,plusBy
v1.0.0,dot
v1.0.0,plusBy
v1.0.0,@Test
v1.0.0,public void dotDenseFloatVector() throws Exception {
v1.0.0,int dim = 1000;
v1.0.0,Random random = new Random(System.currentTimeMillis());
v1.0.0,
v1.0.0,double[] values = new double[dim];
v1.0.0,float[] values_1 = new float[dim];
v1.0.0,for (int i = 0; i < dim; i++) {
v1.0.0,values[i] = random.nextDouble();
v1.0.0,values_1[i] = random.nextFloat();
v1.0.0,}
v1.0.0,
v1.0.0,"DenseDoubleVector vec = new DenseDoubleVector(dim, values);"
v1.0.0,"TDoubleVector vec_1 = new DenseFloatVector(dim, values_1);"
v1.0.0,
v1.0.0,double sum = 0.0;
v1.0.0,for (int i = 0; i < dim; i++) {
v1.0.0,sum += values[i] * values_1[i];
v1.0.0,}
v1.0.0,
v1.0.0,"assertEquals(sum, vec.dot(vec_1));"
v1.0.0,
v1.0.0,}
v1.0.0,@Test
v1.0.0,public void plusDenseFlaotVector() throws Exception {
v1.0.0,"double[] value_1 = new double[]{0.1, 0.2, 0.3, 0.4, 0.5};"
v1.0.0,"double[] value_2 = new double[]{0.1f, 0.2f, 0.3f, 0.4f, 0.5f};"
v1.0.0,"DenseDoubleVector vec = new DenseDoubleVector(5, value_1);"
v1.0.0,"TDoubleVector vec_1 = new DenseFloatVector(5, value_2);"
v1.0.0,
v1.0.0,TDoubleVector vec_2 = vec.plus(vec_1);
v1.0.0,for (int i = 0; i < vec.size(); i++)
v1.0.0,"assertEquals(value_1[i] + value_2[i], vec_2.get(i));"
v1.0.0,
v1.0.0,
v1.0.0,"TDoubleVector vec_3 = vec.plus(vec_1, 2.0);"
v1.0.0,
v1.0.0,for (int i = 0; i < vec.size(); i++)
v1.0.0,"assertEquals(vec_3.get(i), value_1[i] + 2 * value_2[i]);"
v1.0.0,
v1.0.0,double[] oldValues = vec.getValues().clone();
v1.0.0,
v1.0.0,vec.plusBy(vec_1);
v1.0.0,
v1.0.0,for (int i = 0; i < vec.size(); i++)
v1.0.0,"assertEquals(vec.get(i), oldValues[i] + vec_1.get(i));"
v1.0.0,
v1.0.0,oldValues = vec.getValues().clone();
v1.0.0,
v1.0.0,"vec.plusBy(vec_1, 3);"
v1.0.0,
v1.0.0,for (int i = 0; i < vec.size(); i++)
v1.0.0,"assertEquals(vec.get(i), oldValues[i] + 3 * vec_1.get(i));"
v1.0.0,}
v1.0.0,@Test
v1.0.0,public void plusByArrayTest() {
v1.0.0,DenseFloatVector vec = new DenseFloatVector(dim);
v1.0.0,"int[] index = genSortedIndexs(nnz, dim);"
v1.0.0,float[] deltF = genFloatArray(nnz);
v1.0.0,double[] deltD = genDoubleArray(nnz);
v1.0.0,
v1.0.0,float[] oldVal = vec.getValues().clone();
v1.0.0,"vec.plusBy(index, deltF);"
v1.0.0,for (int i = 0; i < nnz; i++) {
v1.0.0,int idx = index[i];
v1.0.0,"assertEquals(oldVal[idx] + deltF[i], vec.get(idx));"
v1.0.0,}
v1.0.0,
v1.0.0,oldVal = vec.getValues().clone();
v1.0.0,"vec.plusBy(index, deltD);"
v1.0.0,for (int i = 0; i < nnz; i++) {
v1.0.0,int idx = index[i];
v1.0.0,"assertEquals(oldVal[idx] + (float) deltD[i], vec.get(idx));"
v1.0.0,}
v1.0.0,
v1.0.0,oldVal = vec.getValues().clone();
v1.0.0,"vec.plusBy(index, deltF);"
v1.0.0,for (int i = 0; i < nnz; i++) {
v1.0.0,int idx = index[i];
v1.0.0,"assertEquals(oldVal[idx] + deltF[i], vec.get(idx));"
v1.0.0,}
v1.0.0,
v1.0.0,oldVal = vec.getValues().clone();
v1.0.0,"vec.plusBy(index, deltD);"
v1.0.0,for (int i = 0; i < nnz; i++) {
v1.0.0,int idx = index[i];
v1.0.0,"assertEquals(oldVal[idx] + (float) deltD[i], vec.get(idx));"
v1.0.0,}
v1.0.0,}
v1.0.0,dot
v1.0.0,plus
v1.0.0,plusBy
v1.0.0,dot
v1.0.0,plus
v1.0.0,plusBy
v1.0.0,@Test
v1.0.0,public void plusBy3() throws Exception {
v1.0.0,"float[][] value = {{1.0f, 2.0f}, {3.0f, 4.0f}};"
v1.0.0,"DenseFloatMatrix mat = new DenseFloatMatrix(2, 2,value);"
v1.0.0,"TFloatVector vec = new DenseFloatVector(2, new float[]{1.0f, 1.0f});"
v1.0.0,vec.setRowId(0);
v1.0.0,"TDoubleVector vec_1 = new DenseDoubleVector(2, new double[]{1.0f, 1.0f});"
v1.0.0,vec_1.setRowId(1);
v1.0.0,TDoubleVector vec_2 = new SparseDoubleVector(2);
v1.0.0,"vec_2.set(1, 1.0);"
v1.0.0,vec_2.setRowId(0);
v1.0.0,
v1.0.0,mat.plusBy(vec);
v1.0.0,mat.plusBy(vec_1);
v1.0.0,mat.plusBy(vec_2);
v1.0.0,
v1.0.0,"assertEquals(2.0f, mat.get(0, 0));"
v1.0.0,"assertEquals(4.0f, mat.get(0, 1));"
v1.0.0,"assertEquals(4.0f, mat.get(1, 0));"
v1.0.0,"assertEquals(5.0f, mat.get(1, 1));"
v1.0.0,}
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,test worker getActiveTaskNum
v1.0.0,test worker getTaskNum
v1.0.0,test worker getTaskManager
v1.0.0,test workerId
v1.0.0,test workerAttemptId
v1.0.0,tet worker initFinished
v1.0.0,test worker getInitMinclock
v1.0.0,test worker loacation
v1.0.0,test AppId
v1.0.0,test Conf
v1.0.0,test UserName
v1.0.0,master location
v1.0.0,masterClient
v1.0.0,test psAgent
v1.0.0,test worker get dataBlockManager
v1.0.0,workerGroup.getSplits();
v1.0.0,application
v1.0.0,lcation
v1.0.0,workerGroup info
v1.0.0,worker info
v1.0.0,task
v1.0.0,Matrix parameters
v1.0.0,Set basic configuration keys
v1.0.0,Use local deploy mode and dummy data spliter
v1.0.0,Create an Angel client
v1.0.0,Add different types of matrix
v1.0.0,using mock object
v1.0.0,verification
v1.0.0,Stubbing
v1.0.0,Default does nothing.
v1.0.0,The app injection is optional
v1.0.0,"renderText(""hello world"");"
v1.0.0,"user choose a workerGroupID from the workergroups page,"
v1.0.0,now we should change the AngelApp params and render the workergroup page;
v1.0.0,"static final String WORKER_ID = ""worker.id"";"
v1.0.0,"div(""#logo"")."
v1.0.0,"img(""/static/hadoop-st.png"")._()."
v1.0.0,import org.apache.hadoop.yarn.webapp.view.FooterBlock;
v1.0.0,import org.apache.hadoop.yarn.webapp.view.HeaderBlock;
v1.0.0,JQueryUI.jsnotice(html);
v1.0.0,import org.apache.hadoop.conf.Configuration;
v1.0.0,import java.lang.reflect.Field;
v1.0.0,get block locations from file system
v1.0.0,create a list of all block and their locations
v1.0.0,"if the file is not splitable, just create the one block with"
v1.0.0,full file length
v1.0.0,each split can be a maximum of maxSize
v1.0.0,if remainder is between max and 2*max - then
v1.0.0,"instead of creating splits of size max, left-max we"
v1.0.0,create splits of size left/2 and left/2. This is
v1.0.0,a heuristic to avoid creating really really small
v1.0.0,splits.
v1.0.0,add this block to the block --> node locations map
v1.0.0,"For blocks that do not have host/rack information,"
v1.0.0,assign to default  rack.
v1.0.0,add this block to the rack --> block map
v1.0.0,Add this host to rackToNodes map
v1.0.0,add this block to the node --> block map
v1.0.0,"if the file system does not have any rack information, then"
v1.0.0,use dummy rack location.
v1.0.0,The topology paths have the host name included as the last
v1.0.0,component. Strip it.
v1.0.0,get tokens for all the required FileSystems..
v1.0.0,"TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs,"
v1.0.0,job.getConfiguration());
v1.0.0,Whether we need to recursive look into the directory structure
v1.0.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.0.0,user provided one (if any).
v1.0.0,all the files in input set
v1.0.0,it is allowed for maxSize to be 0. Disable smoothing load for such cases
v1.0.0,process all nodes and create splits that are local to a node. Generate
v1.0.0,"one split per node iteration, and walk over nodes multiple times to"
v1.0.0,distribute the splits across nodes.
v1.0.0,Skip the node if it has previously been marked as completed.
v1.0.0,"for each block, copy it into validBlocks. Delete it from"
v1.0.0,blockToNodes so that the same block does not appear in
v1.0.0,two different splits.
v1.0.0,Remove all blocks which may already have been assigned to other
v1.0.0,splits.
v1.0.0,"if the accumulated split size exceeds the maximum, then"
v1.0.0,create this split.
v1.0.0,create an input split and add it to the splits array
v1.0.0,Remove entries from blocksInNode so that we don't walk these
v1.0.0,again.
v1.0.0,Done creating a single split for this node. Move on to the next
v1.0.0,node so that splits are distributed across nodes.
v1.0.0,This implies that the last few blocks (or all in case maxSize=0)
v1.0.0,were not part of a split. The node is complete.
v1.0.0,if there were any blocks left over and their combined size is
v1.0.0,"larger than minSplitNode, then combine them into one split."
v1.0.0,Otherwise add them back to the unprocessed pool. It is likely
v1.0.0,that they will be combined with other blocks from the
v1.0.0,same rack later on.
v1.0.0,This condition also kicks in when max split size is not set. All
v1.0.0,blocks on a node will be grouped together into a single split.
v1.0.0,haven't created any split on this machine. so its ok to add a
v1.0.0,smaller one for parallelism. Otherwise group it in the rack for
v1.0.0,balanced size create an input split and add it to the splits
v1.0.0,array
v1.0.0,Remove entries from blocksInNode so that we don't walk this again.
v1.0.0,The node is done. This was the last set of blocks for this node.
v1.0.0,Put the unplaced blocks back into the pool for later rack-allocation.
v1.0.0,Node is done. All blocks were fit into node-local splits.
v1.0.0,Check if node-local assignments are complete.
v1.0.0,All nodes have been walked over and marked as completed or all blocks
v1.0.0,have been assigned. The rest should be handled via rackLock assignment.
v1.0.0,"LOG.info(""DEBUG: Terminated node allocation with : CompletedNodes: """
v1.0.0,"+ completedNodes.size() + "", size left: "" + totalLength);"
v1.0.0,"if blocks in a rack are below the specified minimum size, then keep them"
v1.0.0,"in 'overflow'. After the processing of all racks is complete, these"
v1.0.0,overflow blocks will be combined into splits.
v1.0.0,Process all racks over and over again until there is no more work to do.
v1.0.0,Create one split for this rack before moving over to the next rack.
v1.0.0,Come back to this rack after creating a single split for each of the
v1.0.0,remaining racks.
v1.0.0,"Process one rack location at a time, Combine all possible blocks that"
v1.0.0,reside on this rack as one split. (constrained by minimum and maximum
v1.0.0,split size).
v1.0.0,iterate over all racks
v1.0.0,"for each block, copy it into validBlocks. Delete it from"
v1.0.0,blockToNodes so that the same block does not appear in
v1.0.0,two different splits.
v1.0.0,"if the accumulated split size exceeds the maximum, then"
v1.0.0,create this split.
v1.0.0,create an input split and add it to the splits array
v1.0.0,"if we created a split, then just go to the next rack"
v1.0.0,"if there is a minimum size specified, then create a single split"
v1.0.0,"otherwise, store these blocks into overflow data structure"
v1.0.0,There were a few blocks in this rack that
v1.0.0,remained to be processed. Keep them in 'overflow' block list.
v1.0.0,These will be combined later.
v1.0.0,Process all overflow blocks
v1.0.0,"This might cause an exiting rack location to be re-added,"
v1.0.0,but it should be ok.
v1.0.0,"if the accumulated split size exceeds the maximum, then"
v1.0.0,create this split.
v1.0.0,create an input split and add it to the splits array
v1.0.0,"Process any remaining blocks, if any."
v1.0.0,create an input split
v1.0.0,add this split to the list that is returned
v1.0.0,long num = totLength / maxSize;
v1.0.0,all blocks for all the files in input set
v1.0.0,mapping from a rack name to the list of blocks it has
v1.0.0,mapping from a block to the nodes on which it has replicas
v1.0.0,mapping from a node to the list of blocks that it contains
v1.0.0,populate all the blocks for all files
v1.0.0,stop all services
v1.0.0,1.write application state to file so that the client can get the state of the application
v1.0.0,if master exit
v1.0.0,2.clear tmp and staging directory
v1.0.0,waiting for client to get application state
v1.0.0,stop the RPC server
v1.0.0,add a shutdown hook
v1.0.0,init app state storage
v1.0.0,init event dispacher
v1.0.0,init location manager
v1.0.0,init container allocator
v1.0.0,init a rpc service
v1.0.0,recover matrix meta if needed
v1.0.0,recover ps attempt information if need
v1.0.0,init parameter server manager
v1.0.0,recover task information if needed
v1.0.0,init psagent manager and register psagent manager event
v1.0.0,a dummy data spliter is just for test now
v1.0.0,recover data splits information if needed
v1.0.0,init worker manager and register worker manager event
v1.0.0,register slow worker/ps checker
v1.0.0,register app manager event and finish event
v1.0.0,start a web service if use yarn deploy mode
v1.0.0,load from app state storage first if attempt index great than 1(the master is not the first
v1.0.0,retry)
v1.0.0,"if load failed, just build a new MatrixMetaManager"
v1.0.0,load ps attempt index from app state storage first if attempt index great than 1(the master
v1.0.0,is not the first retry)
v1.0.0,load task information from app state storage first if attempt index great than 1(the master
v1.0.0,is not the first retry)
v1.0.0,"if load failed, just build a new AMTaskManager"
v1.0.0,load data splits information from app state storage first if attempt index great than 1(the
v1.0.0,master is not the first retry)
v1.0.0,"if load failed, we need to recalculate the data splits"
v1.0.0,parse parameter server counters
v1.0.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.0.0,refresh last heartbeat timestamp
v1.0.0,send a state update event to the specific PSAttempt
v1.0.0,check if parameter server can commit now.
v1.0.0,check matrix metadata inconsistencies between master and parameter server.
v1.0.0,"if a matrix exists on the Master and does not exist on ps, then it is necessary to notify ps to establish the matrix"
v1.0.0,"if a matrix exists on the ps and does not exist on master, then it is necessary to notify ps to remove the matrix"
v1.0.0,"if psAttemptId is not in monitor set, just return a PSCOMMAND_SHUTDOWN command."
v1.0.0,check whether psagent heartbeat timeout
v1.0.0,check whether parameter server heartbeat timeout
v1.0.0,check whether worker heartbeat timeout
v1.0.0,choose a unused port
v1.0.0,start RPC server
v1.0.0,find matrix partitions from master matrix meta manager for this parameter server
v1.0.0,remove this parameter server attempt from monitor set
v1.0.0,remove this parameter server attempt from monitor set
v1.0.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.0.0,find workergroup in worker manager
v1.0.0,"if this worker group does not initialized, just return WORKERGROUP_NOTREADY"
v1.0.0,"if this worker group run over, just return WORKERGROUP_EXITED"
v1.0.0,"if this worker group is running now, return tasks, workers, data splits for it"
v1.0.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.0.0,"if worker attempt id is not in monitor set, we should shutdown it"
v1.0.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.0.0,"in ANGEL_PS mode, task id may can not know advance"
v1.0.0,update the clock for this matrix
v1.0.0,"get Task meta from task manager, if can not find, just new a AMTask object and put it to task manager"
v1.0.0,"in ANGEL_PS mode, task id may can not know advance"
v1.0.0,update task iteration
v1.0.0,private boolean matrixInited;
v1.0.0,"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.0.0,dispatch matrix partitions to parameter servers
v1.0.0,update matrix id generator
v1.0.0,"check whether the matrix name conflicts with the existing matrix names, the matrix name must be only"
v1.0.0,dispatch matrix partitions to parameter servers
v1.0.0,get matrix ids in the parameter server report
v1.0.0,get the matrices parameter server need to create and delete
v1.0.0,"if a matrix exists on parameter server but not exist on master, we should notify the parameter server to remove this matrix"
v1.0.0,"if a matrix exists on master but not exist on parameter server, this parameter server need build it."
v1.0.0,waitForMatrixReleaseOnPS(matrixId);
v1.0.0,"Calculate how many splits we need. As each task handles a separate split of data, so we want"
v1.0.0,the number of splits equal to the number of tasks
v1.0.0,split data
v1.0.0,dispatch the splits to workergroups
v1.0.0,split data
v1.0.0,dispatch the splits to workergroups
v1.0.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.0.0,"first, then divided by expected split number"
v1.0.0,get input format class from configuration and then instantiation a input format object
v1.0.0,split data
v1.0.0,Set split minsize and maxsize to expected split size. We need to get the total size of data
v1.0.0,"first, then divided by expected split number"
v1.0.0,get input format class from configuration and then instantiation a input format object
v1.0.0,split data
v1.0.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.0.0,need to fine tune the number of workergroup and task based on the actual split number
v1.0.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.0.0,Record the location information for the splits in order to data localized schedule
v1.0.0,"Since the actual split size is sometimes not exactly equal to the expected split size, we"
v1.0.0,need to fine tune the number of workergroup and task based on the actual split number
v1.0.0,"Dispatch data splits to workergroups, each SplitClassification corresponds to a workergroup."
v1.0.0,Record the location information for the splits in order to data localized schedule
v1.0.0,write meta data to a temporary file
v1.0.0,rename the temporary file to final file
v1.0.0,"if the file exists, read from file and deserialize it"
v1.0.0,write task meta
v1.0.0,write ps meta
v1.0.0,generate a temporary file
v1.0.0,write task meta to the temporary file first
v1.0.0,rename the temporary file to the final file
v1.0.0,"if last final task file exist, remove it"
v1.0.0,find task meta file which has max timestamp
v1.0.0,"if the file does not exist, just return null"
v1.0.0,read task meta from file and deserialize it
v1.0.0,generate a temporary file
v1.0.0,write ps meta to the temporary file first.
v1.0.0,rename the temporary file to the final file
v1.0.0,"if the old final file exist, just remove it"
v1.0.0,find ps meta file
v1.0.0,"if ps meta file does not exist, just return null"
v1.0.0,read ps meta from file and deserialize it
v1.0.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.0.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.0.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.0.0,create the topology tables
v1.0.0,Transitions from the NEW state.
v1.0.0,PA_FAILMSG
v1.0.0,Transitions from the UNASSIGNED state.
v1.0.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG"
v1.0.0,event
v1.0.0,Transitions from the ASSIGNED state.
v1.0.0,"this happened when launch thread run slowly, and PA_REGISTER event"
v1.0.0,dispatched before PA_CONTAINER_LAUNCHED event
v1.0.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.0.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.0.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.0.0,create the topology tables
v1.0.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will"
v1.0.0,retry another attempt or failed
v1.0.0,release container
v1.0.0,TODO
v1.0.0,set the launch time
v1.0.0,"set tarckerName,httpPort, which used by webserver"
v1.0.0,added to psManager so psManager can monitor it;
v1.0.0,psAttempt.getContext().getParameterServerManager.registerPSAttempt(psAttempt.attemptId);
v1.0.0,set the finish time only if launch time is set
v1.0.0,"ParameterServerJVM.setVMEnv(myEnv, conf);"
v1.0.0,Set up the launch command
v1.0.0,Duplicate the ByteBuffers for access by multiple containers.
v1.0.0,Construct the actual Container
v1.0.0,Application resources
v1.0.0,Application environment
v1.0.0,Service data
v1.0.0,Tokens
v1.0.0,Set up JobConf to be localized properly on the remote NM.
v1.0.0,Setup DistributedCache
v1.0.0,Setup up task credentials buffer
v1.0.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.0.0,Add the env variables passed by the user
v1.0.0,Set logging level in the environment.
v1.0.0,"This is so that, if the child forks another ""bin/hadoop"" (common in"
v1.0.0,streaming) it will have the correct loglevel.
v1.0.0,Setup the log4j prop
v1.0.0,Add main class and its arguments
v1.0.0,Finally add the jvmID
v1.0.0,vargs.add(String.valueOf(jvmID.getId()));
v1.0.0,Final commmand
v1.0.0,Transitions from the NEW state.
v1.0.0,Transitions from the RUNNING state.
v1.0.0,Transitions from the SUCCEEDED state
v1.0.0,Transitions from the KILLED state
v1.0.0,Transitions from the FAILED state
v1.0.0,Transitions from the NEW state.
v1.0.0,Transitions from the SCHEDULED state.
v1.0.0,Transitions from the RUNNING state.
v1.0.0,"another attempt launched,"
v1.0.0,Transitions from the SUCCEEDED state
v1.0.0,Transitions from the KILLED state
v1.0.0,Transitions from the FAILED state
v1.0.0,add diagnostic
v1.0.0,Set up the launch command
v1.0.0,Duplicate the ByteBuffers for access by multiple containers.
v1.0.0,Construct the actual Container
v1.0.0,Application resources
v1.0.0,Application environment
v1.0.0,Service data
v1.0.0,Tokens
v1.0.0,Set up JobConf to be localized properly on the remote NM.
v1.0.0,Setup DistributedCache
v1.0.0,Setup up task credentials buffer
v1.0.0,LocalStorageToken is needed irrespective of whether security is enabled
v1.0.0,or not.
v1.0.0,"TokenCache.setJobToken(jobToken, taskCredentials);"
v1.0.0,"Add pwd to LD_LIBRARY_PATH, add this before adding anything else"
v1.0.0,Construct the actual Container
v1.0.0,The null fields are per-container and will be constructed for each
v1.0.0,container separately.
v1.0.0,Set up the launch command
v1.0.0,Duplicate the ByteBuffers for access by multiple containers.
v1.0.0,Construct the actual Container
v1.0.0,"a * in the classpath will only find a .jar, so we need to filter out"
v1.0.0,all .jars and add everything else
v1.0.0,Propagate the system classpath when using the mini cluster
v1.0.0,Add standard Hadoop classes
v1.0.0,Cache archives
v1.0.0,Cache files
v1.0.0,Sanity check
v1.0.0,Add URI fragment or just the filename
v1.0.0,Add the env variables passed by the user
v1.0.0,Set logging level in the environment.
v1.0.0,Setup the log4j prop
v1.0.0,Add main class and its arguments
v1.0.0,Finally add the jvmID
v1.0.0,vargs.add(String.valueOf(jvmID.getId()));
v1.0.0,Final commmand
v1.0.0,Add the env variables passed by the user
v1.0.0,Set logging level in the environment.
v1.0.0,Setup the log4j prop
v1.0.0,Add main class and its arguments
v1.0.0,Final commmand
v1.0.0,"if amTask is not null, we should clone task state from it"
v1.0.0,"if all parameter server complete commit, master can commit now"
v1.0.0,init and start master committer
v1.0.0,Transitions from the NEW state.
v1.0.0,Transitions from the UNASSIGNED state.
v1.0.0,"when user kill task, or task timeout, psAttempt will receive TA_FAILMSG event"
v1.0.0,Transitions from the ASSIGNED state.
v1.0.0,"this happened when launch thread run slowly, and PA_REGISTER event dispatched before"
v1.0.0,PA_CONTAINER_LAUNCHED event
v1.0.0,Transitions from the PSAttemptStateInternal.RUNNING state.
v1.0.0,Transitions from the PSAttemptStateInternal.COMMITTING state
v1.0.0,Transitions from the PSAttemptStateInternal.KILLED state
v1.0.0,Transitions from the PSAttemptStateInternal.FAILED state
v1.0.0,create the topology tables
v1.0.0,reqeuest resource:send a resource request to the resource allocator
v1.0.0,"Once the resource is applied, build and send the launch request to the container launcher"
v1.0.0,deallocator the resource of the ps attempt:send a resource deallocator request to the
v1.0.0,resource allocator
v1.0.0,set the launch time
v1.0.0,add the ps attempt to the heartbeat timeout monitoring list
v1.0.0,parse ps attempt location and put it to location manager
v1.0.0,"send PS_ATTEMPT_FAILED to AMParameterServer, AMParameterServer will retry another attempt"
v1.0.0,or failed
v1.0.0,remove ps attempt id from heartbeat timeout monitor list
v1.0.0,release container:send a release request to container launcher
v1.0.0,set the finish time only if launch time is set
v1.0.0,private long scheduledTime;
v1.0.0,Transitions from the NEW state.
v1.0.0,Transitions from the SCHEDULED state.
v1.0.0,Transitions from the RUNNING state.
v1.0.0,"another attempt launched,"
v1.0.0,Transitions from the SUCCEEDED state
v1.0.0,Transitions from the KILLED state
v1.0.0,Transitions from the FAILED state
v1.0.0,add diagnostic
v1.0.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.0.0,start a new attempt for this ps
v1.0.0,notify ps manager
v1.0.0,add diagnostic
v1.0.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.0.0,start a new attempt for this ps
v1.0.0,notify ps manager
v1.0.0,notify the event handler of state change
v1.0.0,"If new state is not RUNNING and COMMITTING, add it to state timeout monitor"
v1.0.0,"if forcedState is set, just return"
v1.0.0,else get state from state machine
v1.0.0,add this worker group to the success set
v1.0.0,check if all worker group run over
v1.0.0,add this worker group to the failed set
v1.0.0,check if too many worker groups are failed or killed
v1.0.0,notify a run failed event
v1.0.0,add this worker group to the failed set
v1.0.0,check if too many worker groups are failed or killed
v1.0.0,notify a run failed event
v1.0.0,calculate the actual number of worker groups and the total number of tasks based on the number of data split
v1.0.0,"init all tasks , workers and worker groups and put them to the corresponding maps"
v1.0.0,just return the total task number now
v1.0.0,TODO
v1.0.0,"if workerAttempt is not null, we should clone task state from it"
v1.0.0,from NEW state
v1.0.0,from SCHEDULED state
v1.0.0,get data splits location for data locality
v1.0.0,reqeuest resource:send a resource request to the resource allocator
v1.0.0,"once the resource is applied, build and send the launch request to the container launcher"
v1.0.0,notify failed message to the worker
v1.0.0,notify killed message to the worker
v1.0.0,release the allocated container
v1.0.0,notify failed message to the worker
v1.0.0,remove the worker attempt from heartbeat timeout listen list
v1.0.0,release the allocated container
v1.0.0,notify killed message to the worker
v1.0.0,remove the worker attempt from heartbeat timeout listen list
v1.0.0,clean the container
v1.0.0,notify failed message to the worker
v1.0.0,remove the worker attempt from heartbeat timeout listen list
v1.0.0,record the finish time
v1.0.0,clean the container
v1.0.0,notify killed message to the worker
v1.0.0,remove the worker attempt from heartbeat timeout listening list
v1.0.0,record the finish time
v1.0.0,"if the worker attempt launch successfully, add it to heartbeat timeout listening list"
v1.0.0,set worker attempt location
v1.0.0,notify the register message to the worker
v1.0.0,record the launch time
v1.0.0,update worker attempt metrics
v1.0.0,update tasks metrics
v1.0.0,clean the container
v1.0.0,notify the worker attempt run successfully message to the worker
v1.0.0,record the finish time
v1.0.0,init a worker attempt for the worker
v1.0.0,schedule the worker attempt
v1.0.0,add diagnostic
v1.0.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.0.0,init and start a new attempt for this ps
v1.0.0,notify worker manager
v1.0.0,add diagnostic
v1.0.0,check whether the number of failed attempts is less than the maximum number of allowed
v1.0.0,init and start a new attempt for this ps
v1.0.0,notify worker manager
v1.0.0,"If we need Yarn to restart a new application master, we should not unregister from Yarn RM"
v1.0.0,register to Yarn RM
v1.0.0,send heartbeat to Yarn RM every rmPollInterval milliseconds
v1.0.0,"catch YarnRuntimeException, we should exit and need not retry"
v1.0.0,build heartbeat request
v1.0.0,send heartbeat request to rm
v1.0.0,"This can happen if the RM has been restarted. If it is in that state,"
v1.0.0,this application must clean itself up.
v1.0.0,Setting NMTokens
v1.0.0,assgin containers
v1.0.0,"if some container is not assigned, release them"
v1.0.0,handle finish containers
v1.0.0,dispatch container exit message to corresponding components
v1.0.0,killed by framework
v1.0.0,killed by framework
v1.0.0,killed by framework
v1.0.0,get application finish state
v1.0.0,build application diagnostics
v1.0.0,TODO:add a job history for angel
v1.0.0,build unregister request
v1.0.0,send unregister request to rm
v1.0.0,Note this down for next interaction with ResourceManager
v1.0.0,based on blacklisting comments above we can end up decrementing more
v1.0.0,than requested. so guard for that.
v1.0.0,send the updated resource request to RM
v1.0.0,send 0 container count requests also to cancel previous requests
v1.0.0,Update resource requests
v1.0.0,try to assign to all nodes first to match node local
v1.0.0,try to match all rack local
v1.0.0,assign remaining
v1.0.0,Update resource requests
v1.0.0,send the container-assigned event to task attempt
v1.0.0,build the start container request use launch context
v1.0.0,send the start request to Yarn nm
v1.0.0,send the message that the container starts successfully to the corresponding component
v1.0.0,"after launching, send launched event to task attempt to move"
v1.0.0,it from ASSIGNED to RUNNING state
v1.0.0,send the message that the container starts failed to the corresponding component
v1.0.0,kill the remote container if already launched
v1.0.0,start a thread pool to startup the container
v1.0.0,See if we need up the pool size only if haven't reached the
v1.0.0,maximum limit yet.
v1.0.0,nodes where containers will run at *this* point of time. This is
v1.0.0,*not* the cluster size and doesn't need to be.
v1.0.0,"Bump up the pool size to idealPoolSize+INITIAL_POOL_SIZE, the"
v1.0.0,later is just a buffer so we are not always increasing the
v1.0.0,pool-size
v1.0.0,the events from the queue are handled in parallel
v1.0.0,using a thread pool
v1.0.0,return if already stopped
v1.0.0,shutdown any containers that might be left running
v1.0.0,Build and initialize rpc client to master
v1.0.0,Build local location
v1.0.0,"Initialize matrix info, this method will wait until master accepts the information from"
v1.0.0,client
v1.0.0,Get ps locations from master and put them to the location cache.
v1.0.0,Initialize matrix meta information
v1.0.0,Start heartbeat thread if need
v1.0.0,Start all services
v1.0.0,Register to master first
v1.0.0,Report state to master every specified time
v1.0.0,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.0.0,Stop all modules
v1.0.0,Exit the process if on ANGEL_PS_PSAGENT mode
v1.0.0,Notify run success to master only on ANGEL_PS_PSAGENT running mode
v1.0.0,Stop all modules
v1.0.0,Exit the process if on ANGEL_PS_PSAGENT mode
v1.0.0,get configuration from config file
v1.0.0,set localDir with enviroment set by nm.
v1.0.0,Update generic resource counters
v1.0.0,Updating resources specified in ResourceCalculatorProcessTree
v1.0.0,Remove the CPU time consumed previously by JVM reuse
v1.0.0,array stores clock for each row and clock
v1.0.0,local task num
v1.0.0,mapping from task index to taskId
v1.0.0,mapping from taskId to task index
v1.0.0,TODO Auto-generated method stub
v1.0.0,Generate a flush request and put it to request queue
v1.0.0,Generate a clock request and put it to request queue
v1.0.0,Generate a merge request and put it to request queue
v1.0.0,"If the matrix op log cache does not exist for the matrix, create a new one for the"
v1.0.0,matrix
v1.0.0,and add it to cache maps
v1.0.0,Add the message to the tree map
v1.0.0,"If there are flush / clock requests blocked, we need to put this merge request into"
v1.0.0,the waiting queue
v1.0.0,Launch a merge worker to merge the update to matrix op log cache
v1.0.0,Remove the message from the tree map
v1.0.0,Wake up blocked flush/clock request
v1.0.0,Add flush/clock request to listener list to waiting for all the existing
v1.0.0,updates are merged
v1.0.0,Wake up blocked flush/clock request
v1.0.0,"If all updates are merged for this matrix, we need wake up flush/clock requests which are"
v1.0.0,blocked.
v1.0.0,Get next merge message sequence id
v1.0.0,Wake up listeners(flush/clock requests) that have little sequence id than current merge
v1.0.0,position
v1.0.0,Wake up blocked merge requests
v1.0.0,Get minimal sequence id from listeners
v1.0.0,"If hogwild mode is enabled on the number of local task is more than 1 on SSP mode, we"
v1.0.0,should flush updates to local matrix storage
v1.0.0,unused now
v1.0.0,Get partitions for the matrix
v1.0.0,"Filter it, removing zero values"
v1.0.0,Doing average or not
v1.0.0,Split this row according the matrix partitions
v1.0.0,Add the splits to the result container
v1.0.0,"For each partition, we generate a update split."
v1.0.0,"Although the split is empty for partitions those without any update data,"
v1.0.0,we still need to generate a update split to update the clock info on ps.
v1.0.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.0.0,Then submit normal task until reach upper limit of flow control or all tasks are submit
v1.0.0,"if submit task in getQueue failed, we should make up the last chosen get queue index"
v1.0.0,"LOG.info(""choose put server "" + psIds[index]);"
v1.0.0,allocate the bytebuf
v1.0.0,"check the location of server is ready, if not, we should wait"
v1.0.0,get a channel to server from pool
v1.0.0,"if channel is not valid, it means maybe the connections to the server are closed"
v1.0.0,channelManager.removeChannelPool(loc);
v1.0.0,find the partition request context from cache
v1.0.0,Check if the result of the sub-request is received
v1.0.0,Update received result number
v1.0.0,Get row splits received
v1.0.0,Put the row split to the cache(row index to row splits map)
v1.0.0,"If all splits of the row are received, means this row can be merged"
v1.0.0,TODO Auto-generated method stub
v1.0.0,"Check futures, if the result of a sub-request is received, put it to the result queue"
v1.0.0,Now we just support pipelined row splits merging for dense type row
v1.0.0,Wait until the clock value of this row is greater than or equal to the value
v1.0.0,Get partitions for this row
v1.0.0,First get this row from matrix storage
v1.0.0,"If the row exists in the matrix storage and the clock value meets the requirements, just"
v1.0.0,return
v1.0.0,Get row splits of this row from the matrix cache first
v1.0.0,"If the row split does not exist in cache, get it from parameter server"
v1.0.0,Wait the final result
v1.0.0,Put it to the matrix cache
v1.0.0,Split the matrix oplog according to the matrix partitions
v1.0.0,"If need update clock, we should send requests to all partitions"
v1.0.0,use update index if exist
v1.0.0,Filter the rowIds which are fetching now
v1.0.0,Send the rowIndex to rpc dispatcher and return immediately
v1.0.0,Get the partition to sub-row splits map:use to storage the rows stored in a matrix partition
v1.0.0,Generate dispatch items and add them to the corresponding queues
v1.0.0,Pre-fetching is disable default
v1.0.0,matrix id to clock map
v1.0.0,"task index, it must be unique for whole application"
v1.0.0,matrix id to task update index map. each task may only update some specific part for a matrix
v1.0.0,if (matrixManager.getMatrixMeta(createResponse.getMatrixId()) != null) {
v1.0.0,return matrixManager.getMatrixMeta(createResponse.getMatrixId());
v1.0.0,}
v1.0.0,Deserialize data splits meta
v1.0.0,Get workers
v1.0.0,Send request to every ps
v1.0.0,Wait the responses
v1.0.0,Update clock cache
v1.0.0,Get row from cache.
v1.0.0,"if row clock is satisfy ssp staleness limit, just return."
v1.0.0,Get row from ps.
v1.0.0,"For ASYNC mode, just get from pss."
v1.0.0,"For BSP/SSP, get rows from storage/cache first"
v1.0.0,Get from ps.
v1.0.0,"For ASYNC, just get rows from pss."
v1.0.0,no more retries.
v1.0.0,calculate sleep time and return.
v1.0.0,parse the i-th sleep-time
v1.0.0,parse the i-th number-of-retries
v1.0.0,calculateSleepTime may overflow.
v1.0.0,"A few common retry policies, with no delays."
v1.0.0,response will be null for one way messages.
v1.0.0,serial number and list size
v1.0.0,indicates whether this connection's life cycle is managed
v1.0.0,See if we already have a connection (common case)
v1.0.0,create a unique lock for this RS + protocol (if necessary)
v1.0.0,get the RS lock
v1.0.0,do one more lookup in case we were stalled above
v1.0.0,Only create isa when we need to.
v1.0.0,definitely a cache miss. establish an RPC for
v1.0.0,this RS
v1.0.0,Throw what the RemoteException was carrying.
v1.0.0,check
v1.0.0,every
v1.0.0,minutes
v1.0.0,TODO
v1.0.0,创建failoverHandler
v1.0.0,"The number of times this invocation handler has ever been failed over,"
v1.0.0,before this method invocation attempt. Used to prevent concurrent
v1.0.0,failed method invocations from triggering multiple failover attempts.
v1.0.0,Make sure that concurrent failed method invocations
v1.0.0,only cause a
v1.0.0,single actual fail over.
v1.0.0,RpcController + Message in the method args
v1.0.0,(generated code from RPC bits in .proto files have
v1.0.0,RpcController)
v1.0.0,"LOG.info(""method "" + method.getName() + ""construct request time = """
v1.0.0,+ (System.currentTimeMillis() - beforeConstructTs));
v1.0.0,get an instance of the method arg type
v1.0.0,RpcController + Message in the method args
v1.0.0,(generated code from RPC bits in .proto files have
v1.0.0,RpcController)
v1.0.0,Message (hand written code usually has only a single
v1.0.0,argument)
v1.0.0,log any RPC responses that are slower than the configured
v1.0.0,warn
v1.0.0,response time or larger than configured warning size
v1.0.0,"when tagging, we let TooLarge trump TooSmall to keep"
v1.0.0,output simple
v1.0.0,note that large responses will often also be slow.
v1.0.0,provides a count of log-reported slow responses
v1.0.0,RpcController + Message in the method args
v1.0.0,(generated code from RPC bits in .proto files have
v1.0.0,RpcController)
v1.0.0,unexpected
v1.0.0,"in the protobuf methods, args[1] is the only significant argument"
v1.0.0,for JSON encoding
v1.0.0,base information that is reported regardless of type of call
v1.0.0,DefaultChannelFuture.setUseDeadLockChecker(false);
v1.0.0,Set up.
v1.0.0,Configure the event pipeline factory.
v1.0.0,Make a new connection.
v1.0.0,Need to reconnect
v1.0.0,Upgrade to write lock
v1.0.0,Downgrade to read lock:
v1.0.0,(TODO: why use writeLock? why not use this.channel instead of channel?
v1.0.0,Remove all pending requests (will be canceled after relinquishing
v1.0.0,write lock).
v1.0.0,Cancel any pending requests by sending errors to the callbacks:
v1.0.0,Close the channel:
v1.0.0,Close the connection:
v1.0.0,Shut down all thread pools to exit.
v1.0.0,"LOG.info(""serial "" + serial + ""start time = "" + System.currentTimeMillis());"
v1.0.0,See NettyServer.prepareResponse for where we write out the response.
v1.0.0,"It writes the call.id (int), a boolean signifying any error (and if"
v1.0.0,"so the exception name/trace), and the response bytes"
v1.0.0,Read the call id.
v1.0.0,"When the stream is closed, protobuf doesn't raise an EOFException,"
v1.0.0,"instead, it returns a null message object."
v1.0.0,channel = e.getChannel();
v1.0.0,channel = e.getChannel();
v1.0.0,"LOG.info(""method "" + dataPack.getSerial() + "" received ts = "" +"
v1.0.0,System.currentTimeMillis());
v1.0.0,Make an ml rpc client.
v1.0.0,"It would be good widen this to just Throwable, but IOException is what we"
v1.0.0,allow now
v1.0.0,not implemented
v1.0.0,not implemented
v1.0.0,"track what RpcEngine is used by a proxy class, for stopProxy()"
v1.0.0,cache of RpcEngines by protocol
v1.0.0,return the RpcEngine configured to handle a protocol
v1.0.0,We only handle the ConnectException.
v1.0.0,This is the exception we can't handle.
v1.0.0,check if timed out
v1.0.0,wait for retry
v1.0.0,IGNORE
v1.0.0,return the RpcEngine that handles a proxy object
v1.0.0,The default implementation works synchronously
v1.0.0,punt: allocate a new buffer & copy into it
v1.0.0,"LOG.info(System.getProperty(""user.dir""));"
v1.0.0,get tokens for all the required FileSystems..
v1.0.0,Whether we need to recursive look into the directory structure
v1.0.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.0.0,user provided one (if any).
v1.0.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.0.0,get tokens for all the required FileSystems..
v1.0.0,Whether we need to recursive look into the directory structure
v1.0.0,creates a MultiPathFilter with the hiddenFileFilter and the
v1.0.0,user provided one (if any).
v1.0.0,"LOG.info(""Total input paths to process : "" + result.size());"
v1.0.0,a simple hdfs copy function assume src path and dest path are in same hdfs
v1.0.0,and FileSystem object has same schema
v1.0.0,"LOG.warn(""interrupted while sleeping"", ie);"
v1.0.0,private static UnpooledByteBufAllocator allocator = new UnpooledByteBufAllocator(false);
v1.0.0,public static String getHostname() {
v1.0.0,try {
v1.0.0,"return new StringBuilder().append("""").append(InetAddress.getLocalHost()).toString();"
v1.0.0,} catch (UnknownHostException uhe) {
v1.0.0,}
v1.0.0,"return new StringBuilder().append("""").append(uhe).toString();"
v1.0.0,}
v1.0.0,"public static void startupShutdownMessage(Class<?> clazz, String[] args, Log LOG) {"
v1.0.0,String hostname = getHostname();
v1.0.0,String classname = clazz.getSimpleName();
v1.0.0,"LOG.info(toStartupShutdownString(""STARTUP_MSG: "", new String[]{new"
v1.0.0,"StringBuilder().append(""Starting "").append(classname).toString(), new"
v1.0.0,"StringBuilder().append(""  host = "").append(hostname).toString(), new"
v1.0.0,"StringBuilder().append(""  args = "").append(Arrays.asList(args)).toString(), new"
v1.0.0,"StringBuilder().append(""  version = "").append(VersionInfo.getVersion()).toString(), new"
v1.0.0,"StringBuilder().append(""  build = "").append(VersionInfo.getUrl()).append("" -r "").append(VersionInfo.getRevision()).append(""; compiled by '"").append(VersionInfo.getUser()).append(""' on "").append(VersionInfo.getDate()).toString()}));"
v1.0.0,
v1.0.0,"Runtime.getRuntime().addShutdownHook(new Thread(LOG, classname, hostname) {"
v1.0.0,public void run() {
v1.0.0,"this.val$LOG.info(StringUtils.access$000(""SHUTDOWN_MSG: "", new String[]{""Shutting down "" +"
v1.0.0,"this.val$classname + "" at "" + this.val$hostname}));"
v1.0.0,}
v1.0.0,});
v1.0.0,}
v1.0.0,"We we interrupted because we're meant to stop? If not, just"
v1.0.0,continue ignoring the interruption
v1.0.0,Recalculate waitTime.
v1.0.0,// Begin delegation to Thread
v1.0.0,// End delegation to Thread
v1.0.0,load angel system configuration
v1.0.0,load user configuration:
v1.0.0,1. user config file
v1.0.0,2. command lines
v1.0.0,"add user resource files to ""angel.lib.jars"" to upload to hdfs"
v1.0.0,load user job jar if it exist
v1.0.0,Expand the environment variable
v1.0.0,instance submitter class
v1.0.0,Add default fs(local fs) for lib jars.
v1.0.0,Obtain filename from path
v1.0.0,Split filename to prexif and suffix (extension)
v1.0.0,Check if the filename is okay
v1.0.0,Prepare temporary file
v1.0.0,Prepare buffer for data copying
v1.0.0,Open and check input stream
v1.0.0,Open output stream and copy data between source file in JAR and the temporary file
v1.0.0,"If read/write fails, close streams safely before throwing an exception"
v1.0.0,"Finally, load the library"
v1.0.0,little endian load order
v1.0.0,tail
v1.0.0,fallthrough
v1.0.0,fallthrough
v1.0.0,finalization
v1.0.0,fmix(h1);
v1.0.0,----------
v1.0.0,body
v1.0.0,----------
v1.0.0,tail
v1.0.0,----------
v1.0.0,finalization
v1.0.0,----------
v1.0.0,body
v1.0.0,----------
v1.0.0,tail
v1.0.0,----------
v1.0.0,finalization
v1.0.0,JobStateProto jobState = report.getJobState();
v1.0.0,generate tmp output directory
v1.0.0,the leaf level file should be readable by others
v1.0.0,the subdirs in the path should have execute permissions for
v1.0.0,others
v1.0.0,2.get job id
v1.0.0,Credentials credentials = new Credentials();
v1.0.0,4.copy resource files to hdfs
v1.0.0,5.write configuration to a xml file
v1.0.0,6.create am container context
v1.0.0,7.Submit to ResourceManager
v1.0.0,8.get app master client
v1.0.0,Create a number of filenames in the JobTracker's fs namespace
v1.0.0,add all the command line files/ jars and archive
v1.0.0,first copy them to jobtrackers filesystem
v1.0.0,should not throw a uri exception
v1.0.0,should not throw an uri excpetion
v1.0.0,set the timestamps of the archives and files
v1.0.0,set the public/private visibility of the archives and files
v1.0.0,get DelegationToken for each cached file
v1.0.0,check if we do not need to copy the files
v1.0.0,is jt using the same file system.
v1.0.0,just checking for uri strings... doing no dns lookups
v1.0.0,to see if the filesystems are the same. This is not optimal.
v1.0.0,but avoids name resolution.
v1.0.0,this might have name collisions. copy will throw an exception
v1.0.0,parse the original path to create new path
v1.0.0,check for ports
v1.0.0,Write job file to JobTracker's fs
v1.0.0,Setup resource requirements
v1.0.0,Setup LocalResources
v1.0.0,Setup security tokens
v1.0.0,Setup the command to run the AM
v1.0.0,Add AM user command opts
v1.0.0,Final command
v1.0.0,Setup the CLASSPATH in environment
v1.0.0,"i.e. add { Hadoop jars, job jar, CWD } to classpath."
v1.0.0,Setup the environment variables for Admin first
v1.0.0,"Setup the environment variables (LD_LIBRARY_PATH, etc)"
v1.0.0,Parse distributed cache
v1.0.0,Setup ContainerLaunchContext for AM container
v1.0.0,Set up the ApplicationSubmissionContext
v1.0.0,resposne.encode(buf);
v1.0.0,TODO:
v1.0.0,resposne.encode(buf);
v1.0.0,TODO:
v1.0.0,resposne.encode(buf);
v1.0.0,TODO:
v1.0.0,resposne.encode(buf);
v1.0.0,TODO:
v1.0.0,private final ParameterServerId serverId;
v1.0.0,private final PSIdProto idProto;
v1.0.0,to exit
v1.0.0,mkdir does not throw exception if path exits
v1.0.0,commitTaskPool.shutdown();
v1.0.0,private final ParameterServer psServer;
v1.0.0,TODO
v1.0.0,"when we should write snapshot to hdfs? clearly, we have two methods:"
v1.0.0,"1. write snapshot at regular time, if there are updates, just write them."
v1.0.0,"2. write snapshot every N iterations, this method depends on notification of master"
v1.0.0,"FSDataOutputStream output = fileContext.create(snapshotsTempFilePath,"
v1.0.0,EnumSet.of(CreateFlag.CREATE));
v1.0.0,@brief get filename of the old snapshot written before
v1.0.0,"no snapshotFile write before, maybe write snapshots the first time"
v1.0.0,start end
v1.0.0,rowtype
v1.0.0,data.rewind();
v1.0.0,data.rewind();
v1.0.0,data.rewind();
v1.0.0,Pass the matrix and partition number field
v1.0.0,Mapping from taskId to clock value.
v1.0.0,int[] keys = sparseRep.getKeys();
v1.0.0,int[] values = sparseRep.getValues();
v1.0.0,boolean[] used = sparseRep.getUsed();
v1.0.0,nnz = 0;
v1.0.0,for (int i = 0; i < keys.length; i++)
v1.0.0,if (used[i]) {
v1.0.0,"denseRep.put(keys[i], values[i]);"
v1.0.0,nnz++;
v1.0.0,}
v1.0.0,sparseRep = null;
v1.0.0,int[] keys = sparseRep.getKeys();
v1.0.0,int[] values = sparseRep.getValues();
v1.0.0,boolean[] used = sparseRep.getUsed();
v1.0.0,for (int i = 0; i < keys.length; i++)
v1.0.0,if (used[i]) {
v1.0.0,"denseRep.put(keys[i], values[i]);"
v1.0.0,}
v1.0.0,sparseRep = null;
v1.0.0,output.writeInt(data.length);
v1.0.0,@Override
v1.0.0,public void serialize(ByteBuf buf) {
v1.0.0,if (sparseRep != null)
v1.0.0,return serializeSparse();
v1.0.0,else if (denseRep != null)
v1.0.0,return serializeDense();
v1.0.0,return serializeEmpty();
v1.0.0,}
v1.0.0,int[] keys = sparseRep.getKeys();
v1.0.0,int[] values = sparseRep.getValues();
v1.0.0,boolean[] used = sparseRep.getUsed();
v1.0.0,int idx = 0;
v1.0.0,for (int i = 0; i < keys.length; i++)
v1.0.0,if (used[i]) {
v1.0.0,"keysBuf.put(idx, keys[i]);"
v1.0.0,"valuesBuf.put(idx, values[i]);"
v1.0.0,idx++;
v1.0.0,}
v1.0.0,int[] keys = sparseRep.getKeys();
v1.0.0,int[] values = sparseRep.getValues();
v1.0.0,boolean[] used = sparseRep.getUsed();
v1.0.0,"int ov, k, v;"
v1.0.0,for (int i = 0; i < keys.length; i++) {
v1.0.0,if (used[i]) {
v1.0.0,k = keys[i];
v1.0.0,ov = denseRep.get(k);
v1.0.0,v = ov + values[i];
v1.0.0,"denseRep.put(k, v);"
v1.0.0,if (ov != 0 && v == 0)
v1.0.0,nnz--;
v1.0.0,}
v1.0.0,}
v1.0.0,"add the PSAgentContext,need fix"
v1.0.0,set MatrixPartitionLocation
v1.0.0,set attribute
v1.0.0,TODO Auto-generated method stub
v1.0.0,@brief Sorted index for non-zero items
v1.0.0,@brief Number of non-zero items in this vector
v1.0.0,@brief Array to store values.
v1.0.0,@brief sum of the square of all of element
v1.0.0,"LOG.error(""Cannot perform plus operation on SparseDoubleSortedVector"");"
v1.0.0,TODO Auto-generated method stub
v1.0.0,TODO:
v1.0.0,TODO:
v1.0.0,write the max abs
v1.0.0,"Thread.currentThread().getContextClassLoader().getResourceAsStream(""feature_conf.xml"");"
v1.0.0,this.matchList = new ArrayList<Match>();
v1.0.0,"LOG.debug(String.format(""index: %d, value: %s"", index, value));"
v1.0.0,"LOG.debug(String.format(""index: %d, value: %s"", index, value));"
v1.0.0,"LOG.debug(String.format(""index: %d, value: %s"", index, value));"
v1.0.0,"LOG.debug(String.format(""index: %d, value: %s"", index, value));"
v1.0.0,"LOG.debug(""target="" + target);"
v1.0.0,TODO Auto-generated method stub
v1.0.0,TODO Auto-generated method stub
v1.0.0,TODO Auto-generated method stub
v1.0.0,get configuration from config file
v1.0.0,set localDir with enviroment set by nm.
v1.0.0,get master location
v1.0.0,init task manager and start tasks
v1.0.0,start heartbeat thread
v1.0.0,taskManager.assignTaskIds(response.getTaskidsList());
v1.0.0,todo
v1.0.0,"if worker timeout, it may be knocked off."
v1.0.0,"SUCCESS, do nothing"
v1.0.0,heartbeatFailedTime = 0;
v1.0.0,private KEY currentKey;
v1.0.0,will be created
v1.0.0,TODO Auto-generated method stub
v1.0.0,Bitmap bitmap = new Bitmap();
v1.0.0,int max = indexArray[size - 1];
v1.0.0,byte [] bitIndexArray = new byte[max / 8 + 1];
v1.0.0,for(int i = 0; i < size; i++){
v1.0.0,int bitIndex = indexArray[i] >> 3;
v1.0.0,int bitOffset = indexArray[i] - (bitIndex << 3);
v1.0.0,switch(bitOffset){
v1.0.0,case 0:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x01);break;
v1.0.0,case 1:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x02);break;
v1.0.0,case 2:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x04);break;
v1.0.0,case 3:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x08);break;
v1.0.0,case 4:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x10);break;
v1.0.0,case 5:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x20);break;
v1.0.0,case 6:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x40);break;
v1.0.0,case 7:bitIndexArray[bitIndex] = (byte) (bitIndexArray[bitIndex] & 0x80);break;
v1.0.0,}
v1.0.0,}
v1.0.0,//////////////////////////////
v1.0.0,Application Configs
v1.0.0,//////////////////////////////
v1.0.0,//////////////////////////////
v1.0.0,Master Configs
v1.0.0,//////////////////////////////
v1.0.0,//////////////////////////////
v1.0.0,Worker Configs
v1.0.0,//////////////////////////////
v1.0.0,//////////////////////////////
v1.0.0,Task Configs
v1.0.0,//////////////////////////////
v1.0.0,//////////////////////////////
v1.0.0,ParameterServer Configs
v1.0.0,//////////////////////////////
v1.0.0,////////////////// IPC //////////////////////////
v1.0.0,//////////////////////////////
v1.0.0,Matrix transfer Configs.
v1.0.0,//////////////////////////////
v1.0.0,//////////////////////////////
v1.0.0,Matrix transfer Configs.
v1.0.0,//////////////////////////////
v1.0.0,Configs used to ANGEL_PS_PSAGENT running mode future.
v1.0.0,model parse
v1.0.0,private Configuration conf;
v1.0.0,TODO Auto-generated constructor stub
v1.0.0,set basic configuration keys
v1.0.0,use local deploy mode and dummy dataspliter
v1.0.0,"conf.setInt(AngelConfiguration.ANGEL_PREPROCESS_VECTOR_MAXDIM, 10000);"
v1.0.0,"conf.set(AngelConfiguration.ANGEL_MODEL_PATH, LOCAL_FS + TMP_PATH + ""/out"");"
v1.0.0,Set trainning data path
v1.0.0,Set save model path
v1.0.0,Set log path
v1.0.0,Set actionType train
v1.0.0,get a angel client
v1.0.0,add matrix
v1.0.0,Feature number of train data
v1.0.0,Number of nonzero features
v1.0.0,Tree number
v1.0.0,Tree depth
v1.0.0,Split number
v1.0.0,Feature sample ratio
v1.0.0,Data format
v1.0.0,Learning rate
v1.0.0,Set basic configuration keys
v1.0.0,Use local deploy mode and dummy data spliter
v1.0.0,"set input, output path"
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,Set GBDT algorithm parameters
v1.0.0,Submit GBDT Train Task
v1.0.0,Load Model from HDFS.
v1.0.0,Set local deploy mode
v1.0.0,Set basic configuration keys
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,"Set trainning data, save model, log path"
v1.0.0,Set actionType train
v1.0.0,Set MF algorithm parameters
v1.0.0,Feature number of train data
v1.0.0,Total iteration number
v1.0.0,Validation sample Ratio
v1.0.0,"Data format, libsvm or dummy"
v1.0.0,Train batch number per epoch.
v1.0.0,Batch number
v1.0.0,Learning rate
v1.0.0,Decay of learning rate
v1.0.0,Regularization coefficient
v1.0.0,Set local deploy mode
v1.0.0,Set basic configuration keys
v1.0.0,Set data format
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,set sgd LR algorithm parameters #feature #epoch
v1.0.0,Set trainning data path
v1.0.0,Set save model path
v1.0.0,Set log path
v1.0.0,Set actionType train
v1.0.0,Set trainning data path
v1.0.0,Set load model path
v1.0.0,Set save model path
v1.0.0,Set log path
v1.0.0,Set actionType incremental train
v1.0.0,Set trainning data path
v1.0.0,Set load model path
v1.0.0,Set predict result path
v1.0.0,Set actionType prediction
v1.0.0,LOG.info(sigmoid(data[i]));
v1.0.0,LOG.info(Math.exp(-data[i]));
v1.0.0,when b is a negative number
v1.0.0,Cluster center number
v1.0.0,Feature number of train data
v1.0.0,Total iteration number
v1.0.0,Sample ratio per mini-batch
v1.0.0,C
v1.0.0,Set local deploy mode
v1.0.0,Set basic configuration keys
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,set Kmeans algorithm parameters #cluster #feature #epoch
v1.0.0,Set trainning data path
v1.0.0,Set data format
v1.0.0,Set save model path
v1.0.0,Set log sava path
v1.0.0,Set actionType train
v1.0.0,Set load model path
v1.0.0,Set predict result path
v1.0.0,Set actionType prediction
v1.0.0,Set log sava path
v1.0.0,Feature number of train data
v1.0.0,Total iteration number
v1.0.0,Validation Ratio
v1.0.0,Data format
v1.0.0,Train batch number per epoch.
v1.0.0,Learning rate
v1.0.0,Decay of learning rate
v1.0.0,Regularization coefficient
v1.0.0,Set basic configuration keys
v1.0.0,Set data format
v1.0.0,Use local deploy mode
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,set sgd SVM algorithm parameters
v1.0.0,"set input, output path"
v1.0.0,Set save model path
v1.0.0,Set actionType train
v1.0.0,Set log path
v1.0.0,Submit LR Train Task
v1.0.0,Set trainning data path
v1.0.0,Set load model path
v1.0.0,Set save model path
v1.0.0,Set actionType incremental train
v1.0.0,Set log path
v1.0.0,Feature number of train data
v1.0.0,Total iteration number
v1.0.0,Validation sample Ratio
v1.0.0,"Data format, libsvm or dummy"
v1.0.0,Train batch number per epoch.
v1.0.0,Learning rate
v1.0.0,Decay of learning rate
v1.0.0,Regularization coefficient
v1.0.0,Set local deploy mode
v1.0.0,Set basic configuration keys
v1.0.0,Set data format
v1.0.0,"set angel resource parameters #worker, #task, #PS"
v1.0.0,set sgd LR algorithm parameters #feature #epoch
v1.0.0,Set trainning data path
v1.0.0,Set save model path
v1.0.0,Set log path
v1.0.0,Set actionType train
v1.0.0,Set trainning data path
v1.0.0,Set load model path
v1.0.0,Set save model path
v1.0.0,Set actionType incremental train
v1.0.0,Set log path
v1.0.0,Set trainning data path
v1.0.0,Set load model path
v1.0.0,Set predict result path
v1.0.0,Set log sava path
v1.0.0,Set actionType prediction
v1.0.0,double z=pre*y;
v1.0.0,if(z<=0) return 0.5-z;
v1.0.0,"else if(z>0 && z<1) return 0.5*Math.pow(1-z,2);"
v1.0.0,return 0.0;
v1.0.0,if (pre * y <= 0)
v1.0.0,return y;
v1.0.0,else if (pre * y > 0 && pre * y < 1)
v1.0.0,return (1 - pre * y) * y;
v1.0.0,return 0.0;
v1.0.0,feature index used to split
v1.0.0,feature value used to split
v1.0.0,loss change after split this node
v1.0.0,grad stats of the left child
v1.0.0,grad stats of the right child
v1.0.0,"LOG.info(""Constructor with fid = -1"");"
v1.0.0,fid = -1: no split currently
v1.0.0,the minimal split value is the minimal value of feature
v1.0.0,the splits do not include the maximal value of feature
v1.0.0,"1. the average distance, (maxValue - minValue) / splitNum"
v1.0.0,2. calculate the candidate split value
v1.0.0,insIdx is the index of instances
v1.0.0,loop over features
v1.0.0,"int[] counts = {10, 2, 1, 2, 0, 3, 0, 5};"
v1.0.0,setting
v1.0.0,ranking.
v1.0.0,clear all the information
v1.0.0,map feature id to the position in units
v1.0.0,add hist unit of each feature
v1.0.0,"loop instance's position, find those belong to nid"
v1.0.0,calculate the sum of gradient and hess
v1.0.0,"queue of nodes to be expanded, -1 means no work"
v1.0.0,map active node to its working index offset in qexpand
v1.0.0,"can be -1, which means the node is not actively expanding"
v1.0.0,position of each instances in the tree
v1.0.0,"can be negative, which means this ins2Node is no longer expanding"
v1.0.0,used to candidate split cut value
v1.0.0,"HistSet of all nodes, use node2Work to find the position"
v1.0.0,loss function
v1.0.0,gradient statistics
v1.0.0,create loss function
v1.0.0,calculate gradient info
v1.0.0,"add root node, including split entry"
v1.0.0,add root node work
v1.0.0,create split value helper
v1.0.0,init instance position to root
v1.0.0,init histogram
v1.0.0,calculate grad info of each instance
v1.0.0,add active work(node) to queue
v1.0.0,add new work to the queue
v1.0.0,init histogram
v1.0.0,"get one work(node) from queue, -1 means no active work"
v1.0.0,get candidate split values
v1.0.0,build gradient histogram
v1.0.0,get candidate split value
v1.0.0,"find the best split from candidates, add new node to tree"
v1.0.0,"add new node,"
v1.0.0,set node's left and right children
v1.0.0,"create left and right children node, add them to regtree"
v1.0.0,create node stats for children nodes add them to regtree
v1.0.0,job after splits
v1.0.0,"update work queue, add new work to queue, update node to work"
v1.0.0,"update instance pos,"
v1.0.0,no extra work for leaf node
v1.0.0,"update work queue, set finished node to inactive"
v1.0.0,"set node's work to inactive, set node2Work to inactivate"
v1.0.0,add children node's work to queue
v1.0.0,update instance ins2Node
v1.0.0,update instance's corresponding node
v1.0.0,"LOG.info(String.format(""Move ins[%d] fid[%d] fvalue[%f] to node[%d]"","
v1.0.0,"insIdx, fid, fvalue, 2 * nid + 1));"
v1.0.0,"LOG.info(String.format(""Move ins[%d] fid[%d] fvalue[%f] to node[%d]"","
v1.0.0,"insIdx, fid, fvalue, 2 * nid + 2));"
v1.0.0,check whether there exist active works in the queue
v1.0.0,change the node to leaf
v1.0.0,change to leaf
v1.0.0,update the preds of instances
v1.0.0,evaluate the pre result
v1.0.0,build the regression tree
v1.0.0,"if reach the max depth, set it to leaf"
v1.0.0,1. new feature's histogram (grad + hess)
v1.0.0,size: sampled_featureNum * (2 * splitNum)
v1.0.0,"in other words, concatenate each feature's histogram"
v1.0.0,2. get the span of this node
v1.0.0,------ 3. using sparse-aware method to build histogram ---
v1.0.0,"first add grads of all instances to the first bin of all features, then loop the non-zero"
v1.0.0,entries
v1.0.0,the grad sum and hess sum of all the instances
v1.0.0,3.1. get the instance index
v1.0.0,3.2. get the grad and hess of the instance
v1.0.0,3.3. add to the sum
v1.0.0,"LOG.info(String.format(""Instance[%d]: indices size[%d], indices%s"","
v1.0.0,"insIdx, instance.getIndices().length, Arrays.toString(instance.getIndices())));"
v1.0.0,"LOG.info(String.format(""Instance[%d]: values size[%d], values%s "","
v1.0.0,"insIdx, instance.getValues().length, Arrays.toString(instance.getValues())));"
v1.0.0,3.4. loop the non-zero entries
v1.0.0,3.4.1. get feature value
v1.0.0,3.4.2. current feature's position in the sampled feature set
v1.0.0,3.4.3. find the position of feature value in a histogram
v1.0.0,"the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
v1.0.0,3.4.4. add the grad and hess to the corresponding bin
v1.0.0,3.4.5. add the reverse to the bin that contains 0.0f
v1.0.0,int gradStartIdx = 2 * splitNum * fPos;
v1.0.0,int hessStartIdx = gradStartIdx + splitNum;
v1.0.0,if (idx % 50000 == 0 && fPos % 5 == 0) {
v1.0.0,"LOG.info(String.format(""instance:%d, feature pos: %d, index of 0.0f: %d, "" +"
v1.0.0,"""grad index: %d, hess index: %d"","
v1.0.0,"idx, fPos, fZeroValueIdx, gradZeroIdx, hessZeroIdx));"
v1.0.0,}
v1.0.0,"LOG.info(String.format(""Add negative grad to index[%d] value[%f], "" +"
v1.0.0,"""add negative hess to index[%d] value[%f]"","
v1.0.0,"gradStartIdx, curGrad, hessStartIdx, curHess));"
v1.0.0,"LOG.info(String.format(""Update 0-th bin grad to %f, 0-th bin hess to %f"","
v1.0.0,"histogram.get(gradStartIdx), histogram.get(hessStartIdx)));"
v1.0.0,4. add the grad and hess sum to the first bin of all features
v1.0.0,int startIdx = fid * 2 * splitNum;
v1.0.0,if (fid % 5000 == 0) {
v1.0.0,"LOG.info(String.format(""feature pos: %d, index of 0.0f: %d, "" +"
v1.0.0,"""grad index: %d, hess index: %d"","
v1.0.0,"fid, fZeroValueIdx, gradZeroIdx, hessZeroIdx));"
v1.0.0,}
v1.0.0,// 3. loop over all the features of all the instances on this node
v1.0.0,for (int idx = nodeStart; idx <= nodeEnd; idx++) {
v1.0.0,// 3.1. get the instance index
v1.0.0,int insIdx = this.controller.instancePos[idx]; // the instance index
v1.0.0,SparseDoubleSortedVector instance = this.controller.dataMeta.instances.get(insIdx);
v1.0.0,// 3.2. get instance indices and values
v1.0.0,//int[] indices = instance.getIndices();
v1.0.0,//double[] values = instance.getValues();
v1.0.0,//for (int i = 0; i < indices.length; i++) {
v1.0.0,for(int fid = 0; fid < instance.getDimension(); fid++) {
v1.0.0,// 3.3. get feature id
v1.0.0,//int fid = indices[i];
v1.0.0,// 3.4. current feature's position in the sampled feature set
v1.0.0,"int fPos = findFidPlace(this.controller.fset, fid);"
v1.0.0,if (fPos == -1) {
v1.0.0,continue;
v1.0.0,}
v1.0.0,// 3.5. get feature value
v1.0.0,//float fv = (float) values[i];
v1.0.0,float fv = (float) instance.get(fid);
v1.0.0,// 3.6. find the position of feature value in a histogram
v1.0.0,"// the search area in the sketch is [fid * #splitNum, (fid+1) * #splitNum - 1]"
v1.0.0,"int fvalueIdx = findFvaluePlace(this.controller.sketches, fv,"
v1.0.0,"fid * this.controller.param.numSplit, (fid + 1) * this.controller.param.numSplit - 1);"
v1.0.0,// 3.7. get the grad and hess of the instance
v1.0.0,GradPair gradPair = this.controller.gradPairs.get(insIdx);
v1.0.0,// 3.8. the updated position in the histogram
v1.0.0,// since the siz of histogram = grad(# splitNum) + hess(# splitNum)
v1.0.0,// the hessIndex = gradIndex + numSplit
v1.0.0,//if (insIdx % 100000 == 0) {
v1.0.0,//
v1.0.0,"LOG.info(String.format(""Instance[%d], fid: %d, fpos: %d, fvalueIdx: %d, grad: %f, hess: %f"","
v1.0.0,"// insIdx, fid, fPos, fvalueIdx, gradPair.getGrad(), gradPair.getHess()));"
v1.0.0,//}
v1.0.0,int gradIdx = 2 * this.controller.param.numSplit * fPos + fvalueIdx;
v1.0.0,int hessIdx = 2 * this.controller.param.numSplit * fPos + fvalueIdx + this.controller.param.numSplit;
v1.0.0,// 3.9. add grad and hess to the corresponding histogram
v1.0.0,"histogram.set(gradIdx, histogram.get(gradIdx) + gradPair.getGrad());"
v1.0.0,"histogram.set(hessIdx, histogram.get(hessIdx) + gradPair.getHess());"
v1.0.0,}
v1.0.0,}
v1.0.0,find the best split result of the histogram of a tree node
v1.0.0,1. calculate the gradStats of the root node
v1.0.0,"1.1. update the grad stats of the root node on PS, only called once by leader worker"
v1.0.0,2. loop over features
v1.0.0,2.1. get the ture feature id in the sampled feature set
v1.0.0,2.2. get the indexes of histogram of this feature
v1.0.0,"LOG.info(String.format(""Histogram of feature[%d]: %s"", trueFid,"
v1.0.0,Arrays.toString(curHistogram.getValues())));
v1.0.0,2.3. find the best split of current feature
v1.0.0,"LOG.info(String.format(""Best split of feature[%d]: value[%f], gain[%f]"","
v1.0.0,"trueFid, curSplit.getFvalue(), curSplit.getLossChg()));"
v1.0.0,2.4. update the best split result if possible
v1.0.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.0.0,3. update the grad stats of children node
v1.0.0,3.1. update the left child
v1.0.0,3.2. update the right child
v1.0.0,find the best split result of one feature
v1.0.0,"LOG.info(String.format(""Find best split for fid[%d] in histogram[size:%d], startIdx[%d]"","
v1.0.0,"fid, histogram.getDimension(), startIdx));"
v1.0.0,1. set the feature id
v1.0.0,2. create the best left stats and right stats
v1.0.0,3. the gain of the root node
v1.0.0,"LOG.info(String.format(""Node[%d] feature[%d]: sumGrad[%f], sumHess[%f], gain[%f]"","
v1.0.0,"this.nid, fid, rootStats.sumGrad, rootStats.sumHess, rootGain));"
v1.0.0,4. create the temp left and right grad stats
v1.0.0,5. loop over all the data in histogram
v1.0.0,5.1. get the grad and hess of current hist bin
v1.0.0,5.2. check whether we can split with current left hessian
v1.0.0,right = root - left
v1.0.0,5.3. check whether we can split with current right hessian
v1.0.0,5.4. calculate the current loss gain
v1.0.0,5.5. check whether we should update the split result with current loss gain
v1.0.0,split value = sketches[splitIdx+1]
v1.0.0,"LOG.info(String.format(""Find new best split: fid[%d], fvalue[%f], lossChg[%f]"","
v1.0.0,"splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg));"
v1.0.0,"LOG.info(String.format(""Left child of node[%d]: sumGrad[%f], sumHess[%f]; "" +"
v1.0.0,"""right child of node[%d]: sumGrad[%f], sumHess[%f]; lossChg[%f]"","
v1.0.0,"this.nid, leftStats.sumGrad, leftStats.sumHess,"
v1.0.0,"this.nid, rightStats.sumGrad, rightStats.sumHess, lossChg));"
v1.0.0,"5.6. if should update, also update the best left and right grad stats"
v1.0.0,6. set the best left and right grad stats
v1.0.0,"LOG.info(String.format(""Best left node grad: sumGrad[%f], sumHess[%f]"","
v1.0.0,"bestLeftStat.sumGrad, bestLeftStat.sumHess));"
v1.0.0,"LOG.info(String.format(""Best right node grad: sumGrad[%f], sumHess[%f]"","
v1.0.0,"bestRightStat.sumGrad, bestRightStat.sumHess));"
v1.0.0,partition number
v1.0.0,cols of each partition
v1.0.0,"update the grad stats of the root node on PS, only called once by leader worker"
v1.0.0,3. update the grad stats of children node
v1.0.0,3.1. update the left child
v1.0.0,3.2. update the right child
v1.0.0,1. calculate the total grad sum and hess sum
v1.0.0,2. create the grad stats of the node
v1.0.0,1. calculate the total grad sum and hess sum
v1.0.0,2. create the grad stats of the node
v1.0.0,1. calculate the total grad sum and hess sum
v1.0.0,2. create the grad stats of the node
v1.0.0,"loop all the possible split value, start from split[1], since the first item is the minimal"
v1.0.0,feature value
v1.0.0,find the best split result of the histogram of a tree node
v1.0.0,2.2. get the indexes of histogram of this feature
v1.0.0,"LOG.info(String.format(""Histogram of feature[%d]: %s"", trueFid,"
v1.0.0,Arrays.toString(curHistogram.getValues())));
v1.0.0,2.3. find the best split of current feature
v1.0.0,"LOG.info(String.format(""Best split of feature[%d]: value[%f], gain[%f]"","
v1.0.0,"trueFid, curSplit.getFvalue(), curSplit.getLossChg()));"
v1.0.0,2.4. update the best split result if possible
v1.0.0,find the best split result of one feature
v1.0.0,1. set the feature id
v1.0.0,splitEntry.setFid(fid);
v1.0.0,2. create the best left stats and right stats
v1.0.0,3. the gain of the root node
v1.0.0,4. create the temp left and right grad stats
v1.0.0,5. loop over all the data in histogram
v1.0.0,5.1. get the grad and hess of current hist bin
v1.0.0,5.2. check whether we can split with current left hessian
v1.0.0,right = root - left
v1.0.0,5.3. check whether we can split with current right hessian
v1.0.0,5.4. calculate the current loss gain
v1.0.0,5.5. check whether we should update the split result with current loss gain
v1.0.0,"LOG.info(String.format(""The current split: fid[%d], split index[%f], lossChg[%f]"","
v1.0.0,"fid, (float) splitIdx, lossChg));"
v1.0.0,split value = sketches[splitIdx+1]
v1.0.0,"5.6. if should update, also update the best left and right grad stats"
v1.0.0,6. set the best left and right grad stats
v1.0.0,find the best split result of a serve row on the PS
v1.0.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.0.0,2.2. get the start index in histogram of this feature
v1.0.0,"LOG.info(String.format(""Histogram of feature[%d]: %s"", trueFid,"
v1.0.0,Arrays.toString(curHistogram.getValues())));
v1.0.0,2.3. find the best split of current feature
v1.0.0,"LOG.info(String.format(""Best split of feature[%d]: value[%f], gain[%f]"","
v1.0.0,"trueFid, curSplit.getFvalue(), curSplit.getLossChg()));"
v1.0.0,2.4. update the best split result if possible
v1.0.0,"find the best split result of one feature from a server row, used by the PS"
v1.0.0,1. set the feature id
v1.0.0,2. create the best left stats and right stats
v1.0.0,3. the gain of the root node
v1.0.0,"LOG.info(String.format(""Node[%d] feature[%d]: sumGrad[%f], sumHess[%f], gain[%f]"","
v1.0.0,"this.nid, fid, rootStats.sumGrad, rootStats.sumHess, rootGain));"
v1.0.0,4. create the temp left and right grad stats
v1.0.0,5. loop over all the data in histogram
v1.0.0,5.1. get the grad and hess of current hist bin
v1.0.0,5.2. check whether we can split with current left hessian
v1.0.0,right = root - left
v1.0.0,5.3. check whether we can split with current right hessian
v1.0.0,5.4. calculate the current loss gain
v1.0.0,5.5. check whether we should update the split result with current loss gain
v1.0.0,"here we set the fvalue=splitIndex, split value = sketches[splitIdx+1], the task use"
v1.0.0,index to find fvalue
v1.0.0,"LOG.info(String.format(""Find new best split: fid[%d], fvalue[%f], lossChg[%f]"","
v1.0.0,"splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg));"
v1.0.0,"LOG.info(String.format(""Left child of node[%d]: sumGrad[%f], sumHess[%f]; "" +"
v1.0.0,"""right child of node[%d]: sumGrad[%f], sumHess[%f]; lossChg[%f]"","
v1.0.0,"this.nid, leftStats.sumGrad, leftStats.sumHess,"
v1.0.0,"this.nid, rightStats.sumGrad, rightStats.sumHess, lossChg));"
v1.0.0,"5.6. if should update, also update the best left and right grad stats"
v1.0.0,6. set the best left and right grad stats
v1.0.0,"LOG.info(String.format(""Best left node grad: sumGrad[%f], sumHess[%f]"","
v1.0.0,"bestLeftStat.sumGrad, bestLeftStat.sumHess));"
v1.0.0,"LOG.info(String.format(""Best right node grad: sumGrad[%f], sumHess[%f]"","
v1.0.0,"bestRightStat.sumGrad, bestRightStat.sumHess));"
v1.0.0,"features used in this tree, if equals null, means use all the features without sampling"
v1.0.0,node in the tree
v1.0.0,the gradient info of each instances
v1.0.0,initialize nodes
v1.0.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v1.0.0,ridx)
v1.0.0,check if necessary information is ready
v1.0.0,"same as add, reduce is used in All Reduce"
v1.0.0,features used in this tree
v1.0.0,node in the tree
v1.0.0,the gradient info of each instances
v1.0.0,initialize feature id
v1.0.0,initialize nodes
v1.0.0,"add root node, creete split entry"
v1.0.0,"initialize statistic of the root, including gradient stats"
v1.0.0,"LOG.info(String.format(""Add fid[%d] fvalue[%f] to [%d]-bin"", this.fid, fv, i));"
v1.0.0,loop over all the data in hist
v1.0.0,whether we can split with current hessian
v1.0.0,right = root -left
v1.0.0,whether we can split with current hessian
v1.0.0,"LOG.info(String.format(""Left child of node[%d]: sumGrad[%f], sumHess[%f]; "" +"
v1.0.0,"""right child of node[%d]: sumGrad[%f], sumHess[%f]; lossChg[%f]"","
v1.0.0,"this.nid, leftStats.sumGrad, leftStats.sumHess,"
v1.0.0,"this.nid, rightStats.sumGrad, rightStats.sumHess, lossChg));"
v1.0.0,"LOG.info(String.format(""The best split of node[%d]: fid[%d], fvalue[%f], lossChg[%f]"","
v1.0.0,"this.nid, splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg));"
v1.0.0,gradient
v1.0.0,second order gradient
v1.0.0,logistic loss for binary classification task.
v1.0.0,"logistic loss, but predict un-transformed margin"
v1.0.0,check if label in range
v1.0.0,if (!prob) {
v1.0.0,preds.clear();
v1.0.0,preds.addAll(rec);
v1.0.0,}
v1.0.0,return the default evaluation metric for the objective
v1.0.0,read partition header
v1.0.0,deal with row according the rowType
v1.0.0,TODO Auto-generated method stub
v1.0.0,tree.tree[topic + K] = (row.get(topic) + beta) / (n_k.get(topic) + vbeta);
v1.0.0,Inc update to local buffers
v1.0.0,The starting point
v1.0.0,There's always an unused entry.
v1.0.0,The starting point
v1.0.0,There's always an unused entry.
v1.0.0,The starting point
v1.0.0,There's always an unused entry.
v1.0.0,The starting point
v1.0.0,There's always an unused entry.
v1.0.0,The starting point
v1.0.0,There's always an unused entry.
v1.0.0,The starting point
v1.0.0,There's always an unused entry.
v1.0.0,print();
v1.0.0,The starting point
v1.0.0,There's always an unused entry.
v1.0.0,The starting point
v1.0.0,There's always an unused entry.
v1.0.0,loss function
v1.0.0,gradient and hessian
v1.0.0,tree node
v1.0.0,initialize the phase
v1.0.0,current tree and depth
v1.0.0,create loss function
v1.0.0,calculate grad info of each instance
v1.0.0,"create data sketch, push candidate split value to PS"
v1.0.0,1. calculate candidate split value
v1.0.0,"float[][] splits = TAvgDisSplit.getSplitValue(this.dataMeta, this.param.numSplit);"
v1.0.0,"LOG.info(String.format(""Local sketch size[%d]: %s"","
v1.0.0,"sketchVec.getDimension(), Arrays.toString(sketchVec.getValues())));"
v1.0.0,2. push local sketch to PS
v1.0.0,sketchClient.activateOpLog(new String[] {this.param.sketchName});
v1.0.0,3. set phase to GET_SKETCH
v1.0.0,"pull the global sketch from PS, only called once by each worker"
v1.0.0,"LOG.info(String.format(""Sketch vector: %s"", Arrays.toString(sketchVector.getValues())));"
v1.0.0,this.sketches = Floats.toArray(Doubles.asList(sketchVector.getValues()));
v1.0.0,sample feature
v1.0.0,push sampled feature set to the current tree
v1.0.0,this.taskContext.activateOpLog(new String[] {this.param.sampledFeaturesName});
v1.0.0,create new tree
v1.0.0,"pull sampled features, initialize tree nodes, reset active nodes, reset instance position,"
v1.0.0,calculate gradient
v1.0.0,"1. create new tree, initialize tree nodes and node stats"
v1.0.0,"2. initialize feature set, if sampled, get from PS, otherwise use all the features"
v1.0.0,2.1. pull the sampled features of the current tree
v1.0.0,"2.2. if use all the featues, only called one"
v1.0.0,"3. reset active tree nodes, set all tree nodes to inactive, set thread status to idle"
v1.0.0,4. set root node to active
v1.0.0,"5. reset instance position, set the root node's span"
v1.0.0,6. calculate gradient
v1.0.0,7. set phase to run active
v1.0.0,1. start threads of active tree nodes
v1.0.0,1.1. start threads for active nodes to generate histogram
v1.0.0,1.2. set thread status to running
v1.0.0,1.3. set the oplog to active
v1.0.0,activeOpLogSet.add(histParaName);
v1.0.0,"2. check thread stats, if all threads finish, return"
v1.0.0,this.taskContext.activateOpLog(activeOpLogSet);
v1.0.0,clock
v1.0.0,find split
v1.0.0,"1. find responsible tree node, using RR scheme"
v1.0.0,2. pull gradient histogram
v1.0.0,2.1. get the name of this node's gradient histogram on PS
v1.0.0,2.2. pull the histogram
v1.0.0,find best split result of this tree node
v1.0.0,"2.4. using server split, each partition of the histogram contains its best split result"
v1.0.0,find the best split from all partitions
v1.0.0,"2.5. the updated split result (tree node/feature/value/gain) on PS,"
v1.0.0,"2.4. otherwise, the returned histogram contains the gradient info"
v1.0.0,"2.5. the updated split result (tree node/feature/value/gain) on PS,"
v1.0.0,2.6. reset this tree node's gradient histogram to 0
v1.0.0,3. push split feature to PS
v1.0.0,4. push split value to PS
v1.0.0,5. push split gain to PS
v1.0.0,6. set phase to AFTER_SPLIT
v1.0.0,clock
v1.0.0,1. get split feature
v1.0.0,2. get split value
v1.0.0,3. get split gain
v1.0.0,4. get node weight
v1.0.0,"LOG.info(String.format(""Node grad stats: %s"", Arrays.toString(nodeGradStatsVec.getValues())));"
v1.0.0,5. split node
v1.0.0,5.1. set the children nodes of this node
v1.0.0,5.2. set split info and grad stats to this node
v1.0.0,5.2. create children nodes
v1.0.0,"5.3. create node stats for children nodes, and add them to the tree"
v1.0.0,5.4. reset instance position
v1.0.0,"5.5. add new active nodes if possible, inc depth, otherwise finish this tree"
v1.0.0,"LOG.info(String.format(""Current depth: %d, max depth: %d"","
v1.0.0,"this.currentDepth, this.param.maxDepth));"
v1.0.0,5.6. set children nodes to leaf nodes
v1.0.0,5.7. set nid to leaf node
v1.0.0,5.8. deactivate active node
v1.0.0,"6. check if there is active node, if not, finish current tree"
v1.0.0,boolean hasActive = hasActiveTNode();
v1.0.0,if (!hasActive) {
v1.0.0,finishCurrentTree();
v1.0.0,this.phase = GBDTPhase.NEW_TREE;
v1.0.0,} else {
v1.0.0,finishCurrentDepth();
v1.0.0,this.phase = GBDTPhase.RUN_ACTIVE;
v1.0.0,}
v1.0.0,clock
v1.0.0,"split the span of one node, reset the instance position"
v1.0.0,in case this worker has no instance on this node
v1.0.0,set the span of left child
v1.0.0,set the span of right child
v1.0.0,"1. left to right, find the first instance that should be in the right child"
v1.0.0,"2. right to left, find the first instance that should be in the left child"
v1.0.0,3. swap two instances
v1.0.0,4. find the cut pos
v1.0.0,than the split value
v1.0.0,5. set the span of left child
v1.0.0,6. set the span of right child
v1.0.0,set tree node to active
v1.0.0,set node to leaf
v1.0.0,set node to inactive
v1.0.0,finish current tree
v1.0.0,calculate the error
v1.0.0,predict();
v1.0.0,finish current depth
v1.0.0,set the tree phase
v1.0.0,check if there is active node
v1.0.0,check if finish all the tree
v1.0.0,update node's grad stats on PS
v1.0.0,"called during splitting in DistributedHistHelper, update the grad stats of children nodes after"
v1.0.0,find the best split
v1.0.0,the root node's stats is updated by leader worker by one time
v1.0.0,// 1.1. get the index of the grad and hess
v1.0.0,"int[] nodeIndice = { nid, nid + this.activeNode.length };"
v1.0.0,// 1.2. get the grad sum and hess sum
v1.0.0,"double[] weightValue = { gradStats.sumGrad, gradStats.sumHess };"
v1.0.0,1.3. create the update
v1.0.0,for (int i = 0; i < nodeIndice.length; i++) {
v1.0.0,"vec.set(nodeIndice[i], weightValue[i]);"
v1.0.0,}
v1.0.0,1.4. push the update to PS
v1.0.0,"LOG.info(String.format(""Update the prediction of instance[%d] to %f, label[%f]"","
v1.0.0,"insIdx, this.dataMeta.preds[insIdx], this.dataMeta.labels[insIdx]));"
v1.0.0,the leader task adds node prediction to flush list
v1.0.0,1. name of this node's grad histogram on PS
v1.0.0,2. build the grad histogram of this node
v1.0.0,3. push the histograms to PS
v1.0.0,"LOG.info(String.format(""Histogram: size[%d] %s"", histogram.getDimension(),"
v1.0.0,Arrays.toString(histogram.getValues())));
v1.0.0,4. reset thread stats to finished
v1.0.0,int sendStartCol = startFid * 7; // each split contains 7 doubles
v1.0.0,step size for a tree
v1.0.0,number of class
v1.0.0,minimum loss change required for a split
v1.0.0,maximum depth of a tree
v1.0.0,number of features
v1.0.0,number of nonzero
v1.0.0,number of candidates split value
v1.0.0,----- the rest parameters are less important ----
v1.0.0,base instance weight
v1.0.0,minimum amount of hessian(weight) allowed in a child
v1.0.0,L2 regularization factor
v1.0.0,L1 regularization factor
v1.0.0,default direction choice
v1.0.0,maximum delta update we can add in weight estimation
v1.0.0,this parameter can be used to stabilize update
v1.0.0,default=0 means no constraint on weight delta
v1.0.0,whether we want to do subsample for row
v1.0.0,whether to subsample columns for each tree
v1.0.0,accuracy of sketch
v1.0.0,accuracy of sketch
v1.0.0,leaf vector size
v1.0.0,option for parallelization
v1.0.0,option to open cacheline optimization
v1.0.0,whether to not print info during training.
v1.0.0,return MathUtils.sqr(sumGrad) / (sumHess + regLambda);
v1.0.0,feature index used to split
v1.0.0,feature value used to split
v1.0.0,loss change after split this node
v1.0.0,grad stats of the left child
v1.0.0,grad stats of the right child
v1.0.0,"LOG.info(""Constructor with fid = -1"");"
v1.0.0,fid = -1: no split currently
v1.0.0,"whether this is simply statistics, only need to call Add(gpair), instead of Add(gpair, info,"
v1.0.0,ridx)
v1.0.0,"same as add, reduce is used in All Reduce"
v1.0.0,maximum depth of the tree
v1.0.0,number of features used for tree construction
v1.0.0,"minimum loss change required for a split, otherwise stop split"
v1.0.0,----- the rest parameters are less important ----
v1.0.0,default direction choice
v1.0.0,whether we want to do sample data
v1.0.0,whether to sample columns during tree construction
v1.0.0,whether to use histogram for split
v1.0.0,number of histogram units
v1.0.0,whether to print info during training.
v1.0.0,----- the rest parameters are obtained after training ----
v1.0.0,total number of nodes
v1.0.0,number of deleted nodes */
v1.0.0,gradient
v1.0.0,second order gradient
v1.0.0,1. calculate the total grad sum and hess sum
v1.0.0,2. create the grad stats of the node
v1.0.0,find the best split result of a serve row on the PS
v1.0.0,"2. the fid here is the index in the sampled feature set, rather than the true feature id"
v1.0.0,2.2. get the start index in histogram of this feature
v1.0.0,"LOG.info(String.format(""Histogram of feature[%d]: %s"", trueFid,"
v1.0.0,Arrays.toString(curHistogram.getValues())));
v1.0.0,2.3. find the best split of current feature
v1.0.0,"LOG.info(String.format(""Best split of feature[%d]: value[%f], gain[%f]"","
v1.0.0,"trueFid, curSplit.getFvalue(), curSplit.getLossChg()));"
v1.0.0,2.4. update the best split result if possible
v1.0.0,"find the best split result of one feature from a server row, used by the PS"
v1.0.0,"LOG.info(String.format(""Find the best split for fid[%d] in server row, size[%d], startIdx[%d]"","
v1.0.0,"fid, row.size(), startIdx));"
v1.0.0,StringBuilder sb = new StringBuilder();
v1.0.0,for (int i = startIdx; i < startIdx + 2 * splitNum; i++) {
v1.0.0,"sb.append(row.getData().get(i) + "", "");"
v1.0.0,}
v1.0.0,"LOG.info(""Server row: "" + sb.toString());"
v1.0.0,1. set the feature id
v1.0.0,splitEntry.setFid(fid);
v1.0.0,2. create the best left stats and right stats
v1.0.0,3. the gain of the root node
v1.0.0,"LOG.info(String.format(""Feature[%d]: sumGrad[%f], sumHess[%f], gain[%f]"","
v1.0.0,"fid, rootStats.sumGrad, rootStats.sumHess, rootGain));"
v1.0.0,4. create the temp left and right grad stats
v1.0.0,5. loop over all the data in histogram
v1.0.0,5.1. get the grad and hess of current hist bin
v1.0.0,5.2. check whether we can split with current left hessian
v1.0.0,right = root - left
v1.0.0,5.3. check whether we can split with current right hessian
v1.0.0,5.4. calculate the current loss gain
v1.0.0,5.5. check whether we should update the split result with current loss gain
v1.0.0,"LOG.info(String.format(""The current split: fid[%d], split index[%f], lossChg[%f]"","
v1.0.0,"fid, (float) splitIdx, lossChg));"
v1.0.0,"here we set the fvalue=splitIndex, split value = sketches[splitIdx+1], the task use"
v1.0.0,index to find fvalue
v1.0.0,"LOG.info(String.format(""Find new best split: fid[%d], fvalue[%f], lossChg[%f]"","
v1.0.0,"splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg));"
v1.0.0,"LOG.info(String.format(""Left child of node[%d]: sumGrad[%f], sumHess[%f]; "" +"
v1.0.0,"""right child of node[%d]: sumGrad[%f], sumHess[%f]; lossChg[%f]"","
v1.0.0,"this.nid, leftStats.sumGrad, leftStats.sumHess,"
v1.0.0,"this.nid, rightStats.sumGrad, rightStats.sumHess, lossChg));"
v1.0.0,"5.6. if should update, also update the best left and right grad stats"
v1.0.0,6. set the best left and right grad stats
v1.0.0,"LOG.info(String.format(""The best split for fid[%d], split feature[%d]: split index[%f], lossChg[%f], """
v1.0.0,+
v1.0.0,"""leftSumGrad[%f], leftSumHess[%f], rightSumGrad[%f], rightSumHess[%f]"","
v1.0.0,"fid, splitEntry.fid, splitEntry.fvalue, splitEntry.lossChg,"
v1.0.0,"splitEntry.leftGradStat.sumGrad, splitEntry.leftGradStat.sumHess,"
v1.0.0,"splitEntry.rightGradStat.sumGrad, splitEntry.rightGradStat.sumHess));"
v1.0.0,grad.timesBy(-1.0 * lr);
v1.0.0,System.out.println(Arrays.toString(distArr));
v1.0.0,"System.out.println(String.format(""Epoch[%d] batch[%d], loss[%f]"", epoch, batch, loss));"
v1.0.0,"System.out.println(""Start calculate loss and auc, sample number: "" + totalNum);"
v1.0.0,"System.out.println(""Sort cost "" + (System.currentTimeMillis() - sortStartTime) + ""ms, Scores list size: """
v1.0.0,"+ scoresArray.length + "", sorted values:"" + scoresArray[0] + "","""
v1.0.0,"+ scoresArray[scoresArray.length / 5] + "","" + scoresArray[scoresArray.length / 3] + "","""
v1.0.0,"+ scoresArray[scoresArray.length / 2] + "","" + scoresArray[scoresArray.length - 1]);"
v1.0.0,"System.out.println(""M = "" + M + "", N = "" + N + "", sigma = "" + sigma + "", AUC = "" + aucResult);"
v1.0.0,"System.out.println(String.format(""Validation TP=%d, TN=%d, FP=%d, FN=%d"", truePos, trueNeg, falsePos,falseNeg));"
v1.0.0,"System.out.println(input + "" | "" + hashFunc.encode(input));"
v1.0.0,step size for a tree
v1.0.0,number of class
v1.0.0,minimum loss change required for a split
v1.0.0,maximum depth of a tree
v1.0.0,number of features
v1.0.0,number of nonzero
v1.0.0,number of candidates split value
v1.0.0,----- the rest parameters are less important ----
v1.0.0,base instance weight
v1.0.0,minimum amount of hessian(weight) allowed in a child
v1.0.0,L2 regularization factor
v1.0.0,L1 regularization factor
v1.0.0,default direction choice
v1.0.0,maximum delta update we can add in weight estimation
v1.0.0,this parameter can be used to stabilize update
v1.0.0,default=0 means no constraint on weight delta
v1.0.0,whether we want to do subsample for row
v1.0.0,whether to subsample columns for each tree
v1.0.0,accuracy of sketch
v1.0.0,accuracy of sketch
v1.0.0,leaf vector size
v1.0.0,option for parallelization
v1.0.0,option to open cacheline optimization
v1.0.0,whether to not print info during training.
v1.0.0,return MathUtils.sqr(sumGrad) / (sumHess + regLambda);
v1.0.0,"get feature type, 0:empty 1:all equal 2:real"
v1.0.0,"quantile sketch, size = featureNum * splitNum"
v1.0.0,"gradient histograms, size = treeNodeNum * featureNum * splitNum"
v1.0.0,"active tree nodes, size = pow(2, treeDepth) -1"
v1.0.0,sampled features. size = treeNum * sampleRatio * featureNum
v1.0.0,"split features, size = treeNum * treeNodeNum"
v1.0.0,"split values, size = treeNum * treeNodeNum"
v1.0.0,"split gains, size = treeNum * treeNodeNum"
v1.0.0,"node weights, size = treeNum * treeNodeNum"
v1.0.0,"node preds, size = treeNum * treeNodeNum"
v1.0.0,if using PS to perform split
v1.0.0,maximum depth of the tree
v1.0.0,number of features used for tree construction
v1.0.0,"minimum loss change required for a split, otherwise stop split"
v1.0.0,----- the rest parameters are less important ----
v1.0.0,default direction choice
v1.0.0,whether we want to do sample data
v1.0.0,whether to sample columns during tree construction
v1.0.0,whether to use histogram for split
v1.0.0,number of histogram units
v1.0.0,whether to print info during training.
v1.0.0,----- the rest parameters are obtained after training ----
v1.0.0,total number of nodes
v1.0.0,number of deleted nodes */
