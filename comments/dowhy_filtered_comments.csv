Version,Commit Message
v0.12,-*- coding: utf-8 -*-
v0.12,
v0.12,Configuration file for the Sphinx documentation builder.
v0.12,
v0.12,This file does only contain a selection of the most common options. For a
v0.12,full list see the documentation:
v0.12,http://www.sphinx-doc.org/en/stable/config
v0.12,-- Path setup --------------------------------------------------------------
v0.12,"If extensions (or modules to document with autodoc) are in another directory,"
v0.12,add these directories to sys.path here. If the directory is relative to the
v0.12,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.12,
v0.12,-- Project information -----------------------------------------------------
v0.12,Version Information (for version-switcher)
v0.12,-- General configuration ---------------------------------------------------
v0.12,"If your documentation needs a minimal Sphinx version, state it here."
v0.12,
v0.12,needs_sphinx = '1.0'
v0.12,"Add any Sphinx extension module names here, as strings. They can be"
v0.12,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.12,ones.
v0.12,sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme
v0.12,already loads it
v0.12,"Add any paths that contain templates here, relative to this directory."
v0.12,The suffix(es) of source filenames.
v0.12,You can specify multiple suffix as a list of string:
v0.12,
v0.12,"source_suffix = ['.rst', '.md']"
v0.12,The master toctree document.
v0.12,The language for content autogenerated by Sphinx. Refer to documentation
v0.12,for a list of supported languages.
v0.12,
v0.12,This is also used if you do content translation via gettext catalogs.
v0.12,"Usually you set ""language"" from the command line for these cases."
v0.12,"List of patterns, relative to source directory, that match files and"
v0.12,directories to ignore when looking for source files.
v0.12,This pattern also affects html_static_path and html_extra_path .
v0.12,The name of the Pygments (syntax highlighting) style to use.
v0.12,-- Options for HTML output -------------------------------------------------
v0.12,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.12,a list of builtin themes.
v0.12,
v0.12,Theme options are theme-specific and customize the look and feel of a theme
v0.12,"further.  For a list of options available for each theme, see the"
v0.12,documentation.
v0.12,
v0.12,"Add any paths that contain custom static files (such as style sheets) here,"
v0.12,"relative to this directory. They are copied after the builtin static files,"
v0.12,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.12,"Custom sidebar templates, must be a dictionary that maps document names"
v0.12,to template names.
v0.12,
v0.12,The default sidebars (for documents that don't match any pattern) are
v0.12,defined by theme itself.  Builtin themes are using these templates by
v0.12,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.12,'searchbox.html']``.
v0.12,
v0.12,html_sidebars = {}
v0.12,-- Options for HTMLHelp output ---------------------------------------------
v0.12,Output file base name for HTML help builder.
v0.12,-- Options for LaTeX output ------------------------------------------------
v0.12,The paper size ('letterpaper' or 'a4paper').
v0.12,
v0.12,"'papersize': 'letterpaper',"
v0.12,"The font size ('10pt', '11pt' or '12pt')."
v0.12,
v0.12,"'pointsize': '10pt',"
v0.12,Additional stuff for the LaTeX preamble.
v0.12,
v0.12,"'preamble': '',"
v0.12,Latex figure (float) alignment
v0.12,
v0.12,"'figure_align': 'htbp',"
v0.12,Grouping the document tree into LaTeX files. List of tuples
v0.12,"(source start file, target name, title,"
v0.12,"author, documentclass [howto, manual, or own class])."
v0.12,-- Options for manual page output ------------------------------------------
v0.12,One entry per manual page. List of tuples
v0.12,"(source start file, name, description, authors, manual section)."
v0.12,-- Options for Texinfo output ----------------------------------------------
v0.12,Grouping the document tree into Texinfo files. List of tuples
v0.12,"(source start file, target name, title, author,"
v0.12,"dir menu entry, description, category)"
v0.12,-- Options for Epub output -------------------------------------------------
v0.12,Bibliographic Dublin Core info.
v0.12,The unique identifier of the text. This can be a ISBN number
v0.12,or the project homepage.
v0.12,
v0.12,epub_identifier = ''
v0.12,A unique identification for the text.
v0.12,
v0.12,epub_uid = ''
v0.12,A list of files that should not be packed into the epub file.
v0.12,-- Extension configuration -------------------------------------------------
v0.12,-- Options for todo extension ----------------------------------------------
v0.12,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.12,init docstrings should also be included in class
v0.12,Only uncomment for faster testing/building docs without compiling notebooks
v0.12,"nbsphinx_execute = ""never"""
v0.12,Patch all of the published versions
v0.12,check old RST version (<= v0.8)
v0.12,Remove old version links
v0.12,Append updated version links
v0.12,requires stdin input for identify in weighting sampler
v0.12,will be removed
v0.12,"applied notebook, not necessary to test each time"
v0.12,needs xgboost too
v0.12,Slow Notebooks
v0.12,"TODO: should probably move more notebooks here to ignore, because"
v0.12,most get tested by the documentation generation.
v0.12,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.12,can import dowhy
v0.12,"""--ExecutePreprocessor.timeout=600"","
v0.12,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.12,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.12,treated = self.df[self.df['z']==1]
v0.12,self.att = np.mean(treated['y1'] - treated['y0'])
v0.12,def test_average_treatment_effect(self):
v0.12,est_ate = 1
v0.12,bias = est_ate - self.ate
v0.12,print(bias)
v0.12,"self.assertAlmostEqual(self.ate, est_ate)"
v0.12,def test_average_treatment_effect_on_treated(self):
v0.12,est_att = 1
v0.12,self.att=1
v0.12,bias = est_att - self.att
v0.12,print(bias)
v0.12,"self.assertAlmostEqual(self.att, est_att)"
v0.12,removing two common causes
v0.12,removing two common causes
v0.12,removing two common causes
v0.12,removing two common causes
v0.12,removing two common causes
v0.12,"Remove graph variable with name ""W0"" from observed data."
v0.12,Ensure that a log record exists that provides a more detailed view
v0.12,of observed and unobserved graph variables (counts and variable names.)
v0.12,"Default == operator tests if same object. If same object, don't need to check type."
v0.12,"num_frontdoor_variables=1,"
v0.12,creating nx graph instance
v0.12,to be used later for a test. Does not include the replace operation
v0.12,check if all partial R^2 values are between 0 and 1
v0.12,We calculate adjusted estimates for two sets of partial R^2 values.
v0.12,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.12,Creating a model with no unobserved confounders
v0.12,check if all partial R^2 values are between 0 and 1
v0.12,"for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)"
v0.12,Non Parametric estimator
v0.12,We calculate adjusted estimates for two sets of partial R^2 values.
v0.12,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.12,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.12,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.12,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.12,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.12,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.12,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.12,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.12,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.12,we patched figure plotting call to avoid drawing plots during tests
v0.12,We calculate adjusted estimates for two sets of partial R^2 values.
v0.12,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.12,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.12,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.12,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.12,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.12,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.12,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.12,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.12,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.12,we patched figure plotting call to avoid drawing plots during tests
v0.12,comparing test examples from R E-Value package
v0.12,check implementation of Observed Covariate E-value against R package
v0.12,The outcome is a linear function of the confounder
v0.12,"The slope is 1,2 and the intercept is 3"
v0.12,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.12,"TODO: Check directly for correct behavior, rather than checking the rules"
v0.12,"themselves, which can be non-deterministic (all the following are equivalent)"
v0.12,Supports user-provided dataset object
v0.12,To test if there are any exceptions
v0.12,To test if the estimate is identical if refutation parameters are zero
v0.12,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.12,"Ordinarily, we should expect this value to be zero."
v0.12,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.12,"Ordinarily, we should expect this value to be zero."
v0.12,Construct the graph (the graph is constant for all iterations)
v0.12,Generate the GML graph
v0.12,Define the true effect
v0.12,Define experiment params
v0.12,Record the results
v0.12,Run the experiment
v0.12,Generate the data
v0.12,Encode as a pandas df
v0.12,Instantiate the CausalModel
v0.12,Get the estimand
v0.12,Get estimate (DML)
v0.12,Get estimate (Linear Regression)
v0.12,Instantiate the SCM
v0.12,Generate observational data
v0.12,Encode as a pandas df
v0.12,Create the graph describing the causal structure
v0.12,With graph
v0.12,model.view_model()
v0.12,Only P(Y|T) should be present for test to succeed.
v0.12,"Since undirected graph, identify effect must throw an error."
v0.12,Compare with ground truth
v0.12,Compare with ground truth
v0.12,Compare with ground truth
v0.12,Compare with ground truth
v0.12,Causal model initialization
v0.12,Obtain backdoor sets
v0.12,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.12,Causal model initialization
v0.12,Obtain backdoor sets
v0.12,Causal model initialization
v0.12,Obtain backdoor sets
v0.12,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.12,Causal model initialization
v0.12,Obtain backdoor sets
v0.12,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.12,Causal model initialization
v0.12,Obtain backdoor sets
v0.12,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.12,Causal model initialization
v0.12,Obtain backdoor sets
v0.12,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.12,"Example is selected from Pearl J. ""Causality"" 2nd Edition, from chapter 3.3.1 on backoor criterion."
v0.12,"The following simpsons paradox examples are taken from Pearl, J {2013}. ""Understanding Simpson’s Paradox"" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf"
v0.12,"The following are examples given in the ""Book of Why"" by Judea Pearl, chapter ""The Do-operator and the Back-Door Criterion"""
v0.12,Test added for issue #1250
v0.12,Building the causal model
v0.12,For all examples from these papers we use X for the treatment variable
v0.12,instead of A.
v0.12,"Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.12,"Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.12,"Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.12,"Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.12,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.12,L replaces X as the conditional variable
v0.12,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.12,L replaces X as the conditional variable. Uses different costs
v0.12,"Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.12,"A graph where optimal, optimal minimal and optimal min cost are different"
v0.12,The graph from Shrier and Platt (2008)
v0.12,A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.12,Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.12,cov_mat = np.diag(np.ones(num_features))
v0.12,collider: X->Z<-Y
v0.12,chain: X->Z->Y
v0.12,fork: X<-Z->Y
v0.12,"general DAG: X<-Z->Y, X->Y"
v0.12,fork: X<-Z->Y
v0.12,Only Gaussian component changed
v0.12,"Just checking formats, i.e. no need for correlation."
v0.12,"Just checking formats, i.e. no need for correlation."
v0.12,The MSE should be 1 due to the variance of the noise. The RMSE is accordingly 1 / var(Y).
v0.12,Contributions should add up to Var(X2)
v0.12,H(P(Y)) -- Can be precomputed
v0.12,-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))
v0.12,"H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\S)])"
v0.12,"E[P(Y | x_S, X'_\S)]"
v0.12,"H(E[P(Y | x_S, X'_\S)])"
v0.12,"Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions."
v0.12,E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]
v0.12,Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3
v0.12,Having at least one sample from the second class should not raise an error.
v0.12,"Just some random data, since we are only interested in the omitted data."
v0.12,This caused an error before with pandas > 2.0
v0.12,C2 = 3 * A2 + 2 * B2
v0.12,"By default, the strength is measure with respect to the variance."
v0.12,"Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2."
v0.12,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.12,Missing connection between X0 and X1.
v0.12,"For X0 and X1, we set the ground truth noise to further emphasize the misspecification. The inferred noise of X1"
v0.12,would otherwise have a dependency with Z due to the missing connection with X0.
v0.12,Modelling connection between X0 and X1 explicitly.
v0.12,"Here, we misspecify the mechanism on purpose by setting scale to 1 instead of 2."
v0.12,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.12,"Here, changing the mechanism."
v0.12,"Here, changing the mechanism."
v0.12,"Here, changing the mechanism."
v0.12,"Here, changing the mechanism."
v0.12,Defining an anomaly scorer that handles multidimensional inputs.
v0.12,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.12,"In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution"
v0.12,"and it should be ""significantly"" higher than the contribution of the other ones (here, we just arbitrarily say"
v0.12,"it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,"
v0.12,"which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate"
v0.12,"some robustness. Note that due to this and stochastic behaviour of the density estimator, it is"
v0.12,"not possible to analytically compute expected results. Therefore, we rather look at the relations here."
v0.12,"Same idea for the second sample, but here, it is the second variable that is anomalous."
v0.12,"In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be"
v0.12,"""significantly"" higher than the contribution of the other variables. The contribution of both anomalous variables"
v0.12,should be equal (approximately).
v0.12,Defining an anomaly scorer that handles multidimensional inputs.
v0.12,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.12,reduce the score.
v0.12,The contributions should add up to g(x) - E[g(X)]
v0.12,The contributions should add up to g(x) - E[g(X)]
v0.12,The contributions should add up to g(x) - E[g(X)]
v0.12,Three examples:
v0.12,1. X1 is the root cause (+ 10 to the noise)
v0.12,2. X0 is the root cause (+ 10 to the noise)
v0.12,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.12,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.12,Three examples:
v0.12,1. X1 is the root cause (+ 10 to the noise)
v0.12,2. X0 is the root cause (+ 10 to the noise)
v0.12,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.12,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.12,"Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution"
v0.12,algorithm.
v0.12,1. X0 is the root cause (+ 10 to the noise)
v0.12,2. X0 and X1 are the root causes (+ 10 to both noise)
v0.12,3. X2 and X3 are both root causes (+ 10 to both noises)
v0.12,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.12,collider: X->Z<-Y
v0.12,collider: X->Z<-Y
v0.12,chain: X->Y->Z
v0.12,chain: X->Y->Z
v0.12,Empty graph
v0.12,Full DAG
v0.12,DAG with single node
v0.12,DAG with single edge
v0.12,DAG with single edge
v0.12,chain: X->Z->Y
v0.12,Setup data
v0.12,Test LinearDML
v0.12,Checking that the CATE estimates are not identical
v0.12,Test ContinuousTreatmentOrthoForest
v0.12,Checking that the CATE estimates are not identical
v0.12,Test LinearDRLearner
v0.12,Test LinearDML
v0.12,checking that CATE estimates are not identical
v0.12,predict on new data
v0.12,Setup data
v0.12,Test DeepIV
v0.12,Test IntentToTreatDRIV
v0.12,Observed data
v0.12,assumed graph
v0.12,Identify effect
v0.12,Estimate effect
v0.12,A model where X is also a common cause
v0.12,A model where X is also a common cause
v0.12,The case where effect modifier is not a common cause
v0.12,A model where X is also a common cause
v0.12,Create the graph describing the causal structure
v0.12,Generate the data
v0.12,Data to df
v0.12,Create a model
v0.12,Estimate the effect with front-door
v0.12,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.12,More cases where Exception  is expected
v0.12,"Compute confidence intervals, standard error and significance tests"
v0.12,Generate a dataset with some categorical variables (common causes)
v0.12,This configuration is necessary for the test and should not be varied.
v0.12,"For the purposes of the test, these are the categorical columns."
v0.12,"Since their values are integer, convert them to string type to ensure"
v0.12,Categorical handling.
v0.12,Get the values of row 0 for the specified columns
v0.12,Find rows where values differ from row 0 in terms of all values in the specified columns
v0.12,Create a copy of the data and swap the rows.
v0.12,Test 1: Permuting data order does not affect Effect estimate.
v0.12,"This test will not likely fail with a RegressionEstimator, because"
v0.12,the effect of common cause variables is additive and does not contribute to
v0.12,"the estimated effect. However, it could fail with other Estimators."
v0.12,"Test 2: Verify that estimated Outcomes from ""do-operator"" are unchanged"
v0.12,While for some Estimators the Effect is unaffected by changes to common-cause
v0.12,"data, and data ordering, predicted Outcomes should be as all variables can"
v0.12,"contribute to these Outcomes. However, they are only available in a standard"
v0.12,interface for Estimators which support `do(x)`.
v0.12,
v0.12,"In this test, we verify that the result of `do(x)` does not change when we"
v0.12,"present our two datasets (one has swapped first row). If the result differs,"
v0.12,"this is likely due to the encoding of these new data, since the model is"
v0.12,unchanged.
v0.12,
v0.12,"Unlike the Effect test #1 above, this test is verifiable; we can randomize"
v0.12,the values and combinations of the encoded values and verify that under these
v0.12,conditions the result of `do(x)` *does* change. This is the type of error we
v0.12,expect to observe if there's an encoding error - all the encoded variables
v0.12,would change.
v0.12,Test that do(x) result is unchanged despite row permutation
v0.12,"Verify that this test *does* detect errors, when common-cause data changed."
v0.12,Defined a linear dataset with a given set of properties
v0.12,Create a model that captures the same
v0.12,Identify the effects within the model
v0.12,Defined a linear dataset with a given set of properties
v0.12,Create a model that captures the same
v0.12,Identify the effects within the model
v0.12,Defined a linear dataset with a given set of properties
v0.12,Create a model that captures the same
v0.12,Identify the effects within the model
v0.12,Defined a linear dataset with a given set of properties
v0.12,Create a model that captures the same
v0.12,Identify the effects within the model
v0.12,Defined a linear dataset with a given set of properties
v0.12,Create a model that captures the same
v0.12,Identify the effects within the model
v0.12,Use a mix of already-numeric and requires encoding cols:
v0.12,"NB There may be small differences in type but since all values will be used in models as float,"
v0.12,comparison is done as this type.
v0.12,Check same rows
v0.12,Check same number of cols
v0.12,Check values
v0.12,Calculate the sum of absolute differences between the two DataFrames
v0.12,- should be zero (excl. floating point error)
v0.12,Use a mix of already-numeric and requires encoding cols:
v0.12,Initial encode
v0.12,Create new data with permuted rows.
v0.12,Output shape should be unchanged.
v0.12,Encode this new data.
v0.12,Check same rows
v0.12,Check same number of cols
v0.12,Check permuted values are consistent
v0.12,Check if calling the method causes some import or runtime errors
v0.12,TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer
v0.12,newer matplotlib version:
v0.12,AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'
v0.12,Networkx 2.4+ should fix this issue.
v0.12,"plot_adjacency_matrix(causal_graph, is_directed=False)"
v0.12,Helper method to create a DOT file from string content
v0.12,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.12,Unpacking the keyword arguments
v0.12,todo: add docstring for common parameters here and remove from child refuter classes
v0.12,Default value for the number of simulations to be conducted
v0.12,joblib params for parallel processing
v0.12,"Concatenate the confounders, instruments and effect modifiers"
v0.12,Shuffle the confounders
v0.12,Check if all are select or deselect variables
v0.12,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.12,This calculates a two-sided percentile p-value
v0.12,See footnotes in https://journals.sagepub.com/doi/full/10.1177/2515245920911881
v0.12,Get the mean for the simulations
v0.12,Get the standard deviation for the simulations
v0.12,Get the Z Score [(val - mean)/ std_dev ]
v0.12,Initializing the p_value
v0.12,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.12,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.12,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.12,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.12,re.sub only takes string parameter so the first if is to avoid error
v0.12,"if the input is a text file, convert the contained data into string"
v0.12,load dot file
v0.12,TODO This functionality needs to be deprecated. It is a convenience function but can introduce confusion
v0.12,as we are now including the option to initialize CausalGraph with DiGraph or GCM model.
v0.12,Adding node attributes
v0.12,adding penwidth to make the edge bold
v0.12,Adding common causes
v0.12,Adding instruments
v0.12,Adding effect modifiers
v0.12,Assuming the simple form of effect modifier
v0.12,that directly causes the outcome.
v0.12,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.12,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.12,Adding columns in the dataframe as confounders that were not in the graph
v0.12,Adding unobserved confounders
v0.12,removal of only direct edges wrt a target is not implemented for incoming edges
v0.12,also return the number of backdoor paths blocked by observed nodes
v0.12,Assume that nodes1 is the treatment
v0.12,"ignores new_graph parameter, always uses self._graph"
v0.12,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.12,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.12,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.12,Return effect modifiers according to the graph
v0.12,removing all mediators
v0.12,"Also add any effect modifiers that could not be auto-detected (e.g., they are also common causes)"
v0.12,[TODO: double check these work with multivariate implementation:]
v0.12,Exclusion
v0.12,As-if-random setup
v0.12,As-if-random
v0.12,convert the outputted generator into a list
v0.12,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.12,Emit a `UserWarning` if there are any unobserved graph variables and
v0.12,and log a message highlighting data variables that are not part of the graph.
v0.12,Create causal graph object
v0.12,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.12,(Because some effect modifiers may also be common causes)
v0.12,"In such cases, the user-provided modifiers are used."
v0.12,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.12,Import causal discovery class
v0.12,Initialize causal graph object
v0.12,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.12,TODO add dowhy as a prefix to all dowhy estimators
v0.12,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.12,Define the third-party estimation method to be used
v0.12,Process the dowhy estimators
v0.12,"If not fit_estimator, attempt to retrieve existing estimator."
v0.12,Keep original behaviour to create new estimator if none found.
v0.12,Check if estimator's target estimand is identified
v0.12,"Note that while the name of the variable is the same,"
v0.12,"""self.causal_estimator"", this estimator takes in less"
v0.12,parameters than the same from the
v0.12,estimate_effect code. It is not advisable to use the
v0.12,estimator from this function to call estimate_effect
v0.12,with fit_estimator=False.
v0.12,Estimator had been computed in a previous call
v0.12,The default number of simulations for statistical testing
v0.12,The default number of simulations to obtain confidence intervals
v0.12,This should be at least 399 for a 5% error rate:
v0.12,https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf
v0.12,The portion of the total size that should be taken each time to find the confidence intervals
v0.12,1 is the recommended value
v0.12,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.12,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.12,The default Confidence Level
v0.12,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.12,Prefix to add to temporary categorical variables created after discretization
v0.12,std args to be removed from locals() before being passed to args_dict
v0.12,Setting the default interpret method
v0.12,"Check if some parameters were set, otherwise set to default values"
v0.12,Estimate conditional estimates by default
v0.12,TODO Only works for binary treatment
v0.12,Defaulting to class default values if parameters are not provided
v0.12,Checking that there is at least one effect modifier
v0.12,Making sure that effect_modifier_names is a list
v0.12,Making a copy since we are going to be changing effect modifier names
v0.12,"For every numeric effect modifier, adding a temp categorical column"
v0.12,Grouping by effect modifiers and computing effect separately
v0.12,Deleting the temporary categorical columns
v0.12,The array that stores the results of all estimations
v0.12,Find the sample size the proportion with the population size
v0.12,Perform the set number of simulations
v0.12,names of treatment and outcome
v0.12,Using class default parameters if not specified
v0.12,Checking if bootstrap_estimates are already computed
v0.12,Checked if any parameter is changed from the previous std error estimate
v0.12,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.12,Get the variations of each bootstrap estimate and sort
v0.12,"Now we take the (1-p)/2 th and the 1-(1-p)/2 th variations, where p is the chosen confidence level"
v0.12,Get the lower and upper bounds by subtracting the variations from the estimate
v0.12,"Use existing params, if new user defined params are not present"
v0.12,Checking if bootstrap_estimates are already computed
v0.12,Check if any parameter is changed from the previous std error estimate
v0.12,"Use existing params, if new user defined params are not present"
v0.12,Processing the null hypothesis estimates
v0.12,Doing a two-sided test
v0.12,Being conservative with the p-value reported
v0.12,Being conservative with the p-value reported
v0.12,"If the estimate_index is 0, it depends on the number of simulations"
v0.12,Need to test r-squared before supporting
v0.12,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.12,'r-squared': effect_r_squared
v0.12,"elif method == ""r-squared"":"
v0.12,outcome_mean = np.mean(self._outcome)
v0.12,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.12,Assuming a linear model with one variable: the treatment
v0.12,Currently only works for continuous y
v0.12,causal_model = outcome_mean + estimate.value*self._treatment
v0.12,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.12,r_squared = 1 - (squared_residual/total_variance)
v0.12,return r_squared
v0.12,Check if estimator's target estimand is identified
v0.12,Store parameters inside estimate object for refutation methods
v0.12,TODO: This add_params needs to move to the estimator class
v0.12,inside estimate_effect and estimate_conditional_effect
v0.12,"TODO: Remove _data, _treatment_name and _outcome_name from this object"
v0.12,we save them here to enable the methods that required these properties saved in the estimator
v0.12,eventually we should call those methods and just save the results in this object
v0.12,instead of having this object invoke the estimator methods with the data.
v0.12,No estimand was identified (identification failed)
v0.12,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.12,"can't use nx.node_connected_component, because it doesn't work with DiGraphs."
v0.12,"Hence, a manual loop:"
v0.12,also return the number of backdoor paths blocked by observed nodes
v0.12,Assume that nodes1 is the treatment
v0.12,"ignores new_graph parameter, always uses self._graph"
v0.12,removal of only direct edges wrt a target is not implemented for incoming edges
v0.12,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.12,[TODO: double check these work with multivariate implementation:]
v0.12,Exclusion
v0.12,As-if-random setup
v0.12,As-if-random
v0.12,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.12,Adding common causes
v0.12,Adding instruments
v0.12,Adding effect modifiers
v0.12,Assuming the simple form of effect modifier
v0.12,that directly causes the outcome.
v0.12,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.12,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.12,some preprocessing steps
v0.12,parsing the correct graph based on input graph format
v0.12,load dot file
v0.12,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.12,one-hot encode discrete W
v0.12,Now deleting the old continuous value
v0.12,create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause
v0.12,"Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately"
v0.12,One Hot Encoding
v0.12,TODO Ensure that we do not generate weak instruments
v0.12,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.12,Converting treatment to binary if required
v0.12,Generating frontdoor variables if asked for
v0.12,NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.
v0.12,"For categorical t, this example dataset has the effect size for category 2 being exactly"
v0.12,double the effect for category 1
v0.12,This could be changed at this stage by one-hot encoding t and using a custom beta that
v0.12,"sets a different effect for each category {0, 1, 2}"
v0.12,Computing ATE
v0.12,constructing column names for one-hot encoded discrete features
v0.12,Specifying the correct dtypes
v0.12,Now specifying the corresponding graph strings
v0.12,Now writing the gml graph
v0.12,creating data frame
v0.12,Specifying the correct dtypes
v0.12,Now specifying the corresponding graph strings
v0.12,Now writing the gml graph
v0.12,Adding edges between common causes and the frontdoor mediator
v0.12,Error terms
v0.12,else:
v0.12,V = 6 + W0 + tterm + E1
v0.12,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.12,Generating a random normal distribution of integers
v0.12,Generating data for nodes which have no incoming edges
v0.12,"""currset"" variable currently has all the successors of the nodes which had no incoming edges"
v0.12,assuming that all unobserved common causes are numerical and are not affected by one hot encoding
v0.12,Creating a NN to simulate the nuisance function
v0.12,strength of unobserved confounding
v0.12,Computing ATE
v0.12,Specifying the correct dtypes
v0.12,Now writing the gml graph
v0.12,The following code for loading the Lalonde dataset was copied from
v0.12,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.12,
v0.12,"Copyright 2018, Wayfair, Inc."
v0.12,
v0.12,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.12,the following conditions are met:
v0.12,
v0.12,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.12,following disclaimer.
v0.12,
v0.12,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.12,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.12,
v0.12,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.12,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.12,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.12,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.12,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.12,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.12,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.12,DAMAGE.
v0.12,The following code is a slight modification of
v0.12,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.12,
v0.12,"Copyright 2018, Wayfair, Inc."
v0.12,
v0.12,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.12,the following conditions are met:
v0.12,
v0.12,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.12,following disclaimer.
v0.12,
v0.12,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.12,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.12,
v0.12,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.12,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.12,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.12,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.12,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.12,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.12,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.12,DAMAGE.
v0.12,
v0.12,0.0.0 is standard placeholder for poetry-dynamic-versioning
v0.12,any changes to this should not be checked in
v0.12,
v0.12,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.12,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.12,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.12,adapt number of channels
v0.12,save memory
v0.12,Keep same dimensions
v0.12,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.12,single-attribute Causal
v0.12,test environment
v0.12,Subsample 2x for computational convenience
v0.12,Assign a binary label based on the digit
v0.12,Flip label with probability 0.25
v0.12,Assign a color based on the label; flip the color with probability environment
v0.12,Apply the color to the image by zeroing out the other color channel
v0.12,single-attribute Independent
v0.12,test environment
v0.12,Subsample 2x for computational convenience
v0.12,Assign a binary label based on the digit
v0.12,Flip label with probability 0.25
v0.12,multi-attribute Causal + Independent
v0.12,test environment
v0.12,Subsample 2x for computational convenience
v0.12,rotate the image by angle in parameter
v0.12,Assign a binary label based on the digit
v0.12,Flip label with probability 0.25
v0.12,Assign a color based on the label; flip the color with probability environment
v0.12,Apply the color to the image by zeroing out the other color channel
v0.12,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.12,Acause regularization
v0.12,Aconf regularization
v0.12,Aind regularization
v0.12,Asel regularization
v0.12,Compile loss
v0.12,Check if the optimizer is currently supported
v0.12,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.12,The currently supported estimators
v0.12,The default standard deviation for noise
v0.12,The default scaling factor to determine the bucket size
v0.12,The minimum number of points for the estimator to run
v0.12,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.12,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.12,The Default split for the number of data points that fall into the training and validation sets
v0.12,Assuming that outcome is one-dimensional
v0.12,We need to change the identified estimand
v0.12,"We thus, make a copy. This is done as we don't want"
v0.12,to change the original DataFrame
v0.12,We use collections.OrderedDict to maintain the order in which the data is stored
v0.12,Check if we are using an estimator in the transformation list
v0.12,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.12,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.12,"loops. Thus, we can get different values everytime we get the estimator."
v0.12,for _ in range( self._num_simulations ):
v0.12,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.12,Adding an unobserved confounder if provided by the user
v0.12,We set X_train = 0 and outcome_train to be 0
v0.12,"Get the final outcome, after running through all the values in the transformation list"
v0.12,Check if the value of true effect has been already stored
v0.12,We use None as the key as we have no base category for this refutation
v0.12,As we currently support only one treatment
v0.12,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.12,Check if the value of true effect has been already stored
v0.12,This ensures that we calculate the causal effect only once.
v0.12,We use key_train as we map data with respect to the base category of the data
v0.12,As we currently support only one treatment
v0.12,Add h(t) to f(W) to get the dummy outcome
v0.12,We convert to ndarray for ease in indexing
v0.12,The data is of the form
v0.12,sim1: cat1 cat2 ... catn
v0.12,sim2: cat1 cat2 ... catn
v0.12,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.12,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.12,distribution of the refuter.
v0.12,True Causal Effect list
v0.12,Iterating through the refutation for each category
v0.12,We use string arguments to account for both 32 and 64 bit varaibles
v0.12,action for continuous variables
v0.12,Action for categorical variables
v0.12,Find the set difference for each row
v0.12,Choose one out of the remaining
v0.12,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.12,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.12,of the treatment to affect the outcome
v0.12,TODO: Check that the target estimand has backdoor variables?
v0.12,1. Categorical encoding of relevant variables
v0.12,"The encoded data is only used to calculate a parameter, so the encoder can be discarded."
v0.12,2. Standardizing the data
v0.12,Fit a model containing all confounders and compare predictions
v0.12,using all features compared to all features except a given
v0.12,confounder.
v0.12,Estimating the regression coefficient from standardized features to t
v0.12,"By default, return a plot with 10 points"
v0.12,consider 10 values of the effect of the unobserved confounder
v0.12,Fit a model containing all confounders and compare predictions
v0.12,using all features compared to all features except a given
v0.12,confounder.
v0.12,"By default, return a plot with 10 points"
v0.12,consider 10 values of the effect of the unobserved confounder
v0.12,"By default, we add the effect of simulated confounder for treatment."
v0.12,But subtract it from outcome to create a negative correlation
v0.12,assuming that the original confounder's effect was positive on both.
v0.12,This is to remove the effect of the original confounder.
v0.12,"By default, we add the effect of simulated confounder for treatment."
v0.12,But subtract it from outcome to create a negative correlation
v0.12,assuming that the original confounder's effect was positive on both.
v0.12,This is to remove the effect of the original confounder.
v0.12,Obtaining the list of observed variables
v0.12,Taking a subset of the dataframe that has only observed variables
v0.12,Residuals from the outcome model obtained by fitting a linear model
v0.12,Residuals from the treatment model obtained by fitting a linear model
v0.12,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.12,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.12,Choosing a c_star based on the data.
v0.12,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.12,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.12,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.12,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.12,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.12,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.12,TODO (petergtz): This should introduce a Protocol defining the interface this returns instead of the concrete types
v0.12,"If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen"
v0.12,Get a 2D matrix of values
v0.12,"x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN"
v0.12,Store the values into the refute object
v0.12,Adding a label on the contour line for the original estimate
v0.12,Label every other level using strings
v0.12,Default value of the p value taken for the distribution
v0.12,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.12,Mean of the Normal Distribution
v0.12,Standard Deviation of the Normal Distribution
v0.12,Create a new column in the data by the name of placebo
v0.12,Sanity check the data
v0.12,only permute is supported for iv methods
v0.12,"For IV methods, the estimating_instrument_names should also be"
v0.12,changed. Create a copy to avoid modifying original object
v0.12,We need to change the identified estimand
v0.12,"We make a copy as a safety measure, we don't want to change the"
v0.12,original DataFrame
v0.12,Run refutation in parallel
v0.12,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.12,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.12,relationship between the treatment and the outcome.
v0.12,new estimator
v0.12,new effect estimate
v0.12,observed covariate E-value
v0.12,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.12,"if CI crosses null, set its E-value to 1"
v0.12,only report E-value for CI limit closer to null
v0.12,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.12,whether the DGP is assumed to be partially linear
v0.12,features are the observed confounders
v0.12,Now code for benchmarking using covariates begins
v0.12,R^2 of outcome with observed common causes and treatment
v0.12,R^2 of treatment with observed common causes
v0.12,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.12,Assuming that the difference in R2 is the same for wj and new unobserved confounder
v0.12,"for treatment,  Calpha is not a function of the partial R2. So we need a different assumption."
v0.12,Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder
v0.12,"(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment"
v0.12,wrt unobserved confounders in partial-linear models
v0.12,Do the support characterization
v0.12,Recover the samples that are in the support
v0.12,Assess overlap using propensity scores with cross-fitting
v0.12,Check if all supported units are considered to be in the overlap set
v0.12,"NOTE: The original paper implements both DNF and CNF rules, but for simplicity, this code only implements DNF rules"
v0.12,"For DNF rules, a sample is covered if *any* rule applies"
v0.12,whether the DGP is assumed to be partially linear
v0.12,can change this to allow default values that are same as the other parameter
v0.12,Strength of confounding that omitted variables generate in treatment regression
v0.12,computing the point estimate for the bounds
v0.12,common causes after removing the benchmark causes
v0.12,dataframe with treatment and observed common causes after removing benchmark causes
v0.12,R^2 of treatment with observed common causes removing benchmark causes
v0.12,return the variance of alpha_s
v0.12,R^2 of outcome with observed common causes and treatment after removing benchmark causes
v0.12,Obtaining theta_s (the obtained estimate)
v0.12,Creating numpy arrays
v0.12,Setting up cross-validation parameters
v0.12,"tuple of residuals from first stage estimation [0,1], and the confounders [2]"
v0.12,"We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation."
v0.12,Yres = Y - E[Y|W]
v0.12,E[Y|W] = f(x) + theta_s * E[T|W]
v0.12,Yres = Y - f(x) - theta_s * E[T|W]
v0.12,g(s) = theta_s * T + f(x)
v0.12,g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]
v0.12,g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]
v0.12,Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )
v0.12,Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres
v0.12,Y - g(s) = Yres - theta_s * Tres
v0.12,nu_2 is E[alpha_s^2]
v0.12,Now computing scores for finding the (1-a) confidence interval
v0.12,R^2 of treatment with observed common causes
v0.12,R^2 of outcome with treatment and observed common causes
v0.12,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.12,Partial R^2 of treatment after regressing over unobserved confounder and observed common causes
v0.12,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.12,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.12,Adding unadjusted point estimate
v0.12,Adding bounds to partial R^2 values for given strength of confounders
v0.12,Adding a new backdoor variable to the identified estimand
v0.12,Run refutation in parallel
v0.12,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.12,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.12,of the treatment to affect the outcome
v0.12,TODO: Sensitivity Analyzers excluded from list due to different return type
v0.12,Reject H0
v0.12,"a, b and c are all continuous variables"
v0.12,"a, b and c are all discrete variables"
v0.12,c is set of continuous and binary variables and
v0.12,1. either a and b is continuous and the other is binary
v0.12,2. both a and b are binary
v0.12,c is discrete and
v0.12,either a or b is continuous and the other is discrete
v0.12,a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information
v0.12,original_treatment_name: : stores original variable names for labelling
v0.12,common_causes_map : maps the original variable names to variable names in OLS regression
v0.12,benchmark_common_causes: stores variable names in terms of regression model variables
v0.12,original_benchmark_covariates: stores original variable names for labelling
v0.12,estimate: estimate of regression
v0.12,degree_of_freedom: degree of freedom of error in regression
v0.12,standard_error: standard error in regression
v0.12,t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.
v0.12,partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them
v0.12,"r2tu_w: partial R^2  of unobserved confounder ""u"" with treatment ""t"", after conditioning on observed covariates ""w"""
v0.12,"r2yu_tw: partial R^2  of unobserved confounder ""u"" with outcome ""y"", after conditioning on observed covariates ""w"" and treatment ""t"""
v0.12,"r2twj_w: partial R^2 of observed covariate wj with treatment ""t"", after conditioning on observed covariates ""w"" excluding wj"
v0.12,"r2ywj_tw:  partial R^2 of observed covariate wj with outcome ""y"", after conditioning on observed covariates ""w"" (excluding wj) and treatment ""t"""
v0.12,benchmarking_results: dataframe containing information about bounds and bias adjusted terms
v0.12,"stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic"
v0.12,partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment
v0.12,build a new regression model by considering treatment variables as outcome
v0.12,"r2twj_w is partial R^2 of covariate wj with treatment ""t"", after conditioning on covariates w(excluding wj)"
v0.12,"r2ywj_tw is partial R^2 of covariate wj with outcome ""y"", after conditioning on covariates w(excluding wj) and treatment ""t"""
v0.12,r2tu_w is the partial r^2 from regressing u on t after conditioning on w
v0.12,Compute bias adjusted terms
v0.12,Plotting the contour plot
v0.12,Adding contours
v0.12,Adding threshold contour line
v0.12,Adding unadjusted point estimate
v0.12,Adding bounds to partial R^2 values for given strength of confounders
v0.12,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.12,The default subset of the data to be used
v0.12,Run refutation in parallel
v0.12,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.12,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.12,of the treatment to affect the outcome
v0.12,Parameters
v0.12,Bookkeeping
v0.12,Initialize estimators
v0.12,Convert to dataframe if not
v0.12,Format labels
v0.12,Sample from reference measure and construct features
v0.12,Add reference samples
v0.12,Binarize features (fit to data only)
v0.12,Fit estimator
v0.12,Store reference volume
v0.12,Construct features dataframe
v0.12,Construct features dataframe
v0.12,Iterate over columns
v0.12,"logging.info(""Using provided reference range for {}"".format(c))"
v0.12,number of unique values
v0.12,Constant column
v0.12,Binary column
v0.12,Ordinal column (seed = counter so not correlated)
v0.12,For get_params / set_params
v0.12,"Overlap / Support (y = +1), non-overlap (y = 0), and uniform background (y = -1) samples"
v0.12,"We should always have overlap samples, and either background or non-overlap samples"
v0.12,"This will throw an error if, for example, all samples are considered to"
v0.12,be in the overlap region
v0.12,"Initialize with empty and singleton conjunctions, i.e. X plus all-ones feature"
v0.12,Feature indicator and conjunction matrices
v0.12,Iteration counter
v0.12,Formulate master LP
v0.12,Variables
v0.12,Objective function (no penalty on empty conjunction)
v0.12,Constraints
v0.12,This gets activated for DNF
v0.12,Solve problem
v0.12,Extract dual variables
v0.12,Beam search for conjunctions with negative reduced cost
v0.12,Most negative reduced cost among current variables
v0.12,Negative reduced costs found
v0.12,Add to existing conjunctions
v0.12,Reformulate master LP
v0.12,Variables
v0.12,Objective function
v0.12,Constraints
v0.12,Solve problem
v0.12,Extract dual variables
v0.12,Beam search for conjunctions with negative reduced cost
v0.12,Most negative reduced cost among current variables
v0.12,"print('UB.min():', UB.min())"
v0.12,Save generated conjunctions and coefficients
v0.12,Restrict conjunctions to those used by LP
v0.12,"NOTE: This is a greedy approach, so it does not incorporate lambda0 explicitly"
v0.12,"Similarly, it will prefer a larger number of smaller rules if lambda1 is set"
v0.12,"to a larger value, because the incremental cost will be lower."
v0.12,Fraction of reference samples that each conjunction covers
v0.12,Regularization (for each conjunction)
v0.12,Positive samples newly covered (for each conjunction)
v0.12,Costs (for each conjunction)
v0.12,Zero out the rules and only take those which are used
v0.12,Small tolerance on comparisons
v0.12,This can be useful to break ties and favor larger values of xi
v0.12,Compute conjunctions of features
v0.12,Predict labels
v0.12,Use helper function
v0.12,Use helper function
v0.12,Lower bound specific to each singleton solution
v0.12,Initialize output
v0.12,Remove redundant rows by grouping by unique feature combinations and summing residual
v0.12,Initialize queue with root instance
v0.12,Separate data according to positive and negative residuals
v0.12,Iterate over increasing degree while queue is non-empty
v0.12,Initialize list of children to process
v0.12,Process instances in queue
v0.12,inst = instCurr[0]
v0.12,Evaluate all singleton solutions
v0.12,Best solutions that also improve on current output (allow for duplicate removal)
v0.12,Append to current output
v0.12,Remove duplicates
v0.12,Update output
v0.12,Compute lower bounds on higher-degree solutions
v0.12,Evaluate children using weighted average of their costs and LBs
v0.12,Best children with potential to improve on current output and current candidates (allow for duplicate removal)
v0.12,Iterate through best children
v0.12,"New ""zero"" solution"
v0.12,Check if duplicate
v0.12,Add to candidates for further processing
v0.12,Create pricing instance
v0.12,Remove covered rows
v0.12,Remove redundant features
v0.12,Track number of candidates added
v0.12,Update candidates
v0.12,Instances to process in next iteration
v0.12,Conjunctions corresponding to solutions
v0.12,List of categorical columns
v0.12,Number of quantile thresholds used to binarize ordinal features
v0.12,whether to append negations
v0.12,whether to convert thresholds on ordinal features to strings
v0.12,Quantile probabilities
v0.12,Initialize
v0.12,Iterate over columns
v0.12,number of unique values
v0.12,Constant or binary column
v0.12,"Mapping to 0, 1"
v0.12,Categorical column
v0.12,OneHotEncoder object
v0.12,Fit to observed categories
v0.12,Ordinal column
v0.12,Few unique values
v0.12,Thresholds are sorted unique values excluding maximum
v0.12,Many unique values
v0.12,Thresholds are quantiles excluding repetitions
v0.12,Contains NaN values
v0.12,Initialize dataframe
v0.12,Iterate over columns
v0.12,Constant or binary column
v0.12,"Rename values to 0, 1"
v0.12,Categorical column
v0.12,Apply OneHotEncoder
v0.12,Append negations
v0.12,Concatenate
v0.12,Ordinal column
v0.12,Threshold values to produce binary arrays
v0.12,Append negations
v0.12,Convert to dataframe with column labels
v0.12,Ensure that rows corresponding to NaN values are zeroed out
v0.12,Add NaN indicator column
v0.12,Concatenate
v0.12,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.12,set attributions to zero for left out invariant nodes
v0.12,Get parent and child nodes
v0.12,Don't remove node if node has more than 1 children nodes as it can introduce
v0.12,hidden confounders.
v0.12,Remove the middle node
v0.12,Connect parent and child nodes
v0.12,Update the causal mechanism for the child nodes
v0.12,"If the relative change of the score is less than the given threshold, we stop the estimation early."
v0.12,Compute rank of every single test point in the union of the training and the respective test point.
v0.12,+ 1 here to count the test sample itself.
v0.12,The probability to get at most rank k from above is k divided by the total number of samples. Similar for
v0.12,"the case of below. Therefore, to get at most rank k either from above or below is"
v0.12,"min(2*k/total_num_samples, 1). We then get a p-value for exchangeability:"
v0.12,"Note, the output of score_samples are log values."
v0.12,"Note, the output of score_samples are log values."
v0.12,"When all variables are independent (Example C.4), simplify to a group of 0s and a group of 1s (regardless of order)."
v0.12,"Otherwise, we just group the consecutive values (Remark C.1)."
v0.12,Train gamma_K:
v0.12,"Train gamma_k for k = K-1, K-2, ..., 1:"
v0.12,Use the fitted values from previous regression
v0.12,"Train classifiers that will go into alpha_k for k = 1, ..., K:"
v0.12,"For the case where you want to calibrate on different data,"
v0.12,No need if classifier is CalibratedClassifierCv
v0.12,"k = 0 doesn't have parents, get the marginal RN derivative."
v0.12,For k > 0 get the conditional RN derivative dividing the RN derivative for \bar{X}_j
v0.12,by the RN derivative for \bar{X}_{j-1}.
v0.12,"alpha_k should integrate to 1. In small samples this might not be the case, so we standardize:"
v0.12,Regression base estimate:
v0.12,Debiasing terms up to K-1:
v0.12,"When C = [1, 1, ..., 1] we can just take the sample mean of y_eval[T_eval == 1]"
v0.12,"When C = [0, 0, ..., 0] we can just take the sample mean of y_eval[T_eval == 0]"
v0.12,Eliminate nodes that are not ancestry of the outcome:
v0.12,Sort by causal ancestry:
v0.12,Currently only support continuous distributions for auto selection.
v0.12,Estimate distribution parameters from data.
v0.12,Ignore warnings from fitting process.
v0.12,Fit distribution to data.
v0.12,Some distributions might not be compatible with the data.
v0.12,Separate parts of parameters.
v0.12,Check the KL divergence between the distribution of the given and fitted distribution.
v0.12,Identify if this distribution is better.
v0.12,This error is typically raised when the data is discrete and all points are assigned to less cluster than
v0.12,"specified. It can also happen due to duplicated points. In these cases, the current best solution should"
v0.12,be sufficient.
v0.12,"Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]"
v0.12,"Normally, we need to estimate the KL divergence jointly. However, to avoid issues with high dimensional data,"
v0.12,we approximate it by taking the average over the marginal KL divergences.
v0.12,Do not compare with same model class
v0.12,Do not compare with same model class
v0.12,"In case of post non-linear models, we can obtain the conditional expectation directly based on the prediction"
v0.12,"model. To do this, we can just in pass 0 as the noise, since this would evaluate Y = f(X) + 0 in case of an"
v0.12,additive noise model and Y = g(f(X) + 0) in case of a more general model.
v0.12,Estimate the conditional expectation E[Y | x] by generating multiple samples for Y|x and average them.
v0.12,"Since these are categorical values, we just need to look for the most frequent element after we drew"
v0.12,multiple samples for each input.
v0.12,"In the categorical case, this is equivalent to the Brier score. However, the following formulation allows"
v0.12,categorical data with more than two classes.
v0.12,A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This
v0.12,function evaluates the provided causal query multiple times to build a confidence interval based on the returned
v0.12,results.
v0.12,Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order
v0.12,"to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the"
v0.12,fit_and_compute function.
v0.12,
v0.12,**Example usage:**
v0.12,
v0.12,">>> gcm.fit(causal_model, data)"
v0.12,">>> strength_medians, strength_intervals = gcm.confidence_intervals("
v0.12,">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))"
v0.12,
v0.12,"In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the"
v0.12,"confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'"
v0.12,"here to configure the function call. In this case,"
v0.12,"gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to"
v0.12,"lambda : gcm.arrow_strength(causal_model, target_node='Y')."
v0.12,
v0.12,"In order to incorporate uncertainties coming from fitting the causal model(s), we can use"
v0.12,gcm.fit_and_compute instead:
v0.12,">>>  strength_medians, strength_intervals = gcm.confidence_intervals("
v0.12,">>>        gcm.fit_and_compute(gcm.arrow_strength,"
v0.12,">>>                                            causal_model,"
v0.12,">>>                                            bootstrap_training_data=data,"
v0.12,>>>                                            target_node='Y'))
v0.12,This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each
v0.12,run.
v0.12,"Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based"
v0.12,on their topological order.
v0.12,"After drawing samples of the node based on the data generation process, we apply the corresponding"
v0.12,intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this
v0.12,node.
v0.12,"Abduction: For invertible SCMs, we recover exact noise values from data."
v0.12,Action + Prediction: Propagate the intervention downstream using recovered noise values.
v0.12,Check if we need to apply an intervention on the given node.
v0.12,Apply intervention function to the data of the node.
v0.12,Check if the intervention function changes the shape of the data.
v0.12,"For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e."
v0.12,all ancestors of the target.
v0.12,The target node can be a continuous real-valued variable or a categorical variable with at most two classes
v0.12,(i.e. binary).
v0.12,Find all node names in the expression string.
v0.12,"Nothing to fit here, since we know the ground truth."
v0.12,Avoid too many features
v0.12,Making sure there are at least 30% test samples.
v0.12,"Making sure that there are at least 2 samples from one class (here, simply duplicate the point)."
v0.12,Compare number of correct classifications.
v0.12,"Making sure that X and Y have no overlapping values, which would lead to a distance of 0 with k=1 and, thus, to"
v0.12,a division by zero.
v0.12,All elements are equal (or at least less than k samples are different)
v0.12,Balance the classes
v0.12,For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples
v0.12,is unknown beforehand.
v0.12,The method stops if either the change between some consecutive runs is below the given threshold or the
v0.12,maximum number of runs is reached.
v0.12,"In each run, we create one random permutation of players. For instance, given 4 players, a permutation"
v0.12,"could be [3,1,4,2]."
v0.12,Generate k random permutations by sorting the indices of the Halton sequence
v0.12,"Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the"
v0.12,"subsets are [3], [3,1], [3,1,4] [3,1,4,2]."
v0.12,"The result for each subset is cached such that if a subset that has already been evaluated appears again,"
v0.12,we can take this result directly.
v0.12,"To improve the runtime, multiple permutations are evaluated in each run."
v0.12,"The current Shapley values are the average of the estimated values, i.e. we need to divide by the number"
v0.12,of generated permutations here.
v0.12,"In case Shapley values are estimated for multiple samples, e.g., in feature relevance. So, we have a"
v0.12,matrix of Shapley values instead of a vector.
v0.12,"Here, the change between consecutive runs is below the minimum threshold, but to reduce the"
v0.12,"likelihood that this just happened by chance, we require that this happens at least for"
v0.12,num_consecutive_converged_runs times in a row.
v0.12,Check if change in percentage is below threshold
v0.12,"Check for values that are exactly zero. If they don't change between two runs, we consider it as converging."
v0.12,Create all (unique) subsets)
v0.12,"Assigning a 'high' weight, since this resembles ""infinity""."
v0.12,The weight for a subset with a specific length (see paper mentioned in the docstring for more
v0.12,information).
v0.12,TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is
v0.12,"theoretically sound, i.e. make entropy results from different data comparable."
v0.12,Extremely small values can somehow result in negative values.
v0.12,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.12,Sampling from the conditional distribution based on the current sample.
v0.12,"Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that"
v0.12,"are in the given subset. By this, we can simulate the impact on the conditional distribution when removing"
v0.12,only the incoming edges of the variables in the subset.
v0.12,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.12,"In case of the full subset (no randomization), we get the same predictions as when we apply the"
v0.12,"prediction method to the samples of interest, since all noise samples are replaced with a sample of"
v0.12,interest.
v0.12,"In case of the empty subset (all are jointly randomize), it boils down to taking the average over all"
v0.12,"predictions, seeing that the randomization yields the same values for each sample of interest (none of the"
v0.12,samples of interest are used to replace a (jointly) 'randomized' sample).
v0.12,Exact model
v0.12,This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes
v0.12,This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes
v0.12,afterwards.
v0.12,Todo: Remove after https://github.com/py-why/dowhy/pull/943.
v0.12,Smallest possible value. This is used in various algorithm for numerical stability.
v0.12,Note: The alpha level doesn't matter here.
v0.12,Make copy to avoid manipulating the original matrix.
v0.12,"The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,"
v0.12,the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster
v0.12,"than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as"
v0.12,"possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples."
v0.12,"To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many"
v0.12,samples. The number of samples that are evaluated is normally
v0.12,"baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to"
v0.12,"batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is"
v0.12,evaluated one by one in a for-loop.
v0.12,Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple
v0.12,"batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet."
v0.12,"If the batch size would be larger than the remaining amount of samples, it is reduced to only include the"
v0.12,remaining baseline_noise_samples.
v0.12,"The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features"
v0.12,in baseline_feature_indices to their respective values in baseline_noise_samples.
v0.12,"After creating the (potentially large) input data matrix, we can evaluate the prediction method."
v0.12,"Here, offset + index now indicates the sample index in baseline_noise_samples."
v0.12,This would average all prediction results obtained for the 'offset + index'-th sample in
v0.12,"baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)]."
v0.12,This would return all prediction results obtained for the 'offset + index'-th sample in
v0.12,"baseline_noise_samples, i.e. the results are not averaged."
v0.12,Making copy to ensure that the original object is not modified.
v0.12,Permute samples jointly. This still represents an interventional distribution.
v0.12,Permute samples independently.
v0.12,Looks for the first index where the cumulative sum of the probabilities is larger than the threshold.
v0.12,"Note that if there are multiple indices with the same maximum value (as in this case here), the argmax"
v0.12,function returns the first index.
v0.12,"test local Markov condition, null hypothesis: conditional independence"
v0.12,"test edge dependence, null hypothesis: independence"
v0.12,The order of the p-values added to the list is deterministic.
v0.12,"To be able to validate that the graph structure did not change between fitting and causal query, we store the"
v0.12,"parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While"
v0.12,"this would automatically fail when the number of parents is different, there are other more subtle cases,"
v0.12,"where the number is still the same, but it's different parents, and therefore different data. That would yield"
v0.12,"wrong results, but would not fail."
v0.12,Independence tests are symmetric
v0.12,Find out which tests to do
v0.12,Parallelize over tests
v0.12,Gather results
v0.12,Summarize
v0.12,Find out which tests to do
v0.12,Parallelize over tests
v0.12,Gather results
v0.12,Summarize
v0.12,Find out which tests to do
v0.12,Parallelize over tests
v0.12,Gather results
v0.12,Summarize
v0.12,DAG Evaluation
v0.12,Suggestions
v0.12,"Append list of violations (node, non_desc) to get local information"
v0.12,Plot histograms
v0.12,Plot given violations
v0.12,For LMC we highlight X for which X _|/|_ Y \in ND_X | Pa_X
v0.12,For PD we highlight the edge (if Y\in Anc_X -> X are adjacent)
v0.12,For causal minimality we highlight the edge Y \in Pa_X -> X
v0.12,Create Validation header
v0.12,Create Validation summary
v0.12,Close Validation
v0.12,Create Suggestions header
v0.12,Iterate over suggestions
v0.12,Test if we have data for X and Y
v0.12,Test if we have data for Z
v0.12,Eq. (1) in https://arxiv.org/pdf/1810.11363.pdf
v0.12,Subtracting Y here since the cumulative sum includes the current element. The same reason we subtract 1
v0.12,from the count.
v0.12,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.12,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.12,"target quantity (here, variance)."
v0.12,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.12,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.12,"target quantity (here, variance)."
v0.12,"Calculate Ri, the product of the residuals"
v0.12,Standard deviation of the residuals
v0.12,Either X and/or Y is constant.
v0.12,"If Z is empty, we are in the pairwise setting."
v0.12,Either X and/or Y is constant.
v0.12,"If Z is empty, we are in the pairwise setting."
v0.12,Estimate test statistic multiple times on different permutations of the data. The p-value is then the
v0.12,probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted
v0.12,data.
v0.12,Take the lower dimensional variable as target.
v0.12,First stage statistical model
v0.12,Second stage statistical model
v0.12,Check if the treatment is one-dimensional
v0.12,First stage
v0.12,Second Stage
v0.12,Combining the two estimates
v0.12,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.12,Bulding the feature matrix
v0.12,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.12,TODO move this to the identification step
v0.12,Obtain estimate by Wald Estimator
v0.12,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.12,More than 1 instrument. Use 2sls.
v0.12,Checking if Y is binary
v0.12,Enable the user to pass params for a custom propensity model
v0.12,Convert the categorical variables into dummy/indicator variables
v0.12,"Basically, this gives a one hot encoding for each category"
v0.12,The first category is taken to be the base line.
v0.12,Check if the treatment is one-dimensional
v0.12,Checking if the treatment is binary
v0.12,The model is always built on the entire data
v0.12,TODO make treatment_value and control value also as local parameters
v0.12,All treatments are set to the same constant value
v0.12,"Fixing treatment value to the specified value, if provided"
v0.12,treatment_vals and data_df should have same number of rows
v0.12,Bulding the feature matrix
v0.12,Replace treatment values with value supplied; note: Don't change column datatype!
v0.12,The model is always built on the entire data
v0.12,The average treatment effect is a combination of different
v0.12,regression coefficients. Complicated to compute the confidence
v0.12,"interval analytically. For example, if y=a + b1.t + b2.tx, then"
v0.12,the average treatment effect is b1+b2.mean(x).
v0.12,"Refer Gelman, Hill. ARM Book. Chapter 9"
v0.12,http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
v0.12,TODO: Looking for contributions
v0.12,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.12,"variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0."
v0.12,"So for custom treatment and control values, we must multiply the confidence interval by the difference of the two."
v0.12,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.12,"variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0."
v0.12,"So for custom treatment and control values, we must multiply the standard error by the difference of the two."
v0.12,setting method-specific parameters
v0.12,Infer the right strata based on clipping threshold
v0.12,0.5 because there are two values for the treatment
v0.12,To be conservative and allow most strata to be included in the
v0.12,analysis
v0.12,At least 90% of the strata should be included in analysis
v0.12,sum weighted outcomes over all strata  (weight by treated population)
v0.12,TODO - how can we add additional information into the returned estimate?
v0.12,"such as how much clipping was done, or per-strata info for debugging?"
v0.12,sort the dataframe by propensity score
v0.12,create a column 'strata' for each element that marks what strata it belongs to
v0.12,"for each strata, count how many treated and control units there are"
v0.12,throw away strata that have insufficient treatment or control
v0.12,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.12,Setting method specific parameters
v0.12,trim propensity score weights
v0.12,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.12,nips ==> ips / (sum of ips over all units)
v0.12,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.12,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.12,Vanilla IPS estimator
v0.12,The Hajek estimator (or the self-normalized estimator)
v0.12,"Stabilized weights (from Robins, Hernan, Brumback (2000))"
v0.12,Paper: Marginal Structural Models and Causal Inference in Epidemiology
v0.12,Calculating the effect
v0.12,Subtracting the weighted means
v0.12,TODO - how can we add additional information into the returned estimate?
v0.12,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.12,Save parameters for later refutter fitting
v0.12,Enforcing this ordering is necessary to feed through the propensity values from dataset
v0.12,For metalearners only--issue a warning if w contains variables not in x
v0.12,Override the effect_modifiers set in CausalEstimator.__init__()
v0.12,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.12,the latter can be used by other estimator methods later
v0.12,"Instrumental variables names, if present"
v0.12,choosing the instrumental variable to use
v0.12,Calling the econml estimator's fit method
v0.12,"As of v0.9, econml has some kewyord only arguments"
v0.12,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.12,Changing shape to a list for a singleton value
v0.12,Note that self._control_value is assumed to be a singleton value
v0.12,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.12,"For each unit, return the estimated effect of the treatment value"
v0.12,that was actually applied to the unit
v0.12,this assumes a binary treatment regime
v0.12,TODO remove neighbors that are more than a given radius apart
v0.12,Estimating ATT on treated by summing over difference between matched neighbors
v0.12,Estimating ATC
v0.12,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.12,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.12,Handle externally provided estimator classes
v0.12,allowed types of distance metric
v0.12,Dictionary of any user-provided params for the distance metric
v0.12,that will be passed to sklearn nearestneighbors
v0.12,Check if the treatment is one-dimensional
v0.12,Checking if the treatment is binary
v0.12,Convert the categorical variables into dummy/indicator variables
v0.12,"Basically, this gives a one hot encoding for each category"
v0.12,The first category is taken to be the base line.
v0.12,this assumes a binary treatment regime
v0.12,TODO remove neighbors that are more than a given radius apart
v0.12,estimate ATT on treated by summing over difference between matched neighbors
v0.12,Return indices in the original dataframe
v0.12,self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()
v0.12,Now computing ATC
v0.12,Return indices in the original dataframe
v0.12,Add the identification method used in the estimator
v0.12,Check the backdoor variables being used
v0.12,Add the observed confounders and one hot encode the categorical variables
v0.12,Get the data of the unobserved confounders
v0.12,One hot encode the data if they are categorical
v0.12,Check the instrumental variables involved
v0.12,Perform the same actions as the above
v0.12,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.12,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.12,For CATEs
v0.12,TODO we are conditioning on a postive treatment
v0.12,TODO create an expression corresponding to each estimator used
v0.12,Determine columns being encoded
v0.12,"If all columns are already numerical, there may be nothing to encode."
v0.12,"In this case, return original data."
v0.12,Columns to keep in the result - not encoded.
v0.12,Convert the encoded data to a DataFrame
v0.12,Concatenate the encoded DataFrame with the original non-categorical columns
v0.12,Remember encoder
v0.12,variable_type:
v0.12,"A dictionary containing the variable's names and types. 'c' for continuous, 'o'"
v0.12,"for ordered, 'd' for discrete, and 'u' for unordered discrete."
v0.12,[] notation to retain DataFrame rather than Series.
v0.12,"For one_hot_encode type must be categorical, or it won't encode."
v0.12,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.12,Flipping some values
v0.12,Check if the edge already exists
v0.12,"If the edge exists, append the time_lag to the existing tuple"
v0.12,"If the edge does not exist, create a new edge with a tuple containing the time_lag"
v0.12,Read the CSV file into a DataFrame
v0.12,Initialize an empty directed graph
v0.12,Add edges with time lag to the graph
v0.12,Add validation for the time lag column to be a number
v0.12,Check if the edge already exists
v0.12,"If the edge exists, append the time_lag to the existing tuple"
v0.12,"If the edge does not exist, create a new edge with a tuple containing the time_lag"
v0.12,Read the DOT file into a MultiDiGraph
v0.12,Initialize a new DiGraph
v0.12,Iterate over edges of the MultiDiGraph
v0.12,Convert the label to a tuple of time lags
v0.12,Merge the existing time lags with the new ones
v0.12,Remove duplicates by converting to a set and back to a tuple
v0.12,Initialize a directed graph
v0.12,Add nodes with names
v0.12,Iterate over all pairs of nodes
v0.12,Check for directed links
v0.12,Append the time lag to the existing tuple
v0.12,Create a new edge with a tuple containing the time lag
v0.12,Append the time lag to the existing tuple
v0.12,Create a new edge with a tuple containing the time lag
v0.12,Wrapping labels if they are too long
v0.12,This layout tries to mimic the graphviz layout in a simpler form. The depth grows horizontally here instead of
v0.12,vertically.
v0.12,Set the figure size based on the number of nodes
v0.12,"Nodes that are vertically connected, but not neighbors should be connected via a curved edge."
v0.12,All other nodes should be connected with a straight line.
v0.12,Draw labels node labels
v0.12,"Each node gets a depth assigned, based on the distance to the closest root node."
v0.12,"In case of undirected graphs, we just take any node as root node."
v0.12,"No path to root node, ignore this connection then."
v0.12,Counts the number of vertical nodes in the same layers.
v0.12,Creates a matrix indicating whether two nodes are vertical neighbors.
v0.12,Get all y coordinates per layer
v0.12,Sort the y-coordinates
v0.12,Finding p-value using student T test
v0.12,Only consider edges have absolute edge weight > 0.01
v0.12,Modify graph such that it only contains bidirected edges
v0.12,Find c components by finding connected components on the undirected graph
v0.12,Understanding Neural Network weights
v0.12,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.12,add weight column
v0.12,before weights are applied we count number rows in each category
v0.12,which is equivalent to summing over weight=1
v0.12,after weights are applied we need to sum over the given weights
v0.12,"First, calculating mean differences by strata"
v0.12,"Second, without strata"
v0.12,"Third, concatenating them and plotting"
v0.12,Setting estimator attribute for convenience
v0.12,Outcome is numeric
v0.12,Treatments are also numeric or binary
v0.12,Outcome is categorical
v0.12,Treatments are numeric or binary
v0.12,TODO: A common way to show all plots
v0.12,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.12,Get adjacency list
v0.12,If node pair has been fully explored
v0.12,Add node1 to backdoor set of node_pair
v0.12,Check if path is backdoor and does not have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.12,"True if arrow incoming, False if arrow outgoing"
v0.12,"Mark pair (node1, node2) complete"
v0.12,Modify variable count and indices covered
v0.12,Average total effect
v0.12,Natural direct effect
v0.12,Natural indirect effect
v0.12,Controlled direct effect
v0.12,Backdoor method names
v0.12,"First, check if there is a directed path from action to outcome"
v0.12,## 1. BACKDOOR IDENTIFICATION
v0.12,Pick algorithm to compute backdoor sets according to method chosen
v0.12,"First, checking if there are any valid backdoor adjustment sets"
v0.12,"Setting default ""backdoor"" identification adjustment set"
v0.12,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.12,Now checking if there is also a valid iv estimand
v0.12,## 3. FRONTDOOR IDENTIFICATION
v0.12,Now checking if there is a valid frontdoor variable
v0.12,Finally returning the estimand object
v0.12,Pick algorithm to compute backdoor sets according to method chosen
v0.12,"Setting default ""backdoor"" identification adjustment set"
v0.12,Finally returning the estimand object
v0.12,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.12,"First, checking if there are any valid backdoor adjustment sets"
v0.12,"Setting default ""backdoor"" identification adjustment set"
v0.12,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.12,Now checking if there are valid mediator variables
v0.12,Finally returning the estimand object
v0.12,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.12,"First, checking if there are any valid backdoor adjustment sets"
v0.12,"Setting default ""backdoor"" identification adjustment set"
v0.12,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.12,Now checking if there are valid mediator variables
v0.12,Finally returning the estimand object
v0.12,"First, checking if empty set is a valid backdoor set"
v0.12,"If the method is `minimal-adjustment`, return the empty set right away."
v0.12,"Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible."
v0.12,only remove descendants of Y
v0.12,also allow any causes of Y that are not caused by T (for lower variance)
v0.12,remove descendants of T (mediators) and descendants of Y
v0.12,"If var is d-separated from both treatment or outcome, it cannot"
v0.12,be a part of the backdoor set
v0.12,repeat the above search with BACKDOOR_MIN
v0.12,"If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest."
v0.12,"If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set."
v0.12,"If all variables are observed, and the biggest eligible set"
v0.12,"does not satisfy backdoor, then none of its subsets will."
v0.12,Adding a None estimand if no backdoor set found
v0.12,"Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable."
v0.12,Default set is the one with the least number of adjustment variables (optimizing for efficiency)
v0.12,Cond 1: All directed paths intercepted by candidate_var
v0.12,Cond 2: No confounding between treatment and candidate var
v0.12,Cond 3: treatment blocks all confounding between candidate_var and outcome
v0.12,"For simplicity, assuming a one-variable mediation set"
v0.12,"Create estimands dict as per the API for backdoor, but do not return it"
v0.12,"Setting default ""backdoor"" identification adjustment set"
v0.12,"Create estimands dict as per the API for backdoor, but do not return it"
v0.12,"Setting default ""backdoor"" identification adjustment set"
v0.12,"TODO: outputs string for now, but ideally should do symbolic"
v0.12,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.12,TODO Better support for multivariate treatments
v0.12,TODO: support multivariate treatments better.
v0.12,TODO: support multivariate treatments better.
v0.12,TODO: support multivariate treatments better.
v0.12,For direct effect
v0.12,"If no costs are passed, use uniform costs"
v0.12,restriction to ancestors
v0.12,back-door graph
v0.12,moralization
v0.12,Estimators list for returning after identification
v0.12,Line 1
v0.12,"If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y."
v0.12,Line 2
v0.12,"If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y."
v0.12,Modify list of valid nodes
v0.12,Line 3 - forces an action on any node where such an action would have no effect on Y – assuming we already acted on X.
v0.12,Modify adjacency matrix to obtain that corresponding to do(X)
v0.12,Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.
v0.12,"If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases."
v0.12,Modify adjacency matrix to remove treatment variables
v0.12,"Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes."
v0.12,"Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem."
v0.12,Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.
v0.12,Do not show backdoor key unless it is the only backdoor set.
v0.12,Just show the default backdoor set
v0.12,Ensure parent_time_lag is in tuple form
v0.12,Find or create the lagged node for the current node
v0.12,"For each lagged node, create new time-lagged parent nodes and add edges"
v0.12,Add the parent to the queue for further exploration
v0.12,append the lagged nodes
v0.12,If labels provided
v0.12,Return in valid DOT format
v0.12,Get adjacency matrix
v0.12,If labels not provided
v0.12,Obtain valid DOT format
v0.12,If labels provided
v0.12,Return in valid DOT format
v0.12,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.12,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11.1,-*- coding: utf-8 -*-
v0.11.1,
v0.11.1,Configuration file for the Sphinx documentation builder.
v0.11.1,
v0.11.1,This file does only contain a selection of the most common options. For a
v0.11.1,full list see the documentation:
v0.11.1,http://www.sphinx-doc.org/en/stable/config
v0.11.1,-- Path setup --------------------------------------------------------------
v0.11.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.11.1,add these directories to sys.path here. If the directory is relative to the
v0.11.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.11.1,
v0.11.1,-- Project information -----------------------------------------------------
v0.11.1,Version Information (for version-switcher)
v0.11.1,-- General configuration ---------------------------------------------------
v0.11.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.11.1,
v0.11.1,needs_sphinx = '1.0'
v0.11.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.11.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.11.1,ones.
v0.11.1,sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme
v0.11.1,already loads it
v0.11.1,"Add any paths that contain templates here, relative to this directory."
v0.11.1,The suffix(es) of source filenames.
v0.11.1,You can specify multiple suffix as a list of string:
v0.11.1,
v0.11.1,"source_suffix = ['.rst', '.md']"
v0.11.1,The master toctree document.
v0.11.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.11.1,for a list of supported languages.
v0.11.1,
v0.11.1,This is also used if you do content translation via gettext catalogs.
v0.11.1,"Usually you set ""language"" from the command line for these cases."
v0.11.1,"List of patterns, relative to source directory, that match files and"
v0.11.1,directories to ignore when looking for source files.
v0.11.1,This pattern also affects html_static_path and html_extra_path .
v0.11.1,The name of the Pygments (syntax highlighting) style to use.
v0.11.1,-- Options for HTML output -------------------------------------------------
v0.11.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.11.1,a list of builtin themes.
v0.11.1,
v0.11.1,Theme options are theme-specific and customize the look and feel of a theme
v0.11.1,"further.  For a list of options available for each theme, see the"
v0.11.1,documentation.
v0.11.1,
v0.11.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.11.1,"relative to this directory. They are copied after the builtin static files,"
v0.11.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.11.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.11.1,to template names.
v0.11.1,
v0.11.1,The default sidebars (for documents that don't match any pattern) are
v0.11.1,defined by theme itself.  Builtin themes are using these templates by
v0.11.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.11.1,'searchbox.html']``.
v0.11.1,
v0.11.1,html_sidebars = {}
v0.11.1,-- Options for HTMLHelp output ---------------------------------------------
v0.11.1,Output file base name for HTML help builder.
v0.11.1,-- Options for LaTeX output ------------------------------------------------
v0.11.1,The paper size ('letterpaper' or 'a4paper').
v0.11.1,
v0.11.1,"'papersize': 'letterpaper',"
v0.11.1,"The font size ('10pt', '11pt' or '12pt')."
v0.11.1,
v0.11.1,"'pointsize': '10pt',"
v0.11.1,Additional stuff for the LaTeX preamble.
v0.11.1,
v0.11.1,"'preamble': '',"
v0.11.1,Latex figure (float) alignment
v0.11.1,
v0.11.1,"'figure_align': 'htbp',"
v0.11.1,Grouping the document tree into LaTeX files. List of tuples
v0.11.1,"(source start file, target name, title,"
v0.11.1,"author, documentclass [howto, manual, or own class])."
v0.11.1,-- Options for manual page output ------------------------------------------
v0.11.1,One entry per manual page. List of tuples
v0.11.1,"(source start file, name, description, authors, manual section)."
v0.11.1,-- Options for Texinfo output ----------------------------------------------
v0.11.1,Grouping the document tree into Texinfo files. List of tuples
v0.11.1,"(source start file, target name, title, author,"
v0.11.1,"dir menu entry, description, category)"
v0.11.1,-- Options for Epub output -------------------------------------------------
v0.11.1,Bibliographic Dublin Core info.
v0.11.1,The unique identifier of the text. This can be a ISBN number
v0.11.1,or the project homepage.
v0.11.1,
v0.11.1,epub_identifier = ''
v0.11.1,A unique identification for the text.
v0.11.1,
v0.11.1,epub_uid = ''
v0.11.1,A list of files that should not be packed into the epub file.
v0.11.1,-- Extension configuration -------------------------------------------------
v0.11.1,-- Options for todo extension ----------------------------------------------
v0.11.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.11.1,init docstrings should also be included in class
v0.11.1,Only uncomment for faster testing/building docs without compiling notebooks
v0.11.1,"nbsphinx_execute = ""never"""
v0.11.1,Patch all of the published versions
v0.11.1,check old RST version (<= v0.8)
v0.11.1,Remove old version links
v0.11.1,Append updated version links
v0.11.1,requires stdin input for identify in weighting sampler
v0.11.1,will be removed
v0.11.1,"applied notebook, not necessary to test each time"
v0.11.1,needs xgboost too
v0.11.1,Slow Notebooks
v0.11.1,"TODO: should probably move more notebooks here to ignore, because"
v0.11.1,most get tested by the documentation generation.
v0.11.1,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.11.1,can import dowhy
v0.11.1,"""--ExecutePreprocessor.timeout=600"","
v0.11.1,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.11.1,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.11.1,treated = self.df[self.df['z']==1]
v0.11.1,self.att = np.mean(treated['y1'] - treated['y0'])
v0.11.1,def test_average_treatment_effect(self):
v0.11.1,est_ate = 1
v0.11.1,bias = est_ate - self.ate
v0.11.1,print(bias)
v0.11.1,"self.assertAlmostEqual(self.ate, est_ate)"
v0.11.1,def test_average_treatment_effect_on_treated(self):
v0.11.1,est_att = 1
v0.11.1,self.att=1
v0.11.1,bias = est_att - self.att
v0.11.1,print(bias)
v0.11.1,"self.assertAlmostEqual(self.att, est_att)"
v0.11.1,removing two common causes
v0.11.1,removing two common causes
v0.11.1,removing two common causes
v0.11.1,removing two common causes
v0.11.1,removing two common causes
v0.11.1,"Remove graph variable with name ""W0"" from observed data."
v0.11.1,Ensure that a log record exists that provides a more detailed view
v0.11.1,of observed and unobserved graph variables (counts and variable names.)
v0.11.1,"Default == operator tests if same object. If same object, don't need to check type."
v0.11.1,"num_frontdoor_variables=1,"
v0.11.1,creating nx graph instance
v0.11.1,to be used later for a test. Does not include the replace operation
v0.11.1,check if all partial R^2 values are between 0 and 1
v0.11.1,We calculate adjusted estimates for two sets of partial R^2 values.
v0.11.1,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.11.1,Creating a model with no unobserved confounders
v0.11.1,check if all partial R^2 values are between 0 and 1
v0.11.1,"for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)"
v0.11.1,Non Parametric estimator
v0.11.1,We calculate adjusted estimates for two sets of partial R^2 values.
v0.11.1,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11.1,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11.1,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11.1,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11.1,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11.1,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11.1,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11.1,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11.1,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.11.1,we patched figure plotting call to avoid drawing plots during tests
v0.11.1,We calculate adjusted estimates for two sets of partial R^2 values.
v0.11.1,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11.1,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11.1,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11.1,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11.1,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11.1,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11.1,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11.1,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11.1,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.11.1,we patched figure plotting call to avoid drawing plots during tests
v0.11.1,comparing test examples from R E-Value package
v0.11.1,check implementation of Observed Covariate E-value against R package
v0.11.1,The outcome is a linear function of the confounder
v0.11.1,"The slope is 1,2 and the intercept is 3"
v0.11.1,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.11.1,"TODO: Check directly for correct behavior, rather than checking the rules"
v0.11.1,"themselves, which can be non-deterministic (all the following are equivalent)"
v0.11.1,Supports user-provided dataset object
v0.11.1,To test if there are any exceptions
v0.11.1,To test if the estimate is identical if refutation parameters are zero
v0.11.1,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.11.1,"Ordinarily, we should expect this value to be zero."
v0.11.1,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.11.1,"Ordinarily, we should expect this value to be zero."
v0.11.1,Only P(Y|T) should be present for test to succeed.
v0.11.1,"Since undirected graph, identify effect must throw an error."
v0.11.1,Compare with ground truth
v0.11.1,Compare with ground truth
v0.11.1,Compare with ground truth
v0.11.1,Compare with ground truth
v0.11.1,Causal model initialization
v0.11.1,Obtain backdoor sets
v0.11.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.11.1,Causal model initialization
v0.11.1,Obtain backdoor sets
v0.11.1,Causal model initialization
v0.11.1,Obtain backdoor sets
v0.11.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.11.1,Causal model initialization
v0.11.1,Obtain backdoor sets
v0.11.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.11.1,Causal model initialization
v0.11.1,Obtain backdoor sets
v0.11.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.11.1,Causal model initialization
v0.11.1,Obtain backdoor sets
v0.11.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.11.1,"Example is selected from Pearl J. ""Causality"" 2nd Edition, from chapter 3.3.1 on backoor criterion."
v0.11.1,"The following simpsons paradox examples are taken from Pearl, J {2013}. ""Understanding Simpson’s Paradox"" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf"
v0.11.1,"The following are examples given in the ""Book of Why"" by Judea Pearl, chapter ""The Do-operator and the Back-Door Criterion"""
v0.11.1,Building the causal model
v0.11.1,For all examples from these papers we use X for the treatment variable
v0.11.1,instead of A.
v0.11.1,"Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.11.1,"Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.11.1,"Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.11.1,"Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.11.1,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.11.1,L replaces X as the conditional variable
v0.11.1,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.11.1,L replaces X as the conditional variable. Uses different costs
v0.11.1,"Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.11.1,"A graph where optimal, optimal minimal and optimal min cost are different"
v0.11.1,The graph from Shrier and Platt (2008)
v0.11.1,A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.11.1,Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.11.1,cov_mat = np.diag(np.ones(num_features))
v0.11.1,collider: X->Z<-Y
v0.11.1,chain: X->Z->Y
v0.11.1,fork: X<-Z->Y
v0.11.1,"general DAG: X<-Z->Y, X->Y"
v0.11.1,fork: X<-Z->Y
v0.11.1,Only Gaussian component changed
v0.11.1,"Just checking formats, i.e. no need for correlation."
v0.11.1,"Just checking formats, i.e. no need for correlation."
v0.11.1,The MSE should be 1 due to the variance of the noise. The RMSE is accordingly 1 / var(Y).
v0.11.1,Contributions should add up to Var(X2)
v0.11.1,H(P(Y)) -- Can be precomputed
v0.11.1,-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))
v0.11.1,"H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\S)])"
v0.11.1,"E[P(Y | x_S, X'_\S)]"
v0.11.1,"H(E[P(Y | x_S, X'_\S)])"
v0.11.1,"Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions."
v0.11.1,E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]
v0.11.1,Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3
v0.11.1,Having at least one sample from the second class should not raise an error.
v0.11.1,"Just some random data, since we are only interested in the omitted data."
v0.11.1,This caused an error before with pandas > 2.0
v0.11.1,C2 = 3 * A2 + 2 * B2
v0.11.1,"By default, the strength is measure with respect to the variance."
v0.11.1,"Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2."
v0.11.1,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.11.1,Missing connection between X0 and X1.
v0.11.1,"For X0 and X1, we set the ground truth noise to further emphasize the misspecification. The inferred noise of X1"
v0.11.1,would otherwise have a dependency with Z due to the missing connection with X0.
v0.11.1,Modelling connection between X0 and X1 explicitly.
v0.11.1,"Here, we misspecify the mechanism on purpose by setting scale to 1 instead of 2."
v0.11.1,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.11.1,"Here, changing the mechanism."
v0.11.1,"Here, changing the mechanism."
v0.11.1,"Here, changing the mechanism."
v0.11.1,"Here, changing the mechanism."
v0.11.1,Defining an anomaly scorer that handles multidimensional inputs.
v0.11.1,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.11.1,"In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution"
v0.11.1,"and it should be ""significantly"" higher than the contribution of the other ones (here, we just arbitrarily say"
v0.11.1,"it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,"
v0.11.1,"which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate"
v0.11.1,"some robustness. Note that due to this and stochastic behaviour of the density estimator, it is"
v0.11.1,"not possible to analytically compute expected results. Therefore, we rather look at the relations here."
v0.11.1,"Same idea for the second sample, but here, it is the second variable that is anomalous."
v0.11.1,"In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be"
v0.11.1,"""significantly"" higher than the contribution of the other variables. The contribution of both anomalous variables"
v0.11.1,should be equal (approximately).
v0.11.1,Defining an anomaly scorer that handles multidimensional inputs.
v0.11.1,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.11.1,reduce the score.
v0.11.1,The contributions should add up to g(x) - E[g(X)]
v0.11.1,The contributions should add up to g(x) - E[g(X)]
v0.11.1,The contributions should add up to g(x) - E[g(X)]
v0.11.1,Three examples:
v0.11.1,1. X1 is the root cause (+ 10 to the noise)
v0.11.1,2. X0 is the root cause (+ 10 to the noise)
v0.11.1,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.11.1,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.11.1,Three examples:
v0.11.1,1. X1 is the root cause (+ 10 to the noise)
v0.11.1,2. X0 is the root cause (+ 10 to the noise)
v0.11.1,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.11.1,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.11.1,"Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution"
v0.11.1,algorithm.
v0.11.1,1. X0 is the root cause (+ 10 to the noise)
v0.11.1,2. X0 and X1 are the root causes (+ 10 to both noise)
v0.11.1,3. X2 and X3 are both root causes (+ 10 to both noises)
v0.11.1,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.11.1,collider: X->Z<-Y
v0.11.1,collider: X->Z<-Y
v0.11.1,chain: X->Y->Z
v0.11.1,chain: X->Y->Z
v0.11.1,Empty graph
v0.11.1,Full DAG
v0.11.1,DAG with single node
v0.11.1,DAG with single edge
v0.11.1,DAG with single edge
v0.11.1,chain: X->Z->Y
v0.11.1,Setup data
v0.11.1,Test LinearDML
v0.11.1,Checking that the CATE estimates are not identical
v0.11.1,Test ContinuousTreatmentOrthoForest
v0.11.1,Checking that the CATE estimates are not identical
v0.11.1,Test LinearDRLearner
v0.11.1,Test LinearDML
v0.11.1,checking that CATE estimates are not identical
v0.11.1,predict on new data
v0.11.1,Setup data
v0.11.1,Test DeepIV
v0.11.1,Test IntentToTreatDRIV
v0.11.1,Observed data
v0.11.1,assumed graph
v0.11.1,Identify effect
v0.11.1,Estimate effect
v0.11.1,A model where X is also a common cause
v0.11.1,A model where X is also a common cause
v0.11.1,The case where effect modifier is not a common cause
v0.11.1,A model where X is also a common cause
v0.11.1,Create the graph describing the causal structure
v0.11.1,Generate the data
v0.11.1,Data to df
v0.11.1,Create a model
v0.11.1,Estimate the effect with front-door
v0.11.1,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.11.1,More cases where Exception  is expected
v0.11.1,"Compute confidence intervals, standard error and significance tests"
v0.11.1,Defined a linear dataset with a given set of properties
v0.11.1,Create a model that captures the same
v0.11.1,Identify the effects within the model
v0.11.1,Defined a linear dataset with a given set of properties
v0.11.1,Create a model that captures the same
v0.11.1,Identify the effects within the model
v0.11.1,Defined a linear dataset with a given set of properties
v0.11.1,Create a model that captures the same
v0.11.1,Identify the effects within the model
v0.11.1,Defined a linear dataset with a given set of properties
v0.11.1,Create a model that captures the same
v0.11.1,Identify the effects within the model
v0.11.1,Defined a linear dataset with a given set of properties
v0.11.1,Create a model that captures the same
v0.11.1,Identify the effects within the model
v0.11.1,Use a mix of already-numeric and requires encoding cols:
v0.11.1,"NB There may be small differences in type but since all values will be used in models as float,"
v0.11.1,comparison is done as this type.
v0.11.1,Check same rows
v0.11.1,Check same number of cols
v0.11.1,Check values
v0.11.1,Calculate the sum of absolute differences between the two DataFrames
v0.11.1,- should be zero (excl. floating point error)
v0.11.1,Use a mix of already-numeric and requires encoding cols:
v0.11.1,Initial encode
v0.11.1,Create new data with permuted rows.
v0.11.1,Output shape should be unchanged.
v0.11.1,Encode this new data.
v0.11.1,Check same rows
v0.11.1,Check same number of cols
v0.11.1,Check permuted values are consistent
v0.11.1,Check if calling the method causes some import or runtime errors
v0.11.1,TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer
v0.11.1,newer matplotlib version:
v0.11.1,AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'
v0.11.1,Networkx 2.4+ should fix this issue.
v0.11.1,"plot_adjacency_matrix(causal_graph, is_directed=False)"
v0.11.1,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.11.1,Unpacking the keyword arguments
v0.11.1,todo: add docstring for common parameters here and remove from child refuter classes
v0.11.1,Default value for the number of simulations to be conducted
v0.11.1,joblib params for parallel processing
v0.11.1,"Concatenate the confounders, instruments and effect modifiers"
v0.11.1,Shuffle the confounders
v0.11.1,Check if all are select or deselect variables
v0.11.1,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.11.1,This calculates a two-sided percentile p-value
v0.11.1,See footnotes in https://journals.sagepub.com/doi/full/10.1177/2515245920911881
v0.11.1,Get the mean for the simulations
v0.11.1,Get the standard deviation for the simulations
v0.11.1,Get the Z Score [(val - mean)/ std_dev ]
v0.11.1,Initializing the p_value
v0.11.1,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.11.1,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.11.1,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.11.1,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.11.1,re.sub only takes string parameter so the first if is to avoid error
v0.11.1,"if the input is a text file, convert the contained data into string"
v0.11.1,load dot file
v0.11.1,TODO This functionality needs to be deprecated. It is a convenience function but can introduce confusion
v0.11.1,as we are now including the option to initialize CausalGraph with DiGraph or GCM model.
v0.11.1,Adding node attributes
v0.11.1,adding penwidth to make the edge bold
v0.11.1,Adding common causes
v0.11.1,Adding instruments
v0.11.1,Adding effect modifiers
v0.11.1,Assuming the simple form of effect modifier
v0.11.1,that directly causes the outcome.
v0.11.1,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.11.1,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.11.1,Adding columns in the dataframe as confounders that were not in the graph
v0.11.1,Adding unobserved confounders
v0.11.1,removal of only direct edges wrt a target is not implemented for incoming edges
v0.11.1,also return the number of backdoor paths blocked by observed nodes
v0.11.1,Assume that nodes1 is the treatment
v0.11.1,"ignores new_graph parameter, always uses self._graph"
v0.11.1,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.11.1,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.11.1,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.11.1,Return effect modifiers according to the graph
v0.11.1,removing all mediators
v0.11.1,"Also add any effect modifiers that could not be auto-detected (e.g., they are also common causes)"
v0.11.1,[TODO: double check these work with multivariate implementation:]
v0.11.1,Exclusion
v0.11.1,As-if-random setup
v0.11.1,As-if-random
v0.11.1,convert the outputted generator into a list
v0.11.1,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.11.1,return len(dpaths) > 0
v0.11.1,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.11.1,Emit a `UserWarning` if there are any unobserved graph variables and
v0.11.1,and log a message highlighting data variables that are not part of the graph.
v0.11.1,Create causal graph object
v0.11.1,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.11.1,(Because some effect modifiers may also be common causes)
v0.11.1,"In such cases, the user-provided modifiers are used."
v0.11.1,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.11.1,Import causal discovery class
v0.11.1,Initialize causal graph object
v0.11.1,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.11.1,TODO add dowhy as a prefix to all dowhy estimators
v0.11.1,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.11.1,Define the third-party estimation method to be used
v0.11.1,Process the dowhy estimators
v0.11.1,"If not fit_estimator, attempt to retrieve existing estimator."
v0.11.1,Keep original behaviour to create new estimator if none found.
v0.11.1,Check if estimator's target estimand is identified
v0.11.1,"Note that while the name of the variable is the same,"
v0.11.1,"""self.causal_estimator"", this estimator takes in less"
v0.11.1,parameters than the same from the
v0.11.1,estimate_effect code. It is not advisable to use the
v0.11.1,estimator from this function to call estimate_effect
v0.11.1,with fit_estimator=False.
v0.11.1,Estimator had been computed in a previous call
v0.11.1,The default number of simulations for statistical testing
v0.11.1,The default number of simulations to obtain confidence intervals
v0.11.1,This should be at least 399 for a 5% error rate:
v0.11.1,https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf
v0.11.1,The portion of the total size that should be taken each time to find the confidence intervals
v0.11.1,1 is the recommended value
v0.11.1,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.11.1,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.11.1,The default Confidence Level
v0.11.1,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.11.1,Prefix to add to temporary categorical variables created after discretization
v0.11.1,std args to be removed from locals() before being passed to args_dict
v0.11.1,Setting the default interpret method
v0.11.1,"Check if some parameters were set, otherwise set to default values"
v0.11.1,Estimate conditional estimates by default
v0.11.1,TODO Only works for binary treatment
v0.11.1,Defaulting to class default values if parameters are not provided
v0.11.1,Checking that there is at least one effect modifier
v0.11.1,Making sure that effect_modifier_names is a list
v0.11.1,Making a copy since we are going to be changing effect modifier names
v0.11.1,"For every numeric effect modifier, adding a temp categorical column"
v0.11.1,Grouping by effect modifiers and computing effect separately
v0.11.1,Deleting the temporary categorical columns
v0.11.1,The array that stores the results of all estimations
v0.11.1,Find the sample size the proportion with the population size
v0.11.1,Perform the set number of simulations
v0.11.1,names of treatment and outcome
v0.11.1,Using class default parameters if not specified
v0.11.1,Checking if bootstrap_estimates are already computed
v0.11.1,Checked if any parameter is changed from the previous std error estimate
v0.11.1,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.11.1,Get the variations of each bootstrap estimate and sort
v0.11.1,"Now we take the (1-p)/2 th and the 1-(1-p)/2 th variations, where p is the chosen confidence level"
v0.11.1,Get the lower and upper bounds by subtracting the variations from the estimate
v0.11.1,"Use existing params, if new user defined params are not present"
v0.11.1,Checking if bootstrap_estimates are already computed
v0.11.1,Check if any parameter is changed from the previous std error estimate
v0.11.1,"Use existing params, if new user defined params are not present"
v0.11.1,Processing the null hypothesis estimates
v0.11.1,Doing a two-sided test
v0.11.1,Being conservative with the p-value reported
v0.11.1,Being conservative with the p-value reported
v0.11.1,"If the estimate_index is 0, it depends on the number of simulations"
v0.11.1,Need to test r-squared before supporting
v0.11.1,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.11.1,'r-squared': effect_r_squared
v0.11.1,"elif method == ""r-squared"":"
v0.11.1,outcome_mean = np.mean(self._outcome)
v0.11.1,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.11.1,Assuming a linear model with one variable: the treatment
v0.11.1,Currently only works for continuous y
v0.11.1,causal_model = outcome_mean + estimate.value*self._treatment
v0.11.1,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.11.1,r_squared = 1 - (squared_residual/total_variance)
v0.11.1,return r_squared
v0.11.1,Check if estimator's target estimand is identified
v0.11.1,Store parameters inside estimate object for refutation methods
v0.11.1,TODO: This add_params needs to move to the estimator class
v0.11.1,inside estimate_effect and estimate_conditional_effect
v0.11.1,"TODO: Remove _data, _treatment_name and _outcome_name from this object"
v0.11.1,we save them here to enable the methods that required these properties saved in the estimator
v0.11.1,eventually we should call those methods and just save the results in this object
v0.11.1,instead of having this object invoke the estimator methods with the data.
v0.11.1,No estimand was identified (identification failed)
v0.11.1,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.11.1,"can't use nx.node_connected_component, because it doesn't work with DiGraphs."
v0.11.1,"Hence, a manual loop:"
v0.11.1,also return the number of backdoor paths blocked by observed nodes
v0.11.1,Assume that nodes1 is the treatment
v0.11.1,"ignores new_graph parameter, always uses self._graph"
v0.11.1,removal of only direct edges wrt a target is not implemented for incoming edges
v0.11.1,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.11.1,[TODO: double check these work with multivariate implementation:]
v0.11.1,Exclusion
v0.11.1,As-if-random setup
v0.11.1,As-if-random
v0.11.1,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.11.1,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.11.1,return len(dpaths) > 0
v0.11.1,Adding common causes
v0.11.1,Adding instruments
v0.11.1,Adding effect modifiers
v0.11.1,Assuming the simple form of effect modifier
v0.11.1,that directly causes the outcome.
v0.11.1,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.11.1,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.11.1,some preprocessing steps
v0.11.1,parsing the correct graph based on input graph format
v0.11.1,load dot file
v0.11.1,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.11.1,one-hot encode discrete W
v0.11.1,Now deleting the old continuous value
v0.11.1,create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause
v0.11.1,"Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately"
v0.11.1,One Hot Encoding
v0.11.1,TODO Ensure that we do not generate weak instruments
v0.11.1,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.11.1,Converting treatment to binary if required
v0.11.1,Generating frontdoor variables if asked for
v0.11.1,NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.
v0.11.1,"For categorical t, this example dataset has the effect size for category 2 being exactly"
v0.11.1,double the effect for category 1
v0.11.1,This could be changed at this stage by one-hot encoding t and using a custom beta that
v0.11.1,"sets a different effect for each category {0, 1, 2}"
v0.11.1,Computing ATE
v0.11.1,constructing column names for one-hot encoded discrete features
v0.11.1,Specifying the correct dtypes
v0.11.1,Now specifying the corresponding graph strings
v0.11.1,Now writing the gml graph
v0.11.1,creating data frame
v0.11.1,Specifying the correct dtypes
v0.11.1,Now specifying the corresponding graph strings
v0.11.1,Now writing the gml graph
v0.11.1,Adding edges between common causes and the frontdoor mediator
v0.11.1,Error terms
v0.11.1,else:
v0.11.1,V = 6 + W0 + tterm + E1
v0.11.1,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.11.1,Generating a random normal distribution of integers
v0.11.1,Generating data for nodes which have no incoming edges
v0.11.1,"""currset"" variable currently has all the successors of the nodes which had no incoming edges"
v0.11.1,assuming that all unobserved common causes are numerical and are not affected by one hot encoding
v0.11.1,Creating a NN to simulate the nuisance function
v0.11.1,strength of unobserved confounding
v0.11.1,Computing ATE
v0.11.1,Specifying the correct dtypes
v0.11.1,Now writing the gml graph
v0.11.1,The following code for loading the Lalonde dataset was copied from
v0.11.1,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.11.1,
v0.11.1,"Copyright 2018, Wayfair, Inc."
v0.11.1,
v0.11.1,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.11.1,the following conditions are met:
v0.11.1,
v0.11.1,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.11.1,following disclaimer.
v0.11.1,
v0.11.1,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.11.1,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.11.1,
v0.11.1,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.11.1,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.11.1,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.11.1,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.11.1,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.11.1,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.11.1,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.11.1,DAMAGE.
v0.11.1,The following code is a slight modification of
v0.11.1,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.11.1,
v0.11.1,"Copyright 2018, Wayfair, Inc."
v0.11.1,
v0.11.1,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.11.1,the following conditions are met:
v0.11.1,
v0.11.1,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.11.1,following disclaimer.
v0.11.1,
v0.11.1,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.11.1,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.11.1,
v0.11.1,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.11.1,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.11.1,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.11.1,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.11.1,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.11.1,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.11.1,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.11.1,DAMAGE.
v0.11.1,
v0.11.1,0.0.0 is standard placeholder for poetry-dynamic-versioning
v0.11.1,any changes to this should not be checked in
v0.11.1,
v0.11.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11.1,adapt number of channels
v0.11.1,save memory
v0.11.1,Keep same dimensions
v0.11.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11.1,single-attribute Causal
v0.11.1,test environment
v0.11.1,Subsample 2x for computational convenience
v0.11.1,Assign a binary label based on the digit
v0.11.1,Flip label with probability 0.25
v0.11.1,Assign a color based on the label; flip the color with probability environment
v0.11.1,Apply the color to the image by zeroing out the other color channel
v0.11.1,single-attribute Independent
v0.11.1,test environment
v0.11.1,Subsample 2x for computational convenience
v0.11.1,Assign a binary label based on the digit
v0.11.1,Flip label with probability 0.25
v0.11.1,multi-attribute Causal + Independent
v0.11.1,test environment
v0.11.1,Subsample 2x for computational convenience
v0.11.1,rotate the image by angle in parameter
v0.11.1,Assign a binary label based on the digit
v0.11.1,Flip label with probability 0.25
v0.11.1,Assign a color based on the label; flip the color with probability environment
v0.11.1,Apply the color to the image by zeroing out the other color channel
v0.11.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11.1,Acause regularization
v0.11.1,Aconf regularization
v0.11.1,Aind regularization
v0.11.1,Asel regularization
v0.11.1,Compile loss
v0.11.1,Check if the optimizer is currently supported
v0.11.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11.1,The currently supported estimators
v0.11.1,The default standard deviation for noise
v0.11.1,The default scaling factor to determine the bucket size
v0.11.1,The minimum number of points for the estimator to run
v0.11.1,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.11.1,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.11.1,The Default split for the number of data points that fall into the training and validation sets
v0.11.1,Assuming that outcome is one-dimensional
v0.11.1,We need to change the identified estimand
v0.11.1,"We thus, make a copy. This is done as we don't want"
v0.11.1,to change the original DataFrame
v0.11.1,We use collections.OrderedDict to maintain the order in which the data is stored
v0.11.1,Check if we are using an estimator in the transformation list
v0.11.1,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.11.1,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.11.1,"loops. Thus, we can get different values everytime we get the estimator."
v0.11.1,for _ in range( self._num_simulations ):
v0.11.1,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.11.1,Adding an unobserved confounder if provided by the user
v0.11.1,We set X_train = 0 and outcome_train to be 0
v0.11.1,"Get the final outcome, after running through all the values in the transformation list"
v0.11.1,Check if the value of true effect has been already stored
v0.11.1,We use None as the key as we have no base category for this refutation
v0.11.1,As we currently support only one treatment
v0.11.1,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.11.1,Check if the value of true effect has been already stored
v0.11.1,This ensures that we calculate the causal effect only once.
v0.11.1,We use key_train as we map data with respect to the base category of the data
v0.11.1,As we currently support only one treatment
v0.11.1,Add h(t) to f(W) to get the dummy outcome
v0.11.1,We convert to ndarray for ease in indexing
v0.11.1,The data is of the form
v0.11.1,sim1: cat1 cat2 ... catn
v0.11.1,sim2: cat1 cat2 ... catn
v0.11.1,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.11.1,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.11.1,distribution of the refuter.
v0.11.1,True Causal Effect list
v0.11.1,Iterating through the refutation for each category
v0.11.1,We use string arguments to account for both 32 and 64 bit varaibles
v0.11.1,action for continuous variables
v0.11.1,Action for categorical variables
v0.11.1,Find the set difference for each row
v0.11.1,Choose one out of the remaining
v0.11.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.11.1,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.11.1,of the treatment to affect the outcome
v0.11.1,TODO: Check that the target estimand has backdoor variables?
v0.11.1,Standardizing the data
v0.11.1,Fit a model containing all confounders and compare predictions
v0.11.1,using all features compared to all features except a given
v0.11.1,confounder.
v0.11.1,Estimating the regression coefficient from standardized features to t
v0.11.1,"By default, return a plot with 10 points"
v0.11.1,consider 10 values of the effect of the unobserved confounder
v0.11.1,Standardizing the data
v0.11.1,Fit a model containing all confounders and compare predictions
v0.11.1,using all features compared to all features except a given
v0.11.1,confounder.
v0.11.1,"By default, return a plot with 10 points"
v0.11.1,consider 10 values of the effect of the unobserved confounder
v0.11.1,"By default, we add the effect of simulated confounder for treatment."
v0.11.1,But subtract it from outcome to create a negative correlation
v0.11.1,assuming that the original confounder's effect was positive on both.
v0.11.1,This is to remove the effect of the original confounder.
v0.11.1,"By default, we add the effect of simulated confounder for treatment."
v0.11.1,But subtract it from outcome to create a negative correlation
v0.11.1,assuming that the original confounder's effect was positive on both.
v0.11.1,This is to remove the effect of the original confounder.
v0.11.1,Obtaining the list of observed variables
v0.11.1,Taking a subset of the dataframe that has only observed variables
v0.11.1,Residuals from the outcome model obtained by fitting a linear model
v0.11.1,Residuals from the treatment model obtained by fitting a linear model
v0.11.1,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.11.1,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.11.1,Choosing a c_star based on the data.
v0.11.1,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.11.1,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.11.1,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.11.1,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.11.1,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.11.1,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.11.1,TODO (petergtz): This should introduce a Protocol defining the interface this returns instead of the concrete types
v0.11.1,"If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen"
v0.11.1,Get a 2D matrix of values
v0.11.1,"x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN"
v0.11.1,Store the values into the refute object
v0.11.1,Adding a label on the contour line for the original estimate
v0.11.1,Label every other level using strings
v0.11.1,Default value of the p value taken for the distribution
v0.11.1,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.11.1,Mean of the Normal Distribution
v0.11.1,Standard Deviation of the Normal Distribution
v0.11.1,Create a new column in the data by the name of placebo
v0.11.1,Sanity check the data
v0.11.1,only permute is supported for iv methods
v0.11.1,"For IV methods, the estimating_instrument_names should also be"
v0.11.1,changed. Create a copy to avoid modifying original object
v0.11.1,We need to change the identified estimand
v0.11.1,"We make a copy as a safety measure, we don't want to change the"
v0.11.1,original DataFrame
v0.11.1,Run refutation in parallel
v0.11.1,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.11.1,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.11.1,relationship between the treatment and the outcome.
v0.11.1,new estimator
v0.11.1,new effect estimate
v0.11.1,observed covariate E-value
v0.11.1,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.11.1,"if CI crosses null, set its E-value to 1"
v0.11.1,only report E-value for CI limit closer to null
v0.11.1,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.11.1,whether the DGP is assumed to be partially linear
v0.11.1,features are the observed confounders
v0.11.1,Now code for benchmarking using covariates begins
v0.11.1,R^2 of outcome with observed common causes and treatment
v0.11.1,R^2 of treatment with observed common causes
v0.11.1,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.11.1,Assuming that the difference in R2 is the same for wj and new unobserved confounder
v0.11.1,"for treatment,  Calpha is not a function of the partial R2. So we need a different assumption."
v0.11.1,Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder
v0.11.1,"(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment"
v0.11.1,wrt unobserved confounders in partial-linear models
v0.11.1,Do the support characterization
v0.11.1,Recover the samples that are in the support
v0.11.1,Assess overlap using propensity scores with cross-fitting
v0.11.1,Check if all supported units are considered to be in the overlap set
v0.11.1,"NOTE: The original paper implements both DNF and CNF rules, but for simplicity, this code only implements DNF rules"
v0.11.1,"For DNF rules, a sample is covered if *any* rule applies"
v0.11.1,whether the DGP is assumed to be partially linear
v0.11.1,can change this to allow default values that are same as the other parameter
v0.11.1,Strength of confounding that omitted variables generate in treatment regression
v0.11.1,computing the point estimate for the bounds
v0.11.1,common causes after removing the benchmark causes
v0.11.1,dataframe with treatment and observed common causes after removing benchmark causes
v0.11.1,R^2 of treatment with observed common causes removing benchmark causes
v0.11.1,return the variance of alpha_s
v0.11.1,R^2 of outcome with observed common causes and treatment after removing benchmark causes
v0.11.1,Obtaining theta_s (the obtained estimate)
v0.11.1,Creating numpy arrays
v0.11.1,Setting up cross-validation parameters
v0.11.1,"tuple of residuals from first stage estimation [0,1], and the confounders [2]"
v0.11.1,"We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation."
v0.11.1,Yres = Y - E[Y|W]
v0.11.1,E[Y|W] = f(x) + theta_s * E[T|W]
v0.11.1,Yres = Y - f(x) - theta_s * E[T|W]
v0.11.1,g(s) = theta_s * T + f(x)
v0.11.1,g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]
v0.11.1,g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]
v0.11.1,Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )
v0.11.1,Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres
v0.11.1,Y - g(s) = Yres - theta_s * Tres
v0.11.1,nu_2 is E[alpha_s^2]
v0.11.1,Now computing scores for finding the (1-a) confidence interval
v0.11.1,R^2 of treatment with observed common causes
v0.11.1,R^2 of outcome with treatment and observed common causes
v0.11.1,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.11.1,Partial R^2 of treatment after regressing over unobserved confounder and observed common causes
v0.11.1,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.11.1,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.11.1,Adding unadjusted point estimate
v0.11.1,Adding bounds to partial R^2 values for given strength of confounders
v0.11.1,Adding a new backdoor variable to the identified estimand
v0.11.1,Run refutation in parallel
v0.11.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.11.1,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.11.1,of the treatment to affect the outcome
v0.11.1,TODO: Sensitivity Analyzers excluded from list due to different return type
v0.11.1,Reject H0
v0.11.1,"a, b and c are all continuous variables"
v0.11.1,"a, b and c are all discrete variables"
v0.11.1,c is set of continuous and binary variables and
v0.11.1,1. either a and b is continuous and the other is binary
v0.11.1,2. both a and b are binary
v0.11.1,c is discrete and
v0.11.1,either a or b is continuous and the other is discrete
v0.11.1,a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information
v0.11.1,original_treatment_name: : stores original variable names for labelling
v0.11.1,common_causes_map : maps the original variable names to variable names in OLS regression
v0.11.1,benchmark_common_causes: stores variable names in terms of regression model variables
v0.11.1,original_benchmark_covariates: stores original variable names for labelling
v0.11.1,estimate: estimate of regression
v0.11.1,degree_of_freedom: degree of freedom of error in regression
v0.11.1,standard_error: standard error in regression
v0.11.1,t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.
v0.11.1,partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them
v0.11.1,"r2tu_w: partial R^2  of unobserved confounder ""u"" with treatment ""t"", after conditioning on observed covariates ""w"""
v0.11.1,"r2yu_tw: partial R^2  of unobserved confounder ""u"" with outcome ""y"", after conditioning on observed covariates ""w"" and treatment ""t"""
v0.11.1,"r2twj_w: partial R^2 of observed covariate wj with treatment ""t"", after conditioning on observed covariates ""w"" excluding wj"
v0.11.1,"r2ywj_tw:  partial R^2 of observed covariate wj with outcome ""y"", after conditioning on observed covariates ""w"" (excluding wj) and treatment ""t"""
v0.11.1,benchmarking_results: dataframe containing information about bounds and bias adjusted terms
v0.11.1,"stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic"
v0.11.1,partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment
v0.11.1,build a new regression model by considering treatment variables as outcome
v0.11.1,"r2twj_w is partial R^2 of covariate wj with treatment ""t"", after conditioning on covariates w(excluding wj)"
v0.11.1,"r2ywj_tw is partial R^2 of covariate wj with outcome ""y"", after conditioning on covariates w(excluding wj) and treatment ""t"""
v0.11.1,r2tu_w is the partial r^2 from regressing u on t after conditioning on w
v0.11.1,Compute bias adjusted terms
v0.11.1,Plotting the contour plot
v0.11.1,Adding contours
v0.11.1,Adding threshold contour line
v0.11.1,Adding unadjusted point estimate
v0.11.1,Adding bounds to partial R^2 values for given strength of confounders
v0.11.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11.1,The default subset of the data to be used
v0.11.1,Run refutation in parallel
v0.11.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.11.1,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.11.1,of the treatment to affect the outcome
v0.11.1,Parameters
v0.11.1,Bookkeeping
v0.11.1,Initialize estimators
v0.11.1,Convert to dataframe if not
v0.11.1,Format labels
v0.11.1,Sample from reference measure and construct features
v0.11.1,Add reference samples
v0.11.1,Binarize features (fit to data only)
v0.11.1,Fit estimator
v0.11.1,Store reference volume
v0.11.1,Construct features dataframe
v0.11.1,Construct features dataframe
v0.11.1,Iterate over columns
v0.11.1,"logging.info(""Using provided reference range for {}"".format(c))"
v0.11.1,number of unique values
v0.11.1,Constant column
v0.11.1,Binary column
v0.11.1,Ordinal column (seed = counter so not correlated)
v0.11.1,For get_params / set_params
v0.11.1,"Overlap / Support (y = +1), non-overlap (y = 0), and uniform background (y = -1) samples"
v0.11.1,"We should always have overlap samples, and either background or non-overlap samples"
v0.11.1,"This will throw an error if, for example, all samples are considered to"
v0.11.1,be in the overlap region
v0.11.1,"Initialize with empty and singleton conjunctions, i.e. X plus all-ones feature"
v0.11.1,Feature indicator and conjunction matrices
v0.11.1,Iteration counter
v0.11.1,Formulate master LP
v0.11.1,Variables
v0.11.1,Objective function (no penalty on empty conjunction)
v0.11.1,Constraints
v0.11.1,This gets activated for DNF
v0.11.1,Solve problem
v0.11.1,Extract dual variables
v0.11.1,Beam search for conjunctions with negative reduced cost
v0.11.1,Most negative reduced cost among current variables
v0.11.1,Negative reduced costs found
v0.11.1,Add to existing conjunctions
v0.11.1,Reformulate master LP
v0.11.1,Variables
v0.11.1,Objective function
v0.11.1,Constraints
v0.11.1,Solve problem
v0.11.1,Extract dual variables
v0.11.1,Beam search for conjunctions with negative reduced cost
v0.11.1,Most negative reduced cost among current variables
v0.11.1,"print('UB.min():', UB.min())"
v0.11.1,Save generated conjunctions and coefficients
v0.11.1,Restrict conjunctions to those used by LP
v0.11.1,"NOTE: This is a greedy approach, so it does not incorporate lambda0 explicitly"
v0.11.1,"Similarly, it will prefer a larger number of smaller rules if lambda1 is set"
v0.11.1,"to a larger value, because the incremental cost will be lower."
v0.11.1,Fraction of reference samples that each conjunction covers
v0.11.1,Regularization (for each conjunction)
v0.11.1,Positive samples newly covered (for each conjunction)
v0.11.1,Costs (for each conjunction)
v0.11.1,Zero out the rules and only take those which are used
v0.11.1,Small tolerance on comparisons
v0.11.1,This can be useful to break ties and favor larger values of xi
v0.11.1,Compute conjunctions of features
v0.11.1,Predict labels
v0.11.1,Use helper function
v0.11.1,Use helper function
v0.11.1,Lower bound specific to each singleton solution
v0.11.1,Initialize output
v0.11.1,Remove redundant rows by grouping by unique feature combinations and summing residual
v0.11.1,Initialize queue with root instance
v0.11.1,Separate data according to positive and negative residuals
v0.11.1,Iterate over increasing degree while queue is non-empty
v0.11.1,Initialize list of children to process
v0.11.1,Process instances in queue
v0.11.1,inst = instCurr[0]
v0.11.1,Evaluate all singleton solutions
v0.11.1,Best solutions that also improve on current output (allow for duplicate removal)
v0.11.1,Append to current output
v0.11.1,Remove duplicates
v0.11.1,Update output
v0.11.1,Compute lower bounds on higher-degree solutions
v0.11.1,Evaluate children using weighted average of their costs and LBs
v0.11.1,Best children with potential to improve on current output and current candidates (allow for duplicate removal)
v0.11.1,Iterate through best children
v0.11.1,"New ""zero"" solution"
v0.11.1,Check if duplicate
v0.11.1,Add to candidates for further processing
v0.11.1,Create pricing instance
v0.11.1,Remove covered rows
v0.11.1,Remove redundant features
v0.11.1,Track number of candidates added
v0.11.1,Update candidates
v0.11.1,Instances to process in next iteration
v0.11.1,Conjunctions corresponding to solutions
v0.11.1,List of categorical columns
v0.11.1,Number of quantile thresholds used to binarize ordinal features
v0.11.1,whether to append negations
v0.11.1,whether to convert thresholds on ordinal features to strings
v0.11.1,Quantile probabilities
v0.11.1,Initialize
v0.11.1,Iterate over columns
v0.11.1,number of unique values
v0.11.1,Constant or binary column
v0.11.1,"Mapping to 0, 1"
v0.11.1,Categorical column
v0.11.1,OneHotEncoder object
v0.11.1,Fit to observed categories
v0.11.1,Ordinal column
v0.11.1,Few unique values
v0.11.1,Thresholds are sorted unique values excluding maximum
v0.11.1,Many unique values
v0.11.1,Thresholds are quantiles excluding repetitions
v0.11.1,Contains NaN values
v0.11.1,Initialize dataframe
v0.11.1,Iterate over columns
v0.11.1,Constant or binary column
v0.11.1,"Rename values to 0, 1"
v0.11.1,Categorical column
v0.11.1,Apply OneHotEncoder
v0.11.1,Append negations
v0.11.1,Concatenate
v0.11.1,Ordinal column
v0.11.1,Threshold values to produce binary arrays
v0.11.1,Append negations
v0.11.1,Convert to dataframe with column labels
v0.11.1,Ensure that rows corresponding to NaN values are zeroed out
v0.11.1,Add NaN indicator column
v0.11.1,Concatenate
v0.11.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11.1,set attributions to zero for left out invariant nodes
v0.11.1,Get parent and child nodes
v0.11.1,Don't remove node if node has more than 1 children nodes as it can introduce
v0.11.1,hidden confounders.
v0.11.1,Remove the middle node
v0.11.1,Connect parent and child nodes
v0.11.1,Update the causal mechanism for the child nodes
v0.11.1,"If the relative change of the score is less than the given threshold, we stop the estimation early."
v0.11.1,"Note, the output of score_samples are log values."
v0.11.1,"Note, the output of score_samples are log values."
v0.11.1,Currently only support continuous distributions for auto selection.
v0.11.1,Estimate distribution parameters from data.
v0.11.1,Ignore warnings from fitting process.
v0.11.1,Fit distribution to data.
v0.11.1,Some distributions might not be compatible with the data.
v0.11.1,Separate parts of parameters.
v0.11.1,Check the KL divergence between the distribution of the given and fitted distribution.
v0.11.1,Identify if this distribution is better.
v0.11.1,This error is typically raised when the data is discrete and all points are assigned to less cluster than
v0.11.1,"specified. It can also happen due to duplicated points. In these cases, the current best solution should"
v0.11.1,be sufficient.
v0.11.1,"Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]"
v0.11.1,"Normally, we need to estimate the KL divergence jointly. However, to avoid issues with high dimensional data,"
v0.11.1,we approximate it by taking the average over the marginal KL divergences.
v0.11.1,Do not compare with same model class
v0.11.1,Do not compare with same model class
v0.11.1,"In case of post non-linear models, we can obtain the conditional expectation directly based on the prediction"
v0.11.1,"model. To do this, we can just in pass 0 as the noise, since this would evaluate Y = f(X) + 0 in case of an"
v0.11.1,additive noise model and Y = g(f(X) + 0) in case of a more general model.
v0.11.1,Estimate the conditional expectation E[Y | x] by generating multiple samples for Y|x and average them.
v0.11.1,"Since these are categorical values, we just need to look for the most frequent element after we drew"
v0.11.1,multiple samples for each input.
v0.11.1,"In the categorical case, this is equivalent to the Brier score. However, the following formulation allows"
v0.11.1,categorical data with more than two classes.
v0.11.1,A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This
v0.11.1,function evaluates the provided causal query multiple times to build a confidence interval based on the returned
v0.11.1,results.
v0.11.1,Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order
v0.11.1,"to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the"
v0.11.1,fit_and_compute function.
v0.11.1,
v0.11.1,**Example usage:**
v0.11.1,
v0.11.1,">>> gcm.fit(causal_model, data)"
v0.11.1,">>> strength_medians, strength_intervals = gcm.confidence_intervals("
v0.11.1,">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))"
v0.11.1,
v0.11.1,"In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the"
v0.11.1,"confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'"
v0.11.1,"here to configure the function call. In this case,"
v0.11.1,"gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to"
v0.11.1,"lambda : gcm.arrow_strength(causal_model, target_node='Y')."
v0.11.1,
v0.11.1,"In order to incorporate uncertainties coming from fitting the causal model(s), we can use"
v0.11.1,gcm.fit_and_compute instead:
v0.11.1,">>>  strength_medians, strength_intervals = gcm.confidence_intervals("
v0.11.1,">>>        gcm.fit_and_compute(gcm.arrow_strength,"
v0.11.1,">>>                                            causal_model,"
v0.11.1,">>>                                            bootstrap_training_data=data,"
v0.11.1,>>>                                            target_node='Y'))
v0.11.1,This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each
v0.11.1,run.
v0.11.1,"Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based"
v0.11.1,on their topological order.
v0.11.1,"After drawing samples of the node based on the data generation process, we apply the corresponding"
v0.11.1,intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this
v0.11.1,node.
v0.11.1,"Abduction: For invertible SCMs, we recover exact noise values from data."
v0.11.1,Action + Prediction: Propagate the intervention downstream using recovered noise values.
v0.11.1,Check if we need to apply an intervention on the given node.
v0.11.1,Apply intervention function to the data of the node.
v0.11.1,Check if the intervention function changes the shape of the data.
v0.11.1,"For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e."
v0.11.1,all ancestors of the target.
v0.11.1,The target node can be a continuous real-valued variable or a categorical variable with at most two classes
v0.11.1,(i.e. binary).
v0.11.1,Find all node names in the expression string.
v0.11.1,"Nothing to fit here, since we know the ground truth."
v0.11.1,Avoid too many features
v0.11.1,Making sure there are at least 30% test samples.
v0.11.1,"Making sure that there are at least 2 samples from one class (here, simply duplicate the point)."
v0.11.1,Compare number of correct classifications.
v0.11.1,"Making sure that X and Y have no overlapping values, which would lead to a distance of 0 with k=1 and, thus, to"
v0.11.1,a division by zero.
v0.11.1,All elements are equal (or at least less than k samples are different)
v0.11.1,Balance the classes
v0.11.1,For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples
v0.11.1,is unknown beforehand.
v0.11.1,The method stops if either the change between some consecutive runs is below the given threshold or the
v0.11.1,maximum number of runs is reached.
v0.11.1,"In each run, we create one random permutation of players. For instance, given 4 players, a permutation"
v0.11.1,"could be [3,1,4,2]."
v0.11.1,Generate k random permutations by sorting the indices of the Halton sequence
v0.11.1,"Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the"
v0.11.1,"subsets are [3], [3,1], [3,1,4] [3,1,4,2]."
v0.11.1,"The result for each subset is cached such that if a subset that has already been evaluated appears again,"
v0.11.1,we can take this result directly.
v0.11.1,"To improve the runtime, multiple permutations are evaluated in each run."
v0.11.1,"The current Shapley values are the average of the estimated values, i.e. we need to divide by the number"
v0.11.1,of generated permutations here.
v0.11.1,"In case Shapley values are estimated for multiple samples, e.g., in feature relevance. So, we have a"
v0.11.1,matrix of Shapley values instead of a vector.
v0.11.1,"Here, the change between consecutive runs is below the minimum threshold, but to reduce the"
v0.11.1,"likelihood that this just happened by chance, we require that this happens at least for"
v0.11.1,num_consecutive_converged_runs times in a row.
v0.11.1,Check if change in percentage is below threshold
v0.11.1,"Check for values that are exactly zero. If they don't change between two runs, we consider it as converging."
v0.11.1,Create all (unique) subsets)
v0.11.1,"Assigning a 'high' weight, since this resembles ""infinity""."
v0.11.1,The weight for a subset with a specific length (see paper mentioned in the docstring for more
v0.11.1,information).
v0.11.1,TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is
v0.11.1,"theoretically sound, i.e. make entropy results from different data comparable."
v0.11.1,Extremely small values can somehow result in negative values.
v0.11.1,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.11.1,Sampling from the conditional distribution based on the current sample.
v0.11.1,"Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that"
v0.11.1,"are in the given subset. By this, we can simulate the impact on the conditional distribution when removing"
v0.11.1,only the incoming edges of the variables in the subset.
v0.11.1,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.11.1,"In case of the full subset (no randomization), we get the same predictions as when we apply the"
v0.11.1,"prediction method to the samples of interest, since all noise samples are replaced with a sample of"
v0.11.1,interest.
v0.11.1,"In case of the empty subset (all are jointly randomize), it boils down to taking the average over all"
v0.11.1,"predictions, seeing that the randomization yields the same values for each sample of interest (none of the"
v0.11.1,samples of interest are used to replace a (jointly) 'randomized' sample).
v0.11.1,Exact model
v0.11.1,This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes
v0.11.1,This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes
v0.11.1,afterwards.
v0.11.1,Todo: Remove after https://github.com/py-why/dowhy/pull/943.
v0.11.1,Smallest possible value. This is used in various algorithm for numerical stability.
v0.11.1,Make copy to avoid manipulating the original matrix.
v0.11.1,"The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,"
v0.11.1,the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster
v0.11.1,"than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as"
v0.11.1,"possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples."
v0.11.1,"To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many"
v0.11.1,samples. The number of samples that are evaluated is normally
v0.11.1,"baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to"
v0.11.1,"batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is"
v0.11.1,evaluated one by one in a for-loop.
v0.11.1,Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple
v0.11.1,"batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet."
v0.11.1,"If the batch size would be larger than the remaining amount of samples, it is reduced to only include the"
v0.11.1,remaining baseline_noise_samples.
v0.11.1,"The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features"
v0.11.1,in baseline_feature_indices to their respective values in baseline_noise_samples.
v0.11.1,"After creating the (potentially large) input data matrix, we can evaluate the prediction method."
v0.11.1,"Here, offset + index now indicates the sample index in baseline_noise_samples."
v0.11.1,This would average all prediction results obtained for the 'offset + index'-th sample in
v0.11.1,"baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)]."
v0.11.1,This would return all prediction results obtained for the 'offset + index'-th sample in
v0.11.1,"baseline_noise_samples, i.e. the results are not averaged."
v0.11.1,Making copy to ensure that the original object is not modified.
v0.11.1,Permute samples jointly. This still represents an interventional distribution.
v0.11.1,Permute samples independently.
v0.11.1,Looks for the first index where the cumulative sum of the probabilities is larger than the threshold.
v0.11.1,"Note that if there are multiple indices with the same maximum value (as in this case here), the argmax"
v0.11.1,function returns the first index.
v0.11.1,"test local Markov condition, null hypothesis: conditional independence"
v0.11.1,"test edge dependence, null hypothesis: independence"
v0.11.1,The order of the p-values added to the list is deterministic.
v0.11.1,"To be able to validate that the graph structure did not change between fitting and causal query, we store the"
v0.11.1,"parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While"
v0.11.1,"this would automatically fail when the number of parents is different, there are other more subtle cases,"
v0.11.1,"where the number is still the same, but it's different parents, and therefore different data. That would yield"
v0.11.1,"wrong results, but would not fail."
v0.11.1,Independence tests are symmetric
v0.11.1,Find out which tests to do
v0.11.1,Parallelize over tests
v0.11.1,Gather results
v0.11.1,Summarize
v0.11.1,Find out which tests to do
v0.11.1,Parallelize over tests
v0.11.1,Gather results
v0.11.1,Summarize
v0.11.1,Find out which tests to do
v0.11.1,Parallelize over tests
v0.11.1,Gather results
v0.11.1,Summarize
v0.11.1,DAG Evaluation
v0.11.1,Suggestions
v0.11.1,"Append list of violations (node, non_desc) to get local information"
v0.11.1,Plot histograms
v0.11.1,Plot given violations
v0.11.1,For LMC we highlight X for which X _|/|_ Y \in ND_X | Pa_X
v0.11.1,For PD we highlight the edge (if Y\in Anc_X -> X are adjacent)
v0.11.1,For causal minimality we highlight the edge Y \in Pa_X -> X
v0.11.1,Create Validation header
v0.11.1,Create Validation summary
v0.11.1,Close Validation
v0.11.1,Create Suggestions header
v0.11.1,Iterate over suggestions
v0.11.1,Test if we have data for X and Y
v0.11.1,Test if we have data for Z
v0.11.1,Eq. (1) in https://arxiv.org/pdf/1810.11363.pdf
v0.11.1,Subtracting Y here since the cumulative sum includes the current element. The same reason we subtract 1
v0.11.1,from the count.
v0.11.1,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.11.1,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.11.1,"target quantity (here, variance)."
v0.11.1,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.11.1,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.11.1,"target quantity (here, variance)."
v0.11.1,"Calculate Ri, the product of the residuals"
v0.11.1,Standard deviation of the residuals
v0.11.1,Either X and/or Y is constant.
v0.11.1,"If Z is empty, we are in the pairwise setting."
v0.11.1,Either X and/or Y is constant.
v0.11.1,"If Z is empty, we are in the pairwise setting."
v0.11.1,Estimate test statistic multiple times on different permutations of the data. The p-value is then the
v0.11.1,probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted
v0.11.1,data.
v0.11.1,Take the lower dimensional variable as target.
v0.11.1,First stage statistical model
v0.11.1,Second stage statistical model
v0.11.1,Check if the treatment is one-dimensional
v0.11.1,First stage
v0.11.1,Second Stage
v0.11.1,Combining the two estimates
v0.11.1,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.11.1,Bulding the feature matrix
v0.11.1,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.11.1,TODO move this to the identification step
v0.11.1,Obtain estimate by Wald Estimator
v0.11.1,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.11.1,More than 1 instrument. Use 2sls.
v0.11.1,Checking if Y is binary
v0.11.1,Enable the user to pass params for a custom propensity model
v0.11.1,Convert the categorical variables into dummy/indicator variables
v0.11.1,"Basically, this gives a one hot encoding for each category"
v0.11.1,The first category is taken to be the base line.
v0.11.1,Check if the treatment is one-dimensional
v0.11.1,Checking if the treatment is binary
v0.11.1,Data encoders
v0.11.1,encoder_drop_first will not encode the first category value with a bit in 1-hot encoding.
v0.11.1,"It will be implicit instead, by the absence of any bit representing this value in the relevant columns."
v0.11.1,Set to False to include a bit for each value of every categorical variable.
v0.11.1,Remember encoder
v0.11.1,The model is always built on the entire data
v0.11.1,TODO make treatment_value and control value also as local parameters
v0.11.1,All treatments are set to the same constant value
v0.11.1,"Fixing treatment value to the specified value, if provided"
v0.11.1,treatment_vals and data_df should have same number of rows
v0.11.1,Bulding the feature matrix
v0.11.1,Replace treatment values with value supplied; note: Don't change column datatype!
v0.11.1,The model is always built on the entire data
v0.11.1,The average treatment effect is a combination of different
v0.11.1,regression coefficients. Complicated to compute the confidence
v0.11.1,"interval analytically. For example, if y=a + b1.t + b2.tx, then"
v0.11.1,the average treatment effect is b1+b2.mean(x).
v0.11.1,"Refer Gelman, Hill. ARM Book. Chapter 9"
v0.11.1,http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
v0.11.1,TODO: Looking for contributions
v0.11.1,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.11.1,"variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0."
v0.11.1,"So for custom treatment and control values, we must multiply the confidence interval by the difference of the two."
v0.11.1,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.11.1,"variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0."
v0.11.1,"So for custom treatment and control values, we must multiply the standard error by the difference of the two."
v0.11.1,setting method-specific parameters
v0.11.1,Infer the right strata based on clipping threshold
v0.11.1,0.5 because there are two values for the treatment
v0.11.1,To be conservative and allow most strata to be included in the
v0.11.1,analysis
v0.11.1,At least 90% of the strata should be included in analysis
v0.11.1,sum weighted outcomes over all strata  (weight by treated population)
v0.11.1,TODO - how can we add additional information into the returned estimate?
v0.11.1,"such as how much clipping was done, or per-strata info for debugging?"
v0.11.1,sort the dataframe by propensity score
v0.11.1,create a column 'strata' for each element that marks what strata it belongs to
v0.11.1,"for each strata, count how many treated and control units there are"
v0.11.1,throw away strata that have insufficient treatment or control
v0.11.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.11.1,Setting method specific parameters
v0.11.1,trim propensity score weights
v0.11.1,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.11.1,nips ==> ips / (sum of ips over all units)
v0.11.1,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.11.1,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.11.1,Vanilla IPS estimator
v0.11.1,The Hajek estimator (or the self-normalized estimator)
v0.11.1,"Stabilized weights (from Robins, Hernan, Brumback (2000))"
v0.11.1,Paper: Marginal Structural Models and Causal Inference in Epidemiology
v0.11.1,Calculating the effect
v0.11.1,Subtracting the weighted means
v0.11.1,TODO - how can we add additional information into the returned estimate?
v0.11.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.11.1,Save parameters for later refutter fitting
v0.11.1,Enforcing this ordering is necessary to feed through the propensity values from dataset
v0.11.1,For metalearners only--issue a warning if w contains variables not in x
v0.11.1,Override the effect_modifiers set in CausalEstimator.__init__()
v0.11.1,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.11.1,the latter can be used by other estimator methods later
v0.11.1,"Instrumental variables names, if present"
v0.11.1,choosing the instrumental variable to use
v0.11.1,Calling the econml estimator's fit method
v0.11.1,"As of v0.9, econml has some kewyord only arguments"
v0.11.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11.1,Changing shape to a list for a singleton value
v0.11.1,Note that self._control_value is assumed to be a singleton value
v0.11.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.11.1,"For each unit, return the estimated effect of the treatment value"
v0.11.1,that was actually applied to the unit
v0.11.1,this assumes a binary treatment regime
v0.11.1,TODO remove neighbors that are more than a given radius apart
v0.11.1,estimate ATT on treated by summing over difference between matched neighbors
v0.11.1,Now computing ATC
v0.11.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.11.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11.1,Handle externally provided estimator classes
v0.11.1,allowed types of distance metric
v0.11.1,Dictionary of any user-provided params for the distance metric
v0.11.1,that will be passed to sklearn nearestneighbors
v0.11.1,Check if the treatment is one-dimensional
v0.11.1,Checking if the treatment is binary
v0.11.1,Convert the categorical variables into dummy/indicator variables
v0.11.1,"Basically, this gives a one hot encoding for each category"
v0.11.1,The first category is taken to be the base line.
v0.11.1,this assumes a binary treatment regime
v0.11.1,TODO remove neighbors that are more than a given radius apart
v0.11.1,estimate ATT on treated by summing over difference between matched neighbors
v0.11.1,Return indices in the original dataframe
v0.11.1,self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()
v0.11.1,Now computing ATC
v0.11.1,Return indices in the original dataframe
v0.11.1,Add the identification method used in the estimator
v0.11.1,Check the backdoor variables being used
v0.11.1,Add the observed confounders and one hot encode the categorical variables
v0.11.1,Get the data of the unobserved confounders
v0.11.1,One hot encode the data if they are categorical
v0.11.1,Check the instrumental variables involved
v0.11.1,Perform the same actions as the above
v0.11.1,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.11.1,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.11.1,For CATEs
v0.11.1,TODO we are conditioning on a postive treatment
v0.11.1,TODO create an expression corresponding to each estimator used
v0.11.1,Determine columns being encoded
v0.11.1,"If all columns are already numerical, there may be nothing to encode."
v0.11.1,"In this case, return original data."
v0.11.1,Columns to keep in the result - not encoded.
v0.11.1,Convert the encoded data to a DataFrame
v0.11.1,Concatenate the encoded DataFrame with the original non-categorical columns
v0.11.1,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.11.1,Flipping some values
v0.11.1,Wrapping labels if they are too long
v0.11.1,This layout tries to mimic the graphviz layout in a simpler form. The depth grows horizontally here instead of
v0.11.1,vertically.
v0.11.1,Set the figure size based on the number of nodes
v0.11.1,"Nodes that are vertically connected, but not neighbors should be connected via a curved edge."
v0.11.1,All other nodes should be connected with a straight line.
v0.11.1,Draw labels node labels
v0.11.1,"Each node gets a depth assigned, based on the distance to the closest root node."
v0.11.1,"In case of undirected graphs, we just take any node as root node."
v0.11.1,"No path to root node, ignore this connection then."
v0.11.1,Counts the number of vertical nodes in the same layers.
v0.11.1,Creates a matrix indicating whether two nodes are vertical neighbors.
v0.11.1,Get all y coordinates per layer
v0.11.1,Sort the y-coordinates
v0.11.1,Finding p-value using student T test
v0.11.1,Only consider edges have absolute edge weight > 0.01
v0.11.1,Modify graph such that it only contains bidirected edges
v0.11.1,Find c components by finding connected components on the undirected graph
v0.11.1,Understanding Neural Network weights
v0.11.1,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.11.1,add weight column
v0.11.1,before weights are applied we count number rows in each category
v0.11.1,which is equivalent to summing over weight=1
v0.11.1,after weights are applied we need to sum over the given weights
v0.11.1,"First, calculating mean differences by strata"
v0.11.1,"Second, without strata"
v0.11.1,"Third, concatenating them and plotting"
v0.11.1,Setting estimator attribute for convenience
v0.11.1,Outcome is numeric
v0.11.1,Treatments are also numeric or binary
v0.11.1,Outcome is categorical
v0.11.1,Treatments are numeric or binary
v0.11.1,TODO: A common way to show all plots
v0.11.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11.1,Get adjacency list
v0.11.1,If node pair has been fully explored
v0.11.1,Add node1 to backdoor set of node_pair
v0.11.1,Check if path is backdoor and does not have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.11.1,"True if arrow incoming, False if arrow outgoing"
v0.11.1,"Mark pair (node1, node2) complete"
v0.11.1,Modify variable count and indices covered
v0.11.1,Average total effect
v0.11.1,Natural direct effect
v0.11.1,Natural indirect effect
v0.11.1,Controlled direct effect
v0.11.1,Backdoor method names
v0.11.1,"First, check if there is a directed path from action to outcome"
v0.11.1,## 1. BACKDOOR IDENTIFICATION
v0.11.1,Pick algorithm to compute backdoor sets according to method chosen
v0.11.1,"First, checking if there are any valid backdoor adjustment sets"
v0.11.1,"Setting default ""backdoor"" identification adjustment set"
v0.11.1,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.11.1,Now checking if there is also a valid iv estimand
v0.11.1,## 3. FRONTDOOR IDENTIFICATION
v0.11.1,Now checking if there is a valid frontdoor variable
v0.11.1,Finally returning the estimand object
v0.11.1,Pick algorithm to compute backdoor sets according to method chosen
v0.11.1,"Setting default ""backdoor"" identification adjustment set"
v0.11.1,Finally returning the estimand object
v0.11.1,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.11.1,"First, checking if there are any valid backdoor adjustment sets"
v0.11.1,"Setting default ""backdoor"" identification adjustment set"
v0.11.1,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.11.1,Now checking if there are valid mediator variables
v0.11.1,Finally returning the estimand object
v0.11.1,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.11.1,"First, checking if there are any valid backdoor adjustment sets"
v0.11.1,"Setting default ""backdoor"" identification adjustment set"
v0.11.1,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.11.1,Now checking if there are valid mediator variables
v0.11.1,Finally returning the estimand object
v0.11.1,"First, checking if empty set is a valid backdoor set"
v0.11.1,"If the method is `minimal-adjustment`, return the empty set right away."
v0.11.1,"Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible."
v0.11.1,only remove descendants of Y
v0.11.1,also allow any causes of Y that are not caused by T (for lower variance)
v0.11.1,remove descendants of T (mediators) and descendants of Y
v0.11.1,"If var is d-separated from both treatment or outcome, it cannot"
v0.11.1,be a part of the backdoor set
v0.11.1,repeat the above search with BACKDOOR_MIN
v0.11.1,"If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest."
v0.11.1,"If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set."
v0.11.1,"If all variables are observed, and the biggest eligible set"
v0.11.1,"does not satisfy backdoor, then none of its subsets will."
v0.11.1,Adding a None estimand if no backdoor set found
v0.11.1,"Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable."
v0.11.1,Default set is the one with the least number of adjustment variables (optimizing for efficiency)
v0.11.1,Cond 1: All directed paths intercepted by candidate_var
v0.11.1,Cond 2: No confounding between treatment and candidate var
v0.11.1,Cond 3: treatment blocks all confounding between candidate_var and outcome
v0.11.1,"For simplicity, assuming a one-variable mediation set"
v0.11.1,"Create estimands dict as per the API for backdoor, but do not return it"
v0.11.1,"Setting default ""backdoor"" identification adjustment set"
v0.11.1,"Create estimands dict as per the API for backdoor, but do not return it"
v0.11.1,"Setting default ""backdoor"" identification adjustment set"
v0.11.1,"TODO: outputs string for now, but ideally should do symbolic"
v0.11.1,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.11.1,TODO Better support for multivariate treatments
v0.11.1,TODO: support multivariate treatments better.
v0.11.1,TODO: support multivariate treatments better.
v0.11.1,TODO: support multivariate treatments better.
v0.11.1,For direct effect
v0.11.1,"If no costs are passed, use uniform costs"
v0.11.1,restriction to ancestors
v0.11.1,back-door graph
v0.11.1,moralization
v0.11.1,Estimators list for returning after identification
v0.11.1,Line 1
v0.11.1,"If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y."
v0.11.1,Line 2
v0.11.1,"If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y."
v0.11.1,Modify list of valid nodes
v0.11.1,Line 3 - forces an action on any node where such an action would have no effect on Y – assuming we already acted on X.
v0.11.1,Modify adjacency matrix to obtain that corresponding to do(X)
v0.11.1,Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.
v0.11.1,"If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases."
v0.11.1,Modify adjacency matrix to remove treatment variables
v0.11.1,"Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes."
v0.11.1,"Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem."
v0.11.1,Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.
v0.11.1,Do not show backdoor key unless it is the only backdoor set.
v0.11.1,Just show the default backdoor set
v0.11.1,If labels provided
v0.11.1,Return in valid DOT format
v0.11.1,Get adjacency matrix
v0.11.1,If labels not provided
v0.11.1,Obtain valid DOT format
v0.11.1,If labels provided
v0.11.1,Return in valid DOT format
v0.11.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11,-*- coding: utf-8 -*-
v0.11,
v0.11,Configuration file for the Sphinx documentation builder.
v0.11,
v0.11,This file does only contain a selection of the most common options. For a
v0.11,full list see the documentation:
v0.11,http://www.sphinx-doc.org/en/stable/config
v0.11,-- Path setup --------------------------------------------------------------
v0.11,"If extensions (or modules to document with autodoc) are in another directory,"
v0.11,add these directories to sys.path here. If the directory is relative to the
v0.11,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.11,
v0.11,-- Project information -----------------------------------------------------
v0.11,Version Information (for version-switcher)
v0.11,-- General configuration ---------------------------------------------------
v0.11,"If your documentation needs a minimal Sphinx version, state it here."
v0.11,
v0.11,needs_sphinx = '1.0'
v0.11,"Add any Sphinx extension module names here, as strings. They can be"
v0.11,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.11,ones.
v0.11,sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme
v0.11,already loads it
v0.11,"Add any paths that contain templates here, relative to this directory."
v0.11,The suffix(es) of source filenames.
v0.11,You can specify multiple suffix as a list of string:
v0.11,
v0.11,"source_suffix = ['.rst', '.md']"
v0.11,The master toctree document.
v0.11,The language for content autogenerated by Sphinx. Refer to documentation
v0.11,for a list of supported languages.
v0.11,
v0.11,This is also used if you do content translation via gettext catalogs.
v0.11,"Usually you set ""language"" from the command line for these cases."
v0.11,"List of patterns, relative to source directory, that match files and"
v0.11,directories to ignore when looking for source files.
v0.11,This pattern also affects html_static_path and html_extra_path .
v0.11,The name of the Pygments (syntax highlighting) style to use.
v0.11,-- Options for HTML output -------------------------------------------------
v0.11,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.11,a list of builtin themes.
v0.11,
v0.11,Theme options are theme-specific and customize the look and feel of a theme
v0.11,"further.  For a list of options available for each theme, see the"
v0.11,documentation.
v0.11,
v0.11,"Add any paths that contain custom static files (such as style sheets) here,"
v0.11,"relative to this directory. They are copied after the builtin static files,"
v0.11,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.11,"Custom sidebar templates, must be a dictionary that maps document names"
v0.11,to template names.
v0.11,
v0.11,The default sidebars (for documents that don't match any pattern) are
v0.11,defined by theme itself.  Builtin themes are using these templates by
v0.11,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.11,'searchbox.html']``.
v0.11,
v0.11,html_sidebars = {}
v0.11,-- Options for HTMLHelp output ---------------------------------------------
v0.11,Output file base name for HTML help builder.
v0.11,-- Options for LaTeX output ------------------------------------------------
v0.11,The paper size ('letterpaper' or 'a4paper').
v0.11,
v0.11,"'papersize': 'letterpaper',"
v0.11,"The font size ('10pt', '11pt' or '12pt')."
v0.11,
v0.11,"'pointsize': '10pt',"
v0.11,Additional stuff for the LaTeX preamble.
v0.11,
v0.11,"'preamble': '',"
v0.11,Latex figure (float) alignment
v0.11,
v0.11,"'figure_align': 'htbp',"
v0.11,Grouping the document tree into LaTeX files. List of tuples
v0.11,"(source start file, target name, title,"
v0.11,"author, documentclass [howto, manual, or own class])."
v0.11,-- Options for manual page output ------------------------------------------
v0.11,One entry per manual page. List of tuples
v0.11,"(source start file, name, description, authors, manual section)."
v0.11,-- Options for Texinfo output ----------------------------------------------
v0.11,Grouping the document tree into Texinfo files. List of tuples
v0.11,"(source start file, target name, title, author,"
v0.11,"dir menu entry, description, category)"
v0.11,-- Options for Epub output -------------------------------------------------
v0.11,Bibliographic Dublin Core info.
v0.11,The unique identifier of the text. This can be a ISBN number
v0.11,or the project homepage.
v0.11,
v0.11,epub_identifier = ''
v0.11,A unique identification for the text.
v0.11,
v0.11,epub_uid = ''
v0.11,A list of files that should not be packed into the epub file.
v0.11,-- Extension configuration -------------------------------------------------
v0.11,-- Options for todo extension ----------------------------------------------
v0.11,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.11,init docstrings should also be included in class
v0.11,Only uncomment for faster testing/building docs without compiling notebooks
v0.11,"nbsphinx_execute = ""never"""
v0.11,Patch all of the published versions
v0.11,check old RST version (<= v0.8)
v0.11,Remove old version links
v0.11,Append updated version links
v0.11,requires stdin input for identify in weighting sampler
v0.11,will be removed
v0.11,"applied notebook, not necessary to test each time"
v0.11,needs xgboost too
v0.11,Slow Notebooks
v0.11,"TODO: should probably move more notebooks here to ignore, because"
v0.11,most get tested by the documentation generation.
v0.11,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.11,can import dowhy
v0.11,"""--ExecutePreprocessor.timeout=600"","
v0.11,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.11,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.11,treated = self.df[self.df['z']==1]
v0.11,self.att = np.mean(treated['y1'] - treated['y0'])
v0.11,def test_average_treatment_effect(self):
v0.11,est_ate = 1
v0.11,bias = est_ate - self.ate
v0.11,print(bias)
v0.11,"self.assertAlmostEqual(self.ate, est_ate)"
v0.11,def test_average_treatment_effect_on_treated(self):
v0.11,est_att = 1
v0.11,self.att=1
v0.11,bias = est_att - self.att
v0.11,print(bias)
v0.11,"self.assertAlmostEqual(self.att, est_att)"
v0.11,removing two common causes
v0.11,removing two common causes
v0.11,removing two common causes
v0.11,removing two common causes
v0.11,removing two common causes
v0.11,"Remove graph variable with name ""W0"" from observed data."
v0.11,Ensure that a log record exists that provides a more detailed view
v0.11,of observed and unobserved graph variables (counts and variable names.)
v0.11,"num_frontdoor_variables=1,"
v0.11,creating nx graph instance
v0.11,to be used later for a test. Does not include the replace operation
v0.11,check if all partial R^2 values are between 0 and 1
v0.11,We calculate adjusted estimates for two sets of partial R^2 values.
v0.11,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.11,Creating a model with no unobserved confounders
v0.11,check if all partial R^2 values are between 0 and 1
v0.11,"for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)"
v0.11,Non Parametric estimator
v0.11,We calculate adjusted estimates for two sets of partial R^2 values.
v0.11,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.11,we patched figure plotting call to avoid drawing plots during tests
v0.11,We calculate adjusted estimates for two sets of partial R^2 values.
v0.11,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.11,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.11,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.11,we patched figure plotting call to avoid drawing plots during tests
v0.11,comparing test examples from R E-Value package
v0.11,check implementation of Observed Covariate E-value against R package
v0.11,The outcome is a linear function of the confounder
v0.11,"The slope is 1,2 and the intercept is 3"
v0.11,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.11,"TODO: Check directly for correct behavior, rather than checking the rules"
v0.11,"themselves, which can be non-deterministic (all the following are equivalent)"
v0.11,Supports user-provided dataset object
v0.11,To test if there are any exceptions
v0.11,To test if the estimate is identical if refutation parameters are zero
v0.11,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.11,"Ordinarily, we should expect this value to be zero."
v0.11,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.11,"Ordinarily, we should expect this value to be zero."
v0.11,Only P(Y|T) should be present for test to succeed.
v0.11,"Since undirected graph, identify effect must throw an error."
v0.11,Compare with ground truth
v0.11,Compare with ground truth
v0.11,Compare with ground truth
v0.11,Compare with ground truth
v0.11,Causal model initialization
v0.11,Obtain backdoor sets
v0.11,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.11,Causal model initialization
v0.11,Obtain backdoor sets
v0.11,Causal model initialization
v0.11,Obtain backdoor sets
v0.11,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.11,Causal model initialization
v0.11,Obtain backdoor sets
v0.11,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.11,Causal model initialization
v0.11,Obtain backdoor sets
v0.11,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.11,Causal model initialization
v0.11,Obtain backdoor sets
v0.11,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.11,"Example is selected from Pearl J. ""Causality"" 2nd Edition, from chapter 3.3.1 on backoor criterion."
v0.11,"The following simpsons paradox examples are taken from Pearl, J {2013}. ""Understanding Simpson’s Paradox"" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf"
v0.11,"The following are examples given in the ""Book of Why"" by Judea Pearl, chapter ""The Do-operator and the Back-Door Criterion"""
v0.11,Building the causal model
v0.11,For all examples from these papers we use X for the treatment variable
v0.11,instead of A.
v0.11,"Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.11,"Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.11,"Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.11,"Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.11,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.11,L replaces X as the conditional variable
v0.11,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.11,L replaces X as the conditional variable. Uses different costs
v0.11,"Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.11,"A graph where optimal, optimal minimal and optimal min cost are different"
v0.11,The graph from Shrier and Platt (2008)
v0.11,A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.11,Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.11,cov_mat = np.diag(np.ones(num_features))
v0.11,collider: X->Z<-Y
v0.11,chain: X->Z->Y
v0.11,fork: X<-Z->Y
v0.11,"general DAG: X<-Z->Y, X->Y"
v0.11,fork: X<-Z->Y
v0.11,Only Gaussian component changed
v0.11,"Just checking formats, i.e. no need for correlation."
v0.11,"Just checking formats, i.e. no need for correlation."
v0.11,The MSE should be 1 due to the variance of the noise. The RMSE is accordingly 1 / var(Y).
v0.11,Contributions should add up to Var(X2)
v0.11,H(P(Y)) -- Can be precomputed
v0.11,-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))
v0.11,"H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\S)])"
v0.11,"E[P(Y | x_S, X'_\S)]"
v0.11,"H(E[P(Y | x_S, X'_\S)])"
v0.11,"Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions."
v0.11,E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]
v0.11,Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3
v0.11,Having at least one sample from the second class should not raise an error.
v0.11,"Just some random data, since we are only interested in the omitted data."
v0.11,This caused an error before with pandas > 2.0
v0.11,C2 = 3 * A2 + 2 * B2
v0.11,"By default, the strength is measure with respect to the variance."
v0.11,"Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2."
v0.11,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.11,Missing connection between X0 and X1.
v0.11,"For X0 and X1, we set the ground truth noise to further emphasize the misspecification. The inferred noise of X1"
v0.11,would otherwise have a dependency with Z due to the missing connection with X0.
v0.11,Modelling connection between X0 and X1 explicitly.
v0.11,"Here, we misspecify the mechanism on purpose by setting scale to 1 instead of 2."
v0.11,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.11,"Here, changing the mechanism."
v0.11,"Here, changing the mechanism."
v0.11,"Here, changing the mechanism."
v0.11,"Here, changing the mechanism."
v0.11,Defining an anomaly scorer that handles multidimensional inputs.
v0.11,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.11,"In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution"
v0.11,"and it should be ""significantly"" higher than the contribution of the other ones (here, we just arbitrarily say"
v0.11,"it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,"
v0.11,"which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate"
v0.11,"some robustness. Note that due to this and stochastic behaviour of the density estimator, it is"
v0.11,"not possible to analytically compute expected results. Therefore, we rather look at the relations here."
v0.11,"Same idea for the second sample, but here, it is the second variable that is anomalous."
v0.11,"In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be"
v0.11,"""significantly"" higher than the contribution of the other variables. The contribution of both anomalous variables"
v0.11,should be equal (approximately).
v0.11,Defining an anomaly scorer that handles multidimensional inputs.
v0.11,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.11,reduce the score.
v0.11,The contributions should add up to g(x) - E[g(X)]
v0.11,The contributions should add up to g(x) - E[g(X)]
v0.11,The contributions should add up to g(x) - E[g(X)]
v0.11,Three examples:
v0.11,1. X1 is the root cause (+ 10 to the noise)
v0.11,2. X0 is the root cause (+ 10 to the noise)
v0.11,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.11,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.11,Three examples:
v0.11,1. X1 is the root cause (+ 10 to the noise)
v0.11,2. X0 is the root cause (+ 10 to the noise)
v0.11,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.11,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.11,"Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution"
v0.11,algorithm.
v0.11,1. X0 is the root cause (+ 10 to the noise)
v0.11,2. X0 and X1 are the root causes (+ 10 to both noise)
v0.11,3. X2 and X3 are both root causes (+ 10 to both noises)
v0.11,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.11,collider: X->Z<-Y
v0.11,collider: X->Z<-Y
v0.11,chain: X->Y->Z
v0.11,chain: X->Y->Z
v0.11,Empty graph
v0.11,Full DAG
v0.11,DAG with single node
v0.11,DAG with single edge
v0.11,DAG with single edge
v0.11,chain: X->Z->Y
v0.11,Setup data
v0.11,Test LinearDML
v0.11,Checking that the CATE estimates are not identical
v0.11,Test ContinuousTreatmentOrthoForest
v0.11,Checking that the CATE estimates are not identical
v0.11,Test LinearDRLearner
v0.11,Test LinearDML
v0.11,checking that CATE estimates are not identical
v0.11,predict on new data
v0.11,Setup data
v0.11,Test DeepIV
v0.11,Test IntentToTreatDRIV
v0.11,Observed data
v0.11,assumed graph
v0.11,Identify effect
v0.11,Estimate effect
v0.11,A model where X is also a common cause
v0.11,A model where X is also a common cause
v0.11,The case where effect modifier is not a common cause
v0.11,A model where X is also a common cause
v0.11,Create the graph describing the causal structure
v0.11,Generate the data
v0.11,Data to df
v0.11,Create a model
v0.11,Estimate the effect with front-door
v0.11,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.11,More cases where Exception  is expected
v0.11,"Compute confidence intervals, standard error and significance tests"
v0.11,Defined a linear dataset with a given set of properties
v0.11,Create a model that captures the same
v0.11,Identify the effects within the model
v0.11,Defined a linear dataset with a given set of properties
v0.11,Create a model that captures the same
v0.11,Identify the effects within the model
v0.11,Defined a linear dataset with a given set of properties
v0.11,Create a model that captures the same
v0.11,Identify the effects within the model
v0.11,Defined a linear dataset with a given set of properties
v0.11,Create a model that captures the same
v0.11,Identify the effects within the model
v0.11,Defined a linear dataset with a given set of properties
v0.11,Create a model that captures the same
v0.11,Identify the effects within the model
v0.11,Check if calling the method causes some import or runtime errors
v0.11,TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer
v0.11,newer matplotlib version:
v0.11,AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'
v0.11,Networkx 2.4+ should fix this issue.
v0.11,"plot_adjacency_matrix(causal_graph, is_directed=False)"
v0.11,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.11,Unpacking the keyword arguments
v0.11,todo: add docstring for common parameters here and remove from child refuter classes
v0.11,Default value for the number of simulations to be conducted
v0.11,joblib params for parallel processing
v0.11,"Concatenate the confounders, instruments and effect modifiers"
v0.11,Shuffle the confounders
v0.11,Check if all are select or deselect variables
v0.11,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.11,This calculates a two-sided percentile p-value
v0.11,See footnotes in https://journals.sagepub.com/doi/full/10.1177/2515245920911881
v0.11,Get the mean for the simulations
v0.11,Get the standard deviation for the simulations
v0.11,Get the Z Score [(val - mean)/ std_dev ]
v0.11,Initializing the p_value
v0.11,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.11,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.11,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.11,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.11,re.sub only takes string parameter so the first if is to avoid error
v0.11,"if the input is a text file, convert the contained data into string"
v0.11,load dot file
v0.11,TODO This functionality needs to be deprecated. It is a convenience function but can introduce confusion
v0.11,as we are now including the option to initialize CausalGraph with DiGraph or GCM model.
v0.11,Adding node attributes
v0.11,adding penwidth to make the edge bold
v0.11,Adding common causes
v0.11,Adding instruments
v0.11,Adding effect modifiers
v0.11,Assuming the simple form of effect modifier
v0.11,that directly causes the outcome.
v0.11,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.11,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.11,Adding columns in the dataframe as confounders that were not in the graph
v0.11,Adding unobserved confounders
v0.11,removal of only direct edges wrt a target is not implemented for incoming edges
v0.11,also return the number of backdoor paths blocked by observed nodes
v0.11,Assume that nodes1 is the treatment
v0.11,"ignores new_graph parameter, always uses self._graph"
v0.11,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.11,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.11,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.11,Return effect modifiers according to the graph
v0.11,removing all mediators
v0.11,"Also add any effect modifiers that could not be auto-detected (e.g., they are also common causes)"
v0.11,[TODO: double check these work with multivariate implementation:]
v0.11,Exclusion
v0.11,As-if-random setup
v0.11,As-if-random
v0.11,convert the outputted generator into a list
v0.11,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.11,return len(dpaths) > 0
v0.11,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.11,Emit a `UserWarning` if there are any unobserved graph variables and
v0.11,and log a message highlighting data variables that are not part of the graph.
v0.11,Create causal graph object
v0.11,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.11,(Because some effect modifiers may also be common causes)
v0.11,"In such cases, the user-provided modifiers are used."
v0.11,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.11,Import causal discovery class
v0.11,Initialize causal graph object
v0.11,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.11,TODO add dowhy as a prefix to all dowhy estimators
v0.11,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.11,Define the third-party estimation method to be used
v0.11,Process the dowhy estimators
v0.11,Check if estimator's target estimand is identified
v0.11,"Note that while the name of the variable is the same,"
v0.11,"""self.causal_estimator"", this estimator takes in less"
v0.11,parameters than the same from the
v0.11,estimate_effect code. It is not advisable to use the
v0.11,estimator from this function to call estimate_effect
v0.11,with fit_estimator=False.
v0.11,Estimator had been computed in a previous call
v0.11,The default number of simulations for statistical testing
v0.11,The default number of simulations to obtain confidence intervals
v0.11,This should be at least 399 for a 5% error rate:
v0.11,https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf
v0.11,The portion of the total size that should be taken each time to find the confidence intervals
v0.11,1 is the recommended value
v0.11,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.11,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.11,The default Confidence Level
v0.11,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.11,Prefix to add to temporary categorical variables created after discretization
v0.11,std args to be removed from locals() before being passed to args_dict
v0.11,Setting the default interpret method
v0.11,"Check if some parameters were set, otherwise set to default values"
v0.11,Estimate conditional estimates by default
v0.11,TODO Only works for binary treatment
v0.11,Defaulting to class default values if parameters are not provided
v0.11,Checking that there is at least one effect modifier
v0.11,Making sure that effect_modifier_names is a list
v0.11,Making a copy since we are going to be changing effect modifier names
v0.11,"For every numeric effect modifier, adding a temp categorical column"
v0.11,Grouping by effect modifiers and computing effect separately
v0.11,Deleting the temporary categorical columns
v0.11,The array that stores the results of all estimations
v0.11,Find the sample size the proportion with the population size
v0.11,Perform the set number of simulations
v0.11,names of treatment and outcome
v0.11,Using class default parameters if not specified
v0.11,Checking if bootstrap_estimates are already computed
v0.11,Checked if any parameter is changed from the previous std error estimate
v0.11,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.11,Get the variations of each bootstrap estimate and sort
v0.11,"Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level"
v0.11,Get the lower and upper bounds by subtracting the variations from the estimate
v0.11,"Use existing params, if new user defined params are not present"
v0.11,Checking if bootstrap_estimates are already computed
v0.11,Check if any parameter is changed from the previous std error estimate
v0.11,"Use existing params, if new user defined params are not present"
v0.11,Processing the null hypothesis estimates
v0.11,Doing a two-sided test
v0.11,Being conservative with the p-value reported
v0.11,Being conservative with the p-value reported
v0.11,"If the estimate_index is 0, it depends on the number of simulations"
v0.11,Need to test r-squared before supporting
v0.11,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.11,'r-squared': effect_r_squared
v0.11,"elif method == ""r-squared"":"
v0.11,outcome_mean = np.mean(self._outcome)
v0.11,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.11,Assuming a linear model with one variable: the treatment
v0.11,Currently only works for continuous y
v0.11,causal_model = outcome_mean + estimate.value*self._treatment
v0.11,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.11,r_squared = 1 - (squared_residual/total_variance)
v0.11,return r_squared
v0.11,Check if estimator's target estimand is identified
v0.11,Store parameters inside estimate object for refutation methods
v0.11,TODO: This add_params needs to move to the estimator class
v0.11,inside estimate_effect and estimate_conditional_effect
v0.11,"TODO: Remove _data, _treatment_name and _outcome_name from this object"
v0.11,we save them here to enable the methods that required these properties saved in the estimator
v0.11,eventually we should call those methods and just save the results in this object
v0.11,instead of having this object invoke the estimator methods with the data.
v0.11,No estimand was identified (identification failed)
v0.11,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.11,"can't use nx.node_connected_component, because it doesn't work with DiGraphs."
v0.11,"Hence, a manual loop:"
v0.11,also return the number of backdoor paths blocked by observed nodes
v0.11,Assume that nodes1 is the treatment
v0.11,"ignores new_graph parameter, always uses self._graph"
v0.11,removal of only direct edges wrt a target is not implemented for incoming edges
v0.11,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.11,[TODO: double check these work with multivariate implementation:]
v0.11,Exclusion
v0.11,As-if-random setup
v0.11,As-if-random
v0.11,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.11,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.11,return len(dpaths) > 0
v0.11,Adding common causes
v0.11,Adding instruments
v0.11,Adding effect modifiers
v0.11,Assuming the simple form of effect modifier
v0.11,that directly causes the outcome.
v0.11,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.11,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.11,some preprocessing steps
v0.11,parsing the correct graph based on input graph format
v0.11,load dot file
v0.11,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.11,one-hot encode discrete W
v0.11,Now deleting the old continuous value
v0.11,create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause
v0.11,"Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately"
v0.11,One Hot Encoding
v0.11,TODO Ensure that we do not generate weak instruments
v0.11,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.11,Converting treatment to binary if required
v0.11,Generating frontdoor variables if asked for
v0.11,NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.
v0.11,"For categorical t, this example dataset has the effect size for category 2 being exactly"
v0.11,double the effect for category 1
v0.11,This could be changed at this stage by one-hot encoding t and using a custom beta that
v0.11,"sets a different effect for each category {0, 1, 2}"
v0.11,Computing ATE
v0.11,constructing column names for one-hot encoded discrete features
v0.11,Specifying the correct dtypes
v0.11,Now specifying the corresponding graph strings
v0.11,Now writing the gml graph
v0.11,creating data frame
v0.11,Specifying the correct dtypes
v0.11,Now specifying the corresponding graph strings
v0.11,Now writing the gml graph
v0.11,Adding edges between common causes and the frontdoor mediator
v0.11,Error terms
v0.11,else:
v0.11,V = 6 + W0 + tterm + E1
v0.11,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.11,Generating a random normal distribution of integers
v0.11,Generating data for nodes which have no incoming edges
v0.11,"""currset"" variable currently has all the successors of the nodes which had no incoming edges"
v0.11,assuming that all unobserved common causes are numerical and are not affected by one hot encoding
v0.11,Creating a NN to simulate the nuisance function
v0.11,strength of unobserved confounding
v0.11,Computing ATE
v0.11,Specifying the correct dtypes
v0.11,Now writing the gml graph
v0.11,The following code for loading the Lalonde dataset was copied from
v0.11,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.11,
v0.11,"Copyright 2018, Wayfair, Inc."
v0.11,
v0.11,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.11,the following conditions are met:
v0.11,
v0.11,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.11,following disclaimer.
v0.11,
v0.11,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.11,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.11,
v0.11,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.11,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.11,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.11,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.11,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.11,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.11,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.11,DAMAGE.
v0.11,The following code is a slight modification of
v0.11,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.11,
v0.11,"Copyright 2018, Wayfair, Inc."
v0.11,
v0.11,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.11,the following conditions are met:
v0.11,
v0.11,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.11,following disclaimer.
v0.11,
v0.11,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.11,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.11,
v0.11,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.11,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.11,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.11,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.11,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.11,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.11,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.11,DAMAGE.
v0.11,
v0.11,0.0.0 is standard placeholder for poetry-dynamic-versioning
v0.11,any changes to this should not be checked in
v0.11,
v0.11,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11,adapt number of channels
v0.11,save memory
v0.11,Keep same dimensions
v0.11,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11,single-attribute Causal
v0.11,test environment
v0.11,Subsample 2x for computational convenience
v0.11,Assign a binary label based on the digit
v0.11,Flip label with probability 0.25
v0.11,Assign a color based on the label; flip the color with probability environment
v0.11,Apply the color to the image by zeroing out the other color channel
v0.11,single-attribute Independent
v0.11,test environment
v0.11,Subsample 2x for computational convenience
v0.11,Assign a binary label based on the digit
v0.11,Flip label with probability 0.25
v0.11,multi-attribute Causal + Independent
v0.11,test environment
v0.11,Subsample 2x for computational convenience
v0.11,rotate the image by angle in parameter
v0.11,Assign a binary label based on the digit
v0.11,Flip label with probability 0.25
v0.11,Assign a color based on the label; flip the color with probability environment
v0.11,Apply the color to the image by zeroing out the other color channel
v0.11,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11,Acause regularization
v0.11,Aconf regularization
v0.11,Aind regularization
v0.11,Asel regularization
v0.11,Compile loss
v0.11,Check if the optimizer is currently supported
v0.11,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.11,The currently supported estimators
v0.11,The default standard deviation for noise
v0.11,The default scaling factor to determine the bucket size
v0.11,The minimum number of points for the estimator to run
v0.11,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.11,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.11,The Default split for the number of data points that fall into the training and validation sets
v0.11,Assuming that outcome is one-dimensional
v0.11,We need to change the identified estimand
v0.11,"We thus, make a copy. This is done as we don't want"
v0.11,to change the original DataFrame
v0.11,We use collections.OrderedDict to maintain the order in which the data is stored
v0.11,Check if we are using an estimator in the transformation list
v0.11,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.11,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.11,"loops. Thus, we can get different values everytime we get the estimator."
v0.11,for _ in range( self._num_simulations ):
v0.11,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.11,Adding an unobserved confounder if provided by the user
v0.11,We set X_train = 0 and outcome_train to be 0
v0.11,"Get the final outcome, after running through all the values in the transformation list"
v0.11,Check if the value of true effect has been already stored
v0.11,We use None as the key as we have no base category for this refutation
v0.11,As we currently support only one treatment
v0.11,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.11,Check if the value of true effect has been already stored
v0.11,This ensures that we calculate the causal effect only once.
v0.11,We use key_train as we map data with respect to the base category of the data
v0.11,As we currently support only one treatment
v0.11,Add h(t) to f(W) to get the dummy outcome
v0.11,We convert to ndarray for ease in indexing
v0.11,The data is of the form
v0.11,sim1: cat1 cat2 ... catn
v0.11,sim2: cat1 cat2 ... catn
v0.11,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.11,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.11,distribution of the refuter.
v0.11,True Causal Effect list
v0.11,Iterating through the refutation for each category
v0.11,We use string arguments to account for both 32 and 64 bit varaibles
v0.11,action for continuous variables
v0.11,Action for categorical variables
v0.11,Find the set difference for each row
v0.11,Choose one out of the remaining
v0.11,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.11,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.11,of the treatment to affect the outcome
v0.11,TODO: Check that the target estimand has backdoor variables?
v0.11,Standardizing the data
v0.11,Fit a model containing all confounders and compare predictions
v0.11,using all features compared to all features except a given
v0.11,confounder.
v0.11,Estimating the regression coefficient from standardized features to t
v0.11,"By default, return a plot with 10 points"
v0.11,consider 10 values of the effect of the unobserved confounder
v0.11,Standardizing the data
v0.11,Fit a model containing all confounders and compare predictions
v0.11,using all features compared to all features except a given
v0.11,confounder.
v0.11,"By default, return a plot with 10 points"
v0.11,consider 10 values of the effect of the unobserved confounder
v0.11,"By default, we add the effect of simulated confounder for treatment."
v0.11,But subtract it from outcome to create a negative correlation
v0.11,assuming that the original confounder's effect was positive on both.
v0.11,This is to remove the effect of the original confounder.
v0.11,"By default, we add the effect of simulated confounder for treatment."
v0.11,But subtract it from outcome to create a negative correlation
v0.11,assuming that the original confounder's effect was positive on both.
v0.11,This is to remove the effect of the original confounder.
v0.11,Obtaining the list of observed variables
v0.11,Taking a subset of the dataframe that has only observed variables
v0.11,Residuals from the outcome model obtained by fitting a linear model
v0.11,Residuals from the treatment model obtained by fitting a linear model
v0.11,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.11,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.11,Choosing a c_star based on the data.
v0.11,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.11,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.11,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.11,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.11,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.11,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.11,TODO (petergtz): This should introduce a Protocol defining the interface this returns instead of the concrete types
v0.11,"If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen"
v0.11,Get a 2D matrix of values
v0.11,"x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN"
v0.11,Store the values into the refute object
v0.11,Adding a label on the contour line for the original estimate
v0.11,Label every other level using strings
v0.11,Default value of the p value taken for the distribution
v0.11,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.11,Mean of the Normal Distribution
v0.11,Standard Deviation of the Normal Distribution
v0.11,Create a new column in the data by the name of placebo
v0.11,Sanity check the data
v0.11,only permute is supported for iv methods
v0.11,"For IV methods, the estimating_instrument_names should also be"
v0.11,changed. Create a copy to avoid modifying original object
v0.11,We need to change the identified estimand
v0.11,"We make a copy as a safety measure, we don't want to change the"
v0.11,original DataFrame
v0.11,Run refutation in parallel
v0.11,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.11,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.11,relationship between the treatment and the outcome.
v0.11,new estimator
v0.11,new effect estimate
v0.11,observed covariate E-value
v0.11,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.11,"if CI crosses null, set its E-value to 1"
v0.11,only report E-value for CI limit closer to null
v0.11,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.11,whether the DGP is assumed to be partially linear
v0.11,features are the observed confounders
v0.11,Now code for benchmarking using covariates begins
v0.11,R^2 of outcome with observed common causes and treatment
v0.11,R^2 of treatment with observed common causes
v0.11,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.11,Assuming that the difference in R2 is the same for wj and new unobserved confounder
v0.11,"for treatment,  Calpha is not a function of the partial R2. So we need a different assumption."
v0.11,Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder
v0.11,"(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment"
v0.11,wrt unobserved confounders in partial-linear models
v0.11,Do the support characterization
v0.11,Recover the samples that are in the support
v0.11,Assess overlap using propensity scores with cross-fitting
v0.11,Check if all supported units are considered to be in the overlap set
v0.11,"NOTE: The original paper implements both DNF and CNF rules, but for simplicity, this code only implements DNF rules"
v0.11,"For DNF rules, a sample is covered if *any* rule applies"
v0.11,whether the DGP is assumed to be partially linear
v0.11,can change this to allow default values that are same as the other parameter
v0.11,Strength of confounding that omitted variables generate in treatment regression
v0.11,computing the point estimate for the bounds
v0.11,common causes after removing the benchmark causes
v0.11,dataframe with treatment and observed common causes after removing benchmark causes
v0.11,R^2 of treatment with observed common causes removing benchmark causes
v0.11,return the variance of alpha_s
v0.11,R^2 of outcome with observed common causes and treatment after removing benchmark causes
v0.11,Obtaining theta_s (the obtained estimate)
v0.11,Creating numpy arrays
v0.11,Setting up cross-validation parameters
v0.11,"tuple of residuals from first stage estimation [0,1], and the confounders [2]"
v0.11,"We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation."
v0.11,Yres = Y - E[Y|W]
v0.11,E[Y|W] = f(x) + theta_s * E[T|W]
v0.11,Yres = Y - f(x) - theta_s * E[T|W]
v0.11,g(s) = theta_s * T + f(x)
v0.11,g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]
v0.11,g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]
v0.11,Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )
v0.11,Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres
v0.11,Y - g(s) = Yres - theta_s * Tres
v0.11,nu_2 is E[alpha_s^2]
v0.11,Now computing scores for finding the (1-a) confidence interval
v0.11,R^2 of treatment with observed common causes
v0.11,R^2 of outcome with treatment and observed common causes
v0.11,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.11,Partial R^2 of treatment after regressing over unobserved confounder and observed common causes
v0.11,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.11,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.11,Adding unadjusted point estimate
v0.11,Adding bounds to partial R^2 values for given strength of confounders
v0.11,Adding a new backdoor variable to the identified estimand
v0.11,Run refutation in parallel
v0.11,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.11,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.11,of the treatment to affect the outcome
v0.11,TODO: Sensitivity Analyzers excluded from list due to different return type
v0.11,Reject H0
v0.11,"a, b and c are all continuous variables"
v0.11,"a, b and c are all discrete variables"
v0.11,c is set of continuous and binary variables and
v0.11,1. either a and b is continuous and the other is binary
v0.11,2. both a and b are binary
v0.11,c is discrete and
v0.11,either a or b is continuous and the other is discrete
v0.11,a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information
v0.11,original_treatment_name: : stores original variable names for labelling
v0.11,common_causes_map : maps the original variable names to variable names in OLS regression
v0.11,benchmark_common_causes: stores variable names in terms of regression model variables
v0.11,original_benchmark_covariates: stores original variable names for labelling
v0.11,estimate: estimate of regression
v0.11,degree_of_freedom: degree of freedom of error in regression
v0.11,standard_error: standard error in regression
v0.11,t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.
v0.11,partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them
v0.11,"r2tu_w: partial R^2  of unobserved confounder ""u"" with treatment ""t"", after conditioning on observed covariates ""w"""
v0.11,"r2yu_tw: partial R^2  of unobserved confounder ""u"" with outcome ""y"", after conditioning on observed covariates ""w"" and treatment ""t"""
v0.11,"r2twj_w: partial R^2 of observed covariate wj with treatment ""t"", after conditioning on observed covariates ""w"" excluding wj"
v0.11,"r2ywj_tw:  partial R^2 of observed covariate wj with outcome ""y"", after conditioning on observed covariates ""w"" (excluding wj) and treatment ""t"""
v0.11,benchmarking_results: dataframe containing information about bounds and bias adjusted terms
v0.11,"stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic"
v0.11,partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment
v0.11,build a new regression model by considering treatment variables as outcome
v0.11,"r2twj_w is partial R^2 of covariate wj with treatment ""t"", after conditioning on covariates w(excluding wj)"
v0.11,"r2ywj_tw is partial R^2 of covariate wj with outcome ""y"", after conditioning on covariates w(excluding wj) and treatment ""t"""
v0.11,r2tu_w is the partial r^2 from regressing u on t after conditioning on w
v0.11,Compute bias adjusted terms
v0.11,Plotting the contour plot
v0.11,Adding contours
v0.11,Adding threshold contour line
v0.11,Adding unadjusted point estimate
v0.11,Adding bounds to partial R^2 values for given strength of confounders
v0.11,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11,The default subset of the data to be used
v0.11,Run refutation in parallel
v0.11,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.11,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.11,of the treatment to affect the outcome
v0.11,Parameters
v0.11,Bookkeeping
v0.11,Initialize estimators
v0.11,Convert to dataframe if not
v0.11,Format labels
v0.11,Sample from reference measure and construct features
v0.11,Add reference samples
v0.11,Binarize features (fit to data only)
v0.11,Fit estimator
v0.11,Store reference volume
v0.11,Construct features dataframe
v0.11,Construct features dataframe
v0.11,Iterate over columns
v0.11,"logging.info(""Using provided reference range for {}"".format(c))"
v0.11,number of unique values
v0.11,Constant column
v0.11,Binary column
v0.11,Ordinal column (seed = counter so not correlated)
v0.11,For get_params / set_params
v0.11,"Overlap / Support (y = +1), non-overlap (y = 0), and uniform background (y = -1) samples"
v0.11,"We should always have overlap samples, and either background or non-overlap samples"
v0.11,"This will throw an error if, for example, all samples are considered to"
v0.11,be in the overlap region
v0.11,"Initialize with empty and singleton conjunctions, i.e. X plus all-ones feature"
v0.11,Feature indicator and conjunction matrices
v0.11,Iteration counter
v0.11,Formulate master LP
v0.11,Variables
v0.11,Objective function (no penalty on empty conjunction)
v0.11,Constraints
v0.11,This gets activated for DNF
v0.11,Solve problem
v0.11,Extract dual variables
v0.11,Beam search for conjunctions with negative reduced cost
v0.11,Most negative reduced cost among current variables
v0.11,Negative reduced costs found
v0.11,Add to existing conjunctions
v0.11,Reformulate master LP
v0.11,Variables
v0.11,Objective function
v0.11,Constraints
v0.11,Solve problem
v0.11,Extract dual variables
v0.11,Beam search for conjunctions with negative reduced cost
v0.11,Most negative reduced cost among current variables
v0.11,"print('UB.min():', UB.min())"
v0.11,Save generated conjunctions and coefficients
v0.11,Restrict conjunctions to those used by LP
v0.11,"NOTE: This is a greedy approach, so it does not incorporate lambda0 explicitly"
v0.11,"Similarly, it will prefer a larger number of smaller rules if lambda1 is set"
v0.11,"to a larger value, because the incremental cost will be lower."
v0.11,Fraction of reference samples that each conjunction covers
v0.11,Regularization (for each conjunction)
v0.11,Positive samples newly covered (for each conjunction)
v0.11,Costs (for each conjunction)
v0.11,Zero out the rules and only take those which are used
v0.11,Small tolerance on comparisons
v0.11,This can be useful to break ties and favor larger values of xi
v0.11,Compute conjunctions of features
v0.11,Predict labels
v0.11,Use helper function
v0.11,Use helper function
v0.11,Lower bound specific to each singleton solution
v0.11,Initialize output
v0.11,Remove redundant rows by grouping by unique feature combinations and summing residual
v0.11,Initialize queue with root instance
v0.11,Separate data according to positive and negative residuals
v0.11,Iterate over increasing degree while queue is non-empty
v0.11,Initialize list of children to process
v0.11,Process instances in queue
v0.11,inst = instCurr[0]
v0.11,Evaluate all singleton solutions
v0.11,Best solutions that also improve on current output (allow for duplicate removal)
v0.11,Append to current output
v0.11,Remove duplicates
v0.11,Update output
v0.11,Compute lower bounds on higher-degree solutions
v0.11,Evaluate children using weighted average of their costs and LBs
v0.11,Best children with potential to improve on current output and current candidates (allow for duplicate removal)
v0.11,Iterate through best children
v0.11,"New ""zero"" solution"
v0.11,Check if duplicate
v0.11,Add to candidates for further processing
v0.11,Create pricing instance
v0.11,Remove covered rows
v0.11,Remove redundant features
v0.11,Track number of candidates added
v0.11,Update candidates
v0.11,Instances to process in next iteration
v0.11,Conjunctions corresponding to solutions
v0.11,List of categorical columns
v0.11,Number of quantile thresholds used to binarize ordinal features
v0.11,whether to append negations
v0.11,whether to convert thresholds on ordinal features to strings
v0.11,Quantile probabilities
v0.11,Initialize
v0.11,Iterate over columns
v0.11,number of unique values
v0.11,Constant or binary column
v0.11,"Mapping to 0, 1"
v0.11,Categorical column
v0.11,OneHotEncoder object
v0.11,Fit to observed categories
v0.11,Ordinal column
v0.11,Few unique values
v0.11,Thresholds are sorted unique values excluding maximum
v0.11,Many unique values
v0.11,Thresholds are quantiles excluding repetitions
v0.11,Contains NaN values
v0.11,Initialize dataframe
v0.11,Iterate over columns
v0.11,Constant or binary column
v0.11,"Rename values to 0, 1"
v0.11,Categorical column
v0.11,Apply OneHotEncoder
v0.11,Append negations
v0.11,Concatenate
v0.11,Ordinal column
v0.11,Threshold values to produce binary arrays
v0.11,Append negations
v0.11,Convert to dataframe with column labels
v0.11,Ensure that rows corresponding to NaN values are zeroed out
v0.11,Add NaN indicator column
v0.11,Concatenate
v0.11,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11,set attributions to zero for left out invariant nodes
v0.11,Get parent and child nodes
v0.11,Don't remove node if node has more than 1 children nodes as it can introduce
v0.11,hidden confounders.
v0.11,Remove the middle node
v0.11,Connect parent and child nodes
v0.11,Update the causal mechanism for the child nodes
v0.11,"If the relative change of the score is less than the given threshold, we stop the estimation early."
v0.11,"Note, the output of score_samples are log values."
v0.11,"Note, the output of score_samples are log values."
v0.11,Currently only support continuous distributions for auto selection.
v0.11,Estimate distribution parameters from data.
v0.11,Ignore warnings from fitting process.
v0.11,Fit distribution to data.
v0.11,Some distributions might not be compatible with the data.
v0.11,Separate parts of parameters.
v0.11,Check the KL divergence between the distribution of the given and fitted distribution.
v0.11,Identify if this distribution is better.
v0.11,This error is typically raised when the data is discrete and all points are assigned to less cluster than
v0.11,"specified. It can also happen due to duplicated points. In these cases, the current best solution should"
v0.11,be sufficient.
v0.11,"Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]"
v0.11,"Normally, we need to estimate the KL divergence jointly. However, to avoid issues with high dimensional data,"
v0.11,we approximate it by taking the average over the marginal KL divergences.
v0.11,Do not compare with same model class
v0.11,Do not compare with same model class
v0.11,"In case of post non-linear models, we can obtain the conditional expectation directly based on the prediction"
v0.11,"model. To do this, we can just in pass 0 as the noise, since this would evaluate Y = f(X) + 0 in case of an"
v0.11,additive noise model and Y = g(f(X) + 0) in case of a more general model.
v0.11,Estimate the conditional expectation E[Y | x] by generating multiple samples for Y|x and average them.
v0.11,"Since these are categorical values, we just need to look for the most frequent element after we drew"
v0.11,multiple samples for each input.
v0.11,"In the categorical case, this is equivalent to the Brier score. However, the following formulation allows"
v0.11,categorical data with more than two classes.
v0.11,A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This
v0.11,function evaluates the provided causal query multiple times to build a confidence interval based on the returned
v0.11,results.
v0.11,Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order
v0.11,"to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the"
v0.11,fit_and_compute function.
v0.11,
v0.11,**Example usage:**
v0.11,
v0.11,">>> gcm.fit(causal_model, data)"
v0.11,">>> strength_medians, strength_intervals = gcm.confidence_intervals("
v0.11,">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))"
v0.11,
v0.11,"In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the"
v0.11,"confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'"
v0.11,"here to configure the function call. In this case,"
v0.11,"gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to"
v0.11,"lambda : gcm.arrow_strength(causal_model, target_node='Y')."
v0.11,
v0.11,"In order to incorporate uncertainties coming from fitting the causal model(s), we can use"
v0.11,gcm.fit_and_compute instead:
v0.11,">>>  strength_medians, strength_intervals = gcm.confidence_intervals("
v0.11,">>>        gcm.fit_and_compute(gcm.arrow_strength,"
v0.11,">>>                                            causal_model,"
v0.11,">>>                                            bootstrap_training_data=data,"
v0.11,>>>                                            target_node='Y'))
v0.11,This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each
v0.11,run.
v0.11,"Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based"
v0.11,on their topological order.
v0.11,"After drawing samples of the node based on the data generation process, we apply the corresponding"
v0.11,intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this
v0.11,node.
v0.11,"Abduction: For invertible SCMs, we recover exact noise values from data."
v0.11,Action + Prediction: Propagate the intervention downstream using recovered noise values.
v0.11,Check if we need to apply an intervention on the given node.
v0.11,Apply intervention function to the data of the node.
v0.11,Check if the intervention function changes the shape of the data.
v0.11,"For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e."
v0.11,all ancestors of the target.
v0.11,The target node can be a continuous real-valued variable or a categorical variable with at most two classes
v0.11,(i.e. binary).
v0.11,Avoid too many features
v0.11,Making sure there are at least 30% test samples.
v0.11,"Making sure that there are at least 2 samples from one class (here, simply duplicate the point)."
v0.11,Compare number of correct classifications.
v0.11,"Making sure that X and Y have no overlapping values, which would lead to a distance of 0 with k=1 and, thus, to"
v0.11,a division by zero.
v0.11,All elements are equal (or at least less than k samples are different)
v0.11,Balance the classes
v0.11,For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples
v0.11,is unknown beforehand.
v0.11,The method stops if either the change between some consecutive runs is below the given threshold or the
v0.11,maximum number of runs is reached.
v0.11,"In each run, we create one random permutation of players. For instance, given 4 players, a permutation"
v0.11,"could be [3,1,4,2]."
v0.11,Generate k random permutations by sorting the indices of the Halton sequence
v0.11,"Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the"
v0.11,"subsets are [3], [3,1], [3,1,4] [3,1,4,2]."
v0.11,"The result for each subset is cached such that if a subset that has already been evaluated appears again,"
v0.11,we can take this result directly.
v0.11,"To improve the runtime, multiple permutations are evaluated in each run."
v0.11,"The current Shapley values are the average of the estimated values, i.e. we need to divide by the number"
v0.11,of generated permutations here.
v0.11,"In case Shapley values are estimated for multiple samples, e.g., in feature relevance. So, we have a"
v0.11,matrix of Shapley values instead of a vector.
v0.11,"Here, the change between consecutive runs is below the minimum threshold, but to reduce the"
v0.11,"likelihood that this just happened by chance, we require that this happens at least for"
v0.11,num_consecutive_converged_runs times in a row.
v0.11,Check if change in percentage is below threshold
v0.11,"Check for values that are exactly zero. If they don't change between two runs, we consider it as converging."
v0.11,Create all (unique) subsets)
v0.11,"Assigning a 'high' weight, since this resembles ""infinity""."
v0.11,The weight for a subset with a specific length (see paper mentioned in the docstring for more
v0.11,information).
v0.11,TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is
v0.11,"theoretically sound, i.e. make entropy results from different data comparable."
v0.11,Extremely small values can somehow result in negative values.
v0.11,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.11,Sampling from the conditional distribution based on the current sample.
v0.11,"Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that"
v0.11,"are in the given subset. By this, we can simulate the impact on the conditional distribution when removing"
v0.11,only the incoming edges of the variables in the subset.
v0.11,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.11,"In case of the full subset (no randomization), we get the same predictions as when we apply the"
v0.11,"prediction method to the samples of interest, since all noise samples are replaced with a sample of"
v0.11,interest.
v0.11,"In case of the empty subset (all are jointly randomize), it boils down to taking the average over all"
v0.11,"predictions, seeing that the randomization yields the same values for each sample of interest (none of the"
v0.11,samples of interest are used to replace a (jointly) 'randomized' sample).
v0.11,Exact model
v0.11,This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes
v0.11,This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes
v0.11,afterwards.
v0.11,Todo: Remove after https://github.com/py-why/dowhy/pull/943.
v0.11,Smallest possible value. This is used in various algorithm for numerical stability.
v0.11,Make copy to avoid manipulating the original matrix.
v0.11,"The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,"
v0.11,the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster
v0.11,"than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as"
v0.11,"possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples."
v0.11,"To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many"
v0.11,samples. The number of samples that are evaluated is normally
v0.11,"baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to"
v0.11,"batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is"
v0.11,evaluated one by one in a for-loop.
v0.11,Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple
v0.11,"batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet."
v0.11,"If the batch size would be larger than the remaining amount of samples, it is reduced to only include the"
v0.11,remaining baseline_noise_samples.
v0.11,"The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features"
v0.11,in baseline_feature_indices to their respective values in baseline_noise_samples.
v0.11,"After creating the (potentially large) input data matrix, we can evaluate the prediction method."
v0.11,"Here, offset + index now indicates the sample index in baseline_noise_samples."
v0.11,This would average all prediction results obtained for the 'offset + index'-th sample in
v0.11,"baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)]."
v0.11,This would return all prediction results obtained for the 'offset + index'-th sample in
v0.11,"baseline_noise_samples, i.e. the results are not averaged."
v0.11,Making copy to ensure that the original object is not modified.
v0.11,Permute samples jointly. This still represents an interventional distribution.
v0.11,Permute samples independently.
v0.11,Looks for the first index where the cumulative sum of the probabilities is larger than the threshold.
v0.11,"Note that if there are multiple indices with the same maximum value (as in this case here), the argmax"
v0.11,function returns the first index.
v0.11,"test local Markov condition, null hypothesis: conditional independence"
v0.11,"test edge dependence, null hypothesis: independence"
v0.11,The order of the p-values added to the list is deterministic.
v0.11,"To be able to validate that the graph structure did not change between fitting and causal query, we store the"
v0.11,"parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While"
v0.11,"this would automatically fail when the number of parents is different, there are other more subtle cases,"
v0.11,"where the number is still the same, but it's different parents, and therefore different data. That would yield"
v0.11,"wrong results, but would not fail."
v0.11,Independence tests are symmetric
v0.11,Find out which tests to do
v0.11,Parallelize over tests
v0.11,Gather results
v0.11,Summarize
v0.11,Find out which tests to do
v0.11,Parallelize over tests
v0.11,Gather results
v0.11,Summarize
v0.11,Find out which tests to do
v0.11,Parallelize over tests
v0.11,Gather results
v0.11,Summarize
v0.11,DAG Evaluation
v0.11,Suggestions
v0.11,"Append list of violations (node, non_desc) to get local information"
v0.11,Plot histograms
v0.11,Plot given violations
v0.11,For LMC we highlight X for which X _|/|_ Y \in ND_X | Pa_X
v0.11,For PD we highlight the edge (if Y\in Anc_X -> X are adjacent)
v0.11,For causal minimality we highlight the edge Y \in Pa_X -> X
v0.11,Create Validation header
v0.11,Create Validation summary
v0.11,Close Validation
v0.11,Create Suggestions header
v0.11,Iterate over suggestions
v0.11,Test if we have data for X and Y
v0.11,Test if we have data for Z
v0.11,Eq. (1) in https://arxiv.org/pdf/1810.11363.pdf
v0.11,Subtracting Y here since the cumulative sum includes the current element. The same reason we subtract 1
v0.11,from the count.
v0.11,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.11,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.11,"target quantity (here, variance)."
v0.11,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.11,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.11,"target quantity (here, variance)."
v0.11,"Calculate Ri, the product of the residuals"
v0.11,Standard deviation of the residuals
v0.11,Either X and/or Y is constant.
v0.11,"If Z is empty, we are in the pairwise setting."
v0.11,Either X and/or Y is constant.
v0.11,"If Z is empty, we are in the pairwise setting."
v0.11,Estimate test statistic multiple times on different permutations of the data. The p-value is then the
v0.11,probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted
v0.11,data.
v0.11,Take the lower dimensional variable as target.
v0.11,First stage statistical model
v0.11,Second stage statistical model
v0.11,Check if the treatment is one-dimensional
v0.11,First stage
v0.11,Second Stage
v0.11,Combining the two estimates
v0.11,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.11,Bulding the feature matrix
v0.11,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.11,TODO move this to the identification step
v0.11,Obtain estimate by Wald Estimator
v0.11,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.11,More than 1 instrument. Use 2sls.
v0.11,Checking if Y is binary
v0.11,Enable the user to pass params for a custom propensity model
v0.11,Convert the categorical variables into dummy/indicator variables
v0.11,"Basically, this gives a one hot encoding for each category"
v0.11,The first category is taken to be the base line.
v0.11,Check if the treatment is one-dimensional
v0.11,Checking if the treatment is binary
v0.11,The model is always built on the entire data
v0.11,TODO make treatment_value and control value also as local parameters
v0.11,All treatments are set to the same constant value
v0.11,"Fixing treatment value to the specified value, if provided"
v0.11,treatment_vals and data_df should have same number of rows
v0.11,Bulding the feature matrix
v0.11,The model is always built on the entire data
v0.11,Replacing treatment values by given x
v0.11,"First, create interventional tensor in original space"
v0.11,"Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment"
v0.11,The average treatment effect is a combination of different
v0.11,regression coefficients. Complicated to compute the confidence
v0.11,"interval analytically. For example, if y=a + b1.t + b2.tx, then"
v0.11,the average treatment effect is b1+b2.mean(x).
v0.11,"Refer Gelman, Hill. ARM Book. Chapter 9"
v0.11,http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
v0.11,TODO: Looking for contributions
v0.11,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.11,"variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0."
v0.11,"So for custom treatment and control values, we must multiply the confidence interval by the difference of the two."
v0.11,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.11,"variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0."
v0.11,"So for custom treatment and control values, we must multiply the standard error by the difference of the two."
v0.11,setting method-specific parameters
v0.11,Infer the right strata based on clipping threshold
v0.11,0.5 because there are two values for the treatment
v0.11,To be conservative and allow most strata to be included in the
v0.11,analysis
v0.11,At least 90% of the strata should be included in analysis
v0.11,sum weighted outcomes over all strata  (weight by treated population)
v0.11,TODO - how can we add additional information into the returned estimate?
v0.11,"such as how much clipping was done, or per-strata info for debugging?"
v0.11,sort the dataframe by propensity score
v0.11,create a column 'strata' for each element that marks what strata it belongs to
v0.11,"for each strata, count how many treated and control units there are"
v0.11,throw away strata that have insufficient treatment or control
v0.11,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.11,Setting method specific parameters
v0.11,trim propensity score weights
v0.11,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.11,nips ==> ips / (sum of ips over all units)
v0.11,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.11,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.11,Vanilla IPS estimator
v0.11,The Hajek estimator (or the self-normalized estimator)
v0.11,"Stabilized weights (from Robins, Hernan, Brumback (2000))"
v0.11,Paper: Marginal Structural Models and Causal Inference in Epidemiology
v0.11,Calculating the effect
v0.11,Subtracting the weighted means
v0.11,TODO - how can we add additional information into the returned estimate?
v0.11,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.11,Save parameters for later refutter fitting
v0.11,Enforcing this ordering is necessary to feed through the propensity values from dataset
v0.11,For metalearners only--issue a warning if w contains variables not in x
v0.11,Override the effect_modifiers set in CausalEstimator.__init__()
v0.11,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.11,the latter can be used by other estimator methods later
v0.11,"Instrumental variables names, if present"
v0.11,choosing the instrumental variable to use
v0.11,Calling the econml estimator's fit method
v0.11,"As of v0.9, econml has some kewyord only arguments"
v0.11,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11,Changing shape to a list for a singleton value
v0.11,Note that self._control_value is assumed to be a singleton value
v0.11,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.11,"For each unit, return the estimated effect of the treatment value"
v0.11,that was actually applied to the unit
v0.11,this assumes a binary treatment regime
v0.11,TODO remove neighbors that are more than a given radius apart
v0.11,estimate ATT on treated by summing over difference between matched neighbors
v0.11,Now computing ATC
v0.11,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.11,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11,Handle externally provided estimator classes
v0.11,allowed types of distance metric
v0.11,Dictionary of any user-provided params for the distance metric
v0.11,that will be passed to sklearn nearestneighbors
v0.11,Check if the treatment is one-dimensional
v0.11,Checking if the treatment is binary
v0.11,Convert the categorical variables into dummy/indicator variables
v0.11,"Basically, this gives a one hot encoding for each category"
v0.11,The first category is taken to be the base line.
v0.11,this assumes a binary treatment regime
v0.11,TODO remove neighbors that are more than a given radius apart
v0.11,estimate ATT on treated by summing over difference between matched neighbors
v0.11,Return indices in the original dataframe
v0.11,self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()
v0.11,Now computing ATC
v0.11,Return indices in the original dataframe
v0.11,Add the identification method used in the estimator
v0.11,Check the backdoor variables being used
v0.11,Add the observed confounders and one hot encode the categorical variables
v0.11,Get the data of the unobserved confounders
v0.11,One hot encode the data if they are categorical
v0.11,Check the instrumental variables involved
v0.11,Perform the same actions as the above
v0.11,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.11,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.11,For CATEs
v0.11,TODO we are conditioning on a postive treatment
v0.11,TODO create an expression corresponding to each estimator used
v0.11,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.11,Flipping some values
v0.11,Wrapping labels if they are too long
v0.11,This layout tries to mimic the graphviz layout in a simpler form. The depth grows horizontally here instead of
v0.11,vertically.
v0.11,Set the figure size based on the number of nodes
v0.11,"Nodes that are vertically connected, but not neighbors should be connected via a curved edge."
v0.11,All other nodes should be connected with a straight line.
v0.11,Draw labels node labels
v0.11,"Each node gets a depth assigned, based on the distance to the closest root node."
v0.11,"In case of undirected graphs, we just take any node as root node."
v0.11,"No path to root node, ignore this connection then."
v0.11,Counts the number of vertical nodes in the same layers.
v0.11,Creates a matrix indicating whether two nodes are vertical neighbors.
v0.11,Get all y coordinates per layer
v0.11,Sort the y-coordinates
v0.11,Finding p-value using student T test
v0.11,Only consider edges have absolute edge weight > 0.01
v0.11,Modify graph such that it only contains bidirected edges
v0.11,Find c components by finding connected components on the undirected graph
v0.11,Understanding Neural Network weights
v0.11,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.11,add weight column
v0.11,before weights are applied we count number rows in each category
v0.11,which is equivalent to summing over weight=1
v0.11,after weights are applied we need to sum over the given weights
v0.11,"First, calculating mean differences by strata"
v0.11,"Second, without strata"
v0.11,"Third, concatenating them and plotting"
v0.11,Setting estimator attribute for convenience
v0.11,Outcome is numeric
v0.11,Treatments are also numeric or binary
v0.11,Outcome is categorical
v0.11,Treatments are numeric or binary
v0.11,TODO: A common way to show all plots
v0.11,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11,Get adjacency list
v0.11,If node pair has been fully explored
v0.11,Add node1 to backdoor set of node_pair
v0.11,Check if path is backdoor and does not have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.11,"True if arrow incoming, False if arrow outgoing"
v0.11,"Mark pair (node1, node2) complete"
v0.11,Modify variable count and indices covered
v0.11,Average total effect
v0.11,Natural direct effect
v0.11,Natural indirect effect
v0.11,Controlled direct effect
v0.11,Backdoor method names
v0.11,"First, check if there is a directed path from action to outcome"
v0.11,## 1. BACKDOOR IDENTIFICATION
v0.11,Pick algorithm to compute backdoor sets according to method chosen
v0.11,"First, checking if there are any valid backdoor adjustment sets"
v0.11,"Setting default ""backdoor"" identification adjustment set"
v0.11,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.11,Now checking if there is also a valid iv estimand
v0.11,## 3. FRONTDOOR IDENTIFICATION
v0.11,Now checking if there is a valid frontdoor variable
v0.11,Finally returning the estimand object
v0.11,Pick algorithm to compute backdoor sets according to method chosen
v0.11,"Setting default ""backdoor"" identification adjustment set"
v0.11,Finally returning the estimand object
v0.11,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.11,"First, checking if there are any valid backdoor adjustment sets"
v0.11,"Setting default ""backdoor"" identification adjustment set"
v0.11,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.11,Now checking if there are valid mediator variables
v0.11,Finally returning the estimand object
v0.11,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.11,"First, checking if there are any valid backdoor adjustment sets"
v0.11,"Setting default ""backdoor"" identification adjustment set"
v0.11,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.11,Now checking if there are valid mediator variables
v0.11,Finally returning the estimand object
v0.11,"First, checking if empty set is a valid backdoor set"
v0.11,"If the method is `minimal-adjustment`, return the empty set right away."
v0.11,"Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible."
v0.11,only remove descendants of Y
v0.11,also allow any causes of Y that are not caused by T (for lower variance)
v0.11,remove descendants of T (mediators) and descendants of Y
v0.11,"If var is d-separated from both treatment or outcome, it cannot"
v0.11,be a part of the backdoor set
v0.11,repeat the above search with BACKDOOR_MIN
v0.11,"If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest."
v0.11,"If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set."
v0.11,"If all variables are observed, and the biggest eligible set"
v0.11,"does not satisfy backdoor, then none of its subsets will."
v0.11,Adding a None estimand if no backdoor set found
v0.11,"Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable."
v0.11,Default set is the one with the least number of adjustment variables (optimizing for efficiency)
v0.11,Cond 1: All directed paths intercepted by candidate_var
v0.11,Cond 2: No confounding between treatment and candidate var
v0.11,Cond 3: treatment blocks all confounding between candidate_var and outcome
v0.11,"For simplicity, assuming a one-variable mediation set"
v0.11,"Create estimands dict as per the API for backdoor, but do not return it"
v0.11,"Setting default ""backdoor"" identification adjustment set"
v0.11,"Create estimands dict as per the API for backdoor, but do not return it"
v0.11,"Setting default ""backdoor"" identification adjustment set"
v0.11,"TODO: outputs string for now, but ideally should do symbolic"
v0.11,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.11,TODO Better support for multivariate treatments
v0.11,TODO: support multivariate treatments better.
v0.11,TODO: support multivariate treatments better.
v0.11,TODO: support multivariate treatments better.
v0.11,For direct effect
v0.11,"If no costs are passed, use uniform costs"
v0.11,restriction to ancestors
v0.11,back-door graph
v0.11,moralization
v0.11,Estimators list for returning after identification
v0.11,Line 1
v0.11,"If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y."
v0.11,Line 2
v0.11,"If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y."
v0.11,Modify list of valid nodes
v0.11,Line 3 - forces an action on any node where such an action would have no effect on Y – assuming we already acted on X.
v0.11,Modify adjacency matrix to obtain that corresponding to do(X)
v0.11,Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.
v0.11,"If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases."
v0.11,Modify adjacency matrix to remove treatment variables
v0.11,"Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes."
v0.11,"Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem."
v0.11,Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.
v0.11,Do not show backdoor key unless it is the only backdoor set.
v0.11,Just show the default backdoor set
v0.11,If labels provided
v0.11,Return in valid DOT format
v0.11,Get adjacency matrix
v0.11,If labels not provided
v0.11,Obtain valid DOT format
v0.11,If labels provided
v0.11,Return in valid DOT format
v0.11,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.11,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10.1,-*- coding: utf-8 -*-
v0.10.1,
v0.10.1,Configuration file for the Sphinx documentation builder.
v0.10.1,
v0.10.1,This file does only contain a selection of the most common options. For a
v0.10.1,full list see the documentation:
v0.10.1,http://www.sphinx-doc.org/en/stable/config
v0.10.1,-- Path setup --------------------------------------------------------------
v0.10.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.10.1,add these directories to sys.path here. If the directory is relative to the
v0.10.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.10.1,
v0.10.1,-- Project information -----------------------------------------------------
v0.10.1,Version Information (for version-switcher)
v0.10.1,-- General configuration ---------------------------------------------------
v0.10.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.10.1,
v0.10.1,needs_sphinx = '1.0'
v0.10.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.10.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.10.1,ones.
v0.10.1,sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme
v0.10.1,already loads it
v0.10.1,"Add any paths that contain templates here, relative to this directory."
v0.10.1,The suffix(es) of source filenames.
v0.10.1,You can specify multiple suffix as a list of string:
v0.10.1,
v0.10.1,"source_suffix = ['.rst', '.md']"
v0.10.1,The master toctree document.
v0.10.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.10.1,for a list of supported languages.
v0.10.1,
v0.10.1,This is also used if you do content translation via gettext catalogs.
v0.10.1,"Usually you set ""language"" from the command line for these cases."
v0.10.1,"List of patterns, relative to source directory, that match files and"
v0.10.1,directories to ignore when looking for source files.
v0.10.1,This pattern also affects html_static_path and html_extra_path .
v0.10.1,The name of the Pygments (syntax highlighting) style to use.
v0.10.1,-- Options for HTML output -------------------------------------------------
v0.10.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.10.1,a list of builtin themes.
v0.10.1,
v0.10.1,Theme options are theme-specific and customize the look and feel of a theme
v0.10.1,"further.  For a list of options available for each theme, see the"
v0.10.1,documentation.
v0.10.1,
v0.10.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.10.1,"relative to this directory. They are copied after the builtin static files,"
v0.10.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.10.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.10.1,to template names.
v0.10.1,
v0.10.1,The default sidebars (for documents that don't match any pattern) are
v0.10.1,defined by theme itself.  Builtin themes are using these templates by
v0.10.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.10.1,'searchbox.html']``.
v0.10.1,
v0.10.1,html_sidebars = {}
v0.10.1,-- Options for HTMLHelp output ---------------------------------------------
v0.10.1,Output file base name for HTML help builder.
v0.10.1,-- Options for LaTeX output ------------------------------------------------
v0.10.1,The paper size ('letterpaper' or 'a4paper').
v0.10.1,
v0.10.1,"'papersize': 'letterpaper',"
v0.10.1,"The font size ('10pt', '11pt' or '12pt')."
v0.10.1,
v0.10.1,"'pointsize': '10pt',"
v0.10.1,Additional stuff for the LaTeX preamble.
v0.10.1,
v0.10.1,"'preamble': '',"
v0.10.1,Latex figure (float) alignment
v0.10.1,
v0.10.1,"'figure_align': 'htbp',"
v0.10.1,Grouping the document tree into LaTeX files. List of tuples
v0.10.1,"(source start file, target name, title,"
v0.10.1,"author, documentclass [howto, manual, or own class])."
v0.10.1,-- Options for manual page output ------------------------------------------
v0.10.1,One entry per manual page. List of tuples
v0.10.1,"(source start file, name, description, authors, manual section)."
v0.10.1,-- Options for Texinfo output ----------------------------------------------
v0.10.1,Grouping the document tree into Texinfo files. List of tuples
v0.10.1,"(source start file, target name, title, author,"
v0.10.1,"dir menu entry, description, category)"
v0.10.1,-- Options for Epub output -------------------------------------------------
v0.10.1,Bibliographic Dublin Core info.
v0.10.1,The unique identifier of the text. This can be a ISBN number
v0.10.1,or the project homepage.
v0.10.1,
v0.10.1,epub_identifier = ''
v0.10.1,A unique identification for the text.
v0.10.1,
v0.10.1,epub_uid = ''
v0.10.1,A list of files that should not be packed into the epub file.
v0.10.1,-- Extension configuration -------------------------------------------------
v0.10.1,-- Options for todo extension ----------------------------------------------
v0.10.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.10.1,init docstrings should also be included in class
v0.10.1,Only uncomment for faster testing/building docs without compiling notebooks
v0.10.1,"nbsphinx_execute = ""never"""
v0.10.1,Patch all of the published versions
v0.10.1,check old RST version (<= v0.8)
v0.10.1,Remove old version links
v0.10.1,Append updated version links
v0.10.1,requires stdin input for identify in weighting sampler
v0.10.1,will be removed
v0.10.1,"applied notebook, not necessary to test each time"
v0.10.1,needs xgboost too
v0.10.1,Slow Notebooks
v0.10.1,"TODO: should probably move more notebooks here to ignore, because"
v0.10.1,most get tested by the documentation generation.
v0.10.1,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.10.1,can import dowhy
v0.10.1,"""--ExecutePreprocessor.timeout=600"","
v0.10.1,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.10.1,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.10.1,treated = self.df[self.df['z']==1]
v0.10.1,self.att = np.mean(treated['y1'] - treated['y0'])
v0.10.1,def test_average_treatment_effect(self):
v0.10.1,est_ate = 1
v0.10.1,bias = est_ate - self.ate
v0.10.1,print(bias)
v0.10.1,"self.assertAlmostEqual(self.ate, est_ate)"
v0.10.1,def test_average_treatment_effect_on_treated(self):
v0.10.1,est_att = 1
v0.10.1,self.att=1
v0.10.1,bias = est_att - self.att
v0.10.1,print(bias)
v0.10.1,"self.assertAlmostEqual(self.att, est_att)"
v0.10.1,removing two common causes
v0.10.1,removing two common causes
v0.10.1,removing two common causes
v0.10.1,removing two common causes
v0.10.1,"Remove graph variable with name ""W0"" from observed data."
v0.10.1,Ensure that a log record exists that provides a more detailed view
v0.10.1,of observed and unobserved graph variables (counts and variable names.)
v0.10.1,check if all partial R^2 values are between 0 and 1
v0.10.1,We calculate adjusted estimates for two sets of partial R^2 values.
v0.10.1,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.10.1,Creating a model with no unobserved confounders
v0.10.1,check if all partial R^2 values are between 0 and 1
v0.10.1,"for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)"
v0.10.1,Non Parametric estimator
v0.10.1,We calculate adjusted estimates for two sets of partial R^2 values.
v0.10.1,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10.1,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10.1,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10.1,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10.1,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10.1,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10.1,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10.1,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10.1,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.10.1,we patched figure plotting call to avoid drawing plots during tests
v0.10.1,We calculate adjusted estimates for two sets of partial R^2 values.
v0.10.1,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10.1,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10.1,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10.1,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10.1,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10.1,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10.1,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10.1,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10.1,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.10.1,we patched figure plotting call to avoid drawing plots during tests
v0.10.1,comparing test examples from R E-Value package
v0.10.1,check implementation of Observed Covariate E-value against R package
v0.10.1,The outcome is a linear function of the confounder
v0.10.1,"The slope is 1,2 and the intercept is 3"
v0.10.1,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.10.1,"TODO: Check directly for correct behavior, rather than checking the rules"
v0.10.1,"themselves, which can be non-deterministic (all the following are equivalent)"
v0.10.1,Supports user-provided dataset object
v0.10.1,To test if there are any exceptions
v0.10.1,To test if the estimate is identical if refutation parameters are zero
v0.10.1,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.10.1,"Ordinarily, we should expect this value to be zero."
v0.10.1,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.10.1,"Ordinarily, we should expect this value to be zero."
v0.10.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10.1,Only P(Y|T) should be present for test to succeed.
v0.10.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10.1,"Since undirected graph, identify effect must throw an error."
v0.10.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10.1,Compare with ground truth
v0.10.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10.1,Compare with ground truth
v0.10.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10.1,Compare with ground truth
v0.10.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10.1,Compare with ground truth
v0.10.1,Causal model initialization
v0.10.1,Obtain backdoor sets
v0.10.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.10.1,Causal model initialization
v0.10.1,Obtain backdoor sets
v0.10.1,Causal model initialization
v0.10.1,Obtain backdoor sets
v0.10.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.10.1,Causal model initialization
v0.10.1,Obtain backdoor sets
v0.10.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.10.1,Causal model initialization
v0.10.1,Obtain backdoor sets
v0.10.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.10.1,Causal model initialization
v0.10.1,Obtain backdoor sets
v0.10.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.10.1,"Example is selected from Pearl J. ""Causality"" 2nd Edition, from chapter 3.3.1 on backoor criterion."
v0.10.1,"The following simpsons paradox examples are taken from Pearl, J {2013}. ""Understanding Simpson’s Paradox"" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf"
v0.10.1,"The following are examples given in the ""Book of Why"" by Judea Pearl, chapter ""The Do-operator and the Back-Door Criterion"""
v0.10.1,For all examples from these papers we use X for the treatment variable
v0.10.1,instead of A.
v0.10.1,"Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.10.1,"Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.10.1,"Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.10.1,"Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.10.1,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.10.1,L replaces X as the conditional variable
v0.10.1,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.10.1,L replaces X as the conditional variable. Uses different costs
v0.10.1,"Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.10.1,"A graph where optimal, optimal minimal and optimal min cost are different"
v0.10.1,The graph from Shrier and Platt (2008)
v0.10.1,A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.10.1,Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.10.1,cov_mat = np.diag(np.ones(num_features))
v0.10.1,collider: X->Z<-Y
v0.10.1,chain: X->Z->Y
v0.10.1,fork: X<-Z->Y
v0.10.1,"general DAG: X<-Z->Y, X->Y"
v0.10.1,fork: X<-Z->Y
v0.10.1,"Just checking formats, i.e. no need for correlation."
v0.10.1,"Just checking formats, i.e. no need for correlation."
v0.10.1,Contributions should add up to Var(X2)
v0.10.1,H(P(Y)) -- Can be precomputed
v0.10.1,-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))
v0.10.1,"H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\S)])"
v0.10.1,"E[P(Y | x_S, X'_\S)]"
v0.10.1,"H(E[P(Y | x_S, X'_\S)])"
v0.10.1,"Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions."
v0.10.1,E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]
v0.10.1,Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3
v0.10.1,"Just some random data, since we are only interested in the omitted data."
v0.10.1,This caused an error before with pandas > 2.0
v0.10.1,C2 = 3 * A2 + 2 * B2
v0.10.1,"By default, the strength is measure with respect to the variance."
v0.10.1,"Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2."
v0.10.1,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.10.1,Missing connection between X0 and X1.
v0.10.1,"For X0 and X1, we set the ground truth noise to further emphasize the misspecification. The inferred noise of X1"
v0.10.1,would otherwise have a dependency with Z due to the missing connection with X0.
v0.10.1,Modelling connection between X0 and X1 explicitly.
v0.10.1,"Here, we misspecify the mechanism on purpose by setting scale to 1 instead of 2."
v0.10.1,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.10.1,"Here, changing the mechanism."
v0.10.1,"Here, changing the mechanism."
v0.10.1,"Here, changing the mechanism."
v0.10.1,"Here, changing the mechanism."
v0.10.1,Defining an anomaly scorer that handles multidimensional inputs.
v0.10.1,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.10.1,"In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution"
v0.10.1,"and it should be ""significantly"" higher than the contribution of the other ones (here, we just arbitrarily say"
v0.10.1,"it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,"
v0.10.1,"which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate"
v0.10.1,"some robustness. Note that due to this and stochastic behaviour of the density estimator, it is"
v0.10.1,"not possible to analytically compute expected results. Therefore, we rather look at the relations here."
v0.10.1,"Same idea for the second sample, but here, it is the second variable that is anomalous."
v0.10.1,"In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be"
v0.10.1,"""significantly"" higher than the contribution of the other variables. The contribution of both anomalous variables"
v0.10.1,should be equal (approximately).
v0.10.1,Defining an anomaly scorer that handles multidimensional inputs.
v0.10.1,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.10.1,reduce the score.
v0.10.1,The contributions should add up to g(x) - E[g(X)]
v0.10.1,The contributions should add up to g(x) - E[g(X)]
v0.10.1,The contributions should add up to g(x) - E[g(X)]
v0.10.1,Three examples:
v0.10.1,1. X1 is the root cause (+ 10 to the noise)
v0.10.1,2. X0 is the root cause (+ 10 to the noise)
v0.10.1,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.10.1,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.10.1,Three examples:
v0.10.1,1. X1 is the root cause (+ 10 to the noise)
v0.10.1,2. X0 is the root cause (+ 10 to the noise)
v0.10.1,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.10.1,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.10.1,"Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution"
v0.10.1,algorithm.
v0.10.1,1. X0 is the root cause (+ 10 to the noise)
v0.10.1,2. X0 and X1 are the root causes (+ 10 to both noise)
v0.10.1,3. X2 and X3 are both root causes (+ 10 to both noises)
v0.10.1,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.10.1,"Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved."
v0.10.1,collider: X->Z<-Y
v0.10.1,collider: X->Z<-Y
v0.10.1,chain: X->Y->Z
v0.10.1,chain: X->Y->Z
v0.10.1,Empty graph
v0.10.1,Full DAG
v0.10.1,DAG with single node
v0.10.1,DAG with single edge
v0.10.1,DAG with single edge
v0.10.1,chain: X->Z->Y
v0.10.1,Setup data
v0.10.1,Test LinearDML
v0.10.1,Test ContinuousTreatmentOrthoForest
v0.10.1,Test LinearDRLearner
v0.10.1,Setup data
v0.10.1,Test DeepIV
v0.10.1,"Treatment model,"
v0.10.1,Response model
v0.10.1,Test IntentToTreatDRIV
v0.10.1,Observed data
v0.10.1,assumed graph
v0.10.1,Identify effect
v0.10.1,Estimate effect
v0.10.1,A model where X is also a common cause
v0.10.1,A model where X is also a common cause
v0.10.1,The case where effect modifier is not a common cause
v0.10.1,A model where X is also a common cause
v0.10.1,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.10.1,More cases where Exception  is expected
v0.10.1,"Compute confidence intervals, standard error and significance tests"
v0.10.1,Defined a linear dataset with a given set of properties
v0.10.1,Create a model that captures the same
v0.10.1,Identify the effects within the model
v0.10.1,Defined a linear dataset with a given set of properties
v0.10.1,Create a model that captures the same
v0.10.1,Identify the effects within the model
v0.10.1,Defined a linear dataset with a given set of properties
v0.10.1,Create a model that captures the same
v0.10.1,Identify the effects within the model
v0.10.1,Defined a linear dataset with a given set of properties
v0.10.1,Create a model that captures the same
v0.10.1,Identify the effects within the model
v0.10.1,Defined a linear dataset with a given set of properties
v0.10.1,Create a model that captures the same
v0.10.1,Identify the effects within the model
v0.10.1,Check if calling the method causes some import or runtime errors
v0.10.1,TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer
v0.10.1,newer matplotlib version:
v0.10.1,AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'
v0.10.1,Networkx 2.4+ should fix this issue.
v0.10.1,"plot_adjacency_matrix(causal_graph, is_directed=False)"
v0.10.1,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.10.1,Unpacking the keyword arguments
v0.10.1,todo: add docstring for common parameters here and remove from child refuter classes
v0.10.1,Default value for the number of simulations to be conducted
v0.10.1,joblib params for parallel processing
v0.10.1,"Concatenate the confounders, instruments and effect modifiers"
v0.10.1,Shuffle the confounders
v0.10.1,Check if all are select or deselect variables
v0.10.1,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.10.1,This calculates a two-sided percentile p-value
v0.10.1,See footnotes in https://journals.sagepub.com/doi/full/10.1177/2515245920911881
v0.10.1,Get the mean for the simulations
v0.10.1,Get the standard deviation for the simulations
v0.10.1,Get the Z Score [(val - mean)/ std_dev ]
v0.10.1,Initializing the p_value
v0.10.1,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.10.1,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.10.1,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.10.1,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.10.1,re.sub only takes string parameter so the first if is to avoid error
v0.10.1,"if the input is a text file, convert the contained data into string"
v0.10.1,load dot file
v0.10.1,Adding node attributes
v0.10.1,adding penwidth to make the edge bold
v0.10.1,Adding common causes
v0.10.1,Adding instruments
v0.10.1,Adding effect modifiers
v0.10.1,Assuming the simple form of effect modifier
v0.10.1,that directly causes the outcome.
v0.10.1,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.10.1,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.10.1,Adding columns in the dataframe as confounders that were not in the graph
v0.10.1,Adding unobserved confounders
v0.10.1,removal of only direct edges wrt a target is not implemented for incoming edges
v0.10.1,also return the number of backdoor paths blocked by observed nodes
v0.10.1,Assume that nodes1 is the treatment
v0.10.1,"ignores new_graph parameter, always uses self._graph"
v0.10.1,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.10.1,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.10.1,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.10.1,Return effect modifiers according to the graph
v0.10.1,removing all mediators
v0.10.1,"Also add any effect modifiers that could not be auto-detected (e.g., they are also common causes)"
v0.10.1,[TODO: double check these work with multivariate implementation:]
v0.10.1,Exclusion
v0.10.1,As-if-random setup
v0.10.1,As-if-random
v0.10.1,convert the outputted generator into a list
v0.10.1,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.10.1,return len(dpaths) > 0
v0.10.1,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.10.1,Emit a `UserWarning` if there are any unobserved graph variables and
v0.10.1,and log a message highlighting data variables that are not part of the graph.
v0.10.1,Create causal graph object
v0.10.1,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.10.1,(Because some effect modifiers may also be common causes)
v0.10.1,"In such cases, the user-provided modifiers are used."
v0.10.1,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.10.1,Import causal discovery class
v0.10.1,Initialize causal graph object
v0.10.1,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.10.1,TODO add dowhy as a prefix to all dowhy estimators
v0.10.1,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.10.1,Define the third-party estimation method to be used
v0.10.1,Process the dowhy estimators
v0.10.1,Check if estimator's target estimand is identified
v0.10.1,"Note that while the name of the variable is the same,"
v0.10.1,"""self.causal_estimator"", this estimator takes in less"
v0.10.1,parameters than the same from the
v0.10.1,estimate_effect code. It is not advisable to use the
v0.10.1,estimator from this function to call estimate_effect
v0.10.1,with fit_estimator=False.
v0.10.1,Estimator had been computed in a previous call
v0.10.1,The default number of simulations for statistical testing
v0.10.1,The default number of simulations to obtain confidence intervals
v0.10.1,This should be at least 399 for a 5% error rate:
v0.10.1,https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf
v0.10.1,The portion of the total size that should be taken each time to find the confidence intervals
v0.10.1,1 is the recommended value
v0.10.1,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.10.1,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.10.1,The default Confidence Level
v0.10.1,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.10.1,Prefix to add to temporary categorical variables created after discretization
v0.10.1,std args to be removed from locals() before being passed to args_dict
v0.10.1,Setting the default interpret method
v0.10.1,"Check if some parameters were set, otherwise set to default values"
v0.10.1,Estimate conditional estimates by default
v0.10.1,TODO Only works for binary treatment
v0.10.1,Defaulting to class default values if parameters are not provided
v0.10.1,Checking that there is at least one effect modifier
v0.10.1,Making sure that effect_modifier_names is a list
v0.10.1,Making a copy since we are going to be changing effect modifier names
v0.10.1,"For every numeric effect modifier, adding a temp categorical column"
v0.10.1,Grouping by effect modifiers and computing effect separately
v0.10.1,Deleting the temporary categorical columns
v0.10.1,The array that stores the results of all estimations
v0.10.1,Find the sample size the proportion with the population size
v0.10.1,Perform the set number of simulations
v0.10.1,names of treatment and outcome
v0.10.1,Using class default parameters if not specified
v0.10.1,Checking if bootstrap_estimates are already computed
v0.10.1,Checked if any parameter is changed from the previous std error estimate
v0.10.1,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.10.1,Get the variations of each bootstrap estimate and sort
v0.10.1,"Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level"
v0.10.1,Get the lower and upper bounds by subtracting the variations from the estimate
v0.10.1,"Use existing params, if new user defined params are not present"
v0.10.1,Checking if bootstrap_estimates are already computed
v0.10.1,Check if any parameter is changed from the previous std error estimate
v0.10.1,"Use existing params, if new user defined params are not present"
v0.10.1,Processing the null hypothesis estimates
v0.10.1,Doing a two-sided test
v0.10.1,Being conservative with the p-value reported
v0.10.1,Being conservative with the p-value reported
v0.10.1,"If the estimate_index is 0, it depends on the number of simulations"
v0.10.1,Need to test r-squared before supporting
v0.10.1,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.10.1,'r-squared': effect_r_squared
v0.10.1,"elif method == ""r-squared"":"
v0.10.1,outcome_mean = np.mean(self._outcome)
v0.10.1,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.10.1,Assuming a linear model with one variable: the treatment
v0.10.1,Currently only works for continuous y
v0.10.1,causal_model = outcome_mean + estimate.value*self._treatment
v0.10.1,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.10.1,r_squared = 1 - (squared_residual/total_variance)
v0.10.1,return r_squared
v0.10.1,Check if estimator's target estimand is identified
v0.10.1,Store parameters inside estimate object for refutation methods
v0.10.1,TODO: This add_params needs to move to the estimator class
v0.10.1,inside estimate_effect and estimate_conditional_effect
v0.10.1,"TODO: Remove _data, _treatment_name and _outcome_name from this object"
v0.10.1,we save them here to enable the methods that required these properties saved in the estimator
v0.10.1,eventually we should call those methods and just save the results in this object
v0.10.1,instead of having this object invoke the estimator methods with the data.
v0.10.1,No estimand was identified (identification failed)
v0.10.1,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.10.1,"can't use nx.node_connected_component, because it doesn't work with DiGraphs."
v0.10.1,"Hence, a manual loop:"
v0.10.1,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.10.1,one-hot encode discrete W
v0.10.1,Now deleting the old continuous value
v0.10.1,create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause
v0.10.1,"Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately"
v0.10.1,One Hot Encoding
v0.10.1,TODO Ensure that we do not generate weak instruments
v0.10.1,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.10.1,Converting treatment to binary if required
v0.10.1,Generating frontdoor variables if asked for
v0.10.1,NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.
v0.10.1,"For categorical t, this example dataset has the effect size for category 2 being exactly"
v0.10.1,double the effect for category 1
v0.10.1,This could be changed at this stage by one-hot encoding t and using a custom beta that
v0.10.1,"sets a different effect for each category {0, 1, 2}"
v0.10.1,Computing ATE
v0.10.1,constructing column names for one-hot encoded discrete features
v0.10.1,Specifying the correct dtypes
v0.10.1,Now specifying the corresponding graph strings
v0.10.1,Now writing the gml graph
v0.10.1,creating data frame
v0.10.1,Specifying the correct dtypes
v0.10.1,Now specifying the corresponding graph strings
v0.10.1,Now writing the gml graph
v0.10.1,Adding edges between common causes and the frontdoor mediator
v0.10.1,Error terms
v0.10.1,else:
v0.10.1,V = 6 + W0 + tterm + E1
v0.10.1,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.10.1,Generating a random normal distribution of integers
v0.10.1,Generating data for nodes which have no incoming edges
v0.10.1,"""currset"" variable currently has all the successors of the nodes which had no incoming edges"
v0.10.1,assuming that all unobserved common causes are numerical and are not affected by one hot encoding
v0.10.1,Creating a NN to simulate the nuisance function
v0.10.1,strength of unobserved confounding
v0.10.1,Computing ATE
v0.10.1,Specifying the correct dtypes
v0.10.1,Now writing the gml graph
v0.10.1,The following code for loading the Lalonde dataset was copied from
v0.10.1,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.10.1,
v0.10.1,"Copyright 2018, Wayfair, Inc."
v0.10.1,
v0.10.1,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.10.1,the following conditions are met:
v0.10.1,
v0.10.1,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.10.1,following disclaimer.
v0.10.1,
v0.10.1,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.10.1,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.10.1,
v0.10.1,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.10.1,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.10.1,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.10.1,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.10.1,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.10.1,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.10.1,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.10.1,DAMAGE.
v0.10.1,The following code is a slight modification of
v0.10.1,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.10.1,
v0.10.1,"Copyright 2018, Wayfair, Inc."
v0.10.1,
v0.10.1,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.10.1,the following conditions are met:
v0.10.1,
v0.10.1,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.10.1,following disclaimer.
v0.10.1,
v0.10.1,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.10.1,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.10.1,
v0.10.1,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.10.1,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.10.1,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.10.1,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.10.1,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.10.1,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.10.1,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.10.1,DAMAGE.
v0.10.1,
v0.10.1,0.0.0 is standard placeholder for poetry-dynamic-versioning
v0.10.1,any changes to this should not be checked in
v0.10.1,
v0.10.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10.1,adapt number of channels
v0.10.1,save memory
v0.10.1,Keep same dimensions
v0.10.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10.1,single-attribute Causal
v0.10.1,test environment
v0.10.1,Subsample 2x for computational convenience
v0.10.1,Assign a binary label based on the digit
v0.10.1,Flip label with probability 0.25
v0.10.1,Assign a color based on the label; flip the color with probability environment
v0.10.1,Apply the color to the image by zeroing out the other color channel
v0.10.1,single-attribute Independent
v0.10.1,test environment
v0.10.1,Subsample 2x for computational convenience
v0.10.1,Assign a binary label based on the digit
v0.10.1,Flip label with probability 0.25
v0.10.1,multi-attribute Causal + Independent
v0.10.1,test environment
v0.10.1,Subsample 2x for computational convenience
v0.10.1,rotate the image by angle in parameter
v0.10.1,Assign a binary label based on the digit
v0.10.1,Flip label with probability 0.25
v0.10.1,Assign a color based on the label; flip the color with probability environment
v0.10.1,Apply the color to the image by zeroing out the other color channel
v0.10.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10.1,Acause regularization
v0.10.1,Aconf regularization
v0.10.1,Aind regularization
v0.10.1,Asel regularization
v0.10.1,Compile loss
v0.10.1,Check if the optimizer is currently supported
v0.10.1,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10.1,The currently supported estimators
v0.10.1,The default standard deviation for noise
v0.10.1,The default scaling factor to determine the bucket size
v0.10.1,The minimum number of points for the estimator to run
v0.10.1,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.10.1,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.10.1,The Default split for the number of data points that fall into the training and validation sets
v0.10.1,Assuming that outcome is one-dimensional
v0.10.1,We need to change the identified estimand
v0.10.1,"We thus, make a copy. This is done as we don't want"
v0.10.1,to change the original DataFrame
v0.10.1,We use collections.OrderedDict to maintain the order in which the data is stored
v0.10.1,Check if we are using an estimator in the transformation list
v0.10.1,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.10.1,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.10.1,"loops. Thus, we can get different values everytime we get the estimator."
v0.10.1,for _ in range( self._num_simulations ):
v0.10.1,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.10.1,Adding an unobserved confounder if provided by the user
v0.10.1,We set X_train = 0 and outcome_train to be 0
v0.10.1,"Get the final outcome, after running through all the values in the transformation list"
v0.10.1,Check if the value of true effect has been already stored
v0.10.1,We use None as the key as we have no base category for this refutation
v0.10.1,As we currently support only one treatment
v0.10.1,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.10.1,Check if the value of true effect has been already stored
v0.10.1,This ensures that we calculate the causal effect only once.
v0.10.1,We use key_train as we map data with respect to the base category of the data
v0.10.1,As we currently support only one treatment
v0.10.1,Add h(t) to f(W) to get the dummy outcome
v0.10.1,We convert to ndarray for ease in indexing
v0.10.1,The data is of the form
v0.10.1,sim1: cat1 cat2 ... catn
v0.10.1,sim2: cat1 cat2 ... catn
v0.10.1,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.10.1,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.10.1,distribution of the refuter.
v0.10.1,True Causal Effect list
v0.10.1,Iterating through the refutation for each category
v0.10.1,We use string arguments to account for both 32 and 64 bit varaibles
v0.10.1,action for continuous variables
v0.10.1,Action for categorical variables
v0.10.1,Find the set difference for each row
v0.10.1,Choose one out of the remaining
v0.10.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.10.1,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.10.1,of the treatment to affect the outcome
v0.10.1,TODO: Check that the target estimand has backdoor variables?
v0.10.1,Standardizing the data
v0.10.1,Fit a model containing all confounders and compare predictions
v0.10.1,using all features compared to all features except a given
v0.10.1,confounder.
v0.10.1,Estimating the regression coefficient from standardized features to t
v0.10.1,"By default, return a plot with 10 points"
v0.10.1,consider 10 values of the effect of the unobserved confounder
v0.10.1,Standardizing the data
v0.10.1,Fit a model containing all confounders and compare predictions
v0.10.1,using all features compared to all features except a given
v0.10.1,confounder.
v0.10.1,"By default, return a plot with 10 points"
v0.10.1,consider 10 values of the effect of the unobserved confounder
v0.10.1,"By default, we add the effect of simulated confounder for treatment."
v0.10.1,But subtract it from outcome to create a negative correlation
v0.10.1,assuming that the original confounder's effect was positive on both.
v0.10.1,This is to remove the effect of the original confounder.
v0.10.1,"By default, we add the effect of simulated confounder for treatment."
v0.10.1,But subtract it from outcome to create a negative correlation
v0.10.1,assuming that the original confounder's effect was positive on both.
v0.10.1,This is to remove the effect of the original confounder.
v0.10.1,Obtaining the list of observed variables
v0.10.1,Taking a subset of the dataframe that has only observed variables
v0.10.1,Residuals from the outcome model obtained by fitting a linear model
v0.10.1,Residuals from the treatment model obtained by fitting a linear model
v0.10.1,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.10.1,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.10.1,Choosing a c_star based on the data.
v0.10.1,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.10.1,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.10.1,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.10.1,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.10.1,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.10.1,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.10.1,TODO (petergtz): This should introduce a Protocol defining the interface this returns instead of the concrete types
v0.10.1,"If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen"
v0.10.1,Get a 2D matrix of values
v0.10.1,"x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN"
v0.10.1,Store the values into the refute object
v0.10.1,Adding a label on the contour line for the original estimate
v0.10.1,Label every other level using strings
v0.10.1,Default value of the p value taken for the distribution
v0.10.1,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.10.1,Mean of the Normal Distribution
v0.10.1,Standard Deviation of the Normal Distribution
v0.10.1,Create a new column in the data by the name of placebo
v0.10.1,Sanity check the data
v0.10.1,only permute is supported for iv methods
v0.10.1,"For IV methods, the estimating_instrument_names should also be"
v0.10.1,changed. Create a copy to avoid modifying original object
v0.10.1,We need to change the identified estimand
v0.10.1,"We make a copy as a safety measure, we don't want to change the"
v0.10.1,original DataFrame
v0.10.1,Run refutation in parallel
v0.10.1,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.10.1,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.10.1,relationship between the treatment and the outcome.
v0.10.1,new estimator
v0.10.1,new effect estimate
v0.10.1,observed covariate E-value
v0.10.1,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.10.1,"if CI crosses null, set its E-value to 1"
v0.10.1,only report E-value for CI limit closer to null
v0.10.1,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.10.1,whether the DGP is assumed to be partially linear
v0.10.1,features are the observed confounders
v0.10.1,Now code for benchmarking using covariates begins
v0.10.1,R^2 of outcome with observed common causes and treatment
v0.10.1,R^2 of treatment with observed common causes
v0.10.1,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.10.1,Assuming that the difference in R2 is the same for wj and new unobserved confounder
v0.10.1,"for treatment,  Calpha is not a function of the partial R2. So we need a different assumption."
v0.10.1,Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder
v0.10.1,"(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment"
v0.10.1,wrt unobserved confounders in partial-linear models
v0.10.1,Do the support characterization
v0.10.1,Recover the samples that are in the support
v0.10.1,Assess overlap using propensity scores with cross-fitting
v0.10.1,Check if all supported units are considered to be in the overlap set
v0.10.1,"NOTE: The original paper implements both DNF and CNF rules, but for simplicity, this code only implements DNF rules"
v0.10.1,"For DNF rules, a sample is covered if *any* rule applies"
v0.10.1,whether the DGP is assumed to be partially linear
v0.10.1,can change this to allow default values that are same as the other parameter
v0.10.1,Strength of confounding that omitted variables generate in treatment regression
v0.10.1,computing the point estimate for the bounds
v0.10.1,common causes after removing the benchmark causes
v0.10.1,dataframe with treatment and observed common causes after removing benchmark causes
v0.10.1,R^2 of treatment with observed common causes removing benchmark causes
v0.10.1,return the variance of alpha_s
v0.10.1,R^2 of outcome with observed common causes and treatment after removing benchmark causes
v0.10.1,Obtaining theta_s (the obtained estimate)
v0.10.1,Creating numpy arrays
v0.10.1,Setting up cross-validation parameters
v0.10.1,"tuple of residuals from first stage estimation [0,1], and the confounders [2]"
v0.10.1,"We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation."
v0.10.1,Yres = Y - E[Y|W]
v0.10.1,E[Y|W] = f(x) + theta_s * E[T|W]
v0.10.1,Yres = Y - f(x) - theta_s * E[T|W]
v0.10.1,g(s) = theta_s * T + f(x)
v0.10.1,g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]
v0.10.1,g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]
v0.10.1,Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )
v0.10.1,Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres
v0.10.1,Y - g(s) = Yres - theta_s * Tres
v0.10.1,nu_2 is E[alpha_s^2]
v0.10.1,Now computing scores for finding the (1-a) confidence interval
v0.10.1,R^2 of treatment with observed common causes
v0.10.1,R^2 of outcome with treatment and observed common causes
v0.10.1,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.10.1,Partial R^2 of treatment after regressing over unobserved confounder and observed common causes
v0.10.1,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.10.1,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.10.1,Adding unadjusted point estimate
v0.10.1,Adding bounds to partial R^2 values for given strength of confounders
v0.10.1,Adding a new backdoor variable to the identified estimand
v0.10.1,Run refutation in parallel
v0.10.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.10.1,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.10.1,of the treatment to affect the outcome
v0.10.1,TODO: Sensitivity Analyzers excluded from list due to different return type
v0.10.1,Reject H0
v0.10.1,"a, b and c are all continuous variables"
v0.10.1,"a, b and c are all discrete variables"
v0.10.1,c is set of continuous and binary variables and
v0.10.1,1. either a and b is continuous and the other is binary
v0.10.1,2. both a and b are binary
v0.10.1,c is discrete and
v0.10.1,either a or b is continuous and the other is discrete
v0.10.1,a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information
v0.10.1,original_treatment_name: : stores original variable names for labelling
v0.10.1,common_causes_map : maps the original variable names to variable names in OLS regression
v0.10.1,benchmark_common_causes: stores variable names in terms of regression model variables
v0.10.1,original_benchmark_covariates: stores original variable names for labelling
v0.10.1,estimate: estimate of regression
v0.10.1,degree_of_freedom: degree of freedom of error in regression
v0.10.1,standard_error: standard error in regression
v0.10.1,t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.
v0.10.1,partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them
v0.10.1,"r2tu_w: partial R^2  of unobserved confounder ""u"" with treatment ""t"", after conditioning on observed covariates ""w"""
v0.10.1,"r2yu_tw: partial R^2  of unobserved confounder ""u"" with outcome ""y"", after conditioning on observed covariates ""w"" and treatment ""t"""
v0.10.1,"r2twj_w: partial R^2 of observed covariate wj with treatment ""t"", after conditioning on observed covariates ""w"" excluding wj"
v0.10.1,"r2ywj_tw:  partial R^2 of observed covariate wj with outcome ""y"", after conditioning on observed covariates ""w"" (excluding wj) and treatment ""t"""
v0.10.1,benchmarking_results: dataframe containing information about bounds and bias adjusted terms
v0.10.1,"stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic"
v0.10.1,partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment
v0.10.1,build a new regression model by considering treatment variables as outcome
v0.10.1,"r2twj_w is partial R^2 of covariate wj with treatment ""t"", after conditioning on covariates w(excluding wj)"
v0.10.1,"r2ywj_tw is partial R^2 of covariate wj with outcome ""y"", after conditioning on covariates w(excluding wj) and treatment ""t"""
v0.10.1,r2tu_w is the partial r^2 from regressing u on t after conditioning on w
v0.10.1,Compute bias adjusted terms
v0.10.1,Plotting the contour plot
v0.10.1,Adding contours
v0.10.1,Adding threshold contour line
v0.10.1,Adding unadjusted point estimate
v0.10.1,Adding bounds to partial R^2 values for given strength of confounders
v0.10.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10.1,The default subset of the data to be used
v0.10.1,Run refutation in parallel
v0.10.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.10.1,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.10.1,of the treatment to affect the outcome
v0.10.1,Parameters
v0.10.1,Bookkeeping
v0.10.1,Initialize estimators
v0.10.1,Convert to dataframe if not
v0.10.1,Format labels
v0.10.1,Sample from reference measure and construct features
v0.10.1,Add reference samples
v0.10.1,Binarize features (fit to data only)
v0.10.1,Fit estimator
v0.10.1,Store reference volume
v0.10.1,Construct features dataframe
v0.10.1,Construct features dataframe
v0.10.1,Iterate over columns
v0.10.1,"logging.info(""Using provided reference range for {}"".format(c))"
v0.10.1,number of unique values
v0.10.1,Constant column
v0.10.1,Binary column
v0.10.1,Ordinal column (seed = counter so not correlated)
v0.10.1,For get_params / set_params
v0.10.1,"Overlap / Support (y = +1), non-overlap (y = 0), and uniform background (y = -1) samples"
v0.10.1,"We should always have overlap samples, and either background or non-overlap samples"
v0.10.1,"This will throw an error if, for example, all samples are considered to"
v0.10.1,be in the overlap region
v0.10.1,"Initialize with empty and singleton conjunctions, i.e. X plus all-ones feature"
v0.10.1,Feature indicator and conjunction matrices
v0.10.1,Iteration counter
v0.10.1,Formulate master LP
v0.10.1,Variables
v0.10.1,Objective function (no penalty on empty conjunction)
v0.10.1,Constraints
v0.10.1,This gets activated for DNF
v0.10.1,Solve problem
v0.10.1,Extract dual variables
v0.10.1,Beam search for conjunctions with negative reduced cost
v0.10.1,Most negative reduced cost among current variables
v0.10.1,Negative reduced costs found
v0.10.1,Add to existing conjunctions
v0.10.1,Reformulate master LP
v0.10.1,Variables
v0.10.1,Objective function
v0.10.1,Constraints
v0.10.1,Solve problem
v0.10.1,Extract dual variables
v0.10.1,Beam search for conjunctions with negative reduced cost
v0.10.1,Most negative reduced cost among current variables
v0.10.1,"print('UB.min():', UB.min())"
v0.10.1,Save generated conjunctions and coefficients
v0.10.1,Restrict conjunctions to those used by LP
v0.10.1,"NOTE: This is a greedy approach, so it does not incorporate lambda0 explicitly"
v0.10.1,"Similarly, it will prefer a larger number of smaller rules if lambda1 is set"
v0.10.1,"to a larger value, because the incremental cost will be lower."
v0.10.1,Fraction of reference samples that each conjunction covers
v0.10.1,Regularization (for each conjunction)
v0.10.1,Positive samples newly covered (for each conjunction)
v0.10.1,Costs (for each conjunction)
v0.10.1,Zero out the rules and only take those which are used
v0.10.1,Small tolerance on comparisons
v0.10.1,This can be useful to break ties and favor larger values of xi
v0.10.1,Compute conjunctions of features
v0.10.1,Predict labels
v0.10.1,Use helper function
v0.10.1,Use helper function
v0.10.1,Lower bound specific to each singleton solution
v0.10.1,Initialize output
v0.10.1,Remove redundant rows by grouping by unique feature combinations and summing residual
v0.10.1,Initialize queue with root instance
v0.10.1,Separate data according to positive and negative residuals
v0.10.1,Iterate over increasing degree while queue is non-empty
v0.10.1,Initialize list of children to process
v0.10.1,Process instances in queue
v0.10.1,inst = instCurr[0]
v0.10.1,Evaluate all singleton solutions
v0.10.1,Best solutions that also improve on current output (allow for duplicate removal)
v0.10.1,Append to current output
v0.10.1,Remove duplicates
v0.10.1,Update output
v0.10.1,Compute lower bounds on higher-degree solutions
v0.10.1,Evaluate children using weighted average of their costs and LBs
v0.10.1,Best children with potential to improve on current output and current candidates (allow for duplicate removal)
v0.10.1,Iterate through best children
v0.10.1,"New ""zero"" solution"
v0.10.1,Check if duplicate
v0.10.1,Add to candidates for further processing
v0.10.1,Create pricing instance
v0.10.1,Remove covered rows
v0.10.1,Remove redundant features
v0.10.1,Track number of candidates added
v0.10.1,Update candidates
v0.10.1,Instances to process in next iteration
v0.10.1,Conjunctions corresponding to solutions
v0.10.1,List of categorical columns
v0.10.1,Number of quantile thresholds used to binarize ordinal features
v0.10.1,whether to append negations
v0.10.1,whether to convert thresholds on ordinal features to strings
v0.10.1,Quantile probabilities
v0.10.1,Initialize
v0.10.1,Iterate over columns
v0.10.1,number of unique values
v0.10.1,Constant or binary column
v0.10.1,"Mapping to 0, 1"
v0.10.1,Categorical column
v0.10.1,OneHotEncoder object
v0.10.1,Fit to observed categories
v0.10.1,Ordinal column
v0.10.1,Few unique values
v0.10.1,Thresholds are sorted unique values excluding maximum
v0.10.1,Many unique values
v0.10.1,Thresholds are quantiles excluding repetitions
v0.10.1,Contains NaN values
v0.10.1,Initialize dataframe
v0.10.1,Iterate over columns
v0.10.1,Constant or binary column
v0.10.1,"Rename values to 0, 1"
v0.10.1,Categorical column
v0.10.1,Apply OneHotEncoder
v0.10.1,Append negations
v0.10.1,Concatenate
v0.10.1,Ordinal column
v0.10.1,Threshold values to produce binary arrays
v0.10.1,Append negations
v0.10.1,Convert to dataframe with column labels
v0.10.1,Ensure that rows corresponding to NaN values are zeroed out
v0.10.1,Add NaN indicator column
v0.10.1,Concatenate
v0.10.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10.1,set attributions to zero for left out invariant nodes
v0.10.1,Get parent and child nodes
v0.10.1,Don't remove node if node has more than 1 children nodes as it can introduce
v0.10.1,hidden confounders.
v0.10.1,Remove the middle node
v0.10.1,Connect parent and child nodes
v0.10.1,Update the causal mechanism for the child nodes
v0.10.1,"If the relative change of the score is less than the given threshold, we stop the estimation early."
v0.10.1,"Note, the output of score_samples are log values."
v0.10.1,"Note, the output of score_samples are log values."
v0.10.1,Currently only support continuous distributions for auto selection.
v0.10.1,Estimate distribution parameters from data.
v0.10.1,Ignore warnings from fitting process.
v0.10.1,Fit distribution to data.
v0.10.1,Some distributions might not be compatible with the data.
v0.10.1,Separate parts of parameters.
v0.10.1,Check the KL divergence between the distribution of the given and fitted distribution.
v0.10.1,Identify if this distribution is better.
v0.10.1,This error is typically raised when the data is discrete and all points are assigned to less cluster than
v0.10.1,"specified. It can also happen due to duplicated points. In these cases, the current best solution should"
v0.10.1,be sufficient.
v0.10.1,"Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]"
v0.10.1,A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This
v0.10.1,function evaluates the provided causal query multiple times to build a confidence interval based on the returned
v0.10.1,results.
v0.10.1,Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order
v0.10.1,"to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the"
v0.10.1,fit_and_compute function.
v0.10.1,
v0.10.1,**Example usage:**
v0.10.1,
v0.10.1,">>> gcm.fit(causal_model, data)"
v0.10.1,">>> strength_medians, strength_intervals = gcm.confidence_intervals("
v0.10.1,">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))"
v0.10.1,
v0.10.1,"In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the"
v0.10.1,"confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'"
v0.10.1,"here to configure the function call. In this case,"
v0.10.1,"gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to"
v0.10.1,"lambda : gcm.arrow_strength(causal_model, target_node='Y')."
v0.10.1,
v0.10.1,"In order to incorporate uncertainties coming from fitting the causal model(s), we can use"
v0.10.1,gcm.fit_and_compute instead:
v0.10.1,">>>  strength_medians, strength_intervals = gcm.confidence_intervals("
v0.10.1,">>>        gcm.fit_and_compute(gcm.arrow_strength,"
v0.10.1,">>>                                            causal_model,"
v0.10.1,">>>                                            bootstrap_training_data=data,"
v0.10.1,>>>                                            target_node='Y'))
v0.10.1,This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each
v0.10.1,run.
v0.10.1,"Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based"
v0.10.1,on their topological order.
v0.10.1,"After drawing samples of the node based on the data generation process, we apply the corresponding"
v0.10.1,intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this
v0.10.1,node.
v0.10.1,"Abduction: For invertible SCMs, we recover exact noise values from data."
v0.10.1,Action + Prediction: Propagate the intervention downstream using recovered noise values.
v0.10.1,Check if we need to apply an intervention on the given node.
v0.10.1,Apply intervention function to the data of the node.
v0.10.1,Check if the intervention function changes the shape of the data.
v0.10.1,"For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e."
v0.10.1,all ancestors of the target.
v0.10.1,The target node can be a continuous real-valued variable or a categorical variable with at most two classes
v0.10.1,(i.e. binary).
v0.10.1,Avoid too many features
v0.10.1,Making sure there are at least 30% test samples.
v0.10.1,"Making sure that there are at least 2 samples from one class (here, simply duplicate the point)."
v0.10.1,Compare number of correct classifications.
v0.10.1,"Making sure that X and Y have no overlapping values, which would lead to a distance of 0 with k=1 and, thus, to"
v0.10.1,a division by zero.
v0.10.1,For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples
v0.10.1,is unknown beforehand.
v0.10.1,The method stops if either the change between some consecutive runs is below the given threshold or the
v0.10.1,maximum number of runs is reached.
v0.10.1,"In each run, we create one random permutation of players. For instance, given 4 players, a permutation"
v0.10.1,"could be [3,1,4,2]."
v0.10.1,Generate k random permutations by sorting the indices of the Halton sequence
v0.10.1,"Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the"
v0.10.1,"subsets are [3], [3,1], [3,1,4] [3,1,4,2]."
v0.10.1,"The result for each subset is cached such that if a subset that has already been evaluated appears again,"
v0.10.1,we can take this result directly.
v0.10.1,"To improve the runtime, multiple permutations are evaluated in each run."
v0.10.1,"The current Shapley values are the average of the estimated values, i.e. we need to divide by the number"
v0.10.1,of generated permutations here.
v0.10.1,"In case Shapley values are estimated for multiple samples, e.g., in feature relevance. So, we have a"
v0.10.1,matrix of Shapley values instead of a vector.
v0.10.1,"Here, the change between consecutive runs is below the minimum threshold, but to reduce the"
v0.10.1,"likelihood that this just happened by chance, we require that this happens at least for"
v0.10.1,num_consecutive_converged_runs times in a row.
v0.10.1,Check if change in percentage is below threshold
v0.10.1,"Check for values that are exactly zero. If they don't change between two runs, we consider it as converging."
v0.10.1,Create all (unique) subsets)
v0.10.1,"Assigning a 'high' weight, since this resembles ""infinity""."
v0.10.1,The weight for a subset with a specific length (see paper mentioned in the docstring for more
v0.10.1,information).
v0.10.1,TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is
v0.10.1,"theoretically sound, i.e. make entropy results from different data comparable."
v0.10.1,Extremely small values can somehow result in negative values.
v0.10.1,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.10.1,Sampling from the conditional distribution based on the current sample.
v0.10.1,"Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that"
v0.10.1,"are in the given subset. By this, we can simulate the impact on the conditional distribution when removing"
v0.10.1,only the incoming edges of the variables in the subset.
v0.10.1,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.10.1,"In case of the full subset (no randomization), we get the same predictions as when we apply the"
v0.10.1,"prediction method to the samples of interest, since all noise samples are replaced with a sample of"
v0.10.1,interest.
v0.10.1,"In case of the empty subset (all are jointly randomize), it boils down to taking the average over all"
v0.10.1,"predictions, seeing that the randomization yields the same values for each sample of interest (none of the"
v0.10.1,samples of interest are used to replace a (jointly) 'randomized' sample).
v0.10.1,Exact model
v0.10.1,This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes
v0.10.1,This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes
v0.10.1,afterwards.
v0.10.1,Smallest possible value. This is used in various algorithm for numerical stability.
v0.10.1,Make copy to avoid manipulating the original matrix.
v0.10.1,"The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,"
v0.10.1,the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster
v0.10.1,"than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as"
v0.10.1,"possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples."
v0.10.1,"To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many"
v0.10.1,samples. The number of samples that are evaluated is normally
v0.10.1,"baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to"
v0.10.1,"batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is"
v0.10.1,evaluated one by one in a for-loop.
v0.10.1,Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple
v0.10.1,"batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet."
v0.10.1,"If the batch size would be larger than the remaining amount of samples, it is reduced to only include the"
v0.10.1,remaining baseline_noise_samples.
v0.10.1,"The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features"
v0.10.1,in baseline_feature_indices to their respective values in baseline_noise_samples.
v0.10.1,"After creating the (potentially large) input data matrix, we can evaluate the prediction method."
v0.10.1,"Here, offset + index now indicates the sample index in baseline_noise_samples."
v0.10.1,This would average all prediction results obtained for the 'offset + index'-th sample in
v0.10.1,"baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)]."
v0.10.1,This would return all prediction results obtained for the 'offset + index'-th sample in
v0.10.1,"baseline_noise_samples, i.e. the results are not averaged."
v0.10.1,Making copy to ensure that the original object is not modified.
v0.10.1,Permute samples jointly. This still represents an interventional distribution.
v0.10.1,Permute samples independently.
v0.10.1,Looks for the first index where the cumulative sum of the probabilities is larger than the threshold.
v0.10.1,"Note that if there are multiple indices with the same maximum value (as in this case here), the argmax"
v0.10.1,function returns the first index.
v0.10.1,"test local Markov condition, null hypothesis: conditional independence"
v0.10.1,"test edge dependence, null hypothesis: independence"
v0.10.1,The order of the p-values added to the list is deterministic.
v0.10.1,"To be able to validate that the graph structure did not change between fitting and causal query, we store the"
v0.10.1,"parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While"
v0.10.1,"this would automatically fail when the number of parents is different, there are other more subtle cases,"
v0.10.1,"where the number is still the same, but it's different parents, and therefore different data. That would yield"
v0.10.1,"wrong results, but would not fail."
v0.10.1,Independence tests are symmetric
v0.10.1,Find out which tests to do
v0.10.1,Parallelize over tests
v0.10.1,Gather results
v0.10.1,Summarize
v0.10.1,Find out which tests to do
v0.10.1,Parallelize over tests
v0.10.1,Gather results
v0.10.1,Summarize
v0.10.1,Find out which tests to do
v0.10.1,Parallelize over tests
v0.10.1,Gather results
v0.10.1,Summarize
v0.10.1,DAG Evaluation
v0.10.1,Suggestions
v0.10.1,"Append list of violations (node, non_desc) to get local information"
v0.10.1,Plot histograms
v0.10.1,Plot given violations
v0.10.1,For LMC we highlight X for which X _|/|_ Y \in ND_X | Pa_X
v0.10.1,For PD we highlight the edge (if Y\in Anc_X -> X are adjacent)
v0.10.1,For causal minimality we highlight the edge Y \in Pa_X -> X
v0.10.1,Create Validation header
v0.10.1,Create Validation summary
v0.10.1,Close Validation
v0.10.1,Create Suggestions header
v0.10.1,Iterate over suggestions
v0.10.1,Test if we have data for X and Y
v0.10.1,Test if we have data for Z
v0.10.1,Eq. (1) in https://arxiv.org/pdf/1810.11363.pdf
v0.10.1,Subtracting Y here since the cumulative sum includes the current element. The same reason we subtract 1
v0.10.1,from the count.
v0.10.1,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.10.1,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.10.1,"target quantity (here, variance)."
v0.10.1,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.10.1,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.10.1,"target quantity (here, variance)."
v0.10.1,"Calculate Ri, the product of the residuals"
v0.10.1,Standard deviation of the residuals
v0.10.1,Either X and/or Y is constant.
v0.10.1,"If Z is empty, we are in the pairwise setting."
v0.10.1,Either X and/or Y is constant.
v0.10.1,"If Z is empty, we are in the pairwise setting."
v0.10.1,Estimate test statistic multiple times on different permutations of the data. The p-value is then the
v0.10.1,probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted
v0.10.1,data.
v0.10.1,Take the lower dimensional variable as target.
v0.10.1,First stage statistical model
v0.10.1,Second stage statistical model
v0.10.1,Check if the treatment is one-dimensional
v0.10.1,First stage
v0.10.1,Second Stage
v0.10.1,Combining the two estimates
v0.10.1,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.10.1,Bulding the feature matrix
v0.10.1,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.10.1,TODO move this to the identification step
v0.10.1,Obtain estimate by Wald Estimator
v0.10.1,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.10.1,More than 1 instrument. Use 2sls.
v0.10.1,Checking if Y is binary
v0.10.1,Enable the user to pass params for a custom propensity model
v0.10.1,Convert the categorical variables into dummy/indicator variables
v0.10.1,"Basically, this gives a one hot encoding for each category"
v0.10.1,The first category is taken to be the base line.
v0.10.1,Check if the treatment is one-dimensional
v0.10.1,Checking if the treatment is binary
v0.10.1,The model is always built on the entire data
v0.10.1,TODO make treatment_value and control value also as local parameters
v0.10.1,All treatments are set to the same constant value
v0.10.1,"Fixing treatment value to the specified value, if provided"
v0.10.1,treatment_vals and data_df should have same number of rows
v0.10.1,Bulding the feature matrix
v0.10.1,The model is always built on the entire data
v0.10.1,Replacing treatment values by given x
v0.10.1,"First, create interventional tensor in original space"
v0.10.1,"Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment"
v0.10.1,The average treatment effect is a combination of different
v0.10.1,regression coefficients. Complicated to compute the confidence
v0.10.1,"interval analytically. For example, if y=a + b1.t + b2.tx, then"
v0.10.1,the average treatment effect is b1+b2.mean(x).
v0.10.1,"Refer Gelman, Hill. ARM Book. Chapter 9"
v0.10.1,http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
v0.10.1,TODO: Looking for contributions
v0.10.1,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.10.1,"variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0."
v0.10.1,"So for custom treatment and control values, we must multiply the confidence interval by the difference of the two."
v0.10.1,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.10.1,"variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0."
v0.10.1,"So for custom treatment and control values, we must multiply the standard error by the difference of the two."
v0.10.1,setting method-specific parameters
v0.10.1,Infer the right strata based on clipping threshold
v0.10.1,0.5 because there are two values for the treatment
v0.10.1,To be conservative and allow most strata to be included in the
v0.10.1,analysis
v0.10.1,At least 90% of the strata should be included in analysis
v0.10.1,sum weighted outcomes over all strata  (weight by treated population)
v0.10.1,TODO - how can we add additional information into the returned estimate?
v0.10.1,"such as how much clipping was done, or per-strata info for debugging?"
v0.10.1,sort the dataframe by propensity score
v0.10.1,create a column 'strata' for each element that marks what strata it belongs to
v0.10.1,"for each strata, count how many treated and control units there are"
v0.10.1,throw away strata that have insufficient treatment or control
v0.10.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.10.1,Setting method specific parameters
v0.10.1,trim propensity score weights
v0.10.1,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.10.1,nips ==> ips / (sum of ips over all units)
v0.10.1,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.10.1,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.10.1,Vanilla IPS estimator
v0.10.1,The Hajek estimator (or the self-normalized estimator)
v0.10.1,"Stabilized weights (from Robins, Hernan, Brumback (2000))"
v0.10.1,Paper: Marginal Structural Models and Causal Inference in Epidemiology
v0.10.1,Calculating the effect
v0.10.1,Subtracting the weighted means
v0.10.1,TODO - how can we add additional information into the returned estimate?
v0.10.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.10.1,Save parameters for later refutter fitting
v0.10.1,Enforcing this ordering is necessary to feed through the propensity values from dataset
v0.10.1,For metalearners only--issue a warning if w contains variables not in x
v0.10.1,Override the effect_modifiers set in CausalEstimator.__init__()
v0.10.1,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.10.1,the latter can be used by other estimator methods later
v0.10.1,"Instrumental variables names, if present"
v0.10.1,choosing the instrumental variable to use
v0.10.1,Calling the econml estimator's fit method
v0.10.1,"As of v0.9, econml has some kewyord only arguments"
v0.10.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10.1,Changing shape to a list for a singleton value
v0.10.1,Note that self._control_value is assumed to be a singleton value
v0.10.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.10.1,"For each unit, return the estimated effect of the treatment value"
v0.10.1,that was actually applied to the unit
v0.10.1,this assumes a binary treatment regime
v0.10.1,TODO remove neighbors that are more than a given radius apart
v0.10.1,estimate ATT on treated by summing over difference between matched neighbors
v0.10.1,Now computing ATC
v0.10.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.10.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10.1,Handle externally provided estimator classes
v0.10.1,allowed types of distance metric
v0.10.1,Dictionary of any user-provided params for the distance metric
v0.10.1,that will be passed to sklearn nearestneighbors
v0.10.1,Check if the treatment is one-dimensional
v0.10.1,Checking if the treatment is binary
v0.10.1,Convert the categorical variables into dummy/indicator variables
v0.10.1,"Basically, this gives a one hot encoding for each category"
v0.10.1,The first category is taken to be the base line.
v0.10.1,this assumes a binary treatment regime
v0.10.1,TODO remove neighbors that are more than a given radius apart
v0.10.1,estimate ATT on treated by summing over difference between matched neighbors
v0.10.1,Return indices in the original dataframe
v0.10.1,self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()
v0.10.1,Now computing ATC
v0.10.1,Return indices in the original dataframe
v0.10.1,Add the identification method used in the estimator
v0.10.1,Check the backdoor variables being used
v0.10.1,Add the observed confounders and one hot encode the categorical variables
v0.10.1,Get the data of the unobserved confounders
v0.10.1,One hot encode the data if they are categorical
v0.10.1,Check the instrumental variables involved
v0.10.1,Perform the same actions as the above
v0.10.1,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.10.1,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.10.1,For CATEs
v0.10.1,TODO we are conditioning on a postive treatment
v0.10.1,TODO create an expression corresponding to each estimator used
v0.10.1,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.10.1,Flipping some values
v0.10.1,Wrapping labels if they are too long
v0.10.1,This layout tries to mimic the graphviz layout in a simpler form. The depth grows horizontally here instead of
v0.10.1,vertically.
v0.10.1,Set the figure size based on the number of nodes
v0.10.1,"Nodes that are vertically connected, but not neighbors should be connected via a curved edge."
v0.10.1,All other nodes should be connected with a straight line.
v0.10.1,Draw labels node labels
v0.10.1,"Each node gets a depth assigned, based on the distance to the closest root node."
v0.10.1,"In case of undirected graphs, we just take any node as root node."
v0.10.1,"No path to root node, ignore this connection then."
v0.10.1,Counts the number of vertical nodes in the same layers.
v0.10.1,Creates a matrix indicating whether two nodes are vertical neighbors.
v0.10.1,Get all y coordinates per layer
v0.10.1,Sort the y-coordinates
v0.10.1,Finding p-value using student T test
v0.10.1,Only consider edges have absolute edge weight > 0.01
v0.10.1,Modify graph such that it only contains bidirected edges
v0.10.1,Find c components by finding connected components on the undirected graph
v0.10.1,Understanding Neural Network weights
v0.10.1,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.10.1,add weight column
v0.10.1,before weights are applied we count number rows in each category
v0.10.1,which is equivalent to summing over weight=1
v0.10.1,after weights are applied we need to sum over the given weights
v0.10.1,"First, calculating mean differences by strata"
v0.10.1,"Second, without strata"
v0.10.1,"Third, concatenating them and plotting"
v0.10.1,Setting estimator attribute for convenience
v0.10.1,Outcome is numeric
v0.10.1,Treatments are also numeric or binary
v0.10.1,Outcome is categorical
v0.10.1,Treatments are numeric or binary
v0.10.1,TODO: A common way to show all plots
v0.10.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10.1,Get adjacency list
v0.10.1,If node pair has been fully explored
v0.10.1,Add node1 to backdoor set of node_pair
v0.10.1,Check if path is backdoor and does not have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.10.1,"True if arrow incoming, False if arrow outgoing"
v0.10.1,"Mark pair (node1, node2) complete"
v0.10.1,Modify variable count and indices covered
v0.10.1,Average total effect
v0.10.1,Natural direct effect
v0.10.1,Natural indirect effect
v0.10.1,Controlled direct effect
v0.10.1,Backdoor method names
v0.10.1,"First, check if there is a directed path from action to outcome"
v0.10.1,## 1. BACKDOOR IDENTIFICATION
v0.10.1,Pick algorithm to compute backdoor sets according to method chosen
v0.10.1,"First, checking if there are any valid backdoor adjustment sets"
v0.10.1,"Setting default ""backdoor"" identification adjustment set"
v0.10.1,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.10.1,Now checking if there is also a valid iv estimand
v0.10.1,## 3. FRONTDOOR IDENTIFICATION
v0.10.1,Now checking if there is a valid frontdoor variable
v0.10.1,Finally returning the estimand object
v0.10.1,Pick algorithm to compute backdoor sets according to method chosen
v0.10.1,"Setting default ""backdoor"" identification adjustment set"
v0.10.1,Finally returning the estimand object
v0.10.1,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.10.1,"First, checking if there are any valid backdoor adjustment sets"
v0.10.1,"Setting default ""backdoor"" identification adjustment set"
v0.10.1,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.10.1,Now checking if there are valid mediator variables
v0.10.1,Finally returning the estimand object
v0.10.1,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.10.1,"First, checking if there are any valid backdoor adjustment sets"
v0.10.1,"Setting default ""backdoor"" identification adjustment set"
v0.10.1,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.10.1,Now checking if there are valid mediator variables
v0.10.1,Finally returning the estimand object
v0.10.1,"First, checking if empty set is a valid backdoor set"
v0.10.1,"If the method is `minimal-adjustment`, return the empty set right away."
v0.10.1,"Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible."
v0.10.1,only remove descendants of Y
v0.10.1,also allow any causes of Y that are not caused by T (for lower variance)
v0.10.1,remove descendants of T (mediators) and descendants of Y
v0.10.1,"If var is d-separated from both treatment or outcome, it cannot"
v0.10.1,be a part of the backdoor set
v0.10.1,repeat the above search with BACKDOOR_MIN
v0.10.1,"If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest."
v0.10.1,"If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set."
v0.10.1,"If all variables are observed, and the biggest eligible set"
v0.10.1,"does not satisfy backdoor, then none of its subsets will."
v0.10.1,Adding a None estimand if no backdoor set found
v0.10.1,"Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable."
v0.10.1,Default set is the one with the least number of adjustment variables (optimizing for efficiency)
v0.10.1,"For simplicity, assuming a one-variable frontdoor set"
v0.10.1,Cond 1: All directed paths intercepted by candidate_var
v0.10.1,Cond 2: No confounding between treatment and candidate var
v0.10.1,Cond 3: treatment blocks all confounding between candidate_var and outcome
v0.10.1,"For simplicity, assuming a one-variable mediation set"
v0.10.1,"Create estimands dict as per the API for backdoor, but do not return it"
v0.10.1,"Setting default ""backdoor"" identification adjustment set"
v0.10.1,"Create estimands dict as per the API for backdoor, but do not return it"
v0.10.1,"Setting default ""backdoor"" identification adjustment set"
v0.10.1,"TODO: outputs string for now, but ideally should do symbolic"
v0.10.1,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.10.1,TODO Better support for multivariate treatments
v0.10.1,TODO: support multivariate treatments better.
v0.10.1,TODO: support multivariate treatments better.
v0.10.1,TODO: support multivariate treatments better.
v0.10.1,For direct effect
v0.10.1,"If no costs are passed, use uniform costs"
v0.10.1,restriction to ancestors
v0.10.1,back-door graph
v0.10.1,moralization
v0.10.1,Estimators list for returning after identification
v0.10.1,Line 1
v0.10.1,"If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y."
v0.10.1,Line 2
v0.10.1,"If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y."
v0.10.1,Modify list of valid nodes
v0.10.1,Line 3 - forces an action on any node where such an action would have no effect on Y – assuming we already acted on X.
v0.10.1,Modify adjacency matrix to obtain that corresponding to do(X)
v0.10.1,Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.
v0.10.1,"If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases."
v0.10.1,Modify adjacency matrix to remove treatment variables
v0.10.1,"Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes."
v0.10.1,"Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem."
v0.10.1,Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.
v0.10.1,Do not show backdoor key unless it is the only backdoor set.
v0.10.1,Just show the default backdoor set
v0.10.1,self._identified_estimand = self._causal_model.identify_effect()
v0.10.1,"self._identified_estimand,"
v0.10.1,"self._causal_model._treatment,"
v0.10.1,"self._causal_model._outcome,"
v0.10.1,If labels provided
v0.10.1,Return in valid DOT format
v0.10.1,Get adjacency matrix
v0.10.1,If labels not provided
v0.10.1,Obtain valid DOT format
v0.10.1,If labels provided
v0.10.1,Return in valid DOT format
v0.10.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10,-*- coding: utf-8 -*-
v0.10,
v0.10,Configuration file for the Sphinx documentation builder.
v0.10,
v0.10,This file does only contain a selection of the most common options. For a
v0.10,full list see the documentation:
v0.10,http://www.sphinx-doc.org/en/stable/config
v0.10,-- Path setup --------------------------------------------------------------
v0.10,"If extensions (or modules to document with autodoc) are in another directory,"
v0.10,add these directories to sys.path here. If the directory is relative to the
v0.10,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.10,
v0.10,-- Project information -----------------------------------------------------
v0.10,Version Information (for version-switcher)
v0.10,-- General configuration ---------------------------------------------------
v0.10,"If your documentation needs a minimal Sphinx version, state it here."
v0.10,
v0.10,needs_sphinx = '1.0'
v0.10,"Add any Sphinx extension module names here, as strings. They can be"
v0.10,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.10,ones.
v0.10,sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme
v0.10,already loads it
v0.10,"Add any paths that contain templates here, relative to this directory."
v0.10,The suffix(es) of source filenames.
v0.10,You can specify multiple suffix as a list of string:
v0.10,
v0.10,"source_suffix = ['.rst', '.md']"
v0.10,The master toctree document.
v0.10,The language for content autogenerated by Sphinx. Refer to documentation
v0.10,for a list of supported languages.
v0.10,
v0.10,This is also used if you do content translation via gettext catalogs.
v0.10,"Usually you set ""language"" from the command line for these cases."
v0.10,"List of patterns, relative to source directory, that match files and"
v0.10,directories to ignore when looking for source files.
v0.10,This pattern also affects html_static_path and html_extra_path .
v0.10,The name of the Pygments (syntax highlighting) style to use.
v0.10,-- Options for HTML output -------------------------------------------------
v0.10,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.10,a list of builtin themes.
v0.10,
v0.10,Theme options are theme-specific and customize the look and feel of a theme
v0.10,"further.  For a list of options available for each theme, see the"
v0.10,documentation.
v0.10,
v0.10,"Add any paths that contain custom static files (such as style sheets) here,"
v0.10,"relative to this directory. They are copied after the builtin static files,"
v0.10,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.10,"Custom sidebar templates, must be a dictionary that maps document names"
v0.10,to template names.
v0.10,
v0.10,The default sidebars (for documents that don't match any pattern) are
v0.10,defined by theme itself.  Builtin themes are using these templates by
v0.10,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.10,'searchbox.html']``.
v0.10,
v0.10,html_sidebars = {}
v0.10,-- Options for HTMLHelp output ---------------------------------------------
v0.10,Output file base name for HTML help builder.
v0.10,-- Options for LaTeX output ------------------------------------------------
v0.10,The paper size ('letterpaper' or 'a4paper').
v0.10,
v0.10,"'papersize': 'letterpaper',"
v0.10,"The font size ('10pt', '11pt' or '12pt')."
v0.10,
v0.10,"'pointsize': '10pt',"
v0.10,Additional stuff for the LaTeX preamble.
v0.10,
v0.10,"'preamble': '',"
v0.10,Latex figure (float) alignment
v0.10,
v0.10,"'figure_align': 'htbp',"
v0.10,Grouping the document tree into LaTeX files. List of tuples
v0.10,"(source start file, target name, title,"
v0.10,"author, documentclass [howto, manual, or own class])."
v0.10,-- Options for manual page output ------------------------------------------
v0.10,One entry per manual page. List of tuples
v0.10,"(source start file, name, description, authors, manual section)."
v0.10,-- Options for Texinfo output ----------------------------------------------
v0.10,Grouping the document tree into Texinfo files. List of tuples
v0.10,"(source start file, target name, title, author,"
v0.10,"dir menu entry, description, category)"
v0.10,-- Options for Epub output -------------------------------------------------
v0.10,Bibliographic Dublin Core info.
v0.10,The unique identifier of the text. This can be a ISBN number
v0.10,or the project homepage.
v0.10,
v0.10,epub_identifier = ''
v0.10,A unique identification for the text.
v0.10,
v0.10,epub_uid = ''
v0.10,A list of files that should not be packed into the epub file.
v0.10,-- Extension configuration -------------------------------------------------
v0.10,-- Options for todo extension ----------------------------------------------
v0.10,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.10,init docstrings should also be included in class
v0.10,Only uncomment for faster testing/building docs without compiling notebooks
v0.10,"nbsphinx_execute = ""never"""
v0.10,Patch all of the published versions
v0.10,check old RST version (<= v0.8)
v0.10,Remove old version links
v0.10,Append updated version links
v0.10,requires stdin input for identify in weighting sampler
v0.10,will be removed
v0.10,"applied notebook, not necessary to test each time"
v0.10,needs xgboost too
v0.10,Slow Notebooks
v0.10,"TODO: should probably move more notebooks here to ignore, because"
v0.10,most get tested by the documentation generation.
v0.10,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.10,can import dowhy
v0.10,"""--ExecutePreprocessor.timeout=600"","
v0.10,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.10,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.10,treated = self.df[self.df['z']==1]
v0.10,self.att = np.mean(treated['y1'] - treated['y0'])
v0.10,def test_average_treatment_effect(self):
v0.10,est_ate = 1
v0.10,bias = est_ate - self.ate
v0.10,print(bias)
v0.10,"self.assertAlmostEqual(self.ate, est_ate)"
v0.10,def test_average_treatment_effect_on_treated(self):
v0.10,est_att = 1
v0.10,self.att=1
v0.10,bias = est_att - self.att
v0.10,print(bias)
v0.10,"self.assertAlmostEqual(self.att, est_att)"
v0.10,removing two common causes
v0.10,removing two common causes
v0.10,removing two common causes
v0.10,removing two common causes
v0.10,"Remove graph variable with name ""W0"" from observed data."
v0.10,Ensure that a log record exists that provides a more detailed view
v0.10,of observed and unobserved graph variables (counts and variable names.)
v0.10,check if all partial R^2 values are between 0 and 1
v0.10,We calculate adjusted estimates for two sets of partial R^2 values.
v0.10,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.10,Creating a model with no unobserved confounders
v0.10,check if all partial R^2 values are between 0 and 1
v0.10,"for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)"
v0.10,Non Parametric estimator
v0.10,We calculate adjusted estimates for two sets of partial R^2 values.
v0.10,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.10,we patched figure plotting call to avoid drawing plots during tests
v0.10,We calculate adjusted estimates for two sets of partial R^2 values.
v0.10,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.10,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.10,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.10,we patched figure plotting call to avoid drawing plots during tests
v0.10,comparing test examples from R E-Value package
v0.10,check implementation of Observed Covariate E-value against R package
v0.10,The outcome is a linear function of the confounder
v0.10,"The slope is 1,2 and the intercept is 3"
v0.10,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.10,"TODO: Check directly for correct behavior, rather than checking the rules"
v0.10,"themselves, which can be non-deterministic (all the following are equivalent)"
v0.10,Supports user-provided dataset object
v0.10,To test if there are any exceptions
v0.10,To test if the estimate is identical if refutation parameters are zero
v0.10,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.10,"Ordinarily, we should expect this value to be zero."
v0.10,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.10,"Ordinarily, we should expect this value to be zero."
v0.10,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10,Only P(Y|T) should be present for test to succeed.
v0.10,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10,"Since undirected graph, identify effect must throw an error."
v0.10,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10,Compare with ground truth
v0.10,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10,Compare with ground truth
v0.10,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10,Compare with ground truth
v0.10,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.10,Compare with ground truth
v0.10,Causal model initialization
v0.10,Obtain backdoor sets
v0.10,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.10,Causal model initialization
v0.10,Obtain backdoor sets
v0.10,Causal model initialization
v0.10,Obtain backdoor sets
v0.10,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.10,Causal model initialization
v0.10,Obtain backdoor sets
v0.10,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.10,Causal model initialization
v0.10,Obtain backdoor sets
v0.10,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.10,Causal model initialization
v0.10,Obtain backdoor sets
v0.10,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.10,"Example is selected from Pearl J. ""Causality"" 2nd Edition, from chapter 3.3.1 on backoor criterion."
v0.10,"The following simpsons paradox examples are taken from Pearl, J {2013}. ""Understanding Simpson’s Paradox"" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf"
v0.10,"The following are examples given in the ""Book of Why"" by Judea Pearl, chapter ""The Do-operator and the Back-Door Criterion"""
v0.10,For all examples from these papers we use X for the treatment variable
v0.10,instead of A.
v0.10,"Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.10,"Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.10,"Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.10,"Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.10,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.10,L replaces X as the conditional variable
v0.10,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.10,L replaces X as the conditional variable. Uses different costs
v0.10,"Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.10,"A graph where optimal, optimal minimal and optimal min cost are different"
v0.10,The graph from Shrier and Platt (2008)
v0.10,A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.10,Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.10,cov_mat = np.diag(np.ones(num_features))
v0.10,collider: X->Z<-Y
v0.10,chain: X->Z->Y
v0.10,fork: X<-Z->Y
v0.10,"general DAG: X<-Z->Y, X->Y"
v0.10,fork: X<-Z->Y
v0.10,"Just checking formats, i.e. no need for correlation."
v0.10,"Just checking formats, i.e. no need for correlation."
v0.10,Contributions should add up to Var(X2)
v0.10,H(P(Y)) -- Can be precomputed
v0.10,-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))
v0.10,"H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\S)])"
v0.10,"E[P(Y | x_S, X'_\S)]"
v0.10,"H(E[P(Y | x_S, X'_\S)])"
v0.10,"Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions."
v0.10,E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]
v0.10,Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3
v0.10,"Just some random data, since we are only interested in the omitted data."
v0.10,C2 = 3 * A2 + 2 * B2
v0.10,"By default, the strength is measure with respect to the variance."
v0.10,"Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2."
v0.10,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.10,Missing connection between X0 and X1.
v0.10,"For X0 and X1, we set the ground truth noise to further emphasize the misspecification. The inferred noise of X1"
v0.10,would otherwise have a dependency with Z due to the missing connection with X0.
v0.10,Modelling connection between X0 and X1 explicitly.
v0.10,"Here, we misspecify the mechanism on purpose by setting scale to 1 instead of 2."
v0.10,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.10,"Here, changing the mechanism."
v0.10,"Here, changing the mechanism."
v0.10,"Here, changing the mechanism."
v0.10,"Here, changing the mechanism."
v0.10,Defining an anomaly scorer that handles multidimensional inputs.
v0.10,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.10,"In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution"
v0.10,"and it should be ""significantly"" higher than the contribution of the other ones (here, we just arbitrarily say"
v0.10,"it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,"
v0.10,"which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate"
v0.10,"some robustness. Note that due to this and stochastic behaviour of the density estimator, it is"
v0.10,"not possible to analytically compute expected results. Therefore, we rather look at the relations here."
v0.10,"Same idea for the second sample, but here, it is the second variable that is anomalous."
v0.10,"In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be"
v0.10,"""significantly"" higher than the contribution of the other variables. The contribution of both anomalous variables"
v0.10,should be equal (approximately).
v0.10,Defining an anomaly scorer that handles multidimensional inputs.
v0.10,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.10,reduce the score.
v0.10,The contributions should add up to g(x) - E[g(X)]
v0.10,The contributions should add up to g(x) - E[g(X)]
v0.10,The contributions should add up to g(x) - E[g(X)]
v0.10,Three examples:
v0.10,1. X1 is the root cause (+ 10 to the noise)
v0.10,2. X0 is the root cause (+ 10 to the noise)
v0.10,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.10,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.10,Three examples:
v0.10,1. X1 is the root cause (+ 10 to the noise)
v0.10,2. X0 is the root cause (+ 10 to the noise)
v0.10,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.10,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.10,"Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution"
v0.10,algorithm.
v0.10,1. X0 is the root cause (+ 10 to the noise)
v0.10,2. X0 and X1 are the root causes (+ 10 to both noise)
v0.10,3. X2 and X3 are both root causes (+ 10 to both noises)
v0.10,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.10,"Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved."
v0.10,collider: X->Z<-Y
v0.10,collider: X->Z<-Y
v0.10,chain: X->Y->Z
v0.10,chain: X->Y->Z
v0.10,Empty graph
v0.10,Full DAG
v0.10,DAG with single node
v0.10,DAG with single edge
v0.10,DAG with single edge
v0.10,chain: X->Z->Y
v0.10,Setup data
v0.10,Test LinearDML
v0.10,Test ContinuousTreatmentOrthoForest
v0.10,Test LinearDRLearner
v0.10,Setup data
v0.10,Test DeepIV
v0.10,"Treatment model,"
v0.10,Response model
v0.10,Test IntentToTreatDRIV
v0.10,Observed data
v0.10,assumed graph
v0.10,Identify effect
v0.10,Estimate effect
v0.10,A model where X is also a common cause
v0.10,A model where X is also a common cause
v0.10,The case where effect modifier is not a common cause
v0.10,A model where X is also a common cause
v0.10,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.10,More cases where Exception  is expected
v0.10,"Compute confidence intervals, standard error and significance tests"
v0.10,Defined a linear dataset with a given set of properties
v0.10,Create a model that captures the same
v0.10,Identify the effects within the model
v0.10,Defined a linear dataset with a given set of properties
v0.10,Create a model that captures the same
v0.10,Identify the effects within the model
v0.10,Defined a linear dataset with a given set of properties
v0.10,Create a model that captures the same
v0.10,Identify the effects within the model
v0.10,Defined a linear dataset with a given set of properties
v0.10,Create a model that captures the same
v0.10,Identify the effects within the model
v0.10,Defined a linear dataset with a given set of properties
v0.10,Create a model that captures the same
v0.10,Identify the effects within the model
v0.10,Check if calling the method causes some import or runtime errors
v0.10,TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer
v0.10,newer matplotlib version:
v0.10,AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'
v0.10,Networkx 2.4+ should fix this issue.
v0.10,"plot_adjacency_matrix(causal_graph, is_directed=False)"
v0.10,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.10,Unpacking the keyword arguments
v0.10,todo: add docstring for common parameters here and remove from child refuter classes
v0.10,Default value for the number of simulations to be conducted
v0.10,joblib params for parallel processing
v0.10,"Concatenate the confounders, instruments and effect modifiers"
v0.10,Shuffle the confounders
v0.10,Check if all are select or deselect variables
v0.10,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.10,This calculates a two-sided percentile p-value
v0.10,See footnotes in https://journals.sagepub.com/doi/full/10.1177/2515245920911881
v0.10,Get the mean for the simulations
v0.10,Get the standard deviation for the simulations
v0.10,Get the Z Score [(val - mean)/ std_dev ]
v0.10,Initializing the p_value
v0.10,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.10,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.10,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.10,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.10,re.sub only takes string parameter so the first if is to avoid error
v0.10,"if the input is a text file, convert the contained data into string"
v0.10,load dot file
v0.10,Adding node attributes
v0.10,adding penwidth to make the edge bold
v0.10,Adding common causes
v0.10,Adding instruments
v0.10,Adding effect modifiers
v0.10,Assuming the simple form of effect modifier
v0.10,that directly causes the outcome.
v0.10,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.10,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.10,Adding columns in the dataframe as confounders that were not in the graph
v0.10,Adding unobserved confounders
v0.10,removal of only direct edges wrt a target is not implemented for incoming edges
v0.10,also return the number of backdoor paths blocked by observed nodes
v0.10,Assume that nodes1 is the treatment
v0.10,"ignores new_graph parameter, always uses self._graph"
v0.10,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.10,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.10,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.10,Return effect modifiers according to the graph
v0.10,removing all mediators
v0.10,"Also add any effect modifiers that could not be auto-detected (e.g., they are also common causes)"
v0.10,[TODO: double check these work with multivariate implementation:]
v0.10,Exclusion
v0.10,As-if-random setup
v0.10,As-if-random
v0.10,convert the outputted generator into a list
v0.10,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.10,return len(dpaths) > 0
v0.10,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.10,Emit a `UserWarning` if there are any unobserved graph variables and
v0.10,and log a message highlighting data variables that are not part of the graph.
v0.10,Create causal graph object
v0.10,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.10,(Because some effect modifiers may also be common causes)
v0.10,"In such cases, the user-provided modifiers are used."
v0.10,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.10,Import causal discovery class
v0.10,Initialize causal graph object
v0.10,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.10,TODO add dowhy as a prefix to all dowhy estimators
v0.10,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.10,Define the third-party estimation method to be used
v0.10,Process the dowhy estimators
v0.10,Check if estimator's target estimand is identified
v0.10,"Note that while the name of the variable is the same,"
v0.10,"""self.causal_estimator"", this estimator takes in less"
v0.10,parameters than the same from the
v0.10,estimate_effect code. It is not advisable to use the
v0.10,estimator from this function to call estimate_effect
v0.10,with fit_estimator=False.
v0.10,Estimator had been computed in a previous call
v0.10,The default number of simulations for statistical testing
v0.10,The default number of simulations to obtain confidence intervals
v0.10,This should be at least 399 for a 5% error rate:
v0.10,https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf
v0.10,The portion of the total size that should be taken each time to find the confidence intervals
v0.10,1 is the recommended value
v0.10,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.10,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.10,The default Confidence Level
v0.10,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.10,Prefix to add to temporary categorical variables created after discretization
v0.10,std args to be removed from locals() before being passed to args_dict
v0.10,Setting the default interpret method
v0.10,"Check if some parameters were set, otherwise set to default values"
v0.10,Estimate conditional estimates by default
v0.10,TODO Only works for binary treatment
v0.10,Defaulting to class default values if parameters are not provided
v0.10,Checking that there is at least one effect modifier
v0.10,Making sure that effect_modifier_names is a list
v0.10,Making a copy since we are going to be changing effect modifier names
v0.10,"For every numeric effect modifier, adding a temp categorical column"
v0.10,Grouping by effect modifiers and computing effect separately
v0.10,Deleting the temporary categorical columns
v0.10,The array that stores the results of all estimations
v0.10,Find the sample size the proportion with the population size
v0.10,Perform the set number of simulations
v0.10,names of treatment and outcome
v0.10,Using class default parameters if not specified
v0.10,Checking if bootstrap_estimates are already computed
v0.10,Checked if any parameter is changed from the previous std error estimate
v0.10,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.10,Get the variations of each bootstrap estimate and sort
v0.10,"Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level"
v0.10,Get the lower and upper bounds by subtracting the variations from the estimate
v0.10,"Use existing params, if new user defined params are not present"
v0.10,Checking if bootstrap_estimates are already computed
v0.10,Check if any parameter is changed from the previous std error estimate
v0.10,"Use existing params, if new user defined params are not present"
v0.10,"self._outcome = self._data[""dummy_outcome""]"
v0.10,Processing the null hypothesis estimates
v0.10,Doing a two-sided test
v0.10,Being conservative with the p-value reported
v0.10,Being conservative with the p-value reported
v0.10,"If the estimate_index is 0, it depends on the number of simulations"
v0.10,Need to test r-squared before supporting
v0.10,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.10,'r-squared': effect_r_squared
v0.10,"elif method == ""r-squared"":"
v0.10,outcome_mean = np.mean(self._outcome)
v0.10,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.10,Assuming a linear model with one variable: the treatment
v0.10,Currently only works for continuous y
v0.10,causal_model = outcome_mean + estimate.value*self._treatment
v0.10,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.10,r_squared = 1 - (squared_residual/total_variance)
v0.10,return r_squared
v0.10,Check if estimator's target estimand is identified
v0.10,Store parameters inside estimate object for refutation methods
v0.10,TODO: This add_params needs to move to the estimator class
v0.10,inside estimate_effect and estimate_conditional_effect
v0.10,"TODO: Remove _data, _treatment_name and _outcome_name from this object"
v0.10,we save them here to enable the methods that required these properties saved in the estimator
v0.10,eventually we should call those methods and just save the results in this object
v0.10,instead of having this object invoke the estimator methods with the data.
v0.10,No estimand was identified (identification failed)
v0.10,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.10,"can't use nx.node_connected_component, because it doesn't work with DiGraphs."
v0.10,"Hence, a manual loop:"
v0.10,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.10,one-hot encode discrete W
v0.10,Now deleting the old continuous value
v0.10,create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause
v0.10,"Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately"
v0.10,One Hot Encoding
v0.10,TODO Ensure that we do not generate weak instruments
v0.10,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.10,Converting treatment to binary if required
v0.10,Generating frontdoor variables if asked for
v0.10,NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.
v0.10,"For categorical t, this example dataset has the effect size for category 2 being exactly"
v0.10,double the effect for category 1
v0.10,This could be changed at this stage by one-hot encoding t and using a custom beta that
v0.10,"sets a different effect for each category {0, 1, 2}"
v0.10,Computing ATE
v0.10,constructing column names for one-hot encoded discrete features
v0.10,Specifying the correct dtypes
v0.10,Now specifying the corresponding graph strings
v0.10,Now writing the gml graph
v0.10,creating data frame
v0.10,Specifying the correct dtypes
v0.10,Now specifying the corresponding graph strings
v0.10,Now writing the gml graph
v0.10,Adding edges between common causes and the frontdoor mediator
v0.10,Error terms
v0.10,else:
v0.10,V = 6 + W0 + tterm + E1
v0.10,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.10,Generating a random normal distribution of integers
v0.10,Generating data for nodes which have no incoming edges
v0.10,"""currset"" variable currently has all the successors of the nodes which had no incoming edges"
v0.10,assuming that all unobserved common causes are numerical and are not affected by one hot encoding
v0.10,Creating a NN to simulate the nuisance function
v0.10,strength of unobserved confounding
v0.10,Computing ATE
v0.10,Specifying the correct dtypes
v0.10,Now writing the gml graph
v0.10,The following code for loading the Lalonde dataset was copied from
v0.10,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.10,
v0.10,"Copyright 2018, Wayfair, Inc."
v0.10,
v0.10,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.10,the following conditions are met:
v0.10,
v0.10,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.10,following disclaimer.
v0.10,
v0.10,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.10,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.10,
v0.10,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.10,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.10,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.10,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.10,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.10,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.10,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.10,DAMAGE.
v0.10,The following code is a slight modification of
v0.10,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.10,
v0.10,"Copyright 2018, Wayfair, Inc."
v0.10,
v0.10,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.10,the following conditions are met:
v0.10,
v0.10,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.10,following disclaimer.
v0.10,
v0.10,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.10,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.10,
v0.10,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.10,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.10,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.10,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.10,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.10,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.10,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.10,DAMAGE.
v0.10,
v0.10,0.0.0 is standard placeholder for poetry-dynamic-versioning
v0.10,any changes to this should not be checked in
v0.10,
v0.10,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10,adapt number of channels
v0.10,save memory
v0.10,Keep same dimensions
v0.10,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10,single-attribute Causal
v0.10,test environment
v0.10,Subsample 2x for computational convenience
v0.10,Assign a binary label based on the digit
v0.10,Flip label with probability 0.25
v0.10,Assign a color based on the label; flip the color with probability environment
v0.10,Apply the color to the image by zeroing out the other color channel
v0.10,single-attribute Independent
v0.10,test environment
v0.10,Subsample 2x for computational convenience
v0.10,Assign a binary label based on the digit
v0.10,Flip label with probability 0.25
v0.10,multi-attribute Causal + Independent
v0.10,test environment
v0.10,Subsample 2x for computational convenience
v0.10,rotate the image by angle in parameter
v0.10,Assign a binary label based on the digit
v0.10,Flip label with probability 0.25
v0.10,Assign a color based on the label; flip the color with probability environment
v0.10,Apply the color to the image by zeroing out the other color channel
v0.10,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10,Acause regularization
v0.10,Aconf regularization
v0.10,Aind regularization
v0.10,Asel regularization
v0.10,Compile loss
v0.10,Check if the optimizer is currently supported
v0.10,"Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved"
v0.10,The currently supported estimators
v0.10,The default standard deviation for noise
v0.10,The default scaling factor to determine the bucket size
v0.10,The minimum number of points for the estimator to run
v0.10,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.10,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.10,The Default split for the number of data points that fall into the training and validation sets
v0.10,Assuming that outcome is one-dimensional
v0.10,We need to change the identified estimand
v0.10,"We thus, make a copy. This is done as we don't want"
v0.10,to change the original DataFrame
v0.10,We use collections.OrderedDict to maintain the order in which the data is stored
v0.10,Check if we are using an estimator in the transformation list
v0.10,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.10,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.10,"loops. Thus, we can get different values everytime we get the estimator."
v0.10,for _ in range( self._num_simulations ):
v0.10,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.10,Adding an unobserved confounder if provided by the user
v0.10,We set X_train = 0 and outcome_train to be 0
v0.10,"Get the final outcome, after running through all the values in the transformation list"
v0.10,Check if the value of true effect has been already stored
v0.10,We use None as the key as we have no base category for this refutation
v0.10,As we currently support only one treatment
v0.10,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.10,Check if the value of true effect has been already stored
v0.10,This ensures that we calculate the causal effect only once.
v0.10,We use key_train as we map data with respect to the base category of the data
v0.10,As we currently support only one treatment
v0.10,Add h(t) to f(W) to get the dummy outcome
v0.10,We convert to ndarray for ease in indexing
v0.10,The data is of the form
v0.10,sim1: cat1 cat2 ... catn
v0.10,sim2: cat1 cat2 ... catn
v0.10,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.10,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.10,distribution of the refuter.
v0.10,True Causal Effect list
v0.10,Iterating through the refutation for each category
v0.10,We use string arguments to account for both 32 and 64 bit varaibles
v0.10,action for continuous variables
v0.10,Action for categorical variables
v0.10,Find the set difference for each row
v0.10,Choose one out of the remaining
v0.10,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.10,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.10,of the treatment to affect the outcome
v0.10,TODO: Check that the target estimand has backdoor variables?
v0.10,Standardizing the data
v0.10,Fit a model containing all confounders and compare predictions
v0.10,using all features compared to all features except a given
v0.10,confounder.
v0.10,Estimating the regression coefficient from standardized features to t
v0.10,"By default, return a plot with 10 points"
v0.10,consider 10 values of the effect of the unobserved confounder
v0.10,Standardizing the data
v0.10,Fit a model containing all confounders and compare predictions
v0.10,using all features compared to all features except a given
v0.10,confounder.
v0.10,"By default, return a plot with 10 points"
v0.10,consider 10 values of the effect of the unobserved confounder
v0.10,"By default, we add the effect of simulated confounder for treatment."
v0.10,But subtract it from outcome to create a negative correlation
v0.10,assuming that the original confounder's effect was positive on both.
v0.10,This is to remove the effect of the original confounder.
v0.10,"By default, we add the effect of simulated confounder for treatment."
v0.10,But subtract it from outcome to create a negative correlation
v0.10,assuming that the original confounder's effect was positive on both.
v0.10,This is to remove the effect of the original confounder.
v0.10,Obtaining the list of observed variables
v0.10,Taking a subset of the dataframe that has only observed variables
v0.10,Residuals from the outcome model obtained by fitting a linear model
v0.10,Residuals from the treatment model obtained by fitting a linear model
v0.10,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.10,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.10,Choosing a c_star based on the data.
v0.10,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.10,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.10,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.10,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.10,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.10,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.10,TODO (petergtz): This should introduce a Protocol defining the interface this returns instead of the concrete types
v0.10,"If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen"
v0.10,Get a 2D matrix of values
v0.10,"x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN"
v0.10,Store the values into the refute object
v0.10,Adding a label on the contour line for the original estimate
v0.10,Label every other level using strings
v0.10,Default value of the p value taken for the distribution
v0.10,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.10,Mean of the Normal Distribution
v0.10,Standard Deviation of the Normal Distribution
v0.10,Create a new column in the data by the name of placebo
v0.10,Sanity check the data
v0.10,only permute is supported for iv methods
v0.10,"For IV methods, the estimating_instrument_names should also be"
v0.10,changed. Create a copy to avoid modifying original object
v0.10,We need to change the identified estimand
v0.10,"We make a copy as a safety measure, we don't want to change the"
v0.10,original DataFrame
v0.10,Run refutation in parallel
v0.10,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.10,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.10,relationship between the treatment and the outcome.
v0.10,new estimator
v0.10,new effect estimate
v0.10,observed covariate E-value
v0.10,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.10,"if CI crosses null, set its E-value to 1"
v0.10,only report E-value for CI limit closer to null
v0.10,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.10,whether the DGP is assumed to be partially linear
v0.10,features are the observed confounders
v0.10,Now code for benchmarking using covariates begins
v0.10,R^2 of outcome with observed common causes and treatment
v0.10,R^2 of treatment with observed common causes
v0.10,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.10,Assuming that the difference in R2 is the same for wj and new unobserved confounder
v0.10,"for treatment,  Calpha is not a function of the partial R2. So we need a different assumption."
v0.10,Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder
v0.10,"(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment"
v0.10,wrt unobserved confounders in partial-linear models
v0.10,Do the support characterization
v0.10,Recover the samples that are in the support
v0.10,Assess overlap using propensity scores with cross-fitting
v0.10,Check if all supported units are considered to be in the overlap set
v0.10,"NOTE: The original paper implements both DNF and CNF rules, but for simplicity, this code only implements DNF rules"
v0.10,"For DNF rules, a sample is covered if *any* rule applies"
v0.10,whether the DGP is assumed to be partially linear
v0.10,can change this to allow default values that are same as the other parameter
v0.10,Strength of confounding that omitted variables generate in treatment regression
v0.10,computing the point estimate for the bounds
v0.10,common causes after removing the benchmark causes
v0.10,dataframe with treatment and observed common causes after removing benchmark causes
v0.10,R^2 of treatment with observed common causes removing benchmark causes
v0.10,return the variance of alpha_s
v0.10,R^2 of outcome with observed common causes and treatment after removing benchmark causes
v0.10,Obtaining theta_s (the obtained estimate)
v0.10,Creating numpy arrays
v0.10,Setting up cross-validation parameters
v0.10,"tuple of residuals from first stage estimation [0,1], and the confounders [2]"
v0.10,"We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation."
v0.10,Yres = Y - E[Y|W]
v0.10,E[Y|W] = f(x) + theta_s * E[T|W]
v0.10,Yres = Y - f(x) - theta_s * E[T|W]
v0.10,g(s) = theta_s * T + f(x)
v0.10,g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]
v0.10,g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]
v0.10,Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )
v0.10,Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres
v0.10,Y - g(s) = Yres - theta_s * Tres
v0.10,nu_2 is E[alpha_s^2]
v0.10,Now computing scores for finding the (1-a) confidence interval
v0.10,R^2 of treatment with observed common causes
v0.10,R^2 of outcome with treatment and observed common causes
v0.10,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.10,Partial R^2 of treatment after regressing over unobserved confounder and observed common causes
v0.10,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.10,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.10,Adding unadjusted point estimate
v0.10,Adding bounds to partial R^2 values for given strength of confounders
v0.10,Adding a new backdoor variable to the identified estimand
v0.10,Run refutation in parallel
v0.10,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.10,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.10,of the treatment to affect the outcome
v0.10,TODO: Sensitivity Analyzers excluded from list due to different return type
v0.10,Reject H0
v0.10,"a, b and c are all continuous variables"
v0.10,"a, b and c are all discrete variables"
v0.10,c is set of continuous and binary variables and
v0.10,1. either a and b is continuous and the other is binary
v0.10,2. both a and b are binary
v0.10,c is discrete and
v0.10,either a or b is continuous and the other is discrete
v0.10,a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information
v0.10,original_treatment_name: : stores original variable names for labelling
v0.10,common_causes_map : maps the original variable names to variable names in OLS regression
v0.10,benchmark_common_causes: stores variable names in terms of regression model variables
v0.10,original_benchmark_covariates: stores original variable names for labelling
v0.10,estimate: estimate of regression
v0.10,degree_of_freedom: degree of freedom of error in regression
v0.10,standard_error: standard error in regression
v0.10,t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.
v0.10,partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them
v0.10,"r2tu_w: partial R^2  of unobserved confounder ""u"" with treatment ""t"", after conditioning on observed covariates ""w"""
v0.10,"r2yu_tw: partial R^2  of unobserved confounder ""u"" with outcome ""y"", after conditioning on observed covariates ""w"" and treatment ""t"""
v0.10,"r2twj_w: partial R^2 of observed covariate wj with treatment ""t"", after conditioning on observed covariates ""w"" excluding wj"
v0.10,"r2ywj_tw:  partial R^2 of observed covariate wj with outcome ""y"", after conditioning on observed covariates ""w"" (excluding wj) and treatment ""t"""
v0.10,benchmarking_results: dataframe containing information about bounds and bias adjusted terms
v0.10,"stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic"
v0.10,partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment
v0.10,build a new regression model by considering treatment variables as outcome
v0.10,"r2twj_w is partial R^2 of covariate wj with treatment ""t"", after conditioning on covariates w(excluding wj)"
v0.10,"r2ywj_tw is partial R^2 of covariate wj with outcome ""y"", after conditioning on covariates w(excluding wj) and treatment ""t"""
v0.10,r2tu_w is the partial r^2 from regressing u on t after conditioning on w
v0.10,Compute bias adjusted terms
v0.10,Plotting the contour plot
v0.10,Adding contours
v0.10,Adding threshold contour line
v0.10,Adding unadjusted point estimate
v0.10,Adding bounds to partial R^2 values for given strength of confounders
v0.10,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10,The default subset of the data to be used
v0.10,Run refutation in parallel
v0.10,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.10,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.10,of the treatment to affect the outcome
v0.10,Parameters
v0.10,Bookkeeping
v0.10,Initialize estimators
v0.10,Convert to dataframe if not
v0.10,Format labels
v0.10,Sample from reference measure and construct features
v0.10,Add reference samples
v0.10,Binarize features (fit to data only)
v0.10,Fit estimator
v0.10,Store reference volume
v0.10,Construct features dataframe
v0.10,Construct features dataframe
v0.10,Iterate over columns
v0.10,"logging.info(""Using provided reference range for {}"".format(c))"
v0.10,number of unique values
v0.10,Constant column
v0.10,Binary column
v0.10,Ordinal column (seed = counter so not correlated)
v0.10,For get_params / set_params
v0.10,"Overlap / Support (y = +1), non-overlap (y = 0), and uniform background (y = -1) samples"
v0.10,"We should always have overlap samples, and either background or non-overlap samples"
v0.10,"This will throw an error if, for example, all samples are considered to"
v0.10,be in the overlap region
v0.10,"Initialize with empty and singleton conjunctions, i.e. X plus all-ones feature"
v0.10,Feature indicator and conjunction matrices
v0.10,Iteration counter
v0.10,Formulate master LP
v0.10,Variables
v0.10,Objective function (no penalty on empty conjunction)
v0.10,Constraints
v0.10,This gets activated for DNF
v0.10,Solve problem
v0.10,Extract dual variables
v0.10,Beam search for conjunctions with negative reduced cost
v0.10,Most negative reduced cost among current variables
v0.10,Negative reduced costs found
v0.10,Add to existing conjunctions
v0.10,Reformulate master LP
v0.10,Variables
v0.10,Objective function
v0.10,Constraints
v0.10,Solve problem
v0.10,Extract dual variables
v0.10,Beam search for conjunctions with negative reduced cost
v0.10,Most negative reduced cost among current variables
v0.10,"print('UB.min():', UB.min())"
v0.10,Save generated conjunctions and coefficients
v0.10,Restrict conjunctions to those used by LP
v0.10,"NOTE: This is a greedy approach, so it does not incorporate lambda0 explicitly"
v0.10,"Similarly, it will prefer a larger number of smaller rules if lambda1 is set"
v0.10,"to a larger value, because the incremental cost will be lower."
v0.10,Fraction of reference samples that each conjunction covers
v0.10,Regularization (for each conjunction)
v0.10,Positive samples newly covered (for each conjunction)
v0.10,Costs (for each conjunction)
v0.10,Zero out the rules and only take those which are used
v0.10,Small tolerance on comparisons
v0.10,This can be useful to break ties and favor larger values of xi
v0.10,Compute conjunctions of features
v0.10,Predict labels
v0.10,Use helper function
v0.10,Use helper function
v0.10,Lower bound specific to each singleton solution
v0.10,Initialize output
v0.10,Remove redundant rows by grouping by unique feature combinations and summing residual
v0.10,Initialize queue with root instance
v0.10,Separate data according to positive and negative residuals
v0.10,Iterate over increasing degree while queue is non-empty
v0.10,Initialize list of children to process
v0.10,Process instances in queue
v0.10,inst = instCurr[0]
v0.10,Evaluate all singleton solutions
v0.10,Best solutions that also improve on current output (allow for duplicate removal)
v0.10,Append to current output
v0.10,Remove duplicates
v0.10,Update output
v0.10,Compute lower bounds on higher-degree solutions
v0.10,Evaluate children using weighted average of their costs and LBs
v0.10,Best children with potential to improve on current output and current candidates (allow for duplicate removal)
v0.10,Iterate through best children
v0.10,"New ""zero"" solution"
v0.10,Check if duplicate
v0.10,Add to candidates for further processing
v0.10,Create pricing instance
v0.10,Remove covered rows
v0.10,Remove redundant features
v0.10,Track number of candidates added
v0.10,Update candidates
v0.10,Instances to process in next iteration
v0.10,Conjunctions corresponding to solutions
v0.10,List of categorical columns
v0.10,Number of quantile thresholds used to binarize ordinal features
v0.10,whether to append negations
v0.10,whether to convert thresholds on ordinal features to strings
v0.10,Quantile probabilities
v0.10,Initialize
v0.10,Iterate over columns
v0.10,number of unique values
v0.10,Constant or binary column
v0.10,"Mapping to 0, 1"
v0.10,Categorical column
v0.10,OneHotEncoder object
v0.10,Fit to observed categories
v0.10,Ordinal column
v0.10,Few unique values
v0.10,Thresholds are sorted unique values excluding maximum
v0.10,Many unique values
v0.10,Thresholds are quantiles excluding repetitions
v0.10,Contains NaN values
v0.10,Initialize dataframe
v0.10,Iterate over columns
v0.10,Constant or binary column
v0.10,"Rename values to 0, 1"
v0.10,Categorical column
v0.10,Apply OneHotEncoder
v0.10,Append negations
v0.10,Concatenate
v0.10,Ordinal column
v0.10,Threshold values to produce binary arrays
v0.10,Append negations
v0.10,Convert to dataframe with column labels
v0.10,Ensure that rows corresponding to NaN values are zeroed out
v0.10,Add NaN indicator column
v0.10,Concatenate
v0.10,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10,"If the relative change of the score is less than the given threshold, we stop the estimation early."
v0.10,"Note, the output of score_samples are log values."
v0.10,"Note, the output of score_samples are log values."
v0.10,Currently only support continuous distributions for auto selection.
v0.10,Estimate distribution parameters from data.
v0.10,Ignore warnings from fitting process.
v0.10,Fit distribution to data.
v0.10,Some distributions might not be compatible with the data.
v0.10,Separate parts of parameters.
v0.10,Check the KL divergence between the distribution of the given and fitted distribution.
v0.10,Identify if this distribution is better.
v0.10,This error is typically raised when the data is discrete and all points are assigned to less cluster than
v0.10,"specified. It can also happen due to duplicated points. In these cases, the current best solution should"
v0.10,be sufficient.
v0.10,"Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]"
v0.10,A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This
v0.10,function evaluates the provided causal query multiple times to build a confidence interval based on the returned
v0.10,results.
v0.10,Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order
v0.10,"to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the"
v0.10,fit_and_compute function.
v0.10,
v0.10,**Example usage:**
v0.10,
v0.10,">>> gcm.fit(causal_model, data)"
v0.10,">>> strength_medians, strength_intervals = gcm.confidence_intervals("
v0.10,">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))"
v0.10,
v0.10,"In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the"
v0.10,"confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'"
v0.10,"here to configure the function call. In this case,"
v0.10,"gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to"
v0.10,"lambda : gcm.arrow_strength(causal_model, target_node='Y')."
v0.10,
v0.10,"In order to incorporate uncertainties coming from fitting the causal model(s), we can use"
v0.10,gcm.fit_and_compute instead:
v0.10,">>>  strength_medians, strength_intervals = gcm.confidence_intervals("
v0.10,">>>        gcm.fit_and_compute(gcm.arrow_strength,"
v0.10,">>>                                            causal_model,"
v0.10,">>>                                            bootstrap_training_data=data,"
v0.10,>>>                                            target_node='Y'))
v0.10,This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each
v0.10,run.
v0.10,"Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based"
v0.10,on their topological order.
v0.10,"After drawing samples of the node based on the data generation process, we apply the corresponding"
v0.10,intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this
v0.10,node.
v0.10,"Abduction: For invertible SCMs, we recover exact noise values from data."
v0.10,Action + Prediction: Propagate the intervention downstream using recovered noise values.
v0.10,Check if we need to apply an intervention on the given node.
v0.10,Apply intervention function to the data of the node.
v0.10,Check if the intervention function changes the shape of the data.
v0.10,"For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e."
v0.10,all ancestors of the target.
v0.10,The target node can be a continuous real-valued variable or a categorical variable with at most two classes
v0.10,(i.e. binary).
v0.10,Avoid too many features
v0.10,Making sure there are at least 30% test samples.
v0.10,"Making sure that there are at least 2 samples from one class (here, simply duplicate the point)."
v0.10,Compare number of correct classifications.
v0.10,"Making sure that X and Y have no overlapping values, which would lead to a distance of 0 with k=1 and, thus, to"
v0.10,a division by zero.
v0.10,For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples
v0.10,is unknown beforehand.
v0.10,The method stops if either the change between some consecutive runs is below the given threshold or the
v0.10,maximum number of runs is reached.
v0.10,"In each run, we create one random permutation of players. For instance, given 4 players, a permutation"
v0.10,"could be [3,1,4,2]."
v0.10,Generate k random permutations by sorting the indices of the Halton sequence
v0.10,"Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the"
v0.10,"subsets are [3], [3,1], [3,1,4] [3,1,4,2]."
v0.10,"The result for each subset is cached such that if a subset that has already been evaluated appears again,"
v0.10,we can take this result directly.
v0.10,"To improve the runtime, multiple permutations are evaluated in each run."
v0.10,"The current Shapley values are the average of the estimated values, i.e. we need to divide by the number"
v0.10,of generated permutations here.
v0.10,"In case Shapley values are estimated for multiple samples, e.g., in feature relevance. So, we have a"
v0.10,matrix of Shapley values instead of a vector.
v0.10,"Here, the change between consecutive runs is below the minimum threshold, but to reduce the"
v0.10,"likelihood that this just happened by chance, we require that this happens at least for"
v0.10,num_consecutive_converged_runs times in a row.
v0.10,Check if change in percentage is below threshold
v0.10,"Check for values that are exactly zero. If they don't change between two runs, we consider it as converging."
v0.10,Create all (unique) subsets)
v0.10,"Assigning a 'high' weight, since this resembles ""infinity""."
v0.10,The weight for a subset with a specific length (see paper mentioned in the docstring for more
v0.10,information).
v0.10,TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is
v0.10,"theoretically sound, i.e. make entropy results from different data comparable."
v0.10,Extremely small values can somehow result in negative values.
v0.10,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.10,Sampling from the conditional distribution based on the current sample.
v0.10,"Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that"
v0.10,"are in the given subset. By this, we can simulate the impact on the conditional distribution when removing"
v0.10,only the incoming edges of the variables in the subset.
v0.10,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.10,"In case of the full subset (no randomization), we get the same predictions as when we apply the"
v0.10,"prediction method to the samples of interest, since all noise samples are replaced with a sample of"
v0.10,interest.
v0.10,"In case of the empty subset (all are jointly randomize), it boils down to taking the average over all"
v0.10,"predictions, seeing that the randomization yields the same values for each sample of interest (none of the"
v0.10,samples of interest are used to replace a (jointly) 'randomized' sample).
v0.10,Exact model
v0.10,This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes
v0.10,This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes
v0.10,afterwards.
v0.10,Smallest possible value. This is used in various algorithm for numerical stability.
v0.10,Make copy to avoid manipulating the original matrix.
v0.10,"The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,"
v0.10,the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster
v0.10,"than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as"
v0.10,"possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples."
v0.10,"To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many"
v0.10,samples. The number of samples that are evaluated is normally
v0.10,"baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to"
v0.10,"batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is"
v0.10,evaluated one by one in a for-loop.
v0.10,Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple
v0.10,"batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet."
v0.10,"If the batch size would be larger than the remaining amount of samples, it is reduced to only include the"
v0.10,remaining baseline_noise_samples.
v0.10,"The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features"
v0.10,in baseline_feature_indices to their respective values in baseline_noise_samples.
v0.10,"After creating the (potentially large) input data matrix, we can evaluate the prediction method."
v0.10,"Here, offset + index now indicates the sample index in baseline_noise_samples."
v0.10,This would average all prediction results obtained for the 'offset + index'-th sample in
v0.10,"baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)]."
v0.10,This would return all prediction results obtained for the 'offset + index'-th sample in
v0.10,"baseline_noise_samples, i.e. the results are not averaged."
v0.10,Making copy to ensure that the original object is not modified.
v0.10,Permute samples jointly. This still represents an interventional distribution.
v0.10,Permute samples independently.
v0.10,Looks for the first index where the cumulative sum of the probabilities is larger than the threshold.
v0.10,"Note that if there are multiple indices with the same maximum value (as in this case here), the argmax"
v0.10,function returns the first index.
v0.10,"test local Markov condition, null hypothesis: conditional independence"
v0.10,"test edge dependence, null hypothesis: independence"
v0.10,The order of the p-values added to the list is deterministic.
v0.10,"To be able to validate that the graph structure did not change between fitting and causal query, we store the"
v0.10,"parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While"
v0.10,"this would automatically fail when the number of parents is different, there are other more subtle cases,"
v0.10,"where the number is still the same, but it's different parents, and therefore different data. That would yield"
v0.10,"wrong results, but would not fail."
v0.10,Independence tests are symmetric
v0.10,Find out which tests to do
v0.10,Parallelize over tests
v0.10,Gather results
v0.10,Summarize
v0.10,Find out which tests to do
v0.10,Parallelize over tests
v0.10,Gather results
v0.10,Summarize
v0.10,Find out which tests to do
v0.10,Parallelize over tests
v0.10,Gather results
v0.10,Summarize
v0.10,DAG Evaluation
v0.10,Suggestions
v0.10,"Append list of violations (node, non_desc) to get local information"
v0.10,Plot histograms
v0.10,Plot given violations
v0.10,For LMC we highlight X for which X _|/|_ Y \in ND_X | Pa_X
v0.10,For PD we highlight the edge (if Y\in Anc_X -> X are adjacent)
v0.10,For causal minimality we highlight the edge Y \in Pa_X -> X
v0.10,Create Validation header
v0.10,Create Validation summary
v0.10,Close Validation
v0.10,Create Suggestions header
v0.10,Iterate over suggestions
v0.10,Test if we have data for X and Y
v0.10,Test if we have data for Z
v0.10,Eq. (1) in https://arxiv.org/pdf/1810.11363.pdf
v0.10,Subtracting Y here since the cumulative sum includes the current element. The same reason we subtract 1
v0.10,from the count.
v0.10,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.10,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.10,"target quantity (here, variance)."
v0.10,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.10,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.10,"target quantity (here, variance)."
v0.10,"Calculate Ri, the product of the residuals"
v0.10,Standard deviation of the residuals
v0.10,Either X and/or Y is constant.
v0.10,"If Z is empty, we are in the pairwise setting."
v0.10,Either X and/or Y is constant.
v0.10,"If Z is empty, we are in the pairwise setting."
v0.10,Estimate test statistic multiple times on different permutations of the data. The p-value is then the
v0.10,probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted
v0.10,data.
v0.10,Take the lower dimensional variable as target.
v0.10,First stage statistical model
v0.10,Second stage statistical model
v0.10,Check if the treatment is one-dimensional
v0.10,First stage
v0.10,Second Stage
v0.10,Combining the two estimates
v0.10,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.10,Bulding the feature matrix
v0.10,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.10,TODO move this to the identification step
v0.10,Obtain estimate by Wald Estimator
v0.10,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.10,More than 1 instrument. Use 2sls.
v0.10,Checking if Y is binary
v0.10,Enable the user to pass params for a custom propensity model
v0.10,Convert the categorical variables into dummy/indicator variables
v0.10,"Basically, this gives a one hot encoding for each category"
v0.10,The first category is taken to be the base line.
v0.10,Check if the treatment is one-dimensional
v0.10,Checking if the treatment is binary
v0.10,The model is always built on the entire data
v0.10,TODO make treatment_value and control value also as local parameters
v0.10,All treatments are set to the same constant value
v0.10,"Fixing treatment value to the specified value, if provided"
v0.10,treatment_vals and data_df should have same number of rows
v0.10,Bulding the feature matrix
v0.10,The model is always built on the entire data
v0.10,Replacing treatment values by given x
v0.10,"First, create interventional tensor in original space"
v0.10,"Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment"
v0.10,The average treatment effect is a combination of different
v0.10,regression coefficients. Complicated to compute the confidence
v0.10,"interval analytically. For example, if y=a + b1.t + b2.tx, then"
v0.10,the average treatment effect is b1+b2.mean(x).
v0.10,"Refer Gelman, Hill. ARM Book. Chapter 9"
v0.10,http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
v0.10,TODO: Looking for contributions
v0.10,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.10,"variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0."
v0.10,"So for custom treatment and control values, we must multiply the confidence interval by the difference of the two."
v0.10,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.10,"variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0."
v0.10,"So for custom treatment and control values, we must multiply the standard error by the difference of the two."
v0.10,setting method-specific parameters
v0.10,Infer the right strata based on clipping threshold
v0.10,0.5 because there are two values for the treatment
v0.10,To be conservative and allow most strata to be included in the
v0.10,analysis
v0.10,At least 90% of the strata should be included in analysis
v0.10,sum weighted outcomes over all strata  (weight by treated population)
v0.10,TODO - how can we add additional information into the returned estimate?
v0.10,"such as how much clipping was done, or per-strata info for debugging?"
v0.10,sort the dataframe by propensity score
v0.10,create a column 'strata' for each element that marks what strata it belongs to
v0.10,"for each strata, count how many treated and control units there are"
v0.10,throw away strata that have insufficient treatment or control
v0.10,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.10,Setting method specific parameters
v0.10,trim propensity score weights
v0.10,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.10,nips ==> ips / (sum of ips over all units)
v0.10,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.10,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.10,Vanilla IPS estimator
v0.10,The Hajek estimator (or the self-normalized estimator)
v0.10,"Stabilized weights (from Robins, Hernan, Brumback (2000))"
v0.10,Paper: Marginal Structural Models and Causal Inference in Epidemiology
v0.10,Calculating the effect
v0.10,Subtracting the weighted means
v0.10,TODO - how can we add additional information into the returned estimate?
v0.10,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.10,Save parameters for later refutter fitting
v0.10,Enforcing this ordering is necessary to feed through the propensity values from dataset
v0.10,For metalearners only--issue a warning if w contains variables not in x
v0.10,Override the effect_modifiers set in CausalEstimator.__init__()
v0.10,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.10,the latter can be used by other estimator methods later
v0.10,"Instrumental variables names, if present"
v0.10,choosing the instrumental variable to use
v0.10,Calling the econml estimator's fit method
v0.10,"As of v0.9, econml has some kewyord only arguments"
v0.10,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10,Changing shape to a list for a singleton value
v0.10,Note that self._control_value is assumed to be a singleton value
v0.10,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.10,"For each unit, return the estimated effect of the treatment value"
v0.10,that was actually applied to the unit
v0.10,this assumes a binary treatment regime
v0.10,TODO remove neighbors that are more than a given radius apart
v0.10,estimate ATT on treated by summing over difference between matched neighbors
v0.10,Now computing ATC
v0.10,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.10,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10,Handle externally provided estimator classes
v0.10,allowed types of distance metric
v0.10,Dictionary of any user-provided params for the distance metric
v0.10,that will be passed to sklearn nearestneighbors
v0.10,Check if the treatment is one-dimensional
v0.10,Checking if the treatment is binary
v0.10,Convert the categorical variables into dummy/indicator variables
v0.10,"Basically, this gives a one hot encoding for each category"
v0.10,The first category is taken to be the base line.
v0.10,this assumes a binary treatment regime
v0.10,TODO remove neighbors that are more than a given radius apart
v0.10,estimate ATT on treated by summing over difference between matched neighbors
v0.10,Return indices in the original dataframe
v0.10,self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()
v0.10,Now computing ATC
v0.10,Return indices in the original dataframe
v0.10,Add the identification method used in the estimator
v0.10,Check the backdoor variables being used
v0.10,Add the observed confounders and one hot encode the categorical variables
v0.10,Get the data of the unobserved confounders
v0.10,One hot encode the data if they are categorical
v0.10,Check the instrumental variables involved
v0.10,Perform the same actions as the above
v0.10,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.10,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.10,For CATEs
v0.10,TODO we are conditioning on a postive treatment
v0.10,TODO create an expression corresponding to each estimator used
v0.10,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.10,Flipping some values
v0.10,Wrapping labels if they are too long
v0.10,This layout tries to mimic the graphviz layout in a simpler form. The depth grows horizontally here instead of
v0.10,vertically.
v0.10,Set the figure size based on the number of nodes
v0.10,"Nodes that are vertically connected, but not neighbors should be connected via a curved edge."
v0.10,All other nodes should be connected with a straight line.
v0.10,Draw labels node labels
v0.10,"Each node gets a depth assigned, based on the distance to the closest root node."
v0.10,"In case of undirected graphs, we just take any node as root node."
v0.10,"No path to root node, ignore this connection then."
v0.10,Counts the number of vertical nodes in the same layers.
v0.10,Creates a matrix indicating whether two nodes are vertical neighbors.
v0.10,Get all y coordinates per layer
v0.10,Sort the y-coordinates
v0.10,Finding p-value using student T test
v0.10,Only consider edges have absolute edge weight > 0.01
v0.10,Modify graph such that it only contains bidirected edges
v0.10,Find c components by finding connected components on the undirected graph
v0.10,Understanding Neural Network weights
v0.10,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.10,add weight column
v0.10,before weights are applied we count number rows in each category
v0.10,which is equivalent to summing over weight=1
v0.10,after weights are applied we need to sum over the given weights
v0.10,"First, calculating mean differences by strata"
v0.10,"Second, without strata"
v0.10,"Third, concatenating them and plotting"
v0.10,Setting estimator attribute for convenience
v0.10,Outcome is numeric
v0.10,Treatments are also numeric or binary
v0.10,Outcome is categorical
v0.10,Treatments are numeric or binary
v0.10,TODO: A common way to show all plots
v0.10,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10,Get adjacency list
v0.10,If node pair has been fully explored
v0.10,Add node1 to backdoor set of node_pair
v0.10,Check if path is backdoor and does not have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.10,"True if arrow incoming, False if arrow outgoing"
v0.10,"Mark pair (node1, node2) complete"
v0.10,Modify variable count and indices covered
v0.10,Average total effect
v0.10,Natural direct effect
v0.10,Natural indirect effect
v0.10,Controlled direct effect
v0.10,Backdoor method names
v0.10,"First, check if there is a directed path from action to outcome"
v0.10,## 1. BACKDOOR IDENTIFICATION
v0.10,Pick algorithm to compute backdoor sets according to method chosen
v0.10,"First, checking if there are any valid backdoor adjustment sets"
v0.10,"Setting default ""backdoor"" identification adjustment set"
v0.10,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.10,Now checking if there is also a valid iv estimand
v0.10,## 3. FRONTDOOR IDENTIFICATION
v0.10,Now checking if there is a valid frontdoor variable
v0.10,Finally returning the estimand object
v0.10,Pick algorithm to compute backdoor sets according to method chosen
v0.10,"Setting default ""backdoor"" identification adjustment set"
v0.10,Finally returning the estimand object
v0.10,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.10,"First, checking if there are any valid backdoor adjustment sets"
v0.10,"Setting default ""backdoor"" identification adjustment set"
v0.10,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.10,Now checking if there are valid mediator variables
v0.10,Finally returning the estimand object
v0.10,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.10,"First, checking if there are any valid backdoor adjustment sets"
v0.10,"Setting default ""backdoor"" identification adjustment set"
v0.10,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.10,Now checking if there are valid mediator variables
v0.10,Finally returning the estimand object
v0.10,"First, checking if empty set is a valid backdoor set"
v0.10,"If the method is `minimal-adjustment`, return the empty set right away."
v0.10,"Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible."
v0.10,only remove descendants of Y
v0.10,also allow any causes of Y that are not caused by T (for lower variance)
v0.10,remove descendants of T (mediators) and descendants of Y
v0.10,"If var is d-separated from both treatment or outcome, it cannot"
v0.10,be a part of the backdoor set
v0.10,repeat the above search with BACKDOOR_MIN
v0.10,"If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest."
v0.10,"If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set."
v0.10,"If all variables are observed, and the biggest eligible set"
v0.10,"does not satisfy backdoor, then none of its subsets will."
v0.10,Adding a None estimand if no backdoor set found
v0.10,"Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable."
v0.10,Default set is the one with the least number of adjustment variables (optimizing for efficiency)
v0.10,"For simplicity, assuming a one-variable frontdoor set"
v0.10,Cond 1: All directed paths intercepted by candidate_var
v0.10,Cond 2: No confounding between treatment and candidate var
v0.10,Cond 3: treatment blocks all confounding between candidate_var and outcome
v0.10,"For simplicity, assuming a one-variable mediation set"
v0.10,"Create estimands dict as per the API for backdoor, but do not return it"
v0.10,"Setting default ""backdoor"" identification adjustment set"
v0.10,"Create estimands dict as per the API for backdoor, but do not return it"
v0.10,"Setting default ""backdoor"" identification adjustment set"
v0.10,"TODO: outputs string for now, but ideally should do symbolic"
v0.10,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.10,TODO Better support for multivariate treatments
v0.10,TODO: support multivariate treatments better.
v0.10,TODO: support multivariate treatments better.
v0.10,TODO: support multivariate treatments better.
v0.10,For direct effect
v0.10,"If no costs are passed, use uniform costs"
v0.10,restriction to ancestors
v0.10,back-door graph
v0.10,moralization
v0.10,Estimators list for returning after identification
v0.10,Line 1
v0.10,"If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y."
v0.10,Line 2
v0.10,"If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y."
v0.10,Modify list of valid nodes
v0.10,Line 3 - forces an action on any node where such an action would have no effect on Y – assuming we already acted on X.
v0.10,Modify adjacency matrix to obtain that corresponding to do(X)
v0.10,Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.
v0.10,"If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases."
v0.10,Modify adjacency matrix to remove treatment variables
v0.10,"Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes."
v0.10,"Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem."
v0.10,Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.
v0.10,Do not show backdoor key unless it is the only backdoor set.
v0.10,Just show the default backdoor set
v0.10,self._identified_estimand = self._causal_model.identify_effect()
v0.10,"self._identified_estimand,"
v0.10,"self._causal_model._treatment,"
v0.10,"self._causal_model._outcome,"
v0.10,If labels provided
v0.10,Return in valid DOT format
v0.10,Get adjacency matrix
v0.10,If labels not provided
v0.10,Obtain valid DOT format
v0.10,If labels provided
v0.10,Return in valid DOT format
v0.10,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.10,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9.1,-*- coding: utf-8 -*-
v0.9.1,
v0.9.1,Configuration file for the Sphinx documentation builder.
v0.9.1,
v0.9.1,This file does only contain a selection of the most common options. For a
v0.9.1,full list see the documentation:
v0.9.1,http://www.sphinx-doc.org/en/stable/config
v0.9.1,-- Path setup --------------------------------------------------------------
v0.9.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.9.1,add these directories to sys.path here. If the directory is relative to the
v0.9.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.9.1,
v0.9.1,-- Project information -----------------------------------------------------
v0.9.1,Version Information (for version-switcher)
v0.9.1,-- General configuration ---------------------------------------------------
v0.9.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.9.1,
v0.9.1,needs_sphinx = '1.0'
v0.9.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.9.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.9.1,ones.
v0.9.1,sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme
v0.9.1,already loads it
v0.9.1,"Add any paths that contain templates here, relative to this directory."
v0.9.1,The suffix(es) of source filenames.
v0.9.1,You can specify multiple suffix as a list of string:
v0.9.1,
v0.9.1,"source_suffix = ['.rst', '.md']"
v0.9.1,The master toctree document.
v0.9.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.9.1,for a list of supported languages.
v0.9.1,
v0.9.1,This is also used if you do content translation via gettext catalogs.
v0.9.1,"Usually you set ""language"" from the command line for these cases."
v0.9.1,"List of patterns, relative to source directory, that match files and"
v0.9.1,directories to ignore when looking for source files.
v0.9.1,This pattern also affects html_static_path and html_extra_path .
v0.9.1,The name of the Pygments (syntax highlighting) style to use.
v0.9.1,-- Options for HTML output -------------------------------------------------
v0.9.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.9.1,a list of builtin themes.
v0.9.1,
v0.9.1,Theme options are theme-specific and customize the look and feel of a theme
v0.9.1,"further.  For a list of options available for each theme, see the"
v0.9.1,documentation.
v0.9.1,
v0.9.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.9.1,"relative to this directory. They are copied after the builtin static files,"
v0.9.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.9.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.9.1,to template names.
v0.9.1,
v0.9.1,The default sidebars (for documents that don't match any pattern) are
v0.9.1,defined by theme itself.  Builtin themes are using these templates by
v0.9.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.9.1,'searchbox.html']``.
v0.9.1,
v0.9.1,html_sidebars = {}
v0.9.1,-- Options for HTMLHelp output ---------------------------------------------
v0.9.1,Output file base name for HTML help builder.
v0.9.1,-- Options for LaTeX output ------------------------------------------------
v0.9.1,The paper size ('letterpaper' or 'a4paper').
v0.9.1,
v0.9.1,"'papersize': 'letterpaper',"
v0.9.1,"The font size ('10pt', '11pt' or '12pt')."
v0.9.1,
v0.9.1,"'pointsize': '10pt',"
v0.9.1,Additional stuff for the LaTeX preamble.
v0.9.1,
v0.9.1,"'preamble': '',"
v0.9.1,Latex figure (float) alignment
v0.9.1,
v0.9.1,"'figure_align': 'htbp',"
v0.9.1,Grouping the document tree into LaTeX files. List of tuples
v0.9.1,"(source start file, target name, title,"
v0.9.1,"author, documentclass [howto, manual, or own class])."
v0.9.1,-- Options for manual page output ------------------------------------------
v0.9.1,One entry per manual page. List of tuples
v0.9.1,"(source start file, name, description, authors, manual section)."
v0.9.1,-- Options for Texinfo output ----------------------------------------------
v0.9.1,Grouping the document tree into Texinfo files. List of tuples
v0.9.1,"(source start file, target name, title, author,"
v0.9.1,"dir menu entry, description, category)"
v0.9.1,-- Options for Epub output -------------------------------------------------
v0.9.1,Bibliographic Dublin Core info.
v0.9.1,The unique identifier of the text. This can be a ISBN number
v0.9.1,or the project homepage.
v0.9.1,
v0.9.1,epub_identifier = ''
v0.9.1,A unique identification for the text.
v0.9.1,
v0.9.1,epub_uid = ''
v0.9.1,A list of files that should not be packed into the epub file.
v0.9.1,-- Extension configuration -------------------------------------------------
v0.9.1,-- Options for todo extension ----------------------------------------------
v0.9.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.9.1,init docstrings should also be included in class
v0.9.1,Patch all of the published versions
v0.9.1,check old RST version (<= v0.8)
v0.9.1,Remove old version links
v0.9.1,Append updated version links
v0.9.1,requires stdin input for identify in weighting sampler
v0.9.1,requires Rpy2 for lalonde
v0.9.1,will be removed
v0.9.1,"applied notebook, not necessary to test each time"
v0.9.1,needs xgboost too
v0.9.1,Slow Notebooks
v0.9.1,requires Rpy2 for causal discovery
v0.9.1,daily tests of dowhy_causal_discovery_example.ipynb are failing due to cdt/rpy2 config.
v0.9.1,"comment out, since we are switching causal discovery implementations"
v0.9.1,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.9.1,can import dowhy
v0.9.1,"""--ExecutePreprocessor.timeout=600"","
v0.9.1,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.9.1,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.9.1,treated = self.df[self.df['z']==1]
v0.9.1,self.att = np.mean(treated['y1'] - treated['y0'])
v0.9.1,def test_average_treatment_effect(self):
v0.9.1,est_ate = 1
v0.9.1,bias = est_ate - self.ate
v0.9.1,print(bias)
v0.9.1,"self.assertAlmostEqual(self.ate, est_ate)"
v0.9.1,def test_average_treatment_effect_on_treated(self):
v0.9.1,est_att = 1
v0.9.1,self.att=1
v0.9.1,bias = est_att - self.att
v0.9.1,print(bias)
v0.9.1,"self.assertAlmostEqual(self.att, est_att)"
v0.9.1,removing two common causes
v0.9.1,removing two common causes
v0.9.1,removing two common causes
v0.9.1,removing two common causes
v0.9.1,check if all partial R^2 values are between 0 and 1
v0.9.1,We calculate adjusted estimates for two sets of partial R^2 values.
v0.9.1,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.9.1,Creating a model with no unobserved confounders
v0.9.1,check if all partial R^2 values are between 0 and 1
v0.9.1,"for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)"
v0.9.1,Non Parametric estimator
v0.9.1,We calculate adjusted estimates for two sets of partial R^2 values.
v0.9.1,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9.1,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9.1,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9.1,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9.1,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9.1,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9.1,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9.1,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9.1,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.9.1,we patched figure plotting call to avoid drawing plots during tests
v0.9.1,We calculate adjusted estimates for two sets of partial R^2 values.
v0.9.1,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9.1,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9.1,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9.1,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9.1,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9.1,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9.1,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9.1,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9.1,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.9.1,we patched figure plotting call to avoid drawing plots during tests
v0.9.1,comparing test examples from R E-Value package
v0.9.1,check implementation of Observed Covariate E-value against R package
v0.9.1,The outcome is a linear function of the confounder
v0.9.1,"The slope is 1,2 and the intercept is 3"
v0.9.1,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.9.1,Supports user-provided dataset object
v0.9.1,To test if there are any exceptions
v0.9.1,To test if the estimate is identical if refutation parameters are zero
v0.9.1,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.9.1,"Ordinarily, we should expect this value to be zero."
v0.9.1,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.9.1,"Ordinarily, we should expect this value to be zero."
v0.9.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9.1,Only P(Y|T) should be present for test to succeed.
v0.9.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9.1,"Since undirected graph, identify effect must throw an error."
v0.9.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9.1,Compare with ground truth
v0.9.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9.1,Compare with ground truth
v0.9.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9.1,Compare with ground truth
v0.9.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9.1,Compare with ground truth
v0.9.1,Causal model initialization
v0.9.1,Obtain backdoor sets
v0.9.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.9.1,Causal model initialization
v0.9.1,Obtain backdoor sets
v0.9.1,Causal model initialization
v0.9.1,Obtain backdoor sets
v0.9.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.9.1,Causal model initialization
v0.9.1,Obtain backdoor sets
v0.9.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.9.1,Causal model initialization
v0.9.1,Obtain backdoor sets
v0.9.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.9.1,Causal model initialization
v0.9.1,Obtain backdoor sets
v0.9.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.9.1,"Example is selected from Pearl J. ""Causality"" 2nd Edition, from chapter 3.3.1 on backoor criterion."
v0.9.1,"The following simpsons paradox examples are taken from Pearl, J {2013}. ""Understanding Simpson’s Paradox"" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf"
v0.9.1,"The following are examples given in the ""Book of Why"" by Judea Pearl, chapter ""The Do-operator and the Back-Door Criterion"""
v0.9.1,For all examples from these papers we use X for the treatment variable
v0.9.1,instead of A.
v0.9.1,"Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.9.1,"Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.9.1,"Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.9.1,"Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.9.1,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.9.1,L replaces X as the conditional variable
v0.9.1,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.9.1,L replaces X as the conditional variable. Uses different costs
v0.9.1,"Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.9.1,"A graph where optimal, optimal minimal and optimal min cost are different"
v0.9.1,The graph from Shrier and Platt (2008)
v0.9.1,A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.9.1,Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.9.1,cov_mat = np.diag(np.ones(num_features))
v0.9.1,collider: X->Z<-Y
v0.9.1,chain: X->Z->Y
v0.9.1,fork: X<-Z->Y
v0.9.1,"general DAG: X<-Z->Y, X->Y"
v0.9.1,fork: X<-Z->Y
v0.9.1,Contributions should add up to Var(X2)
v0.9.1,Contributions should add up to Var(X2)
v0.9.1,H(P(Y)) -- Can be precomputed
v0.9.1,-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))
v0.9.1,"H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\S)])"
v0.9.1,"E[P(Y | x_S, X'_\S)]"
v0.9.1,"H(E[P(Y | x_S, X'_\S)])"
v0.9.1,"Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions."
v0.9.1,E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]
v0.9.1,"Just checking formats, i.e. no need for correlation."
v0.9.1,"Just checking formats, i.e. no need for correlation."
v0.9.1,Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3
v0.9.1,C2 = 3 * A2 + 2 * B2
v0.9.1,"By default, the strength is measure with respect to the variance."
v0.9.1,"Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2."
v0.9.1,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.9.1,"Here, changing the mechanism."
v0.9.1,"Here, changing the mechanism."
v0.9.1,"Here, changing the mechanism."
v0.9.1,"Here, changing the mechanism."
v0.9.1,Defining an anomaly scorer that handles multidimensional inputs.
v0.9.1,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.9.1,"In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution"
v0.9.1,"and it should be ""significantly"" higher than the contribution of the other ones (here, we just arbitrarily say"
v0.9.1,"it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,"
v0.9.1,"which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate"
v0.9.1,"some robustness. Note that due to this and stochastic behaviour of the density estimator, it is"
v0.9.1,"not possible to analytically compute expected results. Therefore, we rather look at the relations here."
v0.9.1,"Same idea for the second sample, but here, it is the second variable that is anomalous."
v0.9.1,"In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be"
v0.9.1,"""significantly"" higher than the contribution of the other variables. The contribution of both anomalous variables"
v0.9.1,should be equal (approximately).
v0.9.1,Defining an anomaly scorer that handles multidimensional inputs.
v0.9.1,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.9.1,reduce the score.
v0.9.1,The contributions should add up to g(x) - E[g(X)]
v0.9.1,The contributions should add up to g(x) - E[g(X)]
v0.9.1,The contributions should add up to g(x) - E[g(X)]
v0.9.1,Three examples:
v0.9.1,1. X1 is the root cause (+ 10 to the noise)
v0.9.1,2. X0 is the root cause (+ 10 to the noise)
v0.9.1,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.9.1,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.9.1,Three examples:
v0.9.1,1. X1 is the root cause (+ 10 to the noise)
v0.9.1,2. X0 is the root cause (+ 10 to the noise)
v0.9.1,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.9.1,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.9.1,"Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution"
v0.9.1,algorithm.
v0.9.1,1. X0 is the root cause (+ 10 to the noise)
v0.9.1,2. X0 and X1 are the root causes (+ 10 to both noise)
v0.9.1,3. X2 and X3 are both root causes (+ 10 to both noises)
v0.9.1,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.9.1,Check if calling the method causes some import or runtime errors
v0.9.1,TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer
v0.9.1,newer matplotlib version:
v0.9.1,AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'
v0.9.1,Networkx 2.4+ should fix this issue.
v0.9.1,"plot_adjacency_matrix(causal_graph, is_directed=False)"
v0.9.1,Setup data
v0.9.1,Test LinearDML
v0.9.1,Test ContinuousTreatmentOrthoForest
v0.9.1,Test LinearDRLearner
v0.9.1,Setup data
v0.9.1,Test DeepIV
v0.9.1,"Treatment model,"
v0.9.1,Response model
v0.9.1,Test IntentToTreatDRIV
v0.9.1,Observed data
v0.9.1,assumed graph
v0.9.1,Identify effect
v0.9.1,Estimate effect
v0.9.1,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.9.1,More cases where Exception  is expected
v0.9.1,"Compute confidence intervals, standard error and significance tests"
v0.9.1,Defined a linear dataset with a given set of properties
v0.9.1,Create a model that captures the same
v0.9.1,Identify the effects within the model
v0.9.1,Defined a linear dataset with a given set of properties
v0.9.1,Create a model that captures the same
v0.9.1,Identify the effects within the model
v0.9.1,Defined a linear dataset with a given set of properties
v0.9.1,Create a model that captures the same
v0.9.1,Identify the effects within the model
v0.9.1,Defined a linear dataset with a given set of properties
v0.9.1,Create a model that captures the same
v0.9.1,Identify the effects within the model
v0.9.1,Defined a linear dataset with a given set of properties
v0.9.1,Create a model that captures the same
v0.9.1,Identify the effects within the model
v0.9.1,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.9.1,Unpacking the keyword arguments
v0.9.1,todo: add docstring for common parameters here and remove from child refuter classes
v0.9.1,Default value for the number of simulations to be conducted
v0.9.1,joblib params for parallel processing
v0.9.1,"Concatenate the confounders, instruments and effect modifiers"
v0.9.1,Shuffle the confounders
v0.9.1,Check if all are select or deselect variables
v0.9.1,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.9.1,Get the number of simulations
v0.9.1,Sort the simulations
v0.9.1,Obtain the median value
v0.9.1,Performing a two sided test
v0.9.1,np.searchsorted tells us the index if it were a part of the array
v0.9.1,We select side to be left as we want to find the first value that matches
v0.9.1,We subtact 1 as we are finding the value from the right tail
v0.9.1,We take the side to be right as we want to find the last index that matches
v0.9.1,We get the probability with respect to the left tail.
v0.9.1,return twice the determined quantile as this is a two sided test
v0.9.1,Get the mean for the simulations
v0.9.1,Get the standard deviation for the simulations
v0.9.1,Get the Z Score [(val - mean)/ std_dev ]
v0.9.1,Initializing the p_value
v0.9.1,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.9.1,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.9.1,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.9.1,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.9.1,re.sub only takes string parameter so the first if is to avoid error
v0.9.1,"if the input is a text file, convert the contained data into string"
v0.9.1,load dot file
v0.9.1,Adding node attributes
v0.9.1,adding penwidth to make the edge bold
v0.9.1,Adding common causes
v0.9.1,Adding instruments
v0.9.1,Adding effect modifiers
v0.9.1,Assuming the simple form of effect modifier
v0.9.1,that directly causes the outcome.
v0.9.1,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.9.1,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.9.1,Adding columns in the dataframe as confounders that were not in the graph
v0.9.1,Adding unobserved confounders
v0.9.1,removal of only direct edges wrt a target is not implemented for incoming edges
v0.9.1,also return the number of backdoor paths blocked by observed nodes
v0.9.1,Assume that nodes1 is the treatment
v0.9.1,"ignores new_graph parameter, always uses self._graph"
v0.9.1,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.9.1,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.9.1,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.9.1,removing all mediators
v0.9.1,[TODO: double check these work with multivariate implementation:]
v0.9.1,Exclusion
v0.9.1,As-if-random setup
v0.9.1,As-if-random
v0.9.1,convert the outputted generator into a list
v0.9.1,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.9.1,return len(dpaths) > 0
v0.9.1,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.9.1,Create causal graph object
v0.9.1,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.9.1,(Because some effect modifiers may also be common causes)
v0.9.1,"In such cases, the user-provided modifiers are used."
v0.9.1,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.9.1,Import causal discovery class
v0.9.1,Initialize causal graph object
v0.9.1,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.9.1,TODO add dowhy as a prefix to all dowhy estimators
v0.9.1,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.9.1,Define the third-party estimation method to be used
v0.9.1,Process the dowhy estimators
v0.9.1,Check if estimator's target estimand is identified
v0.9.1,"Note that while the name of the variable is the same,"
v0.9.1,"""self.causal_estimator"", this estimator takes in less"
v0.9.1,parameters than the same from the
v0.9.1,estimate_effect code. It is not advisable to use the
v0.9.1,estimator from this function to call estimate_effect
v0.9.1,with fit_estimator=False.
v0.9.1,Estimator had been computed in a previous call
v0.9.1,The default number of simulations for statistical testing
v0.9.1,The default number of simulations to obtain confidence intervals
v0.9.1,The portion of the total size that should be taken each time to find the confidence intervals
v0.9.1,1 is the recommended value
v0.9.1,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.9.1,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.9.1,The default Confidence Level
v0.9.1,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.9.1,Prefix to add to temporary categorical variables created after discretization
v0.9.1,std args to be removed from locals() before being passed to args_dict
v0.9.1,Setting the default interpret method
v0.9.1,"Check if some parameters were set, otherwise set to default values"
v0.9.1,Estimate conditional estimates by default
v0.9.1,TODO Only works for binary treatment
v0.9.1,Defaulting to class default values if parameters are not provided
v0.9.1,Checking that there is at least one effect modifier
v0.9.1,Making sure that effect_modifier_names is a list
v0.9.1,Making a copy since we are going to be changing effect modifier names
v0.9.1,"For every numeric effect modifier, adding a temp categorical column"
v0.9.1,Grouping by effect modifiers and computing effect separately
v0.9.1,Deleting the temporary categorical columns
v0.9.1,The array that stores the results of all estimations
v0.9.1,Find the sample size the proportion with the population size
v0.9.1,Perform the set number of simulations
v0.9.1,names of treatment and outcome
v0.9.1,Using class default parameters if not specified
v0.9.1,Checking if bootstrap_estimates are already computed
v0.9.1,Checked if any parameter is changed from the previous std error estimate
v0.9.1,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.9.1,Get the variations of each bootstrap estimate and sort
v0.9.1,"Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level"
v0.9.1,Get the lower and upper bounds by subtracting the variations from the estimate
v0.9.1,"Use existing params, if new user defined params are not present"
v0.9.1,Checking if bootstrap_estimates are already computed
v0.9.1,Check if any parameter is changed from the previous std error estimate
v0.9.1,"Use existing params, if new user defined params are not present"
v0.9.1,"self._outcome = self._data[""dummy_outcome""]"
v0.9.1,Processing the null hypothesis estimates
v0.9.1,Doing a two-sided test
v0.9.1,Being conservative with the p-value reported
v0.9.1,Being conservative with the p-value reported
v0.9.1,"If the estimate_index is 0, it depends on the number of simulations"
v0.9.1,Need to test r-squared before supporting
v0.9.1,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.9.1,'r-squared': effect_r_squared
v0.9.1,"elif method == ""r-squared"":"
v0.9.1,outcome_mean = np.mean(self._outcome)
v0.9.1,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.9.1,Assuming a linear model with one variable: the treatment
v0.9.1,Currently only works for continuous y
v0.9.1,causal_model = outcome_mean + estimate.value*self._treatment
v0.9.1,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.9.1,r_squared = 1 - (squared_residual/total_variance)
v0.9.1,return r_squared
v0.9.1,Check if estimator's target estimand is identified
v0.9.1,Store parameters inside estimate object for refutation methods
v0.9.1,TODO: This add_params needs to move to the estimator class
v0.9.1,inside estimate_effect and estimate_conditional_effect
v0.9.1,No estimand was identified (identification failed)
v0.9.1,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.9.1,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.9.1,one-hot encode discrete W
v0.9.1,Now deleting the old continuous value
v0.9.1,create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause
v0.9.1,"Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately"
v0.9.1,One Hot Encoding
v0.9.1,Making beta an array
v0.9.1,TODO Ensure that we do not generate weak instruments
v0.9.1,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.9.1,Converting treatment to binary if required
v0.9.1,Generating frontdoor variables if asked for
v0.9.1,NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.
v0.9.1,"For categorical t, this example dataset has the effect size for category 2 being exactly"
v0.9.1,double the effect for category 1
v0.9.1,This could be changed at this stage by one-hot encoding t and using a custom beta that
v0.9.1,"sets a different effect for each category {0, 1, 2}"
v0.9.1,Computing ATE
v0.9.1,constructing column names for one-hot encoded discrete features
v0.9.1,Specifying the correct dtypes
v0.9.1,Now specifying the corresponding graph strings
v0.9.1,Now writing the gml graph
v0.9.1,Making beta an array
v0.9.1,creating data frame
v0.9.1,Specifying the correct dtypes
v0.9.1,Now specifying the corresponding graph strings
v0.9.1,Now writing the gml graph
v0.9.1,Adding edges between common causes and the frontdoor mediator
v0.9.1,Error terms
v0.9.1,else:
v0.9.1,V = 6 + W0 + tterm + E1
v0.9.1,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.9.1,Generating a random normal distribution of integers
v0.9.1,Generating data for nodes which have no incoming edges
v0.9.1,"""currset"" variable currently has all the successors of the nodes which had no incoming edges"
v0.9.1,Making beta an array
v0.9.1,assuming that all unobserved common causes are numerical and are not affected by one hot encoding
v0.9.1,Creating a NN to simulate the nuisance function
v0.9.1,strength of unobserved confounding
v0.9.1,Computing ATE
v0.9.1,Specifying the correct dtypes
v0.9.1,Now writing the gml graph
v0.9.1,The following code for loading the Lalonde dataset was copied from
v0.9.1,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.9.1,
v0.9.1,"Copyright 2018, Wayfair, Inc."
v0.9.1,
v0.9.1,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.9.1,the following conditions are met:
v0.9.1,
v0.9.1,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.9.1,following disclaimer.
v0.9.1,
v0.9.1,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.9.1,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.9.1,
v0.9.1,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.9.1,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.9.1,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.9.1,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.9.1,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.9.1,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.9.1,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.9.1,DAMAGE.
v0.9.1,
v0.9.1,0.0.0 is standard placeholder for poetry-dynamic-versioning
v0.9.1,any changes to this should not be checked in
v0.9.1,
v0.9.1,The currently supported estimators
v0.9.1,The default standard deviation for noise
v0.9.1,The default scaling factor to determine the bucket size
v0.9.1,The minimum number of points for the estimator to run
v0.9.1,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.9.1,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.9.1,The Default split for the number of data points that fall into the training and validation sets
v0.9.1,Assuming that outcome is one-dimensional
v0.9.1,We need to change the identified estimand
v0.9.1,"We thus, make a copy. This is done as we don't want"
v0.9.1,to change the original DataFrame
v0.9.1,We use collections.OrderedDict to maintain the order in which the data is stored
v0.9.1,Check if we are using an estimator in the transformation list
v0.9.1,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.9.1,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.9.1,"loops. Thus, we can get different values everytime we get the estimator."
v0.9.1,for _ in range( self._num_simulations ):
v0.9.1,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.9.1,Adding an unobserved confounder if provided by the user
v0.9.1,We set X_train = 0 and outcome_train to be 0
v0.9.1,"Get the final outcome, after running through all the values in the transformation list"
v0.9.1,Check if the value of true effect has been already stored
v0.9.1,We use None as the key as we have no base category for this refutation
v0.9.1,As we currently support only one treatment
v0.9.1,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.9.1,Check if the value of true effect has been already stored
v0.9.1,This ensures that we calculate the causal effect only once.
v0.9.1,We use key_train as we map data with respect to the base category of the data
v0.9.1,As we currently support only one treatment
v0.9.1,Add h(t) to f(W) to get the dummy outcome
v0.9.1,We convert to ndarray for ease in indexing
v0.9.1,The data is of the form
v0.9.1,sim1: cat1 cat2 ... catn
v0.9.1,sim2: cat1 cat2 ... catn
v0.9.1,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.9.1,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.9.1,distribution of the refuter.
v0.9.1,True Causal Effect list
v0.9.1,Iterating through the refutation for each category
v0.9.1,We use string arguments to account for both 32 and 64 bit varaibles
v0.9.1,action for continuous variables
v0.9.1,Action for categorical variables
v0.9.1,Find the set difference for each row
v0.9.1,Choose one out of the remaining
v0.9.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.9.1,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.9.1,of the treatment to affect the outcome
v0.9.1,Standardizing the data
v0.9.1,Fit a model containing all confounders and compare predictions
v0.9.1,using all features compared to all features except a given
v0.9.1,confounder.
v0.9.1,Estimating the regression coefficient from standardized features to t
v0.9.1,"By default, return a plot with 10 points"
v0.9.1,consider 10 values of the effect of the unobserved confounder
v0.9.1,Standardizing the data
v0.9.1,Fit a model containing all confounders and compare predictions
v0.9.1,using all features compared to all features except a given
v0.9.1,confounder.
v0.9.1,"By default, return a plot with 10 points"
v0.9.1,consider 10 values of the effect of the unobserved confounder
v0.9.1,"By default, we add the effect of simulated confounder for treatment."
v0.9.1,But subtract it from outcome to create a negative correlation
v0.9.1,assuming that the original confounder's effect was positive on both.
v0.9.1,This is to remove the effect of the original confounder.
v0.9.1,"By default, we add the effect of simulated confounder for treatment."
v0.9.1,But subtract it from outcome to create a negative correlation
v0.9.1,assuming that the original confounder's effect was positive on both.
v0.9.1,This is to remove the effect of the original confounder.
v0.9.1,Obtaining the list of observed variables
v0.9.1,Taking a subset of the dataframe that has only observed variables
v0.9.1,Residuals from the outcome model obtained by fitting a linear model
v0.9.1,Residuals from the treatment model obtained by fitting a linear model
v0.9.1,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.9.1,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.9.1,Choosing a c_star based on the data.
v0.9.1,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.9.1,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.9.1,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.9.1,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.9.1,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.9.1,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.9.1,"If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen"
v0.9.1,Get a 2D matrix of values
v0.9.1,"x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN"
v0.9.1,Store the values into the refute object
v0.9.1,Adding a label on the contour line for the original estimate
v0.9.1,Label every other level using strings
v0.9.1,Default value of the p value taken for the distribution
v0.9.1,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.9.1,Mean of the Normal Distribution
v0.9.1,Standard Deviation of the Normal Distribution
v0.9.1,Create a new column in the data by the name of placebo
v0.9.1,Sanity check the data
v0.9.1,only permute is supported for iv methods
v0.9.1,"For IV methods, the estimating_instrument_names should also be"
v0.9.1,changed. Create a copy to avoid modifying original object
v0.9.1,We need to change the identified estimand
v0.9.1,"We make a copy as a safety measure, we don't want to change the"
v0.9.1,original DataFrame
v0.9.1,Run refutation in parallel
v0.9.1,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.9.1,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.9.1,relationship between the treatment and the outcome.
v0.9.1,new estimator
v0.9.1,new effect estimate
v0.9.1,observed covariate E-value
v0.9.1,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.9.1,"if CI crosses null, set its E-value to 1"
v0.9.1,only report E-value for CI limit closer to null
v0.9.1,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.9.1,whether the DGP is assumed to be partially linear
v0.9.1,features are the observed confounders
v0.9.1,Now code for benchmarking using covariates begins
v0.9.1,R^2 of outcome with observed common causes and treatment
v0.9.1,R^2 of treatment with observed common causes
v0.9.1,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.9.1,Assuming that the difference in R2 is the same for wj and new unobserved confounder
v0.9.1,"for treatment,  Calpha is not a function of the partial R2. So we need a different assumption."
v0.9.1,Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder
v0.9.1,"(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment"
v0.9.1,wrt unobserved confounders in partial-linear models
v0.9.1,whether the DGP is assumed to be partially linear
v0.9.1,can change this to allow default values that are same as the other parameter
v0.9.1,Strength of confounding that omitted variables generate in treatment regression
v0.9.1,computing the point estimate for the bounds
v0.9.1,common causes after removing the benchmark causes
v0.9.1,dataframe with treatment and observed common causes after removing benchmark causes
v0.9.1,R^2 of treatment with observed common causes removing benchmark causes
v0.9.1,return the variance of alpha_s
v0.9.1,R^2 of outcome with observed common causes and treatment after removing benchmark causes
v0.9.1,Obtaining theta_s (the obtained estimate)
v0.9.1,Creating numpy arrays
v0.9.1,Setting up cross-validation parameters
v0.9.1,"tuple of residuals from first stage estimation [0,1], and the confounders [2]"
v0.9.1,"We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation."
v0.9.1,Yres = Y - E[Y|W]
v0.9.1,E[Y|W] = f(x) + theta_s * E[T|W]
v0.9.1,Yres = Y - f(x) - theta_s * E[T|W]
v0.9.1,g(s) = theta_s * T + f(x)
v0.9.1,g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]
v0.9.1,g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]
v0.9.1,Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )
v0.9.1,Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres
v0.9.1,Y - g(s) = Yres - theta_s * Tres
v0.9.1,nu_2 is E[alpha_s^2]
v0.9.1,Now computing scores for finding the (1-a) confidence interval
v0.9.1,R^2 of treatment with observed common causes
v0.9.1,R^2 of outcome with treatment and observed common causes
v0.9.1,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.9.1,Partial R^2 of treatment after regressing over unobserved confounder and observed common causes
v0.9.1,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.9.1,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.9.1,Adding unadjusted point estimate
v0.9.1,Adding bounds to partial R^2 values for given strength of confounders
v0.9.1,Adding a new backdoor variable to the identified estimand
v0.9.1,Run refutation in parallel
v0.9.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.9.1,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.9.1,of the treatment to affect the outcome
v0.9.1,TODO: Sensitivity Analyzers excluded from list due to different return type
v0.9.1,Reject H0
v0.9.1,"a, b and c are all continuous variables"
v0.9.1,"a, b and c are all discrete variables"
v0.9.1,c is set of continuous and binary variables and
v0.9.1,1. either a and b is continuous and the other is binary
v0.9.1,2. both a and b are binary
v0.9.1,c is discrete and
v0.9.1,either a or b is continuous and the other is discrete
v0.9.1,a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information
v0.9.1,original_treatment_name: : stores original variable names for labelling
v0.9.1,common_causes_map : maps the original variable names to variable names in OLS regression
v0.9.1,benchmark_common_causes: stores variable names in terms of regression model variables
v0.9.1,original_benchmark_covariates: stores original variable names for labelling
v0.9.1,estimate: estimate of regression
v0.9.1,degree_of_freedom: degree of freedom of error in regression
v0.9.1,standard_error: standard error in regression
v0.9.1,t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.
v0.9.1,partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them
v0.9.1,"r2tu_w: partial R^2  of unobserved confounder ""u"" with treatment ""t"", after conditioning on observed covariates ""w"""
v0.9.1,"r2yu_tw: partial R^2  of unobserved confounder ""u"" with outcome ""y"", after conditioning on observed covariates ""w"" and treatment ""t"""
v0.9.1,"r2twj_w: partial R^2 of observed covariate wj with treatment ""t"", after conditioning on observed covariates ""w"" excluding wj"
v0.9.1,"r2ywj_tw:  partial R^2 of observed covariate wj with outcome ""y"", after conditioning on observed covariates ""w"" (excluding wj) and treatment ""t"""
v0.9.1,benchmarking_results: dataframe containing information about bounds and bias adjusted terms
v0.9.1,"stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic"
v0.9.1,partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment
v0.9.1,build a new regression model by considering treatment variables as outcome
v0.9.1,"r2twj_w is partial R^2 of covariate wj with treatment ""t"", after conditioning on covariates w(excluding wj)"
v0.9.1,"r2ywj_tw is partial R^2 of covariate wj with outcome ""y"", after conditioning on covariates w(excluding wj) and treatment ""t"""
v0.9.1,r2tu_w is the partial r^2 from regressing u on t after conditioning on w
v0.9.1,Compute bias adjusted terms
v0.9.1,Plotting the contour plot
v0.9.1,Adding contours
v0.9.1,Adding threshold contour line
v0.9.1,Adding unadjusted point estimate
v0.9.1,Adding bounds to partial R^2 values for given strength of confounders
v0.9.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9.1,The default subset of the data to be used
v0.9.1,Run refutation in parallel
v0.9.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.9.1,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.9.1,of the treatment to affect the outcome
v0.9.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9.1,"If the relative change of the score is less than the given threshold, we stop the estimation early."
v0.9.1,"Note, the output of score_samples are log values."
v0.9.1,"Note, the output of score_samples are log values."
v0.9.1,Currently only support continuous distributions for auto selection.
v0.9.1,Estimate distribution parameters from data.
v0.9.1,Ignore warnings from fitting process.
v0.9.1,Fit distribution to data.
v0.9.1,Some distributions might not be compatible with the data.
v0.9.1,Separate parts of parameters.
v0.9.1,Check the KL divergence between the distribution of the given and fitted distribution.
v0.9.1,Identify if this distribution is better.
v0.9.1,This error is typically raised when the data is discrete and all points are assigned to less cluster than
v0.9.1,"specified. It can also happen due to duplicated points. In these cases, the current best solution should"
v0.9.1,be sufficient.
v0.9.1,"Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]"
v0.9.1,A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This
v0.9.1,function evaluates the provided causal query multiple times to build a confidence interval based on the returned
v0.9.1,results.
v0.9.1,Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order
v0.9.1,"to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the"
v0.9.1,fit_and_compute function.
v0.9.1,
v0.9.1,**Example usage:**
v0.9.1,
v0.9.1,">>> gcm.fit(causal_model, data)"
v0.9.1,">>> strength_medians, strength_intervals = gcm.confidence_intervals("
v0.9.1,">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))"
v0.9.1,
v0.9.1,"In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the"
v0.9.1,"confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'"
v0.9.1,"here to configure the function call. In this case,"
v0.9.1,"gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to"
v0.9.1,"lambda : gcm.arrow_strength(causal_model, target_node='Y')."
v0.9.1,
v0.9.1,"In order to incorporate uncertainties coming from fitting the causal model(s), we can use"
v0.9.1,gcm.fit_and_compute instead:
v0.9.1,">>>  strength_medians, strength_intervals = gcm.confidence_intervals("
v0.9.1,">>>        gcm.fit_and_compute(gcm.arrow_strength,"
v0.9.1,">>>                                            causal_model,"
v0.9.1,">>>                                            bootstrap_training_data=data,"
v0.9.1,>>>                                            target_node='Y'))
v0.9.1,This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each
v0.9.1,run.
v0.9.1,"Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based"
v0.9.1,on their topological order.
v0.9.1,"After drawing samples of the node based on the data generation process, we apply the corresponding"
v0.9.1,intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this
v0.9.1,node.
v0.9.1,"Abduction: For invertible SCMs, we recover exact noise values from data."
v0.9.1,Action + Prediction: Propagate the intervention downstream using recovered noise values.
v0.9.1,Check if we need to apply an intervention on the given node.
v0.9.1,Apply intervention function to the data of the node.
v0.9.1,Check if the intervention function changes the shape of the data.
v0.9.1,"For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e."
v0.9.1,all ancestors of the target.
v0.9.1,The target node can be a continuous real-valued variable or a categorical variable with at most two classes
v0.9.1,(i.e. binary).
v0.9.1,Avoid too many features
v0.9.1,Making sure there are at least 30% test samples.
v0.9.1,"Making sure that there are at least 2 samples from one class (here, simply duplicate the point)."
v0.9.1,Compare number of correct classifications.
v0.9.1,This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes
v0.9.1,This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes
v0.9.1,afterwards.
v0.9.1,"can't use nx.node_connected_component, because it doesn't work with DiGraphs."
v0.9.1,Hence a manual loop:
v0.9.1,For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples
v0.9.1,is unknown beforehand.
v0.9.1,The method stops if either the change between some consecutive runs is below the given threshold or the
v0.9.1,maximum number of runs is reached.
v0.9.1,"In each run, we create one random permutation of players. For instance, given 4 players, a permutation"
v0.9.1,"could be [3,1,4,2]."
v0.9.1,"Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the"
v0.9.1,"subsets are [3], [3,1], [3,1,4] [3,1,4,2]."
v0.9.1,"The result for each subset is cached such that if a subset that has already been evaluated appears again,"
v0.9.1,we can take this result directly.
v0.9.1,"To improve the runtime, multiple permutations are evaluated in each run."
v0.9.1,"The current Shapley values are the average of the estimated values, i.e. we need to divide by the number"
v0.9.1,of generated permutations here.
v0.9.1,"Here, the change between two runs is below the minimum threshold, but to reduce the likelihood"
v0.9.1,"that this just happened by chance, we require that this happens at least for two runs in a row."
v0.9.1,Create all (unique) subsets)
v0.9.1,"Assigning a 'high' weight, since this resembles ""infinity""."
v0.9.1,The weight for a subset with a specific length (see paper mentioned in the docstring for more
v0.9.1,information).
v0.9.1,TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is
v0.9.1,"theoretically sound, i.e. make entropy results from different data comparable."
v0.9.1,Extremely small values can somehow result in negative values.
v0.9.1,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.9.1,Sampling from the conditional distribution based on the current sample.
v0.9.1,"Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that"
v0.9.1,"are in the given subset. By this, we can simulate the impact on the conditional distribution when removing"
v0.9.1,only the incoming edges of the variables in the subset.
v0.9.1,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.9.1,"In case of the full subset (no randomization), we get the same predictions as when we apply the"
v0.9.1,"prediction method to the samples of interest, since all noise samples are replaced with a sample of"
v0.9.1,interest.
v0.9.1,"In case of the empty subset (all are jointly randomize), it boils down to taking the average over all"
v0.9.1,"predictions, seeing that the randomization yields the same values for each sample of interest (none of the"
v0.9.1,samples of interest are used to replace a (jointly) 'randomized' sample).
v0.9.1,Smallest possible value. This is used in various algorithm for numerical stability.
v0.9.1,Make copy to avoid manipulating the original matrix.
v0.9.1,"The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,"
v0.9.1,the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster
v0.9.1,"than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as"
v0.9.1,"possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples."
v0.9.1,"To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many"
v0.9.1,samples. The number of samples that are evaluated is normally
v0.9.1,"baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to"
v0.9.1,"batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is"
v0.9.1,evaluated one by one in a for-loop.
v0.9.1,Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple
v0.9.1,"batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet."
v0.9.1,"If the batch size would be larger than the remaining amount of samples, it is reduced to only include the"
v0.9.1,remaining baseline_noise_samples.
v0.9.1,"The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features"
v0.9.1,in baseline_feature_indices to their respective values in baseline_noise_samples.
v0.9.1,"After creating the (potentially large) input data matrix, we can evaluate the prediction method."
v0.9.1,"Here, offset + index now indicates the sample index in baseline_noise_samples."
v0.9.1,This would average all prediction results obtained for the 'offset + index'-th sample in
v0.9.1,"baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)]."
v0.9.1,This would return all prediction results obtained for the 'offset + index'-th sample in
v0.9.1,"baseline_noise_samples, i.e. the results are not averaged."
v0.9.1,Making copy to ensure that the original object is not modified.
v0.9.1,Permute samples jointly. This still represents an interventional distribution.
v0.9.1,Permute samples independently.
v0.9.1,"test local Markov condition, null hypothesis: conditional independence"
v0.9.1,"test edge dependence, null hypothesis: independence"
v0.9.1,The order of the p-values added to the list is deterministic.
v0.9.1,"To be able to validate that the graph structure did not change between fitting and causal query, we store the"
v0.9.1,"parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While"
v0.9.1,"this would automatically fail when the number of parents is different, there are other more subtle cases,"
v0.9.1,"where the number is still the same, but it's different parents, and therefore different data. That would yield"
v0.9.1,"wrong results, but would not fail."
v0.9.1,Wrapping labels if they are too long
v0.9.1,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.9.1,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.9.1,"target quantity (here, variance)."
v0.9.1,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.9.1,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.9.1,"target quantity (here, variance)."
v0.9.1,"Calculate Ri, the product of the residuals"
v0.9.1,Standard deviation of the residuals
v0.9.1,Either X and/or Y is constant.
v0.9.1,"If Z is empty, we are in the pairwise setting."
v0.9.1,Either X and/or Y is constant.
v0.9.1,"If Z is empty, we are in the pairwise setting."
v0.9.1,Estimate test statistic multiple times on different permutations of the data. The p-value is then the
v0.9.1,probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted
v0.9.1,data.
v0.9.1,First stage statistical model
v0.9.1,Second stage statistical model
v0.9.1,Check if the treatment is one-dimensional
v0.9.1,First stage
v0.9.1,Second Stage
v0.9.1,Combining the two estimates
v0.9.1,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.9.1,Bulding the feature matrix
v0.9.1,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.9.1,TODO move this to the identification step
v0.9.1,Obtain estimate by Wald Estimator
v0.9.1,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.9.1,More than 1 instrument. Use 2sls.
v0.9.1,Checking if Y is binary
v0.9.1,Enable the user to pass params for a custom propensity model
v0.9.1,Convert the categorical variables into dummy/indicator variables
v0.9.1,"Basically, this gives a one hot encoding for each category"
v0.9.1,The first category is taken to be the base line.
v0.9.1,Check if the treatment is one-dimensional
v0.9.1,Checking if the treatment is binary
v0.9.1,The model is always built on the entire data
v0.9.1,TODO make treatment_value and control value also as local parameters
v0.9.1,All treatments are set to the same constant value
v0.9.1,Using all data by default
v0.9.1,"Fixing treatment value to the specified value, if provided"
v0.9.1,treatment_vals and data_df should have same number of rows
v0.9.1,Bulding the feature matrix
v0.9.1,The model is always built on the entire data
v0.9.1,Replacing treatment values by given x
v0.9.1,"First, create interventional tensor in original space"
v0.9.1,"Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment"
v0.9.1,The average treatment effect is a combination of different
v0.9.1,regression coefficients. Complicated to compute the confidence
v0.9.1,"interval analytically. For example, if y=a + b1.t + b2.tx, then"
v0.9.1,the average treatment effect is b1+b2.mean(x).
v0.9.1,"Refer Gelman, Hill. ARM Book. Chapter 9"
v0.9.1,http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
v0.9.1,TODO: Looking for contributions
v0.9.1,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.9.1,"variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0."
v0.9.1,"So for custom treatment and control values, we must multiply the confidence interval by the difference of the two."
v0.9.1,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.9.1,"variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0."
v0.9.1,"So for custom treatment and control values, we must multiply the standard error by the difference of the two."
v0.9.1,setting method-specific parameters
v0.9.1,Infer the right strata based on clipping threshold
v0.9.1,0.5 because there are two values for the treatment
v0.9.1,To be conservative and allow most strata to be included in the
v0.9.1,analysis
v0.9.1,At least 90% of the strata should be included in analysis
v0.9.1,sum weighted outcomes over all strata  (weight by treated population)
v0.9.1,TODO - how can we add additional information into the returned estimate?
v0.9.1,"such as how much clipping was done, or per-strata info for debugging?"
v0.9.1,sort the dataframe by propensity score
v0.9.1,create a column 'strata' for each element that marks what strata it belongs to
v0.9.1,"for each strata, count how many treated and control units there are"
v0.9.1,throw away strata that have insufficient treatment or control
v0.9.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.9.1,Setting method specific parameters
v0.9.1,trim propensity score weights
v0.9.1,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.9.1,nips ==> ips / (sum of ips over all units)
v0.9.1,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.9.1,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.9.1,Vanilla IPS estimator
v0.9.1,The Hajek estimator (or the self-normalized estimator)
v0.9.1,"Stabilized weights (from Robins, Hernan, Brumback (2000))"
v0.9.1,Paper: Marginal Structural Models and Causal Inference in Epidemiology
v0.9.1,Calculating the effect
v0.9.1,Subtracting the weighted means
v0.9.1,TODO - how can we add additional information into the returned estimate?
v0.9.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.9.1,Save parameters for later refutter fitting
v0.9.1,Enforcing this ordering is necessary to feed through the propensity values from dataset
v0.9.1,For metalearners only--issue a warning if w contains variables not in x
v0.9.1,Override the effect_modifiers set in CausalEstimator.__init__()
v0.9.1,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.9.1,the latter can be used by other estimator methods later
v0.9.1,"Instrumental variables names, if present"
v0.9.1,choosing the instrumental variable to use
v0.9.1,Calling the econml estimator's fit method
v0.9.1,"As of v0.9, econml has some kewyord only arguments"
v0.9.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9.1,Changing shape to a list for a singleton value
v0.9.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.9.1,"For each unit, return the estimated effect of the treatment value"
v0.9.1,that was actually applied to the unit
v0.9.1,this assumes a binary treatment regime
v0.9.1,TODO remove neighbors that are more than a given radius apart
v0.9.1,estimate ATT on treated by summing over difference between matched neighbors
v0.9.1,Now computing ATC
v0.9.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.9.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9.1,Handle externally provided estimator classes
v0.9.1,allowed types of distance metric
v0.9.1,Dictionary of any user-provided params for the distance metric
v0.9.1,that will be passed to sklearn nearestneighbors
v0.9.1,Check if the treatment is one-dimensional
v0.9.1,Checking if the treatment is binary
v0.9.1,Convert the categorical variables into dummy/indicator variables
v0.9.1,"Basically, this gives a one hot encoding for each category"
v0.9.1,The first category is taken to be the base line.
v0.9.1,this assumes a binary treatment regime
v0.9.1,TODO remove neighbors that are more than a given radius apart
v0.9.1,estimate ATT on treated by summing over difference between matched neighbors
v0.9.1,Return indices in the original dataframe
v0.9.1,self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()
v0.9.1,Now computing ATC
v0.9.1,Return indices in the original dataframe
v0.9.1,Add the identification method used in the estimator
v0.9.1,Check the backdoor variables being used
v0.9.1,Add the observed confounders and one hot encode the categorical variables
v0.9.1,Get the data of the unobserved confounders
v0.9.1,One hot encode the data if they are categorical
v0.9.1,Check the instrumental variables involved
v0.9.1,Perform the same actions as the above
v0.9.1,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.9.1,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.9.1,For CATEs
v0.9.1,TODO we are conditioning on a postive treatment
v0.9.1,TODO create an expression corresponding to each estimator used
v0.9.1,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.9.1,Flipping some values
v0.9.1,Finding p-value using student T test
v0.9.1,Only consider edges have absolute edge weight > 0.01
v0.9.1,Modify graph such that it only contains bidirected edges
v0.9.1,Find c components by finding connected components on the undirected graph
v0.9.1,Understanding Neural Network weights
v0.9.1,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.9.1,add weight column
v0.9.1,before weights are applied we count number rows in each category
v0.9.1,which is equivalent to summing over weight=1
v0.9.1,after weights are applied we need to sum over the given weights
v0.9.1,"First, calculating mean differences by strata"
v0.9.1,"Second, without strata"
v0.9.1,"Third, concatenating them and plotting"
v0.9.1,Setting estimator attribute for convenience
v0.9.1,Outcome is numeric
v0.9.1,Treatments are also numeric or binary
v0.9.1,Outcome is categorical
v0.9.1,Treatments are numeric or binary
v0.9.1,TODO: A common way to show all plots
v0.9.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9.1,Get adjacency list
v0.9.1,If node pair has been fully explored
v0.9.1,Add node1 to backdoor set of node_pair
v0.9.1,Check if path is backdoor and does not have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.9.1,"True if arrow incoming, False if arrow outgoing"
v0.9.1,"Mark pair (node1, node2) complete"
v0.9.1,Modify variable count and indices covered
v0.9.1,Average total effect
v0.9.1,Natural direct effect
v0.9.1,Natural indirect effect
v0.9.1,Controlled direct effect
v0.9.1,Backdoor method names
v0.9.1,"First, check if there is a directed path from action to outcome"
v0.9.1,## 1. BACKDOOR IDENTIFICATION
v0.9.1,Pick algorithm to compute backdoor sets according to method chosen
v0.9.1,"First, checking if there are any valid backdoor adjustment sets"
v0.9.1,"Setting default ""backdoor"" identification adjustment set"
v0.9.1,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.9.1,Now checking if there is also a valid iv estimand
v0.9.1,## 3. FRONTDOOR IDENTIFICATION
v0.9.1,Now checking if there is a valid frontdoor variable
v0.9.1,Finally returning the estimand object
v0.9.1,Pick algorithm to compute backdoor sets according to method chosen
v0.9.1,"Setting default ""backdoor"" identification adjustment set"
v0.9.1,Finally returning the estimand object
v0.9.1,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.9.1,"First, checking if there are any valid backdoor adjustment sets"
v0.9.1,"Setting default ""backdoor"" identification adjustment set"
v0.9.1,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.9.1,Now checking if there are valid mediator variables
v0.9.1,Finally returning the estimand object
v0.9.1,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.9.1,"First, checking if there are any valid backdoor adjustment sets"
v0.9.1,"Setting default ""backdoor"" identification adjustment set"
v0.9.1,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.9.1,Now checking if there are valid mediator variables
v0.9.1,Finally returning the estimand object
v0.9.1,"First, checking if empty set is a valid backdoor set"
v0.9.1,"If the method is `minimal-adjustment`, return the empty set right away."
v0.9.1,"Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible."
v0.9.1,only remove descendants of Y
v0.9.1,also allow any causes of Y that are not caused by T (for lower variance)
v0.9.1,remove descendants of T (mediators) and descendants of Y
v0.9.1,"If var is d-separated from both treatment or outcome, it cannot"
v0.9.1,be a part of the backdoor set
v0.9.1,repeat the above search with BACKDOOR_MIN
v0.9.1,"If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest."
v0.9.1,"If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set."
v0.9.1,"If all variables are observed, and the biggest eligible set"
v0.9.1,"does not satisfy backdoor, then none of its subsets will."
v0.9.1,Adding a None estimand if no backdoor set found
v0.9.1,"Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable."
v0.9.1,Default set is the one with the least number of adjustment variables (optimizing for efficiency)
v0.9.1,"For simplicity, assuming a one-variable frontdoor set"
v0.9.1,Cond 1: All directed paths intercepted by candidate_var
v0.9.1,Cond 2: No confounding between treatment and candidate var
v0.9.1,Cond 3: treatment blocks all confounding between candidate_var and outcome
v0.9.1,"For simplicity, assuming a one-variable mediation set"
v0.9.1,"Create estimands dict as per the API for backdoor, but do not return it"
v0.9.1,"Setting default ""backdoor"" identification adjustment set"
v0.9.1,"Create estimands dict as per the API for backdoor, but do not return it"
v0.9.1,"Setting default ""backdoor"" identification adjustment set"
v0.9.1,"TODO: outputs string for now, but ideally should do symbolic"
v0.9.1,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.9.1,TODO Better support for multivariate treatments
v0.9.1,TODO: support multivariate treatments better.
v0.9.1,TODO: support multivariate treatments better.
v0.9.1,TODO: support multivariate treatments better.
v0.9.1,For direct effect
v0.9.1,"If no costs are passed, use uniform costs"
v0.9.1,restriction to ancestors
v0.9.1,back-door graph
v0.9.1,moralization
v0.9.1,Estimators list for returning after identification
v0.9.1,Line 1
v0.9.1,"If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y."
v0.9.1,Line 2
v0.9.1,"If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y."
v0.9.1,Modify list of valid nodes
v0.9.1,Line 3 - forces an action on any node where such an action would have no effect on Y – assuming we already acted on X.
v0.9.1,Modify adjacency matrix to obtain that corresponding to do(X)
v0.9.1,Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.
v0.9.1,"If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases."
v0.9.1,Modify adjacency matrix to remove treatment variables
v0.9.1,"Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes."
v0.9.1,"Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem."
v0.9.1,Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.
v0.9.1,Do not show backdoor key unless it is the only backdoor set.
v0.9.1,Just show the default backdoor set
v0.9.1,self._identified_estimand = self._causal_model.identify_effect()
v0.9.1,"self._identified_estimand,"
v0.9.1,"self._causal_model._treatment,"
v0.9.1,"self._causal_model._outcome,"
v0.9.1,If labels provided
v0.9.1,Return in valid DOT format
v0.9.1,Get adjacency matrix
v0.9.1,If labels not provided
v0.9.1,Obtain valid DOT format
v0.9.1,If labels provided
v0.9.1,Return in valid DOT format
v0.9.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9,-*- coding: utf-8 -*-
v0.9,
v0.9,Configuration file for the Sphinx documentation builder.
v0.9,
v0.9,This file does only contain a selection of the most common options. For a
v0.9,full list see the documentation:
v0.9,http://www.sphinx-doc.org/en/stable/config
v0.9,-- Path setup --------------------------------------------------------------
v0.9,"If extensions (or modules to document with autodoc) are in another directory,"
v0.9,add these directories to sys.path here. If the directory is relative to the
v0.9,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.9,
v0.9,-- Project information -----------------------------------------------------
v0.9,Version Information (for version-switcher)
v0.9,-- General configuration ---------------------------------------------------
v0.9,"If your documentation needs a minimal Sphinx version, state it here."
v0.9,
v0.9,needs_sphinx = '1.0'
v0.9,"Add any Sphinx extension module names here, as strings. They can be"
v0.9,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.9,ones.
v0.9,sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme
v0.9,already loads it
v0.9,"Add any paths that contain templates here, relative to this directory."
v0.9,The suffix(es) of source filenames.
v0.9,You can specify multiple suffix as a list of string:
v0.9,
v0.9,"source_suffix = ['.rst', '.md']"
v0.9,The master toctree document.
v0.9,The language for content autogenerated by Sphinx. Refer to documentation
v0.9,for a list of supported languages.
v0.9,
v0.9,This is also used if you do content translation via gettext catalogs.
v0.9,"Usually you set ""language"" from the command line for these cases."
v0.9,"List of patterns, relative to source directory, that match files and"
v0.9,directories to ignore when looking for source files.
v0.9,This pattern also affects html_static_path and html_extra_path .
v0.9,The name of the Pygments (syntax highlighting) style to use.
v0.9,-- Options for HTML output -------------------------------------------------
v0.9,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.9,a list of builtin themes.
v0.9,
v0.9,Theme options are theme-specific and customize the look and feel of a theme
v0.9,"further.  For a list of options available for each theme, see the"
v0.9,documentation.
v0.9,
v0.9,"Add any paths that contain custom static files (such as style sheets) here,"
v0.9,"relative to this directory. They are copied after the builtin static files,"
v0.9,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.9,"Custom sidebar templates, must be a dictionary that maps document names"
v0.9,to template names.
v0.9,
v0.9,The default sidebars (for documents that don't match any pattern) are
v0.9,defined by theme itself.  Builtin themes are using these templates by
v0.9,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.9,'searchbox.html']``.
v0.9,
v0.9,html_sidebars = {}
v0.9,-- Options for HTMLHelp output ---------------------------------------------
v0.9,Output file base name for HTML help builder.
v0.9,-- Options for LaTeX output ------------------------------------------------
v0.9,The paper size ('letterpaper' or 'a4paper').
v0.9,
v0.9,"'papersize': 'letterpaper',"
v0.9,"The font size ('10pt', '11pt' or '12pt')."
v0.9,
v0.9,"'pointsize': '10pt',"
v0.9,Additional stuff for the LaTeX preamble.
v0.9,
v0.9,"'preamble': '',"
v0.9,Latex figure (float) alignment
v0.9,
v0.9,"'figure_align': 'htbp',"
v0.9,Grouping the document tree into LaTeX files. List of tuples
v0.9,"(source start file, target name, title,"
v0.9,"author, documentclass [howto, manual, or own class])."
v0.9,-- Options for manual page output ------------------------------------------
v0.9,One entry per manual page. List of tuples
v0.9,"(source start file, name, description, authors, manual section)."
v0.9,-- Options for Texinfo output ----------------------------------------------
v0.9,Grouping the document tree into Texinfo files. List of tuples
v0.9,"(source start file, target name, title, author,"
v0.9,"dir menu entry, description, category)"
v0.9,-- Options for Epub output -------------------------------------------------
v0.9,Bibliographic Dublin Core info.
v0.9,The unique identifier of the text. This can be a ISBN number
v0.9,or the project homepage.
v0.9,
v0.9,epub_identifier = ''
v0.9,A unique identification for the text.
v0.9,
v0.9,epub_uid = ''
v0.9,A list of files that should not be packed into the epub file.
v0.9,-- Extension configuration -------------------------------------------------
v0.9,-- Options for todo extension ----------------------------------------------
v0.9,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.9,init docstrings should also be included in class
v0.9,Patch all of the published versions
v0.9,check old RST version (<= v0.8)
v0.9,Remove old version links
v0.9,Append updated version links
v0.9,requires stdin input for identify in weighting sampler
v0.9,requires Rpy2 for lalonde
v0.9,will be removed
v0.9,"applied notebook, not necessary to test each time"
v0.9,needs xgboost too
v0.9,Slow Notebooks
v0.9,requires Rpy2 for causal discovery
v0.9,daily tests of dowhy_causal_discovery_example.ipynb are failing due to cdt/rpy2 config.
v0.9,"comment out, since we are switching causal discovery implementations"
v0.9,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.9,can import dowhy
v0.9,"""--ExecutePreprocessor.timeout=600"","
v0.9,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.9,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.9,treated = self.df[self.df['z']==1]
v0.9,self.att = np.mean(treated['y1'] - treated['y0'])
v0.9,def test_average_treatment_effect(self):
v0.9,est_ate = 1
v0.9,bias = est_ate - self.ate
v0.9,print(bias)
v0.9,"self.assertAlmostEqual(self.ate, est_ate)"
v0.9,def test_average_treatment_effect_on_treated(self):
v0.9,est_att = 1
v0.9,self.att=1
v0.9,bias = est_att - self.att
v0.9,print(bias)
v0.9,"self.assertAlmostEqual(self.att, est_att)"
v0.9,removing two common causes
v0.9,removing two common causes
v0.9,removing two common causes
v0.9,removing two common causes
v0.9,check if all partial R^2 values are between 0 and 1
v0.9,We calculate adjusted estimates for two sets of partial R^2 values.
v0.9,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.9,Creating a model with no unobserved confounders
v0.9,check if all partial R^2 values are between 0 and 1
v0.9,"for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)"
v0.9,Non Parametric estimator
v0.9,We calculate adjusted estimates for two sets of partial R^2 values.
v0.9,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.9,we patched figure plotting call to avoid drawing plots during tests
v0.9,We calculate adjusted estimates for two sets of partial R^2 values.
v0.9,adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9,adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9,adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9,adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9,adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9,adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9,adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9
v0.9,adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3
v0.9,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.9,we patched figure plotting call to avoid drawing plots during tests
v0.9,comparing test examples from R E-Value package
v0.9,check implementation of Observed Covariate E-value against R package
v0.9,The outcome is a linear function of the confounder
v0.9,"The slope is 1,2 and the intercept is 3"
v0.9,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.9,Supports user-provided dataset object
v0.9,To test if there are any exceptions
v0.9,To test if the estimate is identical if refutation parameters are zero
v0.9,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.9,"Ordinarily, we should expect this value to be zero."
v0.9,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.9,"Ordinarily, we should expect this value to be zero."
v0.9,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9,Only P(Y|T) should be present for test to succeed.
v0.9,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9,"Since undirected graph, identify effect must throw an error."
v0.9,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9,Compare with ground truth
v0.9,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9,Compare with ground truth
v0.9,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9,Compare with ground truth
v0.9,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.9,Compare with ground truth
v0.9,Causal model initialization
v0.9,Obtain backdoor sets
v0.9,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.9,Causal model initialization
v0.9,Obtain backdoor sets
v0.9,Causal model initialization
v0.9,Obtain backdoor sets
v0.9,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.9,Causal model initialization
v0.9,Obtain backdoor sets
v0.9,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.9,Causal model initialization
v0.9,Obtain backdoor sets
v0.9,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.9,Causal model initialization
v0.9,Obtain backdoor sets
v0.9,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.9,"Example is selected from Pearl J. ""Causality"" 2nd Edition, from chapter 3.3.1 on backoor criterion."
v0.9,"The following simpsons paradox examples are taken from Pearl, J {2013}. ""Understanding Simpson’s Paradox"" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf"
v0.9,"The following are examples given in the ""Book of Why"" by Judea Pearl, chapter ""The Do-operator and the Back-Door Criterion"""
v0.9,For all examples from these papers we use X for the treatment variable
v0.9,instead of A.
v0.9,"Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.9,"Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.9,"Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.9,"Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika"
v0.9,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.9,L replaces X as the conditional variable
v0.9,"Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.9,L replaces X as the conditional variable. Uses different costs
v0.9,"Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference"
v0.9,"A graph where optimal, optimal minimal and optimal min cost are different"
v0.9,The graph from Shrier and Platt (2008)
v0.9,A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.9,Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri
v0.9,cov_mat = np.diag(np.ones(num_features))
v0.9,collider: X->Z<-Y
v0.9,chain: X->Z->Y
v0.9,fork: X<-Z->Y
v0.9,"general DAG: X<-Z->Y, X->Y"
v0.9,fork: X<-Z->Y
v0.9,Contributions should add up to Var(X2)
v0.9,Contributions should add up to Var(X2)
v0.9,H(P(Y)) -- Can be precomputed
v0.9,-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))
v0.9,"H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\S)])"
v0.9,"E[P(Y | x_S, X'_\S)]"
v0.9,"H(E[P(Y | x_S, X'_\S)])"
v0.9,"Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions."
v0.9,E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]
v0.9,"Just checking formats, i.e. no need for correlation."
v0.9,"Just checking formats, i.e. no need for correlation."
v0.9,Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3
v0.9,C2 = 3 * A2 + 2 * B2
v0.9,"By default, the strength is measure with respect to the variance."
v0.9,"Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2."
v0.9,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.9,"Here, changing the mechanism."
v0.9,"Here, changing the mechanism."
v0.9,"Here, changing the mechanism."
v0.9,"Here, changing the mechanism."
v0.9,Defining an anomaly scorer that handles multidimensional inputs.
v0.9,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.9,"In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution"
v0.9,"and it should be ""significantly"" higher than the contribution of the other ones (here, we just arbitrarily say"
v0.9,"it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,"
v0.9,"which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate"
v0.9,"some robustness. Note that due to this and stochastic behaviour of the density estimator, it is"
v0.9,"not possible to analytically compute expected results. Therefore, we rather look at the relations here."
v0.9,"Same idea for the second sample, but here, it is the second variable that is anomalous."
v0.9,"In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be"
v0.9,"""significantly"" higher than the contribution of the other variables. The contribution of both anomalous variables"
v0.9,should be equal (approximately).
v0.9,Defining an anomaly scorer that handles multidimensional inputs.
v0.9,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.9,reduce the score.
v0.9,The contributions should add up to g(x) - E[g(X)]
v0.9,The contributions should add up to g(x) - E[g(X)]
v0.9,The contributions should add up to g(x) - E[g(X)]
v0.9,Three examples:
v0.9,1. X1 is the root cause (+ 10 to the noise)
v0.9,2. X0 is the root cause (+ 10 to the noise)
v0.9,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.9,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.9,Three examples:
v0.9,1. X1 is the root cause (+ 10 to the noise)
v0.9,2. X0 is the root cause (+ 10 to the noise)
v0.9,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.9,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.9,"Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution"
v0.9,algorithm.
v0.9,1. X0 is the root cause (+ 10 to the noise)
v0.9,2. X0 and X1 are the root causes (+ 10 to both noise)
v0.9,3. X2 and X3 are both root causes (+ 10 to both noises)
v0.9,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.9,Check if calling the method causes some import or runtime errors
v0.9,TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer
v0.9,newer matplotlib version:
v0.9,AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'
v0.9,Networkx 2.4+ should fix this issue.
v0.9,"plot_adjacency_matrix(causal_graph, is_directed=False)"
v0.9,Setup data
v0.9,Test LinearDML
v0.9,Test ContinuousTreatmentOrthoForest
v0.9,Test LinearDRLearner
v0.9,Setup data
v0.9,Test DeepIV
v0.9,"Treatment model,"
v0.9,Response model
v0.9,Test IntentToTreatDRIV
v0.9,Observed data
v0.9,assumed graph
v0.9,Identify effect
v0.9,Estimate effect
v0.9,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.9,More cases where Exception  is expected
v0.9,"Compute confidence intervals, standard error and significance tests"
v0.9,Defined a linear dataset with a given set of properties
v0.9,Create a model that captures the same
v0.9,Identify the effects within the model
v0.9,Defined a linear dataset with a given set of properties
v0.9,Create a model that captures the same
v0.9,Identify the effects within the model
v0.9,Defined a linear dataset with a given set of properties
v0.9,Create a model that captures the same
v0.9,Identify the effects within the model
v0.9,Defined a linear dataset with a given set of properties
v0.9,Create a model that captures the same
v0.9,Identify the effects within the model
v0.9,Defined a linear dataset with a given set of properties
v0.9,Create a model that captures the same
v0.9,Identify the effects within the model
v0.9,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.9,Unpacking the keyword arguments
v0.9,todo: add docstring for common parameters here and remove from child refuter classes
v0.9,Default value for the number of simulations to be conducted
v0.9,joblib params for parallel processing
v0.9,"Concatenate the confounders, instruments and effect modifiers"
v0.9,Shuffle the confounders
v0.9,Check if all are select or deselect variables
v0.9,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.9,Get the number of simulations
v0.9,Sort the simulations
v0.9,Obtain the median value
v0.9,Performing a two sided test
v0.9,np.searchsorted tells us the index if it were a part of the array
v0.9,We select side to be left as we want to find the first value that matches
v0.9,We subtact 1 as we are finding the value from the right tail
v0.9,We take the side to be right as we want to find the last index that matches
v0.9,We get the probability with respect to the left tail.
v0.9,return twice the determined quantile as this is a two sided test
v0.9,Get the mean for the simulations
v0.9,Get the standard deviation for the simulations
v0.9,Get the Z Score [(val - mean)/ std_dev ]
v0.9,Initializing the p_value
v0.9,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.9,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.9,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.9,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.9,re.sub only takes string parameter so the first if is to avoid error
v0.9,"if the input is a text file, convert the contained data into string"
v0.9,load dot file
v0.9,Adding node attributes
v0.9,adding penwidth to make the edge bold
v0.9,Adding common causes
v0.9,Adding instruments
v0.9,Adding effect modifiers
v0.9,Assuming the simple form of effect modifier
v0.9,that directly causes the outcome.
v0.9,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.9,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.9,Adding columns in the dataframe as confounders that were not in the graph
v0.9,Adding unobserved confounders
v0.9,removal of only direct edges wrt a target is not implemented for incoming edges
v0.9,also return the number of backdoor paths blocked by observed nodes
v0.9,Assume that nodes1 is the treatment
v0.9,"ignores new_graph parameter, always uses self._graph"
v0.9,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.9,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.9,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.9,removing all mediators
v0.9,[TODO: double check these work with multivariate implementation:]
v0.9,Exclusion
v0.9,As-if-random setup
v0.9,As-if-random
v0.9,convert the outputted generator into a list
v0.9,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.9,return len(dpaths) > 0
v0.9,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.9,Create causal graph object
v0.9,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.9,(Because some effect modifiers may also be common causes)
v0.9,"In such cases, the user-provided modifiers are used."
v0.9,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.9,Import causal discovery class
v0.9,Initialize causal graph object
v0.9,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.9,TODO add dowhy as a prefix to all dowhy estimators
v0.9,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.9,Define the third-party estimation method to be used
v0.9,Process the dowhy estimators
v0.9,Check if estimator's target estimand is identified
v0.9,"Note that while the name of the variable is the same,"
v0.9,"""self.causal_estimator"", this estimator takes in less"
v0.9,parameters than the same from the
v0.9,estimate_effect code. It is not advisable to use the
v0.9,estimator from this function to call estimate_effect
v0.9,with fit_estimator=False.
v0.9,Estimator had been computed in a previous call
v0.9,The default number of simulations for statistical testing
v0.9,The default number of simulations to obtain confidence intervals
v0.9,The portion of the total size that should be taken each time to find the confidence intervals
v0.9,1 is the recommended value
v0.9,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.9,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.9,The default Confidence Level
v0.9,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.9,Prefix to add to temporary categorical variables created after discretization
v0.9,std args to be removed from locals() before being passed to args_dict
v0.9,Setting the default interpret method
v0.9,"Check if some parameters were set, otherwise set to default values"
v0.9,Estimate conditional estimates by default
v0.9,TODO Only works for binary treatment
v0.9,Defaulting to class default values if parameters are not provided
v0.9,Checking that there is at least one effect modifier
v0.9,Making sure that effect_modifier_names is a list
v0.9,Making a copy since we are going to be changing effect modifier names
v0.9,"For every numeric effect modifier, adding a temp categorical column"
v0.9,Grouping by effect modifiers and computing effect separately
v0.9,Deleting the temporary categorical columns
v0.9,The array that stores the results of all estimations
v0.9,Find the sample size the proportion with the population size
v0.9,Perform the set number of simulations
v0.9,names of treatment and outcome
v0.9,Using class default parameters if not specified
v0.9,Checking if bootstrap_estimates are already computed
v0.9,Checked if any parameter is changed from the previous std error estimate
v0.9,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.9,Get the variations of each bootstrap estimate and sort
v0.9,"Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level"
v0.9,Get the lower and upper bounds by subtracting the variations from the estimate
v0.9,"Use existing params, if new user defined params are not present"
v0.9,Checking if bootstrap_estimates are already computed
v0.9,Check if any parameter is changed from the previous std error estimate
v0.9,"Use existing params, if new user defined params are not present"
v0.9,"self._outcome = self._data[""dummy_outcome""]"
v0.9,Processing the null hypothesis estimates
v0.9,Doing a two-sided test
v0.9,Being conservative with the p-value reported
v0.9,Being conservative with the p-value reported
v0.9,"If the estimate_index is 0, it depends on the number of simulations"
v0.9,Need to test r-squared before supporting
v0.9,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.9,'r-squared': effect_r_squared
v0.9,"elif method == ""r-squared"":"
v0.9,outcome_mean = np.mean(self._outcome)
v0.9,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.9,Assuming a linear model with one variable: the treatment
v0.9,Currently only works for continuous y
v0.9,causal_model = outcome_mean + estimate.value*self._treatment
v0.9,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.9,r_squared = 1 - (squared_residual/total_variance)
v0.9,return r_squared
v0.9,Check if estimator's target estimand is identified
v0.9,Store parameters inside estimate object for refutation methods
v0.9,TODO: This add_params needs to move to the estimator class
v0.9,inside estimate_effect and estimate_conditional_effect
v0.9,No estimand was identified (identification failed)
v0.9,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.9,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.9,one-hot encode discrete W
v0.9,Now deleting the old continuous value
v0.9,create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause
v0.9,"Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately"
v0.9,One Hot Encoding
v0.9,Making beta an array
v0.9,TODO Ensure that we do not generate weak instruments
v0.9,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.9,Converting treatment to binary if required
v0.9,Generating frontdoor variables if asked for
v0.9,NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.
v0.9,"For categorical t, this example dataset has the effect size for category 2 being exactly"
v0.9,double the effect for category 1
v0.9,This could be changed at this stage by one-hot encoding t and using a custom beta that
v0.9,"sets a different effect for each category {0, 1, 2}"
v0.9,Computing ATE
v0.9,constructing column names for one-hot encoded discrete features
v0.9,Specifying the correct dtypes
v0.9,Now specifying the corresponding graph strings
v0.9,Now writing the gml graph
v0.9,Making beta an array
v0.9,creating data frame
v0.9,Specifying the correct dtypes
v0.9,Now specifying the corresponding graph strings
v0.9,Now writing the gml graph
v0.9,Adding edges between common causes and the frontdoor mediator
v0.9,Error terms
v0.9,else:
v0.9,V = 6 + W0 + tterm + E1
v0.9,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.9,Generating a random normal distribution of integers
v0.9,Generating data for nodes which have no incoming edges
v0.9,"""currset"" variable currently has all the successors of the nodes which had no incoming edges"
v0.9,Making beta an array
v0.9,assuming that all unobserved common causes are numerical and are not affected by one hot encoding
v0.9,Creating a NN to simulate the nuisance function
v0.9,strength of unobserved confounding
v0.9,Computing ATE
v0.9,Specifying the correct dtypes
v0.9,Now writing the gml graph
v0.9,The following code for loading the Lalonde dataset was copied from
v0.9,https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).
v0.9,
v0.9,"Copyright 2018, Wayfair, Inc."
v0.9,
v0.9,"Redistribution and use in source and binary forms, with or without modification, are permitted provided that"
v0.9,the following conditions are met:
v0.9,
v0.9,"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the"
v0.9,following disclaimer.
v0.9,
v0.9,"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
v0.9,following disclaimer in the documentation and/or other materials provided with the distribution.
v0.9,
v0.9,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED"
v0.9,"WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
v0.9,PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
v0.9,"DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,"
v0.9,"PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER"
v0.9,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR"
v0.9,"OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH"
v0.9,DAMAGE.
v0.9,
v0.9,0.0.0 is standard placeholder for poetry-dynamic-versioning
v0.9,any changes to this should not be checked in
v0.9,
v0.9,The currently supported estimators
v0.9,The default standard deviation for noise
v0.9,The default scaling factor to determine the bucket size
v0.9,The minimum number of points for the estimator to run
v0.9,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.9,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.9,The Default split for the number of data points that fall into the training and validation sets
v0.9,Assuming that outcome is one-dimensional
v0.9,We need to change the identified estimand
v0.9,"We thus, make a copy. This is done as we don't want"
v0.9,to change the original DataFrame
v0.9,We use collections.OrderedDict to maintain the order in which the data is stored
v0.9,Check if we are using an estimator in the transformation list
v0.9,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.9,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.9,"loops. Thus, we can get different values everytime we get the estimator."
v0.9,for _ in range( self._num_simulations ):
v0.9,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.9,Adding an unobserved confounder if provided by the user
v0.9,We set X_train = 0 and outcome_train to be 0
v0.9,"Get the final outcome, after running through all the values in the transformation list"
v0.9,Check if the value of true effect has been already stored
v0.9,We use None as the key as we have no base category for this refutation
v0.9,As we currently support only one treatment
v0.9,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.9,Check if the value of true effect has been already stored
v0.9,This ensures that we calculate the causal effect only once.
v0.9,We use key_train as we map data with respect to the base category of the data
v0.9,As we currently support only one treatment
v0.9,Add h(t) to f(W) to get the dummy outcome
v0.9,We convert to ndarray for ease in indexing
v0.9,The data is of the form
v0.9,sim1: cat1 cat2 ... catn
v0.9,sim2: cat1 cat2 ... catn
v0.9,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.9,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.9,distribution of the refuter.
v0.9,True Causal Effect list
v0.9,Iterating through the refutation for each category
v0.9,We use string arguments to account for both 32 and 64 bit varaibles
v0.9,action for continuous variables
v0.9,Action for categorical variables
v0.9,Find the set difference for each row
v0.9,Choose one out of the remaining
v0.9,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.9,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.9,of the treatment to affect the outcome
v0.9,Standardizing the data
v0.9,Fit a model containing all confounders and compare predictions
v0.9,using all features compared to all features except a given
v0.9,confounder.
v0.9,Estimating the regression coefficient from standardized features to t
v0.9,"By default, return a plot with 10 points"
v0.9,consider 10 values of the effect of the unobserved confounder
v0.9,Standardizing the data
v0.9,Fit a model containing all confounders and compare predictions
v0.9,using all features compared to all features except a given
v0.9,confounder.
v0.9,"By default, return a plot with 10 points"
v0.9,consider 10 values of the effect of the unobserved confounder
v0.9,"By default, we add the effect of simulated confounder for treatment."
v0.9,But subtract it from outcome to create a negative correlation
v0.9,assuming that the original confounder's effect was positive on both.
v0.9,This is to remove the effect of the original confounder.
v0.9,"By default, we add the effect of simulated confounder for treatment."
v0.9,But subtract it from outcome to create a negative correlation
v0.9,assuming that the original confounder's effect was positive on both.
v0.9,This is to remove the effect of the original confounder.
v0.9,Obtaining the list of observed variables
v0.9,Taking a subset of the dataframe that has only observed variables
v0.9,Residuals from the outcome model obtained by fitting a linear model
v0.9,Residuals from the treatment model obtained by fitting a linear model
v0.9,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.9,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.9,Choosing a c_star based on the data.
v0.9,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.9,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.9,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.9,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.9,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.9,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.9,"If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen"
v0.9,Get a 2D matrix of values
v0.9,"x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN"
v0.9,Store the values into the refute object
v0.9,Adding a label on the contour line for the original estimate
v0.9,Label every other level using strings
v0.9,Default value of the p value taken for the distribution
v0.9,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.9,Mean of the Normal Distribution
v0.9,Standard Deviation of the Normal Distribution
v0.9,Create a new column in the data by the name of placebo
v0.9,Sanity check the data
v0.9,only permute is supported for iv methods
v0.9,"For IV methods, the estimating_instrument_names should also be"
v0.9,changed. Create a copy to avoid modifying original object
v0.9,We need to change the identified estimand
v0.9,"We make a copy as a safety measure, we don't want to change the"
v0.9,original DataFrame
v0.9,Run refutation in parallel
v0.9,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.9,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.9,relationship between the treatment and the outcome.
v0.9,new estimator
v0.9,new effect estimate
v0.9,observed covariate E-value
v0.9,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.9,"if CI crosses null, set its E-value to 1"
v0.9,only report E-value for CI limit closer to null
v0.9,see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf
v0.9,whether the DGP is assumed to be partially linear
v0.9,features are the observed confounders
v0.9,Now code for benchmarking using covariates begins
v0.9,R^2 of outcome with observed common causes and treatment
v0.9,R^2 of treatment with observed common causes
v0.9,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.9,Assuming that the difference in R2 is the same for wj and new unobserved confounder
v0.9,"for treatment,  Calpha is not a function of the partial R2. So we need a different assumption."
v0.9,Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder
v0.9,"(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment"
v0.9,wrt unobserved confounders in partial-linear models
v0.9,whether the DGP is assumed to be partially linear
v0.9,can change this to allow default values that are same as the other parameter
v0.9,Strength of confounding that omitted variables generate in treatment regression
v0.9,computing the point estimate for the bounds
v0.9,common causes after removing the benchmark causes
v0.9,dataframe with treatment and observed common causes after removing benchmark causes
v0.9,R^2 of treatment with observed common causes removing benchmark causes
v0.9,return the variance of alpha_s
v0.9,R^2 of outcome with observed common causes and treatment after removing benchmark causes
v0.9,Obtaining theta_s (the obtained estimate)
v0.9,Creating numpy arrays
v0.9,Setting up cross-validation parameters
v0.9,"tuple of residuals from first stage estimation [0,1], and the confounders [2]"
v0.9,"We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation."
v0.9,Yres = Y - E[Y|W]
v0.9,E[Y|W] = f(x) + theta_s * E[T|W]
v0.9,Yres = Y - f(x) - theta_s * E[T|W]
v0.9,g(s) = theta_s * T + f(x)
v0.9,g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]
v0.9,g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]
v0.9,Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )
v0.9,Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres
v0.9,Y - g(s) = Yres - theta_s * Tres
v0.9,nu_2 is E[alpha_s^2]
v0.9,Now computing scores for finding the (1-a) confidence interval
v0.9,R^2 of treatment with observed common causes
v0.9,R^2 of outcome with treatment and observed common causes
v0.9,"Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment"
v0.9,Partial R^2 of treatment after regressing over unobserved confounder and observed common causes
v0.9,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.9,adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot
v0.9,Adding unadjusted point estimate
v0.9,Adding bounds to partial R^2 values for given strength of confounders
v0.9,Adding a new backdoor variable to the identified estimand
v0.9,Run refutation in parallel
v0.9,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.9,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.9,of the treatment to affect the outcome
v0.9,TODO: Sensitivity Analyzers excluded from list due to different return type
v0.9,Reject H0
v0.9,"a, b and c are all continuous variables"
v0.9,"a, b and c are all discrete variables"
v0.9,c is set of continuous and binary variables and
v0.9,1. either a and b is continuous and the other is binary
v0.9,2. both a and b are binary
v0.9,c is discrete and
v0.9,either a or b is continuous and the other is discrete
v0.9,a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information
v0.9,original_treatment_name: : stores original variable names for labelling
v0.9,common_causes_map : maps the original variable names to variable names in OLS regression
v0.9,benchmark_common_causes: stores variable names in terms of regression model variables
v0.9,original_benchmark_covariates: stores original variable names for labelling
v0.9,estimate: estimate of regression
v0.9,degree_of_freedom: degree of freedom of error in regression
v0.9,standard_error: standard error in regression
v0.9,t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.
v0.9,partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them
v0.9,"r2tu_w: partial R^2  of unobserved confounder ""u"" with treatment ""t"", after conditioning on observed covariates ""w"""
v0.9,"r2yu_tw: partial R^2  of unobserved confounder ""u"" with outcome ""y"", after conditioning on observed covariates ""w"" and treatment ""t"""
v0.9,"r2twj_w: partial R^2 of observed covariate wj with treatment ""t"", after conditioning on observed covariates ""w"" excluding wj"
v0.9,"r2ywj_tw:  partial R^2 of observed covariate wj with outcome ""y"", after conditioning on observed covariates ""w"" (excluding wj) and treatment ""t"""
v0.9,benchmarking_results: dataframe containing information about bounds and bias adjusted terms
v0.9,"stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic"
v0.9,partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment
v0.9,build a new regression model by considering treatment variables as outcome
v0.9,"r2twj_w is partial R^2 of covariate wj with treatment ""t"", after conditioning on covariates w(excluding wj)"
v0.9,"r2ywj_tw is partial R^2 of covariate wj with outcome ""y"", after conditioning on covariates w(excluding wj) and treatment ""t"""
v0.9,r2tu_w is the partial r^2 from regressing u on t after conditioning on w
v0.9,Compute bias adjusted terms
v0.9,Plotting the contour plot
v0.9,Adding contours
v0.9,Adding threshold contour line
v0.9,Adding unadjusted point estimate
v0.9,Adding bounds to partial R^2 values for given strength of confounders
v0.9,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9,The default subset of the data to be used
v0.9,Run refutation in parallel
v0.9,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.9,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.9,of the treatment to affect the outcome
v0.9,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9,"If the relative change of the score is less than the given threshold, we stop the estimation early."
v0.9,"Note, the output of score_samples are log values."
v0.9,"Note, the output of score_samples are log values."
v0.9,Currently only support continuous distributions for auto selection.
v0.9,Estimate distribution parameters from data.
v0.9,Ignore warnings from fitting process.
v0.9,Fit distribution to data.
v0.9,Some distributions might not be compatible with the data.
v0.9,Separate parts of parameters.
v0.9,Check the KL divergence between the distribution of the given and fitted distribution.
v0.9,Identify if this distribution is better.
v0.9,This error is typically raised when the data is discrete and all points are assigned to less cluster than
v0.9,"specified. It can also happen due to duplicated points. In these cases, the current best solution should"
v0.9,be sufficient.
v0.9,"Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]"
v0.9,A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This
v0.9,function evaluates the provided causal query multiple times to build a confidence interval based on the returned
v0.9,results.
v0.9,Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order
v0.9,"to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the"
v0.9,fit_and_compute function.
v0.9,
v0.9,**Example usage:**
v0.9,
v0.9,">>> gcm.fit(causal_model, data)"
v0.9,">>> strength_medians, strength_intervals = gcm.confidence_intervals("
v0.9,">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))"
v0.9,
v0.9,"In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the"
v0.9,"confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'"
v0.9,"here to configure the function call. In this case,"
v0.9,"gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to"
v0.9,"lambda : gcm.arrow_strength(causal_model, target_node='Y')."
v0.9,
v0.9,"In order to incorporate uncertainties coming from fitting the causal model(s), we can use"
v0.9,gcm.fit_and_compute instead:
v0.9,">>>  strength_medians, strength_intervals = gcm.confidence_intervals("
v0.9,">>>        gcm.fit_and_compute(gcm.arrow_strength,"
v0.9,">>>                                            causal_model,"
v0.9,">>>                                            bootstrap_training_data=data,"
v0.9,>>>                                            target_node='Y'))
v0.9,This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each
v0.9,run.
v0.9,"Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based"
v0.9,on their topological order.
v0.9,"After drawing samples of the node based on the data generation process, we apply the corresponding"
v0.9,intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this
v0.9,node.
v0.9,"Abduction: For invertible SCMs, we recover exact noise values from data."
v0.9,Action + Prediction: Propagate the intervention downstream using recovered noise values.
v0.9,Check if we need to apply an intervention on the given node.
v0.9,Apply intervention function to the data of the node.
v0.9,Check if the intervention function changes the shape of the data.
v0.9,"For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e."
v0.9,all ancestors of the target.
v0.9,The target node can be a continuous real-valued variable or a categorical variable with at most two classes
v0.9,(i.e. binary).
v0.9,Avoid too many features
v0.9,Making sure there are at least 30% test samples.
v0.9,"Making sure that there are at least 2 samples from one class (here, simply duplicate the point)."
v0.9,Compare number of correct classifications.
v0.9,This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes
v0.9,This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes
v0.9,afterwards.
v0.9,"can't use nx.node_connected_component, because it doesn't work with DiGraphs."
v0.9,Hence a manual loop:
v0.9,For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples
v0.9,is unknown beforehand.
v0.9,The method stops if either the change between some consecutive runs is below the given threshold or the
v0.9,maximum number of runs is reached.
v0.9,"In each run, we create one random permutation of players. For instance, given 4 players, a permutation"
v0.9,"could be [3,1,4,2]."
v0.9,"Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the"
v0.9,"subsets are [3], [3,1], [3,1,4] [3,1,4,2]."
v0.9,"The result for each subset is cached such that if a subset that has already been evaluated appears again,"
v0.9,we can take this result directly.
v0.9,"To improve the runtime, multiple permutations are evaluated in each run."
v0.9,"The current Shapley values are the average of the estimated values, i.e. we need to divide by the number"
v0.9,of generated permutations here.
v0.9,"Here, the change between two runs is below the minimum threshold, but to reduce the likelihood"
v0.9,"that this just happened by chance, we require that this happens at least for two runs in a row."
v0.9,Create all (unique) subsets)
v0.9,"Assigning a 'high' weight, since this resembles ""infinity""."
v0.9,The weight for a subset with a specific length (see paper mentioned in the docstring for more
v0.9,information).
v0.9,TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is
v0.9,"theoretically sound, i.e. make entropy results from different data comparable."
v0.9,Extremely small values can somehow result in negative values.
v0.9,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.9,Sampling from the conditional distribution based on the current sample.
v0.9,"Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that"
v0.9,"are in the given subset. By this, we can simulate the impact on the conditional distribution when removing"
v0.9,only the incoming edges of the variables in the subset.
v0.9,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.9,"In case of the full subset (no randomization), we get the same predictions as when we apply the"
v0.9,"prediction method to the samples of interest, since all noise samples are replaced with a sample of"
v0.9,interest.
v0.9,"In case of the empty subset (all are jointly randomize), it boils down to taking the average over all"
v0.9,"predictions, seeing that the randomization yields the same values for each sample of interest (none of the"
v0.9,samples of interest are used to replace a (jointly) 'randomized' sample).
v0.9,Smallest possible value. This is used in various algorithm for numerical stability.
v0.9,Make copy to avoid manipulating the original matrix.
v0.9,"The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,"
v0.9,the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster
v0.9,"than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as"
v0.9,"possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples."
v0.9,"To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many"
v0.9,samples. The number of samples that are evaluated is normally
v0.9,"baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to"
v0.9,"batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is"
v0.9,evaluated one by one in a for-loop.
v0.9,Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple
v0.9,"batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet."
v0.9,"If the batch size would be larger than the remaining amount of samples, it is reduced to only include the"
v0.9,remaining baseline_noise_samples.
v0.9,"The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features"
v0.9,in baseline_feature_indices to their respective values in baseline_noise_samples.
v0.9,"After creating the (potentially large) input data matrix, we can evaluate the prediction method."
v0.9,"Here, offset + index now indicates the sample index in baseline_noise_samples."
v0.9,This would average all prediction results obtained for the 'offset + index'-th sample in
v0.9,"baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)]."
v0.9,This would return all prediction results obtained for the 'offset + index'-th sample in
v0.9,"baseline_noise_samples, i.e. the results are not averaged."
v0.9,Making copy to ensure that the original object is not modified.
v0.9,Permute samples jointly. This still represents an interventional distribution.
v0.9,Permute samples independently.
v0.9,"test local Markov condition, null hypothesis: conditional independence"
v0.9,"test edge dependence, null hypothesis: independence"
v0.9,The order of the p-values added to the list is deterministic.
v0.9,"To be able to validate that the graph structure did not change between fitting and causal query, we store the"
v0.9,"parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While"
v0.9,"this would automatically fail when the number of parents is different, there are other more subtle cases,"
v0.9,"where the number is still the same, but it's different parents, and therefore different data. That would yield"
v0.9,"wrong results, but would not fail."
v0.9,Wrapping labels if they are too long
v0.9,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.9,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.9,"target quantity (here, variance)."
v0.9,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.9,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.9,"target quantity (here, variance)."
v0.9,"Calculate Ri, the product of the residuals"
v0.9,Standard deviation of the residuals
v0.9,Either X and/or Y is constant.
v0.9,"If Z is empty, we are in the pairwise setting."
v0.9,Either X and/or Y is constant.
v0.9,"If Z is empty, we are in the pairwise setting."
v0.9,Estimate test statistic multiple times on different permutations of the data. The p-value is then the
v0.9,probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted
v0.9,data.
v0.9,First stage statistical model
v0.9,Second stage statistical model
v0.9,Check if the treatment is one-dimensional
v0.9,First stage
v0.9,Second Stage
v0.9,Combining the two estimates
v0.9,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.9,Bulding the feature matrix
v0.9,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.9,TODO move this to the identification step
v0.9,Obtain estimate by Wald Estimator
v0.9,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.9,More than 1 instrument. Use 2sls.
v0.9,Checking if Y is binary
v0.9,Enable the user to pass params for a custom propensity model
v0.9,Convert the categorical variables into dummy/indicator variables
v0.9,"Basically, this gives a one hot encoding for each category"
v0.9,The first category is taken to be the base line.
v0.9,Check if the treatment is one-dimensional
v0.9,Checking if the treatment is binary
v0.9,The model is always built on the entire data
v0.9,TODO make treatment_value and control value also as local parameters
v0.9,All treatments are set to the same constant value
v0.9,Using all data by default
v0.9,"Fixing treatment value to the specified value, if provided"
v0.9,treatment_vals and data_df should have same number of rows
v0.9,Bulding the feature matrix
v0.9,The model is always built on the entire data
v0.9,Replacing treatment values by given x
v0.9,"First, create interventional tensor in original space"
v0.9,"Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment"
v0.9,The average treatment effect is a combination of different
v0.9,regression coefficients. Complicated to compute the confidence
v0.9,"interval analytically. For example, if y=a + b1.t + b2.tx, then"
v0.9,the average treatment effect is b1+b2.mean(x).
v0.9,"Refer Gelman, Hill. ARM Book. Chapter 9"
v0.9,http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
v0.9,TODO: Looking for contributions
v0.9,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.9,"variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0."
v0.9,"So for custom treatment and control values, we must multiply the confidence interval by the difference of the two."
v0.9,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.9,"variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0."
v0.9,"So for custom treatment and control values, we must multiply the standard error by the difference of the two."
v0.9,setting method-specific parameters
v0.9,Infer the right strata based on clipping threshold
v0.9,0.5 because there are two values for the treatment
v0.9,To be conservative and allow most strata to be included in the
v0.9,analysis
v0.9,At least 90% of the strata should be included in analysis
v0.9,sum weighted outcomes over all strata  (weight by treated population)
v0.9,TODO - how can we add additional information into the returned estimate?
v0.9,"such as how much clipping was done, or per-strata info for debugging?"
v0.9,sort the dataframe by propensity score
v0.9,create a column 'strata' for each element that marks what strata it belongs to
v0.9,"for each strata, count how many treated and control units there are"
v0.9,throw away strata that have insufficient treatment or control
v0.9,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.9,Setting method specific parameters
v0.9,trim propensity score weights
v0.9,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.9,nips ==> ips / (sum of ips over all units)
v0.9,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.9,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.9,Vanilla IPS estimator
v0.9,The Hajek estimator (or the self-normalized estimator)
v0.9,"Stabilized weights (from Robins, Hernan, Brumback (2000))"
v0.9,Paper: Marginal Structural Models and Causal Inference in Epidemiology
v0.9,Calculating the effect
v0.9,Subtracting the weighted means
v0.9,TODO - how can we add additional information into the returned estimate?
v0.9,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.9,Save parameters for later refutter fitting
v0.9,Enforcing this ordering is necessary to feed through the propensity values from dataset
v0.9,For metalearners only--issue a warning if w contains variables not in x
v0.9,Override the effect_modifiers set in CausalEstimator.__init__()
v0.9,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.9,the latter can be used by other estimator methods later
v0.9,"Instrumental variables names, if present"
v0.9,choosing the instrumental variable to use
v0.9,Calling the econml estimator's fit method
v0.9,"As of v0.9, econml has some kewyord only arguments"
v0.9,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9,Changing shape to a list for a singleton value
v0.9,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.9,"For each unit, return the estimated effect of the treatment value"
v0.9,that was actually applied to the unit
v0.9,this assumes a binary treatment regime
v0.9,TODO remove neighbors that are more than a given radius apart
v0.9,estimate ATT on treated by summing over difference between matched neighbors
v0.9,Now computing ATC
v0.9,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.9,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9,Handle externally provided estimator classes
v0.9,allowed types of distance metric
v0.9,Dictionary of any user-provided params for the distance metric
v0.9,that will be passed to sklearn nearestneighbors
v0.9,Check if the treatment is one-dimensional
v0.9,Checking if the treatment is binary
v0.9,Convert the categorical variables into dummy/indicator variables
v0.9,"Basically, this gives a one hot encoding for each category"
v0.9,The first category is taken to be the base line.
v0.9,this assumes a binary treatment regime
v0.9,TODO remove neighbors that are more than a given radius apart
v0.9,estimate ATT on treated by summing over difference between matched neighbors
v0.9,Return indices in the original dataframe
v0.9,self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()
v0.9,Now computing ATC
v0.9,Return indices in the original dataframe
v0.9,Add the identification method used in the estimator
v0.9,Check the backdoor variables being used
v0.9,Add the observed confounders and one hot encode the categorical variables
v0.9,Get the data of the unobserved confounders
v0.9,One hot encode the data if they are categorical
v0.9,Check the instrumental variables involved
v0.9,Perform the same actions as the above
v0.9,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.9,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.9,For CATEs
v0.9,TODO we are conditioning on a postive treatment
v0.9,TODO create an expression corresponding to each estimator used
v0.9,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.9,Flipping some values
v0.9,Finding p-value using student T test
v0.9,Only consider edges have absolute edge weight > 0.01
v0.9,Modify graph such that it only contains bidirected edges
v0.9,Find c components by finding connected components on the undirected graph
v0.9,Understanding Neural Network weights
v0.9,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.9,add weight column
v0.9,before weights are applied we count number rows in each category
v0.9,which is equivalent to summing over weight=1
v0.9,after weights are applied we need to sum over the given weights
v0.9,"First, calculating mean differences by strata"
v0.9,"Second, without strata"
v0.9,"Third, concatenating them and plotting"
v0.9,Setting estimator attribute for convenience
v0.9,Outcome is numeric
v0.9,Treatments are also numeric or binary
v0.9,Outcome is categorical
v0.9,Treatments are numeric or binary
v0.9,TODO: A common way to show all plots
v0.9,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9,Get adjacency list
v0.9,If node pair has been fully explored
v0.9,Add node1 to backdoor set of node_pair
v0.9,Check if path is backdoor and does not have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.9,"True if arrow incoming, False if arrow outgoing"
v0.9,"Mark pair (node1, node2) complete"
v0.9,Modify variable count and indices covered
v0.9,Average total effect
v0.9,Natural direct effect
v0.9,Natural indirect effect
v0.9,Controlled direct effect
v0.9,Backdoor method names
v0.9,"First, check if there is a directed path from action to outcome"
v0.9,## 1. BACKDOOR IDENTIFICATION
v0.9,Pick algorithm to compute backdoor sets according to method chosen
v0.9,"First, checking if there are any valid backdoor adjustment sets"
v0.9,"Setting default ""backdoor"" identification adjustment set"
v0.9,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.9,Now checking if there is also a valid iv estimand
v0.9,## 3. FRONTDOOR IDENTIFICATION
v0.9,Now checking if there is a valid frontdoor variable
v0.9,Finally returning the estimand object
v0.9,Pick algorithm to compute backdoor sets according to method chosen
v0.9,"Setting default ""backdoor"" identification adjustment set"
v0.9,Finally returning the estimand object
v0.9,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.9,"First, checking if there are any valid backdoor adjustment sets"
v0.9,"Setting default ""backdoor"" identification adjustment set"
v0.9,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.9,Now checking if there are valid mediator variables
v0.9,Finally returning the estimand object
v0.9,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.9,"First, checking if there are any valid backdoor adjustment sets"
v0.9,"Setting default ""backdoor"" identification adjustment set"
v0.9,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.9,Now checking if there are valid mediator variables
v0.9,Finally returning the estimand object
v0.9,"First, checking if empty set is a valid backdoor set"
v0.9,"If the method is `minimal-adjustment`, return the empty set right away."
v0.9,"Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible."
v0.9,only remove descendants of Y
v0.9,also allow any causes of Y that are not caused by T (for lower variance)
v0.9,remove descendants of T (mediators) and descendants of Y
v0.9,"If var is d-separated from both treatment or outcome, it cannot"
v0.9,be a part of the backdoor set
v0.9,repeat the above search with BACKDOOR_MIN
v0.9,"If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest."
v0.9,"If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set."
v0.9,"If all variables are observed, and the biggest eligible set"
v0.9,"does not satisfy backdoor, then none of its subsets will."
v0.9,Adding a None estimand if no backdoor set found
v0.9,"Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable."
v0.9,Default set is the one with the least number of adjustment variables (optimizing for efficiency)
v0.9,"For simplicity, assuming a one-variable frontdoor set"
v0.9,Cond 1: All directed paths intercepted by candidate_var
v0.9,Cond 2: No confounding between treatment and candidate var
v0.9,Cond 3: treatment blocks all confounding between candidate_var and outcome
v0.9,"For simplicity, assuming a one-variable mediation set"
v0.9,"Create estimands dict as per the API for backdoor, but do not return it"
v0.9,"Setting default ""backdoor"" identification adjustment set"
v0.9,"Create estimands dict as per the API for backdoor, but do not return it"
v0.9,"Setting default ""backdoor"" identification adjustment set"
v0.9,"TODO: outputs string for now, but ideally should do symbolic"
v0.9,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.9,TODO Better support for multivariate treatments
v0.9,TODO: support multivariate treatments better.
v0.9,TODO: support multivariate treatments better.
v0.9,TODO: support multivariate treatments better.
v0.9,For direct effect
v0.9,"If no costs are passed, use uniform costs"
v0.9,restriction to ancestors
v0.9,back-door graph
v0.9,moralization
v0.9,Estimators list for returning after identification
v0.9,Line 1
v0.9,"If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y."
v0.9,Line 2
v0.9,"If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y."
v0.9,Modify list of valid nodes
v0.9,Line 3 - forces an action on any node where such an action would have no effect on Y – assuming we already acted on X.
v0.9,Modify adjacency matrix to obtain that corresponding to do(X)
v0.9,Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.
v0.9,"If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases."
v0.9,Modify adjacency matrix to remove treatment variables
v0.9,"Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes."
v0.9,"Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem."
v0.9,Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.
v0.9,Do not show backdoor key unless it is the only backdoor set.
v0.9,Just show the default backdoor set
v0.9,self._identified_estimand = self._causal_model.identify_effect()
v0.9,"self._identified_estimand,"
v0.9,"self._causal_model._treatment,"
v0.9,"self._causal_model._outcome,"
v0.9,If labels provided
v0.9,Return in valid DOT format
v0.9,Get adjacency matrix
v0.9,If labels not provided
v0.9,Obtain valid DOT format
v0.9,If labels provided
v0.9,Return in valid DOT format
v0.9,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.9,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.8,Get the long description from the README file
v0.8,Get the required packages
v0.8,Plotting packages are optional to install
v0.8,Loading version number
v0.8,-*- coding: utf-8 -*-
v0.8,
v0.8,Configuration file for the Sphinx documentation builder.
v0.8,
v0.8,This file does only contain a selection of the most common options. For a
v0.8,full list see the documentation:
v0.8,http://www.sphinx-doc.org/en/stable/config
v0.8,-- Path setup --------------------------------------------------------------
v0.8,"If extensions (or modules to document with autodoc) are in another directory,"
v0.8,add these directories to sys.path here. If the directory is relative to the
v0.8,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.8,
v0.8,-- Project information -----------------------------------------------------
v0.8,The short X.Y version
v0.8,"The full version, including alpha/beta/rc tags"
v0.8,-- General configuration ---------------------------------------------------
v0.8,"If your documentation needs a minimal Sphinx version, state it here."
v0.8,
v0.8,needs_sphinx = '1.0'
v0.8,"Add any Sphinx extension module names here, as strings. They can be"
v0.8,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.8,ones.
v0.8,"Add any paths that contain templates here, relative to this directory."
v0.8,The suffix(es) of source filenames.
v0.8,You can specify multiple suffix as a list of string:
v0.8,
v0.8,"source_suffix = ['.rst', '.md']"
v0.8,The master toctree document.
v0.8,The language for content autogenerated by Sphinx. Refer to documentation
v0.8,for a list of supported languages.
v0.8,
v0.8,This is also used if you do content translation via gettext catalogs.
v0.8,"Usually you set ""language"" from the command line for these cases."
v0.8,"List of patterns, relative to source directory, that match files and"
v0.8,directories to ignore when looking for source files.
v0.8,This pattern also affects html_static_path and html_extra_path .
v0.8,The name of the Pygments (syntax highlighting) style to use.
v0.8,-- Options for HTML output -------------------------------------------------
v0.8,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.8,a list of builtin themes.
v0.8,
v0.8,html_theme = 'sphinx-rtd-theme'
v0.8,on_rtd is whether we are on readthedocs.org
v0.8,only import and set the theme if we're building docs locally
v0.8,"otherwise, readthedocs.org uses their theme by default, so no need to specify it"
v0.8,Theme options are theme-specific and customize the look and feel of a theme
v0.8,"further.  For a list of options available for each theme, see the"
v0.8,documentation.
v0.8,
v0.8,html_theme_options = {}
v0.8,"Add any paths that contain custom static files (such as style sheets) here,"
v0.8,"relative to this directory. They are copied after the builtin static files,"
v0.8,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.8,"Custom sidebar templates, must be a dictionary that maps document names"
v0.8,to template names.
v0.8,
v0.8,The default sidebars (for documents that don't match any pattern) are
v0.8,defined by theme itself.  Builtin themes are using these templates by
v0.8,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.8,'searchbox.html']``.
v0.8,
v0.8,html_sidebars = {}
v0.8,-- Options for HTMLHelp output ---------------------------------------------
v0.8,Output file base name for HTML help builder.
v0.8,-- Options for LaTeX output ------------------------------------------------
v0.8,The paper size ('letterpaper' or 'a4paper').
v0.8,
v0.8,"'papersize': 'letterpaper',"
v0.8,"The font size ('10pt', '11pt' or '12pt')."
v0.8,
v0.8,"'pointsize': '10pt',"
v0.8,Additional stuff for the LaTeX preamble.
v0.8,
v0.8,"'preamble': '',"
v0.8,Latex figure (float) alignment
v0.8,
v0.8,"'figure_align': 'htbp',"
v0.8,Grouping the document tree into LaTeX files. List of tuples
v0.8,"(source start file, target name, title,"
v0.8,"author, documentclass [howto, manual, or own class])."
v0.8,-- Options for manual page output ------------------------------------------
v0.8,One entry per manual page. List of tuples
v0.8,"(source start file, name, description, authors, manual section)."
v0.8,-- Options for Texinfo output ----------------------------------------------
v0.8,Grouping the document tree into Texinfo files. List of tuples
v0.8,"(source start file, target name, title, author,"
v0.8,"dir menu entry, description, category)"
v0.8,-- Options for Epub output -------------------------------------------------
v0.8,Bibliographic Dublin Core info.
v0.8,The unique identifier of the text. This can be a ISBN number
v0.8,or the project homepage.
v0.8,
v0.8,epub_identifier = ''
v0.8,A unique identification for the text.
v0.8,
v0.8,epub_uid = ''
v0.8,A list of files that should not be packed into the epub file.
v0.8,-- Extension configuration -------------------------------------------------
v0.8,-- Options for todo extension ----------------------------------------------
v0.8,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.8,init docstrings should also be included in class
v0.8,requires stdin input for identify in weighting sampler
v0.8,requires Rpy2 for lalonde
v0.8,requires Rpy2 for causal discovery
v0.8,very slow
v0.8,will be removed
v0.8,"applied notebook, not necessary to test each time"
v0.8,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.8,can import dowhy
v0.8,"""--ExecutePreprocessor.timeout=600"","
v0.8,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.8,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.8,treated = self.df[self.df['z']==1]
v0.8,self.att = np.mean(treated['y1'] - treated['y0'])
v0.8,def test_average_treatment_effect(self):
v0.8,est_ate = 1
v0.8,bias = est_ate - self.ate
v0.8,print(bias)
v0.8,"self.assertAlmostEqual(self.ate, est_ate)"
v0.8,def test_average_treatment_effect_on_treated(self):
v0.8,est_att = 1
v0.8,self.att=1
v0.8,bias = est_att - self.att
v0.8,print(bias)
v0.8,"self.assertAlmostEqual(self.att, est_att)"
v0.8,removing two common causes
v0.8,removing two common causes
v0.8,removing two common causes
v0.8,removing two common causes
v0.8,check if all partial R^2 values are between 0 and 1
v0.8,We calculate adjusted estimates for two sets of partial R^2 values.
v0.8,Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2
v0.8,Creating a model with no unobserved confounders
v0.8,check if all partial R^2 values are between 0 and 1
v0.8,"for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)"
v0.8,The outcome is a linear function of the confounder
v0.8,"The slope is 1,2 and the intercept is 3"
v0.8,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.8,Supports user-provided dataset object
v0.8,To test if there are any exceptions
v0.8,To test if the estimate is identical if refutation parameters are zero
v0.8,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.8,"Ordinarily, we should expect this value to be zero."
v0.8,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.8,"Ordinarily, we should expect this value to be zero."
v0.8,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.8,Only P(Y|T) should be present for test to succeed.
v0.8,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.8,"Since undirected graph, identify effect must throw an error."
v0.8,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.8,Compare with ground truth
v0.8,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.8,Compare with ground truth
v0.8,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.8,Compare with ground truth
v0.8,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.8,Compare with ground truth
v0.8,Causal model initialization
v0.8,Causal identifier identification
v0.8,Obtain backdoor sets
v0.8,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.8,Causal model initialization
v0.8,Causal identifier identification
v0.8,Obtain backdoor sets
v0.8,Causal model initialization
v0.8,Causal identifier identification
v0.8,Obtain backdoor sets
v0.8,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.8,Causal model initialization
v0.8,Causal identifier identification
v0.8,Obtain backdoor sets
v0.8,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.8,Causal model initialization
v0.8,Causal identifier identification
v0.8,Obtain backdoor sets
v0.8,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.8,Causal model initialization
v0.8,Causal identifier identification
v0.8,Obtain backdoor sets
v0.8,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.8,"Example is selected from Pearl J. ""Causality"" 2nd Edition, from chapter 3.3.1 on backoor criterion."
v0.8,"The following simpsons paradox examples are taken from Pearl, J {2013}. ""Understanding Simpson’s Paradox"" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf"
v0.8,"The following are examples given in the ""Book of Why"" by Judea Pearl, chapter ""The Do-operator and the Back-Door Criterion"""
v0.8,cov_mat = np.diag(np.ones(num_features))
v0.8,collider: X->Z<-Y
v0.8,chain: X->Z->Y
v0.8,fork: X<-Z->Y
v0.8,"general DAG: X<-Z->Y, X->Y"
v0.8,fork: X<-Z->Y
v0.8,Contributions should add up to Var(X2)
v0.8,Contributions should add up to Var(X2)
v0.8,H(P(Y)) -- Can be precomputed
v0.8,-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))
v0.8,"H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\S)])"
v0.8,"E[P(Y | x_S, X'_\S)]"
v0.8,"H(E[P(Y | x_S, X'_\S)])"
v0.8,"Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions."
v0.8,E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]
v0.8,"Just checking formats, i.e. no need for correlation."
v0.8,"Just checking formats, i.e. no need for correlation."
v0.8,"By default, the strength is measure with respect to the variance."
v0.8,"Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2."
v0.8,"If we provide the observational data here, we can mitigate the misspecification of the causal mechanism."
v0.8,"Here, changing the mechanism."
v0.8,"Here, changing the mechanism."
v0.8,"Here, changing the mechanism."
v0.8,"Here, changing the mechanism."
v0.8,Defining an anomaly scorer that handles multidimensional inputs.
v0.8,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.8,"In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution"
v0.8,"and it should be ""significantly"" higher than the contribution of the other ones (here, we just arbitrarily say"
v0.8,"it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,"
v0.8,"which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate"
v0.8,"some robustness. Note that due to this and stochastic behaviour of the density estimator, it is"
v0.8,"not possible to analytically compute expected results. Therefore, we rather look at the relations here."
v0.8,"Same idea for the second sample, but here, it is the second variable that is anomalous."
v0.8,"In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be"
v0.8,"""significantly"" higher than the contribution of the other variables. The contribution of both anomalous variables"
v0.8,should be equal (approximately).
v0.8,Defining an anomaly scorer that handles multidimensional inputs.
v0.8,"Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3."
v0.8,reduce the score.
v0.8,The contributions should add up to g(x) - E[g(X)]
v0.8,The contributions should add up to g(x) - E[g(X)]
v0.8,The contributions should add up to g(x) - E[g(X)]
v0.8,Three examples:
v0.8,1. X1 is the root cause (+ 10 to the noise)
v0.8,2. X0 is the root cause (+ 10 to the noise)
v0.8,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.8,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.8,Three examples:
v0.8,1. X1 is the root cause (+ 10 to the noise)
v0.8,2. X0 is the root cause (+ 10 to the noise)
v0.8,3. X0 and X3 are both root causes (+ 10 to both noises)
v0.8,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.8,"Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution"
v0.8,algorithm.
v0.8,1. X0 is the root cause (+ 10 to the noise)
v0.8,2. X0 and X1 are the root causes (+ 10 to both noise)
v0.8,3. X2 and X3 are both root causes (+ 10 to both noises)
v0.8,"The sum of the scores should add up to the anomaly score of the target (here, X3)."
v0.8,Check if calling the method causes some import or runtime errors
v0.8,TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer
v0.8,newer matplotlib version:
v0.8,AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'
v0.8,Networkx 2.4+ should fix this issue.
v0.8,"plot_adjacency_matrix(causal_graph, is_directed=False)"
v0.8,Setup data
v0.8,Test LinearDML
v0.8,Test ContinuousTreatmentOrthoForest
v0.8,Test LinearDRLearner
v0.8,Setup data
v0.8,Test DeepIV
v0.8,"Treatment model,"
v0.8,Response model
v0.8,Test IntentToTreatDRIV
v0.8,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.8,More cases where Exception  is expected
v0.8,"Compute confidence intervals, standard error and significance tests"
v0.8,Defined a linear dataset with a given set of properties
v0.8,Create a model that captures the same
v0.8,Identify the effects within the model
v0.8,Defined a linear dataset with a given set of properties
v0.8,Create a model that captures the same
v0.8,Identify the effects within the model
v0.8,Defined a linear dataset with a given set of properties
v0.8,Create a model that captures the same
v0.8,Identify the effects within the model
v0.8,Defined a linear dataset with a given set of properties
v0.8,Create a model that captures the same
v0.8,Identify the effects within the model
v0.8,Defined a linear dataset with a given set of properties
v0.8,Create a model that captures the same
v0.8,Identify the effects within the model
v0.8,Backdoor method names
v0.8,"First, check if there is a directed path from action to outcome"
v0.8,## 1. BACKDOOR IDENTIFICATION
v0.8,"First, checking if there are any valid backdoor adjustment sets"
v0.8,"Setting default ""backdoor"" identification adjustment set"
v0.8,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.8,Now checking if there is also a valid iv estimand
v0.8,## 3. FRONTDOOR IDENTIFICATION
v0.8,Now checking if there is a valid frontdoor variable
v0.8,Finally returning the estimand object
v0.8,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.8,"First, checking if there are any valid backdoor adjustment sets"
v0.8,"Setting default ""backdoor"" identification adjustment set"
v0.8,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.8,Now checking if there are valid mediator variables
v0.8,Finally returning the estimand object
v0.8,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.8,"First, checking if there are any valid backdoor adjustment sets"
v0.8,"Setting default ""backdoor"" identification adjustment set"
v0.8,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.8,Now checking if there are valid mediator variables
v0.8,Finally returning the estimand object
v0.8,"First, checking if empty set is a valid backdoor set"
v0.8,"If the method is `minimal-adjustment`, return the empty set right away."
v0.8,"Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible."
v0.8,"If var is d-separated from both treatment or outcome, it cannot"
v0.8,be a part of the backdoor set
v0.8,repeat the above search with BACKDOOR_MIN
v0.8,"If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest."
v0.8,"If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set."
v0.8,"If all variables are observed, and the biggest eligible set"
v0.8,"does not satisfy backdoor, then none of its subsets will."
v0.8,Adding a None estimand if no backdoor set found
v0.8,"Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable."
v0.8,Default set is the one with the least number of adjustment variables (optimizing for efficiency)
v0.8,"For simplicity, assuming a one-variable frontdoor set"
v0.8,Cond 1: All directed paths intercepted by candidate_var
v0.8,Cond 2: No confounding between treatment and candidate var
v0.8,Cond 3: treatment blocks all confounding between candidate_var and outcome
v0.8,"For simplicity, assuming a one-variable mediation set"
v0.8,"Create estimands dict as per the API for backdoor, but do not return it"
v0.8,"Setting default ""backdoor"" identification adjustment set"
v0.8,"Create estimands dict as per the API for backdoor, but do not return it"
v0.8,"Setting default ""backdoor"" identification adjustment set"
v0.8,"TODO: outputs string for now, but ideally should do symbolic"
v0.8,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.8,TODO Better support for multivariate treatments
v0.8,TODO: support multivariate treatments better.
v0.8,TODO: support multivariate treatments better.
v0.8,TODO: support multivariate treatments better.
v0.8,For direct effect
v0.8,Do not show backdoor key unless it is the only backdoor set.
v0.8,Just show the default backdoor set
v0.8,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.8,Unpacking the keyword arguments
v0.8,Default value for the number of simulations to be conducted
v0.8,"Concatenate the confounders, instruments and effect modifiers"
v0.8,Shuffle the confounders
v0.8,Check if all are select or deselect variables
v0.8,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.8,Initializing the p_value
v0.8,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.8,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.8,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.8,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.8,Get the number of simulations
v0.8,Sort the simulations
v0.8,Obtain the median value
v0.8,Performing a two sided test
v0.8,np.searchsorted tells us the index if it were a part of the array
v0.8,We select side to be left as we want to find the first value that matches
v0.8,We subtact 1 as we are finding the value from the right tail
v0.8,We take the side to be right as we want to find the last index that matches
v0.8,We get the probability with respect to the left tail.
v0.8,return twice the determined quantile as this is a two sided test
v0.8,Get the mean for the simulations
v0.8,Get the standard deviation for the simulations
v0.8,Get the Z Score [(val - mean)/ std_dev ]
v0.8,re.sub only takes string parameter so the first if is to avoid error
v0.8,"if the input is a text file, convert the contained data into string"
v0.8,load dot file
v0.8,Adding node attributes
v0.8,adding penwidth to make the edge bold
v0.8,Adding common causes
v0.8,Adding instruments
v0.8,Adding effect modifiers
v0.8,Assuming the simple form of effect modifier
v0.8,that directly causes the outcome.
v0.8,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.8,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.8,Adding columns in the dataframe as confounders that were not in the graph
v0.8,Adding unobserved confounders
v0.8,also return the number of backdoor paths blocked by observed nodes
v0.8,Assume that nodes1 is the treatment
v0.8,"ignores new_graph parameter, always uses self._graph"
v0.8,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.8,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.8,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.8,removing all mediators
v0.8,[TODO: double check these work with multivariate implementation:]
v0.8,Exclusion
v0.8,As-if-random setup
v0.8,As-if-random
v0.8,convert the outputted generator into a list
v0.8,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.8,return len(dpaths) > 0
v0.8,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.8,Create causal graph object
v0.8,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.8,(Because some effect modifiers may also be common causes)
v0.8,"In such cases, the user-provided modifiers are used."
v0.8,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.8,Import causal discovery class
v0.8,Initialize causal graph object
v0.8,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.8,TODO add dowhy as a prefix to all dowhy estimators
v0.8,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.8,Define the third-party estimation method to be used
v0.8,Process the dowhy estimators
v0.8,Check if estimator's target estimand is identified
v0.8,Estimator had been computed in a previous call
v0.8,Store parameters inside estimate object for refutation methods
v0.8,TODO: This add_params needs to move to the estimator class
v0.8,inside estimate_effect and estimate_conditional_effect
v0.8,Check if estimator's target estimand is identified
v0.8,"Note that while the name of the variable is the same,"
v0.8,"""self.causal_estimator"", this estimator takes in less"
v0.8,parameters than the same from the
v0.8,estimate_effect code. It is not advisable to use the
v0.8,estimator from this function to call estimate_effect
v0.8,with fit_estimator=False.
v0.8,Estimator had been computed in a previous call
v0.8,The default number of simulations for statistical testing
v0.8,The default number of simulations to obtain confidence intervals
v0.8,The portion of the total size that should be taken each time to find the confidence intervals
v0.8,1 is the recommended value
v0.8,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.8,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.8,The default Confidence Level
v0.8,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.8,Prefix to add to temporary categorical variables created after discretization
v0.8,std args to be removed from locals() before being passed to args_dict
v0.8,Currently estimation methods only support univariate treatment and outcome
v0.8,Setting the default interpret method
v0.8,Setting treatment and outcome values
v0.8,Now saving the effect modifiers
v0.8,only add the observed nodes
v0.8,"Check if some parameters were set, otherwise set to default values"
v0.8,Estimate conditional estimates by default
v0.8,names of treatment and outcome
v0.8,TODO Only works for binary treatment
v0.8,Defaulting to class default values if parameters are not provided
v0.8,Checking that there is at least one effect modifier
v0.8,Making sure that effect_modifier_names is a list
v0.8,Making a copy since we are going to be changing effect modifier names
v0.8,"For every numeric effect modifier, adding a temp categorical column"
v0.8,Grouping by effect modifiers and computing effect separately
v0.8,Deleting the temporary categorical columns
v0.8,The array that stores the results of all estimations
v0.8,Find the sample size the proportion with the population size
v0.8,Perform the set number of simulations
v0.8,names of treatment and outcome
v0.8,Using class default parameters if not specified
v0.8,Checking if bootstrap_estimates are already computed
v0.8,Checked if any parameter is changed from the previous std error estimate
v0.8,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.8,Get the variations of each bootstrap estimate and sort
v0.8,"Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level"
v0.8,Get the lower and upper bounds by subtracting the variations from the estimate
v0.8,"Use existing params, if new user defined params are not present"
v0.8,Checking if bootstrap_estimates are already computed
v0.8,Check if any parameter is changed from the previous std error estimate
v0.8,"Use existing params, if new user defined params are not present"
v0.8,"self._outcome = self._data[""dummy_outcome""]"
v0.8,Processing the null hypothesis estimates
v0.8,Doing a two-sided test
v0.8,Being conservative with the p-value reported
v0.8,Being conservative with the p-value reported
v0.8,"If the estimate_index is 0, it depends on the number of simulations"
v0.8,Need to test r-squared before supporting
v0.8,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.8,'r-squared': effect_r_squared
v0.8,"elif method == ""r-squared"":"
v0.8,outcome_mean = np.mean(self._outcome)
v0.8,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.8,Assuming a linear model with one variable: the treatment
v0.8,Currently only works for continuous y
v0.8,causal_model = outcome_mean + estimate.value*self._treatment
v0.8,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.8,r_squared = 1 - (squared_residual/total_variance)
v0.8,return r_squared
v0.8,No estimand was identified (identification failed)
v0.8,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.8,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.8,one-hot encode discrete W
v0.8,Now deleting the old continuous value
v0.8,Making beta an array
v0.8,TODO Ensure that we do not generate weak instruments
v0.8,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.8,Converting treatment to binary if required
v0.8,Generating frontdoor variables if asked for
v0.8,NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.
v0.8,"For categorical t, this example dataset has the effect size for category 2 being exactly"
v0.8,double the effect for category 1
v0.8,This could be changed at this stage by one-hot encoding t and using a custom beta that
v0.8,"sets a different effect for each category {0, 1, 2}"
v0.8,Computing ATE
v0.8,constructing column names for one-hot encoded discrete features
v0.8,Specifying the correct dtypes
v0.8,Now specifying the corresponding graph strings
v0.8,Now writing the gml graph
v0.8,Making beta an array
v0.8,creating data frame
v0.8,Specifying the correct dtypes
v0.8,Now specifying the corresponding graph strings
v0.8,Now writing the gml graph
v0.8,Adding edges between common causes and the frontdoor mediator
v0.8,Error terms
v0.8,else:
v0.8,V = 6 + W0 + tterm + E1
v0.8,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.8,Generating a random normal distribution of integers
v0.8,Generating data for nodes which have no incoming edges
v0.8,"""currset"" variable currently has all the successors of the nodes which had no incoming edges"
v0.8,Loading version number
v0.8,The currently supported estimators
v0.8,The default standard deviation for noise
v0.8,The default scaling factor to determine the bucket size
v0.8,The minimum number of points for the estimator to run
v0.8,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.8,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.8,The Default split for the number of data points that fall into the training and validation sets
v0.8,Assuming that outcome is one-dimensional
v0.8,We need to change the identified estimand
v0.8,"We thus, make a copy. This is done as we don't want"
v0.8,to change the original DataFrame
v0.8,We use collections.OrderedDict to maintain the order in which the data is stored
v0.8,Check if we are using an estimator in the transformation list
v0.8,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.8,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.8,"loops. Thus, we can get different values everytime we get the estimator."
v0.8,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.8,Adding an unobserved confounder if provided by the user
v0.8,We set X_train = 0 and outcome_train to be 0
v0.8,"Get the final outcome, after running through all the values in the transformation list"
v0.8,Check if the value of true effect has been already stored
v0.8,We use None as the key as we have no base category for this refutation
v0.8,As we currently support only one treatment
v0.8,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.8,Check if the value of true effect has been already stored
v0.8,This ensures that we calculate the causal effect only once.
v0.8,We use key_train as we map data with respect to the base category of the data
v0.8,As we currently support only one treatment
v0.8,Add h(t) to f(W) to get the dummy outcome
v0.8,We convert to ndarray for ease in indexing
v0.8,The data is of the form
v0.8,sim1: cat1 cat2 ... catn
v0.8,sim2: cat1 cat2 ... catn
v0.8,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.8,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.8,distribution of the refuter.
v0.8,True Causal Effect list
v0.8,Iterating through the refutation for each category
v0.8,We use string arguments to account for both 32 and 64 bit varaibles
v0.8,action for continuous variables
v0.8,Action for categorical variables
v0.8,Find the set difference for each row
v0.8,Choose one out of the remaining
v0.8,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.8,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.8,of the treatment to affect the outcome
v0.8,Standardizing the data
v0.8,Fit a model containing all confounders and compare predictions
v0.8,using all features compared to all features except a given
v0.8,confounder.
v0.8,Estimating the regression coefficient from standardized features to t
v0.8,"By default, return a plot with 10 points"
v0.8,consider 10 values of the effect of the unobserved confounder
v0.8,Standardizing the data
v0.8,Fit a model containing all confounders and compare predictions
v0.8,using all features compared to all features except a given
v0.8,confounder.
v0.8,"By default, return a plot with 10 points"
v0.8,consider 10 values of the effect of the unobserved confounder
v0.8,Get a 2D matrix of values
v0.8,"x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN"
v0.8,Store the values into the refute object
v0.8,Adding a label on the contour line for the original estimate
v0.8,Label every other level using strings
v0.8,"By default, we add the effect of simulated confounder for treatment."
v0.8,But subtract it from outcome to create a negative correlation
v0.8,assuming that the original confounder's effect was positive on both.
v0.8,This is to remove the effect of the original confounder.
v0.8,"By default, we add the effect of simulated confounder for treatment."
v0.8,But subtract it from outcome to create a negative correlation
v0.8,assuming that the original confounder's effect was positive on both.
v0.8,This is to remove the effect of the original confounder.
v0.8,Obtaining the list of observed variables
v0.8,Taking a subset of the dataframe that has only observed variables
v0.8,Residuals from the outcome model obtained by fitting a linear model
v0.8,Residuals from the treatment model obtained by fitting a linear model
v0.8,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.8,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.8,Choosing a c_star based on the data.
v0.8,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.8,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.8,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.8,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.8,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.8,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.8,Default value of the p value taken for the distribution
v0.8,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.8,Mean of the Normal Distribution
v0.8,Standard Deviation of the Normal Distribution
v0.8,only permute is supported for iv methods
v0.8,We need to change the identified estimand
v0.8,"We make a copy as a safety measure, we don't want to change the"
v0.8,original DataFrame
v0.8,"For IV methods, the estimating_instrument_names should also be"
v0.8,changed. So we change it inside the estimate and then restore it
v0.8,back at the end of this method.
v0.8,Create a new column in the data by the name of placebo
v0.8,Sanity check the data
v0.8,Restoring the value of iv_instrument_name
v0.8,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.8,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.8,relationship between the treatment and the outcome.
v0.8,Adding a new backdoor variable to the identified estimand
v0.8,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.8,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.8,of the treatment to affect the outcome
v0.8,Reject H0
v0.8,"a, b and c are all continuous variables"
v0.8,"a, b and c are all discrete variables"
v0.8,c is set of continuous and binary variables and
v0.8,1. either a and b is continuous and the other is binary
v0.8,2. both a and b are binary
v0.8,c is discrete and
v0.8,either a or b is continuous and the other is discrete
v0.8,a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information
v0.8,original_treatment_name: : stores original variable names for labelling
v0.8,common_causes_map : maps the original variable names to variable names in OLS regression
v0.8,benchmark_common_causes: stores variable names in terms of regression model variables
v0.8,original_benchmark_covariates: stores original variable names for labelling
v0.8,estimate: estimate of regression
v0.8,degree_of_freedom: degree of freedom of error in regression
v0.8,standard_error: standard error in regression
v0.8,t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.
v0.8,partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them
v0.8,"r2tu_w: partial R^2  of unobserved confounder ""u"" with treatment ""t"", after conditioning on observed covariates ""w"""
v0.8,"r2yu_tw: partial R^2  of unobserved confounder ""u"" with outcome ""y"", after conditioning on observed covariates ""w"" and treatment ""t"""
v0.8,"r2twj_w: partial R^2 of observed covariate wj with treatment ""t"", after conditioning on observed covariates ""w"" excluding wj"
v0.8,"r2ywj_tw:  partial R^2 of observed covariate wj with outcome ""y"", after conditioning on observed covariates ""w"" (excluding wj) and treatment ""t"""
v0.8,benchmarking_results: dataframe containing information about bounds and bias adjusted terms
v0.8,"stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic"
v0.8,partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment
v0.8,build a new regression model by considering treatment variables as outcome
v0.8,"r2twj_w is partial R^2 of covariate wj with treatment ""t"", after conditioning on covariates w(excluding wj)"
v0.8,"r2ywj_tw is partial R^2 of covariate wj with outcome ""y"", after conditioning on covariates w(excluding wj) and treatment ""t"""
v0.8,r2tu_w is the partial r^2 from regressing u on t after conditioning on w
v0.8,Compute bias adjusted terms
v0.8,Plotting the contour plot
v0.8,Adding contours
v0.8,Adding threshold contour line
v0.8,Adding unadjusted point estimate
v0.8,Adding bounds to partial R^2 values for given strength of confounders
v0.8,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.8,The default subset of the data to be used
v0.8,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.8,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.8,of the treatment to affect the outcome
v0.8,Get adjacency list
v0.8,If node pair has been fully explored
v0.8,Add node1 to backdoor set of node_pair
v0.8,Check if path is backdoor and does not have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.8,"True if arrow incoming, False if arrow outgoing"
v0.8,"Mark pair (node1, node2) complete"
v0.8,Modify variable count and indices covered
v0.8,Estimators list for returning after identification
v0.8,Line 1
v0.8,"If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y."
v0.8,Line 2
v0.8,"If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y."
v0.8,Modify list of valid nodes
v0.8,Line 3 - forces an action on any node where such an action would have no effect on Y – assuming we already acted on X.
v0.8,Modify adjacency matrix to obtain that corresponding to do(X)
v0.8,Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.
v0.8,"If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases."
v0.8,Modify adjacency matrix to remove treatment variables
v0.8,"Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes."
v0.8,"Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem."
v0.8,Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.
v0.8,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.8,"If the relative change of the score is less than the given threshold, we stop the estimation early."
v0.8,"Note, the output of score_samples are log values."
v0.8,"Note, the output of score_samples are log values."
v0.8,Currently only support continuous distributions for auto selection.
v0.8,Estimate distribution parameters from data.
v0.8,Ignore warnings from fitting process.
v0.8,Fit distribution to data.
v0.8,Some distributions might not be compatible with the data.
v0.8,Separate parts of parameters.
v0.8,Check the KL divergence between the distribution of the given and fitted distribution.
v0.8,Identify if this distribution is better.
v0.8,This error is typically raised when the data is discrete and all points are assigned to less cluster than
v0.8,"specified. It can also happen due to duplicated points. In these cases, the current best solution should"
v0.8,be sufficient.
v0.8,"Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]"
v0.8,A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This
v0.8,function evaluates the provided causal query multiple times to build a confidence interval based on the returned
v0.8,results.
v0.8,Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order
v0.8,"to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the"
v0.8,bootstrap_training_and_sampling function.
v0.8,
v0.8,**Example usage:**
v0.8,
v0.8,">>> gcm.fit(causal_model, data)"
v0.8,">>> strength_medians, strength_intervals = gcm.confidence_intervals("
v0.8,">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))"
v0.8,
v0.8,"In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the"
v0.8,"confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'"
v0.8,"here to configure the function call. In this case,"
v0.8,"gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to"
v0.8,"lambda : gcm.arrow_strength(causal_model, target_node='Y')."
v0.8,
v0.8,"In order to incorporate uncertainties coming from fitting the causal model(s), we can use"
v0.8,gcm.bootstrap_training_and_sampling instead:
v0.8,">>>  strength_medians, strength_intervals = gcm.confidence_intervals("
v0.8,">>>        gcm.bootstrap_training_and_sampling(gcm.arrow_strength,"
v0.8,">>>                                            causal_model,"
v0.8,">>>                                            bootstrap_training_data=data,"
v0.8,>>>                                            target_node='Y'))
v0.8,This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each
v0.8,run.
v0.8,"Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based"
v0.8,on their topological order.
v0.8,"After drawing samples of the node based on the data generation process, we apply the corresponding"
v0.8,intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this
v0.8,node.
v0.8,"Abduction: For invertible SCMs, we recover exact noise values from data."
v0.8,Action + Prediction: Propagate the intervention downstream using recovered noise values.
v0.8,Check if we need to apply an intervention on the given node.
v0.8,Apply intervention function to the data of the node.
v0.8,Check if the intervention function changes the shape of the data.
v0.8,"For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e."
v0.8,all ancestors of the target.
v0.8,The target node can be a continuous real-valued variable or a categorical variable with at most two classes
v0.8,(i.e. binary).
v0.8,Making sure there are at least 30% test samples.
v0.8,"Making sure that there are at least 2 samples from one class (here, simply duplicate the point)."
v0.8,Compare number of correct classifications.
v0.8,This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes
v0.8,This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes
v0.8,afterwards.
v0.8,"can't use nx.node_connected_component, because it doesn't work with DiGraphs."
v0.8,Hence a manual loop:
v0.8,For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples
v0.8,is unknown beforehand.
v0.8,The method stops if either the change between some consecutive runs is below the given threshold or the
v0.8,maximum number of runs is reached.
v0.8,"In each run, we create one random permutation of players. For instance, given 4 players, a permutation"
v0.8,"could be [3,1,4,2]."
v0.8,"Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the"
v0.8,"subsets are [3], [3,1], [3,1,4] [3,1,4,2]."
v0.8,"The result for each subset is cached such that if a subset that has already been evaluated appears again,"
v0.8,we can take this result directly.
v0.8,"To improve the runtime, multiple permutations are evaluated in each run."
v0.8,"The current Shapley values are the average of the estimated values, i.e. we need to divide by the number"
v0.8,of generated permutations here.
v0.8,"Here, the change between two runs is below the minimum threshold, but to reduce the likelihood"
v0.8,"that this just happened by chance, we require that this happens at least for two runs in a row."
v0.8,Create all (unique) subsets)
v0.8,"Assigning a 'high' weight, since this resembles ""infinity""."
v0.8,The weight for a subset with a specific length (see paper mentioned in the docstring for more
v0.8,information).
v0.8,TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is
v0.8,"theoretically sound, i.e. make entropy results from different data comparable."
v0.8,Extremely small values can somehow result in negative values.
v0.8,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.8,Sampling from the conditional distribution based on the current sample.
v0.8,"Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that"
v0.8,"are in the given subset. By this, we can simulate the impact on the conditional distribution when removing"
v0.8,only the incoming edges of the variables in the subset.
v0.8,"Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node."
v0.8,"In case of the full subset (no randomization), we get the same predictions as when we apply the"
v0.8,"prediction method to the samples of interest, since all noise samples are replaced with a sample of"
v0.8,interest.
v0.8,"In case of the empty subset (all are jointly randomize), it boils down to taking the average over all"
v0.8,"predictions, seeing that the randomization yields the same values for each sample of interest (none of the"
v0.8,samples of interest are used to replace a (jointly) 'randomized' sample).
v0.8,Smallest possible value. This is used in various algorithm for numerical stability.
v0.8,Make copy to avoid manipulating the original matrix.
v0.8,"The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,"
v0.8,the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster
v0.8,"than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as"
v0.8,"possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples."
v0.8,"To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many"
v0.8,samples. The number of samples that are evaluated is normally
v0.8,"baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to"
v0.8,"batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is"
v0.8,evaluated one by one in a for-loop.
v0.8,Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple
v0.8,"batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet."
v0.8,"If the batch size would be larger than the remaining amount of samples, it is reduced to only include the"
v0.8,remaining baseline_noise_samples.
v0.8,"The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features"
v0.8,in baseline_feature_indices to their respective values in baseline_noise_samples.
v0.8,"After creating the (potentially large) input data matrix, we can evaluate the prediction method."
v0.8,"Here, offset + index now indicates the sample index in baseline_noise_samples."
v0.8,This would average all prediction results obtained for the 'offset + index'-th sample in
v0.8,"baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)]."
v0.8,This would return all prediction results obtained for the 'offset + index'-th sample in
v0.8,"baseline_noise_samples, i.e. the results are not averaged."
v0.8,Making copy to ensure that the original object is not modified.
v0.8,Permute samples jointly. This still represents an interventional distribution.
v0.8,Permute samples independently.
v0.8,"test local Markov condition, null hypothesis: conditional independence"
v0.8,"test edge dependence, null hypothesis: independence"
v0.8,The order of the p-values added to the list is deterministic.
v0.8,"To be able to validate that the graph structure did not change between fitting and causal query, we store the"
v0.8,"parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While"
v0.8,"this would automatically fail when the number of parents is different, there are other more subtle cases,"
v0.8,"where the number is still the same, but it's different parents, and therefore different data. That would yield"
v0.8,"wrong results, but would not fail."
v0.8,Wrapping labels if they are too long
v0.8,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.8,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.8,"target quantity (here, variance)."
v0.8,Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In
v0.8,"case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the"
v0.8,"target quantity (here, variance)."
v0.8,TODO: This is a temporary workaround.
v0.8,"Under some circumstances, the KCI test throws a ""numpy.linalg.LinAlgError: SVD did not converge"""
v0.8,"error, depending on the data samples. This is related to the utilized algorithms by numpy for SVD."
v0.8,"There is actually a robust version for SVD, but it is not included in numpy."
v0.8,"This can either be addressed by some augmenting the data, using a different SVD implementation or"
v0.8,wait until numpy updates the used algorithm.
v0.8,"Not dividing by n, seeing that the expectation and variance are also not divided by n and n**2, respectively."
v0.8,"Taking the sum, because due to numerical issues, the matrices might not be symmetric."
v0.8,Filter out eigenvalues that are too small.
v0.8,Test statistic is given as np.trace(K @ H @ L @ H) / n. Below computes without matrix products.
v0.8,Dividing by n not required since we do not divide the test statistical_tools by n.
v0.8,Estimate test statistic multiple times on different permutations of the data. The p-value is then the
v0.8,probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted
v0.8,data.
v0.8,First stage statistical model
v0.8,Second stage statistical model
v0.8,Required to ensure that self.method_params contains all the
v0.8,parameters needed to create an object of this class
v0.8,Check if the treatment is one-dimensional
v0.8,First stage
v0.8,Second Stage
v0.8,Combining the two estimates
v0.8,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.8,Total  effect of treatment
v0.8,Bulding the feature matrix
v0.8,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.8,Required to ensure that self.method_params contains all the information
v0.8,to create an object of this class
v0.8,choosing the instrumental variable to use
v0.8,TODO move this to the identification step
v0.8,Obtain estimate by Wald Estimator
v0.8,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.8,More than 1 instrument. Use 2sls.
v0.8,Required to ensure that self.method_params contains all the
v0.8,parameters needed to create an object of this class
v0.8,Checking if Y is binary
v0.8,Required to ensure that self.method_params contains all the
v0.8,parameters to create an object of this class
v0.8,Enable the user to pass params for a custom propensity model
v0.8,Check if the treatment is one-dimensional
v0.8,Checking if the treatment is binary
v0.8,Convert the categorical variables into dummy/indicator variables
v0.8,"Basically, this gives a one hot encoding for each category"
v0.8,The first category is taken to be the base line.
v0.8,check if user provides the propensity score column
v0.8,Required to ensure that self.method_params contains all the information
v0.8,to create an object of this class
v0.8,TODO make treatment_value and control value also as local parameters
v0.8,Checking if the model is already trained
v0.8,The model is always built on the entire data
v0.8,All treatments are set to the same constant value
v0.8,Using all data by default
v0.8,"Fixing treatment value to the specified value, if provided"
v0.8,treatment_vals and data_df should have same number of rows
v0.8,Bulding the feature matrix
v0.8,The model is always built on the entire data
v0.8,Replacing treatment values by given x
v0.8,"First, create interventional tensor in original space"
v0.8,"Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment"
v0.8,Required to ensure that self.method_params contains all the
v0.8,parameters to create an object of this class
v0.8,The average treatment effect is a combination of different
v0.8,regression coefficients. Complicated to compute the confidence
v0.8,"interval analytically. For example, if y=a + b1.t + b2.tx, then"
v0.8,the average treatment effect is b1+b2.mean(x).
v0.8,"Refer Gelman, Hill. ARM Book. Chapter 9"
v0.8,http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
v0.8,TODO: Looking for contributions
v0.8,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.8,"variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0."
v0.8,"So for custom treatment and control values, we must multiply the confidence interval by the difference of the two."
v0.8,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.8,"variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0."
v0.8,"So for custom treatment and control values, we must multiply the standard error by the difference of the two."
v0.8,Required to ensure that self.method_params contains all the information
v0.8,to create an object of this class
v0.8,setting method-specific parameters
v0.8,Infer the right strata based on clipping threshold
v0.8,0.5 because there are two values for the treatment
v0.8,To be conservative and allow most strata to be included in the
v0.8,analysis
v0.8,At least 90% of the strata should be included in analysis
v0.8,sum weighted outcomes over all strata  (weight by treated population)
v0.8,TODO - how can we add additional information into the returned estimate?
v0.8,"such as how much clipping was done, or per-strata info for debugging?"
v0.8,sort the dataframe by propensity score
v0.8,create a column 'strata' for each element that marks what strata it belongs to
v0.8,"for each strata, count how many treated and control units there are"
v0.8,throw away strata that have insufficient treatment or control
v0.8,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.8,Required to ensure that self.method_params contains all the information
v0.8,to create an object of this class
v0.8,Setting method specific parameters
v0.8,trim propensity score weights
v0.8,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.8,nips ==> ips / (sum of ips over all units)
v0.8,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.8,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.8,Vanilla IPS estimator
v0.8,The Hajek estimator (or the self-normalized estimator)
v0.8,"Stabilized weights (from Robins, Hernan, Brumback (2000))"
v0.8,Paper: Marginal Structural Models and Causal Inference in Epidemiology
v0.8,Calculating the effect
v0.8,Subtracting the weighted means
v0.8,TODO - how can we add additional information into the returned estimate?
v0.8,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.8,Required to ensure that self.method_params contains all the
v0.8,parameters to create an object of this class
v0.8,For metalearners only--issue a warning if w contains variables not in x
v0.8,Override the effect_modifiers set in CausalEstimator.__init__()
v0.8,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.8,the latter can be used by other estimator methods later
v0.8,"Instrumental variables names, if present"
v0.8,choosing the instrumental variable to use
v0.8,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.8,Calling the econml estimator's fit method
v0.8,"As of v0.9, econml has some kewyord only arguments"
v0.8,Changing shape to a list for a singleton value
v0.8,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.8,this assumes a binary treatment regime
v0.8,TODO remove neighbors that are more than a given radius apart
v0.8,estimate ATT on treated by summing over difference between matched neighbors
v0.8,Now computing ATC
v0.8,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.8,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.8,Handle externally provided estimator classes
v0.8,allowed types of distance metric
v0.8,Required to ensure that self.method_params contains all the
v0.8,parameters to create an object of this class
v0.8,Check if the treatment is one-dimensional
v0.8,Checking if the treatment is binary
v0.8,Convert the categorical variables into dummy/indicator variables
v0.8,"Basically, this gives a one hot encoding for each category"
v0.8,The first category is taken to be the base line.
v0.8,Dictionary of any user-provided params for the distance metric
v0.8,that will be passed to sklearn nearestneighbors
v0.8,this assumes a binary treatment regime
v0.8,TODO remove neighbors that are more than a given radius apart
v0.8,estimate ATT on treated by summing over difference between matched neighbors
v0.8,Return indices in the original dataframe
v0.8,self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()
v0.8,Now computing ATC
v0.8,Return indices in the original dataframe
v0.8,Required to ensure that self.method_params contains all the information
v0.8,to create an object of this class
v0.8,Add the identification method used in the estimator
v0.8,Check the backdoor variables being used
v0.8,Add the observed confounders and one hot encode the categorical variables
v0.8,Get the data of the unobserved confounders
v0.8,One hot encode the data if they are categorical
v0.8,Check the instrumental variables involved
v0.8,Perform the same actions as the above
v0.8,Check if effect modifiers are used
v0.8,Get the class corresponding the the estimator to be used
v0.8,Initialize the object
v0.8,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.8,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.8,For CATEs
v0.8,TODO we are conditioning on a postive treatment
v0.8,TODO create an expression corresponding to each estimator used
v0.8,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.8,Flipping some values
v0.8,Finding p-value using student T test
v0.8,Only consider edges have absolute edge weight > 0.01
v0.8,Modify graph such that it only contains bidirected edges
v0.8,Find c components by finding connected components on the undirected graph
v0.8,Understanding Neural Network weights
v0.8,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.8,add weight column
v0.8,before weights are applied we count number rows in each category
v0.8,which is equivalent to summing over weight=1
v0.8,after weights are applied we need to sum over the given weights
v0.8,"First, calculating mean differences by strata"
v0.8,"Second, without strata"
v0.8,"Third, concatenating them and plotting"
v0.8,Setting estimator attribute for convenience
v0.8,Outcome is numeric
v0.8,Treatments are also numeric or binary
v0.8,Outcome is categorical
v0.8,Treatments are numeric or binary
v0.8,TODO: A common way to show all plots
v0.8,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.8,self._identified_estimand = self._causal_model.identify_effect()
v0.8,"self._identified_estimand,"
v0.8,"self._causal_model._treatment,"
v0.8,"self._causal_model._outcome,"
v0.8,If labels provided
v0.8,Return in valid DOT format
v0.8,Get adjacency matrix
v0.8,If labels not provided
v0.8,Obtain valid DOT format
v0.8,If labels provided
v0.8,Return in valid DOT format
v0.8,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.8,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7.1,Get the long description from the README file
v0.7.1,Get the required packages
v0.7.1,Plotting packages are optional to install
v0.7.1,Loading version number
v0.7.1,-*- coding: utf-8 -*-
v0.7.1,
v0.7.1,Configuration file for the Sphinx documentation builder.
v0.7.1,
v0.7.1,This file does only contain a selection of the most common options. For a
v0.7.1,full list see the documentation:
v0.7.1,http://www.sphinx-doc.org/en/stable/config
v0.7.1,-- Path setup --------------------------------------------------------------
v0.7.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.7.1,add these directories to sys.path here. If the directory is relative to the
v0.7.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.7.1,
v0.7.1,-- Project information -----------------------------------------------------
v0.7.1,The short X.Y version
v0.7.1,"The full version, including alpha/beta/rc tags"
v0.7.1,-- General configuration ---------------------------------------------------
v0.7.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.7.1,
v0.7.1,needs_sphinx = '1.0'
v0.7.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.7.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.7.1,ones.
v0.7.1,"Add any paths that contain templates here, relative to this directory."
v0.7.1,The suffix(es) of source filenames.
v0.7.1,You can specify multiple suffix as a list of string:
v0.7.1,
v0.7.1,"source_suffix = ['.rst', '.md']"
v0.7.1,The master toctree document.
v0.7.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.7.1,for a list of supported languages.
v0.7.1,
v0.7.1,This is also used if you do content translation via gettext catalogs.
v0.7.1,"Usually you set ""language"" from the command line for these cases."
v0.7.1,"List of patterns, relative to source directory, that match files and"
v0.7.1,directories to ignore when looking for source files.
v0.7.1,This pattern also affects html_static_path and html_extra_path .
v0.7.1,The name of the Pygments (syntax highlighting) style to use.
v0.7.1,-- Options for HTML output -------------------------------------------------
v0.7.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.7.1,a list of builtin themes.
v0.7.1,
v0.7.1,html_theme = 'sphinx-rtd-theme'
v0.7.1,on_rtd is whether we are on readthedocs.org
v0.7.1,only import and set the theme if we're building docs locally
v0.7.1,"otherwise, readthedocs.org uses their theme by default, so no need to specify it"
v0.7.1,Theme options are theme-specific and customize the look and feel of a theme
v0.7.1,"further.  For a list of options available for each theme, see the"
v0.7.1,documentation.
v0.7.1,
v0.7.1,html_theme_options = {}
v0.7.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.7.1,"relative to this directory. They are copied after the builtin static files,"
v0.7.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.7.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.7.1,to template names.
v0.7.1,
v0.7.1,The default sidebars (for documents that don't match any pattern) are
v0.7.1,defined by theme itself.  Builtin themes are using these templates by
v0.7.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.7.1,'searchbox.html']``.
v0.7.1,
v0.7.1,html_sidebars = {}
v0.7.1,-- Options for HTMLHelp output ---------------------------------------------
v0.7.1,Output file base name for HTML help builder.
v0.7.1,-- Options for LaTeX output ------------------------------------------------
v0.7.1,The paper size ('letterpaper' or 'a4paper').
v0.7.1,
v0.7.1,"'papersize': 'letterpaper',"
v0.7.1,"The font size ('10pt', '11pt' or '12pt')."
v0.7.1,
v0.7.1,"'pointsize': '10pt',"
v0.7.1,Additional stuff for the LaTeX preamble.
v0.7.1,
v0.7.1,"'preamble': '',"
v0.7.1,Latex figure (float) alignment
v0.7.1,
v0.7.1,"'figure_align': 'htbp',"
v0.7.1,Grouping the document tree into LaTeX files. List of tuples
v0.7.1,"(source start file, target name, title,"
v0.7.1,"author, documentclass [howto, manual, or own class])."
v0.7.1,-- Options for manual page output ------------------------------------------
v0.7.1,One entry per manual page. List of tuples
v0.7.1,"(source start file, name, description, authors, manual section)."
v0.7.1,-- Options for Texinfo output ----------------------------------------------
v0.7.1,Grouping the document tree into Texinfo files. List of tuples
v0.7.1,"(source start file, target name, title, author,"
v0.7.1,"dir menu entry, description, category)"
v0.7.1,-- Options for Epub output -------------------------------------------------
v0.7.1,Bibliographic Dublin Core info.
v0.7.1,The unique identifier of the text. This can be a ISBN number
v0.7.1,or the project homepage.
v0.7.1,
v0.7.1,epub_identifier = ''
v0.7.1,A unique identification for the text.
v0.7.1,
v0.7.1,epub_uid = ''
v0.7.1,A list of files that should not be packed into the epub file.
v0.7.1,-- Extension configuration -------------------------------------------------
v0.7.1,-- Options for todo extension ----------------------------------------------
v0.7.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.7.1,init docstrings should also be included in class
v0.7.1,requires stdin input for identify in weighting sampler
v0.7.1,requires Rpy2 for lalonde
v0.7.1,requires Rpy2 for causal discovery
v0.7.1,very slow
v0.7.1,will be removed
v0.7.1,"applied notebook, not necessary to test each time"
v0.7.1,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.7.1,can import dowhy
v0.7.1,"""--ExecutePreprocessor.timeout=600"","
v0.7.1,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.7.1,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.7.1,treated = self.df[self.df['z']==1]
v0.7.1,self.att = np.mean(treated['y1'] - treated['y0'])
v0.7.1,def test_average_treatment_effect(self):
v0.7.1,est_ate = 1
v0.7.1,bias = est_ate - self.ate
v0.7.1,print(bias)
v0.7.1,"self.assertAlmostEqual(self.ate, est_ate)"
v0.7.1,def test_average_treatment_effect_on_treated(self):
v0.7.1,est_att = 1
v0.7.1,self.att=1
v0.7.1,bias = est_att - self.att
v0.7.1,print(bias)
v0.7.1,"self.assertAlmostEqual(self.att, est_att)"
v0.7.1,removing two common causes
v0.7.1,removing two common causes
v0.7.1,removing two common causes
v0.7.1,removing two common causes
v0.7.1,The outcome is a linear function of the confounder
v0.7.1,"The slope is 1,2 and the intercept is 3"
v0.7.1,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.7.1,Supports user-provided dataset object
v0.7.1,To test if there are any exceptions
v0.7.1,To test if the estimate is identical if refutation parameters are zero
v0.7.1,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.7.1,"Ordinarily, we should expect this value to be zero."
v0.7.1,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.7.1,"Ordinarily, we should expect this value to be zero."
v0.7.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7.1,Only P(Y|T) should be present for test to succeed.
v0.7.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7.1,"Since undirected graph, identify effect must throw an error."
v0.7.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7.1,Compare with ground truth
v0.7.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7.1,Compare with ground truth
v0.7.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7.1,Compare with ground truth
v0.7.1,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7.1,Compare with ground truth
v0.7.1,Causal model initialization
v0.7.1,Causal identifier identification
v0.7.1,Obtain backdoor sets
v0.7.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.7.1,Causal model initialization
v0.7.1,Causal identifier identification
v0.7.1,Obtain backdoor sets
v0.7.1,Causal model initialization
v0.7.1,Causal identifier identification
v0.7.1,Obtain backdoor sets
v0.7.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.7.1,Causal model initialization
v0.7.1,Causal identifier identification
v0.7.1,Obtain backdoor sets
v0.7.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.7.1,Causal model initialization
v0.7.1,Causal identifier identification
v0.7.1,Obtain backdoor sets
v0.7.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.7.1,Causal model initialization
v0.7.1,Causal identifier identification
v0.7.1,Obtain backdoor sets
v0.7.1,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.7.1,"Example is selected from Pearl J. ""Causality"" 2nd Edition, from chapter 3.3.1 on backoor criterion."
v0.7.1,"The following simpsons paradox examples are taken from Pearl, J {2013}. ""Understanding Simpson’s Paradox"" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf"
v0.7.1,"The following are examples given in the ""Book of Why"" by Judea Pearl, chapter ""The Do-operator and the Back-Door Criterion"""
v0.7.1,cov_mat = np.diag(np.ones(num_features))
v0.7.1,Setup data
v0.7.1,Test LinearDML
v0.7.1,Test ContinuousTreatmentOrthoForest
v0.7.1,Test LinearDRLearner
v0.7.1,Setup data
v0.7.1,Test DeepIV
v0.7.1,"Treatment model,"
v0.7.1,Response model
v0.7.1,Test IntentToTreatDRIV
v0.7.1,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.7.1,More cases where Exception  is expected
v0.7.1,"Compute confidence intervals, standard error and significance tests"
v0.7.1,Defined a linear dataset with a given set of properties
v0.7.1,Create a model that captures the same
v0.7.1,Identify the effects within the model
v0.7.1,Defined a linear dataset with a given set of properties
v0.7.1,Create a model that captures the same
v0.7.1,Identify the effects within the model
v0.7.1,Defined a linear dataset with a given set of properties
v0.7.1,Create a model that captures the same
v0.7.1,Identify the effects within the model
v0.7.1,Defined a linear dataset with a given set of properties
v0.7.1,Create a model that captures the same
v0.7.1,Identify the effects within the model
v0.7.1,Defined a linear dataset with a given set of properties
v0.7.1,Create a model that captures the same
v0.7.1,Identify the effects within the model
v0.7.1,Backdoor method names
v0.7.1,"First, check if there is a directed path from action to outcome"
v0.7.1,## 1. BACKDOOR IDENTIFICATION
v0.7.1,"First, checking if there are any valid backdoor adjustment sets"
v0.7.1,"Setting default ""backdoor"" identification adjustment set"
v0.7.1,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.7.1,Now checking if there is also a valid iv estimand
v0.7.1,## 3. FRONTDOOR IDENTIFICATION
v0.7.1,Now checking if there is a valid frontdoor variable
v0.7.1,Finally returning the estimand object
v0.7.1,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.7.1,"First, checking if there are any valid backdoor adjustment sets"
v0.7.1,"Setting default ""backdoor"" identification adjustment set"
v0.7.1,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.7.1,Now checking if there are valid mediator variables
v0.7.1,Finally returning the estimand object
v0.7.1,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.7.1,"First, checking if there are any valid backdoor adjustment sets"
v0.7.1,"Setting default ""backdoor"" identification adjustment set"
v0.7.1,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.7.1,Now checking if there are valid mediator variables
v0.7.1,Finally returning the estimand object
v0.7.1,"First, checking if empty set is a valid backdoor set"
v0.7.1,"If the method is `minimal-adjustment`, return the empty set right away."
v0.7.1,"Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible."
v0.7.1,"If var is d-separated from both treatment or outcome, it cannot"
v0.7.1,be a part of the backdoor set
v0.7.1,repeat the above search with BACKDOOR_MIN
v0.7.1,"If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest."
v0.7.1,"If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set."
v0.7.1,"If all variables are observed, and the biggest eligible set"
v0.7.1,"does not satisfy backdoor, then none of its subsets will."
v0.7.1,Adding a None estimand if no backdoor set found
v0.7.1,"Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable."
v0.7.1,Default set is the one with the least number of adjustment variables (optimizing for efficiency)
v0.7.1,"For simplicity, assuming a one-variable frontdoor set"
v0.7.1,Cond 1: All directed paths intercepted by candidate_var
v0.7.1,Cond 2: No confounding between treatment and candidate var
v0.7.1,Cond 3: treatment blocks all confounding between candidate_var and outcome
v0.7.1,"For simplicity, assuming a one-variable mediation set"
v0.7.1,"Create estimands dict as per the API for backdoor, but do not return it"
v0.7.1,"Setting default ""backdoor"" identification adjustment set"
v0.7.1,"Create estimands dict as per the API for backdoor, but do not return it"
v0.7.1,"Setting default ""backdoor"" identification adjustment set"
v0.7.1,"TODO: outputs string for now, but ideally should do symbolic"
v0.7.1,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.7.1,TODO Better support for multivariate treatments
v0.7.1,TODO: support multivariate treatments better.
v0.7.1,TODO: support multivariate treatments better.
v0.7.1,TODO: support multivariate treatments better.
v0.7.1,For direct effect
v0.7.1,Do not show backdoor key unless it is the only backdoor set.
v0.7.1,Just show the default backdoor set
v0.7.1,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.7.1,Unpacking the keyword arguments
v0.7.1,Default value for the number of simulations to be conducted
v0.7.1,"Concatenate the confounders, instruments and effect modifiers"
v0.7.1,Shuffle the confounders
v0.7.1,Check if all are select or deselect variables
v0.7.1,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.7.1,Initializing the p_value
v0.7.1,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.7.1,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.7.1,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.7.1,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.7.1,Get the number of simulations
v0.7.1,Sort the simulations
v0.7.1,Obtain the median value
v0.7.1,Performing a two sided test
v0.7.1,np.searchsorted tells us the index if it were a part of the array
v0.7.1,We select side to be left as we want to find the first value that matches
v0.7.1,We subtact 1 as we are finding the value from the right tail
v0.7.1,We take the side to be right as we want to find the last index that matches
v0.7.1,We get the probability with respect to the left tail.
v0.7.1,Get the mean for the simulations
v0.7.1,Get the standard deviation for the simulations
v0.7.1,Get the Z Score [(val - mean)/ std_dev ]
v0.7.1,re.sub only takes string parameter so the first if is to avoid error
v0.7.1,"if the input is a text file, convert the contained data into string"
v0.7.1,load dot file
v0.7.1,Adding node attributes
v0.7.1,adding penwidth to make the edge bold
v0.7.1,Adding common causes
v0.7.1,Adding instruments
v0.7.1,Adding effect modifiers
v0.7.1,Assuming the simple form of effect modifier
v0.7.1,that directly causes the outcome.
v0.7.1,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.7.1,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.7.1,Adding columns in the dataframe as confounders that were not in the graph
v0.7.1,Adding unobserved confounders
v0.7.1,also return the number of backdoor paths blocked by observed nodes
v0.7.1,Assume that nodes1 is the treatment
v0.7.1,"ignores new_graph parameter, always uses self._graph"
v0.7.1,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.7.1,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.7.1,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.7.1,removing all mediators
v0.7.1,[TODO: double check these work with multivariate implementation:]
v0.7.1,Exclusion
v0.7.1,As-if-random setup
v0.7.1,As-if-random
v0.7.1,convert the outputted generator into a list
v0.7.1,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.7.1,return len(dpaths) > 0
v0.7.1,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.7.1,Create causal graph object
v0.7.1,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.7.1,(Because some effect modifiers may also be common causes)
v0.7.1,"In such cases, the user-provided modifiers are used."
v0.7.1,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.7.1,Import causal discovery class
v0.7.1,Initialize causal graph object
v0.7.1,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.7.1,TODO add dowhy as a prefix to all dowhy estimators
v0.7.1,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.7.1,Define the third-party estimation method to be used
v0.7.1,Process the dowhy estimators
v0.7.1,Check if estimator's target estimand is identified
v0.7.1,Estimator had been computed in a previous call
v0.7.1,Store parameters inside estimate object for refutation methods
v0.7.1,TODO: This add_params needs to move to the estimator class
v0.7.1,inside estimate_effect and estimate_conditional_effect
v0.7.1,Check if estimator's target estimand is identified
v0.7.1,"Note that while the name of the variable is the same,"
v0.7.1,"""self.causal_estimator"", this estimator takes in less"
v0.7.1,parameters than the same from the
v0.7.1,estimate_effect code. It is not advisable to use the
v0.7.1,estimator from this function to call estimate_effect
v0.7.1,with fit_estimator=False.
v0.7.1,Estimator had been computed in a previous call
v0.7.1,The default number of simulations for statistical testing
v0.7.1,The default number of simulations to obtain confidence intervals
v0.7.1,The portion of the total size that should be taken each time to find the confidence intervals
v0.7.1,1 is the recommended value
v0.7.1,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.7.1,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.7.1,The default Confidence Level
v0.7.1,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.7.1,Prefix to add to temporary categorical variables created after discretization
v0.7.1,std args to be removed from locals() before being passed to args_dict
v0.7.1,Currently estimation methods only support univariate treatment and outcome
v0.7.1,Setting the default interpret method
v0.7.1,Setting treatment and outcome values
v0.7.1,Now saving the effect modifiers
v0.7.1,only add the observed nodes
v0.7.1,"Check if some parameters were set, otherwise set to default values"
v0.7.1,Estimate conditional estimates by default
v0.7.1,names of treatment and outcome
v0.7.1,TODO Only works for binary treatment
v0.7.1,Defaulting to class default values if parameters are not provided
v0.7.1,Checking that there is at least one effect modifier
v0.7.1,Making sure that effect_modifier_names is a list
v0.7.1,Making a copy since we are going to be changing effect modifier names
v0.7.1,"For every numeric effect modifier, adding a temp categorical column"
v0.7.1,Grouping by effect modifiers and computing effect separately
v0.7.1,Deleting the temporary categorical columns
v0.7.1,The array that stores the results of all estimations
v0.7.1,Find the sample size the proportion with the population size
v0.7.1,Perform the set number of simulations
v0.7.1,names of treatment and outcome
v0.7.1,Using class default parameters if not specified
v0.7.1,Checking if bootstrap_estimates are already computed
v0.7.1,Checked if any parameter is changed from the previous std error estimate
v0.7.1,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.7.1,Get the variations of each bootstrap estimate and sort
v0.7.1,"Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level"
v0.7.1,Get the lower and upper bounds by subtracting the variations from the estimate
v0.7.1,"Use existing params, if new user defined params are not present"
v0.7.1,Checking if bootstrap_estimates are already computed
v0.7.1,Check if any parameter is changed from the previous std error estimate
v0.7.1,"Use existing params, if new user defined params are not present"
v0.7.1,"self._outcome = self._data[""dummy_outcome""]"
v0.7.1,Processing the null hypothesis estimates
v0.7.1,Doing a two-sided test
v0.7.1,Being conservative with the p-value reported
v0.7.1,Being conservative with the p-value reported
v0.7.1,"If the estimate_index is 0, it depends on the number of simulations"
v0.7.1,Need to test r-squared before supporting
v0.7.1,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.7.1,'r-squared': effect_r_squared
v0.7.1,"elif method == ""r-squared"":"
v0.7.1,outcome_mean = np.mean(self._outcome)
v0.7.1,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.7.1,Assuming a linear model with one variable: the treatment
v0.7.1,Currently only works for continuous y
v0.7.1,causal_model = outcome_mean + estimate.value*self._treatment
v0.7.1,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.7.1,r_squared = 1 - (squared_residual/total_variance)
v0.7.1,return r_squared
v0.7.1,No estimand was identified (identification failed)
v0.7.1,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.7.1,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.7.1,one-hot encode discrete W
v0.7.1,Now deleting the old continuous value
v0.7.1,Making beta an array
v0.7.1,TODO Ensure that we do not generate weak instruments
v0.7.1,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.7.1,Converting treatment to binary if required
v0.7.1,Generating frontdoor variables if asked for
v0.7.1,NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.
v0.7.1,"For categorical t, this example dataset has the effect size for category 2 being exactly"
v0.7.1,double the effect for category 1
v0.7.1,This could be changed at this stage by one-hot encoding t and using a custom beta that
v0.7.1,"sets a different effect for each category {0, 1, 2}"
v0.7.1,Computing ATE
v0.7.1,constructing column names for one-hot encoded discrete features
v0.7.1,Specifying the correct dtypes
v0.7.1,Now specifying the corresponding graph strings
v0.7.1,Now writing the gml graph
v0.7.1,Making beta an array
v0.7.1,creating data frame
v0.7.1,Specifying the correct dtypes
v0.7.1,Now specifying the corresponding graph strings
v0.7.1,Now writing the gml graph
v0.7.1,Adding edges between common causes and the frontdoor mediator
v0.7.1,Error terms
v0.7.1,else:
v0.7.1,V = 6 + W0 + tterm + E1
v0.7.1,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.7.1,Generating a random normal distribution of integers
v0.7.1,Generating data for nodes which have no incoming edges
v0.7.1,"""currset"" variable currently has all the successors of the nodes which had no incoming edges"
v0.7.1,Loading version number
v0.7.1,The currently supported estimators
v0.7.1,The default standard deviation for noise
v0.7.1,The default scaling factor to determine the bucket size
v0.7.1,The minimum number of points for the estimator to run
v0.7.1,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.7.1,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.7.1,The Default split for the number of data points that fall into the training and validation sets
v0.7.1,Assuming that outcome is one-dimensional
v0.7.1,We need to change the identified estimand
v0.7.1,"We thus, make a copy. This is done as we don't want"
v0.7.1,to change the original DataFrame
v0.7.1,We use collections.OrderedDict to maintain the order in which the data is stored
v0.7.1,Check if we are using an estimator in the transformation list
v0.7.1,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.7.1,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.7.1,"loops. Thus, we can get different values everytime we get the estimator."
v0.7.1,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.7.1,Adding an unobserved confounder if provided by the user
v0.7.1,We set X_train = 0 and outcome_train to be 0
v0.7.1,"Get the final outcome, after running through all the values in the transformation list"
v0.7.1,Check if the value of true effect has been already stored
v0.7.1,We use None as the key as we have no base category for this refutation
v0.7.1,As we currently support only one treatment
v0.7.1,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.7.1,Check if the value of true effect has been already stored
v0.7.1,This ensures that we calculate the causal effect only once.
v0.7.1,We use key_train as we map data with respect to the base category of the data
v0.7.1,As we currently support only one treatment
v0.7.1,Add h(t) to f(W) to get the dummy outcome
v0.7.1,We convert to ndarray for ease in indexing
v0.7.1,The data is of the form
v0.7.1,sim1: cat1 cat2 ... catn
v0.7.1,sim2: cat1 cat2 ... catn
v0.7.1,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.7.1,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.7.1,distribution of the refuter.
v0.7.1,True Causal Effect list
v0.7.1,Iterating through the refutation for each category
v0.7.1,We use string arguments to account for both 32 and 64 bit varaibles
v0.7.1,action for continuous variables
v0.7.1,Action for categorical variables
v0.7.1,Find the set difference for each row
v0.7.1,Choose one out of the remaining
v0.7.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.7.1,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.7.1,of the treatment to affect the outcome
v0.7.1,Standardizing the data
v0.7.1,Fit a model containing all confounders and compare predictions
v0.7.1,using all features compared to all features except a given
v0.7.1,confounder.
v0.7.1,Estimating the regression coefficient from standardized features to t
v0.7.1,"By default, return a plot with 10 points"
v0.7.1,consider 10 values of the effect of the unobserved confounder
v0.7.1,Standardizing the data
v0.7.1,Fit a model containing all confounders and compare predictions
v0.7.1,using all features compared to all features except a given
v0.7.1,confounder.
v0.7.1,"By default, return a plot with 10 points"
v0.7.1,consider 10 values of the effect of the unobserved confounder
v0.7.1,Get a 2D matrix of values
v0.7.1,"x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN"
v0.7.1,Store the values into the refute object
v0.7.1,Adding a label on the contour line for the original estimate
v0.7.1,Label every other level using strings
v0.7.1,"By default, we add the effect of simulated confounder for treatment."
v0.7.1,But subtract it from outcome to create a negative correlation
v0.7.1,assuming that the original confounder's effect was positive on both.
v0.7.1,This is to remove the effect of the original confounder.
v0.7.1,"By default, we add the effect of simulated confounder for treatment."
v0.7.1,But subtract it from outcome to create a negative correlation
v0.7.1,assuming that the original confounder's effect was positive on both.
v0.7.1,This is to remove the effect of the original confounder.
v0.7.1,Obtaining the list of observed variables
v0.7.1,Taking a subset of the dataframe that has only observed variables
v0.7.1,Residuals from the outcome model obtained by fitting a linear model
v0.7.1,Residuals from the treatment model obtained by fitting a linear model
v0.7.1,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.7.1,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.7.1,Choosing a c_star based on the data.
v0.7.1,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.7.1,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.7.1,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.7.1,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.7.1,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.7.1,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.7.1,Default value of the p value taken for the distribution
v0.7.1,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.7.1,Mean of the Normal Distribution
v0.7.1,Standard Deviation of the Normal Distribution
v0.7.1,only permute is supported for iv methods
v0.7.1,We need to change the identified estimand
v0.7.1,"We make a copy as a safety measure, we don't want to change the"
v0.7.1,original DataFrame
v0.7.1,"For IV methods, the estimating_instrument_names should also be"
v0.7.1,changed. So we change it inside the estimate and then restore it
v0.7.1,back at the end of this method.
v0.7.1,Create a new column in the data by the name of placebo
v0.7.1,Sanity check the data
v0.7.1,Restoring the value of iv_instrument_name
v0.7.1,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.7.1,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.7.1,relationship between the treatment and the outcome.
v0.7.1,Adding a new backdoor variable to the identified estimand
v0.7.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.7.1,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.7.1,of the treatment to affect the outcome
v0.7.1,Reject H0
v0.7.1,"a, b and c are all continuous variables"
v0.7.1,"a, b and c are all discrete variables"
v0.7.1,c is set of continuous and binary variables and
v0.7.1,1. either a and b is continuous and the other is binary
v0.7.1,2. both a and b are binary
v0.7.1,c is discrete and
v0.7.1,either a or b is continuous and the other is discrete
v0.7.1,a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information
v0.7.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7.1,The default subset of the data to be used
v0.7.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.7.1,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.7.1,of the treatment to affect the outcome
v0.7.1,Get adjacency list
v0.7.1,If node pair has been fully explored
v0.7.1,Add node1 to backdoor set of node_pair
v0.7.1,Check if path is backdoor and does not have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.7.1,"True if arrow incoming, False if arrow outgoing"
v0.7.1,"Mark pair (node1, node2) complete"
v0.7.1,Modify variable count and indices covered
v0.7.1,Estimators list for returning after identification
v0.7.1,Line 1
v0.7.1,"If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y."
v0.7.1,Line 2
v0.7.1,"If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y."
v0.7.1,Modify list of valid nodes
v0.7.1,Line 3 - forces an action on any node where such an action would have no effect on Y – assuming we already acted on X.
v0.7.1,Modify adjacency matrix to obtain that corresponding to do(X)
v0.7.1,Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.
v0.7.1,"If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases."
v0.7.1,Modify adjacency matrix to remove treatment variables
v0.7.1,"Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes."
v0.7.1,"Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem."
v0.7.1,Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.
v0.7.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7.1,First stage statistical model
v0.7.1,Second stage statistical model
v0.7.1,Required to ensure that self.method_params contains all the
v0.7.1,parameters needed to create an object of this class
v0.7.1,Check if the treatment is one-dimensional
v0.7.1,First stage
v0.7.1,Second Stage
v0.7.1,Combining the two estimates
v0.7.1,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.7.1,Total  effect of treatment
v0.7.1,Bulding the feature matrix
v0.7.1,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.7.1,Required to ensure that self.method_params contains all the information
v0.7.1,to create an object of this class
v0.7.1,choosing the instrumental variable to use
v0.7.1,TODO move this to the identification step
v0.7.1,Obtain estimate by Wald Estimator
v0.7.1,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.7.1,More than 1 instrument. Use 2sls.
v0.7.1,Required to ensure that self.method_params contains all the
v0.7.1,parameters needed to create an object of this class
v0.7.1,Checking if Y is binary
v0.7.1,Required to ensure that self.method_params contains all the
v0.7.1,parameters to create an object of this class
v0.7.1,Enable the user to pass params for a custom propensity model
v0.7.1,Check if the treatment is one-dimensional
v0.7.1,Checking if the treatment is binary
v0.7.1,Convert the categorical variables into dummy/indicator variables
v0.7.1,"Basically, this gives a one hot encoding for each category"
v0.7.1,The first category is taken to be the base line.
v0.7.1,Required to ensure that self.method_params contains all the information
v0.7.1,to create an object of this class
v0.7.1,TODO make treatment_value and control value also as local parameters
v0.7.1,Checking if the model is already trained
v0.7.1,The model is always built on the entire data
v0.7.1,All treatments are set to the same constant value
v0.7.1,Using all data by default
v0.7.1,"Fixing treatment value to the specified value, if provided"
v0.7.1,treatment_vals and data_df should have same number of rows
v0.7.1,Bulding the feature matrix
v0.7.1,The model is always built on the entire data
v0.7.1,Replacing treatment values by given x
v0.7.1,"First, create interventional tensor in original space"
v0.7.1,"Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment"
v0.7.1,Required to ensure that self.method_params contains all the
v0.7.1,parameters to create an object of this class
v0.7.1,The average treatment effect is a combination of different
v0.7.1,regression coefficients. Complicated to compute the confidence
v0.7.1,"interval analytically. For example, if y=a + b1.t + b2.tx, then"
v0.7.1,the average treatment effect is b1+b2.mean(x).
v0.7.1,"Refer Gelman, Hill. ARM Book. Chapter 9"
v0.7.1,http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
v0.7.1,TODO: Looking for contributions
v0.7.1,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.7.1,"variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0."
v0.7.1,"So for custom treatment and control values, we must multiply the confidence interval by the difference of the two."
v0.7.1,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.7.1,"variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0."
v0.7.1,"So for custom treatment and control values, we must multiply the standard error by the difference of the two."
v0.7.1,Required to ensure that self.method_params contains all the information
v0.7.1,to create an object of this class
v0.7.1,setting method-specific parameters
v0.7.1,check if the user provides the propensity score column
v0.7.1,Infer the right strata based on clipping threshold
v0.7.1,0.5 because there are two values for the treatment
v0.7.1,To be conservative and allow most strata to be included in the
v0.7.1,analysis
v0.7.1,At least 90% of the strata should be included in analysis
v0.7.1,sum weighted outcomes over all strata  (weight by treated population)
v0.7.1,TODO - how can we add additional information into the returned estimate?
v0.7.1,"such as how much clipping was done, or per-strata info for debugging?"
v0.7.1,sort the dataframe by propensity score
v0.7.1,create a column 'strata' for each element that marks what strata it belongs to
v0.7.1,"for each strata, count how many treated and control units there are"
v0.7.1,throw away strata that have insufficient treatment or control
v0.7.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.7.1,Required to ensure that self.method_params contains all the information
v0.7.1,to create an object of this class
v0.7.1,Setting method specific parameters
v0.7.1,check if user provides the propensity score column
v0.7.1,trim propensity score weights
v0.7.1,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.7.1,nips ==> ips / (sum of ips over all units)
v0.7.1,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.7.1,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.7.1,Vanilla IPS estimator
v0.7.1,The Hajek estimator (or the self-normalized estimator)
v0.7.1,"Stabilized weights (from Robins, Hernan, Brumback (2000))"
v0.7.1,Paper: Marginal Structural Models and Causal Inference in Epidemiology
v0.7.1,Calculating the effect
v0.7.1,Subtracting the weighted means
v0.7.1,TODO - how can we add additional information into the returned estimate?
v0.7.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.7.1,Required to ensure that self.method_params contains all the
v0.7.1,parameters to create an object of this class
v0.7.1,For metalearners only--issue a warning if w contains variables not in x
v0.7.1,Override the effect_modifiers set in CausalEstimator.__init__()
v0.7.1,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.7.1,the latter can be used by other estimator methods later
v0.7.1,"Instrumental variables names, if present"
v0.7.1,choosing the instrumental variable to use
v0.7.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7.1,Calling the econml estimator's fit method
v0.7.1,"As of v0.9, econml has some kewyord only arguments"
v0.7.1,Changing shape to a list for a singleton value
v0.7.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.7.1,check if the user provides a propensity score column
v0.7.1,this assumes a binary treatment regime
v0.7.1,TODO remove neighbors that are more than a given radius apart
v0.7.1,estimate ATT on treated by summing over difference between matched neighbors
v0.7.1,Now computing ATC
v0.7.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.7.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7.1,Handle externally provided estimator classes
v0.7.1,allowed types of distance metric
v0.7.1,Required to ensure that self.method_params contains all the
v0.7.1,parameters to create an object of this class
v0.7.1,Check if the treatment is one-dimensional
v0.7.1,Checking if the treatment is binary
v0.7.1,Convert the categorical variables into dummy/indicator variables
v0.7.1,"Basically, this gives a one hot encoding for each category"
v0.7.1,The first category is taken to be the base line.
v0.7.1,Dictionary of any user-provided params for the distance metric
v0.7.1,that will be passed to sklearn nearestneighbors
v0.7.1,this assumes a binary treatment regime
v0.7.1,TODO remove neighbors that are more than a given radius apart
v0.7.1,estimate ATT on treated by summing over difference between matched neighbors
v0.7.1,Return indices in the original dataframe
v0.7.1,self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()
v0.7.1,Now computing ATC
v0.7.1,Return indices in the original dataframe
v0.7.1,Required to ensure that self.method_params contains all the information
v0.7.1,to create an object of this class
v0.7.1,Add the identification method used in the estimator
v0.7.1,Check the backdoor variables being used
v0.7.1,Add the observed confounders and one hot encode the categorical variables
v0.7.1,Get the data of the unobserved confounders
v0.7.1,One hot encode the data if they are categorical
v0.7.1,Check the instrumental variables involved
v0.7.1,Perform the same actions as the above
v0.7.1,Check if effect modifiers are used
v0.7.1,Get the class corresponding the the estimator to be used
v0.7.1,Initialize the object
v0.7.1,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.7.1,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.7.1,For CATEs
v0.7.1,TODO we are conditioning on a postive treatment
v0.7.1,TODO create an expression corresponding to each estimator used
v0.7.1,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.7.1,Flipping some values
v0.7.1,Finding p-value using student T test
v0.7.1,Only consider edges have absolute edge weight > 0.01
v0.7.1,Modify graph such that it only contains bidirected edges
v0.7.1,Find c components by finding connected components on the undirected graph
v0.7.1,Understanding Neural Network weights
v0.7.1,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.7.1,add weight column
v0.7.1,before weights are applied we count number rows in each category
v0.7.1,which is equivalent to summing over weight=1
v0.7.1,after weights are applied we need to sum over the given weights
v0.7.1,"First, calculating mean differences by strata"
v0.7.1,"Second, without strata"
v0.7.1,"Third, concatenating them and plotting"
v0.7.1,Setting estimator attribute for convenience
v0.7.1,Outcome is numeric
v0.7.1,Treatments are also numeric or binary
v0.7.1,Outcome is categorical
v0.7.1,Treatments are numeric or binary
v0.7.1,TODO: A common way to show all plots
v0.7.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7.1,self._identified_estimand = self._causal_model.identify_effect()
v0.7.1,"self._identified_estimand,"
v0.7.1,"self._causal_model._treatment,"
v0.7.1,"self._causal_model._outcome,"
v0.7.1,If labels provided
v0.7.1,Return in valid DOT format
v0.7.1,Get adjacency matrix
v0.7.1,If labels not provided
v0.7.1,Obtain valid DOT format
v0.7.1,If labels provided
v0.7.1,Return in valid DOT format
v0.7.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7,Get the long description from the README file
v0.7,Get the required packages
v0.7,Plotting packages are optional to install
v0.7,Loading version number
v0.7,-*- coding: utf-8 -*-
v0.7,
v0.7,Configuration file for the Sphinx documentation builder.
v0.7,
v0.7,This file does only contain a selection of the most common options. For a
v0.7,full list see the documentation:
v0.7,http://www.sphinx-doc.org/en/stable/config
v0.7,-- Path setup --------------------------------------------------------------
v0.7,"If extensions (or modules to document with autodoc) are in another directory,"
v0.7,add these directories to sys.path here. If the directory is relative to the
v0.7,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.7,
v0.7,-- Project information -----------------------------------------------------
v0.7,The short X.Y version
v0.7,"The full version, including alpha/beta/rc tags"
v0.7,-- General configuration ---------------------------------------------------
v0.7,"If your documentation needs a minimal Sphinx version, state it here."
v0.7,
v0.7,needs_sphinx = '1.0'
v0.7,"Add any Sphinx extension module names here, as strings. They can be"
v0.7,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.7,ones.
v0.7,"Add any paths that contain templates here, relative to this directory."
v0.7,The suffix(es) of source filenames.
v0.7,You can specify multiple suffix as a list of string:
v0.7,
v0.7,"source_suffix = ['.rst', '.md']"
v0.7,The master toctree document.
v0.7,The language for content autogenerated by Sphinx. Refer to documentation
v0.7,for a list of supported languages.
v0.7,
v0.7,This is also used if you do content translation via gettext catalogs.
v0.7,"Usually you set ""language"" from the command line for these cases."
v0.7,"List of patterns, relative to source directory, that match files and"
v0.7,directories to ignore when looking for source files.
v0.7,This pattern also affects html_static_path and html_extra_path .
v0.7,The name of the Pygments (syntax highlighting) style to use.
v0.7,-- Options for HTML output -------------------------------------------------
v0.7,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.7,a list of builtin themes.
v0.7,
v0.7,html_theme = 'sphinx-rtd-theme'
v0.7,on_rtd is whether we are on readthedocs.org
v0.7,only import and set the theme if we're building docs locally
v0.7,"otherwise, readthedocs.org uses their theme by default, so no need to specify it"
v0.7,Theme options are theme-specific and customize the look and feel of a theme
v0.7,"further.  For a list of options available for each theme, see the"
v0.7,documentation.
v0.7,
v0.7,html_theme_options = {}
v0.7,"Add any paths that contain custom static files (such as style sheets) here,"
v0.7,"relative to this directory. They are copied after the builtin static files,"
v0.7,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.7,"Custom sidebar templates, must be a dictionary that maps document names"
v0.7,to template names.
v0.7,
v0.7,The default sidebars (for documents that don't match any pattern) are
v0.7,defined by theme itself.  Builtin themes are using these templates by
v0.7,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.7,'searchbox.html']``.
v0.7,
v0.7,html_sidebars = {}
v0.7,-- Options for HTMLHelp output ---------------------------------------------
v0.7,Output file base name for HTML help builder.
v0.7,-- Options for LaTeX output ------------------------------------------------
v0.7,The paper size ('letterpaper' or 'a4paper').
v0.7,
v0.7,"'papersize': 'letterpaper',"
v0.7,"The font size ('10pt', '11pt' or '12pt')."
v0.7,
v0.7,"'pointsize': '10pt',"
v0.7,Additional stuff for the LaTeX preamble.
v0.7,
v0.7,"'preamble': '',"
v0.7,Latex figure (float) alignment
v0.7,
v0.7,"'figure_align': 'htbp',"
v0.7,Grouping the document tree into LaTeX files. List of tuples
v0.7,"(source start file, target name, title,"
v0.7,"author, documentclass [howto, manual, or own class])."
v0.7,-- Options for manual page output ------------------------------------------
v0.7,One entry per manual page. List of tuples
v0.7,"(source start file, name, description, authors, manual section)."
v0.7,-- Options for Texinfo output ----------------------------------------------
v0.7,Grouping the document tree into Texinfo files. List of tuples
v0.7,"(source start file, target name, title, author,"
v0.7,"dir menu entry, description, category)"
v0.7,-- Options for Epub output -------------------------------------------------
v0.7,Bibliographic Dublin Core info.
v0.7,The unique identifier of the text. This can be a ISBN number
v0.7,or the project homepage.
v0.7,
v0.7,epub_identifier = ''
v0.7,A unique identification for the text.
v0.7,
v0.7,epub_uid = ''
v0.7,A list of files that should not be packed into the epub file.
v0.7,-- Extension configuration -------------------------------------------------
v0.7,-- Options for todo extension ----------------------------------------------
v0.7,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.7,requires stdin input for identify in weighting sampler
v0.7,requires Rpy2 for lalonde
v0.7,requires Rpy2 for causal discovery
v0.7,very slow
v0.7,will be removed
v0.7,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.7,can import dowhy
v0.7,"""--ExecutePreprocessor.timeout=600"","
v0.7,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.7,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.7,treated = self.df[self.df['z']==1]
v0.7,self.att = np.mean(treated['y1'] - treated['y0'])
v0.7,def test_average_treatment_effect(self):
v0.7,est_ate = 1
v0.7,bias = est_ate - self.ate
v0.7,print(bias)
v0.7,"self.assertAlmostEqual(self.ate, est_ate)"
v0.7,def test_average_treatment_effect_on_treated(self):
v0.7,est_att = 1
v0.7,self.att=1
v0.7,bias = est_att - self.att
v0.7,print(bias)
v0.7,"self.assertAlmostEqual(self.att, est_att)"
v0.7,removing two common causes
v0.7,The outcome is a linear function of the confounder
v0.7,"The slope is 1,2 and the intercept is 3"
v0.7,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.7,Supports user-provided dataset object
v0.7,To test if there are any exceptions
v0.7,To test if the estimate is identical if refutation parameters are zero
v0.7,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.7,"Ordinarily, we should expect this value to be zero."
v0.7,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.7,"Ordinarily, we should expect this value to be zero."
v0.7,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7,Only P(Y|T) should be present for test to succeed.
v0.7,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7,"Since undirected graph, identify effect must throw an error."
v0.7,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7,Compare with ground truth
v0.7,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7,Compare with ground truth
v0.7,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7,Compare with ground truth
v0.7,"Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)"
v0.7,Compare with ground truth
v0.7,Causal model initialization
v0.7,Causal identifier identification
v0.7,Obtain backdoor sets
v0.7,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.7,Causal model initialization
v0.7,Causal identifier identification
v0.7,Obtain backdoor sets
v0.7,Causal model initialization
v0.7,Causal identifier identification
v0.7,Obtain backdoor sets
v0.7,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.7,Causal model initialization
v0.7,Causal identifier identification
v0.7,Obtain backdoor sets
v0.7,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.7,Causal model initialization
v0.7,Causal identifier identification
v0.7,Obtain backdoor sets
v0.7,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.7,Causal model initialization
v0.7,Causal identifier identification
v0.7,Obtain backdoor sets
v0.7,Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome
v0.7,"Example is selected from Pearl J. ""Causality"" 2nd Edition, from chapter 3.3.1 on backoor criterion."
v0.7,"The following simpsons paradox examples are taken from Pearl, J {2013}. ""Understanding Simpson’s Paradox"" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf"
v0.7,"The following are examples given in the ""Book of Why"" by Judea Pearl, chapter ""The Do-operator and the Back-Door Criterion"""
v0.7,cov_mat = np.diag(np.ones(num_features))
v0.7,Setup data
v0.7,Test LinearDML
v0.7,Test ContinuousTreatmentOrthoForest
v0.7,Test LinearDRLearner
v0.7,Setup data
v0.7,Test DeepIV
v0.7,"Treatment model,"
v0.7,Response model
v0.7,Test IntentToTreatDRIV
v0.7,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.7,More cases where Exception  is expected
v0.7,"Compute confidence intervals, standard error and significance tests"
v0.7,Defined a linear dataset with a given set of properties
v0.7,Create a model that captures the same
v0.7,Identify the effects within the model
v0.7,Defined a linear dataset with a given set of properties
v0.7,Create a model that captures the same
v0.7,Identify the effects within the model
v0.7,Defined a linear dataset with a given set of properties
v0.7,Create a model that captures the same
v0.7,Identify the effects within the model
v0.7,Defined a linear dataset with a given set of properties
v0.7,Create a model that captures the same
v0.7,Identify the effects within the model
v0.7,Defined a linear dataset with a given set of properties
v0.7,Create a model that captures the same
v0.7,Identify the effects within the model
v0.7,Backdoor method names
v0.7,"First, check if there is a directed path from action to outcome"
v0.7,## 1. BACKDOOR IDENTIFICATION
v0.7,"First, checking if there are any valid backdoor adjustment sets"
v0.7,"Setting default ""backdoor"" identification adjustment set"
v0.7,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.7,Now checking if there is also a valid iv estimand
v0.7,## 3. FRONTDOOR IDENTIFICATION
v0.7,Now checking if there is a valid frontdoor variable
v0.7,Finally returning the estimand object
v0.7,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.7,"First, checking if there are any valid backdoor adjustment sets"
v0.7,"Setting default ""backdoor"" identification adjustment set"
v0.7,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.7,Now checking if there are valid mediator variables
v0.7,Finally returning the estimand object
v0.7,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.7,"First, checking if there are any valid backdoor adjustment sets"
v0.7,"Setting default ""backdoor"" identification adjustment set"
v0.7,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.7,Now checking if there are valid mediator variables
v0.7,Finally returning the estimand object
v0.7,"First, checking if empty set is a valid backdoor set"
v0.7,"If the method is `minimal-adjustment`, return the empty set right away."
v0.7,"Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible."
v0.7,"If var is d-separated from both treatment or outcome, it cannot"
v0.7,be a part of the backdoor set
v0.7,repeat the above search with BACKDOOR_MIN
v0.7,"If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest."
v0.7,"If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set."
v0.7,"If all variables are observed, and the biggest eligible set"
v0.7,"does not satisfy backdoor, then none of its subsets will."
v0.7,Adding a None estimand if no backdoor set found
v0.7,"Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable."
v0.7,Default set is the one with the least number of adjustment variables (optimizing for efficiency)
v0.7,"For simplicity, assuming a one-variable frontdoor set"
v0.7,Cond 1: All directed paths intercepted by candidate_var
v0.7,Cond 2: No confounding between treatment and candidate var
v0.7,Cond 3: treatment blocks all confounding between candidate_var and outcome
v0.7,"For simplicity, assuming a one-variable mediation set"
v0.7,"Create estimands dict as per the API for backdoor, but do not return it"
v0.7,"Setting default ""backdoor"" identification adjustment set"
v0.7,"Create estimands dict as per the API for backdoor, but do not return it"
v0.7,"Setting default ""backdoor"" identification adjustment set"
v0.7,"TODO: outputs string for now, but ideally should do symbolic"
v0.7,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.7,TODO Better support for multivariate treatments
v0.7,TODO: support multivariate treatments better.
v0.7,TODO: support multivariate treatments better.
v0.7,TODO: support multivariate treatments better.
v0.7,For direct effect
v0.7,Do not show backdoor key unless it is the only backdoor set.
v0.7,Just show the default backdoor set
v0.7,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.7,Unpacking the keyword arguments
v0.7,Default value for the number of simulations to be conducted
v0.7,"Concatenate the confounders, instruments and effect modifiers"
v0.7,Shuffle the confounders
v0.7,Check if all are select or deselect variables
v0.7,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.7,Initializing the p_value
v0.7,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.7,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.7,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.7,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.7,Get the number of simulations
v0.7,Sort the simulations
v0.7,Obtain the median value
v0.7,Performing a two sided test
v0.7,np.searchsorted tells us the index if it were a part of the array
v0.7,We select side to be left as we want to find the first value that matches
v0.7,We subtact 1 as we are finding the value from the right tail
v0.7,We take the side to be right as we want to find the last index that matches
v0.7,We get the probability with respect to the left tail.
v0.7,Get the mean for the simulations
v0.7,Get the standard deviation for the simulations
v0.7,Get the Z Score [(val - mean)/ std_dev ]
v0.7,load dot file
v0.7,Adding node attributes
v0.7,adding penwidth to make the edge bold
v0.7,Adding common causes
v0.7,Adding instruments
v0.7,Adding effect modifiers
v0.7,Assuming the simple form of effect modifier
v0.7,that directly causes the outcome.
v0.7,"self._graph.add_edge(node_name, outcome, style = ""dotted"", headport=""s"", tailport=""n"")"
v0.7,"self._graph.add_edge(outcome, node_name, style = ""dotted"", headport=""n"", tailport=""s"") # TODO make the ports more general so that they apply not just to top-bottom node configurations"
v0.7,Adding columns in the dataframe as confounders that were not in the graph
v0.7,Adding unobserved confounders
v0.7,also return the number of backdoor paths blocked by observed nodes
v0.7,Assume that nodes1 is the treatment
v0.7,"ignores new_graph parameter, always uses self._graph"
v0.7,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.7,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.7,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.7,removing all mediators
v0.7,[TODO: double check these work with multivariate implementation:]
v0.7,Exclusion
v0.7,As-if-random setup
v0.7,As-if-random
v0.7,convert the outputted generator into a list
v0.7,"dpaths = self.get_all_directed_paths(nodes1, nodes2)"
v0.7,return len(dpaths) > 0
v0.7,Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes
v0.7,Create causal graph object
v0.7,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.7,(Because some effect modifiers may also be common causes)
v0.7,"In such cases, the user-provided modifiers are used."
v0.7,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.7,Import causal discovery class
v0.7,Initialize causal graph object
v0.7,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.7,TODO add dowhy as a prefix to all dowhy estimators
v0.7,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.7,Define the third-party estimation method to be used
v0.7,Process the dowhy estimators
v0.7,Check if estimator's target estimand is identified
v0.7,Estimator had been computed in a previous call
v0.7,Store parameters inside estimate object for refutation methods
v0.7,TODO: This add_params needs to move to the estimator class
v0.7,inside estimate_effect and estimate_conditional_effect
v0.7,Check if estimator's target estimand is identified
v0.7,"Note that while the name of the variable is the same,"
v0.7,"""self.causal_estimator"", this estimator takes in less"
v0.7,parameters than the same from the
v0.7,estimate_effect code. It is not advisable to use the
v0.7,estimator from this function to call estimate_effect
v0.7,with fit_estimator=False.
v0.7,Estimator had been computed in a previous call
v0.7,The default number of simulations for statistical testing
v0.7,The default number of simulations to obtain confidence intervals
v0.7,The portion of the total size that should be taken each time to find the confidence intervals
v0.7,1 is the recommended value
v0.7,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.7,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.7,The default Confidence Level
v0.7,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.7,Prefix to add to temporary categorical variables created after discretization
v0.7,Currently estimation methods only support univariate treatment and outcome
v0.7,Setting the default interpret method
v0.7,Unpacking the keyword arguments
v0.7,Setting treatment and outcome values
v0.7,Now saving the effect modifiers
v0.7,only add the observed nodes
v0.7,"Checking if some parameters were set, otherwise setting to default values"
v0.7,Estimate conditional estimates by default
v0.7,names of treatment and outcome
v0.7,TODO Only works for binary treatment
v0.7,Defaulting to class default values if parameters are not provided
v0.7,Checking that there is at least one effect modifier
v0.7,Making sure that effect_modifier_names is a list
v0.7,Making a copy since we are going to be changing effect modifier names
v0.7,"For every numeric effect modifier, adding a temp categorical column"
v0.7,Grouping by effect modifiers and computing effect separately
v0.7,Deleting the temporary categorical columns
v0.7,The array that stores the results of all estimations
v0.7,Find the sample size the proportion with the population size
v0.7,Perform the set number of simulations
v0.7,names of treatment and outcome
v0.7,Using class default parameters if not specified
v0.7,Checking if bootstrap_estimates are already computed
v0.7,Checked if any parameter is changed from the previous std error estimate
v0.7,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.7,Get the variations of each bootstrap estimate and sort
v0.7,"Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level"
v0.7,Get the lower and upper bounds by subtracting the variations from the estimate
v0.7,"Use existing params, if new user defined params are not present"
v0.7,Checking if bootstrap_estimates are already computed
v0.7,Check if any parameter is changed from the previous std error estimate
v0.7,"Use existing params, if new user defined params are not present"
v0.7,"self._outcome = self._data[""dummy_outcome""]"
v0.7,Processing the null hypothesis estimates
v0.7,Doing a two-sided test
v0.7,Being conservative with the p-value reported
v0.7,Being conservative with the p-value reported
v0.7,"If the estimate_index is 0, it depends on the number of simulations"
v0.7,Need to test r-squared before supporting
v0.7,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.7,'r-squared': effect_r_squared
v0.7,"elif method == ""r-squared"":"
v0.7,outcome_mean = np.mean(self._outcome)
v0.7,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.7,Assuming a linear model with one variable: the treatment
v0.7,Currently only works for continuous y
v0.7,causal_model = outcome_mean + estimate.value*self._treatment
v0.7,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.7,r_squared = 1 - (squared_residual/total_variance)
v0.7,return r_squared
v0.7,No estimand was identified (identification failed)
v0.7,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.7,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.7,one-hot encode discrete W
v0.7,Now deleting the old continuous value
v0.7,Making beta an array
v0.7,TODO Ensure that we do not generate weak instruments
v0.7,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.7,Converting treatment to binary if required
v0.7,Generating frontdoor variables if asked for
v0.7,Computing ATE
v0.7,constructing column names for one-hot encoded discrete features
v0.7,Specifying the correct dtypes
v0.7,Now specifying the corresponding graph strings
v0.7,Now writing the gml graph
v0.7,Making beta an array
v0.7,creating data frame
v0.7,Specifying the correct dtypes
v0.7,Now specifying the corresponding graph strings
v0.7,Now writing the gml graph
v0.7,Adding edges between common causes and the frontdoor mediator
v0.7,Error terms
v0.7,else:
v0.7,V = 6 + W0 + tterm + E1
v0.7,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.7,Loading version number
v0.7,The currently supported estimators
v0.7,The default standard deviation for noise
v0.7,The default scaling factor to determine the bucket size
v0.7,The minimum number of points for the estimator to run
v0.7,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.7,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.7,The Default split for the number of data points that fall into the training and validation sets
v0.7,Assuming that outcome is one-dimensional
v0.7,We need to change the identified estimand
v0.7,"We thus, make a copy. This is done as we don't want"
v0.7,to change the original DataFrame
v0.7,We use collections.OrderedDict to maintain the order in which the data is stored
v0.7,Check if we are using an estimator in the transformation list
v0.7,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.7,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.7,"loops. Thus, we can get different values everytime we get the estimator."
v0.7,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.7,Adding an unobserved confounder if provided by the user
v0.7,We set X_train = 0 and outcome_train to be 0
v0.7,"Get the final outcome, after running through all the values in the transformation list"
v0.7,Check if the value of true effect has been already stored
v0.7,We use None as the key as we have no base category for this refutation
v0.7,As we currently support only one treatment
v0.7,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.7,Check if the value of true effect has been already stored
v0.7,This ensures that we calculate the causal effect only once.
v0.7,We use key_train as we map data with respect to the base category of the data
v0.7,As we currently support only one treatment
v0.7,Add h(t) to f(W) to get the dummy outcome
v0.7,We convert to ndarray for ease in indexing
v0.7,The data is of the form
v0.7,sim1: cat1 cat2 ... catn
v0.7,sim2: cat1 cat2 ... catn
v0.7,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.7,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.7,distribution of the refuter.
v0.7,True Causal Effect list
v0.7,Iterating through the refutation for each category
v0.7,We use string arguments to account for both 32 and 64 bit varaibles
v0.7,action for continuous variables
v0.7,Action for categorical variables
v0.7,Find the set difference for each row
v0.7,Choose one out of the remaining
v0.7,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.7,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.7,of the treatment to affect the outcome
v0.7,Standardizing the data
v0.7,Fit a model containing all confounders and compare predictions
v0.7,using all features compared to all features except a given
v0.7,confounder.
v0.7,Estimating the regression coefficient from standardized features to t
v0.7,"By default, return a plot with 10 points"
v0.7,consider 10 values of the effect of the unobserved confounder
v0.7,Standardizing the data
v0.7,Fit a model containing all confounders and compare predictions
v0.7,using all features compared to all features except a given
v0.7,confounder.
v0.7,"By default, return a plot with 10 points"
v0.7,consider 10 values of the effect of the unobserved confounder
v0.7,Get a 2D matrix of values
v0.7,"x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN"
v0.7,Store the values into the refute object
v0.7,Adding a label on the contour line for the original estimate
v0.7,Label every other level using strings
v0.7,"By default, we add the effect of simulated confounder for treatment."
v0.7,But subtract it from outcome to create a negative correlation
v0.7,assuming that the original confounder's effect was positive on both.
v0.7,This is to remove the effect of the original confounder.
v0.7,"By default, we add the effect of simulated confounder for treatment."
v0.7,But subtract it from outcome to create a negative correlation
v0.7,assuming that the original confounder's effect was positive on both.
v0.7,This is to remove the effect of the original confounder.
v0.7,Obtaining the list of observed variables
v0.7,Taking a subset of the dataframe that has only observed variables
v0.7,Residuals from the outcome model obtained by fitting a linear model
v0.7,Residuals from the treatment model obtained by fitting a linear model
v0.7,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.7,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.7,Choosing a c_star based on the data.
v0.7,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.7,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.7,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.7,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.7,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.7,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.7,Default value of the p value taken for the distribution
v0.7,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.7,Mean of the Normal Distribution
v0.7,Standard Deviation of the Normal Distribution
v0.7,only permute is supported for iv methods
v0.7,We need to change the identified estimand
v0.7,"We make a copy as a safety measure, we don't want to change the"
v0.7,original DataFrame
v0.7,"For IV methods, the estimating_instrument_names should also be"
v0.7,changed. So we change it inside the estimate and then restore it
v0.7,back at the end of this method.
v0.7,Create a new column in the data by the name of placebo
v0.7,Sanity check the data
v0.7,Restoring the value of iv_instrument_name
v0.7,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.7,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.7,relationship between the treatment and the outcome.
v0.7,Adding a new backdoor variable to the identified estimand
v0.7,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.7,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.7,of the treatment to affect the outcome
v0.7,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7,The default subset of the data to be used
v0.7,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.7,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.7,of the treatment to affect the outcome
v0.7,Get adjacency list
v0.7,If node pair has been fully explored
v0.7,Add node1 to backdoor set of node_pair
v0.7,Check if path is backdoor and does not have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.7,"True if arrow incoming, False if arrow outgoing"
v0.7,"Mark pair (node1, node2) complete"
v0.7,Modify variable count and indices covered
v0.7,Estimators list for returning after identification
v0.7,Line 1
v0.7,"If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y."
v0.7,Line 2
v0.7,"If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y."
v0.7,Modify list of valid nodes
v0.7,Line 3 - forces an action on any node where such an action would have no effect on Y – assuming we already acted on X.
v0.7,Modify adjacency matrix to obtain that corresponding to do(X)
v0.7,Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.
v0.7,"If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases."
v0.7,Modify adjacency matrix to remove treatment variables
v0.7,"Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes."
v0.7,"Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem."
v0.7,Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.
v0.7,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7,Check if the treatment is one-dimensional
v0.7,first_stage_features = self.build_first_stage_features()
v0.7,fs_model = self.first_stage_model()
v0.7,"if self._target_estimand.identifier_method==""frontdoor"":"
v0.7,first_stage_outcome = self._frontdoor_variables
v0.7,"elif self._target_estimand.identifier_method==""mediation"":"
v0.7,first_stage_outcome = self._mediators
v0.7,"fs_model.fit(first_stage_features, self._frontdoor_variables)"
v0.7,"self.logger.debug(""Coefficients of the fitted model: "" +"
v0.7,""","".join(map(str, fs_model.coef_)))"
v0.7,residuals = self._frontdoor_variables - fs_model.predict(first_stage_features)
v0.7,"self._data[""residual""] = residuals"
v0.7,First stage
v0.7,Second Stage
v0.7,Combining the two estimates
v0.7,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.7,Total  effect of treatment
v0.7,Bulding the feature matrix
v0.7,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.7,choosing the instrumental variable to use
v0.7,TODO move this to the identification step
v0.7,Obtain estimate by Wald Estimator
v0.7,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.7,More than 1 instrument. Use 2sls.
v0.7,Enable the user to pass params for a custom propensity model
v0.7,Check if the treatment is one-dimensional
v0.7,Checking if the treatment is binary
v0.7,Convert the categorical variables into dummy/indicator variables
v0.7,"Basically, this gives a one hot encoding for each category"
v0.7,The first category is taken to be the base line.
v0.7,TODO make treatment_value and control value also as local parameters
v0.7,Checking if the model is already trained
v0.7,The model is always built on the entire data
v0.7,All treatments are set to the same constant value
v0.7,Using all data by default
v0.7,"Fixing treatment value to the specified value, if provided"
v0.7,treatment_vals and data_df should have same number of rows
v0.7,Bulding the feature matrix
v0.7,The model is always built on the entire data
v0.7,Replacing treatment values by given x
v0.7,The average treatment effect is a combination of different
v0.7,regression coefficients. Complicated to compute the confidence
v0.7,"interval analytically. For example, if y=a + b1.t + b2.tx, then"
v0.7,the average treatment effect is b1+b2.mean(x).
v0.7,"Refer Gelman, Hill. ARM Book. Chapter 9"
v0.7,http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
v0.7,TODO: Looking for contributions
v0.7,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.7,"variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0."
v0.7,"So for custom treatment and control values, we must multiply the confidence interval by the difference of the two."
v0.7,"For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the"
v0.7,"variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0."
v0.7,"So for custom treatment and control values, we must multiply the standard error by the difference of the two."
v0.7,check if the user provides the propensity score column
v0.7,Infer the right strata based on clipping threshold
v0.7,0.5 because there are two values for the treatment
v0.7,To be conservative and allow most strata to be included in the
v0.7,analysis
v0.7,At least 90% of the strata should be included in analysis
v0.7,sum weighted outcomes over all strata  (weight by treated population)
v0.7,TODO - how can we add additional information into the returned estimate?
v0.7,"such as how much clipping was done, or per-strata info for debugging?"
v0.7,sort the dataframe by propensity score
v0.7,create a column 'strata' for each element that marks what strata it belongs to
v0.7,"for each strata, count how many treated and control units there are"
v0.7,throw away strata that have insufficient treatment or control
v0.7,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.7,"'ips_weight', 'ips_normalized_weight', 'ips_stabilized_weight'"
v0.7,check if user provides the propensity score column
v0.7,trim propensity score weights
v0.7,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.7,nips ==> ips / (sum of ips over all units)
v0.7,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.7,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.7,Vanilla IPS estimator
v0.7,The Hajek estimator (or the self-normalized estimator)
v0.7,"Stabilized weights (from Robins, Hernan, Brumback (2000))"
v0.7,Paper: Marginal Structural Models and Causal Inference in Epidemiology
v0.7,Calculating the effect
v0.7,Subtracting the weighted means
v0.7,TODO - how can we add additional information into the returned estimate?
v0.7,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.7,For metalearners only--issue a warning if w contains variables not in x
v0.7,Override the effect_modifiers set in CausalEstimator.__init__()
v0.7,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.7,the latter can be used by other estimator methods later
v0.7,"Instrumental variables names, if present"
v0.7,choosing the instrumental variable to use
v0.7,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7,Calling the econml estimator's fit method
v0.7,"As of v0.9, econml has some kewyord only arguments"
v0.7,Changing shape to a list for a singleton value
v0.7,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.7,check if the user provides a propensity score column
v0.7,this assumes a binary treatment regime
v0.7,TODO remove neighbors that are more than a given radius apart
v0.7,estimate ATT on treated by summing over difference between matched neighbors
v0.7,Now computing ATC
v0.7,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.7,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7,Check if the treatment is one-dimensional
v0.7,Checking if the treatment is binary
v0.7,Setting the number of matches per data point
v0.7,Default distance metric if not provided by the user
v0.7,Convert the categorical variables into dummy/indicator variables
v0.7,"Basically, this gives a one hot encoding for each category"
v0.7,The first category is taken to be the base line.
v0.7,Dictionary of any user-provided params for the distance metric
v0.7,that will be passed to sklearn nearestneighbors
v0.7,this assumes a binary treatment regime
v0.7,TODO remove neighbors that are more than a given radius apart
v0.7,estimate ATT on treated by summing over difference between matched neighbors
v0.7,Return indices in the original dataframe
v0.7,self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()
v0.7,Now computing ATC
v0.7,Return indices in the original dataframe
v0.7,Add the identification method used in the estimator
v0.7,Check the backdoor variables being used
v0.7,Add the observed confounders and one hot encode the categorical variables
v0.7,Get the data of the unobserved confounders
v0.7,One hot encode the data if they are categorical
v0.7,Check the instrumental variables involved
v0.7,Perform the same actions as the above
v0.7,Check if effect modifiers are used
v0.7,Get the class corresponding the the estimator to be used
v0.7,Initialize the object
v0.7,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.7,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.7,For CATEs
v0.7,TODO we are conditioning on a postive treatment
v0.7,TODO create an expression corresponding to each estimator used
v0.7,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.7,Flipping some values
v0.7,Only consider edges have absolute edge weight > 0.01
v0.7,Modify graph such that it only contains bidirected edges
v0.7,Find c components by finding connected components on the undirected graph
v0.7,Understanding Neural Network weights
v0.7,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.7,add weight column
v0.7,before weights are applied we count number rows in each category
v0.7,which is equivalent to summing over weight=1
v0.7,after weights are applied we need to sum over the given weights
v0.7,"First, calculating mean differences by strata"
v0.7,"Second, without strata"
v0.7,"Third, concatenating them and plotting"
v0.7,Setting estimator attribute for convenience
v0.7,Outcome is numeric
v0.7,Treatments are also numeric or binary
v0.7,Outcome is categorical
v0.7,Treatments are numeric or binary
v0.7,TODO: A common way to show all plots
v0.7,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7,self._identified_estimand = self._causal_model.identify_effect()
v0.7,"self._identified_estimand,"
v0.7,"self._causal_model._treatment,"
v0.7,"self._causal_model._outcome,"
v0.7,If labels provided
v0.7,Return in valid DOT format
v0.7,Get adjacency matrix
v0.7,If labels not provided
v0.7,Obtain valid DOT format
v0.7,If labels provided
v0.7,Return in valid DOT format
v0.7,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.7,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.6,Get the long description from the README file
v0.6,Get the required packages
v0.6,Plotting packages are optional to install
v0.6,Loading version number
v0.6,-*- coding: utf-8 -*-
v0.6,
v0.6,Configuration file for the Sphinx documentation builder.
v0.6,
v0.6,This file does only contain a selection of the most common options. For a
v0.6,full list see the documentation:
v0.6,http://www.sphinx-doc.org/en/stable/config
v0.6,-- Path setup --------------------------------------------------------------
v0.6,"If extensions (or modules to document with autodoc) are in another directory,"
v0.6,add these directories to sys.path here. If the directory is relative to the
v0.6,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.6,
v0.6,-- Project information -----------------------------------------------------
v0.6,The short X.Y version
v0.6,"The full version, including alpha/beta/rc tags"
v0.6,-- General configuration ---------------------------------------------------
v0.6,"If your documentation needs a minimal Sphinx version, state it here."
v0.6,
v0.6,needs_sphinx = '1.0'
v0.6,"Add any Sphinx extension module names here, as strings. They can be"
v0.6,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.6,ones.
v0.6,"Add any paths that contain templates here, relative to this directory."
v0.6,The suffix(es) of source filenames.
v0.6,You can specify multiple suffix as a list of string:
v0.6,
v0.6,"source_suffix = ['.rst', '.md']"
v0.6,The master toctree document.
v0.6,The language for content autogenerated by Sphinx. Refer to documentation
v0.6,for a list of supported languages.
v0.6,
v0.6,This is also used if you do content translation via gettext catalogs.
v0.6,"Usually you set ""language"" from the command line for these cases."
v0.6,"List of patterns, relative to source directory, that match files and"
v0.6,directories to ignore when looking for source files.
v0.6,This pattern also affects html_static_path and html_extra_path .
v0.6,The name of the Pygments (syntax highlighting) style to use.
v0.6,-- Options for HTML output -------------------------------------------------
v0.6,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.6,a list of builtin themes.
v0.6,
v0.6,html_theme = 'sphinx-rtd-theme'
v0.6,on_rtd is whether we are on readthedocs.org
v0.6,only import and set the theme if we're building docs locally
v0.6,"otherwise, readthedocs.org uses their theme by default, so no need to specify it"
v0.6,Theme options are theme-specific and customize the look and feel of a theme
v0.6,"further.  For a list of options available for each theme, see the"
v0.6,documentation.
v0.6,
v0.6,html_theme_options = {}
v0.6,"Add any paths that contain custom static files (such as style sheets) here,"
v0.6,"relative to this directory. They are copied after the builtin static files,"
v0.6,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.6,"Custom sidebar templates, must be a dictionary that maps document names"
v0.6,to template names.
v0.6,
v0.6,The default sidebars (for documents that don't match any pattern) are
v0.6,defined by theme itself.  Builtin themes are using these templates by
v0.6,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.6,'searchbox.html']``.
v0.6,
v0.6,html_sidebars = {}
v0.6,-- Options for HTMLHelp output ---------------------------------------------
v0.6,Output file base name for HTML help builder.
v0.6,-- Options for LaTeX output ------------------------------------------------
v0.6,The paper size ('letterpaper' or 'a4paper').
v0.6,
v0.6,"'papersize': 'letterpaper',"
v0.6,"The font size ('10pt', '11pt' or '12pt')."
v0.6,
v0.6,"'pointsize': '10pt',"
v0.6,Additional stuff for the LaTeX preamble.
v0.6,
v0.6,"'preamble': '',"
v0.6,Latex figure (float) alignment
v0.6,
v0.6,"'figure_align': 'htbp',"
v0.6,Grouping the document tree into LaTeX files. List of tuples
v0.6,"(source start file, target name, title,"
v0.6,"author, documentclass [howto, manual, or own class])."
v0.6,-- Options for manual page output ------------------------------------------
v0.6,One entry per manual page. List of tuples
v0.6,"(source start file, name, description, authors, manual section)."
v0.6,-- Options for Texinfo output ----------------------------------------------
v0.6,Grouping the document tree into Texinfo files. List of tuples
v0.6,"(source start file, target name, title, author,"
v0.6,"dir menu entry, description, category)"
v0.6,-- Options for Epub output -------------------------------------------------
v0.6,Bibliographic Dublin Core info.
v0.6,The unique identifier of the text. This can be a ISBN number
v0.6,or the project homepage.
v0.6,
v0.6,epub_identifier = ''
v0.6,A unique identification for the text.
v0.6,
v0.6,epub_uid = ''
v0.6,A list of files that should not be packed into the epub file.
v0.6,-- Extension configuration -------------------------------------------------
v0.6,-- Options for todo extension ----------------------------------------------
v0.6,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.6,requires stdin input for identify in weighting sampler
v0.6,requires Rpy2 for lalonde
v0.6,very slow
v0.6,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.6,can import dowhy
v0.6,"""--ExecutePreprocessor.timeout=600"","
v0.6,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.6,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.6,treated = self.df[self.df['z']==1]
v0.6,self.att = np.mean(treated['y1'] - treated['y0'])
v0.6,def test_average_treatment_effect(self):
v0.6,est_ate = 1
v0.6,bias = est_ate - self.ate
v0.6,print(bias)
v0.6,"self.assertAlmostEqual(self.ate, est_ate)"
v0.6,def test_average_treatment_effect_on_treated(self):
v0.6,est_att = 1
v0.6,self.att=1
v0.6,bias = est_att - self.att
v0.6,print(bias)
v0.6,"self.assertAlmostEqual(self.att, est_att)"
v0.6,removing two common causes
v0.6,The outcome is a linear function of the confounder
v0.6,"The slope is 1,2 and the intercept is 3"
v0.6,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.6,Supports user-provided dataset object
v0.6,To test if there are any exceptions
v0.6,To test if the estimate is identical if refutation parameters are zero
v0.6,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.6,"Ordinarily, we should expect this value to be zero."
v0.6,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.6,"Ordinarily, we should expect this value to be zero."
v0.6,cov_mat = np.diag(np.ones(num_features))
v0.6,Setup data
v0.6,Test LinearDML
v0.6,Test ContinuousTreatmentOrthoForest
v0.6,Test LinearDRLearner
v0.6,Setup data
v0.6,Test DeepIV
v0.6,"Treatment model,"
v0.6,Response model
v0.6,Test IntentToTreatDRIV
v0.6,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.6,More cases where Exception  is expected
v0.6,"Compute confidence intervals, standard error and significance tests"
v0.6,Defined a linear dataset with a given set of properties
v0.6,Create a model that captures the same
v0.6,Identify the effects within the model
v0.6,Defined a linear dataset with a given set of properties
v0.6,Create a model that captures the same
v0.6,Identify the effects within the model
v0.6,Defined a linear dataset with a given set of properties
v0.6,Create a model that captures the same
v0.6,Identify the effects within the model
v0.6,Defined a linear dataset with a given set of properties
v0.6,Create a model that captures the same
v0.6,Identify the effects within the model
v0.6,Defined a linear dataset with a given set of properties
v0.6,Create a model that captures the same
v0.6,Identify the effects within the model
v0.6,## 1. BACKDOOR IDENTIFICATION
v0.6,"First, checking if there are any valid backdoor adjustment sets"
v0.6,"Setting default ""backdoor"" identification adjustment set"
v0.6,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.6,Now checking if there is also a valid iv estimand
v0.6,## 3. FRONTDOOR IDENTIFICATION
v0.6,Now checking if there is a valid frontdoor variable
v0.6,Finally returning the estimand object
v0.6,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.6,"First, checking if there are any valid backdoor adjustment sets"
v0.6,"Setting default ""backdoor"" identification adjustment set"
v0.6,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.6,Now checking if there are valid mediator variables
v0.6,Finally returning the estimand object
v0.6,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.6,"First, checking if there are any valid backdoor adjustment sets"
v0.6,"Setting default ""backdoor"" identification adjustment set"
v0.6,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.6,Now checking if there are valid mediator variables
v0.6,Finally returning the estimand object
v0.6,"First, checking if empty set is a valid backdoor set"
v0.6,"Second, checking for all other sets of variables"
v0.6,causes_t = self._graph.get_causes(self.treatment_name)
v0.6,"causes_y = self._graph.get_causes(self.outcome_name, remove_edges={'sources':self.treatment_name, 'targets':self.outcome_name})"
v0.6,common_causes = list(causes_t.intersection(causes_y))
v0.6,"self.logger.info(""Common causes of treatment and outcome:"" + str(common_causes))"
v0.6,Adding a None estimand if no backdoor set found
v0.6,Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)
v0.6,"For simplicity, assuming a one-variable frontdoor set"
v0.6,"For simplicity, assuming a one-variable mediation set"
v0.6,"Create estimands dict as per the API for backdoor, but do not return it"
v0.6,"Setting default ""backdoor"" identification adjustment set"
v0.6,"Create estimands dict as per the API for backdoor, but do not return it"
v0.6,"Setting default ""backdoor"" identification adjustment set"
v0.6,Adding a None estimand if no backdoor set found
v0.6,Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)
v0.6,"TODO: outputs string for now, but ideally should do symbolic"
v0.6,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.6,TODO Better support for multivariate treatments
v0.6,TODO: support multivariate treatments better.
v0.6,TODO: support multivariate treatments better.
v0.6,TODO: support multivariate treatments better.
v0.6,For direct effect
v0.6,Do not show backdoor key unless it is the only backdoor set.
v0.6,Just show the default backdoor set
v0.6,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.6,Unpacking the keyword arguments
v0.6,Default value for the number of simulations to be conducted
v0.6,"Concatenate the confounders, instruments and effect modifiers"
v0.6,Shuffle the confounders
v0.6,Check if all are select or deselect variables
v0.6,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.6,Initializing the p_value
v0.6,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.6,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.6,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.6,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.6,Get the number of simulations
v0.6,Sort the simulations
v0.6,Obtain the median value
v0.6,Performing a two sided test
v0.6,np.searchsorted tells us the index if it were a part of the array
v0.6,We select side to be left as we want to find the first value that matches
v0.6,We subtact 1 as we are finding the value from the right tail
v0.6,We take the side to be right as we want to find the last index that matches
v0.6,We get the probability with respect to the left tail.
v0.6,Get the mean for the simulations
v0.6,Get the standard deviation for the simulations
v0.6,Get the Z Score [(val - mean)/ std_dev ]
v0.6,load dot file
v0.6,Adding node attributes
v0.6,adding penwidth to make the edge bold
v0.6,Adding common causes
v0.6,Adding instruments
v0.6,Adding effect modifiers
v0.6,Adding columns in the dataframe as confounders that were not in the graph
v0.6,Adding unobserved confounders
v0.6,also return the number of backdoor paths blocked by observed nodes
v0.6,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.6,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.6,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.6,removing all mediators
v0.6,[TODO: double check these work with multivariate implementation:]
v0.6,Exclusion
v0.6,As-if-random setup
v0.6,As-if-random
v0.6,convert the outputted generator into a list
v0.6,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.6,(Because some effect modifiers may also be common causes)
v0.6,"In such cases, the user-provided modifiers are used."
v0.6,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.6,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.6,TODO add dowhy as a prefix to all dowhy estimators
v0.6,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.6,Define the third-party estimation method to be used
v0.6,Process the dowhy estimators
v0.6,Check if estimator's target estimand is identified
v0.6,Store parameters inside estimate object for refutation methods
v0.6,TODO: This add_params needs to move to the estimator class
v0.6,inside estimate_effect and estimate_conditional_effect
v0.6,Check if estimator's target estimand is identified
v0.6,The default number of simulations for statistical testing
v0.6,The default number of simulations to obtain confidence intervals
v0.6,The portion of the total size that should be taken each time to find the confidence intervals
v0.6,1 is the recommended value
v0.6,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.6,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.6,The default Confidence Level
v0.6,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.6,Prefix to add to temporary categorical variables created after discretization
v0.6,Currently estimation methods only support univariate treatment and outcome
v0.6,Setting the default interpret method
v0.6,Unpacking the keyword arguments
v0.6,"Checking if some parameters were set, otherwise setting to default values"
v0.6,Estimate conditional estimates by default
v0.6,Setting more values
v0.6,Now saving the effect modifiers
v0.6,TODO Only works for binary treatment
v0.6,Defaulting to class default values if parameters are not provided
v0.6,Checking that there is at least one effect modifier
v0.6,Making sure that effect_modifier_names is a list
v0.6,Making a copy since we are going to be changing effect modifier names
v0.6,"For every numeric effect modifier, adding a temp categorical column"
v0.6,Grouping by effect modifiers and computing effect separately
v0.6,Deleting the temporary categorical columns
v0.6,The array that stores the results of all estimations
v0.6,Find the sample size the proportion with the population size
v0.6,Perform the set number of simulations
v0.6,Using class default parameters if not specified
v0.6,Checking if bootstrap_estimates are already computed
v0.6,Checked if any parameter is changed from the previous std error estimate
v0.6,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.6,Sort the simulations
v0.6,"Now we take the (1- p)th and the (p)th values, where p is the chosen confidence level"
v0.6,get the values
v0.6,"Use existing params, if new user defined params are not present"
v0.6,Checking if bootstrap_estimates are already computed
v0.6,Check if any parameter is changed from the previous std error estimate
v0.6,"Use existing params, if new user defined params are not present"
v0.6,"self._outcome = self._data[""dummy_outcome""]"
v0.6,Processing the null hypothesis estimates
v0.6,Doing a two-sided test
v0.6,Being conservative with the p-value reported
v0.6,Being conservative with the p-value reported
v0.6,"If the estimate_index is 0, it depends on the number of simulations"
v0.6,Need to test r-squared before supporting
v0.6,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.6,'r-squared': effect_r_squared
v0.6,"elif method == ""r-squared"":"
v0.6,outcome_mean = np.mean(self._outcome)
v0.6,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.6,Assuming a linear model with one variable: the treatment
v0.6,Currently only works for continuous y
v0.6,causal_model = outcome_mean + estimate.value*self._treatment
v0.6,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.6,r_squared = 1 - (squared_residual/total_variance)
v0.6,return r_squared
v0.6,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.6,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.6,one-hot encode discrete W
v0.6,Now deleting the old continuous value
v0.6,Making beta an array
v0.6,TODO Ensure that we do not generate weak instruments
v0.6,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.6,Converting treatment to binary if required
v0.6,Generating frontdoor variables if asked for
v0.6,Computing ATE
v0.6,constructing column names for one-hot encoded discrete features
v0.6,Specifying the correct dtypes
v0.6,Now specifying the corresponding graph strings
v0.6,Now writing the gml graph
v0.6,Making beta an array
v0.6,creating data frame
v0.6,Specifying the correct dtypes
v0.6,Now specifying the corresponding graph strings
v0.6,Now writing the gml graph
v0.6,Adding edges between common causes and the frontdoor mediator
v0.6,Error terms
v0.6,else:
v0.6,V = 6 + W0 + tterm + E1
v0.6,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.6,Loading version number
v0.6,The currently supported estimators
v0.6,The default standard deviation for noise
v0.6,The default scaling factor to determine the bucket size
v0.6,The minimum number of points for the estimator to run
v0.6,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.6,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.6,The Default split for the number of data points that fall into the training and validation sets
v0.6,Assuming that outcome is one-dimensional
v0.6,We need to change the identified estimand
v0.6,"We thus, make a copy. This is done as we don't want"
v0.6,to change the original DataFrame
v0.6,We use collections.OrderedDict to maintain the order in which the data is stored
v0.6,Check if we are using an estimator in the transformation list
v0.6,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.6,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.6,"loops. Thus, we can get different values everytime we get the estimator."
v0.6,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.6,Adding an unobserved confounder if provided by the user
v0.6,We set X_train = 0 and outcome_train to be 0
v0.6,"Get the final outcome, after running through all the values in the transformation list"
v0.6,Check if the value of true effect has been already stored
v0.6,We use None as the key as we have no base category for this refutation
v0.6,As we currently support only one treatment
v0.6,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.6,Check if the value of true effect has been already stored
v0.6,This ensures that we calculate the causal effect only once.
v0.6,We use key_train as we map data with respect to the base category of the data
v0.6,As we currently support only one treatment
v0.6,Add h(t) to f(W) to get the dummy outcome
v0.6,We convert to ndarray for ease in indexing
v0.6,The data is of the form
v0.6,sim1: cat1 cat2 ... catn
v0.6,sim2: cat1 cat2 ... catn
v0.6,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.6,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.6,distribution of the refuter.
v0.6,True Causal Effect list
v0.6,Iterating through the refutation for each category
v0.6,We use string arguments to account for both 32 and 64 bit varaibles
v0.6,action for continuous variables
v0.6,Action for categorical variables
v0.6,Find the set difference for each row
v0.6,Choose one out of the remaining
v0.6,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.6,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.6,of the treatment to affect the outcome
v0.6,Get a 2D matrix of values
v0.6,Store the values into the refute object
v0.6,Obtaining the list of observed variables
v0.6,Taking a subset of the dataframe that has only observed variables
v0.6,Residuals from the outcome model obtained by fitting a linear model
v0.6,Residuals from the treatment model obtained by fitting a linear model
v0.6,Initialising product_cor_metric_observed with a really low value as finding maximum
v0.6,The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.
v0.6,Choosing a c_star based on the data.
v0.6,"The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus."
v0.6,Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which
v0.6,which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables
v0.6,and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval
v0.6,c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star
v0.6,initialising min_distance_between_product_cor_metrics to be a value greater than 1
v0.6,Default value of the p value taken for the distribution
v0.6,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.6,Mean of the Normal Distribution
v0.6,Standard Deviation of the Normal Distribution
v0.6,only permute is supported for iv methods
v0.6,We need to change the identified estimand
v0.6,"We make a copy as a safety measure, we don't want to change the"
v0.6,original DataFrame
v0.6,"For IV methods, the estimating_instrument_names should also be"
v0.6,changed. So we change it inside the estimate and then restore it
v0.6,back at the end of this method.
v0.6,Create a new column in the data by the name of placebo
v0.6,Sanity check the data
v0.6,Restoring the value of iv_instrument_name
v0.6,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.6,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.6,relationship between the treatment and the outcome.
v0.6,Adding a new backdoor variable to the identified estimand
v0.6,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.6,The default subset of the data to be used
v0.6,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.6,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.6,of the treatment to affect the outcome
v0.6,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.6,Check if the treatment is one-dimensional
v0.6,first_stage_features = self.build_first_stage_features()
v0.6,fs_model = self.first_stage_model()
v0.6,"if self._target_estimand.identifier_method==""frontdoor"":"
v0.6,first_stage_outcome = self._frontdoor_variables
v0.6,"elif self._target_estimand.identifier_method==""mediation"":"
v0.6,first_stage_outcome = self._mediators
v0.6,"fs_model.fit(first_stage_features, self._frontdoor_variables)"
v0.6,"self.logger.debug(""Coefficients of the fitted model: "" +"
v0.6,""","".join(map(str, fs_model.coef_)))"
v0.6,residuals = self._frontdoor_variables - fs_model.predict(first_stage_features)
v0.6,"self._data[""residual""] = residuals"
v0.6,First stage
v0.6,Second Stage
v0.6,Combining the two estimates
v0.6,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.6,Total  effect of treatment
v0.6,Bulding the feature matrix
v0.6,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.6,choosing the instrumental variable to use
v0.6,TODO move this to the identification step
v0.6,Obtain estimate by Wald Estimator
v0.6,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.6,More than 1 instrument. Use 2sls.
v0.6,We need to initialize the model when we create any propensity score estimator
v0.6,Check if the treatment is one-dimensional
v0.6,Checking if the treatment is binary
v0.6,Convert the categorical variables into dummy/indicator variables
v0.6,"Basically, this gives a one hot encoding for each category"
v0.6,The first category is taken to be the base line.
v0.6,TODO make treatment_value and control value also as local parameters
v0.6,Checking if the model is already trained
v0.6,The model is always built on the entire data
v0.6,All treatments are set to the same constant value
v0.6,Using all data by default
v0.6,"Fixing treatment value to the specified value, if provided"
v0.6,treatment_vals and data_df should have same number of rows
v0.6,Bulding the feature matrix
v0.6,The model is always built on the entire data
v0.6,Replacing treatment values by given x
v0.6,sort the dataframe by propensity score
v0.6,create a column 'strata' for each element that marks what strata it belongs to
v0.6,"for each strata, count how many treated and control units there are"
v0.6,throw away strata that have insufficient treatment or control
v0.6,"print(""before clipping, here is the distribution of treatment and control per strata"")"
v0.6,"print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())"
v0.6,sum weighted outcomes over all strata  (weight by treated population)
v0.6,TODO - how can we add additional information into the returned estimate?
v0.6,"such as how much clipping was done, or per-strata info for debugging?"
v0.6,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.6,trim propensity score weights
v0.6,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.6,nips ==> ips / (sum of ips over all units)
v0.6,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.6,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.6,Vanilla IPS estimator
v0.6,Also known as the Hajek estimator
v0.6,Stabilized weights
v0.6,Simple normalized estimator (commented out for now)
v0.6,ips_sum = self._data['ips_weight'].sum()
v0.6,self._data['nips_weight'] = self._data['ips_weight'] / ips_sum
v0.6,self._data['ips2'] = self._data['ps'] / (1 - self._data['ps'])
v0.6,treated_ips_sum = (self._data['ips2'] * self._data[self._treatment_name[0]]).sum()
v0.6,control_ips_sum = (self._data['ips2'] * (1 - self._data[self._treatment_name[0]])).sum()
v0.6,self._data['itps_weight'] = self._data['ips2'] / treated_ips_sum
v0.6,self._data['icps_weight'] = self._data['ips2'] / control_ips_sum
v0.6,Calculating the effect
v0.6,TODO - how can we add additional information into the returned estimate?
v0.6,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.6,For metalearners only--issue a warning if w contains variables not in x
v0.6,Override the effect_modifiers set in CausalEstimator.__init__()
v0.6,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.6,the latter can be used by other estimator methods later
v0.6,"Instrumental variables names, if present"
v0.6,choosing the instrumental variable to use
v0.6,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.6,Calling the econml estimator's fit method
v0.6,"As of v0.9, econml has some kewyord only arguments"
v0.6,Changing shape to a list for a singleton value
v0.6,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.6,this assumes a binary treatment regime
v0.6,TODO remove neighbors that are more than a given radius apart
v0.6,estimate ATT on treated by summing over difference between matched neighbors
v0.6,Now computing ATC
v0.6,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.6,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.6,Add the identification method used in the estimator
v0.6,Check the backdoor variables being used
v0.6,Add the observed confounders and one hot encode the categorical variables
v0.6,Get the data of the unobserved confounders
v0.6,One hot encode the data if they are categorical
v0.6,Check the instrumental variables involved
v0.6,Perform the same actions as the above
v0.6,Check if effect modifiers are used
v0.6,Get the class corresponding the the estimator to be used
v0.6,Initialize the object
v0.6,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.6,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.6,For CATEs
v0.6,TODO we are conditioning on a postive treatment
v0.6,TODO create an expression corresponding to each estimator used
v0.6,Generating data with equal 0 and 1 (since ranks are uniformly distributed)
v0.6,Flipping some values
v0.6,Understanding Neural Network weights
v0.6,Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights
v0.6,add weight column
v0.6,before weights are applied we count number rows in each category
v0.6,which is equivalent to summing over weight=1
v0.6,after weights are applied we need to sum over the given weights
v0.6,"First, calculating mean differences by strata"
v0.6,"Second, without strata"
v0.6,"Third, concatenating them and plotting"
v0.6,Setting estimator attribute for convenience
v0.6,Outcome is numeric
v0.6,Treatments are also numeric or binary
v0.6,Outcome is categorical
v0.6,Treatments are numeric or binary
v0.6,TODO: A common way to show all plots
v0.6,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.6,self._identified_estimand = self._causal_model.identify_effect()
v0.6,"self._identified_estimand,"
v0.6,"self._causal_model._treatment,"
v0.6,"self._causal_model._outcome,"
v0.5.1,Get the long description from the README file
v0.5.1,Get the required packages
v0.5.1,Loading version number
v0.5.1,-*- coding: utf-8 -*-
v0.5.1,
v0.5.1,Configuration file for the Sphinx documentation builder.
v0.5.1,
v0.5.1,This file does only contain a selection of the most common options. For a
v0.5.1,full list see the documentation:
v0.5.1,http://www.sphinx-doc.org/en/stable/config
v0.5.1,-- Path setup --------------------------------------------------------------
v0.5.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.5.1,add these directories to sys.path here. If the directory is relative to the
v0.5.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.5.1,
v0.5.1,-- Project information -----------------------------------------------------
v0.5.1,The short X.Y version
v0.5.1,"The full version, including alpha/beta/rc tags"
v0.5.1,-- General configuration ---------------------------------------------------
v0.5.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.5.1,
v0.5.1,needs_sphinx = '1.0'
v0.5.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.5.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.5.1,ones.
v0.5.1,"Add any paths that contain templates here, relative to this directory."
v0.5.1,The suffix(es) of source filenames.
v0.5.1,You can specify multiple suffix as a list of string:
v0.5.1,
v0.5.1,"source_suffix = ['.rst', '.md']"
v0.5.1,The master toctree document.
v0.5.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.5.1,for a list of supported languages.
v0.5.1,
v0.5.1,This is also used if you do content translation via gettext catalogs.
v0.5.1,"Usually you set ""language"" from the command line for these cases."
v0.5.1,"List of patterns, relative to source directory, that match files and"
v0.5.1,directories to ignore when looking for source files.
v0.5.1,This pattern also affects html_static_path and html_extra_path .
v0.5.1,The name of the Pygments (syntax highlighting) style to use.
v0.5.1,-- Options for HTML output -------------------------------------------------
v0.5.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.5.1,a list of builtin themes.
v0.5.1,
v0.5.1,html_theme = 'sphinx-rtd-theme'
v0.5.1,on_rtd is whether we are on readthedocs.org
v0.5.1,only import and set the theme if we're building docs locally
v0.5.1,"otherwise, readthedocs.org uses their theme by default, so no need to specify it"
v0.5.1,Theme options are theme-specific and customize the look and feel of a theme
v0.5.1,"further.  For a list of options available for each theme, see the"
v0.5.1,documentation.
v0.5.1,
v0.5.1,html_theme_options = {}
v0.5.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.5.1,"relative to this directory. They are copied after the builtin static files,"
v0.5.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.5.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.5.1,to template names.
v0.5.1,
v0.5.1,The default sidebars (for documents that don't match any pattern) are
v0.5.1,defined by theme itself.  Builtin themes are using these templates by
v0.5.1,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.5.1,'searchbox.html']``.
v0.5.1,
v0.5.1,html_sidebars = {}
v0.5.1,-- Options for HTMLHelp output ---------------------------------------------
v0.5.1,Output file base name for HTML help builder.
v0.5.1,-- Options for LaTeX output ------------------------------------------------
v0.5.1,The paper size ('letterpaper' or 'a4paper').
v0.5.1,
v0.5.1,"'papersize': 'letterpaper',"
v0.5.1,"The font size ('10pt', '11pt' or '12pt')."
v0.5.1,
v0.5.1,"'pointsize': '10pt',"
v0.5.1,Additional stuff for the LaTeX preamble.
v0.5.1,
v0.5.1,"'preamble': '',"
v0.5.1,Latex figure (float) alignment
v0.5.1,
v0.5.1,"'figure_align': 'htbp',"
v0.5.1,Grouping the document tree into LaTeX files. List of tuples
v0.5.1,"(source start file, target name, title,"
v0.5.1,"author, documentclass [howto, manual, or own class])."
v0.5.1,-- Options for manual page output ------------------------------------------
v0.5.1,One entry per manual page. List of tuples
v0.5.1,"(source start file, name, description, authors, manual section)."
v0.5.1,-- Options for Texinfo output ----------------------------------------------
v0.5.1,Grouping the document tree into Texinfo files. List of tuples
v0.5.1,"(source start file, target name, title, author,"
v0.5.1,"dir menu entry, description, category)"
v0.5.1,-- Options for Epub output -------------------------------------------------
v0.5.1,Bibliographic Dublin Core info.
v0.5.1,The unique identifier of the text. This can be a ISBN number
v0.5.1,or the project homepage.
v0.5.1,
v0.5.1,epub_identifier = ''
v0.5.1,A unique identification for the text.
v0.5.1,
v0.5.1,epub_uid = ''
v0.5.1,A list of files that should not be packed into the epub file.
v0.5.1,-- Extension configuration -------------------------------------------------
v0.5.1,-- Options for todo extension ----------------------------------------------
v0.5.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.5.1,requires stdin input for identify in weighting sampler
v0.5.1,requires Rpy2 for lalonde
v0.5.1,very slow
v0.5.1,Adding the dowhy root folder to the python path so that jupyter notebooks
v0.5.1,can import dowhy
v0.5.1,"""--ExecutePreprocessor.timeout=600"","
v0.5.1,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.5.1,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.5.1,treated = self.df[self.df['z']==1]
v0.5.1,self.att = np.mean(treated['y1'] - treated['y0'])
v0.5.1,def test_average_treatment_effect(self):
v0.5.1,est_ate = 1
v0.5.1,bias = est_ate - self.ate
v0.5.1,print(bias)
v0.5.1,"self.assertAlmostEqual(self.ate, est_ate)"
v0.5.1,def test_average_treatment_effect_on_treated(self):
v0.5.1,est_att = 1
v0.5.1,self.att=1
v0.5.1,bias = est_att - self.att
v0.5.1,print(bias)
v0.5.1,"self.assertAlmostEqual(self.att, est_att)"
v0.5.1,removing two common causes
v0.5.1,The outcome is a linear function of the confounder
v0.5.1,"The slope is 1,2 and the intercept is 3"
v0.5.1,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.5.1,Supports user-provided dataset object
v0.5.1,To test if there are any exceptions
v0.5.1,To test if the estimate is identical if refutation parameters are zero
v0.5.1,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.5.1,"Ordinarily, we should expect this value to be zero."
v0.5.1,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.5.1,"Ordinarily, we should expect this value to be zero."
v0.5.1,cov_mat = np.diag(np.ones(num_features))
v0.5.1,Setup data
v0.5.1,Test LinearDMLCateEstimator
v0.5.1,Test ContinuousTreatmentOrthoForest
v0.5.1,Test LinearDRLearner
v0.5.1,Setup data
v0.5.1,Test DeepIV
v0.5.1,TODO: Test IntentToTreatDRIV when EconML v0.7 comes out
v0.5.1,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.5.1,More cases where Exception  is expected
v0.5.1,"Compute confidence intervals, standard error and significance tests"
v0.5.1,Defined a linear dataset with a given set of properties
v0.5.1,Create a model that captures the same
v0.5.1,Identify the effects within the model
v0.5.1,Defined a linear dataset with a given set of properties
v0.5.1,Create a model that captures the same
v0.5.1,Identify the effects within the model
v0.5.1,Defined a linear dataset with a given set of properties
v0.5.1,Create a model that captures the same
v0.5.1,Identify the effects within the model
v0.5.1,Defined a linear dataset with a given set of properties
v0.5.1,Create a model that captures the same
v0.5.1,Identify the effects within the model
v0.5.1,Defined a linear dataset with a given set of properties
v0.5.1,Create a model that captures the same
v0.5.1,Identify the effects within the model
v0.5.1,## 1. BACKDOOR IDENTIFICATION
v0.5.1,"First, checking if there are any valid backdoor adjustment sets"
v0.5.1,"Setting default ""backdoor"" identification adjustment set"
v0.5.1,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.5.1,Now checking if there is also a valid iv estimand
v0.5.1,## 3. FRONTDOOR IDENTIFICATION
v0.5.1,Now checking if there is a valid frontdoor variable
v0.5.1,Finally returning the estimand object
v0.5.1,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.5.1,"First, checking if there are any valid backdoor adjustment sets"
v0.5.1,"Setting default ""backdoor"" identification adjustment set"
v0.5.1,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.5.1,Now checking if there are valid mediator variables
v0.5.1,Finally returning the estimand object
v0.5.1,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.5.1,"First, checking if there are any valid backdoor adjustment sets"
v0.5.1,"Setting default ""backdoor"" identification adjustment set"
v0.5.1,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.5.1,Now checking if there are valid mediator variables
v0.5.1,Finally returning the estimand object
v0.5.1,"First, checking if empty set is a valid backdoor set"
v0.5.1,"Second, checking for all other sets of variables"
v0.5.1,causes_t = self._graph.get_causes(self.treatment_name)
v0.5.1,"causes_y = self._graph.get_causes(self.outcome_name, remove_edges={'sources':self.treatment_name, 'targets':self.outcome_name})"
v0.5.1,common_causes = list(causes_t.intersection(causes_y))
v0.5.1,"self.logger.info(""Common causes of treatment and outcome:"" + str(common_causes))"
v0.5.1,Adding a None estimand if no backdoor set found
v0.5.1,Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)
v0.5.1,"For simplicity, assuming a one-variable frontdoor set"
v0.5.1,"For simplicity, assuming a one-variable mediation set"
v0.5.1,"Create estimands dict as per the API for backdoor, but do not return it"
v0.5.1,"Setting default ""backdoor"" identification adjustment set"
v0.5.1,"Create estimands dict as per the API for backdoor, but do not return it"
v0.5.1,"Setting default ""backdoor"" identification adjustment set"
v0.5.1,Adding a None estimand if no backdoor set found
v0.5.1,Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)
v0.5.1,"TODO: outputs string for now, but ideally should do symbolic"
v0.5.1,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.5.1,TODO Better support for multivariate treatments
v0.5.1,TODO: support multivariate treatments better.
v0.5.1,TODO: support multivariate treatments better.
v0.5.1,TODO: support multivariate treatments better.
v0.5.1,For direct effect
v0.5.1,Do not show backdoor key unless it is the only backdoor set.
v0.5.1,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.5.1,Unpacking the keyword arguments
v0.5.1,Default value for the number of simulations to be conducted
v0.5.1,"Concatenate the confounders, instruments and effect modifiers"
v0.5.1,Shuffle the confounders
v0.5.1,Check if all are select or deselect variables
v0.5.1,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.5.1,Initializing the p_value
v0.5.1,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.5.1,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.5.1,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.5.1,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.5.1,Get the number of simulations
v0.5.1,Sort the simulations
v0.5.1,Obtain the median value
v0.5.1,Performing a two sided test
v0.5.1,np.searchsorted tells us the index if it were a part of the array
v0.5.1,We select side to be left as we want to find the first value that matches
v0.5.1,We subtact 1 as we are finding the value from the right tail
v0.5.1,We take the side to be right as we want to find the last index that matches
v0.5.1,We get the probability with respect to the left tail.
v0.5.1,Get the mean for the simulations
v0.5.1,Get the standard deviation for the simulations
v0.5.1,Get the Z Score [(val - mean)/ std_dev ]
v0.5.1,load dot file
v0.5.1,Adding node attributes
v0.5.1,TODO do not add it here. CausalIdentifier should call causal_graph to add an unobserved common cause if needed. This also ensures that we do not need get_common_causes in this class.
v0.5.1,adding penwidth to make the edge bold
v0.5.1,Adding common causes
v0.5.1,Adding instruments
v0.5.1,Adding effect modifiers
v0.5.1,Adding columns in the dataframe as confounders that were not in the graph
v0.5.1,Adding unobserved confounders
v0.5.1,also return the number of backdoor paths blocked by observed nodes
v0.5.1,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.5.1,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.5.1,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.5.1,removing all mediators
v0.5.1,[TODO: double check these work with multivariate implementation:]
v0.5.1,Exclusion
v0.5.1,As-if-random setup
v0.5.1,As-if-random
v0.5.1,TODO: move the logging level argument to a json file. Tue 20 Feb 2018 06:56:27 PM DST
v0.5.1,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.5.1,(Because some effect modifiers may also be common causes)
v0.5.1,"In such cases, the user-provided modifiers are used."
v0.5.1,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.5.1,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.5.1,TODO add dowhy as a prefix to all dowhy estimators
v0.5.1,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.5.1,Define the third-party estimation method to be used
v0.5.1,Process the dowhy estimators
v0.5.1,Check if estimator's target estimand is identified
v0.5.1,Store parameters inside estimate object for refutation methods
v0.5.1,Check if estimator's target estimand is identified
v0.5.1,The default number of simulations for statistical testing
v0.5.1,The default number of simulations to obtain confidence intervals
v0.5.1,The portion of the total size that should be taken each time to find the confidence intervals
v0.5.1,1 is the recommended value
v0.5.1,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.5.1,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.5.1,The default Confidence Level
v0.5.1,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.5.1,Prefix to add to temporary categorical variables created after discretization
v0.5.1,Currently estimation methods only support univariate treatment and outcome
v0.5.1,Setting the default interpret method
v0.5.1,Unpacking the keyword arguments
v0.5.1,"Checking if some parameters were set, otherwise setting to default values"
v0.5.1,Estimate conditional estimates by default
v0.5.1,Setting more values
v0.5.1,Now saving the effect modifiers
v0.5.1,TODO Only works for binary treatment
v0.5.1,Defaulting to class default values if parameters are not provided
v0.5.1,Checking that there is at least one effect modifier
v0.5.1,Making sure that effect_modifier_names is a list
v0.5.1,Making a copy since we are going to be changing effect modifier names
v0.5.1,"For every numeric effect modifier, adding a temp categorical column"
v0.5.1,Grouping by effect modifiers and computing effect separately
v0.5.1,Deleting the temporary categorical columns
v0.5.1,The array that stores the results of all estimations
v0.5.1,Find the sample size the proportion with the population size
v0.5.1,Perform the set number of simulations
v0.5.1,Using class default parameters if not specified
v0.5.1,Checking if bootstrap_estimates are already computed
v0.5.1,Checked if any parameter is changed from the previous std error estimate
v0.5.1,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.5.1,Sort the simulations
v0.5.1,"Now we take the (1- p)th and the (p)th values, where p is the chosen confidence level"
v0.5.1,get the values
v0.5.1,"Use existing params, if new user defined params are not present"
v0.5.1,Checking if bootstrap_estimates are already computed
v0.5.1,Check if any parameter is changed from the previous std error estimate
v0.5.1,"Use existing params, if new user defined params are not present"
v0.5.1,"self._outcome = self._data[""dummy_outcome""]"
v0.5.1,Processing the null hypothesis estimates
v0.5.1,Doing a two-sided test
v0.5.1,Being conservative with the p-value reported
v0.5.1,Being conservative with the p-value reported
v0.5.1,"If the estimate_index is 0, it depends on the number of simulations"
v0.5.1,Need to test r-squared before supporting
v0.5.1,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.5.1,'r-squared': effect_r_squared
v0.5.1,"elif method == ""r-squared"":"
v0.5.1,outcome_mean = np.mean(self._outcome)
v0.5.1,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.5.1,Assuming a linear model with one variable: the treatment
v0.5.1,Currently only works for continuous y
v0.5.1,causal_model = outcome_mean + estimate.value*self._treatment
v0.5.1,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.5.1,r_squared = 1 - (squared_residual/total_variance)
v0.5.1,return r_squared
v0.5.1,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.5.1,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.5.1,one-hot encode discrete W
v0.5.1,Now deleting the old continuous value
v0.5.1,Making beta an array
v0.5.1,TODO Ensure that we do not generate weak instruments
v0.5.1,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.5.1,Converting treatment to binary if required
v0.5.1,Generating frontdoor variables if asked for
v0.5.1,Computing ATE
v0.5.1,constructing column names for one-hot encoded discrete features
v0.5.1,Specifying the correct dtypes
v0.5.1,Now specifying the corresponding graph strings
v0.5.1,Now writing the gml graph
v0.5.1,Making beta an array
v0.5.1,creating data frame
v0.5.1,Specifying the correct dtypes
v0.5.1,Now specifying the corresponding graph strings
v0.5.1,Now writing the gml graph
v0.5.1,Adding edges between common causes and the frontdoor mediator
v0.5.1,Error terms
v0.5.1,else:
v0.5.1,V = 6 + W0 + tterm + E1
v0.5.1,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.5.1,Loading version number
v0.5.1,The currently supported estimators
v0.5.1,The default standard deviation for noise
v0.5.1,The default scaling factor to determine the bucket size
v0.5.1,The minimum number of points for the estimator to run
v0.5.1,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.5.1,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.5.1,The Default split for the number of data points that fall into the training and validation sets
v0.5.1,We need to change the identified estimand
v0.5.1,"We thus, make a copy. This is done as we don't want"
v0.5.1,to change the original DataFrame
v0.5.1,We use collections.OrderedDict to maintain the order in which the data is stored
v0.5.1,Check if we are using an estimator in the transformation list
v0.5.1,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.5.1,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.5.1,"loops. Thus, we can get different values everytime we get the estimator."
v0.5.1,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.5.1,We set X_train = 0 and outcome_train to be 0
v0.5.1,"Get the final outcome, after running through all the values in the transformation list"
v0.5.1,Check if the value of true effect has been already stored
v0.5.1,We use None as the key as we have no base category for this refutation
v0.5.1,As we currently support only one treatment
v0.5.1,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.5.1,Check if the value of true effect has been already stored
v0.5.1,This ensures that we calculate the causal effect only once.
v0.5.1,We use key_train as we map data with respect to the base category of the data
v0.5.1,As we currently support only one treatment
v0.5.1,Add h(t) to f(W) to get the dummy outcome
v0.5.1,We convert to ndarray for ease in indexing
v0.5.1,The data is of the form
v0.5.1,sim1: cat1 cat2 ... catn
v0.5.1,sim2: cat1 cat2 ... catn
v0.5.1,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.5.1,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.5.1,distribution of the refuter.
v0.5.1,True Causal Effect list
v0.5.1,Iterating through the refutation for each category
v0.5.1,We use string arguments to account for both 32 and 64 bit varaibles
v0.5.1,action for continuous variables
v0.5.1,Action for categorical variables
v0.5.1,Find the set difference for each row
v0.5.1,Choose one out of the remaining
v0.5.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.5.1,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.5.1,of the treatment to affect the outcome
v0.5.1,Get a 2D matrix of values
v0.5.1,Store the values into the refute object
v0.5.1,Default value of the p value taken for the distribution
v0.5.1,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.5.1,Mean of the Normal Distribution
v0.5.1,Standard Deviation of the Normal Distribution
v0.5.1,We need to change the identified estimand
v0.5.1,"We make a copy as a safety measure, we don't want to change the"
v0.5.1,original DataFrame
v0.5.1,Create a new column in the data by the name of placebo
v0.5.1,Sanity check the data
v0.5.1,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.5.1,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.5.1,relationship between the treatment and the outcome.
v0.5.1,Adding a new backdoor variable to the identified estimand
v0.5.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.5.1,The default subset of the data to be used
v0.5.1,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.5.1,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.5.1,of the treatment to affect the outcome
v0.5.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.5.1,Check if the treatment is one-dimensional
v0.5.1,first_stage_features = self.build_first_stage_features()
v0.5.1,fs_model = self.first_stage_model()
v0.5.1,"if self._target_estimand.identifier_method==""frontdoor"":"
v0.5.1,first_stage_outcome = self._frontdoor_variables
v0.5.1,"elif self._target_estimand.identifier_method==""mediation"":"
v0.5.1,first_stage_outcome = self._mediators
v0.5.1,"fs_model.fit(first_stage_features, self._frontdoor_variables)"
v0.5.1,"self.logger.debug(""Coefficients of the fitted model: "" +"
v0.5.1,""","".join(map(str, fs_model.coef_)))"
v0.5.1,residuals = self._frontdoor_variables - fs_model.predict(first_stage_features)
v0.5.1,"self._data[""residual""] = residuals"
v0.5.1,First stage
v0.5.1,Second Stage
v0.5.1,Combining the two estimates
v0.5.1,This same estimate is valid for frontdoor as well as mediation (NIE)
v0.5.1,Total  effect of treatment
v0.5.1,Bulding the feature matrix
v0.5.1,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.5.1,choosing the instrumental variable to use
v0.5.1,TODO move this to the identification step
v0.5.1,Obtain estimate by Wald Estimator
v0.5.1,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.5.1,More than 1 instrument. Use 2sls.
v0.5.1,We need to initialize the model when we create any propensity score estimator
v0.5.1,Check if the treatment is one-dimensional
v0.5.1,Checking if the treatment is binary
v0.5.1,Convert the categorical variables into dummy/indicator variables
v0.5.1,"Basically, this gives a one hot encoding for each category"
v0.5.1,The first category is taken to be the base line.
v0.5.1,TODO make treatment_value and control value also as local parameters
v0.5.1,Checking if the model is already trained
v0.5.1,The model is always built on the entire data
v0.5.1,All treatments are set to the same constant value
v0.5.1,Using all data by default
v0.5.1,"Fixing treatment value to the specified value, if provided"
v0.5.1,treatment_vals and data_df should have same number of rows
v0.5.1,Bulding the feature matrix
v0.5.1,The model is always built on the entire data
v0.5.1,Replacing treatment values by given x
v0.5.1,sort the dataframe by propensity score
v0.5.1,create a column 'strata' for each element that marks what strata it belongs to
v0.5.1,"for each strata, count how many treated and control units there are"
v0.5.1,throw away strata that have insufficient treatment or control
v0.5.1,"print(""before clipping, here is the distribution of treatment and control per strata"")"
v0.5.1,"print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())"
v0.5.1,sum weighted outcomes over all strata  (weight by treated population)
v0.5.1,TODO - how can we add additional information into the returned estimate?
v0.5.1,"such as how much clipping was done, or per-strata info for debugging?"
v0.5.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.5.1,trim propensity score weights
v0.5.1,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.5.1,nips ==> ips / (sum of ips over all units)
v0.5.1,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.5.1,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.5.1,Vanilla IPS estimator
v0.5.1,Also known as the Hajek estimator
v0.5.1,Stabilized weights
v0.5.1,Simple normalized estimator (commented out for now)
v0.5.1,ips_sum = self._data['ips_weight'].sum()
v0.5.1,self._data['nips_weight'] = self._data['ips_weight'] / ips_sum
v0.5.1,self._data['ips2'] = self._data['ps'] / (1 - self._data['ps'])
v0.5.1,treated_ips_sum = (self._data['ips2'] * self._data[self._treatment_name[0]]).sum()
v0.5.1,control_ips_sum = (self._data['ips2'] * (1 - self._data[self._treatment_name[0]])).sum()
v0.5.1,self._data['itps_weight'] = self._data['ips2'] / treated_ips_sum
v0.5.1,self._data['icps_weight'] = self._data['ips2'] / control_ips_sum
v0.5.1,Calculating the effect
v0.5.1,TODO - how can we add additional information into the returned estimate?
v0.5.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.5.1,Checking if effect modifiers are a subset of common causes
v0.5.1,For metalearners only--issue a warning if w contains variables not in x
v0.5.1,Override the effect_modifiers set in CausalEstimator.__init__()
v0.5.1,"Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names"
v0.5.1,the latter can be used by other estimator methods later
v0.5.1,"Instrumental variables names, if present"
v0.5.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.5.1,Calling the econml estimator's fit method
v0.5.1,Changing shape to a list for a singleton value
v0.5.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.5.1,this assumes a binary treatment regime
v0.5.1,TODO remove neighbors that are more than a given radius apart
v0.5.1,estimate ATT on treated by summing over difference between matched neighbors
v0.5.1,Now computing ATC
v0.5.1,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.5.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.5.1,Add the identification method used in the estimator
v0.5.1,Check the backdoor variables being used
v0.5.1,Add the observed confounders and one hot encode the categorical variables
v0.5.1,Get the data of the unobserved confounders
v0.5.1,One hot encode the data if they are categorical
v0.5.1,Check the instrumental variables involved
v0.5.1,Perform the same actions as the above
v0.5.1,Check if effect modifiers are used
v0.5.1,Get the class corresponding the the estimator to be used
v0.5.1,Initialize the object
v0.5.1,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.5.1,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.5.1,For CATEs
v0.5.1,TODO we are conditioning on a postive treatment
v0.5.1,TODO create an expression corresponding to each estimator used
v0.5.1,add weight column
v0.5.1,before weights are applied we count number rows in each category
v0.5.1,which is equivalent to summing over weight=1
v0.5.1,after weights are applied we need to sum over the given weights
v0.5.1,"First, calculating mean differences by strata"
v0.5.1,"Second, without strata"
v0.5.1,"Third, concatenating them and plotting"
v0.5.1,Setting estimator attribute for convenience
v0.5.1,Outcome is numeric
v0.5.1,Treatments are also numeric or binary
v0.5.1,Outcome is categorical
v0.5.1,Treatments are numeric or binary
v0.5.1,TODO: A common way to show all plots
v0.5.1,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.5.1,self._identified_estimand = self._causal_model.identify_effect()
v0.5.1,"self._identified_estimand,"
v0.5.1,"self._causal_model._treatment,"
v0.5.1,"self._causal_model._outcome,"
v0.5,Get the long description from the README file
v0.5,Get the required packages
v0.5,Loading version number
v0.5,-*- coding: utf-8 -*-
v0.5,
v0.5,Configuration file for the Sphinx documentation builder.
v0.5,
v0.5,This file does only contain a selection of the most common options. For a
v0.5,full list see the documentation:
v0.5,http://www.sphinx-doc.org/en/stable/config
v0.5,-- Path setup --------------------------------------------------------------
v0.5,"If extensions (or modules to document with autodoc) are in another directory,"
v0.5,add these directories to sys.path here. If the directory is relative to the
v0.5,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.5,
v0.5,-- Project information -----------------------------------------------------
v0.5,The short X.Y version
v0.5,"The full version, including alpha/beta/rc tags"
v0.5,-- General configuration ---------------------------------------------------
v0.5,"If your documentation needs a minimal Sphinx version, state it here."
v0.5,
v0.5,needs_sphinx = '1.0'
v0.5,"Add any Sphinx extension module names here, as strings. They can be"
v0.5,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.5,ones.
v0.5,"Add any paths that contain templates here, relative to this directory."
v0.5,The suffix(es) of source filenames.
v0.5,You can specify multiple suffix as a list of string:
v0.5,
v0.5,"source_suffix = ['.rst', '.md']"
v0.5,The master toctree document.
v0.5,The language for content autogenerated by Sphinx. Refer to documentation
v0.5,for a list of supported languages.
v0.5,
v0.5,This is also used if you do content translation via gettext catalogs.
v0.5,"Usually you set ""language"" from the command line for these cases."
v0.5,"List of patterns, relative to source directory, that match files and"
v0.5,directories to ignore when looking for source files.
v0.5,This pattern also affects html_static_path and html_extra_path .
v0.5,The name of the Pygments (syntax highlighting) style to use.
v0.5,-- Options for HTML output -------------------------------------------------
v0.5,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.5,a list of builtin themes.
v0.5,
v0.5,html_theme = 'sphinx-rtd-theme'
v0.5,on_rtd is whether we are on readthedocs.org
v0.5,only import and set the theme if we're building docs locally
v0.5,"otherwise, readthedocs.org uses their theme by default, so no need to specify it"
v0.5,Theme options are theme-specific and customize the look and feel of a theme
v0.5,"further.  For a list of options available for each theme, see the"
v0.5,documentation.
v0.5,
v0.5,html_theme_options = {}
v0.5,"Add any paths that contain custom static files (such as style sheets) here,"
v0.5,"relative to this directory. They are copied after the builtin static files,"
v0.5,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.5,"Custom sidebar templates, must be a dictionary that maps document names"
v0.5,to template names.
v0.5,
v0.5,The default sidebars (for documents that don't match any pattern) are
v0.5,defined by theme itself.  Builtin themes are using these templates by
v0.5,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.5,'searchbox.html']``.
v0.5,
v0.5,html_sidebars = {}
v0.5,-- Options for HTMLHelp output ---------------------------------------------
v0.5,Output file base name for HTML help builder.
v0.5,-- Options for LaTeX output ------------------------------------------------
v0.5,The paper size ('letterpaper' or 'a4paper').
v0.5,
v0.5,"'papersize': 'letterpaper',"
v0.5,"The font size ('10pt', '11pt' or '12pt')."
v0.5,
v0.5,"'pointsize': '10pt',"
v0.5,Additional stuff for the LaTeX preamble.
v0.5,
v0.5,"'preamble': '',"
v0.5,Latex figure (float) alignment
v0.5,
v0.5,"'figure_align': 'htbp',"
v0.5,Grouping the document tree into LaTeX files. List of tuples
v0.5,"(source start file, target name, title,"
v0.5,"author, documentclass [howto, manual, or own class])."
v0.5,-- Options for manual page output ------------------------------------------
v0.5,One entry per manual page. List of tuples
v0.5,"(source start file, name, description, authors, manual section)."
v0.5,-- Options for Texinfo output ----------------------------------------------
v0.5,Grouping the document tree into Texinfo files. List of tuples
v0.5,"(source start file, target name, title, author,"
v0.5,"dir menu entry, description, category)"
v0.5,-- Options for Epub output -------------------------------------------------
v0.5,Bibliographic Dublin Core info.
v0.5,The unique identifier of the text. This can be a ISBN number
v0.5,or the project homepage.
v0.5,
v0.5,epub_identifier = ''
v0.5,A unique identification for the text.
v0.5,
v0.5,epub_uid = ''
v0.5,A list of files that should not be packed into the epub file.
v0.5,-- Extension configuration -------------------------------------------------
v0.5,-- Options for todo extension ----------------------------------------------
v0.5,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.5,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.5,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.5,treated = self.df[self.df['z']==1]
v0.5,self.att = np.mean(treated['y1'] - treated['y0'])
v0.5,def test_average_treatment_effect(self):
v0.5,est_ate = 1
v0.5,bias = est_ate - self.ate
v0.5,print(bias)
v0.5,"self.assertAlmostEqual(self.ate, est_ate)"
v0.5,def test_average_treatment_effect_on_treated(self):
v0.5,est_att = 1
v0.5,self.att=1
v0.5,bias = est_att - self.att
v0.5,print(bias)
v0.5,"self.assertAlmostEqual(self.att, est_att)"
v0.5,removing two common causes
v0.5,The outcome is a linear function of the confounder
v0.5,"The slope is 1,2 and the intercept is 3"
v0.5,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.5,Supports user-provided dataset object
v0.5,To test if there are any exceptions
v0.5,To test if the estimate is identical if refutation parameters are zero
v0.5,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.5,"Ordinarily, we should expect this value to be zero."
v0.5,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.5,"Ordinarily, we should expect this value to be zero."
v0.5,cov_mat = np.diag(np.ones(num_features))
v0.5,Setup data
v0.5,Test LinearDMLCateEstimator
v0.5,Test ContinuousTreatmentOrthoForest
v0.5,Test LinearDRLearner
v0.5,Setup data
v0.5,Test DeepIV
v0.5,TODO: Test IntentToTreatDRIV when EconML v0.7 comes out
v0.5,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.5,More cases where Exception  is expected
v0.5,"Compute confidence intervals, standard error and significance tests"
v0.5,Defined a linear dataset with a given set of properties
v0.5,Create a model that captures the same
v0.5,Identify the effects within the model
v0.5,Defined a linear dataset with a given set of properties
v0.5,Create a model that captures the same
v0.5,Identify the effects within the model
v0.5,Defined a linear dataset with a given set of properties
v0.5,Create a model that captures the same
v0.5,Identify the effects within the model
v0.5,Defined a linear dataset with a given set of properties
v0.5,Create a model that captures the same
v0.5,Identify the effects within the model
v0.5,Defined a linear dataset with a given set of properties
v0.5,Create a model that captures the same
v0.5,Identify the effects within the model
v0.5,## 1. BACKDOOR IDENTIFICATION
v0.5,"First, checking if there are any valid backdoor adjustment sets"
v0.5,"Setting default ""backdoor"" identification adjustment set"
v0.5,## 2. INSTRUMENTAL VARIABLE IDENTIFICATION
v0.5,Now checking if there is also a valid iv estimand
v0.5,## 3. FRONTDOOR IDENTIFICATION
v0.5,Now checking if there is a valid frontdoor variable
v0.5,Finally returning the estimand object
v0.5,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.5,"First, checking if there are any valid backdoor adjustment sets"
v0.5,"Setting default ""backdoor"" identification adjustment set"
v0.5,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.5,Now checking if there are valid mediator variables
v0.5,Finally returning the estimand object
v0.5,## 1. FIRST DOING BACKDOOR IDENTIFICATION
v0.5,"First, checking if there are any valid backdoor adjustment sets"
v0.5,"Setting default ""backdoor"" identification adjustment set"
v0.5,"## 2. SECOND, CHECKING FOR MEDIATORS"
v0.5,Now checking if there are valid mediator variables
v0.5,Finally returning the estimand object
v0.5,"First, checking if empty set is a valid backdoor set"
v0.5,"Second, checking for all other sets of variables"
v0.5,causes_t = self._graph.get_causes(self.treatment_name)
v0.5,"causes_y = self._graph.get_causes(self.outcome_name, remove_edges={'sources':self.treatment_name, 'targets':self.outcome_name})"
v0.5,common_causes = list(causes_t.intersection(causes_y))
v0.5,"self.logger.info(""Common causes of treatment and outcome:"" + str(common_causes))"
v0.5,Adding a None estimand if no backdoor set found
v0.5,Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)
v0.5,"For simplicity, assuming a one-variable frontdoor set"
v0.5,"For simplicity, assuming a one-variable mediation set"
v0.5,"Create estimands dict as per the API for backdoor, but do not return it"
v0.5,"Setting default ""backdoor"" identification adjustment set"
v0.5,"Create estimands dict as per the API for backdoor, but do not return it"
v0.5,"Setting default ""backdoor"" identification adjustment set"
v0.5,Adding a None estimand if no backdoor set found
v0.5,Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)
v0.5,"TODO: outputs string for now, but ideally should do symbolic"
v0.5,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.5,TODO Better support for multivariate treatments
v0.5,"sym_common_causes = [sp.stats.Normal(common_cause, sym_mu, sym_sigma) for common_cause in common_causes]"
v0.5,TODO: support multivariate treatments better.
v0.5,TODO: support multivariate treatments better.
v0.5,TODO: support multivariate treatments better.
v0.5,Do not show backdoor key unless it is the only backdoor set.
v0.5,Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.
v0.5,Unpacking the keyword arguments
v0.5,Default value for the number of simulations to be conducted
v0.5,"Concatenate the confounders, instruments and effect modifiers"
v0.5,Shuffle the confounders
v0.5,Check if all are select or deselect variables
v0.5,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.5,Initializing the p_value
v0.5,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.5,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.5,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.5,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.5,Get the number of simulations
v0.5,Sort the simulations
v0.5,Obtain the median value
v0.5,Performing a two sided test
v0.5,np.searchsorted tells us the index if it were a part of the array
v0.5,We select side to be left as we want to find the first value that matches
v0.5,We subtact 1 as we are finding the value from the right tail
v0.5,We take the side to be right as we want to find the last index that matches
v0.5,We get the probability with respect to the left tail.
v0.5,Get the mean for the simulations
v0.5,Get the standard deviation for the simulations
v0.5,Get the Z Score [(val - mean)/ std_dev ]
v0.5,load dot file
v0.5,Adding node attributes
v0.5,TODO do not add it here. CausalIdentifier should call causal_graph to add an unobserved common cause if needed. This also ensures that we do not need get_common_causes in this class.
v0.5,Adding common causes
v0.5,Adding instruments
v0.5,Adding effect modifiers
v0.5,Adding columns in the dataframe as confounders that were not in the graph
v0.5,Adding unobserved confounders
v0.5,also return the number of backdoor paths blocked by observed nodes
v0.5,remove paths that have nodes1\node1 or nodes2\node2 as intermediate nodes
v0.5,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.5,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.5,removing all mediators
v0.5,[TODO: double check these work with multivariate implementation:]
v0.5,Exclusion
v0.5,As-if-random setup
v0.5,As-if-random
v0.5,TODO: move the logging level argument to a json file. Tue 20 Feb 2018 06:56:27 PM DST
v0.5,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.5,(Because some effect modifiers may also be common causes)
v0.5,"In such cases, the user-provided modifiers are used."
v0.5,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.5,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.5,TODO add dowhy as a prefix to all dowhy estimators
v0.5,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.5,Define the third-party estimation method to be used
v0.5,Process the dowhy estimators
v0.5,Check if estimator's target estimand is identified
v0.5,Store parameters inside estimate object for refutation methods
v0.5,Check if estimator's target estimand is identified
v0.5,The default number of simulations for statistical testing
v0.5,The default number of simulations to obtain confidence intervals
v0.5,The portion of the total size that should be taken each time to find the confidence intervals
v0.5,1 is the recommended value
v0.5,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.5,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.5,The default Confidence Level
v0.5,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.5,Prefix to add to temporary categorical variables created after discretization
v0.5,Currently estimation methods only support univariate treatment and outcome
v0.5,Setting the default interpret method
v0.5,Unpacking the keyword arguments
v0.5,"Checking if some parameters were set, otherwise setting to default values"
v0.5,Estimate conditional estimates by default
v0.5,Setting more values
v0.5,Now saving the effect modifiers
v0.5,TODO Only works for binary treatment
v0.5,Defaulting to class default values if parameters are not provided
v0.5,Checking that there is at least one effect modifier
v0.5,Making sure that effect_modifier_names is a list
v0.5,Making a copy since we are going to be changing effect modifier names
v0.5,"For every numeric effect modifier, adding a temp categorical column"
v0.5,Grouping by effect modifiers and computing effect separately
v0.5,Deleting the temporary categorical columns
v0.5,The array that stores the results of all estimations
v0.5,Find the sample size the proportion with the population size
v0.5,Perform the set number of simulations
v0.5,Using class default parameters if not specified
v0.5,Checking if bootstrap_estimates are already computed
v0.5,Checked if any parameter is changed from the previous std error estimate
v0.5,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.5,Sort the simulations
v0.5,"Now we take the (1- p)th and the (p)th values, where p is the chosen confidence level"
v0.5,get the values
v0.5,"Use existing params, if new user defined params are not present"
v0.5,Checking if bootstrap_estimates are already computed
v0.5,Check if any parameter is changed from the previous std error estimate
v0.5,"Use existing params, if new user defined params are not present"
v0.5,"self._outcome = self._data[""dummy_outcome""]"
v0.5,Processing the null hypothesis estimates
v0.5,Doing a two-sided test
v0.5,Being conservative with the p-value reported
v0.5,Being conservative with the p-value reported
v0.5,"If the estimate_index is 0, it depends on the number of simulations"
v0.5,Need to test r-squared before supporting
v0.5,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.5,'r-squared': effect_r_squared
v0.5,"elif method == ""r-squared"":"
v0.5,outcome_mean = np.mean(self._outcome)
v0.5,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.5,Assuming a linear model with one variable: the treatment
v0.5,Currently only works for continuous y
v0.5,causal_model = outcome_mean + estimate.value*self._treatment
v0.5,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.5,r_squared = 1 - (squared_residual/total_variance)
v0.5,return r_squared
v0.5,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.5,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.5,one-hot encode discrete W
v0.5,Now deleting the old continuous value
v0.5,Making beta an array
v0.5,TODO Ensure that we do not generate weak instruments
v0.5,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.5,Converting treatment to binary if required
v0.5,Generating frontdoor variables if asked for
v0.5,Computing ATE
v0.5,constructing column names for one-hot encoded discrete features
v0.5,Specifying the correct dtypes
v0.5,Now specifying the corresponding graph strings
v0.5,Now writing the gml graph
v0.5,Making beta an array
v0.5,creating data frame
v0.5,Specifying the correct dtypes
v0.5,Now specifying the corresponding graph strings
v0.5,Now writing the gml graph
v0.5,Adding edges between common causes and the frontdoor mediator
v0.5,Error terms
v0.5,else:
v0.5,V = 6 + W0 + tterm + E1
v0.5,Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new
v0.5,Loading version number
v0.5,The currently supported estimators
v0.5,The default standard deviation for noise
v0.5,The default scaling factor to determine the bucket size
v0.5,The minimum number of points for the estimator to run
v0.5,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.5,"The Default True Causal Effect, this is taken to be ZERO by default"
v0.5,The Default split for the number of data points that fall into the training and validation sets
v0.5,We need to change the identified estimand
v0.5,"We thus, make a copy. This is done as we don't want"
v0.5,to change the original DataFrame
v0.5,We use collections.OrderedDict to maintain the order in which the data is stored
v0.5,Check if we are using an estimator in the transformation list
v0.5,The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the
v0.5,"Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation"
v0.5,"loops. Thus, we can get different values everytime we get the estimator."
v0.5,Warn the user that the specified parameter is not applicable when no estimator is present in the transformation
v0.5,We set X_train = 0 and outcome_train to be 0
v0.5,"Get the final outcome, after running through all the values in the transformation list"
v0.5,Check if the value of true effect has been already stored
v0.5,We use None as the key as we have no base category for this refutation
v0.5,As we currently support only one treatment
v0.5,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.5,Check if the value of true effect has been already stored
v0.5,This ensures that we calculate the causal effect only once.
v0.5,We use key_train as we map data with respect to the base category of the data
v0.5,As we currently support only one treatment
v0.5,Add h(t) to f(W) to get the dummy outcome
v0.5,We convert to ndarray for ease in indexing
v0.5,The data is of the form
v0.5,sim1: cat1 cat2 ... catn
v0.5,sim2: cat1 cat2 ... catn
v0.5,Note: We would like the causal_estimator to find the true causal estimate that we have specified through this
v0.5,"refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the"
v0.5,distribution of the refuter.
v0.5,True Causal Effect list
v0.5,Iterating through the refutation for each category
v0.5,We use string arguments to account for both 32 and 64 bit varaibles
v0.5,action for continuous variables
v0.5,Action for categorical variables
v0.5,Find the set difference for each row
v0.5,Choose one out of the remaining
v0.5,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.5,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.5,of the treatment to affect the outcome
v0.5,Get a 2D matrix of values
v0.5,Store the values into the refute object
v0.5,Default value of the p value taken for the distribution
v0.5,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.5,Mean of the Normal Distribution
v0.5,Standard Deviation of the Normal Distribution
v0.5,We need to change the identified estimand
v0.5,"We make a copy as a safety measure, we don't want to change the"
v0.5,original DataFrame
v0.5,Create a new column in the data by the name of placebo
v0.5,Sanity check the data
v0.5,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.5,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.5,relationship between the treatment and the outcome.
v0.5,Adding a new backdoor variable to the identified estimand
v0.5,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.5,The default subset of the data to be used
v0.5,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.5,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.5,of the treatment to affect the outcome
v0.5,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.5,Check if the treatment is one-dimensional
v0.5,first_stage_features = self.build_first_stage_features()
v0.5,fs_model = self.first_stage_model()
v0.5,"if self._target_estimand.identifier_method==""frontdoor"":"
v0.5,first_stage_outcome = self._frontdoor_variables
v0.5,"elif self._target_estimand.identifier_method==""mediation"":"
v0.5,first_stage_outcome = self._mediators
v0.5,"fs_model.fit(first_stage_features, self._frontdoor_variables)"
v0.5,"self.logger.debug(""Coefficients of the fitted model: "" +"
v0.5,""","".join(map(str, fs_model.coef_)))"
v0.5,residuals = self._frontdoor_variables - fs_model.predict(first_stage_features)
v0.5,"self._data[""residual""] = residuals"
v0.5,First stage
v0.5,Second Stage
v0.5,Combining the two estimates
v0.5,Total  effect of treatment
v0.5,Bulding the feature matrix
v0.5,"features = sm.add_constant(features, has_constant='add') # to add an intercept term"
v0.5,choosing the instrumental variable to use
v0.5,TODO move this to the identification step
v0.5,Obtain estimate by Wald Estimator
v0.5,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.5,More than 1 instrument. Use 2sls.
v0.5,We need to initialize the model when we create any propensity score estimator
v0.5,Check if the treatment is one-dimensional
v0.5,Checking if the treatment is binary
v0.5,Convert the categorical variables into dummy/indicator variables
v0.5,"Basically, this gives a one hot encoding for each category"
v0.5,The first category is taken to be the base line.
v0.5,TODO make treatment_value and control value also as local parameters
v0.5,Checking if the model is already trained
v0.5,The model is always built on the entire data
v0.5,All treatments are set to the same constant value
v0.5,Using all data by default
v0.5,"Fixing treatment value to the specified value, if provided"
v0.5,treatment_vals and data_df should have same number of rows
v0.5,Bulding the feature matrix
v0.5,The model is always built on the entire data
v0.5,Replacing treatment values by given x
v0.5,sort the dataframe by propensity score
v0.5,create a column 'strata' for each element that marks what strata it belongs to
v0.5,"for each strata, count how many treated and control units there are"
v0.5,throw away strata that have insufficient treatment or control
v0.5,"print(""before clipping, here is the distribution of treatment and control per strata"")"
v0.5,"print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())"
v0.5,sum weighted outcomes over all strata  (weight by treated population)
v0.5,TODO - how can we add additional information into the returned estimate?
v0.5,"such as how much clipping was done, or per-strata info for debugging?"
v0.5,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.5,trim propensity score weights
v0.5,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.5,nips ==> ips / (sum of ips over all units)
v0.5,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.5,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.5,Vanilla IPS estimator
v0.5,Also known as the Hajek estimator
v0.5,Stabilized weights
v0.5,Simple normalized estimator (commented out for now)
v0.5,ips_sum = self._data['ips_weight'].sum()
v0.5,self._data['nips_weight'] = self._data['ips_weight'] / ips_sum
v0.5,self._data['ips2'] = self._data['ps'] / (1 - self._data['ps'])
v0.5,treated_ips_sum = (self._data['ips2'] * self._data[self._treatment_name[0]]).sum()
v0.5,control_ips_sum = (self._data['ips2'] * (1 - self._data[self._treatment_name[0]])).sum()
v0.5,self._data['itps_weight'] = self._data['ips2'] / treated_ips_sum
v0.5,self._data['icps_weight'] = self._data['ips2'] / control_ips_sum
v0.5,Calculating the effect
v0.5,TODO - how can we add additional information into the returned estimate?
v0.5,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.5,Checking if effect modifiers are a subset of common causes
v0.5,"Instrumental variables names, if present"
v0.5,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.5,Calling the econml estimator's fit method
v0.5,Changing shape to a list for a singleton value
v0.5,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.5,this assumes a binary treatment regime
v0.5,TODO remove neighbors that are more than a given radius apart
v0.5,estimate ATT on treated by summing over difference between matched neighbors
v0.5,Now computing ATC
v0.5,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.5,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.5,Add the identification method used in the estimator
v0.5,Check the backdoor variables being used
v0.5,Add the observed confounders and one hot encode the categorical variables
v0.5,Get the data of the unobserved confounders
v0.5,One hot encode the data if they are categorical
v0.5,Check the instrumental variables involved
v0.5,Perform the same actions as the above
v0.5,Check if effect modifiers are used
v0.5,Get the class corresponding the the estimator to be used
v0.5,Initialize the object
v0.5,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.5,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.5,For CATEs
v0.5,TODO we are conditioning on a postive treatment
v0.5,TODO create an expression corresponding to each estimator used
v0.5,add weight column
v0.5,before weights are applied we count number rows in each category
v0.5,which is equivalent to summing over weight=1
v0.5,after weights are applied we need to sum over the given weights
v0.5,"First, calculating mean differences by strata"
v0.5,"Second, without strata"
v0.5,"Third, concatenating them and plotting"
v0.5,Setting estimator attribute for convenience
v0.5,Outcome is numeric
v0.5,Treatments are also numeric or binary
v0.5,Outcome is categorical
v0.5,Treatments are numeric or binary
v0.5,TODO: A common way to show all plots
v0.5,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.5,self._identified_estimand = self._causal_model.identify_effect()
v0.5,"self._identified_estimand,"
v0.5,"self._causal_model._treatment,"
v0.5,"self._causal_model._outcome,"
v0.4,Get the long description from the README file
v0.4,Get the required packages
v0.4,Loading version number
v0.4,-*- coding: utf-8 -*-
v0.4,
v0.4,Configuration file for the Sphinx documentation builder.
v0.4,
v0.4,This file does only contain a selection of the most common options. For a
v0.4,full list see the documentation:
v0.4,http://www.sphinx-doc.org/en/stable/config
v0.4,-- Path setup --------------------------------------------------------------
v0.4,"If extensions (or modules to document with autodoc) are in another directory,"
v0.4,add these directories to sys.path here. If the directory is relative to the
v0.4,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.4,
v0.4,-- Project information -----------------------------------------------------
v0.4,The short X.Y version
v0.4,"The full version, including alpha/beta/rc tags"
v0.4,-- General configuration ---------------------------------------------------
v0.4,"If your documentation needs a minimal Sphinx version, state it here."
v0.4,
v0.4,needs_sphinx = '1.0'
v0.4,"Add any Sphinx extension module names here, as strings. They can be"
v0.4,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.4,ones.
v0.4,"Add any paths that contain templates here, relative to this directory."
v0.4,The suffix(es) of source filenames.
v0.4,You can specify multiple suffix as a list of string:
v0.4,
v0.4,"source_suffix = ['.rst', '.md']"
v0.4,The master toctree document.
v0.4,The language for content autogenerated by Sphinx. Refer to documentation
v0.4,for a list of supported languages.
v0.4,
v0.4,This is also used if you do content translation via gettext catalogs.
v0.4,"Usually you set ""language"" from the command line for these cases."
v0.4,"List of patterns, relative to source directory, that match files and"
v0.4,directories to ignore when looking for source files.
v0.4,This pattern also affects html_static_path and html_extra_path .
v0.4,The name of the Pygments (syntax highlighting) style to use.
v0.4,-- Options for HTML output -------------------------------------------------
v0.4,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.4,a list of builtin themes.
v0.4,
v0.4,html_theme = 'sphinx-rtd-theme'
v0.4,on_rtd is whether we are on readthedocs.org
v0.4,only import and set the theme if we're building docs locally
v0.4,"otherwise, readthedocs.org uses their theme by default, so no need to specify it"
v0.4,Theme options are theme-specific and customize the look and feel of a theme
v0.4,"further.  For a list of options available for each theme, see the"
v0.4,documentation.
v0.4,
v0.4,html_theme_options = {}
v0.4,"Add any paths that contain custom static files (such as style sheets) here,"
v0.4,"relative to this directory. They are copied after the builtin static files,"
v0.4,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.4,"Custom sidebar templates, must be a dictionary that maps document names"
v0.4,to template names.
v0.4,
v0.4,The default sidebars (for documents that don't match any pattern) are
v0.4,defined by theme itself.  Builtin themes are using these templates by
v0.4,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.4,'searchbox.html']``.
v0.4,
v0.4,html_sidebars = {}
v0.4,-- Options for HTMLHelp output ---------------------------------------------
v0.4,Output file base name for HTML help builder.
v0.4,-- Options for LaTeX output ------------------------------------------------
v0.4,The paper size ('letterpaper' or 'a4paper').
v0.4,
v0.4,"'papersize': 'letterpaper',"
v0.4,"The font size ('10pt', '11pt' or '12pt')."
v0.4,
v0.4,"'pointsize': '10pt',"
v0.4,Additional stuff for the LaTeX preamble.
v0.4,
v0.4,"'preamble': '',"
v0.4,Latex figure (float) alignment
v0.4,
v0.4,"'figure_align': 'htbp',"
v0.4,Grouping the document tree into LaTeX files. List of tuples
v0.4,"(source start file, target name, title,"
v0.4,"author, documentclass [howto, manual, or own class])."
v0.4,-- Options for manual page output ------------------------------------------
v0.4,One entry per manual page. List of tuples
v0.4,"(source start file, name, description, authors, manual section)."
v0.4,-- Options for Texinfo output ----------------------------------------------
v0.4,Grouping the document tree into Texinfo files. List of tuples
v0.4,"(source start file, target name, title, author,"
v0.4,"dir menu entry, description, category)"
v0.4,-- Options for Epub output -------------------------------------------------
v0.4,Bibliographic Dublin Core info.
v0.4,The unique identifier of the text. This can be a ISBN number
v0.4,or the project homepage.
v0.4,
v0.4,epub_identifier = ''
v0.4,A unique identification for the text.
v0.4,
v0.4,epub_uid = ''
v0.4,A list of files that should not be packed into the epub file.
v0.4,-- Extension configuration -------------------------------------------------
v0.4,-- Options for todo extension ----------------------------------------------
v0.4,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.4,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.4,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.4,treated = self.df[self.df['z']==1]
v0.4,self.att = np.mean(treated['y1'] - treated['y0'])
v0.4,def test_average_treatment_effect(self):
v0.4,est_ate = 1
v0.4,bias = est_ate - self.ate
v0.4,print(bias)
v0.4,"self.assertAlmostEqual(self.ate, est_ate)"
v0.4,def test_average_treatment_effect_on_treated(self):
v0.4,est_att = 1
v0.4,self.att=1
v0.4,bias = est_att - self.att
v0.4,print(bias)
v0.4,"self.assertAlmostEqual(self.att, est_att)"
v0.4,removing two common causes
v0.4,The outcome is a linear function of the confounder
v0.4,The slope is 2 and the intercept is 3
v0.4,"As we run with only one common cause and one instrument variable we run with (?, 2)"
v0.4,Supports user-provided dataset object
v0.4,To test if there are any exceptions
v0.4,To test if the estimate is identical if refutation parameters are zero
v0.4,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.4,"Ordinarily, we should expect this value to be zero."
v0.4,This value is hardcoded to be zero as we are runnning this on a linear dataset.
v0.4,"Ordinarily, we should expect this value to be zero."
v0.4,cov_mat = np.diag(np.ones(num_features))
v0.4,Setup data
v0.4,Test LinearDMLCateEstimator
v0.4,Test ContinuousTreatmentOrthoForest
v0.4,Test LinearDRLearner
v0.4,Setup data
v0.4,Test DeepIV
v0.4,TODO: Test IntentToTreatDRIV when EconML v0.7 comes out
v0.4,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.4,More cases where Exception  is expected
v0.4,"Compute confidence intervals, standard error and significance tests"
v0.4,Defined a linear dataset with a given set of properties
v0.4,Create a model that captures the same
v0.4,Identify the effects within the model
v0.4,Defined a linear dataset with a given set of properties
v0.4,Create a model that captures the same
v0.4,Identify the effects within the model
v0.4,Defined a linear dataset with a given set of properties
v0.4,Create a model that captures the same
v0.4,Identify the effects within the model
v0.4,Defined a linear dataset with a given set of properties
v0.4,Create a model that captures the same
v0.4,Identify the effects within the model
v0.4,Defined a linear dataset with a given set of properties
v0.4,Create a model that captures the same
v0.4,Identify the effects within the model
v0.4,Now checking if there is also a valid iv estimand
v0.4,"TODO: outputs string for now, but ideally should do symbolic"
v0.4,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.4,TODO Better support for multivariate treatments
v0.4,"sym_common_causes = [sp.stats.Normal(common_cause, sym_mu, sym_sigma) for common_cause in common_causes]"
v0.4,TODO: support multivariate treatments better.
v0.4,Default value for the number of simulations to be conducted
v0.4,"Concatenate the confounders, instruments and effect modifiers"
v0.4,Shuffle the confounders
v0.4,Check if all are select or deselect variables
v0.4,"Check if all the required_variables belong to confounders, instrumental variables or effect"
v0.4,Initializing the p_value
v0.4,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.4,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.4,Perform Bootstrap Significance Test with the original estimate and the set of refutations
v0.4,Perform Normal Tests of Significance with the original estimate and the set of refutations
v0.4,Get the number of simulations
v0.4,Sort the simulations
v0.4,Obtain the median value
v0.4,Performing a two sided test
v0.4,np.searchsorted tells us the index if it were a part of the array
v0.4,We select side to be left as we want to find the first value that matches
v0.4,We subtact 1 as we are finding the value from the right tail
v0.4,We take the side to be right as we want to find the last index that matches
v0.4,We get the probability with respect to the left tail.
v0.4,Get the mean for the simulations
v0.4,Get the standard deviation for the simulations
v0.4,Get the Z Score [(val - mean)/ std_dev ]
v0.4,load dot file
v0.4,Adding node attributes
v0.4,TODO do not add it here. CausalIdentifier should call causal_graph to add an unobserved common cause if needed. This also ensures that we do not need get_common_causes in this class.
v0.4,Adding common causes
v0.4,Adding instruments
v0.4,Adding effect modifiers
v0.4,Adding columns in the dataframe as confounders that were not in the graph
v0.4,Adding unobserved confounders
v0.4,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.4,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.4,[TODO: double check these work with multivariate implementation:]
v0.4,Exclusion
v0.4,As-if-random setup
v0.4,As-if-random
v0.4,TODO: move the logging level argument to a json file. Tue 20 Feb 2018 06:56:27 PM DST
v0.4,"Sometimes, effect modifiers from the graph may not match those provided by the user."
v0.4,(Because some effect modifiers may also be common causes)
v0.4,"In such cases, the user-provided modifiers are used."
v0.4,"If no effect modifiers are provided,  then the ones from the graph are used."
v0.4,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.4,TODO add dowhy as a prefix to all dowhy estimators
v0.4,This is done as all dowhy estimators have two parts and external ones have two or more parts
v0.4,Define the third-party estimation method to be used
v0.4,Process the dowhy estimators
v0.4,Check if estimator's target estimand is identified
v0.4,Store parameters inside estimate object for refutation methods
v0.4,Check if estimator's target estimand is identified
v0.4,The default number of simulations for statistical testing
v0.4,The default number of simulations to obtain confidence intervals
v0.4,The portion of the total size that should be taken each time to find the confidence intervals
v0.4,1 is the recommended value
v0.4,https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf
v0.4,https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214
v0.4,The default Confidence Level
v0.4,"Number of quantiles to discretize continuous columns, for applying groupby"
v0.4,Prefix to add to temporary categorical variables created after discretization
v0.4,Currently estimation methods only support univariate treatment and outcome
v0.4,Unpacking the keyword arguments
v0.4,"Checking if some parameters were set, otherwise setting to default values"
v0.4,Setting more values
v0.4,Now saving the effect modifiers
v0.4,TODO Only works for binary treatment
v0.4,Defaulting to class default values if parameters are not provided
v0.4,Checking that there is at least one effect modifier
v0.4,Making sure that effect_modifier_names is a list
v0.4,Making a copy since we are going to be changing effect modifier names
v0.4,"For every numeric effect modifier, adding a temp categorical column"
v0.4,Grouping by effect modifiers and computing effect separately
v0.4,Deleting the temporary categorical columns
v0.4,The array that stores the results of all estimations
v0.4,Find the sample size the proportion with the population size
v0.4,Perform the set number of simulations
v0.4,Using class default parameters if not specified
v0.4,Checking if bootstrap_estimates are already computed
v0.4,Checked if any parameter is changed from the previous std error estimate
v0.4,Now use the data obtained from the simulations to get the value of the confidence estimates
v0.4,Sort the simulations
v0.4,"Now we take the (1- p)th and the (p)th values, where p is the chosen confidence level"
v0.4,get the values
v0.4,"Use existing params, if new user defined params are not present"
v0.4,Checking if bootstrap_estimates are already computed
v0.4,Check if any parameter is changed from the previous std error estimate
v0.4,"Use existing params, if new user defined params are not present"
v0.4,"self._outcome = self._data[""dummy_outcome""]"
v0.4,Processing the null hypothesis estimates
v0.4,Doing a two-sided test
v0.4,Being conservative with the p-value reported
v0.4,Being conservative with the p-value reported
v0.4,"If the estimate_index is 0, it depends on the number of simulations"
v0.4,Need to test r-squared before supporting
v0.4,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.4,'r-squared': effect_r_squared
v0.4,"elif method == ""r-squared"":"
v0.4,outcome_mean = np.mean(self._outcome)
v0.4,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.4,Assuming a linear model with one variable: the treatment
v0.4,Currently only works for continuous y
v0.4,causal_model = outcome_mean + estimate.value*self._treatment
v0.4,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.4,r_squared = 1 - (squared_residual/total_variance)
v0.4,return r_squared
v0.4,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.4,Below loop assumes that the last indices of W are alwawys converted to discrete
v0.4,one-hot encode discrete W
v0.4,Now deleting the old continuous value
v0.4,Making beta an array
v0.4,TODO Ensure that we do not generate weak instruments
v0.4,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.4,Converting treatment to binary if required
v0.4,Specifying the correct dtypes
v0.4,Now specifying the corresponding graph strings
v0.4,Now writing the gml graph
v0.4,Making beta an array
v0.4,creating data frame
v0.4,Specifying the correct dtypes
v0.4,Now specifying the corresponding graph strings
v0.4,Now writing the gml graph
v0.4,Loading version number
v0.4,The currently supported estimators
v0.4,The default standard deviation for noise
v0.4,The default scaling factor to determine the bucket size
v0.4,The minimum number of points for the estimator to run
v0.4,"The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator"
v0.4,We need to change the identified estimand
v0.4,"We thus, make a copy. This is done as we don't want"
v0.4,to change the original DataFrame
v0.4,We set X_train = 0 and outcome_train to be 0
v0.4,"Get the final outcome, after running through all the values in the transformation list"
v0.4,"If the number of data points is too few, run the default transformation: [(""zero"",""""),(""noise"", {'std_dev':1} )]"
v0.4,We convert to ndarray for ease in indexing
v0.4,The data is of the form
v0.4,sim1: cat1 cat2 ... catn
v0.4,sim2: cat1 cat2 ... catn
v0.4,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.4,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.4,relationship between the treatment and the outcome.
v0.4,We use string arguments to account for both 32 and 64 bit varaibles
v0.4,action for continuous variables
v0.4,Action for categorical variables
v0.4,Find the set difference for each row
v0.4,Choose one out of the remaining
v0.4,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.4,Ideally that should be the case as running bootstrap should not have a significant effect on the ability
v0.4,of the treatment to affect the outcome
v0.4,Get a 2D matrix of values
v0.4,Store the values into the refute object
v0.4,Default value of the p value taken for the distribution
v0.4,Number of Trials: Number of cointosses to understand if a sample gets the treatment
v0.4,Mean of the Normal Distribution
v0.4,Standard Deviation of the Normal Distribution
v0.4,We need to change the identified estimand
v0.4,"We make a copy as a safety measure, we don't want to change the"
v0.4,original DataFrame
v0.4,Create a new column in the data by the name of placebo
v0.4,Sanity check the data
v0.4,Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter
v0.4,Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal
v0.4,relationship between the treatment and the outcome.
v0.4,Adding a new backdoor variable to the identified estimand
v0.4,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.4,We want to see if the estimate falls in the same distribution as the one generated by the refuter
v0.4,Ideally that should be the case as choosing a subset should not have a significant effect on the ability
v0.4,of the treatment to affect the outcome
v0.4,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.4,choosing the instrumental variable to use
v0.4,TODO move this to the identification step
v0.4,Obtain estimate by Wald Estimator
v0.4,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.4,More than 1 instrument. Use 2sls.
v0.4,We need to initialize the model when we create any propensity score estimator
v0.4,Check if the treatment is one-dimensional
v0.4,Checking if the treatment is binary
v0.4,Convert the categorical variables into dummy/indicator variables
v0.4,"Basically, this gives a one hot encoding for each category"
v0.4,The first category is taken to be the base line.
v0.4,TODO make treatment_value and control value also as local parameters
v0.4,Checking if the model is already trained
v0.4,The model is always built on the entire data
v0.4,All treatments are set to the same constant value
v0.4,Using all data by default
v0.4,"Fixing treatment value to the specified value, if provided"
v0.4,treatment_vals and data_df should have same number of rows
v0.4,Bulding the feature matrix
v0.4,The model is always built on the entire data
v0.4,Replacing treatment values by given x
v0.4,sort the dataframe by propensity score
v0.4,create a column 'strata' for each element that marks what strata it belongs to
v0.4,"for each strata, count how many treated and control units there are"
v0.4,throw away strata that have insufficient treatment or control
v0.4,"print(""before clipping, here is the distribution of treatment and control per strata"")"
v0.4,"print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())"
v0.4,"print(""after clipping at threshold, now we have:"" )"
v0.4,"print(clipped.groupby(['strata',self._treatment_name])[self._outcome_name].count())"
v0.4,sum weighted outcomes over all strata  (weight by treated population)
v0.4,TODO - how can we add additional information into the returned estimate?
v0.4,"such as how much clipping was done, or per-strata info for debugging?"
v0.4,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.4,trim propensity score weights
v0.4,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.4,nips ==> ips / (sum of ips over all units)
v0.4,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.4,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.4,Vanilla IPS estimator
v0.4,Also known as the Hajek estimator
v0.4,Stabilized weights
v0.4,Simple normalized estimator (commented out for now)
v0.4,ips_sum = self._data['ips_weight'].sum()
v0.4,self._data['nips_weight'] = self._data['ips_weight'] / ips_sum
v0.4,self._data['ips2'] = self._data['ps'] / (1 - self._data['ps'])
v0.4,treated_ips_sum = (self._data['ips2'] * self._data[self._treatment_name[0]]).sum()
v0.4,control_ips_sum = (self._data['ips2'] * (1 - self._data[self._treatment_name[0]])).sum()
v0.4,self._data['itps_weight'] = self._data['ips2'] / treated_ips_sum
v0.4,self._data['icps_weight'] = self._data['ips2'] / control_ips_sum
v0.4,Calculating the effect
v0.4,TODO - how can we add additional information into the returned estimate?
v0.4,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.4,Checking if effect modifiers are a subset of common causes
v0.4,"Instrumental variables names, if present"
v0.4,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.4,Calling the econml estimator's fit method
v0.4,Changing shape to a list for a singleton value
v0.4,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.4,this assumes a binary treatment regime
v0.4,TODO remove neighbors that are more than a given radius apart
v0.4,estimate ATT on treated by summing over difference between matched neighbors
v0.4,Now computing ATC
v0.4,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.4,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.4,Add the identification method used in the estimator
v0.4,Check the backdoor variables being used
v0.4,Add the observed confounders and one hot encode the categorical variables
v0.4,Get the data of the unobserved confounders
v0.4,One hot encode the data if they are categorical
v0.4,Check the instrumental variables involved
v0.4,Perform the same actions as the above
v0.4,Check if effect modifiers are used
v0.4,Get the class corresponding the the estimator to be used
v0.4,Initialize the object
v0.4,Both the outcome and the treatment have to be 1D arrays according to the CausalML API
v0.4,We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch
v0.4,TODO we are conditioning on a postive treatment
v0.4,TODO create an expression corresponding to each estimator used
v0.4,self._identified_estimand = self._causal_model.identify_effect()
v0.4,"self._identified_estimand,"
v0.4,"self._causal_model._treatment,"
v0.4,"self._causal_model._outcome,"
v0.2,Get the long description from the README file
v0.2,Get the required packages
v0.2,Loading version number
v0.2,-*- coding: utf-8 -*-
v0.2,
v0.2,Configuration file for the Sphinx documentation builder.
v0.2,
v0.2,This file does only contain a selection of the most common options. For a
v0.2,full list see the documentation:
v0.2,http://www.sphinx-doc.org/en/stable/config
v0.2,-- Path setup --------------------------------------------------------------
v0.2,"If extensions (or modules to document with autodoc) are in another directory,"
v0.2,add these directories to sys.path here. If the directory is relative to the
v0.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.2,
v0.2,-- Project information -----------------------------------------------------
v0.2,The short X.Y version
v0.2,"The full version, including alpha/beta/rc tags"
v0.2,-- General configuration ---------------------------------------------------
v0.2,"If your documentation needs a minimal Sphinx version, state it here."
v0.2,
v0.2,needs_sphinx = '1.0'
v0.2,"Add any Sphinx extension module names here, as strings. They can be"
v0.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.2,ones.
v0.2,"Add any paths that contain templates here, relative to this directory."
v0.2,The suffix(es) of source filenames.
v0.2,You can specify multiple suffix as a list of string:
v0.2,
v0.2,"source_suffix = ['.rst', '.md']"
v0.2,The master toctree document.
v0.2,The language for content autogenerated by Sphinx. Refer to documentation
v0.2,for a list of supported languages.
v0.2,
v0.2,This is also used if you do content translation via gettext catalogs.
v0.2,"Usually you set ""language"" from the command line for these cases."
v0.2,"List of patterns, relative to source directory, that match files and"
v0.2,directories to ignore when looking for source files.
v0.2,This pattern also affects html_static_path and html_extra_path .
v0.2,The name of the Pygments (syntax highlighting) style to use.
v0.2,-- Options for HTML output -------------------------------------------------
v0.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.2,a list of builtin themes.
v0.2,
v0.2,html_theme = 'sphinx-rtd-theme'
v0.2,on_rtd is whether we are on readthedocs.org
v0.2,only import and set the theme if we're building docs locally
v0.2,"otherwise, readthedocs.org uses their theme by default, so no need to specify it"
v0.2,Theme options are theme-specific and customize the look and feel of a theme
v0.2,"further.  For a list of options available for each theme, see the"
v0.2,documentation.
v0.2,
v0.2,html_theme_options = {}
v0.2,"Add any paths that contain custom static files (such as style sheets) here,"
v0.2,"relative to this directory. They are copied after the builtin static files,"
v0.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.2,"Custom sidebar templates, must be a dictionary that maps document names"
v0.2,to template names.
v0.2,
v0.2,The default sidebars (for documents that don't match any pattern) are
v0.2,defined by theme itself.  Builtin themes are using these templates by
v0.2,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.2,'searchbox.html']``.
v0.2,
v0.2,html_sidebars = {}
v0.2,-- Options for HTMLHelp output ---------------------------------------------
v0.2,Output file base name for HTML help builder.
v0.2,-- Options for LaTeX output ------------------------------------------------
v0.2,The paper size ('letterpaper' or 'a4paper').
v0.2,
v0.2,"'papersize': 'letterpaper',"
v0.2,"The font size ('10pt', '11pt' or '12pt')."
v0.2,
v0.2,"'pointsize': '10pt',"
v0.2,Additional stuff for the LaTeX preamble.
v0.2,
v0.2,"'preamble': '',"
v0.2,Latex figure (float) alignment
v0.2,
v0.2,"'figure_align': 'htbp',"
v0.2,Grouping the document tree into LaTeX files. List of tuples
v0.2,"(source start file, target name, title,"
v0.2,"author, documentclass [howto, manual, or own class])."
v0.2,-- Options for manual page output ------------------------------------------
v0.2,One entry per manual page. List of tuples
v0.2,"(source start file, name, description, authors, manual section)."
v0.2,-- Options for Texinfo output ----------------------------------------------
v0.2,Grouping the document tree into Texinfo files. List of tuples
v0.2,"(source start file, target name, title, author,"
v0.2,"dir menu entry, description, category)"
v0.2,-- Options for Epub output -------------------------------------------------
v0.2,Bibliographic Dublin Core info.
v0.2,The unique identifier of the text. This can be a ISBN number
v0.2,or the project homepage.
v0.2,
v0.2,epub_identifier = ''
v0.2,A unique identification for the text.
v0.2,
v0.2,epub_uid = ''
v0.2,A list of files that should not be packed into the epub file.
v0.2,-- Extension configuration -------------------------------------------------
v0.2,-- Options for todo extension ----------------------------------------------
v0.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.2,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.2,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.2,treated = self.df[self.df['z']==1]
v0.2,self.att = np.mean(treated['y1'] - treated['y0'])
v0.2,def test_average_treatment_effect(self):
v0.2,est_ate = 1
v0.2,bias = est_ate - self.ate
v0.2,print(bias)
v0.2,"self.assertAlmostEqual(self.ate, est_ate)"
v0.2,def test_average_treatment_effect_on_treated(self):
v0.2,est_att = 1
v0.2,self.att=1
v0.2,bias = est_att - self.att
v0.2,print(bias)
v0.2,"self.assertAlmostEqual(self.att, est_att)"
v0.2,removing two common causes
v0.2,Supports user-provided dataset object
v0.2,To test if there are any exceptions
v0.2,To test if the estimate is identical if refutation parameters are zero
v0.2,"beta=1,"
v0.2,"num_instruments=2, num_samples=100000,"
v0.2,cov_mat = np.diag(np.ones(num_features))
v0.2,"Not using testsuite from .base/TestEstimtor, custom code below"
v0.2,More cases where Exception  is expected
v0.2,Now checking if there is also a valid iv estimand
v0.2,"TODO: outputs string for now, but ideally should do symbolic"
v0.2,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.2,TODO Better support for multivariate treatments
v0.2,"sym_common_causes = [sp.stats.Normal(common_cause, sym_mu, sym_sigma) for common_cause in common_causes]"
v0.2,TODO: support multivariate treatments better.
v0.2,load dot file
v0.2,TODO do not add it here. CausalIdentifier should call causal_graph to add an unobserved common cause if needed. This also ensures that we do not need get_common_causes in this class.
v0.2,"nx.draw_networkx(self._graph, pos=nx.shell_layout(self._graph))"
v0.2,Adding common causes
v0.2,Adding instruments
v0.2,Adding effect modifiers
v0.2,Adding unobserved confounders
v0.2,Adding unobserved confounders
v0.2,TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.
v0.2,"Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)"
v0.2,[TODO: double check these work with multivariate implementation:]
v0.2,Exclusion
v0.2,As-if-random setup
v0.2,As-if-random
v0.2,TODO: move the logging level argument to a json file. Tue 20 Feb 2018 06:56:27 PM DST
v0.2,"TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected."
v0.2,Check if estimator's target estimand is identified
v0.2,Store parameters inside estimate object for refutation methods
v0.2,Check if estimator's target estimand is identified
v0.2,Currently estimation methods only support univariate treatment and outcome
v0.2,"Checking if some parameters were set, otherwise setting to default values"
v0.2,Setting more values
v0.2,Now saving the effect modifiers
v0.2,TODO Only works for binary treatment
v0.2,Doing a two-sided test
v0.2,Being conservative with the p-value reported
v0.2,Being conservative with the p-value reported
v0.2,Need to test r-squared before supporting
v0.2,"effect_r_squared = self._evaluate_effect_strength(estimate, method=""r-squared"")"
v0.2,'r-squared': effect_r_squared
v0.2,"elif method == ""r-squared"":"
v0.2,outcome_mean = np.mean(self._outcome)
v0.2,total_variance = np.sum(np.square(self._outcome - outcome_mean))
v0.2,Assuming a linear model with one variable: the treatment
v0.2,Currently only works for continuous y
v0.2,causal_model = outcome_mean + estimate.value*self._treatment
v0.2,squared_residual = np.sum(np.square(self._outcome - causal_model))
v0.2,r_squared = 1 - (squared_residual/total_variance)
v0.2,return r_squared
v0.2,"s += ""Variance in outcome explained by treatment: {}\n"".format(self.effect_strength[""r-squared""])"
v0.2,Making beta an array
v0.2,TODO Ensure that we do not generate weak instruments
v0.2,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.2,Converting treatment to binary if required
v0.2,Specifying the correct dtypes
v0.2,Now specifying the corresponding graph strings
v0.2,Now writing the gml graph
v0.2,Making beta an array
v0.2,creating data frame
v0.2,Specifying the correct dtypes
v0.2,Now specifying the corresponding graph strings
v0.2,Now writing the gml graph
v0.2,Loading version number
v0.2,Adding a new backdoor variable to the identified estimand
v0.2,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.2,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.2,choosing the instrumental variable to use
v0.2,TODO move this to the identification step
v0.2,Obtain estimate by Wald Estimator
v0.2,"Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)"
v0.2,More than 1 instrument. Use 2sls.
v0.2,"Instrumental variables names, if present"
v0.2,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.2,Calling the econml estimator's fit method
v0.2,Changing shape to a list for a singleton value
v0.2,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.2,All treatments are set to the same constant value
v0.2,Checking if treatment is one-dimensional
v0.2,Checking if treatment is binary
v0.2,sort the dataframe by propensity score
v0.2,create a column 'strata' for each element that marks what strata it belongs to
v0.2,"for each strata, count how many treated and control units there are"
v0.2,throw away strata that have insufficient treatment or control
v0.2,"print(""before clipping, here is the distribution of treatment and control per strata"")"
v0.2,"print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())"
v0.2,"print(""after clipping at threshold, now we have:"" )"
v0.2,"print(clipped.groupby(['strata',self._treatment_name])[self._outcome_name].count())"
v0.2,sum weighted outcomes over all strata  (weight by treated population)
v0.2,TODO - how can we add additional information into the returned estimate?
v0.2,"such as how much clipping was done, or per-strata info for debugging?"
v0.2,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.2,Checking if treatment is one-dimensional
v0.2,Checking if treatment is binary
v0.2,trim propensity score weights
v0.2,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.2,nips ==> ips / (sum of ips over all units)
v0.2,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.2,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.2,Vanilla IPS estimator
v0.2,Also known as the Hajek estimator
v0.2,Stabilized weights
v0.2,Simple normalized estimator (commented out for now)
v0.2,ips_sum = self._data['ips_weight'].sum()
v0.2,self._data['nips_weight'] = self._data['ips_weight'] / ips_sum
v0.2,self._data['ips2'] = self._data['ps'] / (1 - self._data['ps'])
v0.2,treated_ips_sum = (self._data['ips2'] * self._data[self._treatment_name[0]]).sum()
v0.2,control_ips_sum = (self._data['ips2'] * (1 - self._data[self._treatment_name[0]])).sum()
v0.2,self._data['itps_weight'] = self._data['ips2'] / treated_ips_sum
v0.2,self._data['icps_weight'] = self._data['ips2'] / control_ips_sum
v0.2,Calculating the effect
v0.2,TODO - how can we add additional information into the returned estimate?
v0.2,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.2,Checking if treatment is one-dimensional
v0.2,Checking if treatment is binary
v0.2,this assumes a binary treatment regime
v0.2,TODO remove neighbors that are more than a given radius apart
v0.2,estimate ATT on treated by summing over difference between matched neighbors
v0.2,Now computing ATC
v0.2,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.2,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.2,self._identified_estimand = self._causal_model.identify_effect()
v0.2,"self._identified_estimand,"
v0.2,"self._causal_model._treatment,"
v0.2,"self._causal_model._outcome,"
v0.1.1-alpha,Get the long description from the README file
v0.1.1-alpha,Get the required packages
v0.1.1-alpha,-*- coding: utf-8 -*-
v0.1.1-alpha,
v0.1.1-alpha,Configuration file for the Sphinx documentation builder.
v0.1.1-alpha,
v0.1.1-alpha,This file does only contain a selection of the most common options. For a
v0.1.1-alpha,full list see the documentation:
v0.1.1-alpha,http://www.sphinx-doc.org/en/stable/config
v0.1.1-alpha,-- Path setup --------------------------------------------------------------
v0.1.1-alpha,"If extensions (or modules to document with autodoc) are in another directory,"
v0.1.1-alpha,add these directories to sys.path here. If the directory is relative to the
v0.1.1-alpha,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.1.1-alpha,
v0.1.1-alpha,-- Project information -----------------------------------------------------
v0.1.1-alpha,The short X.Y version
v0.1.1-alpha,"The full version, including alpha/beta/rc tags"
v0.1.1-alpha,-- General configuration ---------------------------------------------------
v0.1.1-alpha,"If your documentation needs a minimal Sphinx version, state it here."
v0.1.1-alpha,
v0.1.1-alpha,needs_sphinx = '1.0'
v0.1.1-alpha,"Add any Sphinx extension module names here, as strings. They can be"
v0.1.1-alpha,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.1.1-alpha,ones.
v0.1.1-alpha,"Add any paths that contain templates here, relative to this directory."
v0.1.1-alpha,The suffix(es) of source filenames.
v0.1.1-alpha,You can specify multiple suffix as a list of string:
v0.1.1-alpha,
v0.1.1-alpha,"source_suffix = ['.rst', '.md']"
v0.1.1-alpha,The master toctree document.
v0.1.1-alpha,The language for content autogenerated by Sphinx. Refer to documentation
v0.1.1-alpha,for a list of supported languages.
v0.1.1-alpha,
v0.1.1-alpha,This is also used if you do content translation via gettext catalogs.
v0.1.1-alpha,"Usually you set ""language"" from the command line for these cases."
v0.1.1-alpha,"List of patterns, relative to source directory, that match files and"
v0.1.1-alpha,directories to ignore when looking for source files.
v0.1.1-alpha,This pattern also affects html_static_path and html_extra_path .
v0.1.1-alpha,The name of the Pygments (syntax highlighting) style to use.
v0.1.1-alpha,-- Options for HTML output -------------------------------------------------
v0.1.1-alpha,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.1.1-alpha,a list of builtin themes.
v0.1.1-alpha,
v0.1.1-alpha,html_theme = 'sphinx-rtd-theme'
v0.1.1-alpha,on_rtd is whether we are on readthedocs.org
v0.1.1-alpha,only import and set the theme if we're building docs locally
v0.1.1-alpha,"otherwise, readthedocs.org uses their theme by default, so no need to specify it"
v0.1.1-alpha,Theme options are theme-specific and customize the look and feel of a theme
v0.1.1-alpha,"further.  For a list of options available for each theme, see the"
v0.1.1-alpha,documentation.
v0.1.1-alpha,
v0.1.1-alpha,html_theme_options = {}
v0.1.1-alpha,"Add any paths that contain custom static files (such as style sheets) here,"
v0.1.1-alpha,"relative to this directory. They are copied after the builtin static files,"
v0.1.1-alpha,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.1.1-alpha,"Custom sidebar templates, must be a dictionary that maps document names"
v0.1.1-alpha,to template names.
v0.1.1-alpha,
v0.1.1-alpha,The default sidebars (for documents that don't match any pattern) are
v0.1.1-alpha,defined by theme itself.  Builtin themes are using these templates by
v0.1.1-alpha,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
v0.1.1-alpha,'searchbox.html']``.
v0.1.1-alpha,
v0.1.1-alpha,html_sidebars = {}
v0.1.1-alpha,-- Options for HTMLHelp output ---------------------------------------------
v0.1.1-alpha,Output file base name for HTML help builder.
v0.1.1-alpha,-- Options for LaTeX output ------------------------------------------------
v0.1.1-alpha,The paper size ('letterpaper' or 'a4paper').
v0.1.1-alpha,
v0.1.1-alpha,"'papersize': 'letterpaper',"
v0.1.1-alpha,"The font size ('10pt', '11pt' or '12pt')."
v0.1.1-alpha,
v0.1.1-alpha,"'pointsize': '10pt',"
v0.1.1-alpha,Additional stuff for the LaTeX preamble.
v0.1.1-alpha,
v0.1.1-alpha,"'preamble': '',"
v0.1.1-alpha,Latex figure (float) alignment
v0.1.1-alpha,
v0.1.1-alpha,"'figure_align': 'htbp',"
v0.1.1-alpha,Grouping the document tree into LaTeX files. List of tuples
v0.1.1-alpha,"(source start file, target name, title,"
v0.1.1-alpha,"author, documentclass [howto, manual, or own class])."
v0.1.1-alpha,-- Options for manual page output ------------------------------------------
v0.1.1-alpha,One entry per manual page. List of tuples
v0.1.1-alpha,"(source start file, name, description, authors, manual section)."
v0.1.1-alpha,-- Options for Texinfo output ----------------------------------------------
v0.1.1-alpha,Grouping the document tree into Texinfo files. List of tuples
v0.1.1-alpha,"(source start file, target name, title, author,"
v0.1.1-alpha,"dir menu entry, description, category)"
v0.1.1-alpha,-- Options for Epub output -------------------------------------------------
v0.1.1-alpha,Bibliographic Dublin Core info.
v0.1.1-alpha,The unique identifier of the text. This can be a ISBN number
v0.1.1-alpha,or the project homepage.
v0.1.1-alpha,
v0.1.1-alpha,epub_identifier = ''
v0.1.1-alpha,A unique identification for the text.
v0.1.1-alpha,
v0.1.1-alpha,epub_uid = ''
v0.1.1-alpha,A list of files that should not be packed into the epub file.
v0.1.1-alpha,-- Extension configuration -------------------------------------------------
v0.1.1-alpha,-- Options for todo extension ----------------------------------------------
v0.1.1-alpha,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.1.1-alpha,"self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))"
v0.1.1-alpha,self.ate = np.mean(self.df['y1'] - self.df['y0'])
v0.1.1-alpha,treated = self.df[self.df['z']==1]
v0.1.1-alpha,self.att = np.mean(treated['y1'] - treated['y0'])
v0.1.1-alpha,def test_average_treatment_effect(self):
v0.1.1-alpha,est_ate = 1
v0.1.1-alpha,bias = est_ate - self.ate
v0.1.1-alpha,print(bias)
v0.1.1-alpha,"self.assertAlmostEqual(self.ate, est_ate)"
v0.1.1-alpha,def test_average_treatment_effect_on_treated(self):
v0.1.1-alpha,est_att = 1
v0.1.1-alpha,self.att=1
v0.1.1-alpha,bias = est_att - self.att
v0.1.1-alpha,print(bias)
v0.1.1-alpha,"self.assertAlmostEqual(self.att, est_att)"
v0.1.1-alpha,cov_mat = np.diag(np.ones(num_features))
v0.1.1-alpha,Now checking if there is also a valid iv estimand
v0.1.1-alpha,"TODO: outputs string for now, but ideally should do symbolic"
v0.1.1-alpha,expressions Mon 19 Feb 2018 04:54:17 PM DST
v0.1.1-alpha,[TODO: support multivariate states]
v0.1.1-alpha,"sym_common_causes = [sp.stats.Normal(common_cause, sym_mu, sym_sigma) for common_cause in common_causes]"
v0.1.1-alpha,[TODO: support multivariate states]
v0.1.1-alpha,TODO: move the logging level argument to a json file. Tue 20 Feb 2018 06:56:27 PM DST
v0.1.1-alpha,Check if estimator's target estimand is identified
v0.1.1-alpha,Check if estimator's target estimand is identified
v0.1.1-alpha,load dot file
v0.1.1-alpha,Adding common causes
v0.1.1-alpha,Adding instruments
v0.1.1-alpha,Adding unobserved confounders
v0.1.1-alpha,[TODO: double check these work with multivariate implementation:]
v0.1.1-alpha,Exclusion
v0.1.1-alpha,As-if-random setup
v0.1.1-alpha,As-if-random
v0.1.1-alpha,Currently estimation methods only support univariate treatment and outcome
v0.1.1-alpha,self._estimate = est
v0.1.1-alpha,Doing a two-sided test
v0.1.1-alpha,Being conservative with the p-value reported
v0.1.1-alpha,Being conservative with the p-value reported
v0.1.1-alpha,TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)
v0.1.1-alpha,identified_estimand = IdentifiedEstimand(
v0.1.1-alpha,"treatment_variable = self._treatment_name,"
v0.1.1-alpha,"outcome_variable = self._outcome_name,"
v0.1.1-alpha,backdoor_variables = new_backdoor_variables)#self._target_estimand.backdoor_variables)#new_backdoor_variables)
v0.1.1-alpha,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.1.1-alpha,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.1.1-alpha,choosing the instrumental variable to use
v0.1.1-alpha,Obtain estimate by Wald Estimator
v0.1.1-alpha,Obtain estimate by Pearl (1995) ratio estimator.
v0.1.1-alpha,y = x+ u; multiply both sides by z and take expectation.
v0.1.1-alpha,sort the dataframe by propensity score
v0.1.1-alpha,create a column 'strata' for each element that marks what strata it belongs to
v0.1.1-alpha,"for each strata, count how many treated and control units there are"
v0.1.1-alpha,throw away strata that have insufficient treatment or control
v0.1.1-alpha,"print(""before clipping, here is the distribution of treatment and control per strata"")"
v0.1.1-alpha,"print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())"
v0.1.1-alpha,"print(""after clipping at threshold, now we have:"" )"
v0.1.1-alpha,"print(clipped.groupby(['strata',self._treatment_name])[self._outcome_name].count())"
v0.1.1-alpha,sum weighted outcomes over all strata  (weight by treated population)
v0.1.1-alpha,TODO - how can we add additional information into the returned estimate?
v0.1.1-alpha,"such as how much clipping was done, or per-strata info for debugging?"
v0.1.1-alpha,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.1.1-alpha,trim propensity score weights
v0.1.1-alpha,ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))
v0.1.1-alpha,nips ==> ips / (sum of ips over all units)
v0.1.1-alpha,icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)
v0.1.1-alpha,itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)
v0.1.1-alpha,TODO - how can we add additional information into the returned estimate?
v0.1.1-alpha,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.1.1-alpha,this assumes a binary treatment regime
v0.1.1-alpha,TODO remove neighbors that are more than a given radius apart
v0.1.1-alpha,estimate ATE on treated by summing over difference between matched neighbors
v0.1.1-alpha,TODO -- fix: we are actually conditioning on positive treatment (d=1)
v0.1.1-alpha,from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version
v0.1.1-alpha,self._identified_estimand = self._causal_model.identify_effect()
v0.1.1-alpha,"self._identified_estimand,"
v0.1.1-alpha,"self._causal_model._treatment,"
v0.1.1-alpha,"self._causal_model._outcome,"
